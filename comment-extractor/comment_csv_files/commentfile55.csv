 SPDX-License-Identifier: GPL-2.0-only

/* L2TP core.

 *

 * Copyright (c) 2008,2009,2010 Katalix Systems Ltd

 *

 * This file contains some code of the original L2TPv2 pppol2tp

 * driver, which has the following copyright:

 *

 * Authors:	Martijn van Oosterhout <kleptog@svana.org>

 *		James Chapman (jchapman@katalix.com)

 * Contributors:

 *		Michal Ostrowski <mostrows@speakeasy.net>

 *		Arnaldo Carvalho de Melo <acme@xconectiva.com.br>

 *		David S. Miller (davem@redhat.com)

 L2TP header constants */

 L2TPv3 default L2-specific sublayer */

 Default trace flags */

/* Private data stored for received packets in the skb.

 per-net private data for this module */

 Lock for write access to l2tp_tunnel_list */

 Lock for write access to l2tp_session_hlist */

/* Session hash global list for L2TPv3.

 * The session_id SHOULD be random according to RFC3931, but several

 * L2TP implementations use incrementing session_ids.  So we do a real

 * hash on the session_id, rather than a simple bitmask.

/* Session hash list.

 * The session_id SHOULD be random according to RFC2661, but several

 * L2TP implementations (Cisco and Microsoft) use incrementing

 * session_ids.  So we do a real hash on the session_id, rather than a

 * simple bitmask.

 the tunnel is freed in the socket destructor */

 Lookup a tunnel. A new reference is held on the returned tunnel. */

/* Lookup a session by interface name.

 * This is very inefficient but is only used by management interfaces.

		/* IP encap expects session IDs to be globally unique, while

		 * UDP encap doesn't.

/*****************************************************************************

 * Receive data handling

/* Queue a skb in order. We come here only if the skb has an L2TP sequence

 * number.

/* Dequeue a single skb.

	/* We're about to requeue the skb, so return resources

	 * to its current owner (a socket receive buffer).

 Bump our Nr */

 call private receive handler */

/* Dequeue skbs from the session's reorder_q, subject to packet order.

 * Skbs that have been in the queue for too long are simply discarded.

	/* If the pkt at the head of the queue has the nr that we

	 * expect to send up next, dequeue it and any other

	 * in-sequence packets behind it.

 If the packet has been pending on the queue for too long, discard it */

		/* Process the skb. We release the queue lock while we

		 * do so to let other contexts process the queue.

/* If packet has sequence numbers, queue it if acceptable. Returns 0 if

 * acceptable, else non-zero.

		/* Packet sequence number is outside allowed window.

		 * Discard it.

		/* Packet reordering enabled. Add skb to session's

		 * reorder queue, in order of ns.

	/* Packet reordering disabled. Discard out-of-sequence packets, while

	 * tracking the number if in-sequence packets after the first OOS packet

	 * is seen. After nr_oos_count_max in-sequence packets, reset the

	 * sequence number to re-enable packet reception.

/* Do receive processing of L2TP data frames. We handle both L2TPv2

 * and L2TPv3 data frames here.

 *

 * L2TPv2 Data Message Header

 *

 *  0                   1                   2                   3

 *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |T|L|x|x|S|x|O|P|x|x|x|x|  Ver  |          Length (opt)         |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |           Tunnel ID           |           Session ID          |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |             Ns (opt)          |             Nr (opt)          |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |      Offset Size (opt)        |    Offset pad... (opt)

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 *

 * Data frames are marked by T=0. All other fields are the same as

 * those in L2TP control frames.

 *

 * L2TPv3 Data Message Header

 *

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |                      L2TP Session Header                      |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |                      L2-Specific Sublayer                     |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |                        Tunnel Payload                      ...

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 *

 * L2TPv3 Session Header Over IP

 *

 *  0                   1                   2                   3

 *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |                           Session ID                          |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |               Cookie (optional, maximum 64 bits)...

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 *                                                                 |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 *

 * L2TPv3 L2-Specific Sublayer Format

 *

 *  0                   1                   2                   3

 *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |x|S|x|x|x|x|x|x|              Sequence Number                  |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 *

 * Cookie value and sublayer format are negotiated with the peer when

 * the session is set up. Unlike L2TPv2, we do not need to parse the

 * packet header to determine if optional fields are present.

 *

 * Caller must already have parsed the frame and determined that it is

 * a data (not control) frame before coming here. Fields up to the

 * session-id have already been parsed and ptr points to the data

 * after the session-id.

 Parse and check optional cookie */

	/* Handle the optional sequence numbers. Sequence numbers are

	 * in different places for L2TPv2 and L2TPv3.

	 *

	 * If we are the LAC, enable/disable sequence numbers under

	 * the control of the LNS.  If no sequence numbers present but

	 * we were expecting them, discard frame.

 Store L2TP info in the skb */

 Skip past nr in the header */

 Store L2TP info in the skb */

		/* Received a packet with sequence numbers. If we're the LAC,

		 * check if we sre sending sequence numbers and if not,

		 * configure it so.

		/* No sequence numbers.

		 * If user has configured mandatory sequence numbers, discard.

		/* If we're the LAC and we're sending sequence numbers, the

		 * LNS has requested that we no longer send sequence numbers.

		 * If we're the LNS and we're sending sequence numbers, the

		 * LAC is broken. Discard the frame.

	/* Session data offset is defined only for L2TPv2 and is

	 * indicated by an optional 16-bit value in the header.

 If offset bit set, skip it. */

	/* Prepare skb for adding to the session's reorder_q.  Hold

	 * packets for max reorder_timeout or 1 second if not

	 * reordering.

	/* Add packet to the session's receive queue. Reordering is done here, if

	 * enabled. Saved L2TP protocol info is stored in skb->sb[].

		/* No sequence numbers. Add the skb to the tail of the

		 * reorder queue. This ensures that it will be

		 * delivered after all previous sequenced skbs.

 Try to dequeue as many skbs from reorder_q as we can. */

/* Drop skbs from the session's reorder_q

/* Internal UDP receive frame. Do the real work of receiving an L2TP data frame

 * here. The skb is not on a list when we get here.

 * Returns 0 if the packet was a data packet and was successfully passed on.

 * Returns 1 if the packet was not a good data packet and could not be

 * forwarded.  All such packets are passed up to userspace to deal with.

 UDP has verified checksum */

 UDP always verifies the packet length. */

 Short packet? */

 Point to L2TP header */

 Get L2TP header flags */

 Check protocol version */

 Get length of L2TP packet */

 If type is control packet, it is handled by userspace. */

 Skip flags */

 If length is present, skip it */

 Extract tunnel and session ID */

 skip reserved bits */

 Find the session context */

 Not found? Pass to userspace to deal with */

 Put UDP header back */

/* UDP encapsulation receive handler. See net/ipv4/udp.c.

 * Return codes:

 * 0 : success.

 * <0: error

 * >0: skb should be passed up to userspace as UDP.

	/* Note that this is called from the encap_rcv hook inside an

	 * RCU-protected region, but without the socket being locked.

	 * Hence we use rcu_dereference_sk_user_data to access the

	 * tunnel data structure rather the usual l2tp_sk_to_tunnel

	 * accessor function.

/************************************************************************

 * Transmit handling

/* Build an L2TP header for the session into the buffer provided.

 Setup L2TP header. */

	/* Setup L2TP header. The header differs slightly for UDP and

	 * IP encapsulations. For UDP, there is 4 bytes of flags.

 Queue the packet to IP for output: tunnel socket lock must be held */

	/* Check that there's enough headroom in the skb to insert IP,

	 * UDP and L2TP headers. If not enough, expand it to

	 * make room. Adjust truesize.

 Setup L2TP header */

 Reset skb netfilter state */

	/* The user-space may change the connection status for the user-space

	 * provided socket at run time: we must check it under the socket lock

	/* Report transmitted length before we add encap header, which keeps

	 * statistics consistent for both UDP and IP encap tx/rx paths.

 Setup UDP header */

 Calculate UDP checksum if configured to do so */

/* If caller requires the skb to have a ppp header, the header must be

 * inserted in the skb data before calling this function.

/*****************************************************************************

 * Tinnel and session create/destroy.

/* Tunnel socket destruct hook.

 * The tunnel context is deleted only when all session sockets have been

 * closed.

 Disable udp encapsulation */

 No longer an encapsulation socket. See net/ipv4/udp.c */

 Remove hooks into tunnel socket */

 Call the original destructor */

 Remove an l2tp session from l2tp_core's hash lists. */

 Remove the session from core hashes */

 Remove from the per-tunnel hash */

 For L2TPv3 we have a per-net hash: remove from there, too */

/* When the tunnel is closed, all the attached sessions need to go too.

			/* Now restart from the beginning of this hash

			 * chain.  We always remove a session from the

			 * list so we are guaranteed to make forward

			 * progress.

 Tunnel socket destroy hook for UDP encapsulation */

 Workqueue tunnel deletion function */

	/* If the tunnel socket was created within the kernel, use

	 * the sk API to release it here.

 Remove the tunnel struct from the tunnel list */

 drop initial ref */

 drop workqueue ref */

/* Create a socket for the tunnel, if one isn't set up by

 * userspace. This is used for static tunnels where there is no

 * managing L2TP daemon.

 *

 * Since we don't want these sockets to keep a namespace alive by

 * themselves, we drop the socket's namespace refcount after creation.

 * These sockets are freed when the namespace exits using the pernet

 * exit hook.

 Init delete workqueue struct */

/* This function is used by the netlink TUNNEL_DELETE command.

/* We come here whenever a session's send_seq, cookie_len or

 * l2specific_type parameters are set.

 Use NR of first received packet */

/*****************************************************************************

 * Init and cleanup

 SPDX-License-Identifier: GPL-2.0-or-later

/* L2TP subsystem debugfs

 *

 * Copyright (c) 2010 Katalix Systems Ltd

 current tunnel */

 index of session within current tunnel */

 NULL means get next tunnel */

 Drop reference taken during previous invocation */

 Drop reference taken during previous invocation */

 NULL tunnel and session indicates end of list */

	/* Drop reference taken by last invocation of l2tp_dfs_next_session()

	 * or l2tp_dfs_next_tunnel().

 display header on line 1 */

	/* Derive the network namespace from the pid opening the

	 * file.

 SPDX-License-Identifier: GPL-2.0-or-later

/* L2TPv3 IP encapsulation support

 *

 * Copyright (c) 2008,2009,2010 Katalix Systems Ltd

 inet_sock has to be the first member of l2tp_ip_sock */

/* When processing receive frames, there are two cases to

 * consider. Data frames consist of a non-zero session-id and an

 * optional cookie. Control frames consist of a regular L2TP header

 * preceded by 32-bits of zeros.

 *

 * L2TPv3 Session Header Over IP

 *

 *  0                   1                   2                   3

 *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |                           Session ID                          |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |               Cookie (optional, maximum 64 bits)...

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 *                                                                 |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 *

 * L2TPv3 Control Message Header Over IP

 *

 *  0                   1                   2                   3

 *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |                      (32 bits of zeros)                       |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |T|L|x|x|S|x|x|x|x|x|x|x|  Ver  |             Length            |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |                     Control Connection ID                     |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |               Ns              |               Nr              |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 *

 * All control frames are passed to userspace.

 Point to L2TP header */

	/* RFC3931: L2TP/IP packets have the first 4 bytes containing

	 * the session_id. If it is 0, the packet is a L2TP control

	 * frame and the session_id value can be discarded.

 Ok, this is a data packet. Lookup the session. */

 Get the tunnel_id from the L2TP header */

 Prevent autobind. We don't have ports. */

 Use device */

 Must bind first - autobinding does not work */

 Charge it to the socket, dropping if the queue is full. */

/* Userspace will call sendmsg() on the tunnel socket to send L2TP

 * control frames.

 Get and verify the address. */

 Allocate a socket buffer */

 Reserve space for headers, putting IP header on 4-byte boundary. */

 Insert 0 session_id */

 Copy user data into skb */

 Use correct destination address if we have options. */

		/* If this fails, retransmit mechanism of transport layer will

		 * keep trying until route appears or the connection times

		 * itself out.

	/* We don't need to clone dst here, it is guaranteed to not disappear.

	 *  __dev_xmit_skb() might force a refcount if needed.

 Queue the packet to IP for output */

 Copy the address. */

/* Use the value of SOCK_DGRAM (2) directory, because __stringify doesn't like

 * enums

 SPDX-License-Identifier: GPL-2.0-or-later

/*****************************************************************************

 * Linux PPP over L2TP (PPPoX/PPPoL2TP) Sockets

 *

 * PPPoX    --- Generic PPP encapsulation socket family

 * PPPoL2TP --- PPP over L2TP (RFC 2661)

 *

 * Version:	2.0.0

 *

 * Authors:	James Chapman (jchapman@katalix.com)

 *

 * Based on original work by Martijn van Oosterhout <kleptog@svana.org>

 *

 * License:

/* This driver handles only L2TP data frames; control frames are handled by a

 * userspace application.

 *

 * To send data in an L2TP session, userspace opens a PPPoL2TP socket and

 * attaches it to a bound UDP socket with local tunnel_id / session_id and

 * peer tunnel_id / session_id set. Data can then be sent or received using

 * regular socket sendmsg() / recvmsg() calls. Kernel parameters of the socket

 * can be read or modified using ioctl() or [gs]etsockopt() calls.

 *

 * When a PPPoL2TP socket is connected with local and peer session_id values

 * zero, the socket is treated as a special tunnel management socket.

 *

 * Here's example userspace code to create a socket for sending/receiving data

 * over an L2TP session:-

 *

 *	struct sockaddr_pppol2tp sax;

 *	int fd;

 *	int session_fd;

 *

 *	fd = socket(AF_PPPOX, SOCK_DGRAM, PX_PROTO_OL2TP);

 *

 *	sax.sa_family = AF_PPPOX;

 *	sax.sa_protocol = PX_PROTO_OL2TP;

 *	sax.pppol2tp.fd = tunnel_fd;	// bound UDP socket

 *	sax.pppol2tp.addr.sin_addr.s_addr = addr->sin_addr.s_addr;

 *	sax.pppol2tp.addr.sin_port = addr->sin_port;

 *	sax.pppol2tp.addr.sin_family = AF_INET;

 *	sax.pppol2tp.s_tunnel  = tunnel_id;

 *	sax.pppol2tp.s_session = session_id;

 *	sax.pppol2tp.d_tunnel  = peer_tunnel_id;

 *	sax.pppol2tp.d_session = peer_session_id;

 *

 *	session_fd = connect(fd, (struct sockaddr *)&sax, sizeof(sax));

 *

 * A pppd plugin that allows PPP traffic to be carried over L2TP using

 * this driver is available from the OpenL2TP project at

 * http://openl2tp.sourceforge.net.

 Space for UDP, L2TP and PPP headers */

/* Number of bytes to build transmit L2TP headers.

 * Unfortunately the size is different depending on whether sequence numbers

 * are enabled.

/* Private data of each session. This data lives at the end of struct

 * l2tp_session, referenced via session->priv[].

 pid that opened the socket */

 Protects .sk */

 Pointer to the session PPPoX socket */

 Copy of .sk, for cleanup */

 For asynchronous release */

/* Retrieves the pppol2tp socket associated to a session.

 * A reference is held on the returned socket, so this function must be paired

 * with sock_put().

/* Helpers to obtain tunnel/session contexts from sockets.

/*****************************************************************************

 * Receive data handling

/* Receive message. This is the recvmsg for the PPPoL2TP socket.

	/* If the socket is bound, send it in to PPP's input queue. Otherwise

	 * queue it on the session socket.

	/* If the first two bytes are 0xFF03, consider that it is the PPP's

	 * Address and Control fields and skip them. The L2TP module has always

	 * worked this way, although, in theory, the use of these fields should

	 * be negotiated and handled at the PPP layer. These fields are

	 * constant: 0xFF is the All-Stations Address and 0x03 the Unnumbered

	 * Information command with Poll/Final bit set to zero (RFC 1662).

/************************************************************************

 * Transmit handling

/* This is the sendmsg for the PPPoL2TP pppol2tp_session socket.  We come here

 * when a user application does a sendmsg() on the session socket. L2TP and

 * PPP headers must be inserted into the user's data.

 Get session and tunnel contexts */

 Allocate a socket buffer */

 2 bytes for PPP_ALLSTATIONS & PPP_UI */

 Reserve space for headers. */

 Add PPP header */

 Copy user data into skb */

/* Transmit function called by generic PPP driver.  Sends PPP frame

 * over PPPoL2TP socket.

 *

 * This is almost the same as pppol2tp_sendmsg(), but rather than

 * being called with a msghdr from userspace, it is called with a skb

 * from the kernel.

 *

 * The supplied skb from ppp doesn't have enough headroom for the

 * insertion of L2TP, UDP and IP headers so we need to allocate more

 * headroom in the skb. This will create a cloned skb. But we must be

 * careful in the error case because the caller will expect to free

 * the skb it supplied, not our cloned skb. So we take care to always

 * leave the original skb unfreed if we return an error.

 Get session and tunnel contexts from the socket */

 IP header */

 UDP header (if L2TP_ENCAPTYPE_UDP) */

 L2TP header */

 2 bytes for PPP_ALLSTATIONS & PPP_UI */

 Setup PPP header */

 Free the original skb */

/*****************************************************************************

 * Session (and tunnel control) socket create/destroy.

/* Really kill the session socket. (Called from sock_put() if

 * refcnt == 0.)

/* Called when the PPPoX socket (session) is closed.

 Signal the death of the socket. */

		/* Rely on the sock_put() call at the end of the function for

		 * dropping the reference held by pppol2tp_sock_to_session().

		 * The last reference will be dropped by pppol2tp_put_sk().

	/* This will delete the session context via

	 * pppol2tp_session_destruct() if the socket's refcnt drops to

	 * zero.

/* socket() handler. Initialize a new struct sock.

/* Rough estimation of the maximum payload size a tunnel can transmit without

 * fragmenting at the lower IP layer. Assumes L2TPv2 with sequence

 * numbers and no IP option. Not quite accurate, but the result is mostly

 * unused anyway.

/* connect() handler. Attach a PPPoX socket to a tunnel UDP socket

 Check for already bound sockets */

 We don't supporting rebinding anyway */

 socket is already attached */

 Don't bind if tunnel_id is 0 */

	/* Special case: create tunnel context if session_id and

	 * peer_session_id is 0. Otherwise look up tunnel using supplied

	 * tunnel id.

			/* Prevent l2tp_tunnel_register() from trying to set up

			 * a kernel socket.

 Error if we can't find the tunnel */

 Error if socket is not prepped */

		/* Using a pre-existing session is fine as long as it hasn't

		 * been connected yet.

	/* Special case: if source & dest session_id == 0x0000, this

	 * socket is being created to manage the tunnel. Just set up

	 * the internal context for use by ioctl() and sockopt()

	 * handlers.

	/* The only header we need to worry about is the L2TP

	 * header. This size is different depending on whether

	 * sequence numbers are enabled for the data channel.

 This is how we get the session context from the socket. */

	/* Keep the reference we've grabbed on the session: sk doesn't expect

	 * the session to disappear. pppol2tp_session_destruct() is responsible

	 * for dropping it.

 Called when creating sessions via the netlink interface. */

 Error if tunnel socket is not prepped */

 Allocate and initialize a new session context. */

 CONFIG_L2TP_V3 */

/* getname() support.

/****************************************************************************

 * ioctl() handlers.

 *

 * The PPPoX socket is created for L2TP sessions: tunnels have their own UDP

 * sockets. However, in order to control kernel tunnel features, we allow

 * userspace to create a special "tunnel" PPPoX socket which is used for

 * control only.  Tunnel PPPoX sockets have session_id == 0 and simply allow

 * the user application to issue L2TP setsockopt(), getsockopt() and ioctl()

 * calls.

	/* If session_id is set, search the corresponding session in the

	 * context of this tunnel and record the session's statistics.

 Not defined for tunnels */

 Not defined for tunnels */

 Session 0 represents the parent tunnel */

/*****************************************************************************

 * setsockopt() / getsockopt() support.

 *

 * The PPPoX socket is created for L2TP sessions: tunnels have their own UDP

 * sockets. In order to control kernel tunnel features, we allow userspace to

 * create a special "tunnel" PPPoX socket which is used for control only.

 * Tunnel PPPoX sockets have session_id == 0 and simply allow the user

 * application to issue L2TP setsockopt(), getsockopt() and ioctl() calls.

/* Tunnel setsockopt() helper.

 Tunnel debug flags option is deprecated */

/* Session setsockopt helper.

 Session debug flags option is deprecated */

/* Main setsockopt() entry point.

 * Does API checks, then calls either the tunnel or session setsockopt

 * handler, according to whether the PPPoL2TP socket is a for a regular

 * session or the special tunnel type.

 Get session context from the socket */

	/* Special case: if session_id == 0x0000, treat as operation on tunnel

/* Tunnel getsockopt helper. Called with sock locked.

 Tunnel debug flags option is deprecated */

/* Session getsockopt helper. Called with sock locked.

 Session debug flags option is deprecated */

/* Main getsockopt() entry point.

 * Does API checks, then calls either the tunnel or session getsockopt

 * handler, according to whether the PPPoX socket is a for a regular session

 * or the special tunnel type.

 Get the session context */

 Special case: if session_id == 0x0000, treat as operation on tunnel */

/*****************************************************************************

 * /proc filesystem for debug

 * Since the original pppol2tp driver provided /proc/net/pppol2tp for

 * L2TPv2, we dump only L2TPv2 tunnels and sessions here.

 current tunnel */

 index of session within current tunnel */

 NULL means get next tunnel */

 Drop reference taken during previous invocation */

 Only accept L2TPv2 tunnels */

 Drop reference taken during previous invocation */

 NULL tunnel and session indicates end of list */

	/* Drop reference taken by last invocation of pppol2tp_next_session()

	 * or pppol2tp_next_tunnel().

 display header on line 1 */

 CONFIG_PROC_FS */

/*****************************************************************************

 * Network namespace

/*****************************************************************************

 * Init and cleanup

 CONFIG_L2TP_V3 */

 SPDX-License-Identifier: GPL-2.0-or-later

/* L2TPv3 ethernet pseudowire driver

 *

 * Copyright (c) 2008,2009,2010 Katalix Systems Ltd

 Default device name. May be overridden by name specified by user */

 via netdev_priv() */

 via l2tp_session_priv() */

	/* No need for synchronize_net() here. We're called by

	 * unregister_netdev*(), which does the synchronisation for us.

 checksums verified by L2TP */

 if the encap is UDP, account for UDP header size */

		/* L3 Overhead couldn't be identified, this could be

		 * because tunnel->sock was NULL or the socket's

		 * address family was not IPv4 or IPv6,

		 * dev mtu stays at 1500.

	/* Adjust MTU, factor overhead - underlay L3, overlay L2 hdr

	 * UDP overhead, if any, was already factored in above.

	/* Register both device and session while holding the rtnl lock. This

	 * ensures that l2tp_eth_delete() will see that there's a device to

	 * unregister, even if it happened to run before we assign spriv->dev.

 SPDX-License-Identifier: GPL-2.0-only

/* L2TP netlink layer, for management

 *

 * Copyright (c) 2008,2009,2010 Katalix Systems Ltd

 *

 * Partly based on the IrDA nelink implementation

 * (see net/irda/irnetlink.c) which is:

 * Copyright (c) 2007 Samuel Ortiz <samuel@sortiz.org>

 * which is in turn partly based on the wireless netlink code:

 * Copyright 2006 Johannes Berg <johannes@sipsolutions.net>

 Accessed under genl lock */

 We don't care if no one is listening */

 We don't care if no one is listening */

 Must have either AF_INET or AF_INET6 address for source and destination */

	/* Managed tunnels take the tunnel socket from userspace.

	 * Unmanaged tunnels must call out the source and destination addresses

	 * for the kernel to create the tunnel socket itself.

/* Append attributes for the tunnel address, handling the different attribute types

 * used for different tunnel encapsulation and AF_INET v.s. AF_INET6.

 L2TPv2 only accepts PPP pseudo-wires */

 can be retrieved by unprivileged users */

 SPDX-License-Identifier: GPL-2.0-or-later

/* L2TPv3 IP encapsulation support for IPv6

 *

 * Copyright (c) 2012 Katalix Systems Ltd

 inet_sock has to be the first member of l2tp_ip6_sock */

	/* ipv6_pinfo has to be the last member of l2tp_ip6_sock, see

	 * inet6_sk_generic

/* When processing receive frames, there are two cases to

 * consider. Data frames consist of a non-zero session-id and an

 * optional cookie. Control frames consist of a regular L2TP header

 * preceded by 32-bits of zeros.

 *

 * L2TPv3 Session Header Over IP

 *

 *  0                   1                   2                   3

 *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |                           Session ID                          |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |               Cookie (optional, maximum 64 bits)...

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 *                                                                 |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 *

 * L2TPv3 Control Message Header Over IP

 *

 *  0                   1                   2                   3

 *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |                      (32 bits of zeros)                       |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |T|L|x|x|S|x|x|x|x|x|x|x|  Ver  |             Length            |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |                     Control Connection ID                     |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 * |               Ns              |               Nr              |

 * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

 *

 * All control frames are passed to userspace.

 Point to L2TP header */

	/* RFC3931: L2TP/IP packets have the first 4 bytes containing

	 * the session_id. If it is 0, the packet is a L2TP control

	 * frame and the session_id value can be discarded.

 Ok, this is a data packet. Lookup the session. */

 Get the tunnel_id from the L2TP header */

 Prevent autobind. We don't have ports. */

 l2tp_ip6 sockets are IPv6 only */

 L2TP is point-point, not multicast */

 Check if the address belongs to the host. */

			/* Binding to link-local address requires an

			 * interface.

		/* ipv4 addr of the socket is invalid.  Only the

		 * unspecified and mapped address have a v4 equivalent.

 Must bind first - autobinding does not work */

 Charge it to the socket, dropping if the queue is full. */

/* Userspace will call sendmsg() on the tunnel socket to send L2TP

 * control frames.

 zero session-id */

	/* Rough check on arithmetic overflow,

	 * better check is made in ip6_append_data().

 Mirror BSD error message compatibility */

 Get and verify the address */

		/* Otherwise it will be difficult to maintain

		 * sk->sk_dst_cache.

 :: means loopback (BSD'ism) */

 Copy the address. */

/* Use the value of SOCK_DGRAM (2) directory, because __stringify doesn't like

 * enums

 SPDX-License-Identifier: GPL-2.0

/* Copyright (c) 2019, Vladimir Oltean <olteanv@gmail.com>

 Is this a TX or an RX header? */

 RX header */

 Trap-to-host format (no trailer present) */

 Timestamp format (trailer present) */

 Meta frame format (for 2-step TX timestamps) */

 TX header */

 Only valid if SJA1110_TX_HEADER_HAS_TRAILER is false */

 Only valid if SJA1110_TX_HEADER_HAS_TRAILER is true */

 Similar to is_link_local_ether_addr(hdr->h_dest) but also covers PTP */

	/* UM10944.pdf section 4.2.17 AVB Parameters:

	 * Structure of the meta-data follow-up frame.

	 * It is in network byte order, so there are no quirks

	 * while unpacking the meta frame.

	 *

	 * Also SJA1105 E/T only populates bits 23:0 of the timestamp

	 * whereas P/Q/R/S does 32 bits. Since the structure is the

	 * same and the E/T puts zeroes in the high-order byte, use

	 * a unified unpacking command for both device series.

 Calls sja1105_port_deferred_xmit in sja1105_main.c */

	/* Increase refcount so the kfree_skb in dsa_slave_xmit

	 * won't really free the packet.

/* Send VLAN tags with a TPID that blends in with whatever VLAN protocol a

 * bridge spanning ports of this switch might have.

	/* Since VLAN awareness is global, then if this port is VLAN-unaware,

	 * all ports are. Use the VLAN-unaware TPID used for tag_8021q.

	/* Port is VLAN-aware, so there is a bridge somewhere (a single one,

	 * we're sure about that). It may not be on this port though, so we

	 * need to find it.

		/* Error is returned only if CONFIG_BRIDGE_VLAN_FILTERING,

		 * which seems pointless to handle, as our port cannot become

		 * VLAN-aware in that case.

	/* If the port is under a VLAN-aware bridge, just slide the

	 * VLAN-tagged packet into the FDB and hope for the best.

	 * This works because we support a single VLAN-aware bridge

	 * across the entire dst, and its VLANs cannot be shared with

	 * any standalone port.

	/* If the port is under a VLAN-unaware bridge, use an imprecise

	 * TX VLAN that targets the bridge's entire broadcast domain,

	 * instead of just the specific port.

/* Transform untagged control packets into pvid-tagged control packets so that

 * all packets sent by this tagger are VLAN-tagged and we can configure the

 * switch to drop untagged packets coming from the DSA master.

	/* If VLAN tag is in hwaccel area, move it to the payload

	 * to deal with both cases uniformly and to ensure that

	 * the VLANs are added in the right order.

 If skb is already VLAN-tagged, leave that VLAN ID in place */

	/* Transmitting management traffic does not rely upon switch tagging,

	 * but instead SPI-installed management routes. Part 2 of this

	 * is the .port_deferred_xmit driver callback.

	/* Transmitting control packets is done using in-band control

	 * extensions, while data packets are transmitted using

	 * tag_8021q TX VLANs.

/* This is a simple state machine which follows the hardware mechanism of

 * generating RX timestamps:

 *

 * After each timestampable skb (all traffic for which send_meta1 and

 * send_meta0 is true, aka all MAC-filtered link-local traffic) a meta frame

 * containing a partial timestamp is immediately generated by the switch and

 * sent as a follow-up to the link-local frame on the CPU port.

 *

 * The meta frames have no unique identifier (such as sequence number) by which

 * one may pair them to the correct timestampable frame.

 * Instead, the switch has internal logic that ensures no frames are sent on

 * the CPU port between a link-local timestampable frame and its corresponding

 * meta follow-up. It also ensures strict ordering between ports (lower ports

 * have higher priority towards the CPU port). For this reason, a per-port

 * data structure is not needed/desirable.

 *

 * This function pairs the link-local frame with its partial timestamp from the

 * meta follow-up frame. The full timestamp will be reconstructed later in a

 * work queue.

	/* Step 1: A timestampable frame was received.

	 * Buffer it until we get its meta frame.

 Do normal processing. */

		/* Was this a link-local frame instead of the meta

		 * that we were expecting?

		/* Hold a reference to avoid dsa_switch_rcv

		 * from freeing the skb.

 Tell DSA we got nothing */

	/* Step 2: The meta frame arrived.

	 * Time to take the stampable skb out of the closet, annotate it

	 * with the partial timestamp, and pretend that we received it

	 * just now (basically masquerade the buffered frame as the meta

	 * frame, which serves no further purpose).

		/* Drop the meta frame if we're not in the right state

		 * to process it.

		/* Was this a meta frame instead of the link-local

		 * that we were expecting?

		/* Free the meta frame and give DSA the buffered stampable_skb

		 * for further processing up the network stack.

/* If the VLAN in the packet is a tag_8021q one, set @source_port and

 * @switch_id and strip the header. Otherwise set @vid and keep it in the

 * packet.

 Try our best with imprecise RX */

 Normal traffic path. */

		/* Management traffic path. Switch embeds the switch ID and

		 * port ID into bytes of the destination MAC, courtesy of

		 * the incl_srcpt options.

 Clear the DMAC bytes that were mangled by the switch */

 We don't care about RX timestamps on the CPU port */

 Discard the meta frame, we've consumed the timestamps it contained */

	/* skb->data points to skb_mac_header(skb) + ETH_HLEN, which is exactly

	 * what we need because the caller has checked the EtherType (which is

	 * located 2 bytes back) and we just need a pointer to the header that

	 * comes afterwards.

 Timestamp frame, we have a trailer */

		/* The timestamp is unaligned, so we need to use packing()

		 * to get it

		/* skb->len counts from skb->data, while start_of_padding

		 * counts from the destination MAC address. Right now skb->data

		 * is still as set by the DSA master, so to trim away the

		 * padding and trailer we need to account for the fact that

		 * skb->data points to skb_mac_header(skb) + ETH_HLEN.

 Trap-to-host frame, no timestamp trailer */

 Advance skb->data past the DSA header */

	/* With skb->data in its final place, update the MAC header

	 * so that eth_hdr() continues to works properly.

 Packets with in-band control extensions might still have RX VLANs */

 No tag added for management frames, all ok */

	/* Management frames have 2 DSA tags on RX, so the needed_headroom we

	 * declared is fine for the generic dissector adjustment procedure.

 For the rest, there is a single DSA tag, the tag_8021q one */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Handling of a single switch chip, part of a switch fabric

 *

 * Copyright (c) 2017 Savoir-faire Linux Inc.

 *	Vivien Didelot <vivien.didelot@savoirfairelinux.com>

 Program the fastest ageing time in case of multiple bridges */

	/* Do not propagate to other switches in the tree if the notifier was

	 * targeted for a single switch.

	/* If the bridge was vlan_filtering, the bridge core doesn't trigger an

	 * event for changing vlan_filtering setting upon slave ports leaving

	 * it. That is a good thing, because that lets us handle it and also

	 * handle the case where the switch's vlan_filtering setting is global

	 * (not per port). When that happens, the correct moment to trigger the

	 * vlan_filtering callback is only when the last port leaves the last

	 * VLAN-aware bridge.

/* Matches for all upstream-facing ports (the CPU port and all upstream-facing

 * DSA links) that sit between the targeted port on which the notifier was

 * emitted and its dedicated CPU port.

 No need to bother with refcounting for user ports */

 No need to bother with refcounting for user ports */

 No need to bother with refcounting for user ports */

 No need to bother with refcounting for user ports */

	/* Do not deprogram the DSA links as they may be used as conduit

	 * for other VLAN members in the fabric.

	/* Now that changing the tag protocol can no longer fail, let's update

	 * the remaining bits which are "duplicated for faster access", and the

	 * bits that depend on the tagger, such as the MTU.

 rtnl_mutex is held in dsa_tree_change_tag_proto */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Handling of a single switch port

 *

 * Copyright (c) 2017 Savoir-faire Linux Inc.

 *	Vivien Didelot <vivien.didelot@savoirfairelinux.com>

/**

 * dsa_port_notify - Notify the switching fabric of changes to a port

 * @dp: port on which change occurred

 * @e: event, must be of type DSA_NOTIFIER_*

 * @v: event-specific value.

 *

 * Notify all switches in the DSA tree that this port's switch belongs to,

 * including this switch itself, of an event. Allows the other switches to

 * reconfigure themselves for cross-chip operations. Can also be used to

 * reconfigure ports without net_devices (CPU ports, DSA links) whenever

 * a user port's state changes.

 flush all VLANs */

	/* When the port becomes standalone it has already left the bridge.

	 * Don't notify the bridge in that case.

		/* Fast age FDB entries or flush appropriate forwarding database

		 * for the given port, if we are moving it from Learning or

		 * Forwarding state, to Disabled or Blocking or Listening state.

		 * Ports that were standalone before the STP state change don't

		 * need to fast age the FDB, since address learning is off in

		 * standalone mode.

	/* Configure the port for standalone mode (no address learning,

	 * flood everything).

	 * The bridge only emits SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS events

	 * when the user requests it through netlink or sysfs, but not

	 * automatically at port join or leave, so we need to handle resetting

	 * the brport flags ourselves. But we even prefer it that way, because

	 * otherwise, some setups might never get the notification they need,

	 * for example, when a port leaves a LAG that offloads the bridge,

	 * it becomes standalone, but as far as the bridge is concerned, no

	 * port ever left.

	/* Port left the bridge, put in BR_STATE_DISABLED by the bridge layer,

	 * so allow it to be in BR_STATE_FORWARDING to be kept functional

 VLAN filtering is handled by dsa_switch_bridge_leave */

	/* Ageing time may be global to the switch chip, so don't change it

	 * here because we have no good reason (or value) to change it to.

 No bridge TX forwarding offload => do nothing */

	/* Notify the chips only once the offload has been deactivated, so

	 * that they can update their configuration accordingly.

 Notify the driver */

	/* Here the interface is already bridged. Reflect the current

	 * configuration so that drivers can program their chips accordingly.

 Don't try to unoffload something that is not offloaded */

	/* Here the port is already unbridged. Reflect the current configuration

	 * so that drivers can program their chips accordingly.

	/* On statically configured aggregates (e.g. loadbalance

	 * without LACP) ports will always be tx_enabled, even if the

	 * link is down. Thus we require both link_up and tx_enabled

	 * in order to include it in the tx set.

	/* Port might have been part of a LAG that in turn was

	 * attached to a bridge.

 Must be called under rcu_read_lock() */

	/* VLAN awareness was off, so the question is "can we turn it on".

	 * We may have had 8021q uppers, those need to go. Make sure we don't

	 * enter an inconsistent state: deny changing the VLAN awareness state

	 * as long as we have 8021q uppers.

			/* br_vlan_get_info() returns -EINVAL or -ENOENT if the

			 * device, respectively the VID is not found, returning

			 * 0 means success, which is a failure for us here.

	/* For cases where enabling/disabling VLAN awareness is global to the

	 * switch, we need to handle the case where multiple bridges span

	 * different ports of the same switch device and one of them has a

	 * different setting than what is being requested.

		/* If it's the same bridge, it also has same

		 * vlan_filtering setting => no need to check

	/* We are called from dsa_slave_switchdev_blocking_event(),

	 * which is not under rcu_read_lock(), unlike

	 * dsa_slave_switchdev_event().

			/* We might be called in the unbind path, so not

			 * all slave devices might still be registered.

/* This enforces legacy behavior for switch drivers which assume they can't

 * receive VLAN configuration when enslaved to a bridge with vlan_filtering=0

 Only called for inband modes */

 SPDX-License-Identifier: GPL-2.0

/* Copyright 2019 NXP

/* If the port is under a VLAN-aware bridge, remove the VLAN header from the

 * payload and move it into the DSA tag, which will make the switch classify

 * the packet to the bridge VLAN. Otherwise, leave the classified VLAN at zero,

 * which is the pvid of standalone and VLAN-unaware bridge ports.

	/* Revert skb->data by the amount consumed by the DSA master,

	 * so it points to the beginning of the frame.

	/* We don't care about the short prefix, it is just for easy entrance

	 * into the DSA master's RX filter. Discard it now by moving it into

	 * the headroom.

	/* And skb->data now points to the extraction frame header.

	 * Keep a pointer to it.

 Now the EFH is part of the headroom as well */

 Reset the pointer to the real MAC header */

 And move skb->data to the correct location again */

 Remove from inet csum the extraction header */

		/* The switch will reflect back some frames sent through

		 * sockets opened on the bare DSA master. These will come back

		 * with src_port equal to the index of the CPU port, for which

		 * there is no slave registered. So don't print any error

		 * message here (ignore and drop those frames).

	/* Ocelot switches copy frames unmodified to the CPU. However, it is

	 * possible for the user to request a VLAN modification through

	 * VCAP_IS1_ACT_VID_REPLACE_ENA. In this case, what will happen is that

	 * the VLAN ID field from the Extraction Header gets updated, but the

	 * 802.1Q header does not (the classified VLAN only becomes visible on

	 * egress through the "port tag" of front-panel ports).

	 * So, for traffic extracted by the CPU, we want to pick up the

	 * classified VLAN and manually replace the existing 802.1Q header from

	 * the packet with it, so that the operating system is always up to

	 * date with the result of tc-vlan actions.

	 * NOTE: In VLAN-unaware mode, we don't want to do that, we want the

	 * frame to remain unmodified, because the classified VLAN is always

	 * equal to the pvid of the ingress port and should not be used for

	 * processing.

 SPDX-License-Identifier: (GPL-2.0 OR MIT)

/*

 * net/dsa/tag_hellcreek.c - Hirschmann Hellcreek switch tag format handling

 *

 * Copyright (C) 2019,2020 Linutronix GmbH

 * Author Kurt Kanzenbach <kurt@linutronix.de>

 *

 * Based on tag_ksz.c.

 Tag encoding */

 Tag decoding */

 SPDX-License-Identifier: GPL-2.0+

/*

 * net/dsa/tag_ksz.c - Microchip KSZ Switch tag format handling

 * Copyright (c) 2017 Microchip Technology

 Typically only one byte is used for tail tag. */

/*

 * For Ingress (Host -> KSZ8795), 1 byte is added before FCS.

 * ---------------------------------------------------------------------------

 * DA(6bytes)|SA(6bytes)|....|Data(nbytes)|tag(1byte)|FCS(4bytes)

 * ---------------------------------------------------------------------------

 * tag : each bit represents port (eg, 0x01=port1, 0x02=port2, 0x10=port5)

 *

 * For Egress (KSZ8795 -> Host), 1 byte is added before FCS.

 * ---------------------------------------------------------------------------

 * DA(6bytes)|SA(6bytes)|....|Data(nbytes)|tag0(1byte)|FCS(4bytes)

 * ---------------------------------------------------------------------------

 * tag0 : zero-based value represents port

 *	  (eg, 0x00=port1, 0x02=port3, 0x06=port7)

 Tag encoding */

/*

 * For Ingress (Host -> KSZ9477), 2 bytes are added before FCS.

 * ---------------------------------------------------------------------------

 * DA(6bytes)|SA(6bytes)|....|Data(nbytes)|tag0(1byte)|tag1(1byte)|FCS(4bytes)

 * ---------------------------------------------------------------------------

 * tag0 : Prioritization (not used now)

 * tag1 : each bit represents port (eg, 0x01=port1, 0x02=port2, 0x10=port5)

 *

 * For Egress (KSZ9477 -> Host), 1 byte is added before FCS.

 * ---------------------------------------------------------------------------

 * DA(6bytes)|SA(6bytes)|....|Data(nbytes)|tag0(1byte)|FCS(4bytes)

 * ---------------------------------------------------------------------------

 * tag0 : zero-based value represents port

 *	  (eg, 0x00=port1, 0x02=port3, 0x06=port7)

 Tag encoding */

 Tag decoding */

 Extra 4-bytes PTP timestamp */

 Tag encoding */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * net/dsa/dsa.c - Hardware switch handling

 * Copyright (c) 2008-2009 Marvell Semiconductor

 * Copyright (c) 2013 Florian Fainelli <florian@openwrt.org>

 Just return the original SKB */

/* Function takes a reference on the module owning the tagger,

 * so dsa_tag_driver_put must be called afterwards.

/* Determine if we should defer delivery of skb until we have a rx timestamp.

 *

 * Called from dsa_switch_rcv. For now, this will only work if tagging is

 * enabled on the switch. Normally the MAC driver would retrieve the hardware

 * timestamp when it reads the packet out of the hardware. However in a DSA

 * switch, the DSA driver owning the interface to which the packet is

 * delivered is never notified unless we do so here.

		/* Packet is to be injected directly on an upper

		 * device, e.g. a team/bond, so skip all DSA-port

		 * specific actions.

 Suspend slave network devices */

 Resume slave network devices */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * net/dsa/slave.c - Slave device handling

 * Copyright (c) 2008-2009 Marvell Semiconductor

 slave mii_bus handling ***************************************************/

 slave device handling ****************************************************/

 Pass through to switch driver if it supports timestamping */

 Must be called under rcu_read_lock() */

	/* Deny adding a bridge VLAN when there is already an 802.1Q upper with

	 * the same VID.

	/* We need the dedicated CPU port to be a member of the VLAN as well.

	 * Even though drivers often handle CPU membership in special ways,

	 * it doesn't make sense to program a PVID, so clear this flag.

	/* Do not deprogram the CPU port as it may be shared with other user

	 * ports which can be members of this VLAN as well.

	/* For non-legacy ports, devlink is used and it takes

	 * care of the name generation. This ndo implementation

	 * should be removed with legacy support.

	/* SKB for netpoll still need to be mangled with the protocol-specific

	 * tag to be successfully transmitted

	/* Queue the SKB for transmission on the parent interface, but

	 * do not modify its EtherType

	/* For tail taggers, we need to pad short frames ourselves, to ensure

	 * that the tail tag does not fail at its role of being at the end of

	 * the packet, once the master interface pads the frame. Account for

	 * that pad length here, and pad later.

 skb_headroom() returns unsigned int... */

 No reallocation needed, yay! */

 Handle tx timestamp if any */

	/* needed_tailroom should still be 'warm' in the cache line from

	 * dsa_realloc_skb(), which has also ensured that padding is safe.

	/* Transmit function may have to reallocate the original SKB,

	 * in which case it must have freed it. Only free it here on error.

 ethtool operations *******************************************************/

 Port's PHY and MAC both need to be EEE capable */

 Port's PHY and MAC both need to be EEE capable */

	/* For non-legacy ports, devlink is used and it takes

	 * care of the name generation. This ndo implementation

	 * should be removed with legacy support.

 This API only allows programming tagged, non-PVID VIDs */

 User port... */

 And CPU port... */

 This API only allows programming tagged, non-PVID VIDs */

	/* Do not deprogram the CPU port as it may be shared with other user

	 * ports which can be members of this VLAN as well.

/* Keep the VLAN RX filtering list in sync with the hardware only if VLAN

 * filtering is enabled. The baseline is that only ports that offload a

 * VLAN-aware bridge are VLAN-aware, and standalone ports are VLAN-unaware,

 * but there are exceptions for quirky hardware.

 *

 * If ds->vlan_filtering_is_global = true, then standalone ports which share

 * the same switch with other ports that offload a VLAN-aware bridge are also

 * inevitably VLAN-aware.

 *

 * To summarize, a DSA switch port offloads:

 *

 * - If standalone (this includes software bridge, software LAG):

 *     - if ds->needs_standalone_vlan_filtering = true, OR if

 *       (ds->vlan_filtering_is_global = true AND there are bridges spanning

 *       this switch chip which have vlan_filtering=1)

 *         - the 8021q upper VLANs

 *     - else (standalone VLAN filtering is not needed, VLAN filtering is not

 *       global, or it is, but no port is under a VLAN-aware bridge):

 *         - no VLAN (any 8021q upper is a software VLAN)

 *

 * - If under a vlan_filtering=0 bridge which it offload:

 *     - if ds->configure_vlan_while_not_filtering = true (default):

 *         - the bridge VLANs. These VLANs are committed to hardware but inactive.

 *     - else (deprecated):

 *         - no VLAN. The bridge VLANs are not restored when VLAN awareness is

 *           enabled, so this behavior is broken and discouraged.

 *

 * - If under a vlan_filtering=1 bridge which it offload:

 *     - the bridge VLANs

 *     - the 8021q upper VLANs

 Make the hardware datapath to/from @dev limited to a common MTU */

	/* Populate the list of ports that are part of the same bridge

	 * as the newly added/modified port

	/* Attempt to configure the entire hardware bridge to the newly added

	 * interface's MTU first, regardless of whether the intention of the

	 * user was to raise or lower it.

	/* Clearly that didn't work out so well, so just set the minimum MTU on

	 * all hardware bridge ports now. If this fails too, then all ports will

	 * still have their old MTU rolled back anyway.

		/* During probe, this function will be called for each slave

		 * device, while not all of them have been allocated. That's

		 * ok, it doesn't change what the maximum is, so ignore it.

		/* Pretend that we already applied the setting, which we

		 * actually haven't (still haven't done all integrity checks)

	/* If the master MTU isn't over limit, there's no need to check the CPU

	 * MTU, since that surely isn't either.

 Start applying stuff */

		/* We only need to propagate the MTU of the CPU port to

		 * upstream switches, so create a non-targeted notifier which

		 * updates all switches.

	/* No need to check that this operation is valid, the callback would

	 * not be called if it was not.

 slave device setup *******************************************************/

	/* The get_fixed_state callback takes precedence over polling the

	 * link GPIO in PHYLINK (see phylink_get_fixed_state).  Only set

	 * this if the switch provides such a callback.

		/* We could not connect to a designated PHY or SFP, so try to

		 * use the switch internal MDIO bus instead

	/* Try to save one extra realloc later in the TX path (in the master)

	 * by also inheriting the master's needed headroom and tailroom.

	 * The 8021q driver also does this.

	/* dsa_port_pre_hsr_leave is not yet necessary since hsr cannot be

	 * meaningfully enslaved to a bridge yet

 Software LAG */

/* Same as dsa_slave_lag_changeupper() except that it calls

 * dsa_slave_prechangeupper()

 Software LAG */

 Deny enslaving a VLAN device into a VLAN-aware bridge */

	/* br_vlan_get_info() returns -EINVAL or -ENOENT if the

	 * device, respectively the VID is not found, returning

	 * 0 means success, which is a failure for us here.

 Everything else is foreign */

	/* FDB entries learned by the software bridge should be installed as

	 * host addresses only if the driver requests assisted learning.

	/* Also treat FDB entries on foreign interfaces bridged with us as host

	 * addresses.

 Called under rcu_read_lock() */

 SPDX-License-Identifier: GPL-2.0

/*

 * Handler for Realtek 4 byte DSA switch tags

 * Currently only supports protocol "A" found in RTL8366RB

 * Copyright (c) 2020 Linus Walleij <linus.walleij@linaro.org>

 *

 * This "proprietary tag" header looks like so:

 *

 * -------------------------------------------------

 * | MAC DA | MAC SA | 0x8899 | 2 bytes tag | Type |

 * -------------------------------------------------

 *

 * The 2 bytes tag form a 16 bit big endian word. The exact

 * meaning has been guessed from packet dumps from ingress

 * frames.

/*

 * 0x1 = Realtek Remote Control protocol (RRCP)

 * 0x2/0x3 seems to be used for loopback testing

 * 0x9 = RTL8306 DSA protocol

 * 0xa = RTL8366RB DSA protocol

 Pad out to at least 60 bytes */

 Set Ethertype */

 The lower bits indicate the port number */

 Not custom, just pass through */

 The 4 upper bits are the protocol */

 Remove RTL4 tag and recalculate checksum */

 SPDX-License-Identifier: GPL-2.0+

/*

 * XRS700x tag format handling

 * Copyright (c) 2008-2009 Marvell Semiconductor

 * Copyright (c) 2020 NovaTech LLC

 Frame is forwarded by hardware, don't forward in software. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel / Lantiq GSWIP V2.0 PMAC tag support

 *

 * Copyright (C) 2017 - 2018 Hauke Mehrtens <hauke@hauke-m.de>

 special tag in TX path header */

 Byte 0 */

 source port ID */

 Byte 1 */

 destination group ID */

 Byte 2 */

 Byte 3 */

 special tag in RX path header */

 Byte 7 */

 Get source port information */

 remove GSWIP tag */

 SPDX-License-Identifier: GPL-2.0

/*

 * Handler for Realtek 8 byte switch tags

 *

 * Copyright (C) 2021 Alvin ipraga <alsi@bang-olufsen.dk>

 *

 * NOTE: Currently only supports protocol "4" found in the RTL8365MB, hence

 * named tag_rtl8_4.

 *

 * This tag header has the following format:

 *

 *  -------------------------------------------

 *  | MAC DA | MAC SA | 8 byte tag | Type | ...

 *  -------------------------------------------

 *     _______________/            \______________________________________

 *    /                                                                   \

 *  0                                  7|8                                 15

 *  |-----------------------------------+-----------------------------------|---

 *  |                               (16-bit)                                | ^

 *  |                       Realtek EtherType [0x8899]                      | |

 *  |-----------------------------------+-----------------------------------| 8

 *  |              (8-bit)              |              (8-bit)              |

 *  |          Protocol [0x04]          |              REASON               | b

 *  |-----------------------------------+-----------------------------------| y

 *  |   (1)  | (1) | (2) |   (1)  | (3) | (1)  | (1) |    (1)    |   (5)    | t

 *  | FID_EN |  X  | FID | PRI_EN | PRI | KEEP |  X  | LEARN_DIS |    X     | e

 *  |-----------------------------------+-----------------------------------| s

 *  |   (1)  |                       (15-bit)                               | |

 *  |  ALLOW |                        TX/RX                                 | v

 *  |-----------------------------------+-----------------------------------|---

 *

 * With the following field descriptions:

 *

 *    field      | description

 *   ------------+-------------

 *    Realtek    | 0x8899: indicates that this is a proprietary Realtek tag;

 *     EtherType |         note that Realtek uses the same EtherType for

 *               |         other incompatible tag formats (e.g. tag_rtl4_a.c)

 *    Protocol   | 0x04: indicates that this tag conforms to this format

 *    X          | reserved

 *   ------------+-------------

 *    REASON     | reason for forwarding packet to CPU

 *               | 0: packet was forwarded or flooded to CPU

 *               | 80: packet was trapped to CPU

 *    FID_EN     | 1: packet has an FID

 *               | 0: no FID

 *    FID        | FID of packet (if FID_EN=1)

 *    PRI_EN     | 1: force priority of packet

 *               | 0: don't force priority

 *    PRI        | priority of packet (if PRI_EN=1)

 *    KEEP       | preserve packet VLAN tag format

 *    LEARN_DIS  | don't learn the source MAC address of the packet

 *    ALLOW      | 1: treat TX/RX field as an allowance port mask, meaning the

 *               |    packet may only be forwarded to ports specified in the

 *               |    mask

 *               | 0: no allowance port mask, TX/RX field is the forwarding

 *               |    port mask

 *    TX/RX      | TX (switch->CPU): port number the packet was received on

 *               | RX (CPU->switch): forwarding port mask (if ALLOW=0)

 *               |                   allowance port mask (if ALLOW=1)

/* Protocols supported:

 *

 * 0x04 = RTL8365MB DSA protocol

 Set Realtek EtherType */

 Set Protocol; zero REASON */

 Zero FID_EN, FID, PRI_EN, PRI, KEEP; set LEARN_DIS */

 Zero ALLOW; set RX (CPU->switch) forwarding port mask */

 Parse Realtek EtherType */

 Parse Protocol */

 Parse REASON */

 Parse TX (switch->CPU) */

 Remove tag and recalculate checksum */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2019 Pengutronix, Oleksij Rempel <kernel@pengutronix.de>

/* AR9331_HDR_RESERVED - not used or may be version field.

 * According to the AR8216 doc it should 0b10. On AR9331 it is 0b11 on RX path

 * and should be set to 0b11 to make it work.

 0b10 for AR8216 and 0b11 for AR9331 */

 Get source port information */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * net/dsa/dsa2.c - Hardware switch handling, binding version 2

 * Copyright (c) 2008-2009 Marvell Semiconductor

 * Copyright (c) 2013 Florian Fainelli <florian@openwrt.org>

 * Copyright (c) 2016 Andrew Lunn <andrew@lunn.ch>

 Track the bridges with forwarding offload enabled */

/**

 * dsa_tree_notify - Execute code for all switches in a DSA switch tree.

 * @dst: collection of struct dsa_switch devices to notify.

 * @e: event, must be of type DSA_NOTIFIER_*

 * @v: event-specific value.

 *

 * Given a struct dsa_switch_tree, this can be used to run a function once for

 * each member DSA switch. The other alternative of traversing the tree is only

 * through its ports list, which does not uniquely list the switches.

/**

 * dsa_broadcast - Notify all DSA trees in the system.

 * @e: event, must be of type DSA_NOTIFIER_*

 * @v: event-specific value.

 *

 * Can be used to notify the switching fabric of events such as cross-chip

 * bridging between disjoint trees (such as islands of tagger-compatible

 * switches bridged by an incompatible middle switch).

 *

 * WARNING: this function is not reliable during probe time, because probing

 * between trees is asynchronous and not all DSA trees might have probed.

/**

 * dsa_lag_map() - Map LAG netdev to a linear LAG ID

 * @dst: Tree in which to record the mapping.

 * @lag: Netdev that is to be mapped to an ID.

 *

 * dsa_lag_id/dsa_lag_dev can then be used to translate between the

 * two spaces. The size of the mapping space is determined by the

 * driver by setting ds->num_lag_ids. It is perfectly legal to leave

 * it unset if it is not needed, in which case these functions become

 * no-ops.

 Already mapped */

	/* No IDs left, which is OK. Some drivers do not need it. The

	 * ones that do, e.g. mv88e6xxx, will discover that dsa_lag_id

	 * returns an error for this device when joining the LAG. The

	 * driver can then return -EOPNOTSUPP back to DSA, which will

	 * fall back to a software LAG.

/**

 * dsa_lag_unmap() - Remove a LAG ID mapping

 * @dst: Tree in which the mapping is recorded.

 * @lag: Netdev that was mapped.

 *

 * As there may be multiple users of the mapping, it is only removed

 * if there are no other references to it.

 There are remaining users of this mapping */

	/* When preparing the offload for a port, it will have a valid

	 * dp->bridge_dev pointer but a not yet valid dp->bridge_num.

	 * However there might be other ports having the same dp->bridge_dev

	 * and a valid dp->bridge_num, so just ignore this port.

 First port that offloads TX forwarding for this bridge */

	/* Check if the bridge is still in use, otherwise it is time

	 * to clean it up so we can reuse this bridge_num later.

/* Assign the default CPU port (the first one in the tree) to all ports of the

 * fabric which don't already have one as part of their own switch.

/* Perform initial assignment of CPU ports to user ports and DSA links in the

 * fabric, giving preference to CPU ports local to each switch. Default to

 * using the first CPU port in the switch tree if the port does not have a CPU

 * port local to this switch.

 Prefer a local CPU port */

 Prefer the first local CPU port found */

/* Destroy the current devlink port, and create a new one which has the UNUSED

 * flavour. At this point, any call to ds->ops->port_setup has been already

 * balanced out by a call to ds->ops->port_teardown, so we know that any

 * devlink port regions the driver had are now unregistered. We then call its

 * ds->ops->port_setup again, in order for the driver to re-create them on the

 * new devlink port.

		/* On error, leave the devlink port registered,

		 * dsa_switch_teardown will clean it up later.

	/* Initialize ds->phys_mii_mask before registering the slave MDIO bus

	 * driver and before ops->setup() has run, since the switch drivers and

	 * the slave MDIO bus driver rely on these values for probing PHY

	 * devices or not

	/* Add the switch to devlink before calling setup, so that setup can

	 * add dpipe tables

	/* Setup devlink port instances now, so that the switch

	 * setup() can register regions etc, against the ports

/* First tear down the non-shared, then the shared ports. This ensures that

 * all work items scheduled by our switchdev handlers for user ports have

 * completed before we destroy the refcounting kept on the shared ports.

/* Since the dsa/tagging sysfs device attribute is per master, the assumption

 * is that all DSA switches within a tree share the same tagger, otherwise

 * they would have formed disjoint trees (different "dsa,member" values).

	/* At the moment we don't allow changing the tag protocol under

	 * traffic. The rtnl_mutex also happens to serialize concurrent

	 * attempts to change the tagging protocol. If we ever lift the IFF_UP

	 * restriction, there needs to be another mutex which serializes this.

	/* It is possible to stack DSA switches onto one another when that

	 * happens the switch driver may want to know if its tagging protocol

	 * is going to work in such a configuration.

	/* If the master device is not itself a DSA slave in a disjoint DSA

	 * tree, then return immediately.

 Find out which protocol the switch would prefer. */

 See if the user wants to override that preference. */

		/* In the case of multiple CPU ports per switch, the tagging

		 * protocol is still reference-counted only per switch tree.

	/* At this point, the tree may be configured to use a different

	 * tagger than the one chosen by the switch driver during

	 * .setup, in the case when a user selects a custom protocol

	 * through the DT.

	 *

	 * This is resolved by syncing the driver with the tree in

	 * dsa_switch_setup_tag_protocol once .setup has run and the

	 * driver is ready to accept calls to .change_tag_protocol. If

	 * the driver does not support the custom protocol at that

	 * point, the tree is wholly rejected, thereby ensuring that the

	 * tree and driver are always in agreement on the protocol to

	 * use.

 The second possibility is "ethernet-ports" */

 Don't error out if this optional property isn't found */

	/* We don't support interconnected switches nor multiple trees via

	 * platform data, so this is the unique switch of the tree.

/* If the DSA master chooses to unregister its net_device on .shutdown, DSA is

 * blocking that operation from completion, due to the dev_hold taken inside

 * netdev_upper_dev_link. Unlink the DSA slave interfaces from being uppers of

 * the DSA master, so that the system can reboot successfully.

		/* Just unlinking ourselves as uppers of the master is not

		 * sufficient. When the master net device unregisters, that will

		 * also call dev_close, which we will catch as NETDEV_GOING_DOWN

		 * and trigger a dev_close on our own devices (dsa_slave_close).

		 * In turn, that will call dev_mc_unsync on the master's net

		 * device. If the master is also a DSA switch port, this will

		 * trigger dsa_slave_set_rx_mode which will call dev_mc_sync on

		 * its own master. Lockdep will complain about the fact that

		 * all cascaded masters have the same dsa_master_addr_list_lock_key,

		 * which it normally would not do if the cascaded masters would

		 * be in a proper upper/lower relationship, which we've just

		 * destroyed.

		 * To suppress the lockdep warnings, let's actually unregister

		 * the DSA slave interfaces too, to avoid the nonsensical

		 * multicast address list synchronization on shutdown.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Regular and Ethertype DSA tagging

 * Copyright (c) 2008-2009 Marvell Semiconductor

 *

 * Regular DSA

 * -----------



 * For untagged (in 802.1Q terms) packets, the switch will splice in

 * the tag between the SA and the ethertype of the original

 * packet. Tagged frames will instead have their outermost .1Q tag

 * converted to a DSA tag. It expects the same layout when receiving

 * packets from the CPU.

 *

 * Example:

 *

 *     .----.----.----.---------

 * Pu: | DA | SA | ET | Payload ...

 *     '----'----'----'---------

 *       6    6    2       N

 *     .----.----.--------.-----.----.---------

 * Pt: | DA | SA | 0x8100 | TCI | ET | Payload ...

 *     '----'----'--------'-----'----'---------

 *       6    6       2      2    2       N

 *     .----.----.-----.----.---------

 * Pd: | DA | SA | DSA | ET | Payload ...

 *     '----'----'-----'----'---------

 *       6    6     4    2       N

 *

 * No matter if a packet is received untagged (Pu) or tagged (Pt),

 * they will both have the same layout (Pd) when they are sent to the

 * CPU. This is done by ignoring 802.3, replacing the ethertype field

 * with more metadata, among which is a bit to signal if the original

 * packet was tagged or not.

 *

 * Ethertype DSA

 * -------------

 * Uses the exact same tag format as regular DSA, but also includes a

 * proper ethertype field (which the mv88e6xxx driver sets to

 * ETH_P_EDSA/0xdada) followed by two zero bytes:

 *

 * .----.----.--------.--------.-----.----.---------

 * | DA | SA | 0xdada | 0x0000 | DSA | ET | Payload ...

 * '----'----'--------'--------'-----'----'---------

 *   6    6       2        2      4    2       N

/**

 * enum dsa_cmd - DSA Command

 * @DSA_CMD_TO_CPU: Set on packets that were trapped or mirrored to

 *     the CPU port. This is needed to implement control protocols,

 *     e.g. STP and LLDP, that must not allow those control packets to

 *     be switched according to the normal rules.

 * @DSA_CMD_FROM_CPU: Used by the CPU to send a packet to a specific

 *     port, ignoring all the barriers that the switch normally

 *     enforces (VLANs, STP port states etc.). No source address

 *     learning takes place. "sudo send packet"

 * @DSA_CMD_TO_SNIFFER: Set on the copies of packets that matched some

 *     user configured ingress or egress monitor criteria. These are

 *     forwarded by the switch tree to the user configured ingress or

 *     egress monitor port, which can be set to the CPU port or a

 *     regular port. If the destination is a regular port, the tag

 *     will be removed before egressing the port. If the destination

 *     is the CPU port, the tag will not be removed.

 * @DSA_CMD_FORWARD: This tag is used on all bulk traffic passing

 *     through the switch tree, including the flows that are directed

 *     towards the CPU. Its device/port tuple encodes the original

 *     source port on which the packet ingressed. It can also be used

 *     on transmit by the CPU to defer the forwarding decision to the

 *     hardware, based on the current config of PVT/VTU/ATU

 *     etc. Source address learning takes places if enabled on the

 *     receiving DSA/CPU port.

/**

 * enum dsa_code - TO_CPU Code

 *

 * @DSA_CODE_MGMT_TRAP: DA was classified as a management

 *     address. Typical examples include STP BPDUs and LLDP.

 * @DSA_CODE_FRAME2REG: Response to a "remote management" request.

 * @DSA_CODE_IGMP_MLD_TRAP: IGMP/MLD signaling.

 * @DSA_CODE_POLICY_TRAP: Frame matched some policy configuration on

 *     the device. Typical examples are matching on DA/SA/VID and DHCP

 *     snooping.

 * @DSA_CODE_ARP_MIRROR: The name says it all really.

 * @DSA_CODE_POLICY_MIRROR: Same as @DSA_CODE_POLICY_TRAP, but the

 *     particular policy was set to trigger a mirror instead of a

 *     trap.

 * @DSA_CODE_RESERVED_6: Unused on all devices up to at least 6393X.

 * @DSA_CODE_RESERVED_7: Unused on all devices up to at least 6393X.

 *

 * A 3-bit code is used to relay why a particular frame was sent to

 * the CPU. We only use this to determine if the packet was mirrored

 * or trapped, i.e. whether the packet has been forwarded by hardware

 * or not.

 *

 * This is the superset of all possible codes. Any particular device

 * may only implement a subset.

		/* When offloading forwarding for a bridge, inject FORWARD

		 * packets on behalf of a virtual switch device with an index

		 * past the physical switches.

 Construct tagged DSA tag from 802.1Q tag. */

 Move CFI field from byte 2 to byte 1. */

 Construct DSA header from untagged frame. */

 The ethertype field is part of the DSA header. */

			/* Remote management is not implemented yet,

			 * drop.

			/* Mark mirrored packets to notify any upper

			 * device (like a bridge) that forwarding has

			 * already been done by hardware.

			/* Traps have, by definition, not been

			 * forwarded by hardware, so don't mark them.

			/* Reserved code, this could be anything. Drop

			 * seems like the safest option.

		/* The exact source port is not available in the tag,

		 * so we inject the frame directly on the upper

		 * team/bond.

	/* When using LAG offload, skb->dev is not a DSA slave interface,

	 * so we cannot call dsa_default_offload_fwd_mark and we need to

	 * special-case it.

	/* If the 'tagged' bit is set; convert the DSA tag to a 802.1Q

	 * tag, and delete the ethertype (extra) if applicable. If the

	 * 'tagged' bit is cleared; delete the DSA tag, and ethertype

	 * if applicable.

		/* Insert 802.1Q ethertype and copy the VLAN-related

		 * fields, but clear the bit that will hold CFI (since

		 * DSA uses that bit location for another purpose).

		/* Move CFI bit from its place in the DSA header to

		 * its 802.1Q-designated place.

 Update packet checksum if skb is CHECKSUM_COMPLETE. */

 CONFIG_NET_DSA_TAG_DSA */

 CONFIG_NET_DSA_TAG_EDSA */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Broadcom tag support

 *

 * Copyright (C) 2014 Broadcom Corporation

 Legacy Broadcom tag (6 bytes) */

 Type fields */

 1st byte in the tag */

 2nd byte in the tag */

 Tag fields */

 3rd byte in the tag */

 6th byte in the tag */

 Newer Broadcom tag (4 bytes) */

/* Tag is constructed and desconstructed using byte by byte access

 * because the tag is placed after the MAC Source Address, which does

 * not make it 4-bytes aligned, so this might cause unaligned accesses

 * on most systems where this is used.

 Ingress and egress opcodes */

 Ingress fields */

 1st byte in the tag */

 2nd byte in the tag */

 3rd byte in the tag */

 Egress fields */

 2nd byte in the tag */

 3rd byte in the tag */

	/* The Ethernet switch we are interfaced with needs packets to be at

	 * least 64 bytes (including FCS) otherwise they will be discarded when

	 * they enter the switch port logic. When Broadcom tags are enabled, we

	 * need to make sure that packets are at least 68 bytes

	 * (including FCS and tag) because the length verification is done after

	 * the Broadcom tag is stripped off the ingress packet.

	 *

	 * Let dsa_slave_xmit() free the SKB

	/* Set the ingress opcode, traffic class, tag enforcment is

	 * deprecated

	/* Now tell the master network device about the desired output queue

	 * as well

/* Frames with this tag have one of these two layouts:

 * -----------------------------------

 * | MAC DA | MAC SA | 4b tag | Type | DSA_TAG_PROTO_BRCM

 * -----------------------------------

 * -----------------------------------

 * | 4b tag | MAC DA | MAC SA | Type | DSA_TAG_PROTO_BRCM_PREPEND

 * -----------------------------------

 * In both cases, at receive time, skb->data points 2 bytes before the actual

 * Ethernet type field and we have an offset of 4bytes between where skb->data

 * and where the payload starts. So the same low-level receive function can be

 * used.

 The opcode should never be different than 0b000 */

	/* We should never see a reserved reason code without knowing how to

	 * handle it

 Locate which port this is coming from */

 Remove Broadcom tag and update checksum */

 Build the tag after the MAC Source Address */

 skb->data points to the EtherType, the tag is right before it */

	/* The Ethernet switch we are interfaced with needs packets to be at

	 * least 64 bytes (including FCS) otherwise they will be discarded when

	 * they enter the switch port logic. When Broadcom tags are enabled, we

	 * need to make sure that packets are at least 70 bytes

	 * (including FCS and tag) because the length verification is done after

	 * the Broadcom tag is stripped off the ingress packet.

	 *

	 * Let dsa_slave_xmit() free the SKB

 Broadcom tag type */

 Broadcom tag value */

 Remove Broadcom tag and update checksum */

 CONFIG_NET_DSA_TAG_BRCM_LEGACY */

 tag is prepended to the packet */

 tag is prepended to the packet */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Handling of a master device, switching frames via its switch fabric CPU port

 *

 * Copyright (c) 2017 Savoir-faire Linux Inc.

 *	Vivien Didelot <vivien.didelot@savoirfairelinux.com>

 We do not want to be NULL-terminated, since this is a prefix */

		/* This function copies ETH_GSTRINGS_LEN bytes, we will mangle

		 * the output after to prepend our CPU port prefix we

		 * constructed earlier

		/* Deny PTP operations on master if there is at least one

		 * switch in the tree that is PTP capable.

 Bad tagger name, or module is not loaded? */

		/* Drop the temporarily held duplicate reference, since

		 * the DSA switch tree uses this tagger.

		/* On failure the old tagger is restored, so we don't need the

		 * driver for the new one.

	/* On success we no longer need the module for the old tagging protocol

 The DSA master must use SET_NETDEV_DEV for this to work. */

	/* If we use a tagging format that doesn't have an ethertype

	 * field, make sure that all packets from this point on get

	 * sent to the tag format's receive function.

	/* If we used a tagging format that doesn't have an ethertype

	 * field, make sure that all packets from this point get sent

	 * without the tag and go through the regular receive path.

 SPDX-License-Identifier: GPL-2.0

/*

 * Mediatek DSA Tag support

 * Copyright (C) 2017 Landen Chao <landen.chao@mediatek.com>

 *		      Sean Wang <sean.wang@mediatek.com>

	/* Build the special tag after the MAC Source Address. If VLAN header

	 * is present, it's required that VLAN header and special tag is

	 * being combined. Only in this way we can allow the switch can parse

	 * the both special and VLAN tag at the same time and then look up VLAN

	 * table with VID.

	/* Mark tag attribute on special tag insertion to notify hardware

	 * whether that's a combined special tag with 802.1Q header.

 Tag control information is kept for 802.1Q */

 Remove MTK tag and recalculate checksum. */

 Get source port information */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2015, The Linux Foundation. All rights reserved.

 Set the version field, and set destination port information */

 Make sure the version is correct */

 Remove QCA tag and recalculate checksum */

 Get source port information */

 SPDX-License-Identifier: GPL-2.0

/* Copyright 2020-2021 NXP

 *

 * An implementation of the software-defined tag_8021q.c tagger format, which

 * also preserves full functionality under a vlan_filtering bridge. It does

 * this by using the TCAM engines for:

 * - pushing the RX VLAN as a second, outer tag, on egress towards the CPU port

 * - redirecting towards the correct front port based on TX VLAN and popping

 *   that on egress

 Calls felix_port_deferred_xmit in felix.c */

	/* Increase refcount so the kfree_skb in dsa_slave_xmit

	 * won't really free the packet.

 SPDX-License-Identifier: GPL-2.0

/* Copyright (c) 2019, Vladimir Oltean <olteanv@gmail.com>

 *

 * This module is not a complete tagger implementation. It only provides

 * primitives for taggers that rely on 802.1Q VLAN tags to use. The

 * dsa_8021q_netdev_ops is registered for API compliance and not used

 * directly by callers.

/* Binary structure of the fake 12-bit VID field (when the TPID is

 * ETH_P_DSA_8021Q):

 *

 * | 11  | 10  |  9  |  8  |  7  |  6  |  5  |  4  |  3  |  2  |  1  |  0  |

 * +-----------+-----+-----------------+-----------+-----------------------+

 * |    DIR    | VBID|    SWITCH_ID    |   VBID    |          PORT         |

 * +-----------+-----+-----------------+-----------+-----------------------+

 *

 * DIR - VID[11:10]:

 *	Direction flags.

 *	* 1 (0b01) for RX VLAN,

 *	* 2 (0b10) for TX VLAN.

 *	These values make the special VIDs of 0, 1 and 4095 to be left

 *	unused by this coding scheme.

 *

 * SWITCH_ID - VID[8:6]:

 *	Index of switch within DSA tree. Must be between 0 and 7.

 *

 * VBID - { VID[9], VID[5:4] }:

 *	Virtual bridge ID. If between 1 and 7, packet targets the broadcast

 *	domain of a bridge. If transmitted as zero, packet targets a single

 *	port. Field only valid on transmit, must be ignored on receive.

 *

 * PORT - VID[3:0]:

 *	Index of switch port. Must be between 0 and 15.

 The VBID value of 0 is reserved for precise TX */

/* Returns the VID to be inserted into the frame from xmit for switch steering

 * instructions on egress. Encodes switch ID and port ID.

/* Returns the VID that will be installed as pvid for this switch port, sent as

 * tagged egress towards the CPU port and decoded by the rcv function.

 Returns the decoded switch ID from the RX VID. */

 Returns the decoded port ID from the RX VID. */

 No need to bother with refcounting for user ports */

 No need to bother with refcounting for user ports */

	/* Since we use dsa_broadcast(), there might be other switches in other

	 * trees which don't support tag_8021q, so don't return an error.

	 * Or they might even support tag_8021q but have not registered yet to

	 * use it (maybe they use another tagger currently).

/* RX VLAN tagging (left) and TX VLAN tagging (right) setup shown for a single

 * front-panel switch port (here swp0).

 *

 * Port identification through VLAN (802.1Q) tags has different requirements

 * for it to work effectively:

 *  - On RX (ingress from network): each front-panel port must have a pvid

 *    that uniquely identifies it, and the egress of this pvid must be tagged

 *    towards the CPU port, so that software can recover the source port based

 *    on the VID in the frame. But this would only work for standalone ports;

 *    if bridged, this VLAN setup would break autonomous forwarding and would

 *    force all switched traffic to pass through the CPU. So we must also make

 *    the other front-panel ports members of this VID we're adding, albeit

 *    we're not making it their PVID (they'll still have their own).

 *  - On TX (ingress from CPU and towards network) we are faced with a problem.

 *    If we were to tag traffic (from within DSA) with the port's pvid, all

 *    would be well, assuming the switch ports were standalone. Frames would

 *    have no choice but to be directed towards the correct front-panel port.

 *    But because we also want the RX VLAN to not break bridging, then

 *    inevitably that means that we have to give them a choice (of what

 *    front-panel port to go out on), and therefore we cannot steer traffic

 *    based on the RX VID. So what we do is simply install one more VID on the

 *    front-panel and CPU ports, and profit off of the fact that steering will

 *    work just by virtue of the fact that there is only one other port that's

 *    a member of the VID we're tagging the traffic with - the desired one.

 *

 * So at the end, each front-panel port will have one RX VID (also the PVID),

 * the RX VID of all other front-panel ports that are in the same bridge, and

 * one TX VID. Whereas the CPU port will have the RX and TX VIDs of all

 * front-panel ports, and on top of that, is also tagged-input and

 * tagged-output (VLAN trunk).

 *

 *               CPU port                               CPU port

 * +-------------+-----+-------------+    +-------------+-----+-------------+

 * |  RX VID     |     |             |    |  TX VID     |     |             |

 * |  of swp0    |     |             |    |  of swp0    |     |             |

 * |             +-----+             |    |             +-----+             |

 * |                ^ T              |    |                | Tagged         |

 * |                |                |    |                | ingress        |

 * |    +-------+---+---+-------+    |    |    +-----------+                |

 * |    |       |       |       |    |    |    | Untagged                   |

 * |    |     U v     U v     U v    |    |    v egress                     |

 * | +-----+ +-----+ +-----+ +-----+ |    | +-----+ +-----+ +-----+ +-----+ |

 * | |     | |     | |     | |     | |    | |     | |     | |     | |     | |

 * | |PVID | |     | |     | |     | |    | |     | |     | |     | |     | |

 * +-+-----+-+-----+-+-----+-+-----+-+    +-+-----+-+-----+-+-----+-+-----+-+

 *   swp0    swp1    swp2    swp3           swp0    swp1    swp2    swp3

 Don't match on self */

 Install the RX VID of the targeted port in our VLAN table */

 Install our RX VID into the targeted port's VLAN table */

 Remove the RX VID of the targeted port from our VLAN table */

 Remove our RX VID from the targeted port's VLAN table */

 Set up a port's tag_8021q RX and TX VLAN for standalone mode operation */

	/* The CPU port is implicitly configured by

	 * configuring the front-panel ports

	/* Add this user port's RX VID to the membership list of all others

	 * (including itself). This is so that bridging will not be hindered.

	 * L2 forwarding rules still take precedence when there are no VLAN

	 * restrictions, so there are no concerns about leaking traffic.

 Add @rx_vid to the master's RX filter. */

 Finally apply the TX VID on this port and on the CPU port */

	/* The CPU port is implicitly configured by

	 * configuring the front-panel ports

	/* skb->data points at skb_mac_header, which

	 * is fine for vlan_insert_tag.

 SPDX-License-Identifier: GPL-2.0+

/*

 * net/dsa/tag_trailer.c - Trailer tag format handling

 * Copyright (c) 2008-2009 Marvell Semiconductor

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2017 Pengutronix, Juergen Borleis <jbe@pengutronix.de>

/* To define the outgoing port and to discover the incoming port a regular

 * VLAN tag is used by the LAN9303. But its VID meaning is 'special':

 *

 *       Dest MAC       Src MAC        TAG    Type

 * ...| 1 2 3 4 5 6 | 1 2 3 4 5 6 | 1 2 3 4 | 1 2 |...

 *                                |<------->|

 * TAG:

 *    |<------------->|

 *    |  1  2 | 3  4  |

 *      TPID    VID

 *     0x8100

 *

 * VID bit 3 indicates a request for an ALR lookup.

 *

 * If VID bit 3 is zero, then bits 0 and 1 specify the destination port

 * (0, 1, 2) or broadcast (3) or the source port (1, 2).

 *

 * VID bit 4 is used to specify if the STP port state should be overridden.

 * Required when no forwarding between the external ports should happen.

/* Decide whether to transmit using ALR lookup, or transmit directly to

 * port using tag. ALR learning is performed only when using ALR lookup.

 * If the two external ports are bridged and the frame is unicast,

 * then use ALR lookup to allow ALR learning on CPU port.

 * Otherwise transmit directly to port with STP state override.

 * See also: lan9303_separate_ports() and lan9303.pdf 6.4.10.1

 provide 'LAN9303_TAG_LEN' bytes additional space */

 make room between MACs and Ether-Type */

	/* remove the special VLAN tag between the MAC addresses

	 * and the current ethertype field.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * NetLabel NETLINK Interface

 *

 * This file defines the NETLINK interface for the NetLabel system.  The

 * NetLabel system manages static and dynamic label mappings for network

 * protocols such as CIPSO and RIPSO.

 *

 * Author: Paul Moore <paul@paul-moore.com>

/*

 * (c) Copyright Hewlett-Packard Development Company, L.P., 2006

/*

 * NetLabel NETLINK Setup Functions

/**

 * netlbl_netlink_init - Initialize the NETLINK communication channel

 *

 * Description:

 * Call out to the NetLabel components so they can register their families and

 * commands with the Generic NETLINK mechanism.  Returns zero on success and

 * non-zero on failure.

 *

/*

 * NetLabel Audit Functions

/**

 * netlbl_audit_start_common - Start an audit message

 * @type: audit message type

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Start an audit message using the type specified in @type and fill the audit

 * message with some fields common to all NetLabel audit messages.  Returns

 * a pointer to the audit buffer on success, NULL on failure.

 *

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * NetLabel Network Address Lists

 *

 * This file contains network address list functions used to manage ordered

 * lists of network addresses for use by the NetLabel subsystem.  The NetLabel

 * system manages static and dynamic label mappings for network protocols such

 * as CIPSO and RIPSO.

 *

 * Author: Paul Moore <paul@paul-moore.com>

/*

 * (c) Copyright Hewlett-Packard Development Company, L.P., 2008

/*

 * Address List Functions

/**

 * netlbl_af4list_search - Search for a matching IPv4 address entry

 * @addr: IPv4 address

 * @head: the list head

 *

 * Description:

 * Searches the IPv4 address list given by @head.  If a matching address entry

 * is found it is returned, otherwise NULL is returned.  The caller is

 * responsible for calling the rcu_read_[un]lock() functions.

 *

/**

 * netlbl_af4list_search_exact - Search for an exact IPv4 address entry

 * @addr: IPv4 address

 * @mask: IPv4 address mask

 * @head: the list head

 *

 * Description:

 * Searches the IPv4 address list given by @head.  If an exact match if found

 * it is returned, otherwise NULL is returned.  The caller is responsible for

 * calling the rcu_read_[un]lock() functions.

 *

/**

 * netlbl_af6list_search - Search for a matching IPv6 address entry

 * @addr: IPv6 address

 * @head: the list head

 *

 * Description:

 * Searches the IPv6 address list given by @head.  If a matching address entry

 * is found it is returned, otherwise NULL is returned.  The caller is

 * responsible for calling the rcu_read_[un]lock() functions.

 *

/**

 * netlbl_af6list_search_exact - Search for an exact IPv6 address entry

 * @addr: IPv6 address

 * @mask: IPv6 address mask

 * @head: the list head

 *

 * Description:

 * Searches the IPv6 address list given by @head.  If an exact match if found

 * it is returned, otherwise NULL is returned.  The caller is responsible for

 * calling the rcu_read_[un]lock() functions.

 *

 IPv6 */

/**

 * netlbl_af4list_add - Add a new IPv4 address entry to a list

 * @entry: address entry

 * @head: the list head

 *

 * Description:

 * Add a new address entry to the list pointed to by @head.  On success zero is

 * returned, otherwise a negative value is returned.  The caller is responsible

 * for calling the necessary locking functions.

 *

	/* in order to speed up address searches through the list (the common

	 * case) we need to keep the list in order based on the size of the

	 * address mask such that the entry with the widest mask (smallest

/**

 * netlbl_af6list_add - Add a new IPv6 address entry to a list

 * @entry: address entry

 * @head: the list head

 *

 * Description:

 * Add a new address entry to the list pointed to by @head.  On success zero is

 * returned, otherwise a negative value is returned.  The caller is responsible

 * for calling the necessary locking functions.

 *

	/* in order to speed up address searches through the list (the common

	 * case) we need to keep the list in order based on the size of the

	 * address mask such that the entry with the widest mask (smallest

 IPv6 */

/**

 * netlbl_af4list_remove_entry - Remove an IPv4 address entry

 * @entry: address entry

 *

 * Description:

 * Remove the specified IP address entry.  The caller is responsible for

 * calling the necessary locking functions.

 *

/**

 * netlbl_af4list_remove - Remove an IPv4 address entry

 * @addr: IP address

 * @mask: IP address mask

 * @head: the list head

 *

 * Description:

 * Remove an IP address entry from the list pointed to by @head.  Returns the

 * entry on success, NULL on failure.  The caller is responsible for calling

 * the necessary locking functions.

 *

/**

 * netlbl_af6list_remove_entry - Remove an IPv6 address entry

 * @entry: address entry

 *

 * Description:

 * Remove the specified IP address entry.  The caller is responsible for

 * calling the necessary locking functions.

 *

/**

 * netlbl_af6list_remove - Remove an IPv6 address entry

 * @addr: IP address

 * @mask: IP address mask

 * @head: the list head

 *

 * Description:

 * Remove an IP address entry from the list pointed to by @head.  Returns the

 * entry on success, NULL on failure.  The caller is responsible for calling

 * the necessary locking functions.

 *

 IPv6 */

/*

 * Audit Helper Functions

/**

 * netlbl_af4list_audit_addr - Audit an IPv4 address

 * @audit_buf: audit buffer

 * @src: true if source address, false if destination

 * @dev: network interface

 * @addr: IP address

 * @mask: IP address mask

 *

 * Description:

 * Write the IPv4 address and address mask, if necessary, to @audit_buf.

 *

/**

 * netlbl_af6list_audit_addr - Audit an IPv6 address

 * @audit_buf: audit buffer

 * @src: true if source address, false if destination

 * @dev: network interface

 * @addr: IP address

 * @mask: IP address mask

 *

 * Description:

 * Write the IPv6 address and address mask, if necessary, to @audit_buf.

 *

 IPv6 */

 CONFIG_AUDIT */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * NetLabel Unlabeled Support

 *

 * This file defines functions for dealing with unlabeled packets for the

 * NetLabel system.  The NetLabel system manages static and dynamic label

 * mappings for network protocols such as CIPSO and RIPSO.

 *

 * Author: Paul Moore <paul@paul-moore.com>

/*

 * (c) Copyright Hewlett-Packard Development Company, L.P., 2006 - 2008

/* NOTE: at present we always use init's network namespace since we don't

 *       presently support different namespaces even though the majority of

/* The unlabeled connection hash table which we use to map network interfaces

 * and addresses of unlabeled packets to a user specified secid value for the

 * LSM.  The hash table is used to lookup the network interface entry

 * (struct netlbl_unlhsh_iface) and then the interface entry is used to

 * lookup an IP address match from an ordered list.  If a network interface

 * match can not be found in the hash table then the default entry

 * (netlbl_unlhsh_def) is used.  The IP address entry list

 * (struct netlbl_unlhsh_addr) is ordered such that the entries with a

 * larger netmask come first.

 Argument struct for netlbl_unlhsh_walk() */

 Unlabeled connection hash table */

/* updates should be so rare that having one spinlock for the entire

 Accept unlabeled packets flag */

 NetLabel Generic NETLINK unlabeled family */

 NetLabel Netlink attribute policy */

/*

 * Unlabeled Connection Hash Table Functions

/**

 * netlbl_unlhsh_free_iface - Frees an interface entry from the hash table

 * @entry: the entry's RCU field

 *

 * Description:

 * This function is designed to be used as a callback to the call_rcu()

 * function so that memory allocated to a hash table interface entry can be

 * released safely.  It is important to note that this function does not free

 * the IPv4 and IPv6 address lists contained as part of an interface entry.  It

 * is up to the rest of the code to make sure an interface entry is only freed

 * once it's address lists are empty.

 *

 IPv6 */

	/* no need for locks here since we are the only one with access to this

 IPv6 */

/**

 * netlbl_unlhsh_hash - Hashing function for the hash table

 * @ifindex: the network interface/device to hash

 *

 * Description:

 * This is the hashing function for the unlabeled hash table, it returns the

 * bucket number for the given device/interface.  The caller is responsible for

 * ensuring that the hash table is protected with either a RCU read lock or

 * the hash table lock.

 *

/**

 * netlbl_unlhsh_search_iface - Search for a matching interface entry

 * @ifindex: the network interface

 *

 * Description:

 * Searches the unlabeled connection hash table and returns a pointer to the

 * interface entry which matches @ifindex, otherwise NULL is returned.  The

 * caller is responsible for ensuring that the hash table is protected with

 * either a RCU read lock or the hash table lock.

 *

/**

 * netlbl_unlhsh_add_addr4 - Add a new IPv4 address entry to the hash table

 * @iface: the associated interface entry

 * @addr: IPv4 address in network byte order

 * @mask: IPv4 address mask in network byte order

 * @secid: LSM secid value for entry

 *

 * Description:

 * Add a new address entry into the unlabeled connection hash table using the

 * interface entry specified by @iface.  On success zero is returned, otherwise

 * a negative value is returned.

 *

/**

 * netlbl_unlhsh_add_addr6 - Add a new IPv6 address entry to the hash table

 * @iface: the associated interface entry

 * @addr: IPv6 address in network byte order

 * @mask: IPv6 address mask in network byte order

 * @secid: LSM secid value for entry

 *

 * Description:

 * Add a new address entry into the unlabeled connection hash table using the

 * interface entry specified by @iface.  On success zero is returned, otherwise

 * a negative value is returned.

 *

 IPv6 */

/**

 * netlbl_unlhsh_add_iface - Adds a new interface entry to the hash table

 * @ifindex: network interface

 *

 * Description:

 * Add a new, empty, interface entry into the unlabeled connection hash table.

 * On success a pointer to the new interface entry is returned, on failure NULL

 * is returned.

 *

/**

 * netlbl_unlhsh_add - Adds a new entry to the unlabeled connection hash table

 * @net: network namespace

 * @dev_name: interface name

 * @addr: IP address in network byte order

 * @mask: address mask in network byte order

 * @addr_len: length of address/mask (4 for IPv4, 16 for IPv6)

 * @secid: LSM secid value for the entry

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Adds a new entry to the unlabeled connection hash table.  Returns zero on

 * success, negative values on failure.

 *

 IPv6 */

/**

 * netlbl_unlhsh_remove_addr4 - Remove an IPv4 address entry

 * @net: network namespace

 * @iface: interface entry

 * @addr: IP address

 * @mask: IP address mask

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Remove an IP address entry from the unlabeled connection hash table.

 * Returns zero on success, negative values on failure.

 *

/**

 * netlbl_unlhsh_remove_addr6 - Remove an IPv6 address entry

 * @net: network namespace

 * @iface: interface entry

 * @addr: IP address

 * @mask: IP address mask

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Remove an IP address entry from the unlabeled connection hash table.

 * Returns zero on success, negative values on failure.

 *

 IPv6 */

/**

 * netlbl_unlhsh_condremove_iface - Remove an interface entry

 * @iface: the interface entry

 *

 * Description:

 * Remove an interface entry from the unlabeled connection hash table if it is

 * empty.  An interface entry is considered to be empty if there are no

 * address entries assigned to it.

 *

 IPv6 */

 IPv6 */

/**

 * netlbl_unlhsh_remove - Remove an entry from the unlabeled hash table

 * @net: network namespace

 * @dev_name: interface name

 * @addr: IP address in network byte order

 * @mask: address mask in network byte order

 * @addr_len: length of address/mask (4 for IPv4, 16 for IPv6)

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Removes and existing entry from the unlabeled connection hash table.

 * Returns zero on success, negative values on failure.

 *

 IPv6 */

/*

 * General Helper Functions

/**

 * netlbl_unlhsh_netdev_handler - Network device notification handler

 * @this: notifier block

 * @event: the event

 * @ptr: the netdevice notifier info (cast to void)

 *

 * Description:

 * Handle network device events, although at present all we care about is a

 * network device going away.  In the case of a device going away we clear any

 * related entries from the unlabeled connection hash table.

 *

 XXX - should this be a check for NETDEV_DOWN or _UNREGISTER? */

/**

 * netlbl_unlabel_acceptflg_set - Set the unlabeled accept flag

 * @value: desired value

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Set the value of the unlabeled accept flag to @value.

 *

/**

 * netlbl_unlabel_addrinfo_get - Get the IPv4/6 address information

 * @info: the Generic NETLINK info block

 * @addr: the IP address

 * @mask: the IP address mask

 * @len: the address length

 *

 * Description:

 * Examine the Generic NETLINK message and extract the IP address information.

 * Returns zero on success, negative values on failure.

 *

/*

 * NetLabel Command Handlers

/**

 * netlbl_unlabel_accept - Handle an ACCEPT message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated ACCEPT message and set the accept flag accordingly.

 * Returns zero on success, negative values on failure.

 *

/**

 * netlbl_unlabel_list - Handle a LIST message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated LIST message and respond with the current status.

 * Returns zero on success, negative values on failure.

 *

/**

 * netlbl_unlabel_staticadd - Handle a STATICADD message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated STATICADD message and add a new unlabeled

 * connection entry to the hash table.  Returns zero on success, negative

 * values on failure.

 *

	/* Don't allow users to add both IPv4 and IPv6 addresses for a

	 * single entry.  However, allow users to create two entries, one each

	 * for IPv4 and IPv4, with the same LSM security context which should

/**

 * netlbl_unlabel_staticadddef - Handle a STATICADDDEF message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated STATICADDDEF message and add a new default

 * unlabeled connection entry.  Returns zero on success, negative values on

 * failure.

 *

	/* Don't allow users to add both IPv4 and IPv6 addresses for a

	 * single entry.  However, allow users to create two entries, one each

	 * for IPv4 and IPv6, with the same LSM security context which should

/**

 * netlbl_unlabel_staticremove - Handle a STATICREMOVE message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated STATICREMOVE message and remove the specified

 * unlabeled connection entry.  Returns zero on success, negative values on

 * failure.

 *

	/* See the note in netlbl_unlabel_staticadd() about not allowing both

/**

 * netlbl_unlabel_staticremovedef - Handle a STATICREMOVEDEF message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated STATICREMOVEDEF message and remove the default

 * unlabeled connection entry.  Returns zero on success, negative values on

 * failure.

 *

	/* See the note in netlbl_unlabel_staticadd() about not allowing both

/**

 * netlbl_unlabel_staticlist_gen - Generate messages for STATICLIST[DEF]

 * @cmd: command/message

 * @iface: the interface entry

 * @addr4: the IPv4 address entry

 * @addr6: the IPv6 address entry

 * @arg: the netlbl_unlhsh_walk_arg structure

 *

 * Description:

 * This function is designed to be used to generate a response for a

 * STATICLIST or STATICLISTDEF message.  When called either @addr4 or @addr6

 * can be specified, not both, the other unspecified entry should be set to

 * NULL by the caller.  Returns the size of the message on success, negative

 * values on failure.

 *

/**

 * netlbl_unlabel_staticlist - Handle a STATICLIST message

 * @skb: the NETLINK buffer

 * @cb: the NETLINK callback

 *

 * Description:

 * Process a user generated STATICLIST message and dump the unlabeled

 * connection hash table in a form suitable for use in a kernel generated

 * STATICLIST message.  Returns the length of @skb.

 *

 IPv6 */

/**

 * netlbl_unlabel_staticlistdef - Handle a STATICLISTDEF message

 * @skb: the NETLINK buffer

 * @cb: the NETLINK callback

 *

 * Description:

 * Process a user generated STATICLISTDEF message and dump the default

 * unlabeled connection entry in a form suitable for use in a kernel generated

 * STATICLISTDEF message.  Returns the length of @skb.

 *

 IPv6 */

/*

 * NetLabel Generic NETLINK Command Definitions

/*

 * NetLabel Generic NETLINK Protocol Functions

/**

 * netlbl_unlabel_genl_init - Register the Unlabeled NetLabel component

 *

 * Description:

 * Register the unlabeled packet NetLabel component with the Generic NETLINK

 * mechanism.  Returns zero on success, negative values on failure.

 *

/*

 * NetLabel KAPI Hooks

/**

 * netlbl_unlabel_init - Initialize the unlabeled connection hash table

 * @size: the number of bits to use for the hash buckets

 *

 * Description:

 * Initializes the unlabeled connection hash table and registers a network

 * device notification handler.  This function should only be called by the

 * NetLabel subsystem itself during initialization.  Returns zero on success,

 * non-zero values on error.

 *

/**

 * netlbl_unlabel_getattr - Get the security attributes for an unlabled packet

 * @skb: the packet

 * @family: protocol family

 * @secattr: the security attributes

 *

 * Description:

 * Determine the security attributes, if any, for an unlabled packet and return

 * them in @secattr.  Returns zero on success and negative values on failure.

 *

	/* When resolving a fallback label, check the sk_buff version as

	 * it is possible (e.g. SCTP) to have family = PF_INET6 while

	 * receiving ip_hdr(skb)->version = 4.

 IPv6 */

 IPv6 */

/**

 * netlbl_unlabel_defconf - Set the default config to allow unlabeled packets

 *

 * Description:

 * Set the default NetLabel configuration to allow incoming unlabeled packets

 * and to send unlabeled network traffic by default.

 *

	/* Only the kernel is allowed to call this function and the only time

	 * it is called is at bootup before the audit subsystem is reporting

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * NetLabel Management Support

 *

 * This file defines the management functions for the NetLabel system.  The

 * NetLabel system manages static and dynamic label mappings for network

 * protocols such as CIPSO and RIPSO.

 *

 * Author: Paul Moore <paul@paul-moore.com>

/*

 * (c) Copyright Hewlett-Packard Development Company, L.P., 2006, 2008

 NetLabel configured protocol counter */

 Argument struct for netlbl_domhsh_walk() */

 NetLabel Generic NETLINK CIPSOv4 family */

 NetLabel Netlink attribute policy */

/*

 * Helper Functions

/**

 * netlbl_mgmt_add_common - Handle an ADD message

 * @info: the Generic NETLINK info block

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Helper function for the ADD and ADDDEF messages to add the domain mappings

 * from the message to the hash table.  See netlabel.h for a description of the

 * message format.  Returns zero on success, negative values on failure.

 *

	/* NOTE: internally we allow/use a entry->def.type value of

	 *       NETLBL_NLTYPE_ADDRSELECT but we don't currently allow users

	 *       to pass that as a protocol value because we need to know the

 IPv6 */

 IPv6 */

/**

 * netlbl_mgmt_listentry - List a NetLabel/LSM domain map entry

 * @skb: the NETLINK buffer

 * @entry: the map entry

 *

 * Description:

 * This function is a helper function used by the LISTALL and LISTDEF command

 * handlers.  The caller is responsible for ensuring that the RCU read lock

 * is held.  Returns zero on success, negative values on failure.

 *

 IPv6 */

/*

 * NetLabel Command Handlers

/**

 * netlbl_mgmt_add - Handle an ADD message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated ADD message and add the domains from the message

 * to the hash table.  See netlabel.h for a description of the message format.

 * Returns zero on success, negative values on failure.

 *

/**

 * netlbl_mgmt_remove - Handle a REMOVE message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated REMOVE message and remove the specified domain

 * mappings.  Returns zero on success, negative values on failure.

 *

/**

 * netlbl_mgmt_listall_cb - netlbl_domhsh_walk() callback for LISTALL

 * @entry: the domain mapping hash table entry

 * @arg: the netlbl_domhsh_walk_arg structure

 *

 * Description:

 * This function is designed to be used as a callback to the

 * netlbl_domhsh_walk() function for use in generating a response for a LISTALL

 * message.  Returns the size of the message on success, negative values on

 * failure.

 *

/**

 * netlbl_mgmt_listall - Handle a LISTALL message

 * @skb: the NETLINK buffer

 * @cb: the NETLINK callback

 *

 * Description:

 * Process a user generated LISTALL message and dumps the domain hash table in

 * a form suitable for use in a kernel generated LISTALL message.  Returns zero

 * on success, negative values on failure.

 *

/**

 * netlbl_mgmt_adddef - Handle an ADDDEF message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated ADDDEF message and respond accordingly.  Returns

 * zero on success, negative values on failure.

 *

/**

 * netlbl_mgmt_removedef - Handle a REMOVEDEF message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated REMOVEDEF message and remove the default domain

 * mapping.  Returns zero on success, negative values on failure.

 *

/**

 * netlbl_mgmt_listdef - Handle a LISTDEF message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated LISTDEF message and dumps the default domain

 * mapping in a form suitable for use in a kernel generated LISTDEF message.

 * Returns zero on success, negative values on failure.

 *

/**

 * netlbl_mgmt_protocols_cb - Write an individual PROTOCOL message response

 * @skb: the skb to write to

 * @cb: the NETLINK callback

 * @protocol: the NetLabel protocol to use in the message

 *

 * Description:

 * This function is to be used in conjunction with netlbl_mgmt_protocols() to

 * answer a application's PROTOCOLS message.  Returns the size of the message

 * on success, negative values on failure.

 *

/**

 * netlbl_mgmt_protocols - Handle a PROTOCOLS message

 * @skb: the NETLINK buffer

 * @cb: the NETLINK callback

 *

 * Description:

 * Process a user generated PROTOCOLS message and respond accordingly.

 *

/**

 * netlbl_mgmt_version - Handle a VERSION message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated VERSION message and respond accordingly.  Returns

 * zero on success, negative values on failure.

 *

/*

 * NetLabel Generic NETLINK Command Definitions

/*

 * NetLabel Generic NETLINK Protocol Functions

/**

 * netlbl_mgmt_genl_init - Register the NetLabel management component

 *

 * Description:

 * Register the NetLabel management component with the Generic NETLINK

 * mechanism.  Returns zero on success, negative values on failure.

 *

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * NetLabel CALIPSO/IPv6 Support

 *

 * This file defines the CALIPSO/IPv6 functions for the NetLabel system.  The

 * NetLabel system manages static and dynamic label mappings for network

 * protocols such as CIPSO and CALIPSO.

 *

 * Authors: Paul Moore <paul@paul-moore.com>

 *          Huw Davies <huw@codeweavers.com>

/* (c) Copyright Hewlett-Packard Development Company, L.P., 2006

 * (c) Copyright Huw Davies <huw@codeweavers.com>, 2015

 Argument struct for calipso_doi_walk() */

 Argument struct for netlbl_domhsh_walk() */

 NetLabel Generic NETLINK CALIPSO family */

 NetLabel Netlink attribute policy */

/* NetLabel Command Handlers

/**

 * netlbl_calipso_add_pass - Adds a CALIPSO pass DOI definition

 * @info: the Generic NETLINK info block

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Create a new CALIPSO_MAP_PASS DOI definition based on the given ADD message

 * and add it to the CALIPSO engine.  Return zero on success and non-zero on

 * error.

 *

/**

 * netlbl_calipso_add - Handle an ADD message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Create a new DOI definition based on the given ADD message and add it to the

 * CALIPSO engine.  Returns zero on success, negative values on failure.

 *

/**

 * netlbl_calipso_list - Handle a LIST message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated LIST message and respond accordingly.

 * Returns zero on success and negative values on error.

 *

/**

 * netlbl_calipso_listall_cb - calipso_doi_walk() callback for LISTALL

 * @doi_def: the CALIPSO DOI definition

 * @arg: the netlbl_calipso_doiwalk_arg structure

 *

 * Description:

 * This function is designed to be used as a callback to the

 * calipso_doi_walk() function for use in generating a response for a LISTALL

 * message.  Returns the size of the message on success, negative values on

 * failure.

 *

/**

 * netlbl_calipso_listall - Handle a LISTALL message

 * @skb: the NETLINK buffer

 * @cb: the NETLINK callback

 *

 * Description:

 * Process a user generated LISTALL message and respond accordingly.  Returns

 * zero on success and negative values on error.

 *

/**

 * netlbl_calipso_remove_cb - netlbl_calipso_remove() callback for REMOVE

 * @entry: LSM domain mapping entry

 * @arg: the netlbl_domhsh_walk_arg structure

 *

 * Description:

 * This function is intended for use by netlbl_calipso_remove() as the callback

 * for the netlbl_domhsh_walk() function; it removes LSM domain map entries

 * which are associated with the CALIPSO DOI specified in @arg.  Returns zero on

 * success, negative values on failure.

 *

/**

 * netlbl_calipso_remove - Handle a REMOVE message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated REMOVE message and respond accordingly.  Returns

 * zero on success, negative values on failure.

 *

/* NetLabel Generic NETLINK Command Definitions

/* NetLabel Generic NETLINK Protocol Functions

/**

 * netlbl_calipso_genl_init - Register the CALIPSO NetLabel component

 *

 * Description:

 * Register the CALIPSO packet NetLabel component with the Generic NETLINK

 * mechanism.  Returns zero on success, negative values on failure.

 *

/**

 * netlbl_calipso_ops_register - Register the CALIPSO operations

 * @ops: ops to register

 *

 * Description:

 * Register the CALIPSO packet engine operations.

 *

/**

 * calipso_doi_add - Add a new DOI to the CALIPSO protocol engine

 * @doi_def: the DOI structure

 * @audit_info: NetLabel audit information

 *

 * Description:

 * The caller defines a new DOI for use by the CALIPSO engine and calls this

 * function to add it to the list of acceptable domains.  The caller must

 * ensure that the mapping table specified in @doi_def->map meets all of the

 * requirements of the mapping type (see calipso.h for details).  Returns

 * zero on success and non-zero on failure.

 *

/**

 * calipso_doi_free - Frees a DOI definition

 * @doi_def: the DOI definition

 *

 * Description:

 * This function frees all of the memory associated with a DOI definition.

 *

/**

 * calipso_doi_remove - Remove an existing DOI from the CALIPSO protocol engine

 * @doi: the DOI value

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Removes a DOI definition from the CALIPSO engine.  The NetLabel routines will

 * be called to release their own LSM domain mappings as well as our own

 * domain list.  Returns zero on success and negative values on failure.

 *

/**

 * calipso_doi_getdef - Returns a reference to a valid DOI definition

 * @doi: the DOI value

 *

 * Description:

 * Searches for a valid DOI definition and if one is found it is returned to

 * the caller.  Otherwise NULL is returned.  The caller must ensure that

 * calipso_doi_putdef() is called when the caller is done.

 *

/**

 * calipso_doi_putdef - Releases a reference for the given DOI definition

 * @doi_def: the DOI definition

 *

 * Description:

 * Releases a DOI definition reference obtained from calipso_doi_getdef().

 *

/**

 * calipso_doi_walk - Iterate through the DOI definitions

 * @skip_cnt: skip past this number of DOI definitions, updated

 * @callback: callback for each DOI definition

 * @cb_arg: argument for the callback function

 *

 * Description:

 * Iterate over the DOI definition list, skipping the first @skip_cnt entries.

 * For each entry call @callback, if @callback returns a negative value stop

 * 'walking' through the list and return.  Updates the value in @skip_cnt upon

 * return.  Returns zero on success, negative values on failure.

 *

/**

 * calipso_sock_getattr - Get the security attributes from a sock

 * @sk: the sock

 * @secattr: the security attributes

 *

 * Description:

 * Query @sk to see if there is a CALIPSO option attached to the sock and if

 * there is return the CALIPSO security attributes in @secattr.  This function

 * requires that @sk be locked, or privately held, but it does not do any

 * locking itself.  Returns zero on success and negative values on failure.

 *

/**

 * calipso_sock_setattr - Add a CALIPSO option to a socket

 * @sk: the socket

 * @doi_def: the CALIPSO DOI to use

 * @secattr: the specific security attributes of the socket

 *

 * Description:

 * Set the CALIPSO option on the given socket using the DOI definition and

 * security attributes passed to the function.  This function requires

 * exclusive access to @sk, which means it either needs to be in the

 * process of being created or locked.  Returns zero on success and negative

 * values on failure.

 *

/**

 * calipso_sock_delattr - Delete the CALIPSO option from a socket

 * @sk: the socket

 *

 * Description:

 * Removes the CALIPSO option from a socket, if present.

 *

/**

 * calipso_req_setattr - Add a CALIPSO option to a connection request socket

 * @req: the connection request socket

 * @doi_def: the CALIPSO DOI to use

 * @secattr: the specific security attributes of the socket

 *

 * Description:

 * Set the CALIPSO option on the given socket using the DOI definition and

 * security attributes passed to the function.  Returns zero on success and

 * negative values on failure.

 *

/**

 * calipso_req_delattr - Delete the CALIPSO option from a request socket

 * @req: the request socket

 *

 * Description:

 * Removes the CALIPSO option from a request socket, if present.

 *

/**

 * calipso_optptr - Find the CALIPSO option in the packet

 * @skb: the packet

 *

 * Description:

 * Parse the packet's IP header looking for a CALIPSO option.  Returns a pointer

 * to the start of the CALIPSO option on success, NULL if one if not found.

 *

/**

 * calipso_getattr - Get the security attributes from a memory block.

 * @calipso: the CALIPSO option

 * @secattr: the security attributes

 *

 * Description:

 * Inspect @calipso and return the security attributes in @secattr.

 * Returns zero on success and negative values on failure.

 *

/**

 * calipso_skbuff_setattr - Set the CALIPSO option on a packet

 * @skb: the packet

 * @doi_def: the CALIPSO DOI to use

 * @secattr: the security attributes

 *

 * Description:

 * Set the CALIPSO option on the given packet based on the security attributes.

 * Returns a pointer to the IP header on success and NULL on failure.

 *

/**

 * calipso_skbuff_delattr - Delete any CALIPSO options from a packet

 * @skb: the packet

 *

 * Description:

 * Removes any and all CALIPSO options from the given packet.  Returns zero on

 * success, negative values on failure.

 *

/**

 * calipso_cache_invalidate - Invalidates the current CALIPSO cache

 *

 * Description:

 * Invalidates and frees any entries in the CALIPSO cache.  Returns zero on

 * success and negative values on failure.

 *

/**

 * calipso_cache_add - Add an entry to the CALIPSO cache

 * @calipso_ptr: the CALIPSO option

 * @secattr: the packet's security attributes

 *

 * Description:

 * Add a new entry into the CALIPSO label mapping cache.

 * Returns zero on success, negative values on failure.

 *

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * NetLabel CIPSO/IPv4 Support

 *

 * This file defines the CIPSO/IPv4 functions for the NetLabel system.  The

 * NetLabel system manages static and dynamic label mappings for network

 * protocols such as CIPSO and RIPSO.

 *

 * Author: Paul Moore <paul@paul-moore.com>

/*

 * (c) Copyright Hewlett-Packard Development Company, L.P., 2006

 Argument struct for cipso_v4_doi_walk() */

 Argument struct for netlbl_domhsh_walk() */

 NetLabel Generic NETLINK CIPSOv4 family */

 NetLabel Netlink attribute policy */

/*

 * Helper Functions

/**

 * netlbl_cipsov4_add_common - Parse the common sections of a ADD message

 * @info: the Generic NETLINK info block

 * @doi_def: the CIPSO V4 DOI definition

 *

 * Description:

 * Parse the common sections of a ADD message and fill in the related values

 * in @doi_def.  Returns zero on success, negative values on failure.

 *

/*

 * NetLabel Command Handlers

/**

 * netlbl_cipsov4_add_std - Adds a CIPSO V4 DOI definition

 * @info: the Generic NETLINK info block

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Create a new CIPSO_V4_MAP_TRANS DOI definition based on the given ADD

 * message and add it to the CIPSO V4 engine.  Return zero on success and

 * non-zero on error.

 *

/**

 * netlbl_cipsov4_add_pass - Adds a CIPSO V4 DOI definition

 * @info: the Generic NETLINK info block

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Create a new CIPSO_V4_MAP_PASS DOI definition based on the given ADD message

 * and add it to the CIPSO V4 engine.  Return zero on success and non-zero on

 * error.

 *

/**

 * netlbl_cipsov4_add_local - Adds a CIPSO V4 DOI definition

 * @info: the Generic NETLINK info block

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Create a new CIPSO_V4_MAP_LOCAL DOI definition based on the given ADD

 * message and add it to the CIPSO V4 engine.  Return zero on success and

 * non-zero on error.

 *

/**

 * netlbl_cipsov4_add - Handle an ADD message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Create a new DOI definition based on the given ADD message and add it to the

 * CIPSO V4 engine.  Returns zero on success, negative values on failure.

 *

/**

 * netlbl_cipsov4_list - Handle a LIST message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated LIST message and respond accordingly.  While the

 * response message generated by the kernel is straightforward, determining

 * before hand the size of the buffer to allocate is not (we have to generate

 * the message to know the size).  In order to keep this function sane what we

 * do is allocate a buffer of NLMSG_GOODSIZE and try to fit the response in

 * that size, if we fail then we restart with a larger buffer and try again.

 * We continue in this manner until we hit a limit of failed attempts then we

 * give up and just send an error message.  Returns zero on success and

 * negative values on error.

 *

 XXX - this limit is a guesstimate */

/**

 * netlbl_cipsov4_listall_cb - cipso_v4_doi_walk() callback for LISTALL

 * @doi_def: the CIPSOv4 DOI definition

 * @arg: the netlbl_cipsov4_doiwalk_arg structure

 *

 * Description:

 * This function is designed to be used as a callback to the

 * cipso_v4_doi_walk() function for use in generating a response for a LISTALL

 * message.  Returns the size of the message on success, negative values on

 * failure.

 *

/**

 * netlbl_cipsov4_listall - Handle a LISTALL message

 * @skb: the NETLINK buffer

 * @cb: the NETLINK callback

 *

 * Description:

 * Process a user generated LISTALL message and respond accordingly.  Returns

 * zero on success and negative values on error.

 *

/**

 * netlbl_cipsov4_remove_cb - netlbl_cipsov4_remove() callback for REMOVE

 * @entry: LSM domain mapping entry

 * @arg: the netlbl_domhsh_walk_arg structure

 *

 * Description:

 * This function is intended for use by netlbl_cipsov4_remove() as the callback

 * for the netlbl_domhsh_walk() function; it removes LSM domain map entries

 * which are associated with the CIPSO DOI specified in @arg.  Returns zero on

 * success, negative values on failure.

 *

/**

 * netlbl_cipsov4_remove - Handle a REMOVE message

 * @skb: the NETLINK buffer

 * @info: the Generic NETLINK info block

 *

 * Description:

 * Process a user generated REMOVE message and respond accordingly.  Returns

 * zero on success, negative values on failure.

 *

/*

 * NetLabel Generic NETLINK Command Definitions

/*

 * NetLabel Generic NETLINK Protocol Functions

/**

 * netlbl_cipsov4_genl_init - Register the CIPSOv4 NetLabel component

 *

 * Description:

 * Register the CIPSOv4 packet NetLabel component with the Generic NETLINK

 * mechanism.  Returns zero on success, negative values on failure.

 *

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * NetLabel Domain Hash Table

 *

 * This file manages the domain hash table that NetLabel uses to determine

 * which network labeling protocol to use for a given domain.  The NetLabel

 * system manages static and dynamic label mappings for network protocols such

 * as CIPSO and RIPSO.

 *

 * Author: Paul Moore <paul@paul-moore.com>

/*

 * (c) Copyright Hewlett-Packard Development Company, L.P., 2006, 2008

 Domain hash table */

/* updates should be so rare that having one spinlock for the entire hash table

/*

 * Domain Hash Table Helper Functions

/**

 * netlbl_domhsh_free_entry - Frees a domain hash table entry

 * @entry: the entry's RCU field

 *

 * Description:

 * This function is designed to be used as a callback to the call_rcu()

 * function so that the memory allocated to a hash table entry can be released

 * safely.

 *

 IPv6 */

 IPv6 */

/**

 * netlbl_domhsh_hash - Hashing function for the domain hash table

 * @key: the domain name to hash

 *

 * Description:

 * This is the hashing function for the domain hash table, it returns the

 * correct bucket number for the domain.  The caller is responsible for

 * ensuring that the hash table is protected with either a RCU read lock or the

 * hash table lock.

 *

	/* This is taken (with slight modification) from

/**

 * netlbl_domhsh_search - Search for a domain entry

 * @domain: the domain

 * @family: the address family

 *

 * Description:

 * Searches the domain hash table and returns a pointer to the hash table

 * entry if found, otherwise NULL is returned.  @family may be %AF_UNSPEC

 * which matches any address family entries.  The caller is responsible for

 * ensuring that the hash table is protected with either a RCU read lock or the

 * hash table lock.

 *

/**

 * netlbl_domhsh_search_def - Search for a domain entry

 * @domain: the domain

 * @family: the address family

 *

 * Description:

 * Searches the domain hash table and returns a pointer to the hash table

 * entry if an exact match is found, if an exact match is not present in the

 * hash table then the default entry is returned if valid otherwise NULL is

 * returned.  @family may be %AF_UNSPEC which matches any address family

 * entries.  The caller is responsible ensuring that the hash table is

 * protected with either a RCU read lock or the hash table lock.

 *

/**

 * netlbl_domhsh_audit_add - Generate an audit entry for an add event

 * @entry: the entry being added

 * @addr4: the IPv4 address information

 * @addr6: the IPv6 address information

 * @result: the result code

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Generate an audit record for adding a new NetLabel/LSM mapping entry with

 * the given information.  Caller is responsible for holding the necessary

 * locks.

 *

 IPv6 */

/**

 * netlbl_domhsh_validate - Validate a new domain mapping entry

 * @entry: the entry to validate

 *

 * This function validates the new domain mapping entry to ensure that it is

 * a valid entry.  Returns zero on success, negative values on failure.

 *

 IPv6 */

 IPv6 */

/*

 * Domain Hash Table Functions

/**

 * netlbl_domhsh_init - Init for the domain hash

 * @size: the number of bits to use for the hash buckets

 *

 * Description:

 * Initializes the domain hash table, should be called only by

 * netlbl_user_init() during initialization.  Returns zero on success, non-zero

 * values on error.

 *

/**

 * netlbl_domhsh_add - Adds a entry to the domain hash table

 * @entry: the entry to add

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Adds a new entry to the domain hash table and handles any updates to the

 * lower level protocol handler (i.e. CIPSO).  @entry->family may be set to

 * %AF_UNSPEC which will add an entry that matches all address families.  This

 * is only useful for the unlabelled type and will only succeed if there is no

 * existing entry for any address family with the same domain.  Returns zero

 * on success, negative on failure.

 *

 IPv6 */

	/* XXX - we can remove this RCU read lock as the spinlock protects the

	 *       entire function, but before we do we need to fixup the

	 *       netlbl_af[4,6]list RCU functions to do "the right thing" with

				/* Already checked in

 IPv6 */

		/* we only allow the addition of address selectors if all of

 IPv6 */

 IPv6 */

 cleanup the new entry since we've moved everything over */

/**

 * netlbl_domhsh_add_default - Adds the default entry to the domain hash table

 * @entry: the entry to add

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Adds a new default entry to the domain hash table and handles any updates

 * to the lower level protocol handler (i.e. CIPSO).  Returns zero on success,

 * negative on failure.

 *

/**

 * netlbl_domhsh_remove_entry - Removes a given entry from the domain table

 * @entry: the entry to remove

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Removes an entry from the domain hash table and handles any updates to the

 * lower level protocol handler (i.e. CIPSO).  Caller is responsible for

 * ensuring that the RCU read lock is held.  Returns zero on success, negative

 * on failure.

 *

 IPv6 */

 IPv6 */

 IPv6 */

/**

 * netlbl_domhsh_remove_af4 - Removes an address selector entry

 * @domain: the domain

 * @addr: IPv4 address

 * @mask: IPv4 address mask

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Removes an individual address selector from a domain mapping and potentially

 * the entire mapping if it is empty.  Returns zero on success, negative values

 * on failure.

 *

 IPv6 */

 IPv6 */

 the domain mapping is empty so remove it from the mapping table */

	/* yick, we can't use call_rcu here because we don't have a rcu head

	 * pointer but hopefully this should be a rare case so the pause

/**

 * netlbl_domhsh_remove_af6 - Removes an address selector entry

 * @domain: the domain

 * @addr: IPv6 address

 * @mask: IPv6 address mask

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Removes an individual address selector from a domain mapping and potentially

 * the entire mapping if it is empty.  Returns zero on success, negative values

 * on failure.

 *

 the domain mapping is empty so remove it from the mapping table */

	/* yick, we can't use call_rcu here because we don't have a rcu head

	 * pointer but hopefully this should be a rare case so the pause

 IPv6 */

/**

 * netlbl_domhsh_remove - Removes an entry from the domain hash table

 * @domain: the domain to remove

 * @family: address family

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Removes an entry from the domain hash table and handles any updates to the

 * lower level protocol handler (i.e. CIPSO).  @family may be %AF_UNSPEC which

 * removes all address family entries.  Returns zero on success, negative on

 * failure.

 *

/**

 * netlbl_domhsh_remove_default - Removes the default entry from the table

 * @family: address family

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Removes/resets the default entry corresponding to @family from the domain

 * hash table and handles any updates to the lower level protocol handler

 * (i.e. CIPSO).  @family may be %AF_UNSPEC which removes all address family

 * entries.  Returns zero on success, negative on failure.

 *

/**

 * netlbl_domhsh_getentry - Get an entry from the domain hash table

 * @domain: the domain name to search for

 * @family: address family

 *

 * Description:

 * Look through the domain hash table searching for an entry to match @domain,

 * with address family @family, return a pointer to a copy of the entry or

 * NULL.  The caller is responsible for ensuring that rcu_read_[un]lock() is

 * called.

 *

/**

 * netlbl_domhsh_getentry_af4 - Get an entry from the domain hash table

 * @domain: the domain name to search for

 * @addr: the IP address to search for

 *

 * Description:

 * Look through the domain hash table searching for an entry to match @domain

 * and @addr, return a pointer to a copy of the entry or NULL.  The caller is

 * responsible for ensuring that rcu_read_[un]lock() is called.

 *

/**

 * netlbl_domhsh_getentry_af6 - Get an entry from the domain hash table

 * @domain: the domain name to search for

 * @addr: the IP address to search for

 *

 * Description:

 * Look through the domain hash table searching for an entry to match @domain

 * and @addr, return a pointer to a copy of the entry or NULL.  The caller is

 * responsible for ensuring that rcu_read_[un]lock() is called.

 *

 IPv6 */

/**

 * netlbl_domhsh_walk - Iterate through the domain mapping hash table

 * @skip_bkt: the number of buckets to skip at the start

 * @skip_chain: the number of entries to skip in the first iterated bucket

 * @callback: callback for each entry

 * @cb_arg: argument for the callback function

 *

 * Description:

 * Iterate over the domain mapping hash table, skipping the first @skip_bkt

 * buckets and @skip_chain entries.  For each entry in the table call

 * @callback, if @callback returns a negative value stop 'walking' through the

 * table and return.  Updates the values in @skip_bkt and @skip_chain on

 * return.  Returns zero on success, negative values on failure.

 *

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * NetLabel Kernel API

 *

 * This file defines the kernel API for the NetLabel system.  The NetLabel

 * system manages static and dynamic label mappings for network protocols such

 * as CIPSO and RIPSO.

 *

 * Author: Paul Moore <paul@paul-moore.com>

/*

 * (c) Copyright Hewlett-Packard Development Company, L.P., 2006, 2008

/*

 * Configuration Functions

/**

 * netlbl_cfg_map_del - Remove a NetLabel/LSM domain mapping

 * @domain: the domain mapping to remove

 * @family: address family

 * @addr: IP address

 * @mask: IP address mask

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Removes a NetLabel/LSM domain mapping.  A @domain value of NULL causes the

 * default domain mapping to be removed.  Returns zero on success, negative

 * values on failure.

 *

 IPv6 */

/**

 * netlbl_cfg_unlbl_map_add - Add a new unlabeled mapping

 * @domain: the domain mapping to add

 * @family: address family

 * @addr: IP address

 * @mask: IP address mask

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Adds a new unlabeled NetLabel/LSM domain mapping.  A @domain value of NULL

 * causes a new default domain mapping to be added.  Returns zero on success,

 * negative values on failure.

 *

 IPv6 */

/**

 * netlbl_cfg_unlbl_static_add - Adds a new static label

 * @net: network namespace

 * @dev_name: interface name

 * @addr: IP address in network byte order (struct in[6]_addr)

 * @mask: address mask in network byte order (struct in[6]_addr)

 * @family: address family

 * @secid: LSM secid value for the entry

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Adds a new NetLabel static label to be used when protocol provided labels

 * are not present on incoming traffic.  If @dev_name is NULL then the default

 * interface will be used.  Returns zero on success, negative values on failure.

 *

 IPv6 */

/**

 * netlbl_cfg_unlbl_static_del - Removes an existing static label

 * @net: network namespace

 * @dev_name: interface name

 * @addr: IP address in network byte order (struct in[6]_addr)

 * @mask: address mask in network byte order (struct in[6]_addr)

 * @family: address family

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Removes an existing NetLabel static label used when protocol provided labels

 * are not present on incoming traffic.  If @dev_name is NULL then the default

 * interface will be used.  Returns zero on success, negative values on failure.

 *

 IPv6 */

/**

 * netlbl_cfg_cipsov4_add - Add a new CIPSOv4 DOI definition

 * @doi_def: CIPSO DOI definition

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Add a new CIPSO DOI definition as defined by @doi_def.  Returns zero on

 * success and negative values on failure.

 *

/**

 * netlbl_cfg_cipsov4_del - Remove an existing CIPSOv4 DOI definition

 * @doi: CIPSO DOI

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Remove an existing CIPSO DOI definition matching @doi.  Returns zero on

 * success and negative values on failure.

 *

/**

 * netlbl_cfg_cipsov4_map_add - Add a new CIPSOv4 DOI mapping

 * @doi: the CIPSO DOI

 * @domain: the domain mapping to add

 * @addr: IP address

 * @mask: IP address mask

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Add a new NetLabel/LSM domain mapping for the given CIPSO DOI to the NetLabel

 * subsystem.  A @domain value of NULL adds a new default domain mapping.

 * Returns zero on success, negative values on failure.

 *

/**

 * netlbl_cfg_calipso_add - Add a new CALIPSO DOI definition

 * @doi_def: CALIPSO DOI definition

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Add a new CALIPSO DOI definition as defined by @doi_def.  Returns zero on

 * success and negative values on failure.

 *

 IPv6 */

 IPv6 */

/**

 * netlbl_cfg_calipso_del - Remove an existing CALIPSO DOI definition

 * @doi: CALIPSO DOI

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Remove an existing CALIPSO DOI definition matching @doi.  Returns zero on

 * success and negative values on failure.

 *

 IPv6 */

/**

 * netlbl_cfg_calipso_map_add - Add a new CALIPSO DOI mapping

 * @doi: the CALIPSO DOI

 * @domain: the domain mapping to add

 * @addr: IP address

 * @mask: IP address mask

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Add a new NetLabel/LSM domain mapping for the given CALIPSO DOI to the

 * NetLabel subsystem.  A @domain value of NULL adds a new default domain

 * mapping.  Returns zero on success, negative values on failure.

 *

 IPv6 */

 IPv6 */

/*

 * Security Attribute Functions

/**

 * _netlbl_catmap_getnode - Get a individual node from a catmap

 * @catmap: pointer to the category bitmap

 * @offset: the requested offset

 * @cm_flags: catmap flags, see _CM_F_*

 * @gfp_flags: memory allocation flags

 *

 * Description:

 * Iterate through the catmap looking for the node associated with @offset.

 * If the _CM_F_ALLOC flag is set in @cm_flags and there is no associated node,

 * one will be created and inserted into the catmap.  If the _CM_F_WALK flag is

 * set in @cm_flags and there is no associated node, the next highest node will

 * be returned.  Returns a pointer to the node on success, NULL on failure.

 *

/**

 * netlbl_catmap_walk - Walk a LSM secattr catmap looking for a bit

 * @catmap: the category bitmap

 * @offset: the offset to start searching at, in bits

 *

 * Description:

 * This function walks a LSM secattr category bitmap starting at @offset and

 * returns the spot of the first set bit or -ENOENT if no bits are set.

 *

/**

 * netlbl_catmap_walkrng - Find the end of a string of set bits

 * @catmap: the category bitmap

 * @offset: the offset to start searching at, in bits

 *

 * Description:

 * This function walks a LSM secattr category bitmap starting at @offset and

 * returns the spot of the first cleared bit or -ENOENT if the offset is past

 * the end of the bitmap.

 *

/**

 * netlbl_catmap_getlong - Export an unsigned long bitmap

 * @catmap: pointer to the category bitmap

 * @offset: pointer to the requested offset

 * @bitmap: the exported bitmap

 *

 * Description:

 * Export a bitmap with an offset greater than or equal to @offset and return

 * it in @bitmap.  The @offset must be aligned to an unsigned long and will be

 * updated on return if different from what was requested; if the catmap is

 * empty at the requested offset and beyond, the @offset is set to (u32)-1.

 * Returns zero on success, negative values on failure.

 *

 only allow aligned offsets */

 a null catmap is equivalent to an empty one */

/**

 * netlbl_catmap_setbit - Set a bit in a LSM secattr catmap

 * @catmap: pointer to the category bitmap

 * @bit: the bit to set

 * @flags: memory allocation flags

 *

 * Description:

 * Set the bit specified by @bit in @catmap.  Returns zero on success,

 * negative values on failure.

 *

/**

 * netlbl_catmap_setrng - Set a range of bits in a LSM secattr catmap

 * @catmap: pointer to the category bitmap

 * @start: the starting bit

 * @end: the last bit in the string

 * @flags: memory allocation flags

 *

 * Description:

 * Set a range of bits, starting at @start and ending with @end.  Returns zero

 * on success, negative values on failure.

 *

/**

 * netlbl_catmap_setlong - Import an unsigned long bitmap

 * @catmap: pointer to the category bitmap

 * @offset: offset to the start of the imported bitmap

 * @bitmap: the bitmap to import

 * @flags: memory allocation flags

 *

 * Description:

 * Import the bitmap specified in @bitmap into @catmap, using the offset

 * in @offset.  The offset must be aligned to an unsigned long.  Returns zero

 * on success, negative values on failure.

 *

 only allow aligned offsets */

/* Bitmap functions

/**

 * netlbl_bitmap_walk - Walk a bitmap looking for a bit

 * @bitmap: the bitmap

 * @bitmap_len: length in bits

 * @offset: starting offset

 * @state: if non-zero, look for a set (1) bit else look for a cleared (0) bit

 *

 * Description:

 * Starting at @offset, walk the bitmap from left to right until either the

 * desired bit is found or we reach the end.  Return the bit offset, -1 if

 * not found, or -2 if error.

/**

 * netlbl_bitmap_setbit - Sets a single bit in a bitmap

 * @bitmap: the bitmap

 * @bit: the bit

 * @state: if non-zero, set the bit (1) else clear the bit (0)

 *

 * Description:

 * Set a single bit in the bitmask.  Returns zero on success, negative values

 * on error.

 gcc always rounds to zero when doing integer division */

/*

 * LSM Functions

/**

 * netlbl_enabled - Determine if the NetLabel subsystem is enabled

 *

 * Description:

 * The LSM can use this function to determine if it should use NetLabel

 * security attributes in it's enforcement mechanism.  Currently, NetLabel is

 * considered to be enabled when it's configuration contains a valid setup for

 * at least one labeled protocol (i.e. NetLabel can understand incoming

 * labeled packets of at least one type); otherwise NetLabel is considered to

 * be disabled.

 *

	/* At some point we probably want to expose this mechanism to the user

	 * as well so that admins can toggle NetLabel regardless of the

/**

 * netlbl_sock_setattr - Label a socket using the correct protocol

 * @sk: the socket to label

 * @family: protocol family

 * @secattr: the security attributes

 *

 * Description:

 * Attach the correct label to the given socket using the security attributes

 * specified in @secattr.  This function requires exclusive access to @sk,

 * which means it either needs to be in the process of being created or locked.

 * Returns zero on success, -EDESTADDRREQ if the domain is configured to use

 * network address selectors (can't blindly label the socket), and negative

 * values on all other failures.

 *

 IPv6 */

/**

 * netlbl_sock_delattr - Delete all the NetLabel labels on a socket

 * @sk: the socket

 *

 * Description:

 * Remove all the NetLabel labeling from @sk.  The caller is responsible for

 * ensuring that @sk is locked.

 *

 IPv6 */

/**

 * netlbl_sock_getattr - Determine the security attributes of a sock

 * @sk: the sock

 * @secattr: the security attributes

 *

 * Description:

 * Examines the given sock to see if any NetLabel style labeling has been

 * applied to the sock, if so it parses the socket label and returns the

 * security attributes in @secattr.  Returns zero on success, negative values

 * on failure.

 *

 IPv6 */

/**

 * netlbl_conn_setattr - Label a connected socket using the correct protocol

 * @sk: the socket to label

 * @addr: the destination address

 * @secattr: the security attributes

 *

 * Description:

 * Attach the correct label to the given connected socket using the security

 * attributes specified in @secattr.  The caller is responsible for ensuring

 * that @sk is locked.  Returns zero on success, negative values on failure.

 *

			/* just delete the protocols we support for right now

			/* just delete the protocols we support for right now

 IPv6 */

/**

 * netlbl_req_setattr - Label a request socket using the correct protocol

 * @req: the request socket to label

 * @secattr: the security attributes

 *

 * Description:

 * Attach the correct label to the given socket using the security attributes

 * specified in @secattr.  Returns zero on success, negative values on failure.

 *

 IPv6 */

/**

* netlbl_req_delattr - Delete all the NetLabel labels on a socket

* @req: the socket

*

* Description:

* Remove all the NetLabel labeling from @req.

*

 IPv6 */

/**

 * netlbl_skbuff_setattr - Label a packet using the correct protocol

 * @skb: the packet

 * @family: protocol family

 * @secattr: the security attributes

 *

 * Description:

 * Attach the correct label to the given packet using the security attributes

 * specified in @secattr.  Returns zero on success, negative values on failure.

 *

			/* just delete the protocols we support for right now

			/* just delete the protocols we support for right now

 IPv6 */

/**

 * netlbl_skbuff_getattr - Determine the security attributes of a packet

 * @skb: the packet

 * @family: protocol family

 * @secattr: the security attributes

 *

 * Description:

 * Examines the given packet to see if a recognized form of packet labeling

 * is present, if so it parses the packet label and returns the security

 * attributes in @secattr.  Returns zero on success, negative values on

 * failure.

 *

 IPv6 */

/**

 * netlbl_skbuff_err - Handle a LSM error on a sk_buff

 * @skb: the packet

 * @family: the family

 * @error: the error code

 * @gateway: true if host is acting as a gateway, false otherwise

 *

 * Description:

 * Deal with a LSM problem when handling the packet in @skb, typically this is

 * a permission denied problem (-EACCES).  The correct action is determined

 * according to the packet's labeling protocol.

 *

/**

 * netlbl_cache_invalidate - Invalidate all of the NetLabel protocol caches

 *

 * Description:

 * For all of the NetLabel protocols that support some form of label mapping

 * cache, invalidate the cache.  Returns zero on success, negative values on

 * error.

 *

 IPv6 */

/**

 * netlbl_cache_add - Add an entry to a NetLabel protocol cache

 * @skb: the packet

 * @family: the family

 * @secattr: the packet's security attributes

 *

 * Description:

 * Add the LSM security attributes for the given packet to the underlying

 * NetLabel protocol's label mapping cache.  Returns zero on success, negative

 * values on error.

 *

 IPv6 */

/*

 * Protocol Engine Functions

/**

 * netlbl_audit_start - Start an audit message

 * @type: audit message type

 * @audit_info: NetLabel audit information

 *

 * Description:

 * Start an audit message using the type specified in @type and fill the audit

 * message with some fields common to all NetLabel audit messages.  This

 * function should only be used by protocol engines, not LSMs.  Returns a

 * pointer to the audit buffer on success, NULL on failure.

 *

/*

 * Setup Functions

/**

 * netlbl_init - Initialize NetLabel

 *

 * Description:

 * Perform the required NetLabel initialization before first use.

 *

 SPDX-License-Identifier: GPL-2.0

/* Copyright 2011-2014 Autronica Fire and Security AS

 *

 * Author(s):

 *	2011-2014 Arvid Brodin, arvid.brodin@alten.se

 *

 * Frame handler other utility functions for HSR and PRP.

 Packets from dev_loopback_xmit() do not have L2 header, bail out */

 Directly kill frames sent by ourselves */

	/* For HSR, only tagged frames are expected (unless the device offloads

	 * HSR tag removal), but for PRP there could be non tagged frames as

	 * well from Single attached nodes (SANs).

 Don't allow HSR on non-ethernet like devices */

 Don't allow enslaving hsr devices */

	/* HSR over bonded devices has not been tested, but I'm not sure it

	 * won't work...

 Setup device to be added to the HSR bridge. */

 This port already exists */

 SPDX-License-Identifier: GPL-2.0

/* Copyright 2011-2014 Autronica Fire and Security AS

 *

 * Author(s):

 *	2011-2014 Arvid Brodin, arvid.brodin@alten.se

 * This file contains device methods for creating, using and destroying

 * virtual HSR or PRP devices.

 Went up */

 Went down */

	/* netif_stacked_transfer_operstate() cannot be used here since

	 * it doesn't set IF_OPER_LOWERLAYERDOWN (?)

 Nothing to do here. */

	/* Mask out all features that, if supported by one device, should be

	 * enabled for all devices (see NETIF_F_ONE_FOR_ALL).

	 *

	 * Anything that's off in mask will not be enabled - so only things

	 * that were in features originally, and also is in NETIF_F_ONE_FOR_ALL,

	 * may become enabled.

	/* skb size is same for PRP/HSR frames, only difference

	 * being, for PRP it is a trailer and for HSR it is a

	 * header

 From HSRv1 on we have separate supervision sequence numbers. */

 TODO: Why 12 in HSRv0? */

 Payload: MacAddressA */

 From HSRv1 on we have separate supervision sequence numbers. */

 Payload: MacAddressA */

/* Announce (supervision frame) timer function

 Prevent recursive tx locking */

	/* VLAN on top of HSR needs testing and probably some work on

	 * hsr_header_create() etc.

	/* Not sure about this. Taken from bridge code. netdev_features.h says

	 * it means "Does not change network namespaces".

/* Return true if dev is a HSR master; return false otherwise.

 Default multicast address for HSR Supervision frames */

 initialize protocol specific functions */

		/* For PRP, lan_id has most significant 3 bits holding

		 * the net_id of PRP_LAN_ID

 Make sure we recognize frames from ourselves in hsr_rcv() */

 Overflow soon to find bugs easier: */

 Make sure the 1st call to netif_carrier_on() gets through */

 SPDX-License-Identifier: GPL-2.0

/* Copyright 2011-2014 Autronica Fire and Security AS

 *

 * Author(s):

 *	2011-2014 Arvid Brodin, arvid.brodin@alten.se

 *

 * Frame router for HSR and PRP.

/* The uses I can see for these HSR supervision frames are:

 * 1) Use the frames that are sent after node initialization ("HSR_TLV.Type =

 *    22") to reset any sequence_nr counters belonging to that node. Useful if

 *    the other node's counter has been reset for some reason.

 *    --

 *    Or not - resetting the counter and bridging the frame would create a

 *    loop, unfortunately.

 *

 * 2) Use the LifeCheck frames to detect ring breaks. I.e. if no LifeCheck

 *    frame is received from a particular node, we know something is wrong.

 *    We just register these (as with normal frames) and throw them away.

 *

 * 3) Allow different MAC addresses for the two slave interfaces, using the

 *    MacAddressA field.

 Correct addr? */

 Correct ether type?. */

 Get the supervision header from correct location. */

 Okay HSRv1. */

 Get next tlv */

	/* if this is a redbox supervision frame we need to verify

	 * that more data is available

 tlv length must be a length of a mac address */

 make sure another tlv follows */

 get next tlv */

 end of tlvs must follow at the end */

 Unexpected */

 trim the skb by len - HSR_HLEN to exclude RCT */

 Unexpected */

 Add net_id in the upper 3 bits of lane_id */

 Tailroom for PRP rct should have been created before calling this */

 pad to minimum packet size which is 60 + 6 (HSR tag) */

/* If the original frame was an HSR tagged frame, just clone it to be sent

 * unchanged. Otherwise, create a private frame especially tagged for 'port'.

 set the lane id properly */

 Create the new skb with enough headroom to fit the HSR tag */

	/* skb_put_padto free skb on error and hsr_fill_tag returns NULL in

	 * that case

		/* Address substitution (IEC62439-3 pp 26, 50): replace mac

		 * address of outgoing frame with that of the outgoing slave's.

/* Forward the frame through all devices except:

 * - Back through the receiving device

 * - If it's a HSR frame: through a device where it has passed before

 * - if it's a PRP frame: through another PRP slave device (no bridge)

 * - To the local HSR master only if the frame is directly addressed to it, or

 *   a non-supervision multicast or broadcast frame.

 *

 * HSR slave devices should insert a HSR tag into the frame, or forward the

 * frame unchanged if it's already tagged. Interlink devices should strip HSR

 * tags if they're of the non-HSR type (but only after duplicate discard). The

 * master device always strips HSR tags.

 Don't send frame back the way it came */

 Don't deliver locally unless we should */

 Deliver frames directly addressed to us to master only */

		/* If hardware duplicate generation is enabled, only send out

		 * one port.

		/* Don't send frame over port where it has been sent before.

		 * Also fro SAN, this shouldn't be done.

		/* Check if frame is to be dropped. Eg. for PRP no forward

		 * between ports.

 Sequence nr for the master node */

 HSRv0 supervisory frames double as a tag so treat them as tagged. */

 Check if skb contains hsr_ethhdr */

 HSR tagged frame :- Data or Supervision */

 Standard frame or PRP from master port */

 Supervision frame */

 Check if skb contains ethhdr */

 Unknown node and !is_supervision, or no mem */

 FIXME: */

 Must be called holding rcu read lock (because of the port parameter) */

	/* Gets called for ingress frames as well as egress from master port.

	 * So check and increment stats for master port only here.

 SPDX-License-Identifier: GPL-2.0

/* Copyright 2011-2014 Autronica Fire and Security AS

 *

 * Author(s):

 *	2011-2014 Arvid Brodin, arvid.brodin@alten.se

 *

 * The HSR spec says never to forward the same frame twice on the same

 * interface. A frame is identified by its source MAC address and its HSR

 * sequence number. This code keeps track of senders and their sequence numbers

 * to allow filtering of duplicate frames, and to detect HSR ring errors.

 * Same code handles filtering of duplicates for PRP as well.

	TODO: use hash lists for mac addresses (linux/jhash.h)?    */

/* seq_nr_after(a, b) - return true if a is after (higher in sequence than) b,

 * false otherwise.

	/* Remove inconsistency where

	 * seq_nr_after(a, b) == seq_nr_before(a, b)

/* Search for mac entry. Caller must hold rcu read lock.

/* Helper for device init; the self_node_db is used in hsr_rcv() to recognize

 * frames from self that's been looped over the HSR ring.

 Mark if the SAN node is over LAN_A or LAN_B */

/* Allocate an hsr_node and add it to node_db. 'addr' is the node's address_A;

 * seq_out is used to initialize filtering of outgoing duplicate frames

 * originating from the newly added node.

	/* We are only interested in time diffs here, so use current jiffies

	 * as initialization. (0 could trigger an spurious ring error warning).

/* Get the hsr_node from which 'skb' was sent.

	/* Everyone may create a node entry, connected node to a HSR/PRP

	 * device.

		/* Use the existing sequence_nr from the tag as starting point

		 * for filtering duplicate frames.

/* Use the Supervision frame's info about an eventual macaddress_B for merging

 * nodes that has previously had their macaddress_B registered as a separate

 * node.

	/* Here either frame->skb_hsr or frame->skb_prp should be

	 * valid as supervision frame always will have protocol

	 * header info.

 Leave the ethernet header. */

 And leave the HSR tag. */

 And leave the HSR sup tag. */

 get HSR sup payload */

 Merge node_curr (registered on macaddress_B) into node_real */

 No frame received from AddrA of this node yet */

 No mem */

 Node has already been merged */

 Leave the first HSR sup payload. */

 Get second supervision tlv */

 And check if it is a redbox mac TLV */

		/* We could stop here after pushing hsr_sup_payload,

		 * or proceed and allow macaddress_B and for redboxes.

 Sanity check length */

 Leave the second HSR sup tlv. */

 Get redbox mac address. */

 Check if redbox mac and node mac are equal. */

 This is a redbox supervision frame for a VDAN! */

 Push back here */

/* 'skb' is a frame meant for this host, that is to be passed to upper layers.

 *

 * If the frame was sent by a node's B interface, replace the source

 * address with that node's "official" address (macaddress_A) so that upper

 * layers recognize where it came from.

/* 'skb' is a frame meant for another host.

 * 'port' is the outgoing interface

 *

 * Substitute the target (dest) MAC address if necessary, so the it matches the

 * recipient interface MAC address, regardless of whether that is the

 * recipient's A or B interface.

 * This is needed to keep the packets flowing through switches that learn on

 * which "side" the different interfaces are.

	/* Don't register incoming frames without a valid sequence number. This

	 * ensures entries of restarted nodes gets pruned so that they can

	 * re-register and resume communications.

/* 'skb' is a HSR Ethernet frame (with a HSR tag inserted), with a valid

 * ethhdr->h_source address and skb->mac_header set.

 *

 * Return:

 *	 1 if frame can be shown to have been sent recently on this interface,

 *	 0 otherwise, or

 *	 negative error code on error

/* Remove stale sequence_nr records. Called by timer every

 * HSR_LIFE_CHECK_INTERVAL (two seconds or so).

		/* Don't prune own node. Neither time_in[HSR_PT_SLAVE_A]

		 * nor time_in[HSR_PT_SLAVE_B], will ever be updated for

		 * the master port. Thus the master node will be repeatedly

		 * pruned leading to packet loss.

 Shorthand */

 Check for timestamps old enough to risk wrap-around */

		/* Get age of newest frame from node.

		 * At least one time_in is OK here; nodes get pruned long

		 * before both time_ins can get stale

 Warn of ring error only as long as we get frames at all */

 Prune old entries */

 Note that we need to free this entry later: */

 Restart timer */

 Present sequence numbers as if they were incoming on interface */

/*

 * debugfs code for HSR & PRP

 * Copyright (C) 2019 Texas Instruments Incorporated

 *

 * Author(s):

 *	Murali Karicheri <m-karicheri2@ti.com>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License as

 * published by the Free Software Foundation version 2.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether express or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

 hsr_node_table_show - Formats and prints node_table entries */

 skip self node */

/* hsr_debugfs_init - create hsr node_table file for dumping

 * the node table

 *

 * Description:

 * When debugfs is configured this routine sets up the node_table file per

 * hsr device for dumping the node_table entries

/* hsr_debugfs_term - Tear down debugfs intrastructure

 *

 * Description:

 * When Debugfs is configured this routine removes debugfs file system

 * elements that are specific to hsr

 debugfs_remove() internally checks NULL and ERROR */

 SPDX-License-Identifier: GPL-2.0

/* Copyright 2011-2014 Autronica Fire and Security AS

 *

 * Author(s):

 *	2011-2014 Arvid Brodin, arvid.brodin@alten.se

 *

 * Event handling for HSR and PRP devices.

 Not an HSR device */

 Resend of notification concerning removed device? */

 Administrative state DOWN */

 Administrative state UP */

 Link (carrier) state changes */

			/* This should not happen since there's no

			 * ndo_set_mac_address() for HSR devices - i.e. not

			 * supported.

 Make sure we recognize frames from ourselves in hsr_rcv() */

 Handled in ndo_change_mtu() */

		/* HSR works only on Ethernet devices. Refuse slave to change

		 * its type.

 Slave event notifications */

 SPDX-License-Identifier: GPL-2.0

/* Copyright 2011-2014 Autronica Fire and Security AS

 *

 * Author(s):

 *	2011-2014 Arvid Brodin, arvid.brodin@alten.se

 *

 * Routines for handling Netlink messages for HSR and PRP.

/* Here, it seems a netdevice has already been allocated for us, and the

 * hsr_dev_setup routine has been executed. Nice!

 attribute policy */

/* This is called if for some node with MAC address addr, we only get frames

 * over one of the slave interfaces. This would indicate an open network ring

 * (i.e. a link has failed somewhere).

/* This is called when we haven't heard from the node with MAC address addr for

 * some time (just before the node is removed from the node table/list).

/* HSR_C_GET_NODE_STATUS lets userspace query the internal HSR node table

 * about the status of a specific node in the network, defined by its MAC

 * address.

 *

 * Input: hsr ifindex, node mac address

 * Output: hsr ifindex, node mac address (copied from request),

 *	   age of latest frame from node over slave 1, slave 2 [ms]

 For receiving */

 For sending */

 Send reply */

 Fall through */

/* Get a list of MacAddressA of all nodes known to this node (including self).

 Send reply */

 Fall through */

 SPDX-License-Identifier: GPL-2.0

 fork usermode process */

 health check that usermode process started correctly */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright Jonathan Naylor G4KLX (g4klx@g4klx.demon.co.uk)

 * Copyright Darryl Miles G7LED (dlm@g7led.demon.co.uk)

 End of fragment */

/*

 * State machine for state 1, Awaiting Connection State.

 * The handling of the timer(s) is in file nr_timer.c.

 * Handling of state 0 and connection release is in netrom.c.

/*

 * State machine for state 2, Awaiting Release State.

 * The handling of the timer(s) is in file nr_timer.c

 * Handling of state 0 and connection release is in netrom.c.

/*

 * State machine for state 3, Connected State.

 * The handling of the timer(s) is in file nr_timer.c

 * Handling of state 0 and connection release is in netrom.c.

		/*

		 * Window is full, ack it immediately.

 Higher level upcall for a LAPB frame - called with sk locked */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright Tomi Manninen OH2BNS (oh2bns@sral.fi)

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright Jonathan Naylor G4KLX (g4klx@g4klx.demon.co.uk)

/*

 *	This routine purges all of the queues of frames.

/*

 * This routine purges the input queue of those frames that have been

 * acknowledged. This replaces the boxes labelled "V(a) <- N(r)" on the

 * SDL diagram.

	/*

	 * Remove all the ack-ed frames from the ack queue.

/*

 * Requeue all the un-ack-ed frames on the output queue to be picked

 * up by nr_kick called from the timer. This arrangement handles the

 * possibility of an empty output queue.

/*

 *	Validate that the value of nr is between va and vs. Return true or

 *	false for testing.

/*

 *	Check that ns is within the receive window.

/*

 *  This routine is called when the HDLC layer internally generates a

 *  control frame.

	/*

	 *	Space for AX.25 and NET/ROM network header

/*

 * This routine is called to send an error reply.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright Jonathan Naylor G4KLX (g4klx@g4klx.demon.co.uk)

 For the statistics structure. */

/*

 *	Only allow IP over NET/ROM frames through if the netrom device is up.

 Spoof incoming device */

 New-style flags. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright Jonathan Naylor G4KLX (g4klx@g4klx.demon.co.uk)

 * Copyright Darryl Miles G7LED (dlm@g7led.demon.co.uk)

/*

 *	This is where all NET/ROM frames pass, except for IP-over-NET/ROM which

 *	cannot be fragmented in this manner.

 Save a copy of the Transport Header */

 Copy the user data */

 Duplicate the Transport Header */

 Throw it on the queue */

 Throw it on the queue */

/*

 *	This procedure is passed a buffer descriptor for an iframe. It builds

 *	the rest of the control part of the frame and then writes it out.

	/*

	 * Transmit data until either we're out of data to send or

	 * the window is full.

	/*

	 * Dequeue the frame and copy it.

		/*

		 * Transmit the frame copy.

		/*

		 * Requeue the original data frame.

	/*

	 *	Add the protocol byte and network header.

/*

 * The following routines are taken from page 170 of the 7th ARRL Computer

 * Networking Conference paper, as is the whole state machine.

/*

 * Never send a NAK when we are CHOKEd.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright Jonathan Naylor G4KLX (g4klx@g4klx.demon.co.uk)

 * Copyright Alan Cox GW4PTS (alan@lxorguk.ukuu.org.uk)

 * Copyright Darryl Miles G7LED (dlm@g7led.demon.co.uk)

 For TIOCINQ/OUTQ */

/*

 * NETROM network devices are virtual network devices encapsulating NETROM

 * frames into AX.25 which will be sent through an AX.25 device, so form a

 * special "super class" of normal net devices; split their locks off into a

 * separate class since they always nest.

/*

 *	Socket removal during an interrupt is now safe.

/*

 *	Kill all bound sockets on a dropped device.

/*

 *	Handle device status changes.

/*

 *	Add a socket to the bound sockets list.

/*

 *	Find a socket that wants to accept the Connect Request we just

 *	received.

/*

 *	Find a connected NET/ROM socket given my circuit IDs.

/*

 *	Find a connected NET/ROM socket given their circuit IDs.

/*

 *	Find next free circuit ID.

/*

 *	Deferred destroy.

/*

 *	Handler for deferred kills.

/*

 *	This is called from user mode and the timers. Thus it protects itself

 *	against interrupt users but doesn't worry about being called during

 *	work. Once it is removed from the queue no interrupt or bottom half

 *	will touch it and we are (fairly 8-) ) safe.

 Flush the queues */

 A pending connection */

 Queue the unaccepted socket for death */

 Defer: outstanding buffers */

/*

 *	Handling for system calls applied via the various interfaces to a

 *	NET/ROM socket object.

	/*

	 * Only the super user can set an arbitrary user callsign.

 Connect completed during a ERESTARTSYS event */

 No reconnect on a seqpacket socket */

 Must bind first - autobinding in this may or may not work */

 Finish the bind */

 Move to connecting socket, start sending Connect Requests */

 Now the loop */

	/*

	 * A Connect Ack with Choke or timeout or failed routing will go to

	 * closed.

 Always set at this point */

	/*

	 *	The write queue this time is holding sockets ready to use

	 *	hooked into the SABM we saved

 Now attach up the new socket */

	/*

	 *	skb->data points to the netrom frame start

	/*

	 * Check for an incoming IP over NET/ROM frame.

	/*

	 * Find an existing socket connection, based on circuit ID, if it's

	 * a Connect Request base it on their circuit ID.

	 *

	 * Circuit ID 0/0 is not valid but it could still be a "reset" for a

	 * circuit that no longer exists at the other end ...

	/*

	 * Now it should be a CONNREQ.

		/*

		 * Here it would be nice to be able to send a reset but

		 * NET/ROM doesn't have one.  We've tried to extend the protocol

		 * by sending NR_CONNACK | NR_CHOKE_FLAGS replies but that

		 * apparently kills BPQ boxes... :-(

		 * So now we try to follow the established behaviour of

		 * G8PZT's Xrouter which is sending packets with command type 7

		 * as an extension of the protocol.

 Fill in his circuit details */

 Window negotiation */

 L4 timeout negotiation */

	/* Build a packet - the conventional user limit is 236 bytes. We can

	/*

	 *	Push down the NET/ROM header

 Build a NET/ROM Transport header */

 To be filled in later */

      Ditto            */

	/*

	 *	Put the data on the end

 User data follows immediately after the NET/ROM transport header */

 Shove it onto the queue */

	/*

	 * This works for seqpacket too. The receiver has ordered the queue for

	 * us! We do one quick check first though

 Now we can treat all alike */

 These two are safe on a single CPU system as only user tasks fiddle here */

 CONFIG_PROC_FS */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) 1996 Mike Shaver (shaver@zeroknowledge.com)

/*

 *	Values taken from NET/ROM documentation.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) Jonathan Naylor G4KLX (g4klx@g4klx.demon.co.uk)

 * Copyright (C) 2002 Ralf Baechle DO1GRB (ralf@gnu.org)

 initialized by sock_init_data */

		/* Magic here: If we listen() and a new link dies before it

		/*

		 * Check for the state of the receive buffer.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright Jonathan Naylor G4KLX (g4klx@g4klx.demon.co.uk)

 * Copyright Alan Cox GW4PTS (alan@lxorguk.ukuu.org.uk)

 * Copyright Tomi Manninen OH2BNS (oh2bns@sral.fi)

 For TIOCINQ/OUTQ */

      re-sort the routes in quality order.    */

/*

 *	Add a new route to a node, and in the process add the node and the

 *	neighbour if it is new.

 Can't add routes to ourself */

	/*

	 * The L2 link to a neighbour has failed in the past

	 * and now a frame comes from this neighbour. We assume

	 * it was a temporary trouble with the link and reset the

	 * routes now (and not wait for a node broadcast).

 refcount initialized at 1 */

 We have space at the bottom, slot it in */

 It must be better than the worst */

 Now re-sort the routes in quality order */

/*

 *	"Delete" a node. Strictly speaking remove a route to a node. The node

 *	is only deleted if no routes are left to it.

/*

 *	Lock a neighbour with a quality.

 refcount is initialized at 1 */

/*

 *	"Delete" a neighbour. The neighbour is only removed if the number

 *	of nodes that may use it is zero.

/*

 *	Decrement the obsolescence count by one. If a route is reduced to a

 *	count of zero, remove it. Also remove any unlocked neighbours with

 *	zero nodes routing via it.

 A locked entry */

 From 1 -> 0 */

/*

 *	A device has been removed. Remove its routes and neighbours.

/*

 *	Check that the device given is a valid AX.25 interface that is "up".

 *	Or a valid ethernet interface with an AX.25 callsign binding.

/*

 *	Find the first active NET/ROM device, usually "nr0".

/*

 *	Find the NET/ROM device for the given callsign.

/*

 *	Handle the ioctls that control the routing functions.

/*

 * 	A level 2 link has timed out, therefore it appears to be a poor link,

 *	then don't use that neighbour until it is reset.

/*

 *	Route a frame to an appropriate AX.25 connection. A NULL ax25_cb

 *	indicates an internally generated frame.

 Its for me */

 Its from me */

 Its Time-To-Live has expired */

	/* We are going to change the netrom headers so we should get our

	   own skb, we also did not know until now how much header space

/*

 *	Free all memory associated with the nodes and routes lists.

 SPDX-License-Identifier: GPL-2.0-only

 max memory we will use for mpls_route */

/* Maximum number of labels to look ahead at when selecting a path of

 * a multipath route

 The size of the layer 2.5 labels to be added for this route */

 The amount of data the layer 2 frame can hold */

 Read and decode the current label */

		/* RFC6790 - reserved labels MUST NOT be used as keys

		 * for the load-balancing function

			/* The entropy label follows the entropy label

			 * indicator, so this means that the entropy

			 * label was just added to the hash - no need to

			 * go any deeper either in the label stack or in the

			 * payload

 found bottom label; does skb have room for a header? */

/* number of alive nexthops (rt->rt_nhn_alive) and the flags for

 * a next hop (nh->nh_flags) are modified by netdev event handlers.

 * Since those fields can change at any moment, use READ_ONCE to

 * access both.

	/* No need to look further into packet if there's only

	 * one path

	/* The IPv4 code below accesses through the IPv4 header

	 * checksum, which is 12 bytes into the packet.

	 * The IPv6 code below accesses through the IPv6 hop limit

	 * which is 8 bytes into the packet.

	 *

	 * For all supported cases there should always be at least 12

	 * bytes of packet data present.  The IPv4 header is 20 bytes

	 * without options and the IPv6 header is always 40 bytes

	 * long.

		/* If propagating TTL, take the decremented TTL from

		 * the incoming MPLS header, otherwise decrement the

		 * TTL, but only if not 0 to avoid underflow.

		/* If propagating TTL, take the decremented TTL from

		 * the incoming MPLS header, otherwise decrement the

		 * hop limit, but only if not 0 to avoid underflow.

 Should have decided which protocol it is by now */

 Careful this entire function runs inside of an rcu critical section */

 Read and decode the label */

 Pop the label */

 Verify ttl is valid */

 Find the output device */

 Verify the destination can hold the packet */

 Ensure there is enough space for the headers in the skb */

 Penultimate hop popping */

 Push the new labels */

 If via wasn't specified then send out using device address */

/* all nexthops within a route have the same size based on max

 * number of labels and max via length for a hop

 Ignore reserved labels for now */

 If we removed a route free it now */

 The caller is holding rtnl anyways, so release the dev reference */

 Ensure this is a supported device */

 Validate the address family */

 Unsupported address family */

		/* number of nexthops is tracked by a u8.

		 * Check for overflow.

 leftover implies invalid nexthop configuration, discard it */

		/* neither weighted multipath nor any flags

		 * are supported

 Reserved labels may not be set */

 The full 20 bit range may not be supported. */

 If a label was not specified during insert pick one */

 Append makes no sense with mpls */

 NETCONFA_IFINDEX */

 -EMSGSIZE implies BUG in mpls_netconf_msgsize_devconf() */

 -EMSGSIZE implies BUG in mpls_netconf_msgsize_devconf() */

	/* Table data contains only offsets relative to the base of

	 * the mdev at this point, so make them absolute.

 if there are no more nexthops, delete the route */

	/* len needs to be an even multiple of 4 (the label size). Number

	 * of labels is a u8 so check for overflow.

 Limit the number of new labels allowed */

 when label == NULL, caller wants number of labels */

		/* Ensure the bottom of stack flag is properly set

		 * and ttl and tc are both clear.

			/* RFC3032: This is a label that an LSR may

			 * assign and distribute, but which never

			 * actually appears in the encapsulation.

 Any value is acceptable for rtm_protocol */

	/* As mpls uses destination specific addresses

	 * (or source specific address in the case of multicast)

	 * all addresses have universal scope.

 Unsupported attribute */

 length of rtnetlink header + attributes */

		/* for MPLS, there is only 1 table with fixed type and flags.

		 * If either are set in the filter then return nothing.

 RTA_DST */

 RTA_TTL_PROPAGATE */

 RTA_OIF */

 RTA_VIA */

 RTA_NEWDST */

 each nexthop is packed in an attribute */

 RTA_VIA */

 nested attribute */

 -EMSGSIZE implies BUG in lfib_nlmsg_size */

 -EMSGSIZE implies BUG in lfib_nlmsg_size */

 Push new labels */

 In case the predefined labels need to be populated */

 Remember the original table */

 Free any labels beyond the new table */

 Copy over the old labels */

 If needed set the predefined labels */

 Update the global pointers */

	/* Table data contains only offsets relative to the base of

	 * the mdev at this point, so make them absolute.

	/* An rcu grace period has passed since there was a device in

	 * the network namespace (and thus the last in flight packet)

	 * left this network namespace.  This is because

	 * unregister_netdevice_many and netdev_run_todo has completed

	 * for each network device that was in this network namespace.

	 *

	 * As such no additional rcu synchronization is necessary when

	 * freeing the platform_label table.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	MPLS GSO Support

 *

 *	Authors: Simon Horman (horms@verge.net.au)

 *

 *	Based on: GSO portions of net/ipv4/gre.c

 Setup inner SKB. */

 Segment inner packet. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * mpls tunnels	An implementation mpls tunnels using the light weight tunnel

 *		infrastructure

 *

 * Authors:	Roopa Prabhu, <roopa@cumulusnetworks.com>

 The size of the layer 2.5 labels to be added for this route */

 Find the output device */

	/* Obtain the ttl using the following set of rules.

	 *

	 * LWT ttl propagation setting:

	 *  - disabled => use default TTL value from LWT

	 *  - enabled  => use TTL value from IPv4/IPv6 header

	 *  - default  =>

	 *   Global ttl propagation setting:

	 *    - disabled => use default TTL value from global setting

	 *    - enabled => use TTL value from IPv4/IPv6 header

 Verify the destination can hold the packet */

 Ensure there is enough space for the headers in the skb */

 Push the new labels */

 6PE (RFC 4798) */

 determine number of labels */

 TTL 0 implies propagate from IP header */

/*

   BlueZ - Bluetooth protocol stack for Linux

   Copyright (C) 2000-2001 Qualcomm Incorporated



   Written 2000,2001 by Maxim Krasnyansky <maxk@qualcomm.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Bluetooth kernel library. */

 Bluetooth error codes to Unix errno mapping */

 SPDX-License-Identifier: GPL-2.0

 Bluetooth HCI driver model support. */

/*

 * The rfcomm tty device will possibly retain even when conn

 * is down, and sysfs doesn't support move zombie device,

 * so we should move the device before conn device is destroyed.

/*

   BlueZ - Bluetooth protocol stack for Linux

   Copyright (c) 2000-2001, 2010, Code Aurora Forum. All rights reserved.



   Written 2000,2001 by Maxim Krasnyansky <maxk@qualcomm.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Bluetooth HCI event handling. */

 Handle HCI Event packets */

	/* It is possible that we receive Inquiry Complete event right

	 * before we receive Inquiry Cancel Command Complete event, in

	 * which case the latter event should have status of Command

	 * Disallowed (0x0c). This should not be treated as error, since

	 * we actually achieve what Inquiry Cancel wants to achieve,

	 * which is to end the last Inquiry session.

 wake_up_bit advises about this barrier */

	/* Set discovery state to stopped if we're not doing LE active

	 * scanning.

 Reset all non-persistent flags */

	/* Adjust default settings according to features

	/* Update only in case the adv instance since handle 0x00 shall be using

	 * HCI_OP_LE_SET_RANDOM_ADDR since that allows both extended and

	 * non-extended adverting.

	/* If we're doing connection initiation as peripheral. Set a

	 * timeout in case something goes wrong.

			/* If just one instance was disabled check if there are

			 * any other instance enabled before clearing HCI_LE_ADV

 All instances shall be considered disabled */

		/* We do this here instead of when setting DISCOVERY_STOPPED

		 * since the latter would potentially require waiting for

		 * inquiry to stop too.

		/* Cancel this timer so that we don't try to disable scanning

		 * when it's already disabled.

		/* The HCI_LE_SCAN_INTERRUPTED flag indicates that we

		 * interrupted scanning due to a connect request. Mark

		 * therefore discovery as stopped. If this was not

		 * because of a connect request advertising might have

		 * been disabled because of active scanning, so

		 * re-enable it again if necessary.

 Store in hdev for instance 0 */

 Update adv data as tx power is known now */

	/* Only request authentication for SSP connections or non-SSP

	 * devices with sec_level MEDIUM or HIGH or if MITM protection

	 * is requested.

	/* Update the mgmt connected state if necessary. Be careful with

	 * conn objects that exist but are not (yet) connected however.

	 * Only those in BT_CONFIG or BT_CONNECTED states can be

	 * considered connected.

	/* If the device was not found in a list of found devices names of which

	 * are pending. there is no need to continue resolving a next name as it

	 * will be done upon receiving another Remote Name Request Complete

	/* If successful wait for the name req complete event before

		/* If the disconnection failed for any reason, the upper layer

		 * does not retry to disconnect in current implementation.

		 * Hence, we need to do some basic cleanup here and re-enable

		 * advertising if necessary.

	/* When using controller based address resolution, then the new

	 * address types 0x02 and 0x03 are used. These types need to be

	 * converted back into either public address or random address type

	/* Store the initiator and responder address information which

	 * is needed for SMP. These values will not change during the

	 * lifetime of the connection.

	/* We don't want the connection attempt to stick around

	 * indefinitely since LE doesn't have a page timeout concept

	 * like BR/EDR. Set a timer for any connection that doesn't use

	 * the accept list for connecting.

	/* All connection failure handling is taken care of by the

	 * hci_le_conn_failed function which is triggered by the HCI

	 * request completion callbacks used for connecting.

	/* All connection failure handling is taken care of by the

	 * hci_le_conn_failed function which is triggered by the HCI

	 * request completion callbacks used for connecting.

 wake_up_bit advises about this barrier */

		/* When BR/EDR inquiry is active and no LE scanning is in

		 * progress, then change discovery state to indicate completion.

		 *

		 * When running LE scanning and BR/EDR inquiry simultaneously

		 * and the LE scan already finished, then change the discovery

		 * state to indicate completion.

		/* When BR/EDR inquiry is active and no LE scanning is in

		 * progress, then change discovery state to indicate completion.

		 *

		 * When running LE scanning and BR/EDR inquiry simultaneously

		 * and the LE scan already finished, then change the discovery

		 * state to indicate completion.

		/* Connection may not exist if auto-connected. Check the bredr

		 * allowlist to see if this device is allowed to auto connect.

		 * If link is an ACL type, create a connection class

		 * automatically.

		 *

		 * Auto-connect will only occur if the event filter is

		 * programmed with a given address. Right now, event filter is

		 * only used during suspend.

 Get remote features */

 Set packet type for incoming connection */

	/* Require HCI_CONNECTABLE or an accept list entry to accept the

	 * connection. These features are only touched through mgmt so

	 * only do the checks if HCI_MGMT is set.

 Connection accepted */

 Become central */

 Remain peripheral */

	/* The suspend notifier is waiting for all devices to disconnect so

	 * clear the bit from pending tasks and inform the wait queue.

	/* Re-enable advertising if necessary, since it might

	 * have been disabled by the connection. From the

	 * HCI_LE_Set_Advertise_Enable command description in

	 * the core specification (v4.0):

	 * "The Controller shall continue advertising until the Host

	 * issues an LE_Set_Advertise_Enable command with

	 * Advertising_Enable set to 0x00 (Advertising is disabled)

	 * or until a connection is created or until the Advertising

	 * is timed out due to Directed Advertising."

	/* While unexpected, the read_enc_key_size command may fail. The most

	 * secure approach is to then assume the key size is 0 to force a

	 * disconnection.

 Encryption implies authentication */

 P-256 authentication key implies FIPS */

	/* We should disregard the current RPA and generate a new one

	 * whenever the encryption procedure fails.

 Check link security requirements are met */

		/* Notify upper layers so they can cleanup before

		 * disconnecting.

 Try reading the encryption key size for encrypted ACL links */

		/* Only send HCI_Read_Encryption_Key_Size if the

		 * controller really supports it. If it doesn't, assume

		 * the default size (16).

	/* Set the default Authenticated Payload Timeout after

	 * an LE Link is established. As per Core Spec v5.0, Vol 2, Part B

	 * Section 3.3, the HCI command WRITE_AUTH_PAYLOAD_TIMEOUT should be

	 * sent when the link is active and Encryption is enabled, the conn

	 * type can be either LE or ACL and controller must support LMP Ping.

	 * Ensure for AES-CCM encryption as well.

	/* Indicate request completion if the command failed. Also, if

	 * we're not waiting for a special event and we get a success

	 * command status we should try to flag the request as completed

	 * (since for this kind of commands there will not be a command

	 * complete event).

	/* Update connection information since adding the key will have

	 * fixed up the type in the case of changed combination keys.

	/* Keep debug keys around only if the HCI_KEEP_DEBUG_KEYS flag

	 * is set. If it's not set simply remove the key from the kernel

	 * list (we've still notified user space about it but with

	 * store_hint being 0).

			/* It is mandatory by the Bluetooth specification that

			 * Extended Inquiry Results are only used when Secure

			 * Simple Pairing is enabled, but some devices violate

			 * this.

			 *

			 * To make these devices work, the internal SSP

			 * enabled flag needs to be cleared if the remote host

		/* When the link type in the event indicates SCO connection

		 * and lookup of the connection object fails, then check

		 * if an eSCO connection object exists.

		 *

		 * The core limits the synchronous connections to either

		 * SCO or eSCO. The eSCO connection is preferred and tried

		 * to be setup first and until successfully established,

		 * the link type will be hinted as eSCO.

		/* The synchronous connection complete event should only be

		 * sent once per new connection. Receiving a successful

		 * complete event when the connection status is already

		 * BT_CONNECTED means that the device is misbehaving and sent

		 * multiple complete event packets for the same new connection.

		 *

		 * Registering the device more than once can corrupt kernel

		 * memory, hence upon detecting this invalid event, we report

		 * an error and ignore the packet.

 Connection Accept Timeout */

 Connection Rejected due to Limited Resources */

 Unsupported Feature or Parameter Value */

 SCO interval rejected */

 Unsupported Remote Feature */

 Invalid LMP Parameters */

 Unspecified error */

 Unsupported LMP Parameter value */

	/* Notify only in case of SCO over HCI transport data path which

	 * is zero and non-zero value shall be non-HCI transport data path

	/* For BR/EDR the necessary steps are taken through the

	 * auth_complete event.

 If remote requests no-bonding follow that lead */

	/* If both remote and local have enough IO capabilities, require

	 * MITM protection

 No MITM protection possible so ignore remote requirement */

		/* When Secure Connections is enabled, then just

		 * return the present value stored with the OOB

		 * data. The stored value contains the right present

		 * information. However it can only be trusted when

		 * not in Secure Connection Only mode.

		/* When Secure Connections Only mode is enabled, then

		 * the P-256 values are required. If they are not

		 * available, then do not declare that OOB data is

		 * present.

	/* When Secure Connections is not enabled or actually

	 * not supported by the hardware, then check that if

	 * P-192 data values are present.

	/* Allow pairing if we're pairable, the initiators of the

	 * pairing or if the remote is not requesting bonding.

		/* Change the IO capability from KeyboardDisplay

 If we are initiators, there is no remote information yet */

			/* Request MITM protection if our IO caps allow it

			 * except for the no-bonding case.

		/* If we're not bondable, force one of the non-bondable

		 * authentication requirement values.

	/* If we require MITM but the remote device can't provide that

	 * (it has NoInputNoOutput) then reject the confirmation

	 * request. We check the security level here since it doesn't

	 * necessarily match conn->auth_type.

 If no side requires MITM protection; auto-accept */

		/* If we're not the initiators request authorization to

		 * proceed from user space (mgmt_user_confirm with

		 * confirm_hint set to 1). The exception is if neither

		 * side had MITM or if the local IO capability is

		 * NoInputNoOutput, in which case we do auto-accept

		/* If there already exists link key in local host, leave the

		 * decision to user space since the remote device could be

		 * legitimate or malicious.

 Reset the authentication requirement to unknown */

	/* To avoid duplicate auth_failed events to user space we check

	 * the HCI_CONN_AUTH_PEND flag which will be set if we

	 * initiated the authentication. A traditional auth_complete

	 * event gets always produced as initiator and is also mapped to

 Create AMP hchan */

		/* Check if the controller has set a Local RPA then it must be

		 * used instead or hdev->rpa.

		/* Check if the controller has set a Local RPA then it must be

		 * used instead or hdev->rpa.

			/* In case of ext adv, resp_addr will be updated in

			 * Adv Terminated event.

		/* For incoming connections, set the default minimum

		 * and maximum connection interval. They will be used

		 * to check if the parameters are in range and if not

		 * trigger the connection update procedure.

	/* All controllers implicitly stop advertising in the event of a

	 * connection, so ensure that the state bit is cleared.

		/* If we didn't have a hci_conn object previously

		 * but we're in central role this must be something

		 * initiated using an accept list. Since accept list based

		 * connections are not "first class citizens" we don't

		 * have full tracking of them. Therefore, we go ahead

		 * with a "best effort" approach of determining the

		 * initiator address based on the HCI_PRIVACY flag.

	/* Lookup the identity address from the stored connection

	 * address and address type.

	 *

	 * When establishing connections to an identity address, the

	 * connection procedure will store the resolvable random

	 * address first. Now if it can be converted back into the

	 * identity address, start using the identity address from

	 * now on.

 Drop the connection if the device is blocked */

	/* Store current advertising instance as connection advertising instance

	 * when sotfware rotation is in use so it can be re-enabled when

	 * disconnected.

	/* The remote features procedure is defined for central

	 * role only. So only in case of an initiated connection

	 * request the remote features.

	 *

	 * If the local controller supports peripheral-initiated features

	 * exchange, then requesting the remote features in peripheral

	 * role is possible. Otherwise just transition into the

	 * connected state without requesting the remote features.

 Remove advertising as it has been terminated */

		/* Store handle in the connection so the correct advertising

		 * instance can be re-enabled when disconnected.

 This function requires the caller holds hdev->lock */

 If the event is not connectable don't proceed further */

 Ignore if the device is blocked */

	/* Most controller will fail if we try to create new connections

	 * while we have an existing one in peripheral role.

	/* If we're not connectable only connect devices that we have in

	 * our pend_le_conns list.

			/* Only devices advertising with ADV_DIRECT_IND are

			 * triggering a connection attempt. This is allowing

			 * incoming connections from peripheral devices.

			/* Devices advertising with ADV_IND or ADV_DIRECT_IND

			 * are triggering a connection attempt. This means

			 * that incoming connections from peripheral device are

			 * accepted and also outgoing connections to peripheral

			 * devices are established when found.

		/* If HCI_AUTO_CONN_EXPLICIT is set, conn is already owned

		 * by higher layer that tried to connect, if no then

		 * store the pointer since we don't really have any

		 * other owner of the object besides the params that

		 * triggered it. This way we can abort the connection if

		 * the parameters get removed and keep the reference

		 * count consistent once the connection is established.

		/* If hci_connect() returns -EBUSY it means there is already

		 * an LE connection attempt going on. Since controllers don't

		 * support more than one connection attempt at the time, we

		 * don't consider this an error case.

	/* Find the end of the data in case the report contains padded zero

	 * bytes at the end causing an invalid length value.

	 *

	 * When data is NULL, len is 0 so there is no need for extra ptr

	 * check as 'ptr < data + 0' is already false in such case.

	/* Adjust for actual length. This handles the case when remote

	 * device is advertising with incorrect data length.

	/* If the direct address is present, then this report is from

	 * a LE Direct Advertising Report event. In that case it is

	 * important to see if the address is matching the local

	 * controller address.

		/* Only resolvable random addresses are valid for these

		 * kind of reports and others can be ignored.

		/* If the controller is not using resolvable random

		 * addresses, then this report can be ignored.

		/* If the local IRK of the controller does not match

		 * with the resolvable random address provided, then

		 * this report can be ignored.

 Check if we need to convert to identity address */

	/* Check if we have been requested to connect to this device.

	 *

	 * direct_addr is set only for directed advertising reports (it is NULL

	 * for advertising reports) and is already verified to be RPA above.

		/* Store report for later inclusion by

		 * mgmt_device_connected

	/* Passive scanning shouldn't trigger any device found events,

	 * except for devices marked as CONN_REPORT for which we do send

	 * device found events, or advertisement monitoring requested.

	/* When receiving non-connectable or scannable undirected

	 * advertising reports, this means that the remote device is

	 * not connectable and then clearly indicate this in the

	 * device found event.

	 *

	 * When receiving a scan response, then there is no way to

	 * know if the remote device is connectable or not. However

	 * since scan responses are merged with a previously seen

	 * advertising report, the flags field from that report

	 * will be used.

	 *

	 * In the really unlikely case that a controller get confused

	 * and just sends a scan response event, then it is marked as

	 * not connectable as well.

	/* If there's nothing pending either store the data from this

	 * event or send an immediate device found event if the data

	 * should not be stored for later.

		/* If the report will trigger a SCAN_REQ store it for

		 * later merging.

 Check if the pending report is for the same device as the new one */

	/* If the pending data doesn't match this report or this isn't a

	 * scan response (e.g. we got a duplicate ADV_IND) then force

	 * sending of the pending data.

 Send out whatever is in the cache, but skip duplicates */

		/* If the new report will trigger a SCAN_REQ store it for

		 * later merging.

		/* The advertising reports cannot be merged, so clear

		 * the pending report and send out a device found event.

	/* If we get here we've got a pending ADV_IND or ADV_SCAN_IND and

	 * the new event is a SCAN_RSP. We can therefore proceed with

	 * sending a merged device found event.

			/* If the local controller supports peripheral-initiated

			 * features exchange, but the remote controller does

			 * not, then it is possible that the error code 0x1a

			 * for unsupported remote feature gets returned.

			 *

			 * In this specific case, allow the connection to

			 * transition into connected state and mark it as

			 * successful.

 With SC both EDiv and Rand are set to zero */

 For non-SC keys check that EDiv and Rand match */

	/* Ref. Bluetooth Core SPEC pages 1975 and 2004. STK is a

	 * temporary key used to encrypt a connection following

	 * pairing. It is used during the Encrypted Session Setup to

	 * distribute the keys. Later, security can be re-established

	 * using a distributed LTK.

	/* Check if request ended in Command Status - no way to retrieve

	 * any extra parameters in this case.

	/* If we are currently suspended and this is the first BT event seen,

	 * save the wake reason associated with the event.

	/* Default to remote wake. Values for wake_reason are documented in the

	 * Bluez mgmt api docs.

	/* Once configured for remote wakeup, we should only wake up for

	 * reconnections. It's useful to see which device is waking us up so

	 * keep track of the bdaddr of the connection event that woke us up.

	/* If it looks like we might end up having to call

	 * req_complete_skb, store a pristine copy of the skb since the

	 * various handlers may modify the original one through

	 * skb_pull() calls, etc.

 Store wake reason if we're suspended */

/*

   BlueZ - Bluetooth protocol stack for Linux



   Copyright (C) 2014 Intel Corporation



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

	/* If an error occurred during request building, remove all HCI

	 * commands queued on the HCI request queue.

 Do not allow empty requests */

 Execute request and wait for completion. */

		/* ENODATA means the HCI request command queue is empty.

		 * This can happen when a request with conditionals doesn't

		 * trigger any commands to be sent. This is normal behavior

		 * and should not trigger an error return.

 Serialize all requests */

	/* check the state after obtaing the lock to protect the HCI_UP

	 * against any races from hci_dev_do_close when the controller

	 * gets removed.

 Queue a command to an asynchronous HCI request */

	/* If an error occurred during request building, there is no point in

	 * queueing the HCI command. We can simply return.

 160 msec page scan interval */

/* Return true if interleave_scan wasn't started until exiting this function,

 * otherwise, return false

	/* Do interleaved scan only if all of the following are true:

	 * - There is at least one ADV monitor

	 * - At least one pending LE connection or one device to be scanned for

	 * - Monitor offloading is not supported

	 * If so, we should alternate between allowlist scan and one without

	 * any filters to save power.

/* This function controls the background scanning based on hdev->pend_le_conns

 * list. If there are pending LE connection we start the background scanning,

 * otherwise we stop it.

 *

 * This function requires the caller holds hdev->lock.

 No point in doing scanning if LE support hasn't been enabled */

 If discovery is active don't interfere with it */

	/* Reset RSSI and UUID filters when starting background scanning

	 * since these filters are meant for service discovery only.

	 *

	 * The Start Discovery and Start Service Discovery operations

	 * ensure to set proper values for RSSI threshold and UUID

	 * filter list. So it is safe to just reset them here.

		/* If there is no pending LE connections or devices

		 * to be scanned for or no ADV monitors, we should stop the

		 * background scanning.

 If controller is not scanning we are done. */

		/* If there is at least one pending LE connection, we should

		 * keep the background scan running.

		/* If controller is connecting, we should not start scanning

		 * since some controllers are not able to scan and connect at

		 * the same time.

		/* If controller is currently scanning, we stop it to ensure we

		 * don't miss any advertising (due to duplicates filter).

 Disable address resolution */

 Adds connection to accept list if needed. On error, returns -1. */

 Already in accept list */

 Select filter policy to accept all advertising */

 Accept list can not be used with RPAs */

 During suspend, only wakeable devices can be in accept list */

	/* We allow usage of accept list even with RPAs in suspend. In the worst

	 * case, we won't be able to wake from devices that use the privacy1.2

	 * features. Additionally, once we support privacy1.2 and IRK

	 * offloading, we can update this to also check for those conditions.

	/* Go through the current accept list programmed into the

	 * controller one by one and check if that address is still

	 * in the list of pending connections or list of devices to

	 * report. If not present in either list, then queue the

	 * command to remove it from the controller.

		/* If the device is not likely to connect or report,

		 * remove it from the accept list.

 Accept list can not be used with RPAs */

	/* Since all no longer valid accept list entries have been

	 * removed, walk through the list of pending connections

	 * and ensure that any new device gets programmed into

	 * the controller.

	 *

	 * If the list of the devices is larger than the list of

	 * available accept list entries in the controller, then

	 * just abort and return filer policy value to not use the

	 * accept list.

	/* After adding all new pending connections, walk through

	 * the list of pending reports and also add these to the

	 * accept list if there is still space. Abort if space runs out.

	/* Use the allowlist unless the following conditions are all true:

	 * - We are not currently suspending

	 * - There are 1 or more ADV monitors registered and it's not offloaded

	 * - Interleaved scanning is not currently using the allowlist

 Select filter policy to use accept list */

	/* Use ext scanning if set ext scan param and ext scan enable is

	 * supported

 Returns true if an le connection is in the scanning state */

/* Ensure to call hci_req_add_le_scan_disable() first to disable the

 * controller based address resolution to be able to reconfigure

 * resolving list.

 Default is to enable duplicates filter */

 Background scanning should run with address resolution */

	/* Set require_privacy to false since no SCAN_REQ are send

	 * during passive scanning. Not using an non-resolvable address

	 * here is important so that peer devices using direct

	 * advertising with our address will be correctly reported

	 * by the controller.

	/* Adding or removing entries from the accept list must

	 * happen before enabling scanning. The controller does

	 * not allow accept list modification while scanning.

	/* When the controller is using random resolvable addresses and

	 * with that having LE privacy enabled, then controllers with

	 * Extended Scanner Filter Policies support can now enable support

	 * for handling directed advertising.

	 *

	 * So instead of using filter polices 0x00 (no accept list)

	 * and 0x01 (accept list enabled) use the new filter policies

	 * 0x02 (no accept list) and 0x03 (accept list enabled).

		/* Disable duplicates filter when scanning for advertisement

		 * monitor for the following reasons.

		 *

		 * For HW pattern filtering (ex. MSFT), Realtek and Qualcomm

		 * controllers ignore RSSI_Sampling_Period when the duplicates

		 * filter is enabled.

		 *

		 * For SW pattern filtering, when we're not doing interleaved

		 * scanning, it is necessary to disable duplicates filter,

		 * otherwise hosts can only receive one advertisement and it's

		 * impossible to know if a peer is still in range.

 Always clear event filter when starting */

 This function requires the caller holds hdev->lock */

	/* Call to disable any advertisements active on the controller.

	 * This will succeed even if no advertisements are configured.

 If we are using software rotation, pause the loop */

 This function requires the caller holds hdev->lock */

 Call for each tracked instance to be re-enabled */

		/* Schedule for most recent instance to be restarted and begin

		 * the software rotation loop

 This function requires the caller holds hdev->lock */

 No need to block when enabling since it's on resume path */

 Call with hci_dev_lock */

 Mark device as suspended */

 Pause discovery if not already stopped */

 Stop directed advertising */

 Pause other advertisements */

 Disable page scan if enabled */

 Disable LE passive scan if enabled */

 Disable advertisement filters */

 Prevent disconnects from causing scanning to be re-enabled */

 Run commands before disconnecting */

 Soft disconnect everything (power off) */

 Unpause to take care of updating scanning params */

 Enable event filter for paired devices */

 Enable passive scan at lower duty cycle */

 Pause scan changes again. */

 Clear any event filters and restore scan state */

 Reset passive/background scanning to normal */

 Enable all of the advertisement filters */

 Unpause directed advertising */

 Resume other advertisements */

 Unpause discovery */

 If privacy is not enabled don't use RPA */

 If basic privacy mode is enabled use RPA */

	/* If limited privacy mode is enabled don't use RPA if we're

	 * both discoverable and bondable.

	/* We're neither bondable nor discoverable in the limited

	 * privacy mode, therefore use RPA.

 If there is no connection we are OK to advertise. */

 Check le_states if there is any connection in peripheral role. */

		/* Peripheral connection state and non connectable mode bit 20.

		/* Peripheral connection state and connectable mode bit 38

		 * and scannable bit 21.

 Check le_states if there is any connection in central role. */

 Central connection state and non connectable mode bit 18. */

		/* Central connection state and connectable mode bit 35 and

		 * scannable 19.

	/* If the "connectable" instance flag was not set, then choose between

	 * ADV_IND and ADV_NONCONN_IND based on the global connectable setting.

	/* Clear the HCI_LE_ADV bit temporarily so that the

	 * hci_update_random_address knows that it's safe to go ahead

	 * and write a new random address. The flag will be set back on

	 * as soon as the SET_ADV_ENABLE HCI command completes.

	/* Set require_privacy to true only when non-connectable

	 * advertising is used. In that case it is fine to use a

	 * non-resolvable private address.

 There's nothing to do if the data hasn't changed */

 There's nothing to do if the data hasn't changed */

 Don't continue interleaving if it was canceled */

	/* If privacy is enabled use a resolvable private address. If

	 * current RPA has expired then generate a new one.

		/* If Controller supports LL Privacy use own address type is

		 * 0x03

	/* In case of required privacy without resolvable private address,

	 * use an non-resolvable private address. This is useful for

	 * non-connectable advertising.

			/* The non-resolvable private address is generated

			 * from random six bytes with the two most significant

			 * bits cleared.

			/* The non-resolvable private address shall not be

			 * equal to the public address.

 No privacy so use a public address. */

	/* If we're advertising or initiating an LE connection we can't

	 * go ahead and change the random address at this time. This is

	 * because the eventual initiator address used for the

	 * subsequently created connection will be undefined (some

	 * controllers use the new address and others the one we had

	 * when the operation started).

	 *

	 * In this kind of scenario skip the update and let the random

	 * address be updated at the next cycle.

	/* If the "connectable" instance flag was not set, then choose between

	 * ADV_IND and ADV_NONCONN_IND based on the global connectable setting.

	/* Set require_privacy to true only when non-connectable

	 * advertising is used. In that case it is fine to use a

	 * non-resolvable private address.

 In all other cases use 1M */

 Check if random address need to be updated */

			/* Instance 0x00 doesn't have an adv_info, instead it

			 * uses hdev->random_addr to track its address so

			 * whenever it needs to be updated this also set the

			 * random address since hdev->random_addr is shared with

			 * scan state machine.

	/* Set duration per instance since controller is responsible for

	 * scheduling it.

 Time = N * 10 ms */

 If request specifies an instance that doesn't exist, fail */

 Instance 0x00 indicates all advertising instances will be disabled */

 If request specifies an instance that doesn't exist, fail */

	/* If instance isn't pending, the chip knows about it, and it's safe to

	 * disable

	/* A zero timeout means unlimited advertising. As long as there is

	 * only one instance, duration should be ignored. We still set a timeout

	 * in case further instances are being added later on.

	 *

	 * If the remaining lifetime of the instance is more than the duration

	 * then the timeout corresponds to the duration, otherwise it will be

	 * reduced to the remaining instance lifetime.

	/* The remaining time is being reduced unless the instance is being

	 * advertised without time limit.

 Only use work for scheduling instances with legacy advertising */

	/* If we're just re-scheduling the same instance again then do not

	 * execute any HCI commands. This happens when a single instance is

	 * being advertised.

/* For a single instance:

 * - force == true: The instance will be removed even when its remaining

 *   lifetime is not zero.

 * - force == false: the instance will be deactivated but kept stored unless

 *   the remaining lifetime is zero.

 *

 * For instance == 0x00:

 * - force == true: All instances will be removed regardless of their timeout

 *   setting.

 * - force == false: Only instances that have a timeout will be removed.

 Cancel any timeout concerning the removed instance(s). */

	/* Get the next instance to advertise BEFORE we remove

	 * the current one. This can be the same instance again

	 * if there is only one instance.

 Don't advertise a removed instance. */

	/* If privacy is enabled use a resolvable private address. If

	 * current RPA has expired or there is something else than

	 * the current RPA in use, then generate a new one.

		/* If Controller supports LL Privacy use own address type is

		 * 0x03

	/* In case of required privacy without resolvable private address,

	 * use an non-resolvable private address. This is useful for active

	 * scanning and non-connectable advertising.

			/* The non-resolvable private address is generated

			 * from random six bytes with the two most significant

			 * bits cleared.

			/* The non-resolvable private address shall not be

			 * equal to the public address.

	/* If forcing static address is in use or there is no public

	 * address use the static address as random address (but skip

	 * the HCI command if the current random address is already the

	 * static one.

	 *

	 * In case BR/EDR has been disabled on a dual-mode controller

	 * and a static address has been configured, then use that

	 * address instead of the public BR/EDR address.

	/* Neither privacy nor static address is being used so use a

	 * public address.

	/* If BR/EDR is not enabled and we disable advertising as a

	 * by-product of disabling connectable, we need to update the

	 * advertising flags.

 Update the advertising parameters if necessary */

 Limited discoverable mode */

 LIAC */

 GIAC */

 General discoverable mode */

 GIAC */

	/* Advertising instances don't use the global discoverable setting, so

	 * only update AD if advertising was enabled using Set Advertising.

		/* Discoverable mode affects the local advertising

		 * address in limited privacy mode.

			/* SCO rejection has its own limited set of

			 * allowed error values (0x0D-0x0F) which isn't

			 * compatible with most values passed to this

			 * function. To be safe hard-code one of the

			 * values that's suitable for SCO.

	/* If we were running LE only scan, change discovery state. If

	 * we were running both LE and BR/EDR inquiry simultaneously,

	 * and BR/EDR inquiry is already finished, stop discovery,

	 * otherwise BR/EDR inquiry will stop discovery when finished.

	 * If we will resolve remote device name, do not change

	 * discovery state.

 If controller is not scanning we are done. */

	/* When the scan was started, hdev->le_scan_disable has been queued

	 * after duration from scan_start. During scan restart this job

	 * has been canceled, and we need to queue it again after proper

	 * timeout, to make sure that scan does not run indefinitely.

 Accept list is not used for discovery */

 Default is to enable duplicates filter */

 Discovery doesn't require controller address resolution */

	/* If controller is scanning, it means the background scanning is

	 * running. Thus, we should temporarily stop it in order to set the

	 * discovery scanning parameters.

	/* All active scans will be done with either a resolvable private

	 * address (when privacy feature has been enabled) or non-resolvable

	 * private address.

		/* Duplicate filter should be disabled when some advertisement

		 * monitor is activated, otherwise AdvMon can only receive one

		 * advertisement for one peer(*) during active scanning, and

		 * might report loss to these peers.

		 *

		 * Note that different controllers have different meanings of

		 * |duplicate|. Some of them consider packets with the same

		 * address as duplicate, and others consider packets with the

		 * same address and the same RSSI as duplicate. Although in the

		 * latter case we don't need to disable duplicate filter, but

		 * it is common to have active scanning for a short period of

		 * time, the power impact should be neglectable.

		/* When running simultaneous discovery, the LE scanning time

		 * should occupy the whole discovery time sine BR/EDR inquiry

		 * and LE scanning are scheduled by the controller.

		 *

		 * For interleaving discovery in comparison, BR/EDR inquiry

		 * and LE scanning are done sequentially with separate

		 * timeouts.

			/* During simultaneous discovery, we double LE scan

			 * interval. We must leave some time for the controller

			 * to do BR/EDR inquiry.

	/* When service discovery is used and the controller has a

	 * strict duplicate filter, it is important to remember the

	 * start and duration of the scan. This is required for

	 * restarting scanning during the discovery phase.

 Passive scanning */

 No further actions needed for LE-only discovery */

	/* When discoverable timeout triggers, then just make sure

	 * the limited discoverable flag is cleared. Even in the case

	 * of a timeout triggered from general discoverable, it is

	 * safe to unconditionally clear the flag.

		/* Check first if we already have the right

		 * host state (host features set)

		/* Make sure the controller has a good default for

		 * advertising data. This also applies to the case

		 * where BR/EDR was toggled during the AUTO_OFF phase.

	/* Register the available SMP channels (BR/EDR and LE) only when

	 * successfully powering on the controller. This late

	 * registration is required so that LE SMP can clearly decide if

	 * the public address or static address is used.

/*

   BlueZ - Bluetooth protocol stack for Linux



   Copyright (C) 2015  Intel Corporation



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Time stamp */

 SPDX-License-Identifier: GPL-2.0-only

/*

   Copyright (c) 2010,2011 Code Aurora Forum.  All rights reserved.

   Copyright (c) 2011,2012 Intel Corp.



 Global AMP Manager list */

 A2MP build & send command helper functions */

 hci_dev_list shall be locked */

 Processing A2MP messages */

 check that packet is not broken for now */

 at minimum the BR/EDR needs to be listed */

 check that packet is not broken for now */

 Fall back to L2CAP init sequence */

 TODO send A2MP_CHANGE_RSP */

 Make sure that other request is not processed */

 Save remote ASSOC data */

 Create Phys Link */

	/* Reply error now and success after HCI Write Remote AMP Assoc

	   command complete with success status

 TODO Disconnect Phys Link here */

 Handle A2MP signalling */

	/* Always free skb and return success error code to prevent

 Not implemented for A2MP */

 AMP Manager functions */

 Remote AMP ctrl list initialization */

/*

   BlueZ - Bluetooth protocol stack for Linux

   Copyright (C) 2000-2001 Qualcomm Incorporated

   Copyright (C) 2009-2010 Gustavo F. Padovan <gustavo@padovan.org>

   Copyright (C) 2010 Google Inc.

   Copyright (C) 2011 ProFUSION Embedded Systems



   Written 2000,2001 by Maxim Krasnyansky <maxk@qualcomm.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Bluetooth L2CAP sockets. */

 PSM must be odd and lsb of upper byte must be 0 */

 Restrict usage of well-known PSMs */

 Valid LE_PSM ranges are defined only until 0x00ff */

 Restrict fixed, SIG assigned PSM values to CAP_NET_BIND_SERVICE */

 We only allow ATT user space socket */

		/* Fixed channels default to the L2CAP core not holding a

		 * hci_conn reference for them. For fixed channels mapping to

		 * L2CAP sockets we do want to hold a reference so set the

		 * appropriate flag to request it.

	/* Check that the socket wasn't bound to something that

	 * conflicts with the address given to connect(). If chan->src

	 * is BDADDR_ANY it means bind() was never used, in which case

	 * chan->src_type and la.l2_bdaddr_type do not need to match.

		/* Old user space versions will try to incorrectly bind

		 * the ATT socket using BDADDR_BREDR. We need to accept

		 * this and fix up the source address type only when

		 * both the source CID and destination CID indicate

		 * ATT. Anything else is an invalid combination.

		/* We don't have the hdev available here to make a

		 * better decision on random vs public, but since all

		 * user space versions that exhibit this issue anyway do

		 * not support random local addresses assuming public

		 * here is good enough.

 We only allow ATT user space socket */

	/* Listening channels need to use nested locking in order not to

	 * cause lockdep warnings when the created child channels end up

	 * being locked in the same thread as the parent channel.

 Wait for an incoming connection. (wake-one). */

		/* LE sockets should use BT_SNDMTU/BT_RCVMTU, but since

		 * legacy ATT code depends on getsockopt for

		 * L2CAP_OPTIONS we need to let this pass.

 Only BR/EDR modes are supported here */

 Only BR/EDR modes are supported here */

 TODO: Add support for ECRED PDUs to BR/EDR */

 change security for LE channels */

 or for ACL link */

			/* proceed further only when we have l2cap_conn and

		/* Setting is not supported as it's the remote side that

		 * decides this.

 Attempt to put pending rx data in the socket buffer */

	/* Restore data flow when half of the receive buffer is

	 * available.  This avoids resending large numbers of

	 * frames.

/* Kill socket (only if zapped and orphan)

 * Must be called on unlocked socket, with l2cap channel lock.

 Kill poor orphan */

 Timeout to prevent infinite loop */

	/* 'how' parameter is mapped to sk_shutdown as follows:

	 * SHUT_RD   (0) --> RCV_SHUTDOWN  (1)

	 * SHUT_WR   (1) --> SEND_SHUTDOWN (2)

	 * SHUT_RDWR (2) --> SHUTDOWN_MASK (3)

 prevent sk structure from being freed whilst unlocked */

 prevent chan structure from being freed whilst unlocked */

		/* After waiting for ACKs, check whether shutdown

		 * has already been actioned to close the L2CAP

		 * link such as by l2cap_disconnection_req().

	/* Try setting the RCV_SHUTDOWN bit, return early if SEND_SHUTDOWN

	 * is already set

 prevent conn structure from being freed */

 mutex lock must be taken before l2cap_chan_lock() */

 Close not yet accepted channels */

 Check for backlog size */

		/* Even if no filter is attached, we could potentially

		 * get errors from security modules, etc.

	/* For ERTM, handle one skb that doesn't fit into the recv

	 * buffer.  This is important to do because the data frames

	 * have already been acked, so the skb cannot be discarded.

	 *

	 * Notify the l2cap core that the buffer is full, so the

	 * LOCAL_BUSY state is entered and no more frames are

	 * acked and reassembled until there is buffer space

	 * available.

	/* This callback can be called both for server (BT_LISTEN)

	 * sockets as well as "normal" ones. To avoid lockdep warnings

	 * with child socket locking (through l2cap_sock_cleanup_listen)

	 * we need separation into separate nesting levels. The simplest

	 * way to accomplish this is to inherit the nesting level used

	 * for the channel.

 Only zap after cleanup to avoid use after free race */

 Default config options */

/*

 * ECDH helper functions - KPP wrappings

 *

 * Copyright (C) 2017 Intel Corporation

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation;

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

 * IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

 * CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

 *

 * ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

 * COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

 * SOFTWARE IS DISCLAIMED.

/* compute_ecdh_secret() - function assumes that the private key was

 *                         already set.

 * @tfm:          KPP tfm handle allocated with crypto_alloc_kpp().

 * @public_key:   pair's ecc public key.

 * secret:        memory where the ecdh computed shared secret will be saved.

 *

 * Return: zero on success; error code in case of error.

 x */

 y */

/* set_ecdh_privkey() - set or generate ecc private key.

 *

 * Function generates an ecc private key in the crypto subsystem when receiving

 * a NULL private key or sets the received key when not NULL.

 *

 * @tfm:           KPP tfm handle allocated with crypto_alloc_kpp().

 * @private_key:   user's ecc private key. When not NULL, the key is expected

 *                 in little endian format.

 *

 * Return: zero on success; error code in case of error.

 fall through */

/* generate_ecdh_public_key() - function assumes that the private key was

 *                              already set.

 *

 * @tfm:          KPP tfm handle allocated with crypto_alloc_kpp().

 * @public_key:   memory where the computed ecc public key will be saved.

 *

 * Return: zero on success; error code in case of error.

	/* The public key is handed back in little endian as expected by

	 * the Security Manager Protocol.

 x */

 y */

/* generate_ecdh_keys() - generate ecc key pair.

 *

 * @tfm:          KPP tfm handle allocated with crypto_alloc_kpp().

 * @public_key:   memory where the computed ecc public key will be saved.

 *

 * Return: zero on success; error code in case of error.

 SPDX-License-Identifier: GPL-2.0

/*

 * BlueZ - Bluetooth protocol stack for Linux

 *

 * Copyright (C) 2021 Intel Corporation

 no space left for name (+ NULL + type + len) */

 use complete name if present and fits */

 use short name if present */

	/* use shortened full name if present, we already know that name

	 * is longer then HCI_MAX_SHORT_NAME_LENGTH

 Stop if not enough space to put next UUID */

 Stop if not enough space to put next UUID */

 Stop if not enough space to put next UUID */

 EIR Data type */

 EIR Data length */

 Return 0 when the current instance identifier is invalid. */

	/* If instance already has the flags set skip adding it once

	 * again.

	/* The Add Advertising command allows userspace to set both the general

	 * and limited discoverable flags.

		/* If a discovery flag wasn't provided, simply use the global

		 * settings.

		/* If flags would still be empty, then there is no need to

		 * include the "Flags" AD field".

 Provide Tx Power only if we can provide a valid value for it */

/*

   BlueZ - Bluetooth protocol stack for Linux

   Copyright (c) 2000-2001, 2010, Code Aurora Forum. All rights reserved.



   Written 2000,2001 by Maxim Krasnyansky <maxk@qualcomm.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Bluetooth HCI connection handling. */

 S3 */

 S2 */

 S1 */

 D1 */

 D0 */

 D1 */

 D0 */

 T2 */

 T1 */

 This function requires the caller holds hdev->lock */

 Check if we need to convert to identity address */

	/* The connection attempt was doing scan for new RPA, and is

	 * in scan phase. If params are not associated with any other

	 * autoconnect action, remove them completely. If they are, just unmark

	 * them as waiting for connection, by clearing explicit_connect field.

 return instead of break to avoid duplicate scan update */

 Check that the hci_conn is still around */

	/* We can't call hci_conn_del/hci_conn_cleanup here since that

	 * could deadlock with another hci_conn_del() call that's holding

	 * hci_dev_lock and doing cancel_delayed_work_sync(&conn->disc_work).

	 * Instead, grab temporary extra references to the hci_dev and

	 * hci_conn and perform the necessary cleanup in a separate work

	 * callback.

	/* Even though we hold a reference to the hdev, many other

	 * things might get cleaned up meanwhile, including the hdev's

	 * own workqueue, so we can't use that for scheduling.

	/* Many controllers disallow HCI Create Connection while it is doing

	 * HCI Inquiry. So we cancel the Inquiry first before issuing HCI Create

	 * Connection. This may cause the MGMT discovering state to become false

	 * without user space's request but it is okay since the MGMT Discovery

	 * APIs do not promise that discovery should be done forever. Instead,

	 * the user space monitors the status of MGMT discovering and it may

	 * request for discovery again when this flag becomes false.

		/* Put this connection to "pending" state so that it will be

		 * executed after the inquiry cancel command complete event.

	/* When we are central of an established connection and it enters

	 * the disconnect timeout, then go ahead and try to read the

	 * current clock offset.  Processing of the result is done

	 * within the event handling and hci_clock_offset_evt function.

 for offload use case, codec needs to configured before opening SCO */

 Device _must_ be locked */

	/* FIXME: It was observed that in pairing failed scenario, refcnt

	 * drops below 0. Probably this is because l2cap_conn_del calls

	 * l2cap_chan_del for each channel, and inside l2cap_chan_del conn is

	 * dropped. After that loop hci_chan_del is called which also drops

	 * conn. For now make sure that ACL is alive if refcnt is higher then 0,

	 * otherwise drop it.

 LE connections in scanning state need special handling */

 Enter sniff mode */

	/* We could end up here due to having done directed advertising,

	 * so clean up the state if necessary. This should however only

	 * happen with broken hardware or if low duty cycle was used

	 * (which doesn't have a timeout of its own).

 Disable LE Advertising */

 Set Default Authenticated payload timeout to 30s */

 conn->src should reflect the local identity address */

	/* The SCO and eSCO connections will only be notified when their

	 * setup has been completed. This is different to ACL links which

	 * can be notified right away.

 Unacked frames */

	/* Remove the connection from the list and cleanup its remaining

	 * state. This is a separate function since for some cases like

	 * BT_CONNECT_SCAN we *only* want the cleanup part without the

	 * rest of hci_conn_del.

		/* Simple routing:

		 *   No source address - find interface with bdaddr != dst

		 *   Source address    - find interface with bdaddr == src

 Convert from HCI to three-value type */

 This function requires the caller holds hdev->lock */

	/* If the status indicates successful cancellation of

	 * the attempt (i.e. Unknown Connection Id) there's no point of

	 * notifying failure since we'll go back to keep trying to

	 * connect. The only exception is explicit connect requests

	 * where a timeout + cancel does indicate an actual failure.

	/* The suspend notifier is waiting for all devices to disconnect and an

	 * LE connect cancel will result in an hci_le_conn_failed. Once the last

	 * connection is deleted, we should also wake the suspend queue to

	 * complete suspend operations.

	/* Since we may have temporarily stopped the background scanning in

	 * favor of connection establishment, we should restart it.

	/* Re-enable advertising in case this was a failed connection

	 * attempt as a peripheral.

	/* If direct address was provided we use it instead of current

	 * address.

 direct address is always RPA */

		/* Update random address, but set require_privacy to false so

		 * that we never connect with an non-resolvable address.

		/* Set require_privacy to false so that the remote device has a

		 * chance of identifying us.

 Use instance 0 for directed adv */

		/* As per Core Spec 5.2 Vol 2, PART E, Sec 7.8.53, for

		 * advertising_event_property LE_LEGACY_ADV_DIRECT_IND

		 * does not supports advertising data when the advertising set already

		 * contains some, the controller shall return erroc code 'Invalid

		 * HCI Command Parameters(0x12).

		 * So it is required to remove adv set for handle 0x00. since we use

		 * instance 0 for directed adv.

		/* Clear the HCI_LE_ADV bit temporarily so that the

		 * hci_update_random_address knows that it's safe to go ahead

		 * and write a new random address. The flag will be set back on

		 * as soon as the SET_ADV_ENABLE HCI command completes.

		/* Set require_privacy to false so that the remote device has a

		 * chance of identifying us.

		/* Some controllers might reject command if intervals are not

		 * within range for undirected advertising.

		 * BCM20702A0 is known to be affected by this.

	/* This ensures that during disable le_scan address resolution

	 * will not be disabled if it is followed by le_create_conn

 Let's make sure that le is enabled.*/

	/* Since the controller supports only one LE connection attempt at a

	 * time, we return -EBUSY if there is any connection attempt running.

	/* If there's already a connection object but it's not in

	 * scanning state it means it must already be established, in

	 * which case we can't do anything else except report a failure

	 * to connect.

	/* Check if the destination address has been resolved by the controller

	 * since if it did then the identity address shall be used.

		/* When given an identity address with existing identity

		 * resolving key, the connection needs to be established

		 * to a resolvable random address.

		 *

		 * Storing the resolvable random address is required here

		 * to handle connection failures. The address will later

		 * be resolved back into the original identity address

		 * from the connect request.

	/* Disable advertising if we're active. For central role

	 * connections most controllers will refuse to connect if

	 * advertising is enabled, and for peripheral role connections we

	 * anyway have to disable it in order to start directed

	 * advertising. Any registered advertisements will be

	 * re-enabled after the connection attempt is finished.

 If requested to connect as peripheral use directed advertising */

		/* If we're active scanning most controllers are unable

		 * to initiate advertising. Simply reject the attempt.

	/* If controller is scanning, we stop it since some controllers are

	 * not able to scan and connect at the same time. Also set the

	 * HCI_LE_SCAN_INTERRUPTED flag so that the command complete

	 * handler for scan disabling knows to set the correct discovery

	 * state.

 This function requires the caller holds hdev->lock */

		/* If we created new params, mark them to be deleted in

		 * hci_connect_le_scan_cleanup. It's different case than

		 * existing disabled params, those will stay after cleanup.

 We're trying to connect, so make sure params are at pend_le_conns */

 This function requires the caller holds hdev->lock */

 Let's make sure that le is enabled.*/

	/* Some devices send ATT messages as soon as the physical link is

	 * established. To be able to handle these ATT messages, the user-

	 * space first establishes the connection and then starts the pairing

	 * process.

	 *

	 * So if a hci_conn object already exists for the following connection

	 * attempt, we simply update pending_sec_level and auth_type fields

	 * and return the object found.

 defer SCO setup until mode change completed */

 Check link security requirement */

	/* In Secure Connections Only mode, it is required that Secure

	 * Connections is used and the link is encrypted with AES-CCM

	 * using a P-256 authenticated combination key.

	 /* AES encryption is required for Level 4:

	  *

	  * BLUETOOTH CORE SPECIFICATION Version 5.2 | Vol 3, Part C

	  * page 1319:

	  *

	  * 128-bit equivalent strength for link and encryption keys

	  * required using FIPS approved algorithms (E0 not allowed,

	  * SAFER+ not allowed, and P-192 not allowed; encryption key

	  * not shortened)

 Authenticate remote device */

 Make sure we preserve an existing MITM requirement*/

		/* If we're already encrypted set the REAUTH_PEND flag,

		 * otherwise set the ENCRYPT_PEND.

 Encrypt the link */

 Enable security */

 For sdp we don't need the link key. */

	/* For non 2.1 devices and low security level we don't need the link

 For other security levels we need the link key. */

	/* An authenticated FIPS approved combination key has sufficient

	/* An authenticated combination key has sufficient security for

	/* An unauthenticated combination key has sufficient security for

	/* A combination key has always sufficient security for the security

	   levels 1 or 2. High security level requires the combination key

	   is generated using maximum PIN code length (16).

		/* Ensure that the encryption key size has been read,

		 * otherwise stall the upper layer responses.

 Nothing else needed, all requirements are met */

 Check secure link requirement */

 Accept if non-secure or higher security level is required */

 Accept if secure or higher security level is already present */

 Reject not secure link */

 Switch role */

 Enter active mode */

 Drop all connection on the device */

 Check pending connect attempts */

 Prevent new hci_chan's to be created for this hci_conn */

	/* BLUETOOTH CORE SPECIFICATION Version 5.2 | Vol 2, Part B page 471:

	 * Table 6.2: Packets defined for synchronous, asynchronous, and

	 * CPB logical transport types.

		/* SCO logical transport (1 Mb/s):

		 * HV1, HV2, HV3 and DV.

		/* ACL logical transport (1 Mb/s) ptt=0:

		 * DH1, DM3, DH3, DM5 and DH5.

		/* ACL logical transport (2 Mb/s) ptt=1:

		 * 2-DH1, 2-DH3 and 2-DH5.

		/* ACL logical transport (3 Mb/s) ptt=1:

		 * 3-DH1, 3-DH3 and 3-DH5.

 eSCO logical transport (1 Mb/s): EV3, EV4 and EV5 */

 eSCO logical transport (2 Mb/s): 2-EV3, 2-EV5 */

 eSCO logical transport (3 Mb/s): 3-EV3, 3-EV5 */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2021 Intel Corporation */

 this codec doesn't have capabilities */

 validate codecs length before accessing */

 enumerate codec capabilities of standard codecs */

 validate vendor codecs length before accessing */

 enumerate vendor codec capabilities */

 check for payload data length before accessing */

 check for payload data length before accessing */

/*

   BlueZ - Bluetooth protocol stack for Linux

   Copyright (C) 2000-2001 Qualcomm Incorporated



   Written 2000,2001 by Maxim Krasnyansky <maxk@qualcomm.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Bluetooth address family and sockets. */

 Bluetooth sockets */

/* Calling function must hold the sk lock.

 * bt_sk(sk)->parent must be non-NULL meaning sk is in the parent list.

 Prevent early freeing of sk due to unlink and sock_kill */

		/* Check sk has not already been unlinked via

		 * bt_accept_unlink() due to serialisation caused by sk locking

			/* Restart the loop as sk is no longer in the list

			 * and also avoid a potential infinite loop because

			 * list_for_each_entry_safe() is not thread safe.

 sk is safely in the parent list so reduce reference count */

 FIXME: Is this check still needed */

 Pulling partial data */

 Pulling all frag data */

 put message back and return */

 This function expects the sk lock to be held when called */

 This function expects the sk lock to be held when called */

 SPDX-License-Identifier: GPL-2.0-only

/*

   Copyright (c) 2011,2012 Intel Corp.



 Remote AMP Controllers interface */

 Physical Link interface */

 AMP crypto key generation interface */

 Legacy key */

 BR/EDR Link Key concatenated together with itself */

 Derive Generic AMP Link Key (gamp) */

 Derive Dedicated AMP Link Key: "802b" is 802.11 PAL keyID */

 Read other fragments */

 Send A2MP Rsp when all fragments are received */

 Read Local AMP Assoc final link information data */

 Write AMP Assoc data fragments, returns true with last fragment written*/

 Send A2MP create phylink rsp when all fragments are written */

/*

   BlueZ - Bluetooth protocol stack for Linux

   Copyright (C) 2000-2001 Qualcomm Incorporated



   Written 2000,2001 by Maxim Krasnyansky <maxk@qualcomm.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Bluetooth HCI sockets. */

 ----- HCI socket interface ----- */

 Socket info */

 Security filter */

 Packet types */

 Events */

 Commands */

 OGF_LINK_CTL */

 OGF_LINK_POLICY */

 OGF_HOST_CTL */

 OGF_INFO_PARAM */

 OGF_STATUS_PARAM */

 Apply filter */

 Extra filter for event packets only */

 Check filter only when opcode is set */

 Send frame to RAW socket */

 Don't send frame to the socket it came from */

 Don't send frame to other channel types */

 Create a private copy with headroom */

 Put type byte before the data */

 Send frame to sockets with specific channel */

 Ignore socket without the flag set */

 Skip the original socket */

 Send frame to monitor socket */

 Create a private copy with headroom */

 Put header before the data */

 Ignore socket without the flag set */

 Skip the original socket */

 No message needed when cookie is not present */

 No message for unsupported format */

 No message needed when cookie is not present */

 No message for unsupported format */

 Generate internal stack event */

 Send event to monitor */

 Send event to sockets */

 Wake up sockets using this dead device */

 Send event to monitor */

			/* When releasing a user channel exclusive access,

			 * call hci_dev_do_close directly instead of calling

			 * hci_dev_close to ensure the exclusive access will

			 * be released and the controller brought back down.

			 *

			 * The checking of HCI_AUTO_OFF is not needed in this

			 * case since it will have been cleared already when

			 * opening the user channel.

 Ioctls that require bound socket */

	/* When calling an ioctl on an unbound raw socket, then ensure

	 * that the monitor gets informed. Ensure that the resulting event

	 * is only send once by checking if the cookie exists or not. The

	 * socket cookie will be only ever generated once for the lifetime

	 * of a given socket.

 Send event to monitor */

	/* Allow detaching from dead device and attaching to alive device, if

	 * the caller wants to re-bind (instead of close) this socket in

	 * response to hci_sock_dev_event(HCI_DEV_UNREG) notification.

			/* In the case when a cookie has already been assigned,

			 * then there has been already an ioctl issued against

			 * an unbound socket and with that triggered an open

			 * notification. Send a close notification first to

			 * allow the state transition to bounded.

 Send event to monitor */

				/* In case the transport is already up and

				 * running, clear the error here.

				 *

				 * This can happen when opening a user

				 * channel and HCI_AUTO_OFF grace period

				 * is still active.

			/* In the case when a cookie has already been assigned,

			 * this socket will transition from a raw socket into

			 * a user channel socket. For a clean transition, send

			 * the close notification first.

		/* The user channel is restricted to CAP_NET_ADMIN

		 * capabilities and with that implicitly trusted.

 Send event to monitor */

		/* The monitor interface is restricted to CAP_NET_RAW

		 * capabilities and with that implicitly trusted.

		/* Users with CAP_NET_ADMIN capabilities are allowed

		 * access to all management commands and events. For

		 * untrusted users the interface is restricted and

		 * also only untrusted events are sent.

		/* At the moment the index and unconfigured index events

		 * are enabled unconditionally. Setting them on each

		 * socket when binding keeps this functionality. They

		 * however might be cleared later and then sending of these

		 * events will be disabled, but that is then intentional.

		 *

		 * This also enables generic events that are safe to be

		 * received by untrusted users. Example for such events

		 * are changes to settings, class of device, name etc.

				/* In the case when a cookie has already been

				 * assigned, this socket will transition from

				 * a raw socket into a control socket. To

				 * allow for a clean transition, send the

				 * close notification first.

 Send event to monitor */

 Default MTU to HCI_MAX_FRAME_SIZE if not set */

 Send event to monitor */

	/* The logging frame consists at minimum of the standard header,

	 * the priority byte, the ident length byte and at least one string

	 * terminator NUL byte. Anything shorter are invalid packets.

		/* Only the priorities 0-7 are valid and with that any other

		 * value results in an invalid packet.

		 *

		 * The priority byte is followed by an ident length byte and

		 * the NUL terminated ident string. Check that the ident

		 * length is not overflowing the packet and also that the

		 * ident string itself is NUL terminated. In case the ident

		 * length is zero, the length value actually doubles as NUL

		 * terminator identifier.

		 *

		 * The message follows the ident string (if present) and

		 * must be NUL terminated. Otherwise it is not a valid packet.

		/* No permission check is needed for user channel

		 * since that gets enforced when binding the socket.

		 *

		 * However check that the packet type is valid.

		/* Since the opcode has already been extracted here, store

		 * a copy of the value for later use by the drivers.

			/* Stand-alone HCI commands must be flagged as

			 * single-command requests.

		/* Don't allow changing MTU for channels that are meant for HCI

		 * traffic only.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Google Corporation

 Please see mgmt-api.txt for documentation of these values */

 First pass to validate the tlv */

 Please see mgmt-api.txt for documentation of these values */

/*

   BlueZ - Bluetooth protocol stack for Linux

   Copyright (C) 2011 Nokia Corporation and/or its subsidiary(-ies).



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

/* Low-level debug macros to be used for stuff that we don't want

 * accidentally in dmesg, i.e. the values of the various crypto keys

 * and the inputs & outputs of crypto functions.

 Keys which are not distributed with Secure Connections */

 Maximum message length that can be passed to aes_cmac */

 Secure Connections OOB data */

 Bitmask of allowed commands */

 SMP Pairing Request */

 SMP Pairing Response */

 SMP Pairing Random (local) */

 SMP Pairing Random (remote) */

 SMP Pairing Confirm */

 SMP Temporary Key */

 Remote OOB ra/rb value */

 Local OOB ra/rb value */

 Secure Connections variables */

/* These debug key values are defined in the SMP section of the core

 * specification. debug_pk is the public debug key and debug_sk the

 * private debug key.

/* The following functions map to the LE SC SMP crypto functions

 * AES-CMAC, f4, f5, f6, g2 and h6.

 Swap key and message from LSB to MSB */

	/* The btle, salt and length "magic" values are as defined in

	 * the SMP section of the Bluetooth core specification. In ASCII

	 * the btle value ends up being 'btle'. The salt is just a

	 * random number whereas length is the value 256 in little

	 * endian format.

 Counter */

 Counter */

/* The following functions map to the legacy SMP crypto functions e, c1,

 * s1 and ah.

 The most significant octet of key corresponds to k[0] */

 Most significant octet of plaintextData corresponds to data[0] */

 Most significant octet of encryptedData corresponds to data[0] */

 p1 = pres || preq || _rat || _iat */

 res = r XOR p1 */

 res = e(k, res) */

 p2 = padding || ia || ra */

 res = res XOR p2 */

 res = e(k, res) */

 Just least significant octets from r1 and r2 are considered */

 r' = padding || r */

	/* The output of the random address function ah is:

	 *	ah(k, r) = e(k, r') mod 2^24

	 * The output of the security function e is then truncated to 24 bits

	 * by taking the least significant 24 bits of the output of e as the

	 * result of ah.

 Clear two most significant bits */

 Set second most significant bit */

 Generate key pair for Secure Connections */

			/* This is unlikely, but we need to check that

			 * we didn't accidentally generate a debug key.

	/* Ensure that we don't leave any debug key around if debug key

	 * support hasn't been explicitly enabled.

 If pairing failed clean up any keys we might have */

	/* If either side has unknown io_caps, use JUST_CFM (which gets

	 * converted later to JUST_WORKS if we're initiators.

 Initialize key for JUST WORKS */

	/* If neither side wants MITM, either "just" confirm an incoming

	 * request or use just-works for outgoing ones. The JUST_CFM

	 * will be converted to JUST_WORKS if necessary later in this

	 * function. If either side has MITM look up the method from the

	 * table.

 Don't confirm locally initiated pairing attempts */

 Don't bother user space with no IO capabilities */

	/* If Just Works, Continue with Zero TK and ask user-space for

	/* If this function is used for SC -> legacy fallback we

	 * can only recover the just-works case.

 Not Just Works/Confirm results in MITM Authentication */

	/* If both devices have Keyboard-Display I/O, the initiator

	 * Confirms and the responder Enters the passkey.

 Generate random passkey. */

		/* Even though there's no _RESPONDER suffix this is the

		 * responder STK we're adding for later lookup (the initiator

		 * STK never needs to be stored).

		/* The LTKs, IRKs and CSRKs should be persistent only if

		 * both sides had the bonding bit set in their

		 * authentication requests.

		/* Now that user space can be considered to know the

		 * identity address track the connection based on it

		 * from now on (assuming this is an LE link).

			/* Don't keep debug keys around if the relevant

			 * flag is not set.

 From core spec. Spells out in ASCII as 'lebr'. */

 SALT = 0x000000000000000000000000746D7031 */

 From core spec. Spells out in ASCII as 'tmp1'. */

	/* Allow the first expected phase 3 PDU. The rest of the PDUs

	 * will be allowed in each PDU handler to ensure we receive

	 * them in the correct order.

 From core spec. Spells out in ASCII as 'brle'. */

 SALT = 0x000000000000000000000000746D7032 */

 From core spec. Spells out in ASCII as 'tmp2'. */

 The responder sends its keys first */

 Clear the keys which are generated but not distributed */

		/* Make sure we generate only the significant amount of

		 * bytes based on the encryption key size, and set the rest

		 * of the value to zeroes.

		/* The hci_conn contains the local identity address

		 * after the connection has been established.

		 *

		 * This is true even when the connection has been

		 * established using a resolvable random address.

 Generate a new random key */

 If there are still keys to be received wait for them */

 Ignore the PDU if we've already done 20 rounds (0 - 19) */

 Generate MacKey and LTK */

		/* The round is only complete when the initiator

		 * receives pairing random.

 Start the next round */

 Passkey rounds are complete - start DHKey Check */

 Initiating device starts the round */

 Initiator sends DHKey check first */

 If it is our turn to send Pairing Confirm, do so now */

 We didn't start the pairing, so match remote */

	/* If the remote side's OOB flag is set it means it has

	 * successfully received our local OOB data - therefore set the

	 * flag to indicate that local OOB is in use.

 SMP over BR/EDR requires special treatment */

 We must have a BR/EDR SC link */

 Clear bits which are generated but not distributed */

 If we need MITM check that it can be achieved */

	/* Strictly speaking we shouldn't allow Pairing Confirm for the

	 * SC case, however some implementations incorrectly copy RFU auth

	 * req bits from our security request, which may create a false

	 * positive SC enablement.

 Clear bits which are generated but not distributed */

 Wait for Public Key from Initiating Device */

 Request setup of TK */

 Generate key pair for Secure Connections */

			/* This is unlikely, but we need to check that

			 * we didn't accidentally generate a debug key.

	/* If the remote side's OOB flag is set it means it has

	 * successfully received our local OOB data - therefore set the

	 * flag to indicate that local OOB is in use.

	/* Update remote key distribution in case the remote cleared

	 * some bits that we had enabled in our request.

 For BR/EDR this means we're done and can start phase 3 */

 Clear bits which are generated but not distributed */

 If we need MITM check that it can be achieved */

	/* Update remote key distribution in case the remote cleared

	 * some bits that we had enabled in our request.

 Clear bits which are generated but not distributed */

 Can't compose response until we have been confirmed */

/* Work-around for some implementations that incorrectly copy RFU bits

 * from our security request and thereby create the impression that

 * we're doing SC when in fact the remote doesn't support it.

 The issue is only observed when we're in responder role */

 Rebuild key dist flags which may have been cleared for SC */

 Public Key exchange must happen before any other steps */

 Passkey entry has special treatment */

 Only Just-Works pairing requires extra checks */

		/* If there already exists long term key in local host, leave

		 * the decision to user space since the remote device could

		 * be legitimate or malicious.

			/* Set passkey to 0. The value can be any number since

			 * it'll be ignored anyway.

 Generate MacKey and LTK */

 We never store STKs for initiator role, so clear this flag */

	/* If we're encrypted with an STK but the caller prefers using

	 * LTK claim insufficient security. This way we allow the

	 * connection to be re-encrypted with an LTK, even if the LTK

	 * provides the same level of security. Only exception is if we

	 * don't have an LTK (e.g. because of key distribution bits).

		/* If link is already encrypted with sufficient security we

		 * still need refresh encryption as per Core Spec 5.0 Vol 3,

		 * Part H 2.4.6

 This may be NULL if there's an unexpected disconnection */

 If SMP is already in progress ignore this request */

	/* Don't attempt to set MITM if setting is overridden by debugfs

	 * Needed to pass certification test SM/MAS/PKE/BV-01-C

		/* Require MITM if IO Capability allows or the security level

		 * requires it.

		/* Set keys to NULL to make sure smp_failure() does not try to

 Pairing is aborted if any blocked keys are distributed */

 Mark the information as received */

 Pairing is aborted if any blocked keys are distributed */

 Mark the information as received */

	/* Strictly speaking the Core Specification (4.1) allows sending

	 * an empty address which would force us to rely on just the IRK

	 * as "identity information". However, since such

	 * implementations are not known of and in order to not over

	 * complicate our implementation, simply pretend that we never

	 * received an IRK for such a device.

	 *

	 * The Identity Address must also be a Static Random or Public

	 * Address, which hci_is_identity_address() checks for.

	/* Drop IRK if peer is using identity address during pairing but is

	 * providing different address as identity information.

	 *

	 * Microsoft Surface Precision Mouse is known to have this bug.

 Mark the information as received */

	/* The preq/prsp contain the raw Pairing Request/Response PDUs

	 * which are needed as inputs to some crypto functions. To get

	 * the "struct smp_cmd_pairing" from them we need to skip the

	 * first byte which contains the opcode.

	/* If either side wants MITM, look up the method from the table,

	 * otherwise use JUST WORKS.

 Don't confirm locally initiated pairing attempts */

	/* Check if remote and local public keys are the same and debug key is

	 * not in use.

	/* Non-initiating device sends its public key after receiving

	 * the key from the initiating device.

	/* Compute the shared secret on the same crypto tfm on which the private

	 * key was set/generated.

 JUST_WORKS and JUST_CFM result in an unauthenticated key */

	/* The Initiating device waits for the non-initiating device to

	 * send the confirm value.

 Responder sends DHKey check as response to initiator */

	/* If we don't have a context the only allowed commands are

	 * pairing request and security request.

 Only new pairings are interesting */

 Don't bother if we're not encrypted */

 Only initiator may initiate SMP over BR/EDR */

 Secure Connections support must be enabled */

 BR/EDR must use Secure Connections for SMP */

 If our LE support is not enabled don't do anything */

 Don't bother if remote LE support is not enabled */

 Remote must support SMP fixed chan for BR/EDR */

 Don't bother if SMP is already ongoing */

 Prepare and send the BR/EDR SMP Pairing Request */

	/* No need to call l2cap_chan_hold() here since we already own

	 * the reference taken in smp_new_conn_cb(). This is just the

	 * first time that we tie it to a specific pointer. The code in

	 * l2cap_core.c ensures that there's no risk this function wont

	 * get called if smp_new_conn_cb was previously called.

	/* Other L2CAP channels may request SMP routines in order to

	 * change the security level. This means that the SMP channel

	 * lock must be considered in its own category to avoid lockdep

	 * warnings.

 None of these are implemented for the root channel */

 Set correct nesting level for a parent/listening channel */

	/* If the controller does not support Low Energy operation, then

	 * there is also no need to register any SMP channel.

 Flag can be already set here (due to power toggle) */

/*

   BlueZ - Bluetooth protocol stack for Linux



   Copyright (C) 2010  Nokia Corporation

   Copyright (C) 2011-2012 Intel Corporation



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Bluetooth HCI Management interface */

 HCI to MGMT error code conversion table */

 Unknown Command */

 No Connection */

 Hardware Failure */

 Page Timeout */

 Authentication Failed */

 PIN or Key Missing */

 Memory Full */

 Connection Timeout */

 Max Number of Connections */

 Max Number of SCO Connections */

 ACL Connection Exists */

 Command Disallowed */

 Rejected Limited Resources */

 Rejected Security */

 Rejected Personal */

 Host Timeout */

 Unsupported Feature */

 Invalid Parameters */

 OE User Ended Connection */

 OE Low Resources */

 OE Power Off */

 Connection Terminated */

 Repeated Attempts */

 Pairing Not Allowed */

 Unknown LMP PDU */

 Unsupported Remote Feature */

 SCO Offset Rejected */

 SCO Interval Rejected */

 Air Mode Rejected */

 Invalid LMP Parameters */

 Unspecified Error */

 Unsupported LMP Parameter Value */

 Role Change Not Allowed */

 LMP Response Timeout */

 LMP Error Transaction Collision */

 LMP PDU Not Allowed */

 Encryption Mode Not Accepted */

 Unit Link Key Used */

 QoS Not Supported */

 Instant Passed */

 Pairing Not Supported */

 Transaction Collision */

 Reserved for future use */

 Unacceptable Parameter */

 QoS Rejected */

 Classification Not Supported */

 Insufficient Security */

 Parameter Out Of Range */

 Reserved for future use */

 Role Switch Pending */

 Reserved for future use */

 Slot Violation */

 Role Switch Failed */

 EIR Too Large */

 Simple Pairing Not Supported */

 Host Busy Pairing */

 Rejected, No Suitable Channel */

 Controller Busy */

 Unsuitable Connection Interval */

 Directed Advertising Timeout */

 Terminated Due to MIC Failure */

 Connection Establishment Failed */

 MAC Connection Failed */

		/* Devices marked as raw-only are neither configured

		 * nor unconfigured controllers.

		/* Devices marked as raw-only are neither configured

		 * nor unconfigured controllers.

		/* Devices marked as raw-only are neither configured

		 * nor unconfigured controllers.

	/* If this command is called at least once, then all the

	 * default index and unconfigured index events are disabled

	 * and from now on only extended index events are used.

		/* When the experimental feature for LL Privacy support is

		 * enabled, then advertising is no longer supported.

	/* The current setting for static address has two purposes. The

	 * first is to indicate if the static address will be used and

	 * the second is to indicate if it is actually set.

	 *

	 * This means if the static address is not configured, this flag

	 * will never be set. If the address is configured, then if the

	 * address is actually used decides if the flag is set or not.

	 *

	 * For single mode LE only controllers and dual-mode controllers

	 * with BR/EDR disabled, the existence of the static address will

	 * be evaluated.

	/* If there's a pending mgmt command the flags will not yet have

	 * their final values, so check for this first.

	/* If there's a pending mgmt command the flag will not yet have

	 * it's final value, so check for this first.

	/* The generation of a new RPA and programming it into the

	 * controller happens in the hci_req_enable_advertising()

	 * function.

	/* Non-mgmt controlled devices get this bit set

	 * implicitly so that pairing works for them, however

	 * for mgmt we require user-space to explicitly enable

	 * it

	/* If this command is called at least once, then the events

	 * for class of device and local name changes are disabled

	 * and only the new extended controller information event

	 * is used.

 0x15 == Terminated due to Power Off */

 Disconnect connections, stop scans, etc */

 ENODATA means there were no HCI commands queued */

	/* Disabling discoverable requires that no timeout is set,

	 * and enabling limited discoverable requires a timeout.

		/* Setting limited discoverable when powered off is

		 * not a valid operation since it requires a timeout

		 * and so no need to check HCI_LIMITED_DISCOVERABLE.

	/* If the current mode is the same, then just update the timeout

	 * value with the new value. And if only the timeout gets updated,

	 * then no need for any HCI transactions.

	/* Cancel any potential discoverable timeout that might be

	 * still active and store new timeout value. The arming of

	 * the timeout happens in the complete handler.

 Limited discoverable mode */

		/* In limited privacy mode the change of bondable mode

		 * may affect the local advertising address.

	/* Make sure the controller has a good default for

	 * advertising data. Restrict the update to when LE

	 * has actually been enabled. During power on, the

	 * update in powered_update_hci will take care of it.

	/* Bluetooth single mode LE only controllers or dual-mode

	 * controllers configured as LE only devices, do not allow

	 * switching LE off. These have either LE enabled explicitly

	 * or BR/EDR has been previously switched off.

	 *

	 * When trying to enable an already enabled LE, then gracefully

	 * send a positive response. Trying to disable it however will

	 * result into rejection.

/* This is a helper function to test for pending mgmt commands that can

 * cause CoD or EIR HCI commands. We can only allow one such pending

 * mgmt command at a time since otherwise we cannot easily track what

 * the current values are, will be, and based on that calculate if a new

 * HCI command needs to be sent and if yes with what value.

		/* Always ignore debug keys and require a new pairing if

		 * the user wants to use them.

		/* If disconnection is requested, then look up the

		 * connection. If the remote device is connected, it

		 * will be later used to terminate the link.

		 *

		 * Setting it to NULL explicitly will cause no

		 * termination of the link.

 LE address type */

 Abort any ongoing SMP pairing. Removes ltk and irk if they exist. */

	/* Defer clearing up the connection parameters until closing to

	 * give a chance of keeping them if a repairing happens.

 Disable auto-connection parameters if present */

	/* If disconnection is not requested, then clear the connection

	 * variable so that the link is not terminated.

	/* If the connection variable is set, then termination of the

	 * link is requested.

 Fallback to LE Random address type */

 Fallback to BR/EDR type */

 Recalculate length in case of filtered SCO connections, etc */

 So we don't get further callbacks for this connection */

	/* The device is paired so there is no need to remove

	 * its connection parameters anymore.

		/* When pairing a new device, it is expected to remember

		 * this device for future connections. Adding the connection

		 * parameter information ahead of time allows tracking

		 * of the peripheral preferred values and will speed up any

		 * further connection establishment.

		 *

		 * If connection parameters already exist, then they

		 * will be kept and this function does nothing.

 For LE, just connecting isn't a proof that the pairing finished */

	/* Since user doesn't want to proceed with the connection, abort any

	 * ongoing pairing and then terminate the link if it was created

	 * because of the pair device action.

 Continue with pairing via HCI */

 stop if current instance doesn't need to be changed */

	/* If the old values are the same as the new ones just return a

	 * direct command complete event.

	/* The name is stored in the scan response data and so

	 * no need to update the advertising data here.

	/* When the Read Simple Pairing Options command is supported, then

	 * the remote public key validation is supported.

	 *

	 * Alternatively, when Microsoft extensions are available, they can

	 * indicate support for public key validation as well.

 Remote public key validation (BR/EDR) */

 Remote public key validation (LE) */

	/* When the Read Encryption Key Size command is supported, then the

	 * encryption key size is enforced.

 Encryption key size enforcement (BR/EDR) */

 Encryption key size enforcement (LE) */

	/* When the Read Simple Pairing Options command is supported, then

	 * also max encryption key size information is provided.

	/* Append the min/max LE tx power parameters if we were able to fetch

	 * it from the controller

 d4992530-b9ec-469f-ab01-6c481c47da1c */

 330859bc-7506-492d-9370-9a6f0614037f */

 a6695ace-ee7f-4fb9-881a-5fac66c629af */

 671b10b5-42c0-4696-9227-eb28d1b049d6 */

 15c0a148-c273-11ea-b3de-0242ac130004 */

 Enough space for 5 features: 2 + 20 * 5 */

 Central */

 Peripheral */

 Simultaneous */

	/* After reading the experimental features information, enable

	 * the events to update client on any future change.

 The zero key uuid is special. Multiple exp features are set through it. */

 Command requires to use the non-controller index */

 Parameters are limited to a single octet */

 Only boolean on/off is supported */

 Command requires to use the controller index */

 Changes can only be made when controller is powered down */

 Parameters are limited to a single octet */

 Only boolean on/off is supported */

 Enable LL privacy + supported settings changed */

 Disable LL privacy + supported settings changed */

 Command requires to use a valid controller index */

 Parameters are limited to a single octet */

 Only boolean on/off is supported */

 Command requires to use a valid controller index */

 Parameters are limited to a single octet */

 Only boolean on/off is supported */

 end with a null feature */

 All supported features are currently enabled */

		/* Default values. These numbers are the least constricting

		 * parameters for MSFT API to work, so it behaves as if there

		 * are no rssi parameter to consider. May need to be changed

		 * if other API are to be supported.

 monitor can be removed without forwarding request to controller */

			/* Enforce zero-valued 192-bit parameters as

			 * long as legacy SMP OOB isn't implemented.

			/* In case one of the P-192 values is set to zero,

			 * then just disable OOB data for P-192.

		/* In case one of the P-256 values is set to zero, then just

		 * disable OOB data for P-256.

 Handle suspend notifier */

 Can't start discovery when it is paused */

	/* Clear the discovery filter first to free any previously

	 * allocated memory for the UUID list.

	/* Clear the discovery filter first to free any previously

	 * allocated memory for the UUID list.

 Handle suspend notifier */

 Handle suspend notifier */

	/* If "Set Advertising" was just disabled and instance advertising was

	 * set up earlier, then re-enable multi-instance advertising.

	/* Enabling the experimental LL Privay support disables support for

	 * advertising.

	/* The following conditions are ones which mean that we should

	 * not do any HCI communication but directly send a mgmt

	 * response to user space (after toggling the flag if

	 * necessary).

		/* Switch to instance "0" for the Set Advertising setting.

		 * We cannot use update_[adv|scan_rsp]_data() here as the

		 * HCI_ADVERTISING flag is not yet set.

 Two most significant bits shall be set */

	/* If background scan is running, restart it so new parameters are

	 * loaded.

		/* We need to restore the flag if related HCI commands

		 * failed.

 Reject disabling when powered on */

		/* When configuring a dual-mode controller to operate

		 * with LE only and using a static address, then switching

		 * BR/EDR back on is not allowed.

		 *

		 * Dual-mode controllers shall operate with the public

		 * address as its identity address for BR/EDR and LE. So

		 * reject the attempt to create an invalid configuration.

		 *

		 * The same restrictions applies when secure connections

		 * has been enabled. For BR/EDR this is a controller feature

		 * while for LE it is a host stack feature. This means that

		 * switching BR/EDR back on when secure connections has been

		 * enabled is not a supported transaction.

	/* We need to flip the bit already here so that

	 * hci_req_update_adv_data generates the correct flags.

	/* Since only the advertising data flags will change, there

	 * is no need to update the scan response data.

	/* If user space supports this command it is also expected to

	 * handle IRKs. Therefore, set the HCI_RPA_RESOLVING flag.

 Two most significant bits shall be set */

 Two most significant bits shall be set */

	/* Commands sent in request are either Read RSSI or Read Transmit Power

	 * Level so we check which one was last sent to retrieve connection

	 * handle.  Both commands have handle as first parameter so it's safe to

	 * cast data on the same command struct.

	 *

	 * First command sent is always Read RSSI and we fail only if it fails.

	 * In other case we simply override error to indicate success as we

	 * already remembered if TX power value is actually valid.

	/* To avoid client trying to guess when to poll again for information we

	 * calculate conn info age as random value between min/max set in hdev.

	/* Query controller to refresh cached values if they are too old or were

	 * never read.

		/* For LE links TX power does not change thus we don't need to

		 * query for it once value is known.

 Max TX power needs to be read only once per connection */

 Cache is valid, just reply with values cached in hci_conn */

 Piconet clock */

 This function requires the caller holds hdev->lock */

		/* If auto connect is being disabled when we're trying to

		 * connect to device, keep connecting.

 Only incoming connections action is supported for now */

	/* Kernel internally uses conn_params with resolvable private

	 * address, but Add Device allows only identity addresses.

	 * Make sure it is enforced before calling

	 * hci_conn_params_lookup.

	/* If the connection parameters don't exist for this device,

	 * they will be created and configured with defaults.

		/* Kernel internally uses conn_params with resolvable private

		 * address, but Remove Device allows only identity addresses.

		 * Make sure it is enforced before calling

		 * hci_conn_params_lookup.

		/* This should return the active RPA, but since the RPA

		 * is only programmed on demand, it is really hard to fill

		 * this in at the moment. For now disallow retrieving

		 * local out-of-band data when privacy is in use.

		 *

		 * Returning the identity address will not help here since

		 * pairing happens before the identity resolving key is

		 * known and thus the connection establishment happens

		 * based on the RPA and not the identity address.

	/* In extended adv TX_POWER returned from Set Adv Param

	 * will be always valid.

	/* Enabling the experimental LL Privay support disables support for

	 * advertising.

 Make sure that the data is correctly formatted. */

		/* If the current field length would exceed the total data

		 * length, then it's invalid.

	/* The current implementation only supports a subset of the specified

	 * flags. Also need to check mutual exclusiveness of sec flags.

	/* Enabling the experimental LL Privay support disables support for

	 * advertising.

	/* Only trigger an advertising added event if a new instance was

	 * actually added.

		/* If the currently advertised instance is being changed then

		 * cancel the current advertising and schedule the next

		 * instance. If there is only one instance then the overridden

		 * advertising data will be visible right away.

		/* Immediately advertise the new instance if no other

		 * instance is currently being advertised.

	/* If the HCI_ADVERTISING flag is set or the device isn't powered or

	 * there is no instance to be advertised then we have no HCI

	 * communication to make. Simply return.

	/* We're good to go, update advertising data, parameters, and start

	 * advertising.

	/* While we're at it, inform userspace of the available space for this

	 * advertisement, given the flags that will be used.

		/* If this advertisement was previously advertising and we

		 * failed to update it, we signal that it has been removed and

		 * delete its structure

	/* The purpose of breaking add_advertising into two separate MGMT calls

	 * for params and data is to allow more parameters to be added to this

	 * structure in the future. For this reason, we verify that we have the

	 * bare minimum structure we know of when the interface was defined. Any

	 * extra parameters we don't know about will be ignored in this request.

 In new interface, we require that we are powered to register */

 Parse defined parameters from request, use defaults otherwise */

 Create advertising instance with no advertising or response data */

 Submit request for advertising params if ext adv available */

		/* Updating parameters of an active instance will return a

		 * Command Disallowed error, so we must first disable the

		 * instance if it is active.

 In new interface, we require that we are powered to register */

 Validate new data */

 Set the data in the advertising instance */

	/* We're good to go, update advertising data, parameters, and start

	 * advertising.

 If using software rotation, determine next instance to use */

			/* If the currently advertised instance is being changed

			 * then cancel the current advertising and schedule the

			 * next instance. If there is only one instance then the

			 * overridden advertising data will be visible right

			 * away

			/* Immediately advertise the new instance if no other

			 * instance is currently being advertised.

		/* If the HCI_ADVERTISING flag is set or there is no instance to

		 * be advertised then we have no HCI communication to make.

		 * Simply return.

	/* We were successful in updating data, so trigger advertising_added

	 * event if this is an instance that wasn't previously advertising. If

	 * a failure occurs in the requests we initiated, we will remove the

	 * instance again in add_advertising_complete

	/* A failure status here only means that we failed to disable

	 * advertising. Otherwise, the advertising instance has been removed,

	 * so report success.

	/* Enabling the experimental LL Privay support disables support for

	 * advertising.

 If we use extended advertising, instance is disabled and removed */

	/* If no HCI commands have been collected so far or the HCI_ADVERTISING

	 * flag is set or the device isn't powered then we have no HCI

	 * communication to make. Simply return.

	/* The current implementation only supports a subset of the specified

	 * flags.

 0x0000 (no command) */

 This function requires the caller holds hdev->lock */

		/* Needed for AUTO_OFF case where might not "really"

		 * have been powered off.

	/* If the power off is because of hdev unregistration let

	 * use the appropriate INVALID_INDEX status. Otherwise use

	 * NOT_POWERED. We cover both scenarios here since later in

	 * mgmt_index_removed() any hci_conn callbacks will have already

	 * been triggered, potentially causing misleading DISCONNECTED

	 * status responses.

	/* Devices using resolvable or non-resolvable random addresses

	 * without providing an identity resolving key don't require

	 * to store long term keys. Their addresses will change the

	 * next time around.

	 *

	 * Only when a remote device provides an identity address

	 * make sure the long term key is stored. If the remote

	 * identity is known, the long term keys are internally

	 * mapped to the identity address. So allow static random

	 * and public addresses here.

	/* Make sure we copy only the significant bytes based on the

	 * encryption key size, and set the rest of the value to zeroes.

	/* Devices using resolvable or non-resolvable random addresses

	 * without providing an identity resolving key don't require

	 * to store signature resolving keys. Their addresses will change

	 * the next time around.

	 *

	 * Only when a remote device provides an identity address

	 * make sure the signature resolving key is stored. So allow

	 * static random and public addresses here.

	/* We must ensure that the EIR Data fields are ordered and

	 * unique. Keep it simple for now and avoid the problem by not

	 * adding any BR/EDR data to the LE adv.

	/* The connection is still in hci_conn_hash so test for 1

	 * instead of 0 to know if this is the last one.

 Report disconnects due to suspend */

	/* The connection is still in hci_conn_hash so test for 1

	 * instead of 0 to know if this is the last one.

		/* If this is a HCI command related to powering on the

		 * HCI dev don't send any mgmt signals.

 If controller is not scanning we are done. */

	/* If a RSSI threshold has been specified, and

	 * HCI_QUIRK_STRICT_DUPLICATE_FILTER is not set, then all results with

	 * a RSSI smaller than the RSSI threshold will be dropped. If the quirk

	 * is set, let it through for further processing, as we might need to

	 * restart the scan.

	 *

	 * For BR/EDR devices (pre 1.2) providing no RSSI during inquiry,

	 * the results are also dropped.

		/* If a list of UUIDs is provided in filter, results with no

		 * matching UUID should be dropped.

	/* If duplicate filtering does not report RSSI changes, then restart

	 * scanning to ensure updated result with updated RSSI values.

 Validate RSSI value against the RSSI threshold once more. */

	/* Don't send events for a non-kernel initiated discovery. With

	 * LE one exception is if we have pend_le_reports > 0 in which

	 * case we're doing passive scanning and want these events.

 We are using service discovery */

 Check for limited discoverable bit */

	/* Make sure that the buffer is big enough. The 5 extra bytes

	 * are for the potential CoD field.

	/* In case of device discovery with BR/EDR devices (pre 1.2), the

	 * RSSI value was reported as 0 when not available. This behavior

	 * is kept when using device discovery. This is required for full

	 * backwards compatibility with the API.

	 *

	 * However when using service discovery, the value 127 will be

	 * returned when the RSSI is not available.

 Copy EIR or advertising data into event */

 Append scan response data to event */

/*

   BlueZ - Bluetooth protocol stack for Linux



   Copyright (C) 2014 Intel Corporation



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

		/* The Bluetooth UUID values are stored in big endian,

		 * but with reversed byte order. So convert them into

		 * the right order for the %pUb modifier.

	/* If the controller does not support BR/EDR Secure Connections

	 * feature, then the BR/EDR SMP channel shall not be present.

	 *

	 * To test this with Bluetooth 4.0 controllers, create a debugfs

	 * switch that allows forcing BR/EDR SMP support and accepting

	 * cross-transport pairing on non-AES encrypted connections.

	/* Require the RPA timeout to be at least 30 seconds and at most

	 * 24 hours.

	/* For controllers with a public address, provide a debug

	 * option to force the usage of the configured static

	 * address. By default the public address is used.

	/* When the diagnostic flags are not persistent and the transport

	 * is not active or in user channel operation, then there is no need

	 * for the vendor callback. Instead just store the desired value and

	 * the setting will be programmed when the controller gets powered on.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2021 Intel Corporation

 LE Get Vendor Capabilities Command */

/*

   BlueZ - Bluetooth protocol stack for Linux

   Copyright (C) 2000-2001 Qualcomm Incorporated

   Copyright (C) 2009-2010 Gustavo F. Padovan <gustavo@padovan.org>

   Copyright (C) 2010 Google Inc.

   Copyright (C) 2011 ProFUSION Embedded Systems

   Copyright (c) 2012 Code Aurora Forum.  All rights reserved.



   Written 2000,2001 by Maxim Krasnyansky <maxk@qualcomm.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Bluetooth L2CAP core. */

 ---- L2CAP channels ---- */

/* Find channel with given SCID.

/* Find channel with given DCID.

 * Returns locked channel.

 Override the defaults (which are for conn-oriented) */

 ---- L2CAP sequence number lists ---- */

/* For ERTM, ordered lists of sequence numbers must be tracked for

 * SREJ requests that are received and for frames that are to be

 * retransmitted. These seq_list functions implement a singly-linked

 * list in an array, where membership in the list can also be checked

 * in constant time. Items can also be added to the tail of the list

 * and removed from the head in constant time, without further memory

 * allocs or frees.

	/* Allocated size is a power of 2 to map sequence numbers

	 * (which may be up to 14 bits) in to a smaller array that is

	 * sized for the negotiated ERTM transmit windows.

 Constant-time check for list membership */

 All appends happen in constant time */

	/* __set_chan_timer() calls l2cap_chan_hold(chan) while scheduling

	 * this work. No need to call l2cap_chan_hold(chan) here again.

 Set default lock nesting level */

 This flag is cleared in l2cap_chan_ready() */

 Derive MPS from connection MTU to stop HCI fragmentation */

 Give enough credits for a full packet */

 L2CAP implementations shall support a minimum MPS of 64 octets */

 Alloc CID for connection-oriented socket */

 Connectionless socket */

 Caller will set CID and CID specific MTU values */

 Raw socket can send/recv signalling messages only */

 Only keep a reference for fixed channels if they requested it */

 Delete from channel list */

		/* Reference was only held for non-fixed channels or

		 * fixed channels that explicitly requested it using the

		 * FLAG_HOLD_HCI_CONN flag.

 Service level security */

	/* Get next available identificator.

	 *    1 - 128 are used by kernel.

	 *  129 - 199 are reserved.

	 *  200 - 254 are used by utilities like l2ping, etc.

	/* Use NO_FLUSH if supported or we have an LE link (which does

	/* Use NO_FLUSH for LE links (where this is the only option) or

	 * if the BR/EDR link supports it and flushing has not been

	 * explicitly requested (through FLAG_FLUSHABLE).

 S-Frame */

 I-Frame */

 S-Frame */

 I-Frame */

 Check EFS parameters */

	/* The channel may have already been flagged as connected in

	 * case of receiving data before the L2CAP info req/rsp

	 * procedure is complete.

 This clears all conf flags, including CONF_NOT_COMPLETE */

 Only add deferred channels with the same PID/PSM */

 Set the same ident so we can match on the rsp */

 Include all channels deferred */

	/* The minimum encryption key size needs to be enforced by the

	 * host stack before establishing any L2CAP connections. The

	 * specification in theory allows a minimum of 1, but to align

	 * BR/EDR and LE transports, a minimum of 7 is chosen.

	 *

	 * This check might also be called for unencrypted connections

	 * that have no key size requirements. Ensure that the link is

	 * actually encrypted before enforcing a key size.

 On FIPS security level, key size must be 16 bytes */

 ---- L2CAP connections ---- */

	/* For outgoing pairing which doesn't necessarily have an

	 * associated socket (e.g. mgmt_pair_device).

	/* For LE peripheral connections, make sure the connection interval

	 * is in the range of the minimum and maximum interval that has

	 * been configured for this connection. If not, then trigger

	 * the connection update procedure.

 Notify sockets that we cannot guaranty reliability anymore */

/*

 * l2cap_user

 * External modules can register l2cap_user objects on l2cap_conn. The ->probe

 * callback is called during registration. The ->remove callback is called

 * during unregistration.

 * An l2cap_user object can either be explicitly unregistered or when the

 * underlying l2cap_conn object is deleted. This guarantees that l2cap->hcon,

 * l2cap->hchan, .. are valid as long as the remove callback hasn't been called.

 * External modules must own a reference to the l2cap_conn object if they intend

 * to call l2cap_unregister_user(). The l2cap_conn object might get destroyed at

 * any time if they don't.

	/* We need to check whether l2cap_conn is registered. If it is not, we

	 * must not register the l2cap_user. l2cap_conn_del() is unregisters

	 * l2cap_conn objects, but doesn't provide its own locking. Instead, it

	 * relies on the parent hci_conn object to be locked. This itself relies

	 * on the hci_dev object to be locked. So we must lock the hci device

 conn->hchan is NULL after l2cap_conn_del() was called */

	/* We can not call flush_work(&conn->pending_rx_work) here since we

	 * might block if we are running on a worker from the same workqueue

	 * pending_rx_work is waiting on.

 Force the connection to be immediately dropped */

 Kill channels */

 ---- Socket interface ---- */

/* Find socket with psm and source / destination bdaddr.

 * Returns closest match.

 Exact match. */

 Closest match */

		/* Clone after data has been modified. Data is assumed to be

		   read-only (for locking purposes) on cloned sk_buffs.

			/* Cloned sk_buffs are read-only, so we need a

			 * writeable copy

 Update skb contents */

 Update FCS */

 If any i-frames were sent, they included an ack */

		/* Ack now if the window is 3/4ths full.

		 * Calculate without mul or div

 Continuation fragments (no L2CAP header) */

 Create L2CAP header */

 Create L2CAP header */

 Create L2CAP header */

 Control header is populated later */

	/* It is critical that ERTM PDUs fit in a single HCI fragment,

	 * so fragmented skbs are not used.  The HCI layer's handling

	 * of fragmented skbs is not compatible with ERTM's queueing.

 PDU size is derived from the HCI MTU */

 Constrain PDU size for BR/EDR connections */

 Adjust for largest possible L2CAP overhead. */

 Remote device may have requested smaller PDUs */

 Create L2CAP header */

 Connectionless channel */

		/* Channel lock is released before requesting new skb and then

		 * reacquired thus we need to recheck channel state.

 Check outgoing MTU */

 Check outgoing MTU */

 Create a basic PDU */

		/* Channel lock is released before requesting new skb and then

		 * reacquired thus we need to recheck channel state.

 Check outgoing MTU */

		/* Do segmentation before calling in to the state machine,

		 * since it's possible to block while waiting for memory

		 * allocation.

		/* The channel could have been closed while segmenting,

		 * check that it is still connected.

		/* If the skbs were not queued for sending, they'll still be in

		 * seg_queue and need to be purged.

 Capture initial list head to allow only one pass through the list. */

			/* The SREJ_SENT state must be aborted if we are to

			 * enter the LOCAL_BUSY state.

 Nothing to process */

 Queue data, but don't send. */

			/* The SREJ_SENT state must be aborted if we are to

			 * enter the LOCAL_BUSY state.

 Ignore */

 Ignore event */

 Copy frame to all raw sockets on that connection */

 Don't send frame to the channel it came from */

 ---- L2CAP signalling commands ---- */

 Continuation fragments (no L2CAP header) */

		/* Class 1 devices have must have ERTM timeouts

		 * exceeding the Link Supervision Timeout.  The

		 * default Link Supervision Timeout for AMP

		 * controllers is 10 seconds.

		 *

		 * Class 1 devices use 0xffffffff for their

		 * best-effort flush timeout, so the clamping logic

		 * will result in a timeout that meets the above

		 * requirement.  ERTM timeouts are 16-bit values, so

		 * the maximum timeout is 65.535 seconds.

 Convert timeout to milliseconds and round */

		/* This is the recommended formula for class 2 devices

		 * that start ERTM timers when packets are sent to the

		 * controller.

 use extended control field */

	/* The 2-DH1 packet has between 2 and 56 information bytes

	 * (including the 2-byte payload header)

	/* The 3-DH1 packet has between 2 and 85 information bytes

	 * (including the 2-byte payload header)

	/* The 2-DH3 packet has between 2 and 369 information bytes

	 * (including the 2-byte payload header)

	/* The 3-DH3 packet has between 2 and 554 information bytes

	 * (including the 2-byte payload header)

	/* The 2-DH5 packet has between 2 and 681 information bytes

	 * (including the 2-byte payload header)

	/* The 3-DH5 packet has between 2 and 1023 information bytes

	 * (including the 2-byte payload header)

		/* Configure output options and let the other side know

 Send PENDING Conf Rsp */

 Reset ident so only one response is sent */

 Include all channels pending with the same ident */

	/* Use sane default values in case a misbehaving remote device

	 * did not send an RFC or extended window size option.

 Check if we have socket listening on psm */

 Check if the ACL is secure enough (if not SDP) */

 Check for valid dynamic CID range (as per Erratum 3253) */

 Check if we already have channel with that dcid */

	/* For certain devices (ex: HID mouse), support for authentication,

	 * pairing and bonding is optional. For such devices, inorder to avoid

	 * the ACL alive for too long after L2CAP disconnection, reset the ACL

	 * disc_timeout back to HCI_DISCONN_TIMEOUT during L2CAP connect.

				/* Force pending result for AMP controllers.

				 * The connection will succeed after the

				 * physical link is up.

	/* FCS is enabled only in ERTM or streaming mode, if one or both

	 * sides request it.

 Reject if config buffer is too small. */

 Store config. */

 Incomplete config. Send empty response. */

 Complete config. */

 Reset config buffer. */

	/* Got Conf Rsp PENDING from remote side and assume we sent

 check compatibility */

 Send rsp for BR/EDR channel */

 throw out any old stored conf requests */

 L2CAP Info req/rsp are unbound to channels, add extra checks */

 For controller id 0 make BR/EDR connection */

 Validate AMP controller id */

 Placeholder - release the logical link */

 Logical link setup failed */

 Create channel failure, disconnect */

			/* Remote has only sent pending or

			 * success responses, clean up

		/* Other amp move states imply that the move

		 * has already aborted

		/* Move confirm will be sent after a success

		 * response is received

 Move was not in expected state, free the channel */

 Call with chan locked */

 Ignore logical link if channel is on BR/EDR */

 Placeholder - start physical link setup */

 Outgoing channel on AMP */

 Revert to BR/EDR connect */

 Incoming channel on AMP */

 Send successful response */

 Send negative response */

 Placeholder - get hci_chan for logical link */

 Logical link is ready to go */

 Wait for logical link to be ready */

 Logical link not available */

 Restart data transmission */

 Invoke with locked chan */

	/* Detect a move collision.  Only send a collision response

	 * if this side has "lost", otherwise proceed with the move.

	 * The winner has the larger bd_addr.

 Moving to BR/EDR */

 Placeholder - uncomment when amp functions are available */

amp_accept_physical(chan, req->dest_amp_id);*/

		/* Move confirm will be sent when logical link

		 * is complete.

			/* Logical link is up or moving to BR/EDR,

			 * proceed with move

 Moving to AMP */

			/* Remote is ready, send confirm immediately

			 * after logical link is ready

			/* Both logical link and move success

			 * are required to confirm

 Placeholder - get hci_chan for logical link */

 Logical link not available */

		/* If the logical link is not yet connected, do not

		 * send confirmation.

 Logical link is already ready to go */

 Can confirm now */

			/* Now only need move success

			 * to confirm

 Any other amp move state means the move failed. */

 Could not locate channel, icid is best guess */

 Cleanup - cancel move */

 Spec requires a response even if the icid was not found */

		/* If we already have MITM protection we can't do

		 * anything.

 We'll need to send a new Connect Request */

 Check if we have socket listening on psm */

 Check for valid dynamic CID range */

 Check if we already have channel with that dcid */

		/* The following result value is actually not defined

		 * for LE CoC but we use it to let the function know

		 * that it should bail out after doing its cleanup

		 * instead of sending a response.

		/* Return 0 so that we don't trigger an unnecessary

		 * command reject packet.

 Resume sending */

 Check if we have socket listening on psm */

 Check for valid dynamic CID range */

 Check if we already have channel with that dcid */

 Init response */

 Check that there is a dcid for each pending channel */

 Check if dcid is already in use */

			/* If a device receives a

			 * L2CAP_CREDIT_BASED_CONNECTION_RSP packet with an

			 * already-assigned Destination CID, then both the

			 * original channel and the new channel shall be

			 * immediately discarded and not used.

			/* If we already have MITM protection we can't do

			 * anything.

 We'll need to send a new Connect Request */

 If dcid was not set it means channels was refused */

		/* If the MTU value is decreased for any of the included

		 * channels, then the receiver shall disconnect all

		 * included channels.

 Send pending iframes */

		/* F-bit wasn't sent in an s-frame or i-frame yet, so

		 * send it now.

	/* skb->len reflects data in skb as well as all fragments

	 * skb->data_len reflects only data in fragments

 Reassembly complete */

 Placeholder */

	/* Pass sequential frames to l2cap_reassemble_sdu()

	 * until a gap is encountered.

			/* See notes below regarding "double poll" and

			 * invalid packets.

		/* A source of invalid packets is a "double poll" condition,

		 * where delays cause us to send multiple poll packets.  If

		 * the remote stack receives and processes both polls,

		 * sequence numbers can wrap around in such a way that a

		 * resent frame has a sequence number that looks like new data

		 * with a sequence gap.  This would trigger an erroneous SREJ

		 * request.

		 *

		 * Fortunately, this is impossible with a tx window that's

		 * less than half of the maximum sequence number, which allows

		 * invalid frames to be safely ignored.

		 *

		 * With tx window sizes greater than half of the tx window

		 * maximum, the frame is invalid and cannot be ignored.  This

		 * causes a disconnect.

			/* Can't issue SREJ frames in the local busy state.

			 * Drop this frame, it will be seen as missing

			 * when local busy is exited.

			/* There was a gap in the sequence, so an SREJ

			 * must be sent for each missing frame.  The

			 * current frame is stored for later use.

 Keep frame for reassembly later */

			/* Got a frame that can't be reassembled yet.

			 * Save it for later, and send SREJs to cover

			 * the missing frames.

			/* This frame was requested with an SREJ, but

			 * some expected retransmitted frames are

			 * missing.  Request retransmission of missing

			 * SREJ'd frames.

 We've already queued this frame.  Drop this copy. */

			/* Expecting a later sequence number, so this frame

			 * was already received.  Ignore it completely.

	/* Rewind next_tx_seq to the point expected

	 * by the receiver.

	/* Rewind next_tx_seq to the point expected

	 * by the receiver.

 Make sure reqseq is for a packet that has been sent but not acked */

 shut it down */

	/*

	 * We can just drop the corrupted I-frame here.

	 * Receiver will miss it and start proper recovery

	 * procedures and ask for retransmission.

		/* Validate F-bit - F=0 always valid, F=1 only

		 * valid in TX WAIT_F

 Only I-frames are expected in streaming mode */

 Validate F and P bits */

 Wait recv to confirm reception before updating the credits */

 Update credits whenever an SDU is received */

	/* Update if remote had run out of credits, this should only happens

	 * if the remote is not using the entire MPS.

 Detect if remote is not able to use the selected MPS */

 Adjust the number of credits */

	/* We can't return an error here since we took care of the skb

	 * freeing internally. An error return would cause the caller to

	 * do a double-free of the skb.

 Drop packet and return */

	/* If we receive data on a fixed channel before the info req/rsp

	 * procedure is done simply assume that the channel is supported

	 * and mark it as ready.

		/* If socket recv buffers overflows we drop data here

		 * which is *bad* because L2CAP has to be reliable.

		 * But we don't have any other choice. L2CAP doesn't

 Store remote BD_ADDR and PSM for msg_name */

	/* Since we can't actively block incoming LE connections we must

	 * at least ensure that we ignore incoming data from them.

 PSM must be odd and lsb of upper byte must be 0 */

 Only count deferred channels with the same PID/PSM */

 Already connecting */

 Already connected */

 Can connect */

 Set destination address and psm */

		/* Convert from L2CAP channel address type to HCI address type

 Check if there isn't too many channels being connected */

 Update source addr of the socket */

 l2cap_chan_add takes its own ref so we can drop this one */

	/* Release chan->sport so that it can be reused by other

	 * sockets (as it's only used for listening sockets).

 ---- L2CAP interface with lower layer (HCI) ---- */

 Find listening sockets and check their link_mode */

/* Find the next fixed channel in BT_LISTEN state, continue iteration

 * from an existing channel in the list or from the beginning of the

 * global list (by passing NULL as first parameter).

 If device is blocked, do not create channels for it */

	/* Find fixed channels and notify them of the new connection. We

	 * use multiple individual lookups, continuing each time where

	 * we left off, because the list lock would prevent calling the

	 * potentially sleeping l2cap_chan_lock() function.

 Client fixed channels should override server ones */

 Append fragment into frame respecting the maximum len of rx_skb */

 Allocate skb for the complete frame (with header) */

 Init rx_len */

 Copy as much as the rx_skb can hold */

 Append just enough to complete the header */

 If header could not be read just continue */

 Check if rx_skb has enough space to received all fragments */

 Update expected len */

	/* Reset conn->rx_skb since it will need to be reallocated in order to

	 * fit all fragments.

 Reallocates rx_skb using the exact expected length */

 For AMP controller do not create l2cap conn */

		/* Start fragment may not contain the L2CAP length so just

		 * copy the initial byte when that happens and use conn->mtu as

		 * expected length.

 Complete frame received */

 Append fragment into frame (with header) */

 Complete the L2CAP length if it has not been read */

 Header still could not be read just continue */

 Append fragment into frame (with header) */

			/* Complete frame received. l2cap_recv_frame

			 * takes ownership of the skb so set the global

			 * rx_skb pointer to NULL first.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Google Corporation

 This function requires the caller holds hdev->lock */

 All monitors have been reregistered */

 If success, we return and wait for monitor added callback */

 Otherwise remove the monitor and keep registering */

 This function requires the caller holds hdev->lock */

 All monitors have been removed */

 If success, return and wait for monitor removed callback */

 Otherwise free the monitor and keep removing */

 This function requires the caller holds hdev->lock */

		/* Quitely remove all monitors on suspend to avoid waking up

		 * the system.

 This function requires the caller holds hdev->lock */

		/* Monitors are removed on suspend, so we need to add all

		 * monitors on resume.

 Reset existing MSFT data before re-reading */

		/* Monitors get removed on power off, so we need to explicitly

		 * tell the controller to re-monitor.

	/* The controller will silently remove all monitors on power off.

	 * Therefore, remove handle_data mapping and reset monitor state.

	/* When the extension has defined an event prefix, check that it

	 * matches, and otherwise just return.

	/* Every event starts at least with an event code and the rest of

	 * the data is variable and depends on the event code.

/* is_mgmt = true matches the handle exposed to userspace via mgmt.

 * is_mgmt = false matches the handle used by the msft controller.

 * This function requires the caller holds hdev->lock

 If in restart/reregister sequence, keep registering. */

		/* Do not free the monitor if it is being removed due to

		 * suspend. It will be re-monitored on resume.

 If in suspend/remove sequence, keep removing. */

	/* If remove all monitors is required, we need to continue the process

	 * here because the earlier it was paused when waiting for the

	 * response from controller.

	/* Error 0x0C would be returned if the filter enabled status is

	 * already set to whatever we were trying to set.

	 * Although the default state should be disabled, some controller set

	 * the initial value to enabled. Because there is no way to know the

	 * actual initial value before sending this command, here we also treat

	 * error 0x0C as success.

	/* High_threshold_timeout is not supported,

	 * once high_threshold is reached, events are immediately reported.

 Sampling period from 0x00 to 0xFF are all allowed */

 No additional check needed for pattern-based monitor */

 This function requires the caller holds hdev->lock */

 the length also includes data_type and offset */

 This function requires the caller holds hdev->lock */

 This function requires the caller holds hdev->lock */

 If no matched handle, just remove without telling controller */

 This function requires the caller holds hdev->lock */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2015, Heiner Kallweit <hkallweit1@gmail.com>

 initialize power_led */

/*

   BlueZ - Bluetooth protocol stack for Linux

   Copyright (C) 2000-2001 Qualcomm Incorporated



   Written 2000,2001 by Maxim Krasnyansky <maxk@qualcomm.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Bluetooth SCO sockets. */

 ---- SCO connections ---- */

 ----- SCO socket info ----- */

 ---- SCO timers ---- */

 ---- SCO connections ---- */

/* Delete channel.

 Kill socket */

 Ensure no more work items will run before freeing conn. */

 Update source addr of the socket */

 Check outgoing MTU */

 -------- Socket interface ---------- */

/* Find socket listening on source bdaddr.

 * Returns closest match.

 Exact match. */

 Closest match */

 Close not yet accepted channels */

/* Kill socket (only if zapped and orphan)

 * Must be called on unlocked socket.

 Kill poor orphan */

 Must be called on unlocked socket. */

 Set destination address and psm */

 Wait for an incoming connection. (wake-one). */

 Ignored */

 use CVSD settings as fallback */

 Explicitly check for these values */

 find total buffer size required to copy codec + caps */

		/* Iterate all the codecs supported over SCO and populate

		 * codec data

 find codec capabilities data length */

 copy codec capabilities data */

 Wake up parent */

 ----- SCO interface with lower layer (HCI) ----- */

 Find listening sockets */

/*

   BlueZ - Bluetooth protocol stack for Linux



   Copyright (C) 2014 Intel Corporation



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 fall through*/

/* This is run when CONFIG_BT_SELFTEST=y and CONFIG_BT=m and is just a

 * wrapper to allow running this at module init.

 *

 * If CONFIG_BT_SELFTEST=n, then this code is not compiled at all.

/* This is run when CONFIG_BT_SELFTEST=y and CONFIG_BT=y and is run

 * via late_initcall() as last item in the initialization sequence.

 *

 * If CONFIG_BT_SELFTEST=n, then this code is not compiled at all.

/*

   BlueZ - Bluetooth protocol stack for Linux

   Copyright (C) 2000-2001 Qualcomm Incorporated

   Copyright (C) 2011 ProFUSION Embedded Systems



   Written 2000,2001 by Maxim Krasnyansky <maxk@qualcomm.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Bluetooth HCI core. */

 HCI device list */

 HCI callback list */

 HCI ID Numbering */

 Reset device */

 Read Local Supported Features */

 Read Local Version */

 Read BD Address */

 Read Local Version */

 Read Local Supported Commands */

 Read Local AMP Info */

 Read Data Blk size */

 Read Flow Control Mode */

 Read Location Data */

	/* Read Local Supported Features. Not all AMP controllers

	 * support this so it's placed conditionally in the second

	 * stage init.

 Reset */

 Read Buffer Size (ACL mtu, max pkt, etc.) */

 Read Class of Device */

 Read Local Name */

 Read Voice Setting */

 Read Number of Supported IAC */

 Read Current IAC LAP */

 Clear Event Filters */

 Connection accept timeout ~20 secs */

 Read LE Buffer Size */

 Read LE Local Supported Features */

 Read LE Supported States */

 LE-only controllers have LE implicitly enabled */

	/* The second byte is 0xff instead of 0x9f (two reserved bits

	 * disabled) since a Broadcom 1.2 dongle doesn't respond to the

	 * command otherwise.

	/* CSR 1.1 dongles does not accept any bitfield so don't try to set

	 * any event mask for pre 1.2 devices.

 Flow Specification Complete */

 Use a different default for LE-only devices */

 Command Complete */

 Command Status */

 Hardware Error */

		/* If the controller supports the Disconnect command, enable

		 * the corresponding event. In addition enable packet flow

		 * control related events.

 Disconnection Complete */

 Number of Completed Packets */

 Data Buffer Overflow */

		/* If the controller supports the Read Remote Version

		 * Information command, enable the corresponding event.

			events[1] |= 0x08; /* Read Remote Version Information

					    * Complete

 Encryption Change */

 Encryption Key Refresh Complete */

 Inquiry Result with RSSI */

 Read Remote Extended Features Complete */

 Synchronous Connection Complete */

 Synchronous Connection Changed */

 Sniff Subrating */

 Encryption Key Refresh Complete */

 Extended Inquiry Result */

 Enhanced Flush Complete */

 Link Supervision Timeout Changed */

 IO Capability Request */

 IO Capability Response */

 User Confirmation Request */

 User Passkey Request */

 Remote OOB Data Request */

 Simple Pairing Complete */

 User Passkey Notification */

 Keypress Notification */

		events[7] |= 0x10;	/* Remote Host Supported

					 * Features Notification

 LE Meta-Event */

	/* All Bluetooth 1.2 and later controllers should support the

	 * HCI command for reading the local supported commands.

	 *

	 * Unfortunately some controllers indicate Bluetooth 1.2 support,

	 * but do not have support for this command. If that is the case,

	 * the driver can quirk the behavior and skip reading the local

	 * supported commands.

		/* When SSP is available, then the host features page

		 * should also be available as well. However some

		 * controllers list the max_page as 0 as long as SSP

		 * has not been enabled. To achieve proper debugging

		 * output, force the minimum max_page to 1 at least.

		/* If Extended Inquiry Result events are supported, then

		 * they are clearly preferred over Inquiry Result with RSSI

		 * events.

 LE-only devices do not support explicit enablement */

	/* If Connectionless Peripheral Broadcast central role is supported

	 * enable all necessary events for it.

 Triggered Clock Capture */

 Synchronization Train Complete */

 Peripheral Page Response Timeout */

 CPB Channel Map Change */

	/* If Connectionless Peripheral Broadcast peripheral role is supported

	 * enable all necessary events for it.

 Synchronization Train Received */

 CPB Receive */

 CPB Timeout */

 Truncated Page Complete */

 Enable Authenticated Payload Timeout Expired event if supported */

	/* Some Broadcom based controllers indicate support for Set Event

	 * Mask Page 2 command, but then actually do not support it. Since

	 * the default value is all bits set to zero, the command is only

	 * required if the event mask has to be changed. In case no change

	 * to the event mask is needed, skip this command.

	/* Some older Broadcom based Bluetooth 1.2 controllers do not

	 * support the Read Page Scan Type command. Check support for

	 * this command in the bit mask of supported commands.

 LE Long Term Key Request */

		/* If controller supports the Connection Parameters Request

		 * Link Layer Procedure, enable the corresponding event.

			events[0] |= 0x20;	/* LE Remote Connection

						 * Parameter Request

		/* If the controller supports the Data Length Extension

		 * feature, enable the corresponding event.

 LE Data Length Change */

		/* If the controller supports LL Privacy feature, enable

		 * the corresponding event.

			events[1] |= 0x02;	/* LE Enhanced Connection

						 * Complete

		/* If the controller supports Extended Scanner Filter

		 * Policies, enable the corresponding event.

			events[1] |= 0x04;	/* LE Direct Advertising

						 * Report

		/* If the controller supports Channel Selection Algorithm #2

		 * feature, enable the corresponding event.

			events[2] |= 0x08;	/* LE Channel Selection

						 * Algorithm

		/* If the controller supports the LE Set Scan Enable command,

		 * enable the corresponding advertising report event.

 LE Advertising Report */

		/* If the controller supports the LE Create Connection

		 * command, enable the corresponding event.

 LE Connection Complete */

		/* If the controller supports the LE Connection Update

		 * command, enable the corresponding event.

			events[0] |= 0x04;	/* LE Connection Update

						 * Complete

		/* If the controller supports the LE Read Remote Used Features

		 * command, enable the corresponding event.

			events[0] |= 0x08;	/* LE Read Remote Used

						 * Features Complete

		/* If the controller supports the LE Read Local P-256

		 * Public Key command, enable the corresponding event.

			events[0] |= 0x80;	/* LE Read Local P-256

						 * Public Key Complete

		/* If the controller supports the LE Generate DHKey

		 * command, enable the corresponding event.

 LE Generate DHKey Complete */

		/* If the controller supports the LE Set Default PHY or

		 * LE Set PHY commands, enable the corresponding event.

 LE PHY Update Complete */

		/* If the controller supports LE Set Extended Scan Parameters

		 * and LE Set Extended Scan Enable commands, enable the

		 * corresponding event.

			events[1] |= 0x10;	/* LE Extended Advertising

						 * Report

		/* If the controller supports the LE Extended Advertising

		 * command, enable the corresponding event.

			events[2] |= 0x02;	/* LE Advertising Set

						 * Terminated

 Read LE Advertising Channel TX Power */

			/* HCI TS spec forbids mixing of legacy and extended

			 * advertising commands wherein READ_ADV_TX_POWER is

			 * also included. So do not call it if extended adv

			 * is supported otherwise controller will return

			 * COMMAND_DISALLOWED for extended commands.

 Read LE Min/Max Tx Power*/

 Read LE Accept List Size */

 Clear LE Accept List */

 Read LE Resolving List Size */

 Clear LE Resolving List */

 Set RPA timeout */

 Read LE Maximum Data Length */

 Read LE Suggested Default Data Length */

 Read LE Number of Supported Advertising Sets */

 Read features beyond page 1 if available */

	/* Some Broadcom based Bluetooth controllers do not support the

	 * Delete Stored Link Key command. They are clearly indicating its

	 * absence in the bit mask of supported commands.

	 *

	 * Check the supported commands and only if the command is marked

	 * as supported send it. If not supported assume that the controller

	 * does not have actual support for stored link keys which makes this

	 * command redundant anyway.

	 *

	 * Some controllers indicate that they support handling deleting

	 * stored link keys, but they don't. The quirk lets a driver

	 * just disable this command.

 Set event mask page 2 if the HCI command for it is supported */

 Read local pairing options if the HCI command is supported */

 Get MWS transport configuration if the HCI command is supported */

 Check for Synchronization Train support */

 Enable Secure Connections if supported and configured */

	/* Set erroneous data reporting if supported to the wideband speech

	 * setting value

 Set Suggested Default Data Length to maximum if supported */

 Set Default PHY parameters if command is supported */

	/* HCI_PRIMARY covers both single-mode LE, BR/EDR and dual-mode

	 * BR/EDR/LE type controllers. AMP controllers only need the

	 * first two stages of init.

 Read local codec list if the HCI command is supported */

	/* This function is only called when the controller is actually in

	 * configured state. When the controller is marked as unconfigured,

	 * this initialization procedure is not run.

	 *

	 * It means that it is possible that a controller runs through its

	 * setup phase and then discovers missing settings. If that is the

	 * case, then this function will not be called. It then will only

	 * be called during the config phase.

	 *

	 * So only when in setup phase or config phase, create the debugfs

	 * entries and register the SMP channels.

 Reset */

 Read Local Version */

 Read BD Address */

 Inquiry and Page scans */

 Authentication */

 Encryption */

 Default link policy */

/* Get HCI device by index.

 ---- Inquiry support ---- */

 Entry not in the cache. Add new one. */

 Start Inquiry */

 Restrict maximum inquiry length to 60 seconds */

		/* Wait until Inquiry procedure finishes (HCI_INQUIRY flag is

		 * cleared). If it is interrupted by a signal, return -EINTR.

	/* for unlimited number of responses we will use buffer with

	 * 255 entries

	/* cache_dump can't sleep. Therefore we allocate temp buffer and then

	 * copy it to the user space.

/**

 * hci_dev_get_bd_addr_from_property - Get the Bluetooth Device Address

 *				       (BD_ADDR) for a HCI device from

 *				       a firmware node property.

 * @hdev:	The HCI device

 *

 * Search the firmware node for 'local-bd-address'.

 *

 * All-zero BD addresses are rejected, because those could be properties

 * that exist in the firmware tables, but were not updated by the firmware. For

 * example, the DTS could define 'local-bd-address', with zero BD addresses.

		/* Check for rfkill but allow the HCI setup stage to

		 * proceed (which in itself doesn't cause any RF activity).

		/* Check for valid public address or a configured static

		 * random address, but let the HCI setup proceed to

		 * be able to determine if there is a public address

		 * or not.

		 *

		 * In case of user channel usage, it is not important

		 * if a public address or static random address is

		 * available.

		 *

		 * This check is only valid for BR/EDR controllers

		 * since AMP controllers do not have an address.

		/* The transport driver can set the quirk to mark the

		 * BD_ADDR invalid before creating the HCI device or in

		 * its setup callback.

				/* If setting of the BD_ADDR from the device

				 * property succeeds, then treat the address

				 * as valid even if the invalid BD_ADDR

				 * quirk indicates otherwise.

		/* The transport driver can set these quirks before

		 * creating the HCI device or in its setup callback.

		 *

		 * For the invalid BD_ADDR quirk it is possible that

		 * it becomes a valid address if the bootloader does

		 * provide it (see above).

		 *

		 * In case any of them is set, the controller has to

		 * start up as unconfigured.

		/* For an unconfigured controller it is required to

		 * read at least the version information provided by

		 * the Read Local Version Information command.

		 *

		 * If the set_bdaddr driver callback is provided, then

		 * also the original Bluetooth public device address

		 * will be read using the Read BD Address command.

		/* If public address change is configured, ensure that

		 * the address gets programmed. If the driver does not

		 * support changing the public address, fail the power

		 * on procedure.

	/* If the HCI Reset command is clearing all diagnostic settings,

	 * then they need to be reprogrammed after the init procedure

	 * completed.

 Init failed, cleanup */

		/* Since hci_rx_work() is possible to awake new cmd_work

		 * it should be flushed first to avoid unexpected call of

		 * hci_cmd_work()

 ---- HCI ioctl helpers ---- */

	/* Devices that are marked as unconfigured can only be powered

	 * up as user channel. Trying to bring them up as normal devices

	 * will result into a failure. Only user channel operation is

	 * possible.

	 *

	 * When this function is called for a user channel, the flag

	 * HCI_USER_CHANNEL will be set first before attempting to

	 * open the device.

	/* We need to ensure that no other power on/off work is pending

	 * before proceeding to call hci_dev_do_open. This is

	 * particularly important if the setup procedure has not yet

	 * completed.

	/* After this call it is guaranteed that the setup procedure

	 * has finished. This means that error conditions like RFKILL

	 * or no valid public or static random address apply.

	/* For controllers not using the management interface and that

	 * are brought up using legacy ioctl, set the HCI_BONDABLE bit

	 * so that pairing works for them. Once the management interface

	 * is in use this bit will be cleared again and userspace has

	 * to explicitly enable it.

 This function requires the caller holds hdev->lock */

 Execute vendor specific shutdown routine */

 Flush RX and TX works */

	/* Avoid potential lockdep warnings from the *_flush() calls by

	 * ensuring the workqueue is empty up front.

 Reset device */

 flush cmd  work */

 Drop queues */

 Drop last sent command */

	/* After this point our queues are empty

 Clear flags */

 Controller radio is available but is currently powered down */

 Drop queues */

	/* Avoid potential lockdep warnings from the *_flush() calls by

	 * ensuring the workqueue is empty up front.

 In case this was disabled through mgmt */

 Auth must be enabled first */

		/* Ensure that the connectable and discoverable states

		 * get correctly modified as this was a non-mgmt change.

		/* When the auto-off is configured it means the transport

		 * is running, but in that case still indicate that the

		 * device is actually down.

	/* When the auto-off is configured it means the transport

	 * is running, but in that case still indicate that the

	 * device is actually down.

 ---- Interface to HCI drivers ---- */

	/* During the HCI setup phase, a few error conditions are

	 * ignored and they need to be checked now. If they are still

	 * valid, it is important to turn the device back off.

		/* For unconfigured devices, set the HCI_RAW flag

		 * so that userspace can easily identify them.

		/* For fully configured devices, this will send

		 * the Index Added event. For unconfigured devices,

		 * it will send Unconfigued Index Added event.

		 *

		 * Devices with HCI_QUIRK_RAW_DEVICE are ignored

		 * and no event will be send.

		/* When the controller is now configured, then it

		 * is important to clear the HCI_RAW flag.

		/* Powering on the controller with HCI_CONFIG set only

		 * happens with the transition from unconfigured to

		 * configured. This will send the Index Added event.

 Legacy key */

 Debug keys are insecure so don't store them persistently */

 Changed combination key and there's no previous one */

 Security mode 3 case */

 BR/EDR key derived using SC from an LE link */

 Neither local nor remote side had no-bonding as requirement */

 Local side had dedicated bonding as requirement */

 Remote side had dedicated bonding as requirement */

	/* If none of the above criteria match, then don't store the key

 Identity Address must be public or static random */

	/* Some buggy controller combinations generate a changed

	 * combination key for legacy pairing even when there's no

 Convert to HCI addr type which struct smp_ltk uses */

 HCI command timer function */

 HCI ncmd timer function */

	/* During HCI_INIT phase no events can be injected if the ncmd timer

	 * triggers since the procedure has its own timeout handling.

 This is an irrecoverable state, inject hardware error event */

 This function requires the caller holds hdev->lock */

 This function requires the caller holds hdev->lock */

 This function requires the caller holds hdev->lock */

 This function requires the caller holds hdev->lock */

 This function requires the caller holds hdev->lock */

 This function requires the caller holds hdev->lock */

 If advertisement doesn't exist, we can't modify its data */

 This function requires the caller holds hdev->lock */

		/* Instance 0 always manages the "Tx Power" and "Flags"

		 * fields

		/* For instance 0, the HCI_ADVERTISING_CONNECTABLE setting

		 * corresponds to the "connectable" instance flag.

 Return 0 when we got an invalid instance identifier. */

 Instance 0x00 always set local name */

 This function requires the caller holds hdev->lock */

/* Frees the monitor structure and do some bookkeepings.

 * This function requires the caller holds hdev->lock.

/* Assigns handle to a monitor, and if offloading is supported and power is on,

 * also attempts to forward the request to the controller.

 * Returns true if request is forwarded (result is pending), false otherwise.

 * This function requires the caller holds hdev->lock.

 Message was not forwarded to controller - not an error */

/* Attempts to tell the controller and free the monitor. If somehow the

 * controller doesn't have a corresponding handle, remove anyway.

 * Returns true if request is forwarded (result is pending), false otherwise.

 * This function requires the caller holds hdev->lock.

 also goes here when powered off */

 In case no matching handle registered, just free the monitor */

/* Returns true if request is forwarded (result is pending), false otherwise.

 * This function requires the caller holds hdev->lock.

/* Returns true if request is forwarded (result is pending), false otherwise.

 * This function requires the caller holds hdev->lock.

 This function requires the caller holds hdev->lock */

 This function requires the caller holds hdev->lock */

 This function requires the caller holds hdev->lock */

 This function requires the caller holds hdev->lock */

 This function requires the caller holds hdev->lock */

 This function requires the caller holds hdev->lock */

		/* If trying to establish one time connection to disabled

		 * device, leave the params, but mark them as just once.

 This function requires the caller holds hdev->lock */

/* Copy the Identity Address of the controller.

 *

 * If the controller has a public BD_ADDR, then by default use that one.

 * If this is a LE only controller without a public address, default to

 * the static random address.

 *

 * For debugging purposes it is possible to force controllers with a

 * public address to use the static random address instead.

 *

 * In case BR/EDR has been disabled on a dual-mode controller and

 * userspace has configured a static address, then that address

 * becomes the identity address instead of the public BR/EDR address.

 Alloc HCI device */

 Fixme: May need ALIGN-ment? */

 One IAC support is mandatory */

 No Input No Output */

 Default to internal use */

 Default to disable */

 default 1.28 sec page scan */

 Free HCI device */

 will free via device release */

 Register HCI device */

	/* Do not allow HCI_AMP devices to register at index 0,

	 * so the index can be used as the AMP controller ID.

		/* Assume BR/EDR support until proven otherwise (such as

		 * through reading supported features during init.

	/* Devices that are marked for raw-only usage are unconfigured

	 * and should not be included in normal operation.

 Unregister HCI device */

	/* mgmt_index_removed should take care of emptying the

 Actual cleanup is deferred until hci_release_dev(). */

 Release HCI device */

 Suspend HCI device */

 Suspend should only act on when powered. */

 If powering down, wait for completion. */

	/* Suspend consists of two actions:

	 *  - First, disconnect everything and make the controller not

	 *    connectable (disabling scanning)

	 *  - Second, program event filter/accept list and enable scan

 Only configure accept list if device may wakeup. */

	/* We always allow suspend even if suspend preparation failed and

	 * attempt to recover in resume.

 Resume HCI device */

 Resume should only act on when powered. */

 If powering down don't attempt to resume */

 Reset HCI device */

 Send Hardware Error to upper stack */

 Receive frame from HCI drivers */

 Incoming skb */

 Time stamp */

 Receive diagnostic message from HCI drivers */

 Mark as diagnostic packet */

 Time stamp */

 ---- Interface to upper protocols ---- */

 Time stamp */

 Send copy to monitor */

 Send copy to the sockets */

 Get rid of skb owner, prior to sending to the driver. */

 Send HCI command */

	/* Stand-alone HCI commands must be flagged as

	 * single-command requests.

		/* A controller receiving a command shall respond with either

		 * a Command Status Event or a Command Complete Event.

		 * Therefore, all standard HCI commands must be sent via the

		 * standard API, using hci_send_cmd or hci_cmd_sync helpers.

		 * Some vendors do not comply with this rule for vendor-specific

		 * commands and do not return any event. We want to support

		 * unresponded commands for such cases only.

 Get data from the previously sent command */

 Send HCI command and wait for command complete event */

 Send ACL data */

 Non fragmented */

 Fragmented */

		/* Queue all fragments atomically. We need to use spin_lock_bh

		 * here because of 6LoWPAN links, as there this function is

		 * called from softirq and using normal spin lock could cause

		 * deadlocks.

 Send SCO data */

 ---- HCI TX task (outgoing data) ---- */

 HCI Connection scheduler */

	/* We don't have to lock device here. Connections are always

 Kill stalled connections */

 Calculate count of blocks used by this packet */

		/* ACL tx timeout must be longer than maximum

 Schedule SCO */

 Stop if priority has changed */

 Send pending SCO packets right away */

 Stop if priority has changed */

 No ACL link over BR/EDR controller */

 No AMP link over AMP controller */

 Stop if priority has changed */

 Send pending SCO packets right away */

 Schedule queues and send stuff to HCI driver */

 Send next queued raw (unknown type) packet */

 ----- HCI RX task (incoming data processing) ----- */

 ACL data packet */

 Send to upper protocol */

 SCO data packet */

 Send to upper protocol */

	/* If the completed command doesn't match the last one that was

	 * sent we need to do special handling of it.

		/* Some CSR based controllers generate a spontaneous

		 * reset complete event during init and any pending

		 * command will never be completed. In such a case we

		 * need to resend whatever was the last sent

		 * command.

 If we reach this point this event matches the last command sent */

	/* If the command succeeded and there's still more commands in

	 * this request the request is not yet complete.

	/* If this was the last command in a request the complete

	 * callback would be found in hdev->sent_cmd instead of the

	 * command queue (hdev->cmd_q).

 Remove all pending commands belonging to this request */

 Send copy to monitor */

 Send copy to the sockets */

		/* If the device has been opened in HCI_USER_CHANNEL,

		 * the userspace has exclusive access to device.

		 * When device is HCI_INIT, we still need to process

		 * the data packets to the driver in order

		 * to complete its setup().

 Don't process data packets in this states. */

 Process frame */

 Send queued commands */

 SPDX-License-Identifier: GPL-2.0-only

/*

   Copyright (c) 2013-2014 Intel Corp.



 for the compression support */

/* The devices list contains those devices that we are acting

 * as a proxy. The BT 6LoWPAN device is a virtual device that

 * connects to the Bluetooth LE device. The real connection to

 * BT device is done via l2cap layer. There exists one

 * virtual device / one BT 6LoWPAN network (=hciX device).

 * The list contains struct lowpan_dev elements.

/* We are listening incoming connections via this channel

 peer addresses in various formats */

 number of items in peers list */

			/* There is neither route nor gateway,

			 * probably the destination is a direct peer.

			/* There is a known gateway

		/* We need to remember the address because it is needed

		 * by bt_xmit() when sending the packet. In bt_xmit(), the

		 * destination routing info is not set.

 use the neighbour cache for matching addresses assigned by SLAAC */

 check that it's our buffer */

 Pull off the 1-byte of 6lowpan header. */

		/* Copy the packet so that the IPv6 header is

		 * properly aligned.

 Packet from BT LE device */

		/* The packet might be sent to 6lowpan interface

		 * because of routing (either via default route

		 * or user set route) so get peer according to

		 * the destination address.

 Packet to BT LE device */

	/* Remember the skb so that we can send EAGAIN to the caller if

	 * we run out of credits.

	/* We must take a copy of the skb before we modify/replace the ipv6

	 * header as the header could be used elsewhere

	/* Return values from setup_header()

	 *  <0 - error, packet is dropped

	 *   0 - this is a multicast packet

	 *   1 - this is unicast packet

		/* We need to send the packet to every device behind this

		 * interface.

 send neighbour adv at startup */

 Notifying peers about us needs to be done without locks held */

 The entry pointer is deleted by the netdev destructor. */

		/* If conn is set, then the netdev is also there and we should

		 * not remove it.

	/* Note that we must allocate using GFP_ATOMIC here as

	 * this function is called originally from netdev hard xmit

	 * function in atomic context.

 The LE_PUBLIC address type is ignored because of BDADDR_ANY */

	/* We make a separate list of peers as the close_cb() will

	 * modify the device peers list so it is better not to mess

	 * with the same list at the same time.

		/* Disconnect existing connections if 6lowpan is

		 * disabled

	/* We make a separate list of devices because the unregister_netdev()

	 * will call device_event() which will also want to modify the same

	 * devices list.

/*

   CMTP implementation for Linux Bluetooth stack (BlueZ).

   Copyright (C) 2002-2003 Marcel Holtmann <marcel@holtmann.org>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Interoperability selector (Bluetooth Device Management) */

/*

   CMTP implementation for Linux Bluetooth stack (BlueZ).

   Copyright (C) 2002-2003 Marcel Holtmann <marcel@holtmann.org>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

		/*

		 * wait_woken() performs the necessary memory barriers

		 * for us; see the header comment for this primitive.

			/* Caller will call fput in case of failure, and so

			 * will cmtp_session kthread.

 Flush the transmit queue */

 Stop session thread */

		/*

		 * See the comment preceding the call to wait_woken()

		 * in cmtp_session().

/*

   CMTP implementation for Linux Bluetooth stack (BlueZ).

   Copyright (C) 2002-2003 Marcel Holtmann <marcel@holtmann.org>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

/*

   HIDP implementation for Linux Bluetooth stack (BlueZ).

   Copyright (C) 2003-2004 Marcel Holtmann <marcel@holtmann.org>

   Copyright (C) 2013 David Herrmann <dh.herrmann@gmail.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 assemble skb, queue message on @transmit and wake up the session thread */

 Keyboard report */

		/* If all the key codes have been set to 0x01, it means

 Mouse report */

 Set up our wait, and send the report request to the device. */

	/* Wait for the return of the report. The returned report

 timeout */

 signal */

 Device returned a HANDSHAKE, indicating  protocol error. */

 Set up our wait, and send the report request to the device. */

 Wait for the ACK from the device. */

 timeout */

 signal */

	/* The HIDP user-space API only contains calls to add and remove

	 * devices. There is no way to forward events of any kind. Therefore,

	 * we have to forcefully disconnect a device on idle-timeouts. This is

	 * unfortunate and weird API design, but it is spec-compliant and

	 * required for backwards-compatibility. Hence, on idle-timeout, we

	 * signal driver-detach events, so poll() will be woken up with an

	 * error-condition on both sockets.

 default condition */

 FIXME: Call into SET_ GET_ handlers here */

 FIXME: Call into SET_ GET_ handlers here */

		/* Device requests a reboot, as this is the only way this error

 Wake up the waiting thread. */

 Flush the transmit queues */

 Returns true if the passed-in skb should be freed by the caller. */

 hidp_get_raw_report() is waiting on this report. */

 dequeue message from @transmit and send via @sock */

/* This function sets up the hid device. It does not add it

	/* NOTE: Some device modules depend on the dst address being stored in

	 * uniq. Please be aware of this before making changes to this behavior.

 True if device is blocked in drivers/hid/hid-quirks.c */

 initialize session devices */

 destroy session devices */

 add HID/input devices to their underlying bus systems */

	/* Both HID and input systems drop a ref-count when unregistering the

	 * device but they don't take a ref-count when registering them. Work

	 * around this by explicitly taking a refcount during registration

 remove HID/input devices from their bus systems */

/*

 * Asynchronous device registration

 * HID device drivers might want to perform I/O during initialization to

 * detect device types. Therefore, call device registration in a separate

 * worker so the HIDP thread can schedule I/O operations.

 * Note that this must be called after the worker thread was initialized

 * successfully. This will then add the devices and increase session state

 * on success, otherwise it will terminate the session thread.

/*

 * Create new session object

 * Allocate session object, initialize static fields, copy input data into the

 * object and take a reference to all sub-objects.

 * This returns 0 on success and puts a pointer to the new session object in

 * \out. Otherwise, an error code is returned.

 * The new session object has an initial ref-count of 1.

 object and runtime management */

 connection management */

 device management */

 session data */

 increase ref-count of the given session by one */

 release callback */

 decrease ref-count of the given session by one */

/*

 * Search the list of active sessions for a session with target address

 * \bdaddr. You must hold at least a read-lock on \hidp_session_sem. As long as

 * you do not release this lock, the session objects cannot vanish and you can

 * safely take a reference to the session yourself.

/*

 * Same as __hidp_session_find() but no locks must be held. This also takes a

 * reference of the returned session (if non-NULL) so you must drop this

 * reference if you no longer use the object.

/*

 * Start session synchronously

 * This starts a session thread and waits until initialization

 * is done or returns an error if it couldn't be started.

 * If this returns 0 the session thread is up and running. You must call

 * hipd_session_stop_sync() before deleting any runtime resources.

/*

 * Terminate session thread

 * Wake up session thread and notify it to stop. This is asynchronous and

 * returns immediately. Call this whenever a runtime error occurs and you want

 * the session to stop.

 * Note: wake_up_interruptible() performs any necessary memory-barriers for us.

	/*

	 * See the comment preceding the call to wait_woken()

	 * in hidp_session_run().

/*

 * Probe HIDP session

 * This is called from the l2cap_conn core when our l2cap_user object is bound

 * to the hci-connection. We get the session via the \user object and can now

 * start the session thread, link it into the global session list and

 * schedule HID/input device registration.

 * The global session-list owns its own reference to the session object so you

 * can drop your own reference after registering the l2cap_user object.

 check that no other session for this device exists */

 HID device registration is async to allow I/O during probe */

/*

 * Remove HIDP session

 * Called from the l2cap_conn core when either we explicitly unregistered

 * the l2cap_user object or if the underlying connection is shut down.

 * We signal the hidp-session thread to shut down, unregister the HID/input

 * devices and unlink the session from the global list.

 * This drops the reference to the session that is owned by the global

 * session-list.

 * Note: We _must_ not synchronosly wait for the session-thread to shut down.

 * This is, because the session-thread might be waiting for an HCI lock that is

 * held while we are called. Therefore, we only unregister the devices and

 * notify the session-thread to terminate. The thread itself owns a reference

 * to the session object so it can safely shut down.

/*

 * Session Worker

 * This performs the actual main-loop of the HIDP worker. We first check

 * whether the underlying connection is still alive, then parse all pending

 * messages and finally send all outstanding messages.

		/*

		 * This thread can be woken up two ways:

		 *  - You call hidp_session_terminate() which sets the

		 *    session->terminate flag and wakes this thread up.

		 *  - Via modifying the socket state of ctrl/intr_sock. This

		 *    thread is woken up by ->sk_state_changed().

 parse incoming intr-skbs */

 send pending intr-skbs */

 parse incoming ctrl-skbs */

 send pending ctrl-skbs */

		/*

		 * wait_woken() performs the necessary memory barriers

		 * for us; see the header comment for this primitive.

/*

 * HIDP session thread

 * This thread runs the I/O for a single HIDP session. Startup is synchronous

 * which allows us to take references to ourself here instead of doing that in

 * the caller.

 * When we are ready to run we notify the caller and call hidp_session_run().

 initialize runtime environment */

	/* This memory barrier is paired with wq_has_sleeper(). See

 notify synchronous startup that we're ready */

 run session */

 cleanup runtime environment */

	/*

	 * If we stopped ourself due to any internal signal, we should try to

	 * unregister our own session here to avoid having it linger until the

	 * parent l2cap_conn dies or user-space cleans it up.

	 * This does not deadlock as we don't do any synchronous shutdown.

	 * Instead, this call has the same semantics as if user-space tried to

	 * delete the session.

 early session check, we check again during session registration */

/*

   HIDP implementation for Linux Bluetooth stack (BlueZ).

   Copyright (C) 2003-2004 Marcel Holtmann <marcel@holtmann.org>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Connected control socket */

 Connected interrupt socket */

/*

   BNEP implementation for Linux Bluetooth stack (BlueZ).

   Copyright (C) 2001-2002 Inventel Systemes

   Written 2001-2002 by

	Clment Moreau <clement.moreau@inventel.fr>

	David Libault  <david.libault@inventel.fr>



   Copyright (C) 2002 Maxim Krasnyansky <maxk@qualcomm.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 (IPv4, ARP)  */

 (RARP, AppleTalk) */

 (IPX, IPv6) */

 Always send broadcast */

 Add address ranges to the multicast hash */

 Iterate from a1 to a2 */

 Increment a1 */

 Ignore these for now */

 Successful response should be sent only once */

 Unknown extension, skip it. */

 BNEP_GENERAL */

 BNEP_CONTROL */

 BNEP_COMPRESSED */

 BNEP_COMPRESSED_SRC_ONLY */

 BNEP_COMPRESSED_DST_ONLY */

 Verify and pull ctrl message since it's already processed */

 Pull: ctrl type (1 b), len (1 b), data (len bytes) */

 Pull: ctrl type (1 b), len (2 b), data (len bytes) */

 Verify and pull out header */

 Strip 802.1p header */

	/* We have to alloc new skb and copy data here :(. Because original skb

 Decompress header and construct ether frame */

 Control frame sent by us */

 FIXME: linearize skb */

 RX */

 TX */

		/*

		 * wait_woken() performs the necessary memory barriers

		 * for us; see the header comment for this primitive.

 Cleanup session */

 Delete network device */

 Wakeup user-space polling for socket errors */

 Release the socket */

 session struct allocated as private part of net_device */

	/* This is rx header therefore addresses are swapped.

	/* Set default mc filter to not filter out any mc addresses

	 * as defined in the BNEP specification (revision 0.95a)

	 * http://grouper.ieee.org/groups/802/15/Bluetooth/BNEP.pdf

 Set default protocol filter */

 Session thread start failed, gotta cleanup. */

/*

   BNEP implementation for Linux Bluetooth stack (BlueZ).

   Copyright (C) 2001-2002 Inventel Systemes

   Written 2001-2002 by

	Clment Moreau <clement.moreau@inventel.fr>

	David Libault  <david.libault@inventel.fr>



   Copyright (C) 2002 Maxim Krasnyansky <maxk@qualcomm.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

 Request all addresses */

 FIXME: We should group addresses here. */

 Determine ether protocol. Based on eth_type_trans. */

	/*

	 * We cannot send L2CAP packets from here as we are potentially in a bh.

	 * So we have to queue them and wake up session thread which is sleeping

	 * on the sk_sleep(sk).

		/* Stop queuing.

/*

   BNEP implementation for Linux Bluetooth stack (BlueZ).

   Copyright (C) 2001-2002 Inventel Systemes

   Written 2001-2002 by

	David Libault  <david.libault@inventel.fr>



   Copyright (C) 2002 Maxim Krasnyansky <maxk@qualcomm.com>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

/*

   RFCOMM implementation for Linux Bluetooth stack (BlueZ).

   Copyright (C) 2002 Maxim Krasnyansky <maxk@qualcomm.com>

   Copyright (C) 2002 Marcel Holtmann <marcel@holtmann.org>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

/*

 * Bluetooth RFCOMM core.

 ---- RFCOMM frame parsing macros ---- */

 MCC macros */

 RPN macros */

 ---- RFCOMM FCS computation ---- */

 reversed, 8-bit, poly=0x07 */

 CRC on 2 bytes */

 FCS on 2 bytes */

 FCS on 3 bytes */

 Check FCS */

 ---- L2CAP callbacks ---- */

 ---- RFCOMM DLCs ---- */

 Check if DLCI already exists */

		/* if closing a dlc in a session that hasn't been started,

		 * just close and unlink the dlc

	/* after waiting on the mutex check the session still exists

	 * then check the dlc still exists

 Queue all fragments atomically. */

/*

   Set/get modem status functions use _local_ status i.e. what we report

   to the other side.

   Remote status is provided by dlc->modem_status() callback.

 ---- RFCOMM sessions ---- */

	/* Do not increment module usage count for listening sessions.

 Close all dlcs */

 Set L2CAP options */

 Set MTU to 0 so L2CAP can auto select the MTU */

 ---- RFCOMM frame sending ---- */

 Type that we didn't like */

 ---- RFCOMM frame reception ---- */

 Data channel */

 Control channel */

 Data DLC */

 Check if DLC exists */

 DLC was previously opened by PN request */

 Notify socket layer about incoming connection */

 PN request */

 PN response */

		/* PN request for non existing DLC.

 This is a request, return default (according to ETSI TS 07.10) settings */

	/* Check for sane values, ignore/accept bit_rate, 8 bits, 1 stop bit,

	/* We should probably do something with this information here. But

	 * for now it's sufficient just to reply -- Bluetooth 1.1 says it's

 no session, so free socket data */

 Trim FCS */

 ---- Connection and data processing ---- */

/* Send data queued for the DLC.

 * Return number of frames left in the queue.

 Send pending MSC */

		/* CFC enabled.

		/* CFC disabled.

		/* We're out of TX credits.

 Get data directly from socket receive queue without copying it. */

	/* Fast check for a new connection.

 Set our callbacks */

		/* We should adjust MTU on incoming sessions.

		/* We can adjust MTU on outgoing sessions.

 Create socket */

 Bind socket */

 Set L2CAP options */

 Set MTU to 0 so L2CAP can auto select the MTU */

 Start listening on the socket */

 Add listening session */

 Process stuff */

 ---- Initialization ---- */

/*

   RFCOMM implementation for Linux Bluetooth stack (BlueZ).

   Copyright (C) 2002 Maxim Krasnyansky <maxk@qualcomm.com>

   Copyright (C) 2002 Marcel Holtmann <marcel@holtmann.org>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

/*

 * RFCOMM sockets.

/* ---- DLC callbacks ----

 *

 * called under rfcomm_dlc_lock()

		/* We have to drop DLC lock here, otherwise

 ---- Socket functions ---- */

/* Find socket with channel and source bdaddr.

 * Returns closest match.

 Exact match. */

 Closest match */

 Detach DLC if it's owned by this socket */

 Close not yet accepted dlcs */

/* Kill socket (only if zapped and orphan)

 * Must be called on unlocked socket.

 Kill poor orphan */

/* Close socket.

 * Must be called on unlocked socket.

 Save source address */

 Wait for an incoming connection. (wake-one). */

/* ---- RFCOMM core layer callbacks ----

 *

 * called under rfcomm_lock()

 Check if we have socket listening on channel */

 Check for backlog size */

 Accept connection and return socket DLC */

/*

   RFCOMM implementation for Linux Bluetooth stack (BlueZ).

   Copyright (C) 2002 Maxim Krasnyansky <maxk@qualcomm.com>

   Copyright (C) 2002 Marcel Holtmann <marcel@holtmann.org>



   This program is free software; you can redistribute it and/or modify

   it under the terms of the GNU General Public License version 2 as

   published by the Free Software Foundation;



   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.

   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY

   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES

   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN

   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF

   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.



   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,

   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS

   SOFTWARE IS DISCLAIMED.

/*

 * RFCOMM TTY.

 magic number for rfcomm struct */

 whole lotta rfcomm devices */

 device node major id of the usb/bluetooth.c driver */

 don't export to userspace */

 ---- Device functions ---- */

 Detach DLC if it's owned by this dev */

	/* It's safe to call module_put() here because socket still

 device-specific initialization: open the dlc */

 we block the open until the dlc->state becomes BT_CONNECTED */

 device-specific cleanup: close the dlc */

 close the dlc */

	/* The lookup results are unsafe to access without the

	 * hci device lock (FIXME: why is this not documented?)

	/* Just because the acl link is in the hash table is no

	 * guarantee the sysfs device has been added ...

	/* It's safe to call __module_get() here because socket already

 ---- Send buffer ---- */

 Limit the outstanding number of packets not yet sent to 40 */

 ---- Device IOCTLs ---- */

 Socket must be connected */

 Validate the channel is unused */

		/* DLC is now used by device.

 only release once */

 Shut down TTY synchronously before freeing rfcomm_dev */

 ---- DLC callbacks ---- */

 ---- TTY functions ---- */

/* do the reverse of install, clearing the tty fields and releasing the

 * reference to tty_port

	/*

	 * purge the dlc->tx_queue to avoid circular dependencies

	 * between dev and dlc

/* we acquire the tty_port reference since it's here the tty is first used

 * by setting the termios. We also populate the driver_data field and install

 * the tty port

 Attach TTY and open DLC */

 install the tty_port */

	/* take over the tty_port reference if the port was created with the

	 * flag RFCOMM_RELEASE_ONHUP. This will force the release of the port

	 * when the last process closes the tty. The behaviour is expected by

	 * userspace.

	/*

	 * FIXME: rfcomm should use proper flow control for

	 * received data. This hack will be unnecessary and can

	 * be removed when that's implemented

 ioctls which we must ignore */

 Handle turning off CRTSCTS */

 Parity on/off and when on, odd/even */

 Mark and space parity are not supported! */

 Setting the x_on / x_off characters */

 Handle setting of stop bits */

	/* POSIX does not support 1.5 stop bits and RFCOMM does not

	 * support 2 stop bits. So a request for 2 stop bits gets

 Handle number of data bits [5-8] */

 Handle baudrate settings */

 9600 is standard accordinag to the RFCOMM specification */

 ---- TTY structure ---- */

/*

 * Copyright (c) 2007, 2020 Oracle and/or its affiliates.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 for DMA_*_DEVICE */

/*

 * XXX

 *  - build with sparse

 *  - should we detect duplicate keys on a socket?  hmm.

 *  - an rdma is an mlock, apply rlimit?

/*

 * get the number of pages by looking at the page indices that the start and

 * end addresses fall in.

 *

 * Returns 0 if the vec is invalid.  It is invalid if the number of bytes

 * causes the address to wrap or overflows an unsigned int.  This comes

 * from being stored in the 'length' member of 'struct scatterlist'.

/*

 * Destroy the transport-specific part of a MR.

/*

 * By the time this is called we can't have any more ioctls called on

 * the socket so we don't need to worry about racing with others.

 Release any MRs associated with this socket */

/*

 * Helper function to pin user pages.

 XXX not a great errno */

	/* If the combination of the addr and size requested for this memory

	 * region causes an integer overflow, return error.

	/* Restrict the size of mr irrespective of underlying transport

	 * To account for unaligned mr regions, subtract one from nr_pages

 XXX clamp nr_pages to limit the size of this alloc? */

	/*

	 * Pin the pages that make up the user buffer and transfer the page

	 * pointers to the mr's sg array.  We check to see if we've mapped

	 * the whole region after transferring the partial page references

	 * to the sg array so that we can have one page ref cleanup path.

	 *

	 * For now we have no flag that tells us whether the mapping is

	 * r/o or r/w. We need to assume r/w, or we'll do a lot of RDMA to

	 * the zero page.

 Stick all pages into the scatterlist */

	/* Obtain a transport specific MR. If this succeeds, the

	 * s/g list is now owned by the MR.

	 * Note that dma_map() implies that pending writes are

		/* In ODP case, we don't GUP pages, so don't need

		 * to release anything.

	/* The user may pass us an unaligned address, but we can only

	 * map page aligned regions. So we keep the offset, and build

	 * a 64bit cookie containing <R_Key, offset> and pass that

	/* Inserting the new MR into the rbtree bumps its

	/*

	 * Initially, just behave like get_mr().

	 * TODO: Implement get_mr as wrapper around this

	 *	 and deprecate it.

/*

 * Free the MR indicated by the given R_Key

 Special case - a null cookie means flush all unused MRs */

	/* Look up the MR given its R_key and remove it from the rbtree

	 * so nobody else finds it.

	 * This should also prevent races with rds_rdma_unuse.

/*

 * This is called when we receive an extension header that

 * tells us this MR was used. It allows us to implement

 * use_once semantics

	/* Get a reference so that the MR won't go away before calling

	 * sync_mr() below.

	/* If it is going to be freed, remove it from the tree now so

	 * that no other thread can find it and free it.

	/* May have to issue a dma_sync on this memory region.

	 * Note we could avoid this if the operation was a RDMA READ,

 Release the reference held above. */

	/* If the MR was marked as invalidate, this will

			/* Mark page dirty if it was possibly modified, which

			 * is the case for a RDMA_READ which copies from remote

			 * to local memory

	/* Mark page dirty if it was possibly modified, which

	 * is the case for a RDMA_READ which copies from remote

/*

 * Count the number of pages needed to describe an incoming iovec array.

 figure out the number of pages in the vector */

		/*

		 * nr_pages for one entry is limited to (UINT_MAX>>PAGE_SHIFT)+1,

		 * so tot_pages cannot overflow without first going negative.

 figure out the number of pages in the vector */

		/*

		 * nr_pages for one entry is limited to (UINT_MAX>>PAGE_SHIFT)+1,

		 * so tot_pages cannot overflow without first going negative.

/*

 * The application asks for a RDMA transfer.

 * Extract all arguments and set up the rdma_op

 XXX not a great errno */

 odp-mr is not supported for multiple requests within one message */

		/* We allocate an uninitialized notifier here, because

		 * we don't want to do that in the completion handler. We

		 * would have to use GFP_ATOMIC there, and don't want to deal

		 * with failed allocations.

	/* The cookie contains the R_Key of the remote memory region, and

	 * optionally an offset into it. This is how we implement RDMA into

	 * unaligned memory.

	 * When setting up the RDMA, we need to add that offset to the

	 * destination address (which is really an offset into the MR)

	 * FIXME: We may want to move this into ib_rdma.c

 don't need to check, rds_rdma_pages() verified nr will be +nonzero */

		/* If it's a WRITE operation, we want to pin the pages for reading.

		 * If it's a READ operation, we need to pin the pages for writing.

/*

 * The application wants us to pass an RDMA destination (aka MR)

 * to the remote

	/* We are reusing a previously mapped MR here. Most likely, the

	 * application has written to the buffer, so we need to explicitly

	 * flush those writes to RAM. Otherwise the HCA may not see them

	 * when doing a DMA from that buffer.

 invalid r_key */

/*

 * The application passes us an address range it wants to enable RDMA

 * to/from. We map the area, and save the <R_Key,offset> pair

 * in rm->m_rdma_cookie. This causes it to be sent along to the peer

 * in an extension header.

/*

 * Fill in rds_message for an atomic request.

 Nonmasked & masked cmsg ops converted to masked hw ops */

 should never happen */

 verify 8 byte-aligned */

		/* We allocate an uninitialized notifier here, because

		 * we don't want to do that in the completion handler. We

		 * would have to use GFP_ATOMIC there, and don't want to deal

		 * with failed allocations.

/*

 * Copyright (c) 2006, 2019 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * The entire 'from' list, including the from element itself, is put on

 * to the tail of the 'to' list.

 fwd decl */

 Recycle frag and attached recv buffer f_sg */

 Recycle inc after freeing attached frags */

 Free attached frags */

	/*

	 * ibinc was taken from recv if recv contained the start of a message.

	 * recvs that were continuations will still have this allocated.

 leak! */

	/* We don't use wait_on_bit()/wake_up_bit() because our waking is in a

	 * hot path and finding waiters is very rare.  We don't want to walk

	 * the system-wide hashed waitqueue buckets in the fast path only to

	 * almost never find waiters.

/*

 * This tries to allocate and post unused work requests after making sure that

 * they have all the allocations they need to queue received fragments into

 * sockets.

	/* the goal here is to just make sure that someone, somewhere

	 * is posting buffers.  If we can't get the refill lock,

	 * let them do their thing

 XXX when can this fail? */

 We're doing flow control - update the window. */

	/* if we're called from the softirq handler, we'll be GFP_NOWAIT.

	 * in this case the ring being low is going to lead to more interrupts

	 * and we can safely let the softirq code take care of it unless the

	 * ring is completely empty.

	 *

	 * if we're called from krdsd, we'll be GFP_KERNEL.  In this case

	 * we might have raced with the softirq code while we had the refill

	 * lock held.  Use rds_ib_ring_low() instead of ring_empty to decide

	 * if we should requeue.

/*

 * We want to recycle several types of recv allocations, like incs and frags.

 * To use this, the *_free() function passes in the ptr to a list_head within

 * the recyclee, as well as the cache to put it on.

 *

 * First, we put the memory on a percpu list. When this reaches a certain size,

 * We move it to an intermediate non-percpu list in a lockless manner, with some

 * xchg/compxchg wizardry.

 *

 * N.B. Instead of a list_head as the anchor, we use a single pointer, which can

 * be NULL and xchg'd. The list is actually empty when the pointer is NULL, and

 * list_empty() will return true with one element is actually present.

 put on front */

	/*

	 * Return our per-cpu first list to the cache's xfer by atomically

	 * grabbing the current xfer list, appending it to our per-cpu list,

	 * and then atomically returning that entire list back to the

	 * cache's xfer list as long as it's still empty.

 XXX needs + offset for multiple recvs per page */

 ic starts out kzalloc()ed */

/*

 * You'd think that with reliable IB connections you wouldn't need to ack

 * messages that have been received.  The problem is that IB hardware generates

 * an ack message before it has DMAed the message into memory.  This creates a

 * potential message loss if the HCA is disabled for any reason between when it

 * sends the ack and before the message is DMAed and processed.  This is only a

 * potential issue if another HCA is available for fail-over.

 *

 * When the remote host receives our ack they'll free the sent message from

 * their send queue.  To decrease the latency of this we always send an ack

 * immediately after we've received messages.

 *

 * For simplicity, we only have one ack in flight at a time.  This puts

 * pressure on senders to have deep enough send queues to absorb the latency of

 * a single ack frame being in flight.  This might not be good enough.

 *

 * This is implemented by have a long-lived send_wr and sge which point to a

 * statically allocated ack frame.  This ack wr does not fall under the ring

 * accounting that the tx and rx wrs do.  The QP attribute specifically makes

 * room for it beyond the ring size.  Send completion notices its special

 * wr_id and avoids working with the ring in that case.

		/* Failed to send. Release the WR, and

		 * force another ACK.

/*

 * There are 3 ways of getting acknowledgements to the peer:

 *  1.	We call rds_ib_attempt_ack from the recv completion handler

 *	to send an ACK-only frame.

 *	However, there can be only one such frame in the send queue

 *	at any time, so we may have to postpone it.

 *  2.	When another (data) packet is transmitted while there's

 *	an ACK in the queue, we piggyback the ACK sequence number

 *	on the data packet.

 *  3.	If the ACK WR is done sending, we get called from the

 *	send queue completion handler, and check whether there's

 *	another ACK pending (postponed because the WR was on the

 *	queue). If so, we transmit it.

 *

 * We maintain 2 variables:

 *  -	i_ack_flags, which keeps track of whether the ACK WR

 *	is currently in the send queue or not (IB_ACK_IN_FLIGHT)

 *  -	i_ack_next, which is the last sequence number we received

 *

 * Potentially, send queue and receive queue handlers can run concurrently.

 * It would be nice to not have to use a spinlock to synchronize things,

 * but the one problem that rules this out is that 64bit updates are

 * not atomic on all platforms. Things would be a lot simpler if

 * we had atomic64 or maybe cmpxchg64 everywhere.

 *

 * Reconnecting complicates this picture just slightly. When we

 * reconnect, we may be seeing duplicate packets. The peer

 * is retransmitting them, because it hasn't seen an ACK for

 * them. It is important that we ACK these.

 *

 * ACK mitigation adds a header flag "ACK_REQUIRED"; any packet with

 * this flag set *MUST* be acknowledged immediately.

/*

 * When we get here, we're called from the recv queue handler.

 * Check whether we ought to transmit an ACK.

 Can we get a send credit? */

/*

 * We get here from the send completion handler, when the

 * adapter tells us the ACK frame was sent.

/*

 * This is called by the regular xmit code when it wants to piggyback

 * an ACK on an outgoing frame.

/*

 * It's kind of lame that we're copying from the posted receive pages into

 * long-lived bitmaps.  We could have posted the bitmaps and rdma written into

 * them.  But receiving new congestion bitmaps should be a *rare* event, so

 * hopefully we won't need to invest that complexity in making it more

 * efficient.  By copying we can share a simpler core with TCP which has to

 * copy.

 catch completely corrupt packets */

 Must be 64bit aligned. */

			/* Record ports that became uncongested, ie

 the congestion map is in little endian order */

 XXX shut down the connection if port 0,0 are seen? */

 Validate the checksum. */

 Process the ACK sequence which comes with every packet */

 Process the credits update if there was one */

		/* This is an ACK-only packet. The fact that it gets

		 * special treatment here is that historically, ACKs

		 * were rather special beasts.

		/*

		 * Usually the frags make their way on to incs and are then freed as

		 * the inc is freed.  We don't go that route, so we have to drop the

		 * page ref ourselves.  We can't just leave the page on the recv

		 * because that confuses the dma mapping of pages and each recv's use

		 * of a partial page.

		 *

		 * FIXME: Fold this into the code path below.

	/*

	 * If we don't already have an inc on the connection then this

	 * fragment has a header and starts a message.. copy its header

	 * into the inc and save the inc so we can hang upcoming fragments

	 * off its list.

		/* We can't just use memcmp here; fragments of a

		/* Evaluate the ACK_REQUIRED flag *after* we received

		 * the complete frame, and after bumping the next_rx

	/* Also process recvs in connecting state because it is possible

	 * to get a recv completion _before_ the rdmacm ESTABLISHED

	 * event is processed.

 We expect errors as the qp is drained during shutdown */

	/* rds_ib_process_recv() doesn't always consume the frag, and

	 * we might not have called it at all if the wc didn't indicate

	 * success. We already unmapped the frag's pages, though, and

	 * the following rds_ib_ring_free() call tells the refill path

	 * that it will not find an allocated frag here. Make sure we

	 * keep that promise by freeing a frag that's still on the ring.

	/* If we ever end up with a really empty receive ring, we're

	 * in deep trouble, as the sender will definitely see RNR

 Default to 30% of all available RAM for recv memory */

/*

 * Copyright (c) 2006 Oracle.  All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * Copyright (c) 2006, 2018 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * All of connection management is simplified by serializing it through

 * work queues that execute in a connection managing thread.

 *

 * TCP wants to send acks through sendpage() in response to data_ready(),

 * but it needs a process context to do so.

 *

 * The receive paths need to allocate but can't drop packets (!) so we have

 * a thread around to block allocating if the receive fast path sees an

 * allocation failure.

/* Grand Unified Theory of connection life cycle:

 * At any point in time, the connection can be in one of these states:

 * DOWN, CONNECTING, UP, DISCONNECTING, ERROR

 *

 * The following transitions are possible:

 *  ANY		  -> ERROR

 *  UP		  -> DISCONNECTING

 *  ERROR	  -> DISCONNECTING

 *  DISCONNECTING -> DOWN

 *  DOWN	  -> CONNECTING

 *  CONNECTING	  -> UP

 *

 * Transition to state DISCONNECTING/DOWN:

 *  -	Inside the shutdown worker; synchronizes with xmit path

 *	through RDS_IN_XMIT, and with connection management callbacks

 *	via c_cm_lock.

 *

 *	For receive callbacks, we rely on the underlying transport

 *	(TCP, IB/RDMA) to provide the necessary synchronisation.

/*

 * This random exponential backoff is relied on to eventually resolve racing

 * connects.

 *

 * If connect attempts race then both parties drop both connections and come

 * here to wait for a random amount of time before trying again.  Eventually

 * the backoff range will be so much greater than the time it takes to

 * establish a connection that one of the pair will establish the connection

 * before the other's random delay fires.

 *

 * Connection attempts that arrive while a connection is already established

 * are also considered to be racing connects.  This lets a connection from

 * a rebooted machine replace an existing stale connection before the transport

 * notices that the connection has failed.

 *

 * We should *always* start with a random backoff; otherwise a broken connection

 * will always take several iterations to be re-established.

 let peer with smaller addr initiate reconnect, to avoid duels */

/* Compare two IPv6 addresses.  Return 0 if the two addresses are equal.

 * Return 1 if the first is greater.  Return -1 if the second is greater.

/*

 * Copyright (c) 2006, 2017 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 the core send_sem serializes this with other xmit and shutdown */

 the core send_sem serializes this with other xmit and shutdown */

		/*

		 * m_ack_seq is set to the sequence number of the last byte of

		 * header and data.  see rds_tcp_is_acked().

 see rds_tcp_write_space() */

 write_space will hit after EAGAIN, all else fatal */

			/* No need to disconnect/reconnect if path_drop

			 * has already been triggered, because, e.g., of

			 * an incoming RST.

/*

 * rm->m_ack_seq is set to the tcp sequence number that corresponds to the

 * last byte of the message, including the header.  This means that the

 * entire message has been received if rm->m_ack_seq is "before" the next

 * unacked byte of the TCP sequence space.  We have to do very careful

 * wrapping 32bit comparisons here.

	/*

	 * write_space is only called when data leaves tcp's send queue if

	 * SOCK_NOSPACE is set.  We set SOCK_NOSPACE every time we put

	 * data in tcp's send queue because we use write_space to parse the

	 * sequence numbers and notice that rds messages have been fully

	 * received.

	 *

	 * tcp's write_space clears SOCK_NOSPACE if the send queue has more

	 * than a certain amount of space. So we need to set it again *after*

	 * we call tcp's write_space or else we might only get called on the

	 * first of a series of incoming tcp acks.

/*

 * Copyright (c) 2006, 2018 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/* When transmitting messages in rds_send_xmit, we need to emerge from

 * time to time and briefly release the CPU. Otherwise the softlock watchdog

 * will kick our shin.

 * Also, it seems fairer to not let one busy connection stall all the

 * others.

 *

 * send_batch_count is the number of times we'll loop in send_xmit. Setting

 * it to 0 will restore the old behavior (where we looped until we had

 * drained the queue).

/*

 * Reset the send state.  Callers must ensure that this doesn't race with

 * rds_send_xmit().

		/* Tell the user the RDMA op is no longer mapped by the

		 * transport. This isn't entirely true (it's flushed out

		 * independently) but as the connection is down, there's

 Mark messages as retransmissions, and move them to the send q */

	/*

	 * We don't use wait_on_bit()/wake_up_bit() because our waking is in a

	 * hot path and finding waiters is very rare.  We don't want to walk

	 * the system-wide hashed waitqueue buckets in the fast path only to

	 * almost never find waiters.

/*

 * We're making the conscious trade-off here to only send one message

 * down the connection at a time.

 *   Pro:

 *      - tx queueing is a simple fifo list

 *   	- reassembly is optional and easily done by transports per conn

 *      - no per flow rx lookup at all, straight to the socket

 *   	- less per-frag memory and wire overhead

 *   Con:

 *      - queued acks can be delayed behind large messages

 *   Depends:

 *      - small message latency is higher behind queued large messages

 *      - large message latency isn't starved by intervening small sends

	/*

	 * sendmsg calls here after having queued its message on the send

	 * queue.  We only have one task feeding the connection at a time.  If

	 * another thread is already feeding the queue then we back off.  This

	 * avoids blocking the caller and trading per-connection data between

	 * caches per message.

 dont requeue send work */

	/*

	 * we record the send generation after doing the xmit acquire.

	 * if someone else manages to jump in and do some work, we'll use

	 * this to avoid a goto restart farther down.

	 *

	 * The acquire_in_xmit() check above ensures that only one

	 * caller can increment c_send_gen at any time.

	/*

	 * rds_conn_shutdown() sets the conn state and then tests RDS_IN_XMIT,

	 * we do the opposite to avoid races.

	/*

	 * spin trying to push headers and data down the connection until

	 * the connection doesn't make forward progress.

		/*

		 * If between sending messages, we can send a pending congestion

		 * map update.

		/*

		 * If not already working on one, grab the next message.

		 *

		 * cp_xmit_rm holds a ref while we're sending this message down

		 * the connction.  We can use this ref while holding the

		 * send_sem.. rds_send_reset() is serialized with it.

			/* we want to process as big a batch as we can, but

			 * we also want to avoid softlockups.  If we've been

			 * through a lot of messages, lets back off and see

			 * if anyone else jumps in

				/*

				 * Move the message from the send queue to the retransmit

				 * list right away.

			/* Unfortunately, the way Infiniband deals with

			 * RDMA to a bad MR key is by moving the entire

			 * queue pair to error state. We cold possibly

			 * recover from that, but right now we drop the

			 * connection.

			 * Therefore, we never retransmit messages with RDMA ops.

 Require an ACK every once in a while */

 The transport either sends the whole rdma or none of it */

			/* The transport owns the mapped memory for now.

			 * You can't unmap it while it's on the send queue

			/* The transport owns the mapped memory for now.

			 * You can't unmap it while it's on the send queue

		/*

		 * A number of cases require an RDS header to be sent

		 * even if there is no data.

		 * We permit 0-byte sends; rds-ping depends on this.

		 * However, if there are exclusively attached silent ops,

		 * we skip the hdr/data send, to enable silent operation.

		/*

		 * A rm will only take multiple times through this loop

		 * if there is a data op. Thus, if the data is sent (or there was

		 * none), then we're done with the rm.

 Nuke any messages we decided not to retransmit. */

 irqs on here, so we can put(), unlike above */

	/*

	 * Other senders can queue a message after we last test the send queue

	 * but before we clear RDS_IN_XMIT.  In that case they'd back off and

	 * not try and send their newly queued message.  We need to check the

	 * send queue after having cleared RDS_IN_XMIT so that their message

	 * doesn't get stuck on the send queue.

	 *

	 * If the transport cannot continue (i.e ret != 0), then it must

	 * call us when more room is available, such as from the tx

	 * completion handler.

	 *

	 * We have an extra generation check here so that if someone manages

	 * to jump in after our release_in_xmit, we'll see that they have done

	 * some work and we will skip our goto

/*

 * This is pretty similar to what happens below in the ACK

 * handling code - except that we call here as soon as we get

 * the IB send completion on the RDMA op and the accompanying

 * message.

/*

 * Just like above, except looks at atomic op

/*

 * This is the same as rds_rdma_send_complete except we

 * don't do any locking - we have all the ingredients (message,

 * socket, socket lock) and can just move the notifier.

 No need to wake the app - caller does this */

/*

 * This removes messages from the socket's list if they're on it.  The list

 * argument must be private to the caller, we must be able to modify it

 * without locks.  The messages must have a reference held for their

 * position on the list.  This function will drop that reference after

 * removing the messages from the 'messages' list regardless of if it found

 * the messages on the socket list or not.

		/*

		 * If we see this flag cleared then we're *sure* that someone

		 * else beat us to removing it from the sock.  If we race

		 * with their flag update we'll get the lock and then really

		 * see that the flag has been cleared.

		 *

		 * The message spinlock makes sure nobody clears rm->m_rs

		 * while we're messing with it. It does not prevent the

		 * message from being removed from the socket, though.

/*

 * Transports call here when they've determined that the receiver queued

 * messages up to, and including, the given sequence number.  Messages are

 * moved to the retrans queue when rds_send_xmit picks them off the send

 * queue. This means that in the TCP case, the message may not have been

 * assigned the m_ack_seq yet - but that's fine as long as tcp_is_acked

 * checks the RDS_MSG_HAS_ACK_SEQ bit.

 order flag updates with spin locks */

 now remove the messages from the sock list as needed */

 get all the messages we're dropping under the rs lock */

 order flag updates with the rs lock */

 Remove the messages from the conn */

		/*

		 * Maybe someone else beat us to removing rm from the conn.

		 * If we race with their flag update we'll get the lock and

		 * then really see that the flag has been cleared.

		/*

		 * Couldn't grab m_rs_lock in top loop (lock ordering),

		 * but we can now.

		/* just in case the code above skipped this message

		 * because RDS_MSG_ON_CONN wasn't set, run it again here

		 * taking m_rs_lock is the only thing that keeps us

		 * from racing with ack processing.

/*

 * we only want this to fire once so we use the callers 'queued'.  It's

 * possible that another thread can race with us and remove the

 * message from the flow with RDS_CANCEL_SENT_TO.

	/* this is the only place which holds both the socket's rs_lock

	/*

	 * If there is a little space in sndbuf, we don't queue anything,

	 * and userspace gets -EAGAIN. But poll() indicates there's send

	 * room. This can lead to bad behavior (spinning) if snd_bytes isn't

	 * freed up by incoming acks. So we check the *old* value of

	 * rs_snd_bytes here to allow the last msg to exceed the buffer,

	 * and poll() now knows no more data can be sent.

		/* let recv side know we are close to send space exhaustion.

		 * This is probably not the optimal way to do it, as this

		 * means we set the flag on *all* messages as soon as our

		 * throughput hits a certain threshold.

		/* The code ordering is a little weird, but we're

/*

 * rds_message is getting to be quite complicated, and we'd like to allocate

 * it all in one go. This figures out how big it needs to be up front.

 these are valid but do no add any size */

 Ensure (DEST, MAP) are never used with (ARGS, ATOMIC) */

		/* As a side effect, RDMA_DEST and RDMA_MAP will set

		 * rm->rdma.m_rdma_cookie and rm->rdma.m_rdma_mr.

				/* Accommodate the get_mr() case which can fail

				 * if connection isn't established yet.

		/* The underlying connection is not up yet.  Need to wait

		 * until it is up to be sure that the non-zero c_path can be

		 * used.  But if we are interrupted, we have to use the zero

		 * c_path in case the connection ends up being non-MP capable.

			/* Cannot wait for the connection be made, so just use

			 * the base c_path.

 expect 1 RDMA CMSG per rds_sendmsg. can still grow if more needed. */

 Mirror Linux UDP mirror of BSD error message compatibility */

 XXX: Perhaps MSG_MORE someday */

				/* It is a mapped address.  Need to do some

				 * sanity checks.

 We only care about consistency with ->connect() */

		/* Cannot send to an IPv4 address using an IPv6 source

		 * address and cannot send to an IPv6 address using an

		 * IPv4 source address.

		/* If the socket is already bound to a link local address,

		 * it can only send to peers on the same link.  But allow

		 * communicating between link local and non-link local address.

 size of rm including all sgs */

 Attach data to the rm */

	/* rds_conn_create has a spinlock that runs with IRQ off.

 Parse any control messages the user may have included. */

 Trigger connection so that its ready for the next retry */

	/*

	 * By now we've committed to the send.  We reuse rds_send_worker()

	 * to retry sends in the rds thread if the transport asks us to.

	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.

	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN

/*

 * send out a probe. Can be shared by rds_send_ping,

 * rds_send_pong, rds_send_hb.

 * rds_send_hb should use h_flags

 *   RDS_FLAG_HB_PING|RDS_FLAG_ACK_REQUIRED

 * or

 *   RDS_FLAG_HB_PONG|RDS_FLAG_ACK_REQUIRED

 schedule the send work on rds_wq */

/*

 * Copyright (c) 2006 Oracle.  All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/**

 * rds_page_remainder_alloc - build up regions of a message.

 *

 * @scat: Scatter list for message

 * @bytes: the number of bytes needed.

 * @gfp: the waiting behaviour of the allocation

 *

 * @gfp is always ored with __GFP_HIGHMEM.  Callers must be prepared to

 * kmap the pages, etc.

 *

 * If @bytes is at least a full page then this just returns a page from

 * alloc_page().

 *

 * If @bytes is a partial page then this stores the unused region of the

 * page in a per-cpu structure.  Future partial-page allocations may be

 * satisfied from that cached region.  This lets us waste less memory on

 * small allocations with minimal complexity.  It works because the transmit

 * path passes read-only page regions down to devices.  They hold a page

 * reference until they are done with the region.

 jump straight to allocation if we're trying for a huge page */

 avoid a tiny region getting stuck by tossing it */

 hand out a fragment from the cached page */

 alloc if there is nothing for us to use */

 did someone race to fill the remainder before us? */

 otherwise install our page and loop around to alloc */

/*

 * Copyright (c) 2006 Oracle.  All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * Copyright (c) 2006, 2018 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 only for info exporting */

/* rds_tcp_tc_count counts only IPv4 connections.

 * rds6_tcp_tc_count counts both IPv4 and IPv6 connections.

 Track rds_tcp_connection structs so they can be cleaned up */

 data is per-net pointer */

 data is per-net pointer */

 seq# of the last byte of data in tcp send buffer */

 done under the callback_lock to serialize with write_space */

/*

 * rds_tcp_reset_callbacks() switches the to the new sock and

 * returns the existing tc->t_sock.

 *

 * The only functions that set tc->t_sock are rds_tcp_set_callbacks

 * and rds_tcp_reset_callbacks.  Send and receive trust that

 * it is set.  The absence of RDS_CONN_UP bit protects those paths

 * from being called while it isn't set.

	/* Need to resolve a duelling SYN between peers.

	 * We have an outstanding SYN to this peer, which may

	 * potentially have transitioned to the RDS_CONN_UP state,

	 * so we must quiesce any send threads before resetting

	 * cp_transport_data. We quiesce these threads by setting

	 * cp_state to something other than RDS_CONN_UP, and then

	 * waiting for any existing threads in rds_send_xmit to

	 * complete release_in_xmit(). (Subsequent threads entering

	 * rds_send_xmit() will bail on !rds_conn_up().

	 *

	 * However an incoming syn-ack at this point would end up

	 * marking the conn as RDS_CONN_UP, and would again permit

	 * rds_send_xmi() threads through, so ideally we would

	 * synchronize on RDS_CONN_UP after lock_sock(), but cannot

	 * do that: waiting on !RDS_IN_XMIT after lock_sock() may

	 * end up deadlocking with tcp_sendmsg(), and the RDS_IN_XMIT

	 * would not get set. As a result, we set c_state to

	 * RDS_CONN_RESETTTING, to ensure that rds_tcp_state_change

	 * cannot mark rds_conn_path_up() in the window before lock_sock()

 reset receive side state for rds_tcp_data_recv() for osock  */

/* Add tc to rds_tcp_tc_list and set tc->t_sock. See comments

 * above rds_tcp_reset_callbacks for notes about synchronization

 * with data path

 done under the callback_lock to serialize with write_space */

 accepted sockets need our listen data ready undone */

/* Handle RDS_INFO_TCP_SOCKETS socket option.  It only returns IPv4

 * connections for backward compatibility.

/* Handle RDS6_INFO_TCP_SOCKETS socket option. It returns both IPv4 and

 * IPv6 connections. IPv4 connection address is returned in an IPv4 mapped

 * address.

	/* If the scope_id is specified, check only those addresses

	 * hosted on the specified interface.

 scope_id is not valid... */

 avoid calling conn_destroy with irqs off */

 all user tos mapped to default 0 for TCP transport */

 per-network namespace private data for this module */

/* All module specific customizations to the RDS-TCP socket should be done in

 * rds_tcp_tune() and applied after socket creation.

	/* {snd, rcv}buf_size default to 0, which implies we let the

	 * stack pick the value, and permit auto-tuning of buffer size.

 Try IPv4 as some systems disable IPv6 */

/* when sysctl is used to modify some kernel socket parameters,this

 * function  resets the RDS connections in that netns  so that we can

 * restart with new parameters.  The assumption is that such reset

 * events are few and far-between.

 reconnect with new parameters */

/*

 * Copyright (c) 2006, 2019 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/* Create a key for the bind hash table manipulation.  Port is in network byte

 * order.

/*

 * Return the rds_sock bound at the given local address.

 *

 * The rx path can race with rds_release.  We notice if rds_release() has

 * marked this socket and don't return a rs ref to the rx path.

 returns -ve errno or +ve port */

	/* We allow an RDS socket to be bound to either IPv4 or IPv6

	 * address.

			/* It is a mapped address.  Need to do some sanity

			 * checks.

 The scope ID must be specified for link local address. */

 RDS socket does not allow re-binding. */

	/* Socket is connected.  The binding address should have the same

	 * scope ID as the connected address, except the case when one is

	 * non-link local address (scope_id is 0).

	/* The transport can be set using SO_RDS_TRANSPORT option before the

	 * socket is bound.

/*

 * Copyright (c) 2009, 2018 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 Global IPv4 and IPv6 RDS RDMA listener cm_id */

 Per IB specification 7.7.3, service level is a 4-bit field. */

 this can be null in the listening path */

	/* Prevent shutdown from tearing down the connection

		/* If the connection is being shut down, bail out

		 * right away. We return 0 so cm_id doesn't get

			/* Reject incoming connections while we're tearing

 XXX do we need to clean up if this fails? */

		/* Connection could have been dropped so make sure the

		 * cm_id is valid before proceeding

 things like device disconnect? */

	/*

	 * XXX I bet this binds the cm_id to a device.  If we want to support

	 * fail-over we'll have to take this into consideration.

/* Initialize the RDS RDMA listeners.  We create two listeners for

 * compatibility reason.  The one on RDS_PORT is used for IPv4

 * requests only.  The one on RDS_CM_PORT is used for IPv6 requests

 * only.  So only IPv6 enabled RDS module will communicate using this

 * port.

 Keep going even when IPv6 is not enabled in the system. */

 stop listening first to ensure no new connections are attempted */

/*

 * Copyright (c) 2006, 2019 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 this is just used for stats gathering :/ */

/*

 * This is called as the final descriptor referencing this socket is closed.

 * We have to unbind the socket so that another socket can be bound to the

 * address it was using.

 *

 * We have to be careful about racing with the incoming path.  sock_orphan()

 * sets SOCK_DEAD and we use that as an indicator to the rx path that new

 * messages shouldn't be queued.

	/* Note - rds_clear_recv_queue grabs rs_recv_lock, so

	 * that ensures the recv path has completed messing

/*

 * Careful not to race with rds_release -> sock_orphan which clears sk_sleep.

 * _bh() isn't OK here, we're called from interrupt handlers.  It's probably OK

 * to wake the waitqueue after sk_sleep is clear as we hold a sock ref, but

 * this seems more conservative.

 * NB - normally, one would use sk_callback_lock for this, but we can

 * get here from interrupts, whereas the network code grabs sk_callback_lock

 * with _lock_bh only - so relying on sk_callback_lock introduces livelocks.

 racey, don't care */

 scope_id is the same as in the bound address. */

		/* If socket is not yet bound and the socket is connected,

		 * set the return address family to be the same as the

		 * connected address, but with 0 address value.  If it is not

		 * connected, set the family to be AF_UNSPEC (value 0) and

		 * the address size to be that of an IPv4 address.

/*

 * RDS' poll is without a doubt the least intuitive part of the interface,

 * as EPOLLIN and EPOLLOUT do not behave entirely as you would expect from

 * a network protocol.

 *

 * EPOLLIN is asserted if

 *  -	there is data on the receive queue.

 *  -	to signal that a previously congested destination may have become

 *	uncongested

 *  -	A notification has been queued to the socket (this can be a congestion

 *	update, or a RDMA completion, or a MSG_ZEROCOPY completion).

 *

 * EPOLLOUT is asserted if there is room on the send queue. This does not mean

 * however, that the next sendmsg() call will succeed. If the application tries

 * to send to a congested destination, the system call may still fail (and

 * return ENOBUFS).

		/* When a congestion map was updated, we signal EPOLLIN for

		 * "historical" reasons. Applications can also poll for

 clear state any time we wake a seen-congested socket */

 racing with another thread binding seems ok here */

 XXX not a great errno */

 Assume IPv4 */

 previously attached to transport */

 unbound */

			/* It is a mapped address.  Need to do some sanity

			 * checks.

			/* If socket is arleady bound to a link local address,

			 * the peer address must be on the same link.

			/* Remember the connected address scope ID.  It will

			 * be checked against the binding local address when

			 * the socket is bound.

 This option only supports IPv4 sockets. */

 XXX too lazy to maintain counts.. */

 This option only supports IPv4 sockets. */

/*

 * Copyright (c) 2016 Oracle.  All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

		/* enforce order of ibmr->u.frmr.fr_state update

		 * before decrementing i_fastreg_inuse_count

 If we've pinned too many pages, request a flush */

	/* Perform a WR for the fast_reg_mr. Each individual page

	 * in the sg list is added to the fast reg page list and placed

	 * inside the fast_reg_mr WR.  The key used is a rolling 8bit

	 * counter, which should guarantee uniqueness.

 Failure here can be because of -ENOMEM as well */

	/* Wait for the registration to complete in order to prevent an invalid

	 * access error resulting from a race between the memory region already

	 * being accessed while registration is still pending.

	/* We want to teardown old ibmr values here and fill it up with

	 * new sg values

		/* enforce order of frmr->fr_inv update

		 * before incrementing i_fastreg_wrs

	/* Wait for the FRMR_IS_FREE (or FRMR_IS_STALE) transition in order to

	 * 1) avoid a silly bouncing between "clean_list" and "drop_list"

	 *    triggered by function "rds_ib_reg_frmr" as it is releases frmr

	 *    regions whose state is not "FRMR_IS_FREE" right away.

	 * 2) prevents an invalid access error in a race

	 *    from a pending "IB_WR_LOCAL_INV" operation

	 *    with a teardown ("dma_unmap_sg", "put_page")

	 *    and de-registration ("ib_dereg_mr") of the corresponding

	 *    memory region.

	/* enforce order of frmr->{fr_reg,fr_inv} update

	 * before incrementing i_fastreg_wrs

 String all ib_mr's onto one list and hand them to ib_unmap_fmr */

 Now we can destroy the DMA mapping and unpin any pages */

 Don't de-allocate if the MR is not free yet */

 TODO: Add FRWR support for RDS_GET_MR using proxy qp*/

/*

 * Copyright (c) 2006, 2020 Oracle and/or its affiliates.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 caller invokes rds_wake_sk_sleep() */

 caller invokes rds_wake_sk_sleep() */

/*

 * This relies on dma_map_sg() not touching sg[].page during merging.

 XXX will have to put_page for page refs */

 For now, refuse to add more than one extension header */

/*

 * If a message has extension headers, retrieve them here.

 * Call like this:

 *

 * unsigned int pos = 0;

 *

 * while (1) {

 *	buflen = sizeof(buffer);

 *	type = rds_message_next_extension(hdr, &pos, buffer, &buflen);

 *	if (type == RDS_EXTHDR_NONE)

 *		break;

 *	...

 * }

	/* Get the extension type and length. For now, the

/*

 * Each rds_message is allocated with extra space for the scatterlist entries

 * rds ops will need. This is to minimize memory allocation count. Then, each rds op

 * can grab SGs when initializing its part of the rds_message.

/*

 * RDS ops use this to grab SG entries from the rm's sg pool.

	/*

	 * now allocate and copy in the data payload.

 now allocate and copy in the data payload.  */

 Dear gcc, sg->page will be null from kzalloc. */

/*

 * If the message is still on the send queue, wait until the transport

 * is done with it. This is particularly important for RDMA operations.

/*

 * Copyright (c) 2006 Oracle.  All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 :.,$s/unsigned long\>.*\<s_\(.*\);/"\1",/g */

/*

 * This gives global counters across all the transports.  The strings

 * are copied in so that the tool doesn't need knowledge of the specific

 * stats that we're exporting.  Some are pretty implementation dependent

 * and may change over time.  That doesn't stop them from being useful.

 *

 * This is the only function in the chain that knows about the byte granular

 * length in userspace.  It converts it to number of stat entries that the

 * rest of the functions operate in.

/*

 * Copyright (c) 2006 Oracle.  All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 hardware will fail CQ creation long before this */

/*

 * This sysctl does nothing.

 *

 * Backwards compatibility with RDS 3.0 wire protocol

 * disables initial FC credit exchange.

 * If it's ever possible to drop 3.0 support,

 * setting this to 1 and moving init/refill of send/recv

 * rings from ib_cm_connect_complete() back into ib_setup_qp()

 * will cause credits to be added before protocol negotiation.

/*

 * Copyright (c) 2006, 2018 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 converting this to RCU is a chore for another day.. */

 rcu read lock must be held or the connection spinlock */

/*

 * This is called by transports as they're bringing down a connection.

 * It clears partial message state so that the transport can start sending

 * and receiving over this connection again in the future.  It is up to

 * the transport to have serialized this call with its send and recv.

	/* Do not clear next_rx_seq here, else we cannot distinguish

	 * retransmitted packets from new packets, and will hand all

	 * of them to the application. That is not consistent with the

/*

 * There is only every one 'conn' for a given pair of addresses in the

 * system at a time.  They contain messages to be retransmitted and so

 * span the lifetime of the actual underlying transport connections.

 *

 * For now they are not garbage collected once they're created.  They

 * are torn down as the module is removed, if ever.

		/* This is a looped back IB connection, and we're

		 * called by the code handling the incoming connect.

		 * We need a second connection object into which we

	/* If the local address is link local, set c_bound_if to be the

	 * index used for this connection.  Otherwise, set it to 0 as

	 * the socket is not bound to an interface.  c_bound_if is used

	 * to look up a socket when a packet is received

	/*

	 * This is where a connection becomes loopback.  If *any* RDS sockets

	 * can bind to the destination address then we'd rather the messages

	 * flow through loopback rather than either transport.

				/* "outgoing" connection to local address.

				 * Protocol says it wants the connection

				 * handled by the loopback transport.

				 * This is what TCP does.

				/* No transport currently in use

				 * should end up here, but if it

				 * does, reset/destroy the connection.

	/*

	 * Since we ran without holding the conn lock, someone could

	 * have created the same conn (either normal or passive) in the

	 * interim. We check while holding the lock. If we won, we complete

	 * init and return our conn. If we lost, we rollback and return the

	 * other one.

 Creating passive conn */

 Creating normal conn */

				/* The ->conn_alloc invocation may have

				 * allocated resource for all paths, so all

				 * of them may have to be freed here.

 shut it down unless it's down already */

		/*

		 * Quiesce the connection mgmt handlers before we start tearing

		 * things down. We don't hold the mutex for the entire

		 * duration of the shutdown operation, else we may be

		 * deadlocking with the CM handler. Instead, the CM event

		 * handler is supposed to check for state DISCONNECTING

			/* This can happen - eg when we're in the middle of tearing

			 * down the connection, and someone unloads the rds module.

			 * Quite reproducible with loopback connections.

			 * Mostly harmless.

			 *

			 * Note that this also happens with rds-tcp because

			 * we could have triggered rds_conn_path_drop in irq

			 * mode from rds_tcp_state change on the receipt of

			 * a FIN, thus we need to recheck for RDS_CONN_ERROR

			 * here.

	/* Then reconnect if it's still live.

	 * The passive side of an IB loopback connection is never added

	 * to the conn hash, so we never trigger a reconnect on this

/* destroy a single rds_conn_path. rds_conn_destroy() iterates over

 * all paths using rds_conn_path_destroy()

 make sure lingering queued work won't try to ref the conn */

 tear down queued messages */

/*

 * Stop and free a connection.

 *

 * This can only be used in very limited circumstances.  It assumes that once

 * the conn has been shutdown that no one else is referencing the connection.

 * We can only ensure this in the rmmod path in the current code.

 Ensure conn will not be scheduled for reconnect */

 shut the connection down */

	/*

	 * The congestion maps aren't freed up here.  They're

	 * freed by rds_cong_exit() after all the connections

	 * have been freed.

 XXX too lazy to maintain counts.. */

 XXX no c_lock usage.. */

			/* We copy as much as we can fit in the buffer,

			 * but we count all items so that the caller

			/* XXX We only copy the information from the first

			 * path for now.  The problem is that if there are

			 * more than one underlying paths, we cannot report

			 * information of all of them using the existing

			 * API.  For example, there is only one next_tx_seq,

			 * which path's next_tx_seq should we report?  It is

			 * a bug in the design of MPRDS.

 XXX no cp_lock usage.. */

			/* We copy as much as we can fit in the buffer,

			 * but we count all items so that the caller

			 * can resize the buffer.

 XXX Future: return the state rather than these funky bits */

 XXX Future: return the state rather than these funky bits */

	/* Just return 1 as there is no error case. This is a helper function

	 * for rds_walk_conn_path_info() and it wants a return value.

 register pernet callback */

 unregister pernet callback */

/*

 * Force a disconnect

/*

 * If the connection is down, trigger a connect. We may have scheduled a

 * delayed reconnect however - in this case we should not interfere.

/* Check connectivity of all paths

/*

 * Copyright (c) 2006, 2017 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * This returns the number of stats entries in the snapshot and only

 * copies them using the iter if there is enough space for them.  The

 * caller passes in the global stats so that we can size and copy while

 * holding the lock.

/*

 * Copyright (c) 2006, 2019 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * we have a clumsy combination of RCU and a rwsem protecting this list

 * because it is used both in the get_mr fast path and while blocking in

 * the FMR flushing path.

 NOTE: if also grabbing ibdev lock, grab this first */

/*

 * rds_ib_destroy_mr_pool() blocks on a few things and mrs drop references

 * from interrupt context so we push freing off into a work struct in krdsd.

 Only handle IB (no iWARP) devices */

 Device must support FRWR */

/*

 * New connections use this to find the device to associate with the

 * connection.  It's not in the fast path so we're not concerned about the

 * performance of the IB call.  (As of this writing, it uses an interrupt

 * blocking spinlock to serialize walking a per-device list of all registered

 * clients.)

 *

 * RCU is used to handle incoming connections racing with device teardown.

 * Rather than use a lock to serialize removal from the client_data and

 * getting a new reference, we use an RCU grace period.  The destruction

 * path removes the device from client_data and then waits for all RCU

 * readers to finish.

 *

 * A new connection can get NULL from this if its arriving on a

 * device that is in the process of being removed.

/*

 * The IB stack is letting us know that a device is going away.  This can

 * happen if the underlying HCA driver is removed or if PCI hotplug is removing

 * the pci function, for example.

 *

 * This can be called at any time and can be racing with any other RDS path.

 stop connection attempts from getting a reference to this device. */

	/*

	 * This synchronize rcu is waiting for readers of both the ib

	 * client data and the devices list to finish before we drop

	 * both of those references.

 We will only ever look at IB transports */

 IPv6 version of rds_ib_conn_info_visitor(). */

 We will only ever look at IB transports */

 IPv6 version of rds_ib_ic_info(). */

/*

 * Early RDS/IB was built to only bind to an address if there is an IPoIB

 * device with that address set.

 *

 * If it were me, I'd advocate for something more flexible.  Sending and

 * receiving should be device-agnostic.  Transports would try and maintain

 * connections between peers who have messages queued.  Userspace would be

 * allowed to influence which paths have priority.  We could call userspace

 * asserting this policy "routing".

	/* Create a CMA ID and try to bind it. This catches both

	 * IB and iWARP capable NICs.

		/* XXX Do a special IPv6 link local address check here.  The

		 * reason is that rdma_bind_addr() always succeeds with IPv6

		 * link local address regardless it is indeed configured in a

		 * system.

			/* Use init_net for now as RDS is not network

			 * name space aware.

 rdma_bind_addr will only succeed for IB & iWARP devices */

	/* due to this, we will claim to support iWARP devices unless we

 wait for rds_ib_dev_free() to complete */

	/* 1:1 user to transport map for RDMA transport.

	 * In future, if custom map is desired, hook can export

	 * user configurable map.

/*

 * Copyright (c) 2006, 2017 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 ignore connecting sockets as they make progress */

		/* Force the peer to reconnect so that we have the

		 * TCP ports going from <smaller-ip>.<transient> to

		 * <larger-ip>.<RDS_TCP_PORT>. We avoid marking the

		 * RDS connection as RDS_CONN_UP until the reconnect,

		 * to avoid RDS datagram loss.

	/* for multipath rds,we only trigger the connection after

	 * the handshake probe has determined the number of paths.

	/*

	 * once we call connect() we can start getting callbacks and they

	 * own the socket

/*

 * Before killing the tcp socket this needs to serialize with callbacks.  The

 * caller has already grabbed the sending sem so we're serialized with other

 * senders.

 *

 * TCP calls the callbacks with the sock lock so we hold it while we reset the

 * callbacks to those set by TCP.  Our callbacks won't execute again once we

 * hold the sock lock.

 tc->tc_sock = NULL */

/*

 * Copyright (c) 2007, 2017 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * This file implements the receive side of the unconventional congestion

 * management in RDS.

 *

 * Messages waiting in the receive queue on the receiving socket are accounted

 * against the sockets SO_RCVBUF option value.  Only the payload bytes in the

 * message are accounted for.  If the number of bytes queued equals or exceeds

 * rcvbuf then the socket is congested.  All sends attempted to this socket's

 * address should return block or return -EWOULDBLOCK.

 *

 * Applications are expected to be reasonably tuned such that this situation

 * very rarely occurs.  An application encountering this "back-pressure" is

 * considered a bug.

 *

 * This is implemented by having each node maintain bitmaps which indicate

 * which ports on bound addresses are congested.  As the bitmap changes it is

 * sent through all the connections which terminate in the local address of the

 * bitmap which changed.

 *

 * The bitmaps are allocated as connections are brought up.  This avoids

 * allocation in the interrupt handling path which queues messages on sockets.

 * The dense bitmaps let transports send the entire bitmap on any bitmap change

 * reasonably efficiently.  This is much easier to implement than some

 * finer-grained communication of per-port congestion.  The sender does a very

 * inexpensive bit test to test if the port it's about to send to is congested

 * or not.

/*

 * Interaction with poll is a tad tricky. We want all processes stuck in

 * poll to wake up and check whether a congested destination became uncongested.

 * The really sad thing is we have no idea which destinations the application

 * wants to send to - we don't even know which rds_connections are involved.

 * So until we implement a more flexible rds poll interface, we have to make

 * do with this:

 * We maintain a global counter that is incremented each time a congestion map

 * update is received. Each rds socket tracks this value, and if rds_poll

 * finds that the saved generation number is smaller than the global generation

 * number, it wakes up the process.

/*

 * Congestion monitoring

/*

 * Yes, a global lock.  It's used so infrequently that it's worth keeping it

 * global to simplify the locking.  It's only used in the following

 * circumstances:

 *

 *  - on connection buildup to associate a conn with its maps

 *  - on map changes to inform conns of a new map to send

 *

 *  It's sadly ordered under the socket callback lock and the connection lock.

 *  Receive paths can mark ports congested from interrupt context so the

 *  lock masks interrupts.

/*

 * There is only ever one bitmap for any address.  Connections try and allocate

 * these bitmaps in the process getting pointers to them.  The bitmaps are only

 * ever freed as the module is removed after all connections have been freed.

/*

 * Put the conn on its local map's list.  This is called when the conn is

 * really added to the hash.  It's nested under the rds_conn_lock, sadly.

			/* We cannot inline the call to rds_send_xmit() here

			 * for two reasons (both pertaining to a TCP transport):

			 * 1. When we get here from the receive path, we

			 *    are already holding the sock_lock (held by

			 *    tcp_v4_rcv()). So inlining calls to

			 *    tcp_setsockopt and/or tcp_sendmsg will deadlock

			 *    when it tries to get the sock_lock())

			 * 2. Interrupts are masked so that we can mark the

			 *    port congested from both send and recv paths.

			 *    (See comment around declaration of rdc_cong_lock).

			 *    An attempt to get the sock_lock() here will

			 *    therefore trigger warnings.

			 * Defer the xmit to rds_send_worker() instead.

/*

 * We're called under the locking that protects the sockets receive buffer

 * consumption.  This makes it a lot easier for the caller to only call us

 * when it knows that an existing set bit needs to be cleared, and vice versa.

 * We can't block and we need to deal with concurrent sockets working against

 * the same per-address map.

 update congestion map for now-closed port */

			/* It would have been nice to have an atomic set_bit on

			/* Test again - a congestion update may have arrived in

/*

 * Allocate a RDS message containing a congestion update.

/*

 * Copyright (c) 2006 Oracle.  All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * Copyright (c) 2006 Oracle.  All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * Locking for IB rings.

 * We assume that allocation is always protected by a mutex

 * in the caller (this is a valid assumption for the current

 * implementation).

 *

 * Freeing always happens in an interrupt, and hence only

 * races with allocations, but not with other free()s.

 *

 * The interaction between allocation and freeing is that

 * the alloc code has to determine the number of free entries.

 * To this end, we maintain two counters; an allocation counter

 * and a free counter. Both are allowed to run freely, and wrap

 * around.

 * The number of used entries is always (alloc_ctr - free_ctr) % NR.

 *

 * The current implementation makes free_ctr atomic. When the

 * caller finds an allocation fails, it should set an "alloc fail"

 * bit and retry the allocation. The "alloc fail" bit essentially tells

 * the CQ completion handlers to wake it up after freeing some

 * more entries.

/*

 * This only happens on shutdown.

 This assumes that atomic_t has at least as many bits as u32 */

	/* We only ever get called from the connection setup code,

/*

 * returns the oldest allocated ring entry.  This will be the next one

 * freed.  This can't be called if there are none allocated.

/*

 * returns the number of completed work requests.

/*

 * Copyright (c) 2006, 2017 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * this is pretty lame, but, whatever.

/*

 * We have a series of skbs that have fragmented pieces of the congestion

 * bitmap.  They must add up to the exact size of the congestion bitmap.  We

 * use the skb helpers to copy those into the pages that make up the in-memory

 * congestion bitmap for the remote address of this connection.  We then tell

 * the congestion core that the bitmap has been changed so that it can wake up

 * sleepers.

 *

 * This is racing with sending paths which are using test_bit to see if the

 * bitmap indicates that their recipient is congested.

 catch completely corrupt packets */

 only returns 0 or -error */

	/*

	 * tcp_read_sock() interprets partial progress as an indication to stop

	 * processing.

			/*

			 * XXX * we might be able to use the __ variants when

			 * we've already serialized at a higher level.

 could be 0 for a 0 len message */

 the caller has to hold the sock lock */

 It's like glib in the kernel! */

 give more than one skb per call */

/*

 * We hold the sock lock to serialize our rds_tcp_recv->tcp_read_sock from

 * data_ready.

 *

 * if we fail to allocate we're in trouble.. blindly wait some time before

 * trying again to see if the VM can free up something for us.

 check for teardown race */

/*

 * Copyright (c) 2006, 2018 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 values below based on xs_udp_default_timeout */

 send a probe 'keepidle' secs after last data */

 number of unack'ed probes before declaring dead */

	/* KEEPINTVL is the interval between successive probes. We follow

	 * the model in xs_tcp_finish_connecting() and re-use keepidle.

/* rds_tcp_accept_one_path(): if accepting on cp_index > 0, make sure the

 * client's ipaddr < server's ipaddr. Otherwise, close the accepted

 * socket and force a reconneect from smaller -> larger ip addr. The reason

 * we special case cp_index 0 is to allow the rds probe ping itself to itself

 * get through efficiently.

 * Since reconnects are only initiated from the node with the numerically

 * smaller ip address, we recycle conns in RDS_CONN_ERROR on the passive side

 * by moving them to CONNECTING in this function.

	/* for mprds, all paths MUST be initiated by the peer

	 * with the smaller address.

		/* Make sure we initiate at least one path if this

		 * has not already been done; rds_start_mprds() will

		 * take care of additional paths, if necessary.

 module unload or netns delete in progress */

	/* sock_create_lite() does not get a hold on the owner module so we

	 * need to do it here.  Note that sock_release() uses sock->ops to

	 * determine if it needs to decrement the reference count.  So set

	 * sock->ops after calling accept() in case that fails.  And there's

	 * no need to do try_module_get() as the listener should have a hold

	 * already.

	/* sk_bound_dev_if is not set if the peer address is not link local

	 * address.  In this case, it happens that mcast_oif is set.  So

	 * just use it.

 local address connection is only allowed via loopback */

	/* An incoming SYN request came in, and TCP just accepted it.

	 *

	 * If the client reboots, this conn will need to be cleaned up.

	 * rds_tcp_state_change() will do that cleanup

 Duelling SYN has been handled in rds_tcp_accept_one() */

 rds_connect_path_complete() marks RDS_CONN_UP */

	/* reset the newly returned accept sock and bail.

	 * It is safe to set linger on new_sock because the RDS connection

	 * has not been brought up on new_sock, so no RDS-level data could

	 * be pending on it. By setting linger, we achieve the side-effect

	 * of avoiding TIME_WAIT state on new_sock.

 check for teardown race */

	/*

	 * ->sk_data_ready is also called for a newly established child socket

	 * before it has been accepted and the accepter has set up their

	 * data_ready.. we only want to queue listen work for our listening

	 * socket

	 *

	 * (*ready)() may be null if we are racing with netns delete, and

	 * the listen socket is being torn down.

 serialize with and prevent further callbacks */

 wait for accepts to stop and close the socket */

/*

 * Copyright (c) 2006, 2017 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * This 'loopback' transport is a special case for flows that originate

 * and terminate on the same machine.

 *

 * Connection build-up notices if the destination address is thought of

 * as a local address by a transport.  At that time it decides to use the

 * loopback transport instead of the bound transport of the sending socket.

 *

 * The loopback transport's sending path just hands the sent rds_message

 * straight to the receiving path via an embedded rds_incoming.

/*

 * Usually a message transits both the sender and receiver's conns as it

 * flows to the receiver.  In the loopback case, though, the receive path

 * is handed the sending conn so the sense of the addresses is reversed.

 Do not send cong updates to loopback */

 For the embedded inc. Matching put is in loop_inc_free() */

/*

 * See rds_loop_xmit(). Since our inc is embedded in the rm, we

 * make sure the rm lives at least until the inc is done.

 we need to at least give the thread something to succeed */

/*

 * Even the loopback transport needs to keep track of its connections,

 * so it can call rds_conn_destroy() on them on exit. N.B. there are

 * 1+ loopback addresses (127.*.*.*) so it's not a bug to have

 * multiple loopback conns allocated, although rather useless.

 avoid calling conn_destroy with irqs off */

/*

 * This is missing .xmit_* because loop doesn't go through generic

 * rds_send_xmit() and doesn't call rds_recv_incoming().  .listen_stop and

 * .laddr_check are missing because transport.c doesn't iterate over

 * rds_loop_transport.

/*

 * Copyright (c) 2006, 2019 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 loop transport doesn't send/recv congestion updates */

 wasn't -> am congested */

 was -> aren't congested */

	/* Require more free space before reporting uncongested to prevent

 do nothing if no change in cong state */

/*

 * Process all extension headers that come with this message.

 Process extension header here */

			/* We ignore the size for now. We could stash it

 Process extension header here */

 if RDS_EXTHDR_NPATHS was not found, default to a single-path */

/* rds_start_mprds() will synchronously start multiple paths when appropriate.

 * The scheme is based on the following rules:

 *

 * 1. rds_sendmsg on first connect attempt sends the probe ping, with the

 *    sender's npaths (s_npaths)

 * 2. rcvr of probe-ping knows the mprds_paths = min(s_npaths, r_npaths). It

 *    sends back a probe-pong with r_npaths. After that, if rcvr is the

 *    smaller ip addr, it starts rds_conn_path_connect_if_down on all

 *    mprds_paths.

 * 3. sender gets woken up, and can move to rds_conn_path_connect_if_down.

 *    If it is the smaller ipaddr, rds_conn_path_connect_if_down can be

 *    called after reception of the probe-pong on all mprds_paths.

 *    Otherwise (sender of probe-ping is not the smaller ip addr): just call

 *    rds_conn_path_connect_if_down on the hashed path. (see rule 4)

 * 4. rds_connect_worker must only trigger a connection if laddr < faddr.

 * 5. sender may end up queuing the packet on the cp. will get sent out later.

 *    when connection is completed.

/*

 * The transport must make sure that this is serialized against other

 * rx and conn reset on this specific conn.

 *

 * We currently assert that only one fragmented message will be sent

 * down a connection at a time.  This lets us reassemble in the conn

 * instead of per-flow which means that we don't have to go digging through

 * flows to tear down partial reassembly progress on conn failure and

 * we save flow lookup and locking for each frag arrival.  It does mean

 * that small messages will wait behind large ones.  Fragmenting at all

 * is only to reduce the memory consumption of pre-posted buffers.

 *

 * The caller passes in saddr and daddr instead of us getting it from the

 * conn.  This lets loopback, who only has one conn for both directions,

 * tell us which roles the addrs in the conn are playing for this message.

	/*

	 * Sequence numbers should only increase.  Messages get their

	 * sequence number as they're queued in a sending conn.  They

	 * can be dropped, though, if the sending socket is closed before

	 * they hit the wire.  So sequence numbers can skip forward

	 * under normal operation.  They can also drop back in the conn

	 * failover case as previously sent messages are resent down the

	 * new instance of a conn.  We drop those, otherwise we have

	 * to assume that the next valid seq does not come after a

	 * hole in the fragment stream.

	 *

	 * The headers don't give us a way to realize if fragments of

	 * a message have been dropped.  We assume that frags that arrive

	 * to a flow are part of the current message on the flow that is

	 * being reassembled.  This means that senders can't drop messages

	 * from the sending conn until all their frags are sent.

	 *

	 * XXX we could spend more on the wire to get more robust failure

	 * detection, arguably worth it to avoid data corruption.

 if this is a handshake ping, start multipath if necessary */

 if this is a handshake pong, start multipath if necessary */

 Process extension headers */

 We can be racing with rds_release() which marks the socket dead. */

 serialize with rds_release -> sock_orphan */

/*

 * be very careful here.  This is being called as the condition in

 * wait_event_*() needs to cope with being called many times.

 XXX make sure this i_conn is reliable */

/*

 * Pull errors off the error queue.

 * If msghdr is NULL, we will just purge the error queue.

 fill holes with zero */

	/* put_cmsg copies to user space and thus may sleep. We can't do this

	 * with rs_lock held, so first grab as many notifications as we can stuff

	 * in the user provided cmsg buffer. We don't try to copy more, to avoid

	 * losing notifications - except when the buffer is so small that it wouldn't

	 * even hold a single notification. Then we give him as much of this single

	 * msg as we can squeeze in, and set MSG_CTRUNC.

	/* If we bailed out because of an error in put_cmsg,

	 * we may be left with one or more notifications that we

/*

 * Queue a congestion notification

/*

 * Receive any control messages.

 udp_recvmsg()->sock_recvtimeo() gets away without locking too.. */

 If there are pending notifications, do those - and nothing else */

		/*

		 * if the message we just copied isn't at the head of the

		 * recv queue then someone else raced us to return it, try

		 * to get the next message.

/*

 * The socket is being shut down and we're asked to drop messages that were

 * queued for recvmsg.  The caller has unbound the socket so the receive path

 * won't queue any more incoming fragments or messages on the socket.

/*

 * inc->i_saddr isn't used here because it is only set in the receive

 * path.

/*

 * Copyright (c) 2006, 2019 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * Convert IB-specific error message to RDS error message and call core

 * completion handler.

	/* If the user asked for a completion notification on this

	 * message, we can implement three different semantics:

	 *  1.	Notify when we received the ACK on the RDS message

	 *	that was queued with the RDMA. This provides reliable

	 *	notification of RDMA status at the expense of a one-way

	 *	packet delay.

	 *  2.	Notify when the IB stack gives us the completion event for

	 *	the RDMA operation.

	 *  3.	Notify when the IB stack gives us the completion event for

	 *	the accompanying RDS messages.

	 * Here, we implement approach #3. To implement approach #2,

	 * we would need to take an event for the rdma WR. To implement #1,

	 * don't call rds_rdma_send_complete at all, and fall back to the notify

	 * handling in the ACK processing code.

	 *

	 * Note: There's no need to explicitly sync any RDMA buffers using

	 * ib_dma_sync_sg_for_cpu - the completion for the RDMA

	 * operation itself unmapped the RDMA buffers, which takes care

	 * of synching.

 unmap atomic recvbuf */

/*

 * Unmap the resources associated with a struct send_work.

 *

 * Returns the rm for no good reason other than it is unobtainable

 * other than by switching on wr.opcode, currently, and the caller,

 * the event handler, needs it.

 In the error case, wc.opcode sometimes contains garbage */

/*

 * The only fast path caller always has a non-zero nr, so we don't

 * bother testing nr before performing the atomic sub.

/*

 * The _oldest/_free ring operations here race cleanly with the alloc/unalloc

 * operations performed in the send path.  As the sender allocs and potentially

 * unallocs the next free entry in the ring it doesn't alter which is

 * the next to be freed, which is what this is concerned with.

				/* If anyone waited for this message to get

				 * flushed out, wake them up now

 We expect errors as the qp is drained during shutdown */

/*

 * This is the main function for allocating credits when sending

 * messages.

 *

 * Conceptually, we have two counters:

 *  -	send credits: this tells us how many WRs we're allowed

 *	to submit without overruning the receiver's queue. For

 *	each SEND WR we post, we decrement this by one.

 *

 *  -	posted credits: this tells us how many WRs we recently

 *	posted to the receive queue. This value is transferred

 *	to the peer as a "credit update" in a RDS header field.

 *	Every time we transmit credits to the peer, we subtract

 *	the amount of transferred credits from this counter.

 *

 * It is essential that we avoid situations where both sides have

 * exhausted their send credits, and are unable to send new credits

 * to the peer. We achieve this by requiring that we send at least

 * one credit update to the peer before exhausting our credits.

 * When new credits arrive, we subtract one credit that is withheld

 * until we've posted new buffers and are ready to transmit these

 * credits (see rds_ib_send_add_credits below).

 *

 * The RDS send code is essentially single-threaded; rds_send_xmit

 * sets RDS_IN_XMIT to ensure exclusive access to the send ring.

 * However, the ACK sending code is independent and can race with

 * message SENDs.

 *

 * In the send path, we need to update the counters for send credits

 * and the counter of posted buffers atomically - when we use the

 * last available credit, we cannot allow another thread to race us

 * and grab the posted credits counter.  Hence, we have to use a

 * spinlock to protect the credit counter, or use atomics.

 *

 * Spinlocks shared between the send and the receive path are bad,

 * because they create unnecessary delays. An early implementation

 * using a spinlock showed a 5% degradation in throughput at some

 * loads.

 *

 * This implementation avoids spinlocks completely, putting both

 * counters into a single atomic, and updating that atomic using

 * atomic_add (in the receive path, when receiving fresh credits),

 * and using atomic_cmpxchg when updating the two counters.

 The last credit must be used to send a credit update. */

 Oops, there aren't that many credits left! */

 Sometimes you get what you want, lalala. */

	/*

	 * If need_posted is non-zero, then the caller wants

	 * the posted regardless of whether any send credits are

	 * available.

 Finally bill everything */

	/* Decide whether to send an update to the peer now.

	 * If we would send a credit update for every single buffer we

	 * post, we would end up with an ACK storm (ACK arrives,

	 * consumes buffer, we refill the ring, send ACK to remote

	 * advertising the newly posted buffer... ad inf)

	 *

	 * Performance pretty much depends on how often we send

	 * credit updates - too frequent updates mean lots of ACKs.

	 * Too infrequent updates, and the peer will run out of

	 * credits and has to throttle.

	 * For the time being, 16 seems to be a good compromise.

	/*

	 * We want to delay signaling completions just enough to get

	 * the batching benefits but not so much that we create dead time

	 * on the wire.

/*

 * This can be called multiple times for a given message.  The first time

 * we see a message we map its scatterlist into the IB device so that

 * we can provide that mapped address to the IB scatter gather entries

 * in the IB work requests.  We translate the scatterlist into a series

 * of work requests that fragment the message.  These work requests complete

 * in order so we pass ownership of the message to the completion handler

 * once we send the final fragment.

 *

 * The RDS core uses the c_send_lock to only enter this function once

 * per connection.  This makes sure that the tx ring alloc/unalloc pairs

 * don't get out of sync and confuse the ring.

 Do not send cong updates to IB loopback */

 FIXME we may overallocate here */

 map the message the first time we see it */

 XXX ? */

 Finalize the header */

		/* If it has a RDMA op, tell the peer we did it. This is

		/* Note - rds_ib_piggyb_ack clears the ACK_REQUIRED bit, so

		 * we should not do this unless we have a chance of at least

		 * sticking the header into the send ring. Which is why we

		/*

		 * Update adv_credits since we reset the ACK_REQUIRED bit.

	/* Sometimes you want to put a fence between an RDMA

	 * READ and the following SEND.

	 * We could either do this all the time

	 * or when requested by the user. Right now, we let

	 * the application choose.

 Each frag gets a header. Msgs may be 0 bytes */

 Set up the header */

 Set up the data, if present */

		/*

		 * Always signal the last one if we're stopping due to flow control.

 add credit and redo the header checksum */

	/* Account the RDS header in the number of bytes we sent, but just once.

 if we finished the message then send completion owns it */

 Put back wrs & credits we didn't use */

 XXX need to worry about failed_wr and partial sends. */

/*

 * Issue atomic operation.

 * A simplified version of the rdma case, we always map 1 SG, and

 * only 8 bytes, for the return value from the atomic operation.

 address of send request in ring */

 FADD */

 map 8 byte retval buffer to the device */

 XXX ? */

 Convert our struct scatterlist to struct ib_sge */

 map the op the first time we see it */

 XXX ? */

	/*

	 * Instead of knowing how to return a partial rdma read/write we insist that there

	 * be enough work requests to send the entire message.

 give a reference to the last op */

	/* We may have a pending ACK or window update we were unable

/*

 * Copyright (c) 2006 Oracle.  All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * This file implements a getsockopt() call which copies a set of fixed

 * sized structs into a user-specified buffer as a means of providing

 * read-only information about RDS.

 *

 * For a given information source there are a given number of fixed sized

 * structs at a given time.  The structs are only copied if the user-specified

 * buffer is big enough.  The destination pages that make up the buffer

 * are pinned for the duration of the copy.

 *

 * This gives us the following benefits:

 *

 * - simple implementation, no copy "position" across multiple calls

 * - consistent snapshot of an info source

 * - atomic copy works well with whatever locking info source has

 * - one portable tool to get rds info across implementations

 * - long-lived tool can get info without allocating

 *

 * at the following costs:

 *

 * - info source copy must be pinned, may be "large"

/*

 * Typically we hold an atomic kmap across multiple rds_info_copy() calls

 * because the kmap is so expensive.  This must be called before using blocking

 * operations while holding the mapping and as the iterator is torn down.

/*

 * get_user_pages() called flush_dcache_page() on the pages for us.

/*

 * @optval points to the userspace buffer that the information snapshot

 * will be copied into.

 *

 * @optlen on input is the size of the buffer in userspace.  @optlen

 * on output is the size of the requested snapshot in bytes.

 *

 * This function returns -errno if there is a failure, particularly -ENOSPC

 * if the given userspace buffer was not large enough to fit the snapshot.

 * On success it returns the positive number of bytes of each array element

 * in the snapshot.

 check for all kinds of wrapping and the like */

 a 0 len call is just trying to probe its length */

 XXX ? */

/*

 * Copyright (c) 2006, 2019 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * Set the selected protocol version

/*

 * Set up flow control

 We're doing flow control */

/*

 * Connection established.

 * We get here for both outgoing and incoming connection.

			/* dp structure start is not guaranteed to be 8 bytes

			 * aligned.  Since dp_ack_seq is 64-bit extended load

			 * operations can be used so go through get_unaligned

			 * to avoid unaligned errors.

 make sure it isn't empty data */

 receive sl from the peer */

	/* Init rings and fill recv. this needs to wait until protocol

	 * negotiation is complete, since ring layout is different

	 * from 3.1 to 4.1.

	/* Post receive buffers - as a side effect, this will update

 update ib_device with this local ipaddr */

	/* If the peer gave us the last packet it saw, process this as if

 Advertise flow control */

/* Plucking the oldest entry from the ring can be done concurrently with

 * the thread refilling the ring.  Each ring operation is protected by

 * spinlocks and the transient state of refilling doesn't change the

 * recording of which entry is oldest.

 *

 * This relies on IB only calling one cq comp_handler for each cq so that

 * there will only be one caller of rds_recv_incoming() per RDS connection.

 if cq has been already reaped, ignore incoming cq event */

 if cq has been already reaped, ignore incoming cq event */

/* Free the DMA memory used to store struct rds_header.

 *

 * @dev: the RDS IB device

 * @hdrs: pointer to the array storing DMA memory pointers

 * @dma_addrs: pointer to the array storing DMA addresses

 * @num_hdars: number of headers to free.

/* Allocate DMA coherent memory to be used to store struct rds_header for

 * sending/receiving packets.  The pointers to the DMA memory and the

 * associated DMA addresses are stored in two arrays.

 *

 * @dev: the RDS IB device

 * @dma_addrs: pointer to the array for storing DMA addresses

 * @num_hdrs: number of headers to allocate

 *

 * It returns the pointer to the array storing the DMA memory pointers.  On

 * error, NULL pointer is returned.

/*

 * This needs to be very careful to not leave IS_ERR pointers around for

 * cleanup to trip over.

	/*

	 * It's normal to see a null device if an incoming connection races

	 * with device removal, so we don't print a warning.

	/* The fr_queue_space is currently set to 512, to add extra space on

	 * completion queue and send queue. This extra space is used for FRWR

	 * registration and invalidation work requests

 add the conn now so that connection establishment has the dev */

 Protection domain and memory range */

 XXX negotiate max send/recv with remote? */

 + 1 to allow for the single ack message */

	/*

	 * XXX this can fail if max_*_wr is too large?  Are we supposed

	 * to back off until we get a value that the hardware can support?

	/*

	 * rdma_cm private data is odd - when there is any private data in the

	 * request, we will be given a pretty large buffer without telling us the

	 * original size. The only way to tell the difference is by looking at

	 * the contents, which are initialized to zero.

	 * If the protocol version fields aren't set, this is a connection attempt

	 * from an older version. This could be 3.0 or 2.0 - we can't tell.

	 * We really should have changed this for OFED 1.3 :-(

 Be paranoid. RDS always has privdata */

 Even if len is crap *now* I still want to check it. -ASG */

/* Given an IPv6 address, find the net_device which hosts that address and

 * return its index.  This is used by the rds_ib_cm_handle_connect() code to

 * find the interface index of where an incoming request comes from when

 * the request is using a link local address.

 *

 * Note one problem in this search.  It is possible that two interfaces have

 * the same link local address.  Unfortunately, this cannot be solved unless

 * the underlying layer gives us the interface which an incoming RDMA connect

 * request comes from.

 Check whether the remote protocol version matches ours. */

		/* If either address is link local, need to find the

		 * interface index in order to create a proper RDS

		 * connection.

 Using init_net for now ..  */

 No index found...  Need to bail out. */

 Use our address to find the correct index. */

 No index found...  Need to bail out. */

 RDS/IB is not currently netns aware, thus init_net */

	/*

	 * The connection request may occur while the

	 * previous connection exist, e.g. in case of failover.

	 * But as connections may be initiated simultaneously

	 * by both hosts, we have a random backoff mechanism -

	 * see the comment above rds_queue_reconnect()

 Wait and see - our connect may still be succeeding */

	/* If the peer gave us the last packet it saw, process this as if

	/* We got halfway through setting up the ib_connection, if we

 rdma_accept() calls rdma_reject() internally if it fails */

	/* If the peer doesn't do protocol negotiation, we must

 advertise flow control */

	/* Beware - returning non-zero tells the rdma_cm to destroy

	 * the cm_id. We should certainly not do it as long as we still

 XXX I wonder what affect the port space has */

 delegate cm event handler to rdma_transport */

/*

 * This is so careful about only cleaning up resources that were built up

 * so that it can be called at any point during startup.  In fact it

 * can be called multiple times for a given connection.

			/* Actually this may happen quite frequently, when

			 * an outgoing connect raced with an incoming connect.

		/* kick off "flush_worker" for all pools in order to reap

		 * all FRMR registrations that are still marked "FRMR_IS_INUSE"

		/*

		 * We want to wait for tx and rx completion to finish

		 * before we tear down the connection, but we have to be

		 * careful not to get stuck waiting on a send ring that

		 * only has unsignaled sends in it.  We've shutdown new

		 * sends before getting here so by waiting for signaled

		 * sends to complete we're ensured that there will be no

		 * more tx processing.

 first destroy the ib state that generates callbacks */

 then free the resources that ib callbacks use */

		/*

		 * Move connection back to the nodev list.

 Clear pending transmit */

 Clear the ACK state */

 Clear flow control state */

 Re-init rings, but retain sizes. */

 XXX too lazy? */

	/*

	 * rds_ib_conn_shutdown() waits for these to be emptied so they

	 * must be initialized before it can be called.

/*

 * Free a connection. Connection must be shut down and not set for reconnect.

	/*

	 * Conn is either on a dev's list or on the nodev list.

	 * A race with shutdown() or connect() would cause problems

	 * (since rds_ibdev would change) but that should never happen.

/*

 * An error occurred on the connection

/*

 * Copyright (c) 2006, 2018 Oracle and/or its affiliates. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the

 * OpenIB.org BSD license below:

 *

 *     Redistribution and use in source and binary forms, with or

 *     without modification, are permitted provided that the following

 *     conditions are met:

 *

 *      - Redistributions of source code must retain the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer.

 *

 *      - Redistributions in binary form must reproduce the above

 *        copyright notice, this list of conditions and the following

 *        disclaimer in the documentation and/or other materials

 *        provided with the distribution.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 conn was previously on the nodev_conns_list */

 place conn on nodev_conns_list */

 avoid calling conn_destroy with irqs off */

 Release the s/g list */

			/* FIXME we need a way to tell a r/w MR

/*

 * given an llist of mrs, put them all into the list_head for more processing

/*

 * this takes a list head of mrs and turns it into linked llist nodes

 * of clusters.  Each cluster has linked llist nodes of

 * MR_CLUSTER_SIZE mrs that are ready for reuse.

/*

 * Flush our pool of MRs.

 * At a minimum, all currently unused MRs are unmapped.

 * If the number of MRs allocated exceeds the limit, we also try

 * to free as many MRs as needed to get back to this limit.

	/* Get the list of all MRs to be dropped. Ordering matters -

	 * we want to put drop_list ahead of free_list.

 more than one entry in llist nodes */

 We do have some empty MRs. Flush them out. */

		/* A MR created and marked as use_once. We use delayed work,

		 * because there is a change that we are in interrupt and can't

		 * call to ib_dereg_mr() directly.

 Return it to the pool's free list */

 If we've pinned too many pages, request a flush */

			/* We get here if the user created a MR marked

			 * as use_once and invalidate at the same time.

 +1 allows for unaligned MRs */

 pool_type == RDS_IB_MR_8K_POOL */

/* By the time this is called all the IB devices should have been torn down and

 * had their pools freed.  As each pool is freed its work struct is waited on,

 * so the pool flushing work queue should be idle by the time we get here.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	LAPB release 002

 *

 *	This code REQUIRES 2.1.15 or higher/ NET3.038

 *

 *	History

 *	LAPB 001	Jonathan Naylor	Started Coding

 *	LAPB 002	Jonathan Naylor	New timer architecture.

/*

 *  This procedure is passed a buffer descriptor for an iframe. It builds

 *  the rest of the control part of the frame and then writes it out.

		/*

		 * Dequeue the frame and copy it.

			/*

			 * Transmit the frame copy.

			/*

			 * Requeue the original data frame.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	LAPB release 002

 *

 *	This code REQUIRES 2.1.15 or higher/ NET3.038

 *

 *	History

 *	LAPB 001	Jonathan Naylor	Started Coding

/*

 *	This routine purges all the queues of frames.

/*

 * This routine purges the input queue of those frames that have been

 * acknowledged. This replaces the boxes labelled "V(a) <- N(r)" on the

 * SDL diagram.

	/*

	 * Remove all the ack-ed frames from the ack queue.

	/*

	 * Requeue all the un-ack-ed frames on the output queue to be picked

	 * up by lapb_kick called from the timer. This arrangement handles the

	 * possibility of an empty output queue.

/*

 *	Validate that the value of nr is between va and vs. Return true or

 *	false for testing.

/*

 *	This routine is the centralised routine for parsing the control

 *	information for the different frame formats.

	/* We always need to look at 2 bytes, sometimes we need

	 * to look at 3 and those cases are handled below.

			/*

			 * I frame - carries NR/NS/PF

			/*

			 * S frame - take out PF/NR

			/*

			 * U frame - take out PF

			/*

			 * I frame - carries NR/NS/PF

			/*

			 * S frame - take out PF/NR

			/*

			 * U frame - take out PF

/*

 *	This routine is called when the HDLC layer internally  generates a

 *	command or  response  for  the remote machine ( eg. RR, UA etc. ).

 *	Only supervisory or unnumbered frames are processed, FRMRs are handled

 *	by lapb_transmit_frmr below.

 S frames carry NR */

/*

 *	This routine generates FRMRs based on information previously stored in

 *	the LAPB control block.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	LAPB release 002

 *

 *	This code REQUIRES 2.1.15 or higher/ NET3.038

 *

 *	History

 *	LAPB 001	Jonathan Naulor	Started Coding

 *	LAPB 002	Jonathan Naylor	New timer architecture.

 *	2000-10-29	Henner Eisen	lapb_data_indication() return status.

/*

 *	State machine for state 0, Disconnected State.

 *	The handling of the timer(s) is in file lapb_timer.c.

/*

 *	State machine for state 1, Awaiting Connection State.

 *	The handling of the timer(s) is in file lapb_timer.c.

/*

 *	State machine for state 2, Awaiting Release State.

 *	The handling of the timer(s) is in file lapb_timer.c

/*

 *	State machine for state 3, Connected State.

 *	The handling of the timer(s) is in file lapb_timer.c

			/*

			 * If upper layer has dropped the frame, we

			 * basically ignore any further protocol

			 * processing. This will cause the peer

			 * to re-transmit the frame later like

			 * a frame lost on the wire.

/*

 *	State machine for state 4, Frame Reject State.

 *	The handling of the timer(s) is in file lapb_timer.c.

/*

 *	Process an incoming LAPB frame

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	LAPB release 002

 *

 *	This code REQUIRES 2.1.15 or higher/ NET3.038

 *

 *	History

 *	LAPB 001	Jonathan Naylor	Started Coding

 *	LAPB 002	Jonathan Naylor	New timer architecture.

 *	2000-10-29	Henner Eisen	lapb_data_indication() return status.

/*

 *	Free an allocated lapb control block.

/*

 *	Socket removal during an interrupt is now safe.

/*

 *	Add a socket to the bound sockets list.

/*

 *	Create an empty LAPB control block.

 Wait for other refs to "lapb" to drop */

 Wait for running timers to stop */

 For now; must be != NET_RX_DROP */

 Handle device status changes. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	LAPB release 002

 *

 *	This code REQUIRES 2.1.15 or higher/ NET3.038

 *

 *	History

 *	LAPB 001	Jonathan Naylor	Started Coding

 *	LAPB 002	Jonathan Naylor	New timer architecture.

 A new timer has been set up */

 The timer has been stopped */

 A new timer has been set up */

 The timer has been stopped */

		/*

		 *	If we are a DCE, send DM up to N2 times, then switch to

		 *	STATE_1 and send SABM(E).

		/*

		 *	Awaiting connection state, send SABM(E), up to N2 times.

		/*

		 *	Awaiting disconnection state, send DISC, up to N2 times.

		/*

		 *	Data transfer state, restransmit I frames, up to N2 times.

		/*

		 *	Frame reject state, restransmit FRMR frames, up to N2 times.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Forwarding decision

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

 Don't forward packets to originating port or forwarding disabled */

	/* Mark the skb for forwarding offload early so that br_handle_vlan()

	 * can know whether to pop the VLAN header on egress or keep it.

/**

 * br_forward - forward a packet to a specific port

 * @to: destination port

 * @skb: packet being forwarded

 * @local_rcv: packet will be received locally after forwarding

 * @local_orig: packet is locally originated

 *

 * Should be called with rcu_read_lock.

 redirect to backup link if the destination port is down */

 called under rcu_read_lock */

		/* Do not flood unicast traffic to ports that turn it off, nor

		 * other traffic if flood off, except for traffic we originate

 Do not flood to ports that enable proxy ARP */

 Even with hairpin, no soliloquies - prevent breaking IPv6 DAD */

 called with rcu_read_lock */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Spanning tree protocol; timer-related code

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

 called under bridge lock */

	/*

	 * According to the spec, the message age timer cannot be

	 * running when we are the root bridge. So..  this was_root

	 * check is redundant. I'm leaving it in for now, though.

 Report ticks left (in USER_HZ) used for API */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Generic parts

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

/*

 * Handle changes in state of network devices enslaved to a bridge.

 *

 * Note: don't care about up/down if bridge itself is down, because

 *     port state is checked when bridge is brought up.

 register of bridge completed, add sysfs entries */

 not a port of a bridge */

 Forbid underlying device to change its type. */

 Propagate to master device */

 Events that may cause spanning tree to refresh */

 called with RTNL or RCU */

 Don't delete static entries */

 called under rtnl_mutex */

/* br_boolopt_toggle - change user-controlled boolean option

 *

 * @br: bridge device

 * @opt: id of the option to change

 * @on: new option value

 * @extack: extack for error messages

 *

 * Changes the value of the respective boolean option to @on taking care of

 * any internal option value mapping and configuration.

 shouldn't be called with unsupported options */

 shouldn't be called with unsupported options */

 private bridge options, controlled by the kernel */

 Wait for completion of call_rcu()'s */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Bridge per vlan tunnel port dst_metadata handling code

 *

 *	Authors:

 *	Roopa Prabhu		<roopa@cumulusnetworks.com>

/* Must be protected by RTNL.

 * Must be called with vid in range from 1 to 4094 inclusive.

/* Must be protected by RTNL.

 * Must be called with vid in range from 1 to 4094 inclusive.

 if already tagged, ignore */

 lookup vid, given tunnel id */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Userspace interface

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

/*

 * Determine initial path cost based on speed.

 * using recommendations from 802.1d standard

 *

 * Since driver might sleep need to not be holding any locks.

 Old silly heuristics based on name */

 assume old 10Mbps */

 Check for port carrier transitions. */

	/* Check if the port is already non-promisc or if it doesn't

	 * support UNICAST filtering.  Without unicast filtering support

	 * we'll end up re-enabling promisc mode anyway, so just check for

	 * it here.

	/* Since we'll be clearing the promisc mode, program the port

	 * first so that we don't have interruption in traffic.

/* When a port is added or removed or when certain port flags

 * change, this function is called to automatically manage

 * promiscuity setting of all the bridge ports.  We are always called

 * under RTNL so can skip using rcu primitives.

	/* If vlan filtering is disabled or bridge interface is placed

	 * into promiscuous mode, place all ports in promiscuous mode.

			/* If the number of auto-ports is <= 1, then all other

			 * ports will have their output configuration

			 * statically specified through fdbs.  Since ingress

			 * on the auto-port becomes forwarding/egress to other

			 * ports and egress configuration is statically known,

			 * we can say that ingress configuration of the

			 * auto-port is also statically known.

			 * This lets us disable promiscuous mode and write

			 * this config to hw.

 if the backup link is already set, clear it */

	/* If port is currently promiscuous, unset promiscuity.

	 * Otherwise, it is a static port so remove all addresses

	 * from it.

/* Delete port(interface) from bridge is done in two steps.

 * via RCU. First step, marks device as down. That deletes

 * all the timers and stops new packets from flowing through.

 *

 * Final cleanup doesn't occur until after all CPU's finished

 * processing packets.

 *

 * Protected from multiple admin operations by RTNL mutex

 Delete bridge device */

 find an available port number */

 zero is reserved */

 called with RTNL but without bridge lock */

 Could not find device */

 Attempt to delete non bridge device! */

 Not shutdown yet. */

 MTU of the bridge pseudo-device: ETH_DATA_LEN or the minimum of the ports */

 if the bridge MTU was manually configured don't mess with it */

	/* change to the minimum MTU and clear the flag which was set by

	 * the bridge ndo_change_mtu callback

/*

 * Recomputes features using slave's features

 called with RTNL */

 Don't allow bridging non-ethernet like devices. */

	/* Also don't allow bridging of net devices that are DSA masters, since

	 * the bridge layer rx_handler prevents the DSA fake ethertype handler

	 * to be invoked, so we don't get the chance to strip off and parse the

	 * DSA switch tag protocol header (the bridge layer just returns

	 * RX_HANDLER_CONSUMED, stopping RX processing for these frames).

	 * The only case where that would not be an issue is when bridging can

	 * already be offloaded, such as when the DSA master is itself a DSA

	 * or plain switchdev port, and is bridged only with other ports from

	 * the same hardware device.

 No bridging of bridges */

 Device has master upper dev */

 No bridging devices that dislike that (e.g. wireless) */

 kobject not yet init'd, manually free */

		/* When updating the port count we also update all ports'

		 * promiscuous mode.

		 * A port leaving promiscuous mode normally gets the bridge's

		 * fdb synced to the unicast filter (if supported), however,

		 * `br_port_clear_promisc` does not distinguish between

		 * non-promiscuous ports and *new* ports, so we need to

		 * sync explicitly here.

		/* Ask for permission to use this MAC address now, even if we

		 * don't end up choosing it below.

 called with RTNL */

	/* Since more than one interface can be attached to a bridge,

	 * there still maybe an alternate path for netconsole to use;

	 * therefore there is no reason for a NETDEV_RELEASE event.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Bridge multicast support.

 *

 * Copyright (c) 2010 Herbert Xu <herbert@gondor.apana.org.au>

/* IMPORTANT: this function must be used only when the contexts cannot be

 * passed down (e.g. timer) and must be used for read-only purposes because

 * the vlan snooping option can change, so it can return any context

 * (non-vlan or vlan). Its initial intended purpose is to read timer values

 * from the *current* context based on the option. At worst that could lead

 * to inconsistent timers when the contexts are changed, i.e. src timer

 * which needs to re-arm with a specific delay taken from the old context

 if vlan snooping is disabled use the port's multicast context */

	/* locking is tricky here, due to different rules for multicast and

	 * vlans we need to take rcu to find the vlan and make sure it has

	 * the BR_VLFLAG_MCAST_ENABLED flag set, it can only change under

	 * multicast_lock which must be already held here, so the vlan's pmctx

	 * can safely be used on return

/* when snooping we need to check if the contexts should be used

 * in the following order:

 * - if pmctx is non-NULL (port), check if it should be used

 * - if pmctx is NULL (bridge), check if brmctx should be used

/* When a port group transitions to (or is added as) EXCLUDE we need to add it

 * to all other ports' S,G entries which are not blocked by the current group

 * for proper replication, the assumption is that any S,G blocked entries

 * are already added so the S,G,port lookup should skip them.

 * When a port group transitions from EXCLUDE -> INCLUDE mode or is being

 * deleted we need to remove it from all ports' S,G entries where it was

 * automatically installed before (i.e. where it's MDB_PG_FLAGS_STAR_EXCL).

 called when adding a new S,G with host_joined == false by default */

 set the host_joined state of all of *,G's S,G entries */

 *,G exclude ports are only added to S,G entries */

	/* we need the STAR_EXCLUDE ports if there are non-STAR_EXCLUDE ports

	 * we should ignore perm entries since they're managed by user-space

	/* currently the host can only have joined the *,G which means

	 * we treat it as EXCLUDE {}, so for an S,G it's considered a

	 * STAR_EXCLUDE entry and we can safely leave it

 we need to add all exclude ports to the S,G */

 if it was added by user-space as perm we can skip next steps */

 the kernel is now responsible for removing this S,G */

 install S,G and based on src's timer enable or disable forwarding */

 Ethernet header */

 IPv6 header + HbH option */

 next hdr */

 length of HbH */

 Router Alert */

 Length of RA Option */

 Type = 0x0000 (MLD) */

 Pad1 */

 Pad1 */

 ICMPv6 */

 NULL is considered valid for host joined groups */

 we're about to select ourselves as querier */

 Take care of the remaining groups, only perm ones should be left */

/* State          Msg type      New state                Actions

 * INCLUDE (A)    IS_IN (B)     INCLUDE (A+B)            (B)=GMI

 * INCLUDE (A)    ALLOW (B)     INCLUDE (A+B)            (B)=GMI

 * EXCLUDE (X,Y)  ALLOW (A)     EXCLUDE (X+A,Y-A)        (A)=GMI

/* State          Msg type      New state                Actions

 * INCLUDE (A)    IS_EX (B)     EXCLUDE (A*B,B-A)        (B-A)=0

 *                                                       Delete (A-B)

 *                                                       Group Timer=GMI

/* State          Msg type      New state                Actions

 * EXCLUDE (X,Y)  IS_EX (A)     EXCLUDE (A-Y,Y*A)        (A-X-Y)=GMI

 *                                                       Delete (X-A)

 *                                                       Delete (Y-A)

 *                                                       Group Timer=GMI

/* State          Msg type      New state                Actions

 * INCLUDE (A)    TO_IN (B)     INCLUDE (A+B)            (B)=GMI

 *                                                       Send Q(G,A-B)

/* State          Msg type      New state                Actions

 * EXCLUDE (X,Y)  TO_IN (A)     EXCLUDE (X+A,Y-A)        (A)=GMI

 *                                                       Send Q(G,X-A)

 *                                                       Send Q(G)

		/* a notification has already been sent and we shouldn't

		 * access pg after the delete so we have to return false

/* State          Msg type      New state                Actions

 * INCLUDE (A)    TO_EX (B)     EXCLUDE (A*B,B-A)        (B-A)=0

 *                                                       Delete (A-B)

 *                                                       Send Q(G,A*B)

 *                                                       Group Timer=GMI

/* State          Msg type      New state                Actions

 * EXCLUDE (X,Y)  TO_EX (A)     EXCLUDE (A-Y,Y*A)        (A-X-Y)=Group Timer

 *                                                       Delete (X-A)

 *                                                       Delete (Y-A)

 *                                                       Send Q(G,A-Y)

 *                                                       Group Timer=GMI

/* State          Msg type      New state                Actions

 * INCLUDE (A)    BLOCK (B)     INCLUDE (A)              Send Q(G,A*B)

/* State          Msg type      New state                Actions

 * EXCLUDE (X,Y)  BLOCK (A)     EXCLUDE (X+(A-Y),Y)      (A-X-Y)=Group Timer

 *                                                       Send Q(G,A-Y)

		/* a notification has already been sent and we shouldn't

		 * access pg after the delete so we have to return false

 reload grec and host addr */

 nest attribute */

 BRIDGE_QUERIER_IP_ADDRESS */

 BRIDGE_QUERIER_IP_PORT */

 BRIDGE_QUERIER_IP_OTHER_TIMER */

 BRIDGE_QUERIER_IPV6_ADDRESS */

 BRIDGE_QUERIER_IPV6_PORT */

 BRIDGE_QUERIER_IPV6_OTHER_TIMER */

 protected by rtnl or rcu */

/* Add port to router_list

 *  list is maintained ordered by pointer value

 *  and locked by br->multicast_lock and RCU

	/* For backwards compatibility for now, only notify if we

	 * switched from no IPv4/IPv6 multicast router to a new

	 * IPv4 or IPv6 multicast router.

/* Add port to router_list

 *  list is maintained ordered by pointer value

 *  and locked by br->multicast_lock and RCU

/* Add port to router_list

 *  list is maintained ordered by pointer value

 *  and locked by br->multicast_lock and RCU

		/* the vlan has the master flag set only when transmitting

		 * through the bridge device

	/* it's okay to check for the flag without the multicast lock because it

	 * can only change under RTNL -> multicast_lock, we need the latter to

	 * sync with timers and packets

 disable/enable non-vlan mcast contexts based on vlan snooping */

	/* BR_VLFLAG_GLOBAL_MCAST_ENABLED relies on eventual consistency and

	 * requires only RTNL to change

	/* For backwards compatibility for now, only notify if there is

	 * no multicast router anymore for both IPv4 and IPv6.

 don't allow timer refresh */

 Refresh the temp router port timer */

	/* br_multicast_join_snoopers has the potential to cause

	 * an MLD Report/Leave to be delivered to br_multicast_rcv,

	 * which would in turn call br_multicast_add_group, which would

	 * attempt to acquire multicast_lock. This function should be

	 * called after the lock has been released to avoid deadlocks on

	 * multicast_lock.

	 *

	 * br_multicast_leave_snoopers does not have the problem since

	 * br_multicast_rcv first checks BROPT_MULTICAST_ENABLED, and

	 * returns without calling br_multicast_ipv4/6_rcv if it's not

	 * enabled. Moved both functions out just for symmetry.

 Currently we support only version 2 and 3 */

 Currently we support version 1 and 2 */

/**

 * br_multicast_list_adjacent - Returns snooped multicast addresses

 * @dev:	The bridge port adjacent to which to retrieve addresses

 * @br_ip_list:	The list to store found, snooped multicast IP addresses in

 *

 * Creates a list of IP addresses (struct br_ip_list) sensed by the multicast

 * snooping feature on all bridge ports of dev's bridge device, excluding

 * the addresses from dev itself.

 *

 * Returns the number of items added to br_ip_list.

 *

 * Notes:

 * - br_ip_list needs to be initialized by caller

 * - br_ip_list might contain duplicates in the end

 *   (needs to be taken care of by caller)

 * - br_ip_list needs to be freed by caller

/**

 * br_multicast_has_querier_anywhere - Checks for a querier on a bridge

 * @dev: The bridge port providing the bridge on which to check for a querier

 * @proto: The protocol family to check for: IGMP -> ETH_P_IP, MLD -> ETH_P_IPV6

 *

 * Checks whether the given interface has a bridge on top and if so returns

 * true if a valid querier exists anywhere on the bridged link layer.

 * Otherwise returns false.

/**

 * br_multicast_has_querier_adjacent - Checks for a querier behind a bridge port

 * @dev: The bridge port adjacent to which to check for a querier

 * @proto: The protocol family to check for: IGMP -> ETH_P_IP, MLD -> ETH_P_IPV6

 *

 * Checks whether the given interface has a bridge on top and if so returns

 * true if a selected querier is behind one of the other ports of this

 * bridge. Otherwise returns false.

/**

 * br_multicast_has_router_adjacent - Checks for a router behind a bridge port

 * @dev: The bridge port adjacent to which to check for a multicast router

 * @proto: The protocol family to check for: IGMP -> ETH_P_IP, MLD -> ETH_P_IPV6

 *

 * Checks whether the given interface has a bridge on top and if so returns

 * true if a multicast router is behind one of the other ports of this

 * bridge. Otherwise returns false.

		/* when compiled without IPv6 support, be conservative and

		 * always assume presence of an IPv6 multicast router

 CONFIG_IPV6 */

 if multicast_disabled is true then igmp type can't be set */

bugs.llvm.org/show_bug.cgi?id=45802#c9 */

 SPDX-License-Identifier: GPL-2.0-or-later

	/* In case the node behaves as MRA then the Test frame needs to have

	 * an Option TLV which includes eventually a sub-option TLV that has

	 * the type AUTO_MGR

 32 bit alligment shall be ensured therefore add 2 bytes */

/* This function is continuously called in the following cases:

 * - when node role is MRM, in this case test_monitor is always set to false

 *   because it needs to notify the userspace that the ring is open and needs to

 *   send MRP_Test frames

 * - when node role is MRA, there are 2 subcases:

 *     - when MRA behaves as MRM, in this case is similar with MRM role

 *     - when MRA behaves as MRC, in this case test_monitor is set to true,

 *       because it needs to detect when it stops seeing MRP_Test frames

 *       from MRM node but it doesn't need to send MRP_Test frames.

		/* Notify that the ring is open only if the ring state is

		 * closed, otherwise it would continue to notify at every

		 * interval.

		 * Also notify that the ring is open when the node has the

		 * role MRA and behaves as MRC. The reason is that the

		 * userspace needs to know when the MRM stopped sending

		 * MRP_Test frames so that the current node to try to take

		 * the role of a MRM.

/* This function is continuously called when the node has the interconnect role

 * MIM. It would generate interconnect test frames and will send them on all 3

 * ports. But will also check if it stop receiving interconnect test frames.

		/* Notify that the interconnect ring is open only if the

		 * interconnect ring state is closed, otherwise it would

		 * continue to notify at every interval.

/* Deletes the MRP instance.

 * note: called under rtnl_lock

 Stop sending MRP_Test frames */

 Stop sending MRP_InTest frames if has an interconnect role */

 Disable the roles */

 Reset the ports */

/* Adds a new MRP instance.

 * note: called under rtnl_lock

	/* If the ring exists, it is not possible to create another one with the

	 * same ring_id

 It is not possible to have the same port part of multiple rings */

/* Deletes the MRP instance from which the port is part of

 * note: called under rtnl_lock

 If the port is not part of a MRP instance just bail out */

/* Deletes existing MRP instance based on ring_id

 * note: called under rtnl_lock

/* Set port state, port state can be forwarding, blocked or disabled

 * note: already called with rtnl_lock

/* Set port role, port role can be primary or secondary

 * note: already called with rtnl_lock

/* Set ring state, ring state can be only Open or Closed

 * note: already called with rtnl_lock

/* Set ring role, ring role can be only MRM(Media Redundancy Manager) or

 * MRC(Media Redundancy Client).

 * note: already called with rtnl_lock

 If there is an error just bailed out */

	/* Now detect if the HW actually applied the role or not. If the HW

	 * applied the role it means that the SW will not to do those operations

	 * anymore. For example if the role ir MRM then the HW will notify the

	 * SW when ring is open, but if the is not pushed to the HW the SW will

	 * need to detect when the ring is open

/* Start to generate or monitor MRP test frames, the frames are generated by

 * HW and if it fails, they are generated by the SW.

 * note: already called with rtnl_lock

	/* Try to push it to the HW and if it fails then continue with SW

	 * implementation and if that also fails then return error.

/* Set in state, int state can be only Open or Closed

 * note: already called with rtnl_lock

/* Set in role, in role can be only MIM(Media Interconnection Manager) or

 * MIC(Media Interconnection Client).

 * note: already called with rtnl_lock

 It is not allowed to disable a port that doesn't exist */

 Stop the generating MRP_InTest frames */

 Remove the port */

 It is not possible to have the same port part of multiple rings */

	/* It is not allowed to set a different interconnect port if the mrp

	 * instance has already one. First it needs to be disabled and after

	 * that set the new port

 If there is an error just bailed out */

	/* Now detect if the HW actually applied the role or not. If the HW

	 * applied the role it means that the SW will not to do those operations

	 * anymore. For example if the role is MIM then the HW will notify the

	 * SW when interconnect ring is open, but if the is not pushed to the HW

	 * the SW will need to detect when the interconnect ring is open.

/* Start to generate MRP_InTest frames, the frames are generated by

 * HW and if it fails, they are generated by the SW.

 * note: already called with rtnl_lock

	/* Try to push it to the HW and if it fails then continue with SW

	 * implementation and if that also fails then return error.

 Determine if the frame type is a ring frame */

 Determine if the frame type is an interconnect frame */

/* Process only MRP Test frame. All the other MRP frames are processed by

 * userspace application

 * note: already called with rcu_read_lock

	/* Each MRP header starts with a version field which is 16 bits.

	 * Therefore skip the version and get directly the TLV header.

	/* Notify the userspace that the ring is closed only when the ring is

	 * not closed

 Determine if the test hdr has a better priority than the node */

/* Process only MRP Test frame. All the other MRP frames are processed by

 * userspace application

 * note: already called with rcu_read_lock

	/* Each MRP header starts with a version field which is 16 bits.

	 * Therefore skip the version and get directly the TLV header.

	/* Only frames that have a better priority than the node will

	 * clear the miss counter because otherwise the node will need to behave

	 * as MRM.

/* Process only MRP InTest frame. All the other MRP frames are processed by

 * userspace application

 * note: already called with rcu_read_lock

	/* Each MRP header starts with a version field which is 16 bits.

	 * Therefore skip the version and get directly the TLV header.

 The check for InTest frame type was already done */

 It needs to process only it's own InTest frames. */

	/* Notify the userspace that the ring is closed only when the ring is

	 * not closed

/* Get the MRP frame type

 * note: already called with rcu_read_lock

	/* Each MRP header starts with a version field which is 16 bits.

	 * Therefore skip the version and get directly the TLV header.

/* This will just forward the frame to the other mrp ring ports, depending on

 * the frame type, ring role and interconnect role

 * note: already called with rcu_read_lock

 If port is disabled don't accept any frames */

	/* If the frame is a ring frame then it is not required to check the

	 * interconnect role and ports to process or forward the frame

 If the role is MRM then don't forward the frames */

		/* If the role is MRA then don't forward the frames if it

		 * behaves as MRM node

		/* If the ring port is in block state it should not forward

		 * In_Test frames

		/* Nodes that behaves as MRM needs to stop forwarding the

		 * frames in case the ring is closed, otherwise will be a loop.

		 * In this case the frame is no forward between the ring ports.

		/* A node that behaves as MRC and doesn't have a interconnect

		 * role then it should forward all frames between the ring ports

		 * because it doesn't have an interconnect port

				/* MIM should not forward it's own InTest

				 * frames

				/* MIM should forward IntLinkChange/Status and

				 * IntTopoChange between ring ports but MIM

				 * should not forward IntLinkChange/Status and

				 * IntTopoChange if the frame was received at

				 * the interconnect port

			/* MIC should forward InTest frames on all ports

			 * regardless of the received port

			/* MIC should forward IntLinkChange frames only if they

			 * are received on ring ports to all the ports

			/* MIC should forward IntLinkStatus frames only to

			 * interconnect port if it was received on a ring port.

			 * If it is received on interconnect port then, it

			 * should be forward on both ring ports

			/* Should forward the InTopo frames only between the

			 * ring ports

 In all the other cases don't forward the frames */

/* Check if the frame was received on a port that is part of MRP ring

 * and if the frame has MRP eth. In that case process the frame otherwise do

 * normal forwarding.

 * note: already called with rcu_read_lock

 If there is no MRP instance do normal forwarding */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Handle firewalling

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

 *	Bart De Schuymer		<bdschuym@pandora.be>

 *

 *	Lennert dedicates this file to Kerstin Wurdinger.

 default value is 1 */

 default value is 0 */

 largest possible L2 header, see br_nf_dev_queue_xmit() */

/* When handing a packet over to the IP layer

 * check whether we have a skb that is in the

 * expected format

 Basic sanity checks */

	/* We should really parse IP options here but until

	 * somebody who actually uses IP options complains to

	 * us we'll just silently ignore the options because

	 * we're lazy!

/* Obtain the correct destination MAC address, while preserving the original

 * source MAC address. If we already know this address, we just copy it. If we

 * don't, we use the neighbour framework to find out. In both cases, we make

 * sure that br_handle_frame_finish() is called afterwards.

			/* the neighbour function below overwrites the complete

			 * MAC header, so we save the Ethernet source address and

			 * protocol number.

 tell br_dev_xmit to continue with forwarding */

 FIXME Need to refragment */

/* This requires some explaining. If DNAT has taken place,

 * we will need to fix up the destination Ethernet address.

 * This is also true when SNAT takes place (for the reply direction).

 *

 * There are two cases to consider:

 * 1. The packet was DNAT'ed to a device in the same bridge

 *    port group as it was received on. We can still bridge

 *    the packet.

 * 2. The packet was DNAT'ed to a different device, either

 *    a non-bridged device or another bridge port group.

 *    The packet will need to be routed.

 *

 * The correct way of distinguishing between these two cases is to

 * call ip_route_input() and to look at skb->dst->dev, which is

 * changed to the destination device if ip_route_input() succeeds.

 *

 * Let's first consider the case that ip_route_input() succeeds:

 *

 * If the output device equals the logical bridge device the packet

 * came in on, we can consider this bridging. The corresponding MAC

 * address will be obtained in br_nf_pre_routing_finish_bridge.

 * Otherwise, the packet is considered to be routed and we just

 * change the destination MAC address so that the packet will

 * later be passed up to the IP stack to be routed. For a redirected

 * packet, ip_route_input() will give back the localhost as output device,

 * which differs from the bridge device.

 *

 * Let's now consider the case that ip_route_input() fails:

 *

 * This can be because the destination address is martian, in which case

 * the packet will be dropped.

 * If IP forwarding is disabled, ip_route_input() will fail, while

 * ip_route_output_key() can return success. The source

 * address for ip_route_output_key() is set to zero, so ip_route_output_key()

 * thinks we're handling a locally generated packet and won't care

 * if IP forwarding is enabled. If the output device equals the logical bridge

 * device, we proceed as if ip_route_input() succeeded. If it differs from the

 * logical bridge port or if ip_route_output_key() fails we drop the packet.

			/* If err equals -EHOSTUNREACH the error is due to a

			 * martian destination or due to the fact that

			 * forwarding is disabled. For most martian packets,

			 * ip_route_output_key() will fail. It won't fail for 2 types of

			 * martian destinations: loopback destinations and destination

			 * 0.0.0.0. In both cases the packet will be dropped because the

				/* - Bridged-and-DNAT'ed traffic doesn't

 Some common code for IPv4/IPv6 */

 Must drop socket now because of tproxy. */

/* Direct IPv6 traffic to br_nf_pre_routing_ipv6.

 * Replicate the checks that IPv4 does on packet reception.

 * Set skb->dev to the bridge device (i.e. parent of the

 * receiving device) to make netfilter happy, the REDIRECT

 * target in particular.  Save the original destination IP

 PF_BRIDGE/FORWARD *************************************************/

/* This is the 'purely bridged' case.  For IP, we pass the packet to

 * netfilter with indev and outdev set to the bridge device,

 * but we are still able to filter on the 'real' indev/outdev

 * because of the physdev module. For ARP, indev and outdev are the

	/* Need exclusive nf_bridge_info since we might have multiple

	/* This is wrong! We should preserve the original fragment

	 * boundaries by preserving frag_list rather than refragmenting.

 PF_BRIDGE/POST_ROUTING ********************************************/

	/* if nf_bridge is set, but ->physoutdev is NULL, this packet came in

	 * on a bridge, but was delivered locally and is now being routed:

	 *

	 * POST_ROUTING was already invoked from the ip stack.

 IP/SABOTAGE *****************************************************/

/* Don't hand locally destined packets to PF_INET(6)/PRE_ROUTING

/* This is called when br_netfilter has called into iptables/netfilter,

 * and DNAT has taken place on a bridge-forwarded packet.

 *

 * neigh->output has created a new MAC header, with local br0 MAC

 * as saddr.

 *

 * This restores the original MAC saddr of the bridged packet

 * before invoking bridge forward logic to transmit the packet.

/* For br_nf_post_routing, we need (prio = NF_BR_PRI_LAST), because

/* recursively invokes nf_hook_slow (again), skipping already-called

 * hooks (< NF_BR_PRI_BRNF).

 *

 * Called with rcu read lock held.

 SPDX-License-Identifier: GPL-2.0-or-later

	/* When this function is called for a port then the br pointer is

	 * invalid, therefor set the br to point correctly

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Ioctl handler

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

 called with RTNL */

/*

 * Format up to a page worth of forwarding table entries

 * userbuf -- where to copy result

 * maxnum  -- maximum number of entries desired

 *            (limited to a page for sanity)

 * offset  -- number of records to skip

 Clamp size to PAGE_SIZE, test maxnum to avoid overflow */

 called with RTNL */

/*

 * Legacy ioctl's through SIOCDEVPRIVATE

 * This interface is deprecated because it was too difficult

 * to do the translation for 32/64bit ioctl compatibility.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Handle firewalling

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

 *	Bart De Schuymer		<bdschuym@pandora.be>

 *

 *	Lennert dedicates this file to Kerstin Wurdinger.

/* We only check the length. A bridge shouldn't do any hop-by-hop stuff

 * anyway

	/* No IP options in IPv6 header; however it should be

	 * checked if some next headers need special treatment

/* PF_BRIDGE/PRE_ROUTING: Undo the changes made for ip6tables

 * PREROUTING and continue the bridge PRE_ROUTING hook. See comment

 * for br_nf_pre_routing_finish(), same logic is used here but

 * equivalent IPv6 function ip6_route_input() called indirectly.

/* Replicate the checks that IPv6 does on packet reception and pass the packet

 * to ip6tables.

 SPDX-License-Identifier: GPL-2.0-only

 Copyright (c) 2020, Nikolay Aleksandrov <nikolay@cumulusnetworks.com>

 check if the options' state of v_curr allow it to enter the range */

 BRIDGE_VLANDB_ENTRY_STATE */

 BRIDGE_VLANDB_ENTRY_TUNNEL_INFO */

 BRIDGE_VLANDB_TINFO_ID */

 BRIDGE_VLANDB_ENTRY_MCAST_ROUTER */

 when working on vlan ranges this is the starting tunnel id */

 vlan info attr is guaranteed by br_vlan_rtm_process_one */

		/* tunnel ids are mapped to each vlan in increasing order,

		 * the starting vlan is in BRIDGE_VLANDB_ENTRY_INFO and v is the

		 * current vlan, so we compute: tun_id + v - vinfo->vid

 vlan options changed, check for range */

 nothing changed and nothing to notify yet */

 BRIDGE_VLANDB_GLOBAL_OPTIONS */

 BRIDGE_VLANDB_GOPTS_ID */

 BRIDGE_VLANDB_GOPTS_MCAST_SNOOPING */

 BRIDGE_VLANDB_GOPTS_MCAST_IGMP_VERSION */

 BRIDGE_VLANDB_GOPTS_MCAST_MLD_VERSION */

 BRIDGE_VLANDB_GOPTS_MCAST_LAST_MEMBER_CNT */

 BRIDGE_VLANDB_GOPTS_MCAST_STARTUP_QUERY_CNT */

 BRIDGE_VLANDB_GOPTS_MCAST_LAST_MEMBER_INTVL */

 BRIDGE_VLANDB_GOPTS_MCAST_MEMBERSHIP_INTVL */

 BRIDGE_VLANDB_GOPTS_MCAST_QUERIER_INTVL */

 BRIDGE_VLANDB_GOPTS_MCAST_QUERY_INTVL */

 BRIDGE_VLANDB_GOPTS_MCAST_QUERY_RESPONSE_INTVL */

 BRIDGE_VLANDB_GOPTS_MCAST_STARTUP_QUERY_INTVL */

 BRIDGE_VLANDB_GOPTS_MCAST_QUERIER */

 BRIDGE_VLANDB_GOPTS_MCAST_QUERIER_STATE */

 BRIDGE_VLANDB_GOPTS_MCAST_ROUTER_PORTS */

 BRIDGE_VLANDB_GOPTS_MCAST_ROUTER_PORTS */

 BRIDGE_VLANDB_GOPTS_RANGE */

 right now notifications are done only with rtnl held */

 need to find the vlan due to flags/options */

 vlan options changed, check for range */

 nothing changed and nothing to notify yet */

 SPDX-License-Identifier: GPL-2.0-or-later

	/* In case of success just return and notify the SW that doesn't need

	 * to do anything

 Continue with SW backup */

	/* If the driver can't configure to run completely the protocol in HW,

	 * then try again to configure the HW so the SW can run the protocol.

	/* If the driver can't configure to run completely the protocol in HW,

	 * then try again to configure the HW so the SW can run the protocol.

 SPDX-License-Identifier: GPL-2.0-or-later

 Calculate the CCM interval in us. */

 Convert the interface interval to CCM PDU value. */

 Convert the CCM PDU value to interval on interface. */

	/* Function ccm_rx_dwork must be called with 1/4

	 * of the configured CC 'expected_interval'

	 * in order to detect CCM defect after 3.25 interval.

	/* The device cannot be deleted until the work_queue functions has

	 * completed. This function is called from ccm_tx_work_expired()

	 * that is a work_queue functions.

 Ethernet header */

 Common CFM Header */

 Sequence number */

 ITU reserved (CFM_CCM_ITU_RESERVED_SIZE octets) */

	/* Generel CFM TLV format:

	 * TLV type:		one byte

	 * TLV value length:	two bytes

	 * TLV value:		'TLV value length' bytes

 Port status TLV. The value length is 1. Total of 4 bytes. */

 Value length */

 Interface status TLV. The value length is 1. Total of 4 bytes. */

 Value length */

 End TLV */

/* This function is called with the configured CC 'expected_interval'

 * in order to drive CCM transmission when enabled.

 Transmission period has ended */

/* This function is called with 1/4 of the configured CC 'expected_interval'

 * in order to detect CCM defect after 3.25 interval.

 After 13 counts (4 * 3,25) then 3.25 intervals are expired */

 3.25 intervals are NOT expired without CCM reception */

 Start timer again */

		/* 3.25 intervals are expired without CCM reception.

		 * CCM defect detected

 Change in CCM defect status - notify */

 TLV is present - get the status TLV */

 Interface status TLV */

 Port status TLV */

 The Sender ID TLV is not handled */

 The Organization-Specific TLV is not handled */

	/* Return the length of this tlv.

	 * This is the length of the value field plus 3 bytes for size of type

	 * field and length field

 note: already called with rcu_read_lock */

 No MEP on this port - must be forwarded */

 The level is above this MEP level - must be forwarded */

 Invalid version */

 The level is below this MEP level */

 CCM PDU received. */

 MA ID is after common header + sequence number + MEP ID */

 MA ID not as expected */

 MEP ID is after common header + sequence number */

 Interval is in common header flags */

 Interval not as expected */

 A valid CCM frame is received */

 Change in CCM defect status - notify */

 Start CCM RX timer */

 RDI is in common header flags */

 Sequence number is after common header */

 Unexpected sequence number */

		/* TLV end is after common header + sequence number + MEP ID +

		 * MA ID + ITU reserved

 Handle all TLVs */

 Max four TLVs possible */

 In PORT domain only one instance can be created per port */

 Empty and free peer MEP list */

 Check for no change in configuration */

 CC is enabled */

 CC is disabled */

 No change in tx_info. */

 Transmission is not enabled - just return */

 Transmission is ongoing, the end time is recalculated */

 Some change in info and transmission is not ongoing */

		/* Some change in info and transmission is ongoing

		 * The end time is recalculated

	/* Start delayed work to transmit CCM frames. It is done with zero delay

	 * to send first frame immediately

/* Deletes the CFM instances on a specific bridge port

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Spanning tree protocol; interface code

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

/* Port id is composed of priority and port number.

 * NB: some bits of priority are dropped to

 *     make room for more ports.

 called under bridge lock */

 NO locks held */

 NO locks held */

 called under bridge lock */

 called under bridge lock */

 call userspace STP and report program errors */

 To start timers on any ports left in blocking */

 To start timers on any ports left in blocking */

 called under bridge lock */

 should be aligned on 2 bytes for ether_addr_equal() */

 should be aligned on 2 bytes for ether_addr_equal() */

 called under bridge lock */

 user has chosen a value so keep it */

 no change */

 Acquires and releases bridge lock */

 called under bridge lock */

 called under bridge lock */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Handle bridge arp/nd proxy/suppress

 *

 *  Copyright (C) 2017 Cumulus Networks

 *  Copyright (c) 2017 Roopa Prabhu <roopa@cumulusnetworks.com>

 *

 *  Authors:

 *	Roopa Prabhu <roopa@cumulusnetworks.com>

 check if ip is configured on upper dev */

 sha */

 tha */

 prevent flooding to neigh suppress ports */

		/* its our local ip, so don't proxy reply

		 * and don't forward to neigh suppress ports

			/* If we have replied or as long as we know the

			 * mac, indicate to arp replied

 opt hdr + ETH_ALEN for target */

 Do we need option processing ? */

 Ethernet header */

 IPv6 header */

 Neighbor Advertisement */

 check if ip is configured on upper dev */

 prevent flooding to neigh suppress ports */

 prevent flooding to neigh suppress ports */

 build neigh table lookup on the vlan device */

		/* its our own ip, so don't proxy reply

		 * and don't forward to arp suppress ports

			/* If we have replied or as long as we know the

			 * mac, indicate to NEIGH_SUPPRESS ports that we

			 * have replied

 SPDX-License-Identifier: GPL-2.0-or-later

	/* When this function is called for a port then the br pointer is

	 * invalid, therefor set the br to point correctly

 Only clear if this is a GETLINK */

 Clear all 'seen' indications */

 Only clear if this is a GETLINK */

 Clear all 'seen' indications */

 SPDX-License-Identifier: GPL-2.0

 Mark the frame for TX forwarding offload if this egress port supports it */

/* Lazily adds the hwdom of the egress bridge port to the bit mask of hwdoms

 * that the skb has been already forwarded to, to avoid further cloning to

 * other ports in the same hwdom by making nbp_switchdev_allowed_egress()

 * return false.

 Flags that can be offloaded to hardware */

 We run from atomic context here */

 joining is yet to be added to the port list. */

 leaving is no longer in the port list. */

		/* Prevent unsupported configurations such as a bridge port

		 * which is a bonding interface, and the member ports are from

		 * different hardware switches.

		/* Tolerate drivers that call switchdev_bridge_port_offload()

		 * more than once for the same bridge port, such as when the

		 * bridge port is an offloaded bonding/team interface.

	/* We cannot walk over br->mdb_list protected just by the rtnl_mutex,

	 * because the write-side protection is br->multicast_lock. But we

	 * need to emulate the [ blocking ] calling context of a regular

	 * switchdev event, so since both br->multicast_lock and RCU read side

	 * critical sections are atomic, we have no choice but to pick the RCU

	 * read side lock, queue up all our events, leave the critical section

	 * and notify switchdev from blocking context.

/* Let the bridge know that this port is offloaded, so that it can assign a

 * switchdev hardware domain to it.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Bridge per vlan tunnel port dst_metadata netlink control interface

 *

 *	Authors:

 *	Roopa Prabhu		<roopa@cumulusnetworks.com>

 nest IFLA_BRIDGE_VLAN_TUNNEL_INFO */

 IFLA_BRIDGE_VLAN_TUNNEL_ID */

 IFLA_BRIDGE_VLAN_TUNNEL_VID */

 IFLA_BRIDGE_VLAN_TUNNEL_FLAGS */

 Count number of vlan infos */

 only a context, bridge vlan not activated */

 add range to skb */

 Count number of vlan infos */

 only a context, bridge vlan not activated */

 send a notification if v_curr can't enter the range and start a new one */

 we start a range only if there are any changes to notify about */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Bridge netlink control interface

 *

 *	Authors:

 *	Stephen Hemminger		<shemminger@osdl.org>

 Count number of vlan infos */

 only a context, bridge vlan not activated */

 Each VLAN is returned in bridge_vlan_info along with flags */

 CFM status info must be added */

 IFLA_BRIDGE_CFM */

 For each status struct the MEP instance (u32) is added */

 MEP instance (u32) + br_cfm_mep_status */

IFLA_BRIDGE_CFM_MEP_STATUS_INSTANCE */

 IFLA_BRIDGE_CFM_MEP_STATUS_OPCODE_UNEXP_SEEN */

 IFLA_BRIDGE_CFM_MEP_STATUS_VERSION_UNEXP_SEEN */

 IFLA_BRIDGE_CFM_MEP_STATUS_RX_LEVEL_LOW_SEEN */

 MEP instance (u32) + br_cfm_cc_peer_status */

 IFLA_BRIDGE_CFM_CC_PEER_STATUS_INSTANCE */

 IFLA_BRIDGE_CFM_CC_PEER_STATUS_PEER_MEPID */

 IFLA_BRIDGE_CFM_CC_PEER_STATUS_CCM_DEFECT */

 IFLA_BRIDGE_CFM_CC_PEER_STATUS_RDI */

 IFLA_BRIDGE_CFM_CC_PEER_STATUS_PORT_TLV_VALUE */

 IFLA_BRIDGE_CFM_CC_PEER_STATUS_IF_TLV_VALUE */

 IFLA_BRIDGE_CFM_CC_PEER_STATUS_SEEN */

 IFLA_BRIDGE_CFM_CC_PEER_STATUS_TLV_SEEN */

 IFLA_BRIDGE_CFM_CC_PEER_STATUS_SEQ_UNEXP_SEEN */

 IFLA_BRPORT_STATE  */

 IFLA_BRPORT_PRIORITY */

 IFLA_BRPORT_COST */

 IFLA_BRPORT_MODE */

 IFLA_BRPORT_GUARD */

 IFLA_BRPORT_PROTECT */

 IFLA_BRPORT_FAST_LEAVE */

 IFLA_BRPORT_MCAST_TO_UCAST */

 IFLA_BRPORT_LEARNING */

 IFLA_BRPORT_UNICAST_FLOOD */

 IFLA_BRPORT_MCAST_FLOOD */

 IFLA_BRPORT_BCAST_FLOOD */

 IFLA_BRPORT_PROXYARP */

 IFLA_BRPORT_PROXYARP_WIFI */

 IFLA_BRPORT_VLAN_TUNNEL */

 IFLA_BRPORT_NEIGH_SUPPRESS */

 IFLA_BRPORT_ISOLATED */

 IFLA_BRPORT_ROOT_ID */

 IFLA_BRPORT_BRIDGE_ID */

 IFLA_BRPORT_DESIGNATED_PORT */

 IFLA_BRPORT_DESIGNATED_COST */

 IFLA_BRPORT_ID */

 IFLA_BRPORT_NO */

 IFLA_BRPORT_TOPOLOGY_CHANGE_ACK */

 IFLA_BRPORT_CONFIG_PENDING */

 IFLA_BRPORT_MESSAGE_AGE_TIMER */

 IFLA_BRPORT_FORWARD_DELAY_TIMER */

 IFLA_BRPORT_HOLD_TIMER */

 IFLA_BRPORT_MULTICAST_ROUTER */

 IFLA_BRPORT_GROUP_FWD_MASK */

 IFLA_BRPORT_MRP_RING_OPEN */

 IFLA_BRPORT_MRP_IN_OPEN */

 IFLA_BRPORT_MCAST_EHT_HOSTS_LIMIT */

 IFLA_BRPORT_MCAST_EHT_HOSTS_CNT */

 IFLA_IFNAME */

 IFLA_ADDRESS */

 IFLA_MASTER */

 IFLA_MTU */

 IFLA_LINK */

 IFLA_OPERSTATE */

 IFLA_PROTINFO */

 IFLA_AF_SPEC */

 IFLA_BRPORT_BACKUP_PORT */

 we might be called only with br->lock */

 add range to skb */

	/* Pack IFLA_BRIDGE_VLAN_INFO's for every vlan

	 * and mark vlan info with begin and end flags

	 * if vlaninfo represents a range

 Call it once more to send any left over vlans */

/*

 * Create one netlink message for one interface

 * Contains port and master info as well as carrier and bridge state.

 Check if  the VID information is requested */

 RCU needed because of the VLAN locking rules (rcu || rtnl) */

 -EMSGSIZE implies BUG in br_nlmsg_size() */

 Notify listeners of a change in bridge or port information */

/*

 * Dump information about all ports, in response to GETLINK

			/* if the MASTER flag is set this will act on the global

			 * per-VLAN entry as well

 needed for vlan-only NEWVLAN/DELVLAN notifications */

 nothing to notify yet */

 v_change_start is set only if the last/whole range changed */

 Change the state of the port and notify spanning tree */

 if kernel STP is running, don't allow changes */

	/* if device is not up, change is not allowed

	 * if link is not present, only allowable state is disabled

 Set/clear or port flags based on attribute */

 Process bridge protocol info on port */

 Change state and parameters on port. */

	/* We want to accept dev as bridge itself if the AF_SPEC

	 * is set to see if someone is setting vlan info on the bridge

 Binary compatibility with old RSTP */

 Delete port information */

 We want to accept dev as bridge itself as well */

		/* Send RTM_NEWLINK because userspace

		 * expects RTM_NEWLINK for vlan dels

 802.3x Pause address */

 802.3ad Slow protocols */

 802.1X PAE address */

 IFLA_BR_FORWARD_DELAY  */

 IFLA_BR_HELLO_TIME */

 IFLA_BR_MAX_AGE */

 IFLA_BR_AGEING_TIME */

 IFLA_BR_STP_STATE */

 IFLA_BR_PRIORITY */

 IFLA_BR_VLAN_FILTERING */

 IFLA_BR_VLAN_PROTOCOL */

 IFLA_BR_VLAN_DEFAULT_PVID */

 IFLA_BR_VLAN_STATS_ENABLED */

 IFLA_BR_VLAN_STATS_PER_PORT */

 IFLA_BR_GROUP_FWD_MASK */

 IFLA_BR_ROOT_ID */

 IFLA_BR_BRIDGE_ID */

 IFLA_BR_ROOT_PORT */

 IFLA_BR_ROOT_PATH_COST */

 IFLA_BR_TOPOLOGY_CHANGE */

 IFLA_BR_TOPOLOGY_CHANGE_DETECTED */

 IFLA_BR_HELLO_TIMER */

 IFLA_BR_TCN_TIMER */

 IFLA_BR_TOPOLOGY_CHANGE_TIMER */

 IFLA_BR_GC_TIMER */

 IFLA_BR_GROUP_ADDR */

 IFLA_BR_MCAST_ROUTER */

 IFLA_BR_MCAST_SNOOPING */

 IFLA_BR_MCAST_QUERY_USE_IFADDR */

 IFLA_BR_MCAST_QUERIER */

 IFLA_BR_MCAST_STATS_ENABLED */

 IFLA_BR_MCAST_HASH_ELASTICITY */

 IFLA_BR_MCAST_HASH_MAX */

 IFLA_BR_MCAST_LAST_MEMBER_CNT */

 IFLA_BR_MCAST_STARTUP_QUERY_CNT */

 IFLA_BR_MCAST_LAST_MEMBER_INTVL */

 IFLA_BR_MCAST_MEMBERSHIP_INTVL */

 IFLA_BR_MCAST_QUERIER_INTVL */

 IFLA_BR_MCAST_QUERY_INTVL */

 IFLA_BR_MCAST_QUERY_RESPONSE_INTVL */

 IFLA_BR_MCAST_STARTUP_QUERY_INTVL */

 IFLA_BR_MCAST_IGMP_VERSION */

 IFLA_BR_MCAST_MLD_VERSION */

 IFLA_BR_MCAST_QUERIER_STATE */

 IFLA_BR_NF_CALL_IPTABLES */

 IFLA_BR_NF_CALL_IP6TABLES */

 IFLA_BR_NF_CALL_ARPTABLES */

 IFLA_BR_MULTI_BOOLOPT */

 we need to count all, even placeholder entries */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Forwarding database

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

/* if topology_changing then use forward_delay (default 15 sec)

 * otherwise keep longer (default 5 minutes)

 NDA_LLADDR */

 NDA_MASTER */

 NDA_VLAN */

 NDA_FDB_EXT_ATTRS */

 NFEA_ACTIVITY_NOTIFY */

 -EMSGSIZE implies BUG in fdb_nlmsg_size() */

 requires bridge hash_lock */

/* When a static FDB entry is added, the mac address from the entry is

 * added to the bridge private HW address list and all required ports

 * are then updated with the new information.

 * Called under RTNL.

/* When a static FDB entry is deleted, the HW address from that entry is

 * also removed from the bridge private HW address list and updates all

 * the ports with needed information.

 * Called under RTNL.

 Delete a local entry if no other port had the same address. */

 Maybe another port has same hw addr? */

 Maybe bridge device has same hw addr? */

		/* it is okay to have multiple ports with same

		 * address, just use the first one.

 delete old one */

			/* if this port has no vlan information

			 * configured, we can safely be done at

			 * this point.

 insert new address,  may fail if invalid address or dup. */

	/* Now add entries for every VLAN configured on the port.

	 * This function runs under RTNL so the bitmap will not change

	 * from under us.

 If old entry was unassociated with any port, then delete it. */

	/* Now remove and add entries for every VLAN configured on the

	 * bridge.  This function runs under RTNL so the bitmap will not

	 * change from under us.

	/* this part is tricky, in order to avoid blocking learning and

	 * consequently forwarding, we rely on rcu to delete objects with

	 * delayed freeing allowing us to continue traversing

 Cleanup minimum 10 milliseconds apart */

 Completely flush all dynamic entries in forwarding database.*/

/* Flush all entries referring to a specific port.

 * if do_all is set also flush static entries

 * if vid is set delete all entries that match the vlan_id

/* Interface used by ATM LANE hook to test

 CONFIG_ATM_LANE */

/*

 * Fill buffer with forwarding table records in

 * the API format.

 ignore pseudo entry for local MAC address */

 convert from internal format to API */

 due to ABI compat need to split into hi/lo */

 Add entry for local address of interface */

 returns true if the fdb was modified */

 some users want to always flood. */

 attempt to update an entry for a local interface */

 fastpath: update of existing entry */

 Take over HW learned entry */

		/* else  we lose race and someone else inserts

		 * it first, don't bother updating

 Dump information about entries, in response to GETNEIGH */

			/* !f->dst is a special case for bridge

			 * It means the MAC belongs to the bridge

			 * Therefore need a little more filtering

			 * we only want to dump the !f->dst case

 returns true if the fdb is modified */

 allow to mark an entry as inactive, usually done on creation */

 enabled activity tracking */

 disabled activity tracking, clear notify state */

 Update (create or replace) forwarding database entry */

 If the port cannot learn allow only local and static entries */

 Add new permanent fdb entry with RTM_NEWNEIGH */

 VID was specified, so use it. */

		/* We have vlans configured on this port and user didn't

		 * specify a VLAN.  To be nice, add/update entry for every

		 * vlan on this port.

 Remove neighbor entry with RTM_DELNEIGH */

 the key here is that static entries change only under rtnl */

 We only care for static entries */

 We only care for static entries */

 We only care for static entries */

 Refresh entry */

 Take over SW learned entry */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Spanning tree protocol; BPDU handling

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

 called under bridge lock */

 called under bridge lock */

/*

 * Called from llc.

 *

 * NO locks, but rcu_read_lock

 compare of protocol id and version */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Handle incoming frames

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

	/* Bridge is just like any other port.  Make sure the

	 * packet is allowed except in promisc mode when someone

	 * may be running packet capture.

 update the multicast stats if the packet is IGMP/MLD */

 note: already called with rcu_read_lock */

 insert into forwarding database after filtering to avoid spoofing */

 by definition the broadcast is also a multicast address */

 check if vlan is allowed, to avoid spoofing */

 note: already called with rcu_read_lock */

 return 1 to signal the okfn() was called so it's ok to use the skb */

 STOLEN */

/* Return 0 if the frame was not processed otherwise 1

 * note: already called with rcu_read_lock

/*

 * Return NULL if skb is handled

 * note: already called with rcu_read_lock

		/*

		 * See IEEE 802.1D Table 7-10 Reserved addresses

		 *

		 * Assignment		 		Value

		 * Bridge Group Address		01-80-C2-00-00-00

		 * (MAC Control) 802.3		01-80-C2-00-00-01

		 * (Link Aggregation) 802.3	01-80-C2-00-00-02

		 * 802.1X PAE address		01-80-C2-00-00-03

		 *

		 * 802.1AB LLDP 		01-80-C2-00-00-0E

		 *

		 * Others reserved for future standardization

 Bridge Group Address */

			/* If STP is turned off,

 IEEE MAC (Pause) */

 802.1AB LLDP */

 Allow selective forwarding for most other protocols */

		/* The else clause should be hit when nf_hook():

		 *   - returns < 0 (drop/error)

		 *   - returns = 0 (stolen/nf_queue)

		 * Thus return 1 from the okfn() to signal the skb is ok to pass

/* This function has no purpose other than to appease the br_port_get_rcu/rtnl

 * helpers which identify bridged ports according to the rx_handler installed

 * on them (so there _needs_ to be a bridge rx_handler even if we don't need it

 * to do anything useful). This bridge won't support traffic to/from the stack,

 * but only hardware bridging. So return RX_HANDLER_PASS so we don't steal

 * frames from the ETH_P_XDSA packet_type handler.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Handle firewalling core

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

 *	Bart De Schuymer		<bdschuym@pandora.be>

 *

 *	Lennert dedicates this file to Kerstin Wurdinger.

/*

 * Initialize bogus route table used to keep netfilter happy.

 * Currently, we fill in the PMTU entry because netfilter

 * refragmentation needs it, and the rt_flags entry because

 * ipt_REJECT needs it.  Future netfilter modules might

 * require us to fill additional fields.

 SPDX-License-Identifier: GPL-2.0

 MDBA_ROUTER_PORT */

 MDBA_ROUTER_PATTR_TIMER */

 MDBA_ROUTER_PATTR_TYPE */

 MDBA_ROUTER_PATTR_INET_TIMER */

 MDBA_ROUTER_PATTR_INET6_TIMER */

 MDBA_ROUTER_PATTR_VID */

 MDBA_ROUTER */

 MDBA_MDB_EATTR_RTPROT */

 MDBA_MDB_EATTR_SOURCE */

 MDBA_MDB_EATTR_SOURCE */

 MDBA_MDB_EATTR_GROUP_MODE */

 MDBA_MDB_EATTR_SRC_LIST nested attr */

		/* MDBA_MDB_SRCLIST_ENTRY nested attr +

		 * MDBA_MDB_SRCATTR_ADDRESS + MDBA_MDB_SRCATTR_TIMER

 L2 mdb */

 host join errors which can happen before creating the group */

 don't allow any flags for host-joined groups */

 host join */

	/* if we are adding a new EXCLUDE port group (*,G) it needs to be also

	 * added to all S,G entries for proper replication, if we are adding

	 * a new INCLUDE port (S,G) then all of *,G EXCLUDE ports need to be

	 * added to it for proper replication

	/* If vlan filtering is enabled and VLAN is not specified

	 * install mdb entry on all vlans configured on the port.

 host leave */

	/* If vlan filtering is enabled and VLAN is not specified

	 * delete mdb entry on all vlans configured on the port.

 SPDX-License-Identifier: GPL-2.0-only

 return true if anything changed, false otherwise */

	/* Try switchdev op first. In case it is not supported, fallback to

	 * 8021q add.

	/* Try switchdev op first. In case it is not supported, fallback to

	 * 8021q del.

/* Returns a master vlan, if it didn't exist it gets created. In all cases

 * a reference is taken to the master vlan before returning.

 missing global ctx, create it now */

 if we had per-port stats configured then free them here */

/* This is the shared VLAN add function which works for both ports and bridge

 * devices. There are four possible calls to this function in terms of the

 * vlan entry type:

 * 1. vlan is being added on a port (no master flags, global entry exists)

 * 2. vlan is being added on a bridge (both master and brentry flags)

 * 3. vlan is being added on a port, but a global entry didn't exist which

 *    is being created right now (master flag set, brentry flag unset), the

 *    global entry is used for global per-vlan features, but not for filtering

 * 4. same as 3 but with both master and brentry flags set so the entry

 *    will be used for filtering in both the port and the bridge

		/* Add VLAN to the device filter if it is supported.

		 * This ensures tagged traffic enters the bridge when

		 * promiscuous mode is disabled by br_manage_promisc().

 need to work on the master vlan too */

 Add the dev mac and count the vlan only if it's usable */

 set the state before publishing */

 take care of disjoint ranges */

 found range end, notify and start next one */

 notify about the last/whole vlan range */

 If this packet was not filtered at input, let it pass */

	/* At this point, we know that the frame was filtered and contains

	 * a valid vlan id.  If the vlan id has untagged flag set,

	 * send untagged; otherwise, send tagged.

	/* Vlan entry must be configured at this point.  The

	 * only exception is the bridge is set in promisc mode and the

	 * packet is destined for the bridge device.  In this case

	 * pass the packet as is.

	/* If the skb will be sent using forwarding offload, the assumption is

	 * that the switchdev will inject the packet into hardware together

	 * with the bridge VLAN, so that it can be forwarded according to that

	 * VLAN. The switchdev should deal with popping the VLAN header in

	 * hardware on each egress port as appropriate. So only strip the VLAN

	 * header if forwarding offload is not being used.

 Called under RCU */

	/* If vlan tx offload is disabled on bridge device and frame was

	 * sent from vlan device on the bridge device, it does not have

	 * HW accelerated vlan tag.

 Tagged frame */

 Protocol-mismatch, empty out vlan_tci for new tag */

 Untagged frame */

		/* Frame had a tag with VID 0 or did not have a tag.

		 * See if pvid is set on this port.  That tells us which

		 * vlan untagged or priority-tagged traffic belongs to.

		/* PVID is set on this port.  Any untagged or priority-tagged

		 * ingress frame is considered to belong to this vlan.

 Untagged Frame. */

			/* Priority-tagged Frame.

			 * At this point, we know that skb->vlan_tci VID

			 * field was 0.

			 * We update only VID field and preserve PCP field.

 if snooping and stats are disabled we can avoid the lookup */

	/* If VLAN filtering is disabled on the bridge, all packets are

	 * permitted.

 Called under RCU. */

 If this packet was not filtered at input, let it pass */

 Called under RCU */

 If filtering was disabled at input, let it pass. */

 Trying to change flags of non-existent bridge vlan */

 It was only kept for port vlans, now make it real */

/* Must be protected by RTNL.

 * Must be called with vid in range from 1 to 4094 inclusive.

 * changed must be true only if the vlan was created or updated

/* Must be protected by RTNL.

 * Must be called with vid in range from 1 to 4094 inclusive.

 Must be protected by RTNL. */

 Bridge Group Address */

 vlan_enabled && ETH_P_8021AD */

 Provider Bridge Group Address */

 Must be protected by RTNL. */

 vlan_enabled && ETH_P_8021AD */

 Add VLANs for the new proto to the device filter. */

 Delete VLANs for the old proto from the device filter. */

 allow to change the option if there are no port vlans configured */

	/* Disable default_pvid on all ports where it is still

	 * configured.

	/* Update default_pvid config only if we do not conflict with

	 * user configuration.

		/* Update default_pvid config only if we do not conflict with

		 * user configuration.

 Only allow default pvid change when filtering is disabled */

/* Must be protected by RTNL.

 * Must be called with vid in range from 1 to 4094 inclusive.

 * changed must be true only if the vlan was created or updated

 Pass the flags to the hardware bridge */

/* Must be protected by RTNL.

 * Must be called with vid in range from 1 to 4094 inclusive.

 Must be protected by RTNL. */

 Must be protected by RTNL. */

 Must be protected by RTNL. */

 v_opts is used to dump the options which must be equal in the whole range */

 BRIDGE_VLANDB_ENTRY */

 BRIDGE_VLANDB_ENTRY_RANGE */

 BRIDGE_VLANDB_ENTRY_INFO */

 bridge vlan options */

 right now notifications are done only with rtnl held */

 need to find the vlan due to flags/options */

 check if v_curr can enter a range ending in range_end */

 global options are dumped only for bridge devices */

 idx must stay at range's beginning until it is filled in */

 advance number of filled vlans */

 advance number of filled vlans */

	/* err will be 0 and range_start will be set in 3 cases here:

	 * - first vlan (range_start == range_end)

	 * - last vlan (range_start == range_end, not in range)

	 * - last vlan range (range_start != range_end, in range)

 validate user-provided flags without RANGE_BEGIN */

 vinfo_last is the range start, vinfo the range end */

 br_process_vlan_info may overwrite vinfo_last */

 notify first if anything changed */

 deal with options */

 this should validate the header and check for remaining bytes */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Sysfs attributes of bridge

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Stephen Hemminger		<shemminger@osdl.org>

/* IMPORTANT: new bridge options must be added with netlink support only

 *            please do not add new sysfs entries

/*

 * Common code for storing bridge parameters.

 802.3x Pause address */

 802.3ad Slow protocols */

 802.1X PAE address */

 16 is RHT_ELASTICITY */

/*

 * Export the forwarding information table as a binary file

 * The records are struct __fdb_entry.

 *

 * Returns the number of bytes read.

 must read whole records */

/*

 * Add entries in sysfs onto the existing network class device

 * for the bridge.

 *   Adds a attribute group "bridge" containing tuning parameters.

 *   Binary attribute containing the forward table

 *   Sub directory to hold links to interfaces.

 *

 * Note: the ifobj exists only to be a subdirectory

 *   to hold links.  The ifobj exists in same data structure

 *   as it's parent the bridge so reference counting works.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Device handling code

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

 net device transmit always called with BH disabled */

 this flag will be cleared if the MTU was automatically adjusted */

 remember the MTU in the rtable for PMTU */

 Allow setting mac address to any valid ethernet address. */

	/* dev_set_mac_addr() can be called by a master device on bridge's

	 * NETDEV_UNREGISTER, but since it's being destroyed do nothing

 Mac address will be changed in br_stp_change_bridge_id(). */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Sysfs attributes of bridge ports

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Stephen Hemminger		<shemminger@osdl.org>

/* IMPORTANT: new bridge port options must be added with netlink support only

 *            please do not add new sysfs entries

 Don't delete local entry

/*

 * Add sysfs entries to ethernet device added to a bridge.

 * Creates a brport subdirectory with bridge attributes.

 * Puts symlink in bridge's brif subdirectory

 Rename bridge's brif symlink */

	/* If a rename fails, the rollback will cause another

	 * rename call with the existing name.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Spanning tree protocol; generic parts

 *	Linux ethernet bridge

 *

 *	Authors:

 *	Lennert Buytenhek		<buytenh@gnu.org>

/* since time values in bpdu are in jiffies and then scaled (1/256)

 * before sending, make sure that is at least one STP tick.

	/* Don't change the state of the ports if they are driven by a different

	 * protocol.

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 Don't change port states if userspace is handling STP */

		/* Multicast is not disabled for the port when it goes in

		 * blocking state because the timers will expire and stop by

		 * themselves without sending more queries.

 called under bridge lock */

 called under bridge lock */

 called under bridge lock */

 Change bridge STP parameter */

 called under bridge lock */

/* Set time interval that dynamic forwarding entries live

 * For pure software bridge, allow values outside the 802.1

 * standard specification for special cases:

 *  0 - entry never ages (all permanent)

 *  1 - entry disappears (no persistence)

 *

 * Offloaded switch entries maybe more restrictive

 called under bridge lock */

		/* On topology change, set the bridge ageing time to twice the

		 * forward delay. Otherwise, restore its default ageing time.

 SPDX-License-Identifier: GPL-2.0-or-later

 Copyright (c) 2020, Nikolay Aleksandrov <nikolay@nvidia.com>

 always allow auto-created zero entry */

	/* we must not count the auto-created zero entry otherwise we won't be

	 * able to track the full list of PG_SRC_ENT_LIMIT entries

 create new set entries from reports */

 delete existing set entries and their (S,G) entries if they were the last */

 flush_entries is true when changing mode */

 if we're changing mode del host and its entries */

 we can be missing sets only if we've deleted some entries */

			/* this is an optimization for TO_INCLUDE where we lower

			 * the set's timeout to LMQT to catch timeout hosts:

			 * - host A (timing out): set entries X, Y

			 * - host B: set entry Z (new from current TO_INCLUDE)

			 *           sends BLOCK Z after LMQT but host A's EHT

			 *           entries still exist (unless lowered to LMQT

			 *           so they can timeout with the S,Gs)

			 * => we wait another LMQT, when we can just delete the

			 *    group immediately

 true means an entry was deleted */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_arp

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *	Tim Gardner <timg@tpi.com>

 *

 *  April, 2002

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_pkttype

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *

 *  April, 2003

 *

 Allow any pkt_type value */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_redirect

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *

 *  April, 2002

 *

 rcu_read_lock()ed by nf_hook_thresh */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_mark

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *

 *  July, 2002

 *

/* The mark target can be used in any chain,

 * I believe adding a mangle table just for marking is total overkill.

 * Marking a frame doesn't really change anything in the frame anyway.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014 Pablo Neira Ayuso <pablo@netfilter.org>

/* We cannot use oldskb->dev, it can be either bridge device (NF_BRIDGE INPUT)

 * or the bridge port (NF_BRIDGE PREROUTING).

 No explicit way to reject this protocol, drop it. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  ebtables

 *

 *  Author:

 *  Bart De Schuymer		<bdschuym@pandora.be>

 *

 *  ebtables.c,v 2.0, July, 2002

 *

 *  This code is strongly inspired by the iptables code which is

 *  Copyright (C) 1999 Paul `Rusty' Russell & Michael J. Neuling

 needed for logical [in,out]-dev filtering */

/* Each cpu has its own set of counters, so there is no need for write_lock in

 * the softirq

 * For reading or updating the counters, the user context needs to

 * get a write_lock

 The size of each set of counters is altered to get cache alignment */

 called when table is needed in the given netns */

 watchers don't give a verdict */

 1 is the wildcard token */

 process standard matches */

 rcu_read_lock()ed by nf_hook_thresh */

 Do some firewalling */

 base for chain jumps */

		/* these should only watch: not modify, nor tell us

		 * what to do with the packet

 standard target */

 act like this is EBT_CONTINUE */

 put all the local variables right */

 jump to a udc */

 I actually like this :) */

 If it succeeds, returns element and locks mutex */

				/* we make userspace set this right,

				 * so there is no misunderstanding

 check if all valid hooks have a chain */

/* this one is very careful, as it is the first function

 * to parse the userspace data

	/* beginning of a new chain

	 * if i == NF_BR_NUMHOOKS it must be a user defined chain

		/* this checks if the previous chain has as many entries

		 * as it said it has

 only RETURN from udc */

 it's a user defined chain */

 a plain old entry, heh */

 this is not checked anywhere else */

/* We need these positions to check that the jumps to a different part of the

 * entries is a jump to the beginning of a new chain.

 we're only interested in chain starts */

 only care about udc */

 these initialisations are depended on later in check_chainloops() */

 we're done */

 don't mess with the struct ebt_entries */

 what hook do we belong to? */

	/* (1 << NF_BR_NUMHOOKS) tells the check functions the rule is on

	 * a base chain

 Reject UNSPEC, xtables verdicts/return values are incompatible */

/* checks for loops and sets the hook mask for udc

 * the hook mask for udc tells us from which base chains the udc can be

 * accessed. This mask is a parameter to the check() functions of the extensions

 end of udc, go back one 'recursion' step */

 put back values of the time when this chain was called */

 make sure we won't see a loop that isn't one */

 jump to another chain */

 bad destination or loop */

 this can't be 0, so the loop test is correct */

 this udc is accessible from the base chain for hooknr */

 do the parsing of the table/chains/entries/matches/watchers/targets, heh */

 used in the checking for chain loops */

	/* make sure chains are ordered after each other in same order

	 * as their corresponding hooks

 do some early checkings and initialize some things */

 holds the expected nr. of entries for the chain */

 holds the up to now counted entries for the chain */

	k = 0; /* holds the total nr. of entries, should equal

		* newinfo->nentries afterwards

 will hold the nr. of user defined chains (udc) */

	/* get the location of the udc, put them in an array

	 * while we're at it, allocate the chainstack

		/* this will get free'd in do_replace()/ebt_register_table()

		 * if an error occurs

 the i'th udc */

 sanity check */

 Check for loops */

	/* we now know the following (along with E=mc):

	 *  - the nr of entries in each chain is right

	 *  - the size of the allocated space is right

	 *  - all valid hooks have a corresponding chain

	 *  - there are no loops

	 *  - wrong data can still be on the level of a single entry

	 *  - could be there are jumps to places that are not the

	 *    beginning of a chain. This can only occur in chains that

	 *    are not accessible from any base chains, so we don't care.

 used to know what we need to clean up if something goes wrong */

 called under write_lock */

 counters of cpu 0 */

 add other counters to those of cpu 0 */

 used to be able to unlock earlier */

	/* the user wants counters back

	 * the check on the size is done later, when we have the lock

 the table doesn't like it */

 we have the mutex lock, so no danger in reading this pointer */

 make sure the table can only be rmmod'ed if it contains no rules */

 we need an atomic snapshot of the counters */

	/* so, a user can change the chains while having messed up her counter

	 * allocation. Only reason why this is done is because this way the lock

	 * is held only once, while this doesn't bring the kernel into a

	 * dangerous state.

 Silent error, can't fail, new table is already in place */

 decrease module count and free resources */

 can be initialized in translate_table() */

 replace the table */

 overflow check */

 Don't add one table to multiple lists. */

 fill in newinfo and parse the entries */

 Hold a reference count if the chains aren't empty */

 userspace just supplied us with counters */

 we want an atomic add of the counters */

 we add to the counters of the first cpu */

	/* ebtables expects 31 bytes long names but xt_match names are 29 bytes

	 * long. Copy 29 bytes and fill remaining bytes with zeroes.

 special case !EBT_ENTRY_OR_ENTRIES */

 userspace might not need the counters */

 called with ebt_mutex locked */

 set the match/watcher/target names right */

 32 bit-userspace compatibility definitions. */

 start of the chains */

 nr of counters userspace expects back */

 where the kernel will put the old counters. */

 struct ebt_entry_match, _target and _watcher have same layout */

 account for possible padding between match_size and ->data */

	/* ebt_among needs special handling. The kernel .matchsize is

	 * set to -1 at registration time; at runtime an EBT_ALIGN()ed

	 * value is expected.

	 * Example: userspace sends 4500, ebt_among.c wants 4504.

 also count the base chain policies */

 userspace might not need the counters */

 kernel buffer to copy (translated) data to */

 total size of kernel buffer */

 amount of data copied so far */

 read position in userspace buffer */

 do not adjust ->buf_user_offset here, we added kernel-side padding */

/* return size of all matches, watchers or target, including necessary

 * alignment and padding.

 add padding before match->data (if any) */

 called for all ebt_entry structures. */

 stores match/watchers/targets & offset of next struct ebt_entry: */

 pull in most part of ebt_entry, it does not need to be changed. */

 matches come first */

	/* 0: matches offset, always follows ebt_entry.

	 * 1: watchers offset, from ebt_entry structure

	 * 2: target offset, from ebt_entry structure

	 * 3: next ebt_entry offset, from ebt_entry structure

	 *

	 * offsets are relative to beginning of struct ebt_entry (i.e., 0).

/* repl->entries_size is the size of the ebt_entry blob in userspace.

 * It might need more memory when copied to a 64 bit kernel in case

 * userspace is 32-bit. So, first task: find out how much memory is needed.

 *

 * Called before validation is performed.

 starting with hook_entry, 32 vs. 64 bit structures are different */

 try real handler in case userland supplied needed padding */

 try real handler in case userland supplied needed padding */

		/* try real handler first in case of userland-side padding.

		 * in case we are dealing with an 'ordinary' 32 bit binary

		 * without 64bit compatibility padding, this will fail right

		 * after copy_from_user when the *len argument is validated.

		 *

		 * the compat_ variant needs to do one pass over the kernel

		 * data set to adjust for size differences before it the check.

 try real handler in case userland supplied needed padding */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ebt_nflog

 *

 *	Author:

 *	Peter Warasin <peter@endian.com>

 *

 *  February, 2008

 *

 * Based on:

 *  xt_NFLOG.c, (C) 2006 by Patrick McHardy <kaber@trash.net>

 *  ebt_ulog.c, (C) 2004 by Bart De Schuymer <bdschuym@pandora.be>

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebtable_nat

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *

 *  April, 2002

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_ip6

 *

 *	Authors:

 *	Manohar Castelino <manohar.r.castelino@intel.com>

 *	Kuo-Lang Tseng <kuo-lang.tseng@intel.com>

 *	Jan Engelhardt <jengelh@medozas.de>

 *

 * Summary:

 * This is just a modification of the IPv4 code written by

 * Bart De Schuymer <bdschuym@pandora.be>

 * with the changes required to support IPv6

 *

 *  Jan, 2008

 min icmpv6 headersize is 4, so sizeof(_pkthdr) is ok. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_snat

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *

 *  June, 2002

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_stp

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *	Stephen Hemminger <shemminger@osdl.org>

 *

 *  July, 2003

 The stp code only considers these */

 Make sure the match only receives stp frames */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_log

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *	Harald Welte <laforge@netfilter.org>

 *

 *  April, 2002

 *

 FIXME: Disabled from containers until syslog ns is supported */

		/* If it's for Ethernet and the lengths are OK,

		 * then log the ARP payload

	/* Remember that we have to use ebt_log_packet() not to break backward

	 * compatibility. We cannot use the default bridge packet logger via

	 * nf_log_packet() with NFT_LOG_TYPE_LOG here. --Pablo

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_ip

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *

 *  April, 2002

 *

 *  Changes:

 *    added ip-sport and ip-dport

 *    Innominate Security Technologies AG <mhopf@innominate.com>

 *    September, 2002

 min icmp/igmp headersize is 4, so sizeof(_pkthdr) is ok. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_dnat

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *

 *  June, 2002

 *

 NF_BR_LOCAL_OUT */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_arpreply

 *

 *	Authors:

 *	Grzegorz Borowiak <grzes@gnu.univ.gda.pl>

 *	Bart De Schuymer <bdschuym@pandora.be>

 *

 *  August, 2003

 *

 SPDX-License-Identifier: GPL-2.0 */

/* Best effort variant of ip_do_fragment which preserves geometry, unless skbuff

 * has been linearized or cloned.

 for offloaded checksums cleanup checksum before fragmentation */

	/*

	 *	Setup starting values

	/* This is a linearized skbuff, the original geometry is lost for us.

	 * This may also be a clone skbuff, we could preserve the geometry for

	 * the copies but probably not worth the effort.

 ip_defrag() expects IPCB() in place. */

 queued */

 Actually only slow path refragmentation needs this. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebtable_broute

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *

 *  April, 2002

 *

 *  This table lets you choose between routing and bridging for frames

 *  entering on a bridge enslaved nic. This table is traversed before any

 *  other ebtables table. See net/bridge/br_input.c.

/* EBT_ACCEPT means the frame will be bridged

 * EBT_DROP means the frame will be routed

	/* DROP in ebtables -t broute means that the

	 * skb should be routed, not bridged.

	 * This is awkward, but can't be changed for compatibility

	 * reasons.

	 *

	 * We map DROP to ACCEPT and set the ->br_netfilter_broute flag.

	/* undo PACKET_HOST mangling done in br_input in case the dst

	 * address matches the logical bridge but not the port.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebtable_filter

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *

 *  April, 2002

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_among

 *

 *	Authors:

 *	Grzegorz Borowiak <grzes@gnu.univ.gda.pl>

 *

 *  August, 2003

 *

	/* You may be puzzled as to how this code works.

	 * Some tricks were used, refer to

	 * 	include/linux/netfilter_bridge/ebt_among.h

	 * as there you can find a solution of this mystery.

 we match only if it contains */

 we match only if it DOES NOT contain */

 we match only if it contains */

 we match only if it DOES NOT contain */

 not present */

 special case */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_mark_m

 *

 *	Authors:

 *	Bart De Schuymer <bdschuym@pandora.be>

 *

 *  July, 2002

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 * 802_3

 *

 * Author:

 * Chris Vitale csv@bluetail.com

 *

 * May 2003

 *

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Description: EBTables 802.1Q match extension kernelspace module.

 * Authors: Nick Fedchik <nick@fedchik.org.ua>

 *          Bart De Schuymer <bdschuym@pandora.be>

 Whole TCI, given from parsed frame */

 VLAN ID, given from frame TCI */

 user_priority, given from frame TCI */

 VLAN encapsulated Type/Length field, given from orig frame */

	/* Tag Control Information (TCI) consists of the following elements:

	 * - User_priority. The user_priority field is three bits in length,

	 * interpreted as a binary number.

	 * - Canonical Format Indicator (CFI). The Canonical Format Indicator

	 * (CFI) is a single bit flag value. Currently ignored.

	 * - VLAN Identifier (VID). The VID is encoded as

	 * an unsigned binary number.

 Checking VLAN Identifier (VID) */

 Checking user_priority */

 Checking Encapsulated Proto (Length/Type) field */

 Is it 802.1Q frame checked? */

	/* Check for bitmask range

	 * True if even one bit is out of mask

 Check for inversion flags range */

	/* Reserved VLAN ID (VID) values

	 * -----------------------------

	 * 0 - The null VLAN ID.

	 * 1 - The default Port VID (PVID)

	 * 0x0FFF - Reserved for implementation use.

	 * if_vlan.h: VLAN_N_VID 4096.

 if id!=0 => check vid range */

			/* Note: This is valid VLAN-tagged frame point.

			 * Any value of user_priority are acceptable,

			 * but should be ignored according to 802.1Q Std.

			 * So we just drop the prio flag.

 Else, id=0 (null VLAN ID)  => user_priority range (any?) */

	/* Check for encapsulated proto range - it is possible to be

	 * any value for u_short range.

	 * if_ether.h:  ETH_ZLEN        60   -  Min. octets in frame sans FCS

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ebt_limit

 *

 *	Authors:

 *	Tom Marshall <tommy@home.tig-grr.com>

 *

 *	Mostly copied from netfilter's ipt_limit.c, see that file for

 *	more explanation

 *

 *  September, 2003

 *

 We're not limited. */

 Precision saver. */

 If multiplying would overflow... */

 Divide first. */

 Check for overflow. */

 User avg in seconds * EBT_LIMIT_SCALE: convert to jiffies * 128. */

/*

 * no conversion function needed --

 * only avg/burst have meaningful values in userspace.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2011  Intel Corporation. All rights reserved.

 This is going to be a listening socket, dsap must be 0 */

 Lock will be free from unlink */

 Wait for an incoming connection. */

 Send a DISC */

	/* Keep this sock alive and therefore do not remove it from the sockets

	 * list until the DISC PDU has been actually sent. Otherwise we would

	 * reply with DM PDUs before sending the DISC one.

 real length of skb */

 Mark read part of skb as used */

 SOCK_STREAM: re-queue skb if it contains unreceived data */

 XXX Queue backlogged skbs */

 SOCK_SEQPACKET: return real length if MSG_TRUNC is set */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * NFC Digital Protocol stack

 * Copyright (c) 2013, Intel Corporation.

 No support fo R-block nor S-block */

	/*

	 * Chaining not supported so skb->len + 1 PCB byte + 2 CRC bytes must

	 * not be greater than remote FSC

		/*

		 * Skip target_found and don't free it for now. This will be

		 * done when receiving the ATS

	/* Successful READ command response is 16 data bytes + 2 CRC bytes long.

	 * Since the driver can't differentiate a ACK/NACK response from a valid

	 * READ response, the CRC calculation must be handled at digital level

	 * even if the driver supports it for this technology.

 ACK response (i.e. successful WRITE). */

 NACK and any other responses are treated as error. */

 All families and sub-families */

 No mask */

	/* Single sub-carrier, high data rate, no AFI, single slot

	 * Inventory command

 Silently ignore SEL_REQ content and send a SEL_RES for NFC-DEP */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2011 Instituto Nokia de Tecnologia

 *

 * Authors:

 *    Aloisio Almeida Jr <aloisio.almeida@openbossa.org>

 *    Lauro Ramos Venancio <lauro.venancio@openbossa.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2011 Instituto Nokia de Tecnologia

 *

 * Authors:

 *    Aloisio Almeida Jr <aloisio.almeida@openbossa.org>

 *    Lauro Ramos Venancio <lauro.venancio@openbossa.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * NFC Digital Protocol stack

 * Copyright (c) 2013, Intel Corporation.

 Delay between each poll frame (ms) */

/**

 * digital_start_poll - start_poll operation

 * @nfc_dev: device to be polled

 * @im_protocols: bitset of nfc initiator protocols to be used for polling

 * @tm_protocols: bitset of nfc transport protocols to be used for polling

 *

 * For every supported protocol, the corresponding polling function is added

 * to the table of polling technologies (ddev->poll_techs[]) using

 * digital_add_poll_tech().

 * When a polling function fails (by timeout or protocol error) the next one is

 * schedule by digital_poll_next_tech() on the poll workqueue (ddev->poll_work).

 crc check is done in digital_in_recv_mifare_res() */

		/* Call the command callback if any and pass it a ENODEV error.

		 * This gives a chance to the command issuer to free any

		 * allocated buffer.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2011  Intel Corporation. All rights reserved.

 * Copyright (C) 2014 Marvell International Ltd.

 Search for local pending SKBs that are related to this socket */

 If we still have a device, we keep the RAW sockets alive */

 SDP */

 This is a WKS, let's check if it's free */

		/*

		 * Check if there already is a non WKS socket bound

		 * to this service name.

 Find the listening sock and set it back to UNBOUND */

 We're looking for a bound socket, not a client one */

 There is no sequence with UI frames */

		/*

		 * UI frames will be freed from the socket layer, so we

		 * need to keep them alive until someone receives them.

 Wake the listening processes */

 Send CC */

 Send DM */

 Try to queue some I frames for transmission */

 Update N(S)/N(R) */

 Pass the payload upstream */

			/*

			 * I frames will be freed from the socket layer, so we

			 * need to keep them alive until someone receives them.

 Remove skbs from the pending queue */

 Remove and free all skbs until ns == nr */

 Re-queue the remaining skbs for transmission */

 Unlink from connecting and link to the client array */

			/*

			 * We found a socket but its ssap has not been reserved

			 * yet. We need to assign it for good and send a reply.

			 * The ssap will be freed when the socket is closed.

 Close and purge all existing sockets */

 1500 ms */

 LLC Link Management */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2011 Instituto Nokia de Tecnologia

 *

 * Authors:

 *    Lauro Ramos Venancio <lauro.venancio@openbossa.org>

 *    Aloisio Almeida Jr <aloisio.almeida@openbossa.org>

 NFC device ID bitmap */

/**

 * nfc_fw_download_done - inform that a firmware download was completed

 *

 * @dev: The nfc device to which firmware was downloaded

 * @firmware_name: The firmware filename

 * @result: The positive value of a standard errno value

/**

 * nfc_dev_up - turn on the NFC device

 *

 * @dev: The nfc device to be turned on

 *

 * The device remains up until the nfc_dev_down function is called.

 We have to enable the device before discovering SEs */

/**

 * nfc_dev_down - turn off the NFC device

 *

 * @dev: The nfc device to be turned off

/**

 * nfc_start_poll - start polling for nfc targets

 *

 * @dev: The nfc device that must start polling

 * @im_protocols: bitset of nfc initiator protocols to be used for polling

 * @tm_protocols: bitset of nfc transport protocols to be used for polling

 *

 * The device remains polling for targets until a target is found or

 * the nfc_stop_poll function is called.

/**

 * nfc_stop_poll - stop polling for nfc targets

 *

 * @dev: The nfc device that must stop polling

/**

 * nfc_activate_target - prepare the target for data exchange

 *

 * @dev: The nfc device that found the target

 * @target_idx: index of the target that must be activated

 * @protocol: nfc protocol that will be used for data exchange

/**

 * nfc_deactivate_target - deactivate a nfc target

 *

 * @dev: The nfc device that found the target

 * @target_idx: index of the target that must be deactivated

 * @mode: idle or sleep?

/**

 * nfc_data_exchange - transceive data

 *

 * @dev: The nfc device that found the target

 * @target_idx: index of the target

 * @skb: data to be sent

 * @cb: callback called when the response is received

 * @cb_context: parameter for the callback function

 *

 * The user must wait for the callback before calling this function again.

 Only LLCP target mode for now */

/**

 * nfc_alloc_send_skb - allocate a skb for data exchange responses

 *

 * @dev: device sending the response

 * @sk: socket sending the response

 * @flags: MSG_DONTWAIT flag

 * @size: size to allocate

 * @err: pointer to memory to store the error code

/**

 * nfc_alloc_recv_skb - allocate a skb for data exchange responses

 *

 * @size: size to allocate

 * @gfp: gfp flags

/**

 * nfc_targets_found - inform that targets were found

 *

 * @dev: The nfc device that found the targets

 * @targets: array of nfc targets found

 * @n_targets: targets array size

 *

 * The device driver must call this function when one or many nfc targets

 * are found. After calling this function, the device driver must stop

 * polling for targets.

 * NOTE: This function can be called with targets=NULL and n_targets=0 to

 * notify a driver error, meaning that the polling operation cannot complete.

 * IMPORTANT: this function must not be called from an atomic context.

 * In addition, it must also not be called from a context that would prevent

 * the NFC Core to call other nfc ops entry point concurrently.

/**

 * nfc_target_lost - inform that an activated target went out of field

 *

 * @dev: The nfc device that had the activated target in field

 * @target_idx: the nfc index of the target

 *

 * The device driver must call this function when the activated target

 * goes out of the field.

 * IMPORTANT: this function must not be called from an atomic context.

 * In addition, it must also not be called from a context that would prevent

 * the NFC Core to call other nfc ops entry point concurrently.

/**

 * nfc_allocate_device - allocate a new nfc device

 *

 * @ops: device operations

 * @supported_protocols: NFC protocols supported by the device

 * @tx_headroom: reserved space at beginning of skb

 * @tx_tailroom: reserved space at end of skb

 first generation must not be 0 */

/**

 * nfc_register_device - register a nfc device in the nfc subsystem

 *

 * @dev: The nfc device to register

/**

 * nfc_unregister_device - unregister a nfc device in the nfc subsystem

 *

 * @dev: The nfc device to unregister

 the first generation must not be 0 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2011 Instituto Nokia de Tecnologia

 *

 * Authors:

 *    Lauro Ramos Venancio <lauro.venancio@openbossa.org>

 *    Aloisio Almeida Jr <aloisio.almeida@openbossa.org>

 *

 * Vendor commands implementation based on net/wireless/nl80211.c

 * which is:

 *

 * Copyright 2006-2010	Johannes Berg <johannes@sipsolutions.net>

 * Copyright 2013-2014  Intel Mobile Communications GmbH

 evt_transaction is no more used */

 evt_transaction is no more used */

 message building helper */

 since there is no private header just add the generic one */

 clear CB data for netlink core to own from now on */

/**

 * nfc_genl_init() - Initialize netlink interface

 *

 * This initialization function registers the nfc netlink family.

/**

 * nfc_genl_exit() - Deinitialize netlink interface

 *

 * This exit function unregisters the nfc netlink family.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2011  Intel Corporation. All rights reserved.

 VERSION */

 MIUX */

 WKS */

 LTO */

 RW */

 SN */

 OPT */

 SDREQ */

 SDRES */

 sdreq->tlv_len is u8, takes uri_len, + 3 for header, + 1 for NULL */

 XXX Add an skb length check */

 If the socket parameters are not set, use the local ones */

 If the socket parameters are not set, use the local ones */

 Reason code */

 Remote is ready but has not acknowledged our frames */

 Remote is not ready and we've been queueing enough frames */

 No need to check for the peer RW for UI frames */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * NFC Digital Protocol stack

 * Copyright (c) 2013, Intel Corporation.

/* Response Waiting Time for ATR_RES PDU in ms

 *

 * RWT(ATR_RES) = RWT(nfcdep,activation) + dRWT(nfcdep) + dT(nfcdep,initiator)

 *

 * with:

 *  RWT(nfcdep,activation) = 4096 * 2^12 / f(c) s

 *  dRWT(nfcdep) = 16 / f(c) s

 *  dT(nfcdep,initiator) = 100 ms

 *  f(c) = 13560000 Hz

/* Response Waiting Time for other DEP PDUs in ms

 *

 * max_rwt = rwt + dRWT(nfcdep) + dT(nfcdep,initiator)

 *

 * with:

 *  rwt = (256 * 16 / f(c)) * 2^wt s

 *  dRWT(nfcdep) = 16 / f(c) s

 *  dT(nfcdep,initiator) = 100 ms

 *  f(c) = 13560000 Hz

 *  0 <= wt <= 14 (given by the target by the TO field of ATR_RES response)

 424F both directions */

		/* If resp is NULL then we're still chaining so return and

		 * wait for the next part of the PDU.  Else, the PDU is

		 * complete so pass it up.

		/* The initiator has received a valid ACK. Free the last sent

		 * PDU and keep on sending chained skb.

 ATN */

			/* The target has received (and replied to) at least one

			 * ATN DEP_REQ.

			/* pni of resp PDU equal to the target current pni - 1

			 * means resp is the previous DEP_REQ PDU received from

			 * the initiator so the target replies with saved_skb

			 * which is the previous DEP_RES saved in

			 * digital_tg_send_dep_res().

			/* atn_count > 0 and PDU pni != curr_nfc_dep_pni - 1

			 * means the target probably did not received the last

			 * DEP_REQ PDU sent by the initiator. The target

			 * fallbacks to normal processing then.

		/* If resp is NULL then we're still chaining so return and

		 * wait for the next part of the PDU.  Else, the PDU is

		 * complete so pass it up.

 NACK */

 ACK */

			/* The target has previously received one or more ATN

			 * PDUs.

			/* If the ACK PNI is equal to the target PNI - 1 means

			 * that the initiator did not receive the previous PDU

			 * sent by the target so re-send it.

			/* Otherwise, the target did not receive the previous

			 * ACK PDU from the initiator. Fallback to normal

			 * processing of chained PDU then.

 Keep on sending chained PDU */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  The NFC Controller Interface is the communication protocol between an

 *  NFC Controller (NFCC) and a Device Host (DH).

 *

 *  Copyright (C) 2011 Texas Instruments, Inc.

 *

 *  Written by Ilan Elias <ilane@ti.com>

 *

 *  Acknowledgements:

 *  This file is based on lib.c, which was written

 *  by Maxim Krasnyansky.

 NCI status codes to Unix errno mapping */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015, Marvell International Ltd.

 *

 * Inspired (hugely) by HCI LDISC implementation in Bluetooth.

 *

 *  Copyright (C) 2000-2001  Qualcomm Incorporated

 *  Copyright (C) 2002-2003  Maxim Krasnyansky <maxk@qualcomm.com>

 *  Copyright (C) 2004-2005  Marcel Holtmann <marcel@holtmann.org>

 TX states  */

 ------ LDISC part ------ */

/* nci_uart_tty_open

 *

 *     Called when line discipline changed to NCI_UART.

 *

 * Arguments:

 *     tty    pointer to tty info structure

 * Return Value:

 *     0 if success, otherwise error code

	/* Error if the tty has no write op instead of leaving an exploitable

	 * hole

 Flush any pending characters in the driver */

/* nci_uart_tty_close()

 *

 *    Called when the line discipline is changed to something

 *    else, the tty is closed, or the tty detects a hangup.

 Detach from the tty */

/* nci_uart_tty_wakeup()

 *

 *    Callback for transmit wakeup. Called when low level

 *    device driver can accept more send data.

 *

 * Arguments:        tty    pointer to associated tty instance data

 * Return Value:    None

/* -- Default recv_buf handler --

 *

 * This handler supposes that NCI frames are sent over UART link without any

 * framing. It reads NCI header, retrieve the packet size and once all packet

 * bytes are received it passes it to nci_uart driver for processing.

	/* Decode all incoming data in packets

	 * and enqueue then for processing.

 If this is the first data of a packet, allocate a buffer */

 Eat byte after byte till full packet header is received */

 Header was received but packet len was not read */

		/* Compute how many bytes are missing and how many bytes can

		 * be consumed.

 Check if packet is fully received */

 Pass RX packet to driver */

 Next packet will be a new one */

/* nci_uart_tty_receive()

 *

 *     Called by tty low level driver when receive data is

 *     available.

 *

 * Arguments:  tty          pointer to tty instance data

 *             data         pointer to received data

 *             flags        pointer to flags for data

 *             count        count of received data in bytes

 *

 * Return Value:    None

/* nci_uart_tty_ioctl()

 *

 *    Process IOCTL system call for the tty device.

 *

 * Arguments:

 *

 *    tty        pointer to tty instance data

 *    file       pointer to open file object for device

 *    cmd        IOCTL command code

 *    arg        argument for IOCTL call (cmd dependent)

 *

 * Return Value:    Command dependent

 We don't provide read/write/poll interface for user space. */

 Queue TX packet */

 Try to start TX (if possible) */

 Set the send callback */

 Add this driver in the driver list */

 Remove this driver from the driver list */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  The NFC Controller Interface is the communication protocol between an

 *  NFC Controller (NFCC) and a Device Host (DH).

 *

 *  Copyright (C) 2011 Texas Instruments, Inc.

 *  Copyright (C) 2014 Marvell International Ltd.

 *

 *  Written by Ilan Elias <ilane@ti.com>

 Complete data exchange transaction and forward skb to nfc core */

 data exchange is complete, stop the data timer */

 forward skb to nfc core */

 no waiting callback, free skb */

 ----------------- NCI TX Data ----------------- */

 first, copy the data */

 second, set the header */

 queue all fragments atomically */

 free the original skb */

 Send NCI data */

 check if the packet need to be fragmented */

 no need to fragment packet */

 fragment packet and queue the fragments */

 ----------------- NCI RX Data ----------------- */

 first, make enough room for the already accumulated data */

 second, combine the two fragments */

 third, free old reassembly */

 need to wait for next fragment, store skb and exit */

 Data received in Target mode, forward to nfc core */

 Rx Data packet */

 strip the nci data header */

 frame I/F => remove the status byte */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2013  Intel Corporation. All rights reserved.

 a NULL skb means we just want the SPI chip select line to raise */

 still set tx_buf non NULL to make the driver happy */

 add the NCI SPI header to the start of the buffer */

 Trick SPI driver to raise chip select */

 wait for NFC chip hardware handshake to complete */

 ---- Interface to NCI SPI drivers ---- */

/**

 * nci_spi_allocate_spi - allocate a new nci spi

 *

 * @spi: SPI device

 * @acknowledge_mode: Acknowledge mode used by the NFC device

 * @delay: delay between transactions in us

 * @ndev: nci dev to send incoming nci frames to

 Use controller max SPI speed by default */

 add the NCI SPI header to the start of the buffer */

 Remove NFCC part of the header: ACK, NACK and MSB payload len */

/**

 * nci_spi_read - read frame from NCI SPI drivers

 *

 * @nspi: The nci spi

 * Context: can sleep

 *

 * This call may only be used from a context that may sleep.  The sleep

 * is non-interruptible, and has no timeout.

 *

 * It returns an allocated skb containing the frame on success, or NULL.

 Retrieve frame from SPI */

		/* In case of acknowledged mode: if ACK or NACK received,

		 * unblock completion of latest frame sent.

	/* If there is no payload (ACK/NACK only frame),

	 * free the socket buffer

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  The NFC Controller Interface is the communication protocol between an

 *  NFC Controller (NFCC) and a Device Host (DH).

 *

 *  Copyright (C) 2011 Texas Instruments, Inc.

 *

 *  Written by Ilan Elias <ilane@ti.com>

 *

 *  Acknowledgements:

 *  This file is based on hci_event.c, which was written

 *  by Maxim Krasnyansky.

 Handle NCI Response packets */

 Handle NCI 1.x ver */

 skip rf extension parameters */

 Complete the request on intf_activated_ntf or generic_error_ntf */

 If target was active, complete the request only in deactivate_ntf */

		/* Note: data_exchange_cb and data_exchange_cb_context need to

		 * be specify out of nci_core_conn_create_rsp_packet

 we got a rsp, stop the cmd timer */

 strip the nci control header */

 trigger the next cmd */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  The NFC Controller Interface is the communication protocol between an

 *  NFC Controller (NFCC) and a Device Host (DH).

 *

 *  Copyright (C) 2011 Texas Instruments, Inc.

 *  Copyright (C) 2014 Marvell International Ltd.

 *

 *  Written by Ilan Elias <ilane@ti.com>

 *

 *  Acknowledgements:

 *  This file is based on hci_core.c, which was written

 *  by Maxim Krasnyansky.

 ---- NCI requests ---- */

 Execute request and wait for completion. */

 Serialize all requests */

	/* check the state after obtaing the lock against any races

	 * from nci_close_device when the device gets removed.

 set rf mapping configurations */

 by default mapping is set to NCI_RF_INTERFACE_FRAME */

 store cb and context to be used on receiving data */

 Init failed, cleanup */

	/* This mutex needs to be held as a barrier for

	 * caller nci_unregister_device

 Drop RX and TX queues */

 Flush RX and TX wq */

 Reset device */

	/* After this point our queues are empty

	 * and no works are scheduled.

 Flush cmd wq */

 Clear flags except NCI_UNREG */

 NCI command timer function */

 NCI data exchange timer function */

 store cb and context to be used on receiving data */

 ---- Interface to NCI drivers ---- */

/**

 * nci_allocate_device - allocate a new nci device

 *

 * @ops: device operations

 * @supported_protocols: NFC protocols supported by the device

 * @tx_headroom: Reserved space at beginning of skb

 * @tx_tailroom: Reserved space at end of skb

/**

 * nci_free_device - deallocate nci device

 *

 * @ndev: The nci device to deallocate

/**

 * nci_register_device - register a nci device in the nfc subsystem

 *

 * @ndev: The nci device to register

/**

 * nci_unregister_device - unregister a nci device in the nfc subsystem

 *

 * @ndev: The nci device to unregister

	/* This set_bit is not protected with specialized barrier,

	 * However, it is fine because the mutex_lock(&ndev->req_lock);

	 * in nci_close_device() will help to emit one.

 conn_info is allocated with devm_kzalloc */

/**

 * nci_recv_frame - receive frame from NCI drivers

 *

 * @ndev: The nci device

 * @skb: The sk_buff to receive

 Queue frame for rx worker thread */

 Get rid of skb owner, prior to sending to the driver. */

 Send copy to sniffer */

 Send NCI command */

 Proprietary commands API */

 ---- NCI TX Data worker thread ---- */

 Send queued tx data */

 Check if data flow control is used */

 ----- NCI RX worker thread (data & control) ----- */

 Send copy to sniffer */

 Process frame */

 check if a data exchange timeout has occurred */

 complete the data exchange transaction, if exists */

 ----- NCI TX CMD worker thread ----- */

 Send queued command */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  The NFC Controller Interface is the communication protocol between an

 *  NFC Controller (NFCC) and a Device Host (DH).

 *

 *  Copyright (C) 2014 Marvell International Ltd.

 *  Copyright (C) 2011 Texas Instruments, Inc.

 *

 *  Written by Ilan Elias <ilane@ti.com>

 *

 *  Acknowledgements:

 *  This file is based on hci_event.c, which was written

 *  by Maxim Krasnyansky.

 Handle NCI Notification packets */

 Handle NCI 2.x core reset notification */

 update the credits */

 trigger the next tx */

		/* Activation failed, so complete the request

 complete the data exchange transaction, if exists */

 This target already exists, add the new protocol */

 This is a new target, check if we've enough room */

	/* If this contains a value of 0x00 (NFCEE Direct RF

	 * Interface) then all following parameters SHALL contain a

	 * value of 0 and SHALL be ignored.

 no RF technology specific parameters */

 no activation params */

 set the available credits to initial value */

 store general bytes to be reported later in dep_link_up */

 Poll mode */

			/* A single target was found and activated

 ndev->state == NCI_W4_HOST_SELECT */

			/* A selected target was activated, so complete the

 Listen mode */

 drop tx data queue */

 drop partial rx data packet */

 complete the data exchange transaction, if exists */

	/* NFCForum NCI 9.2.1 HCI Network Specific Handling

	 * If the NFCC supports the HCI Network, it SHALL return one,

	 * and only one, NFCEE_DISCOVER_NTF with a Protocol type of

	 * HCI Access, even if the HCI Network contains multiple NFCEEs.

 strip the nci control header */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  The NFC Controller Interface is the communication protocol between an

 *  NFC Controller (NFCC) and a Device Host (DH).

 *  This is the HCI over NCI implementation, as specified in the 10.2

 *  section of the NCI 1.1 specification.

 *

 *  Copyright (C) 2014  STMicroelectronics SAS. All rights reserved.

 type -cmd,evt,rsp- + instruction */

 cbit+pipe */

 HCP headers */

 HCP types */

 HCI core */

/* Fragment HCI data over NCI packet.

 * NFC Forum NCI 10.2.2 Data Exchange:

 * The payload of the Data Packets sent on the Logical Connection SHALL be

 * valid HCP packets, as defined within [ETSI_102622]. Each Data Packet SHALL

 * contain a single HCP packet. NCI Segmentation and Reassembly SHALL NOT be

 * applied to Data Messages in either direction. The HCI fragmentation mechanism

 * is used if required.

 If last packet add NCI_HFP_NO_CHAINING */

		/* Save the new created pipe and bind with local gate,

		 * the description for skb->data[3] is destination gate id

		 * but since we received this cmd from host controller, we

		 * are the destination and it is our local gate

 If the pipe is not created report an error */

/* Receive hcp message for pipe, with type and cmd.

 * skb contains optional message data only.

 it's the last fragment. Does it need re-aggregation? */

	/* if this is a response, dispatch immediately to

	 * unblock waiting cmd context. Otherwise, enqueue to dispatch

	 * in separate context where handler can also execute command.

				/* TODO: Cannot clean by deleting pipe...

				 * -> inconsistent state

 Restore gate<->pipe table from some proprietary location. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * shdlc Link Layer Control

 *

 * Copyright (C) 2012  Intel Corporation. All rights reserved.

 aka T3 in spec 10.6.1 */

 window size */

 send ack timeout */

 guard/retransmit timeout */

 next seq num for send */

 next expected seq num for receive */

 oldest sent unacked seq num */

 other side is not ready to receive */

 checks x < y <= z modulo 8 */

 checks x <= y < z modulo 8 */

 x > z -> z+8 > x */

 immediately sends an S frame. */

 immediately sends an U frame. skb may contain optional payload */

/*

 * Free ack_pending frames until y_nr - 1, and reset t2 according to

 * the remaining oldest ack_pending frame sent time

 MUST initially be < y_nr */

/*

 * Receive validated frames from lower layer. skb contains HCI payload only.

 * Handle according to algorithm at spec:10.8.2

 remove control field */

 See spec RR:10.8.3 REJ:10.8.4 */

 see SHDLC 10.7.7 */

			/*

			 * We sent RSET, but chip wants to negotiate or we

			 * got RSET before we managed to send out our.

			/*

			 * Chip resent RSET due to its timeout - Ignote it

			 * as we already sent UA.

			/*

			 * Chip wants to reset link. This is unexpected and

			 * unsupported.

 Send frames according to algorithm at spec:10.8.1 */

/*

 * Called from syscall context to establish shdlc link. Sleeps until

 * link is ready or failure.

/*

 * Receive an incoming shdlc frame. Frame has already been crc-validated.

 * skb contains only LLC header and payload.

 * If skb == NULL, it is a notification that the link below is dead.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * nop (passthrough) Link Layer Control

 *

 * Copyright (C) 2012  Intel Corporation. All rights reserved.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Link Layer Control manager

 *

 * Copyright (C) 2012  Intel Corporation. All rights reserved.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2012  Intel Corporation. All rights reserved.

/*

 * Payload is the HCP message data only. Instruction will be prepended.

 * Guarantees that cb will be called upon completion or timeout delay

 * counted from the moment the cmd is sent to the transport.

 Only the last fragment will have the cb bit set to 1 */

 This is the last fragment, set the cb bit */

/*

 * Receive hcp message for pipe, with type and cmd.

 * skb contains optional message data only.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2012  Intel Corporation. All rights reserved.

 Largest headroom needed for outgoing HCI commands */

		/* Save the new created pipe and bind with local gate,

		 * the description for skb->data[3] is destination gate id

		 * but since we received this cmd from host controller, we

		 * are the destination and it is our local gate

 if driver set the new gate, we will skip the old one */

 no status data? */

			/* TODO: Multiple targets in field, none activated

			 * poll is supposedly stopped, but there is no

			 * single target to activate, so nothing to report

			 * up.

			 * if we need to restart poll, we must save the

			 * protocols from the initial poll and reuse here.

 Restore gate<->pipe table from some proprietary location. */

		/*

		 * TODO: Check RF Error indicator to make sure data is valid.

		 * It seems that HCI cmd can complete without error, but data

		 * can be invalid if an RF error occurred? Ignore for now.

 RF Err ind */

 handled */

 CTR, see spec:10.2.2.1 */

 it's the last fragment. Does it need re-aggregation? */

	/* if this is a response, dispatch immediately to

	 * unblock waiting cmd context. Otherwise, enqueue to dispatch

	 * in separate context where handler can also execute command.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2012  Intel Corporation. All rights reserved.

	/* TODO: Define hci cmd execution delay. Should it be the same

	 * for all commands?

/*

 * HCI command execution completion callback.

 * err will be a standard linux error (may be converted from HCI response)

 * skb contains the response data and must be disposed, or may be NULL if

 * an error occurred

	/* TODO: Define hci cmd execution delay. Should it be the same

	 * for all commands?

/*

 * Execute an hci command sent to gate.

 * skb will contain response data if success. skb can be NULL if you are not

 * interested by the response.

	/* TODO ELa: reg idx must be inserted before param, but we don't want

	 * to ask the caller to do it to keep a simpler API.

	 * For now, just create a new temporary param buffer. This is far from

	 * optimal though, and the plan is to modify APIs to pass idx down to

	 * nfc_hci_hcp_message_tx where the frame is actually built, thereby

	 * eliminating the need for the temp allocation-copy here.

		/* dest host other than host controller will send

		 * number of pipes already open on this gate before

		 * execution. The number can be found in skb->data[0]

	/* TODO: Find out what the identity reference data is

				/* TODO: Cannot clean by deleting pipe...

 SPDX-License-Identifier: GPL-2.0

/* XDP sockets

 *

 * AF_XDP sockets allows a channel between XDP programs and userspace

 * applications.

 * Copyright(c) 2018 Intel Corporation.

 *

 * Author(s): Bjrn Tpel <bjorn.topel@intel.com>

 *	      Magnus Karlsson <magnus.karlsson@intel.com>

/* The buffer pool is stored both in the _rx struct and the _tx struct as we do

 * not know if the device has more tx queues than rx, or the opposite.

 * This might also change during run time.

 Matches smp_wmb() in bind(). */

		/* This is the backpressure mechanism for the Tx path.

		 * Reserve space in the completion queue and only proceed

		 * if there is space in it. This avoids having to implement

		 * any buffering in the Tx path.

 Fallback to the non-batched version */

	/* This is the backpressure mechanism for the Tx path. Try to

	 * reserve space in the completion queue for all packets, but

	 * if there are fewer slots available, just process that many

	 * packets. This avoids having to implement any buffering in

	 * the Tx path.

		/* This is the backpressure mechanism for the Tx path.

		 * Reserve space in the completion queue and only proceed

		 * if there is space in it. This avoids having to implement

		 * any buffering in the Tx path.

 Tell user-space to retry the send */

 Free skb without triggering the perf drop trace */

 Ignore NET_XMIT_CN as packet might have been sent */

 SKB completed but not sent */

 Prefer busy-polling, skip the wakeup. */

 only support non-blocking sockets */

 only support non-blocking sockets */

 Poll needs to drive Tx also in copy mode */

 Make sure queue is ready before it can be seen by others */

 Wait for driver to stop using the xdp socket. */

	/* This function removes the current XDP socket from all the

	 * maps it resides in. We need to take extra care here, due to

	 * the two locks involved. Each map has a lock synchronizing

	 * updates to the entries, and each socket has a lock that

	 * synchronizes access to the list of maps (map_list). For

	 * deadlock avoidance the locks need to be taken in the order

	 * "map lock"->"socket map list lock". We start off by

	 * accessing the socket map list, and take a reference to the

	 * map to guarantee existence between the

	 * xsk_get_map_list_entry() and xsk_map_try_sock_delete()

	 * calls. Then we ask the map to remove the socket, which

	 * tries to remove the socket from the map. Note that there

	 * might be updates to the map between

	 * xsk_get_map_list_entry() and xsk_map_try_sock_delete().

 Cannot specify flags for shared sockets. */

 We have already our own. */

			/* Share the umem with another socket on another qid

			 * and/or device.

 Share the buffer pool with the other socket. */

 Do not allow setting your own fq or cq. */

 This xsk has its own umem. */

 FQ and CQ are now owned by the buffer pool and cleaned up with it. */

		/* Matches smp_rmb() in bind() for shared umem

		 * sockets, and xsk_is_bound().

 Start of packet data area */

 Length of packet data area */

 Tx needs to be explicitly woken up the first time */

 Make sure umem is ready before it can be seen by others */

			/* xdp_ring_offset is identical to xdp_ring_offset_v1

			 * except for the flags field added to the end.

 Matches the smp_wmb() in XDP_UMEM_REG */

 Matches the smp_wmb() in xsk_init_queue */

 Clear device references. */

 no slab */);

 SPDX-License-Identifier: GPL-2.0

/* XSKMAP used for AF_XDP sockets

 * Copyright(c) 2018 Intel Corporation.

/* Elements are kept alive by RCU; either by rcu_read_lock() (from syscall) or

 * by local_bh_disable() (from XDP calls inside NAPI). The

 * rcu_read_lock_bh_held() below makes lockdep accept both.

 SPDX-License-Identifier: GPL-2.0

/* XDP user-space packet buffer

 * Copyright(c) 2018 Intel Corporation.

		/* Strictly speaking we could support this, if:

		 * - huge pages, or*

		 * - using an IOMMU, or

		 * - making sure the memory area is consecutive

		 * but for now, we simply say "computer says no".

		/* Memory area has to be page size aligned. For

		 * simplicity, this might change.

 SPDX-License-Identifier: GPL-2.0

/* XDP sockets monitoring support

 *

 * Copyright(c) 2019 Intel Corporation.

 *

 * Author: Bjrn Tpel <bjorn.topel@intel.com>

 SPDX-License-Identifier: GPL-2.0

/* XDP user-space ring structure

 * Copyright(c) 2018 Intel Corporation.

 SPDX-License-Identifier: GPL-2.0

	/* Tx needs to be explicitly woken up the first time.  Also

	 * for supporting drivers that do not implement this

	 * feature. They will always have to call sendto() or poll().

 For copy-mode, we are done. */

 fallback to copy mode */

 One fill and completion ring required for each queue id. */

 Slow path */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/net/sunrpc/svc.c

 *

 * High-level RPC service routines

 *

 * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>

 *

 * Multiple threads pools and NUMAisation

 * Copyright (c) 2006 Silicon Graphics, Inc.

 * by Greg Banks <gnb@melbourne.sgi.com>

/*

 * Structure for mapping cpus to pools and vice versa.

 * Setup once during sunrpc initialisation.

 protects svc_pool_map.count only */

/*

 * Detect best pool mapping mode heuristically,

 * according to the machine's topology.

		/*

		 * Actually have multiple NUMA nodes,

		 * so split pools on NUMA node boundaries

		/*

		 * Non-trivial SMP, or CONFIG_NUMA on

		 * non-NUMA hardware, e.g. with a generic

		 * x86_64 kernel on Xeons.  In this case we

		 * want to divide the pools on cpu boundaries.

 default: one global pool */

/*

 * Allocate the to_pool[] and pool_to[] arrays.

 * Returns 0 on success or an errno.

/*

 * Initialise the pool map for SVC_POOL_PERCPU mode.

 * Returns number of pools or <0 on error.

 cpus brought online later all get mapped to pool0, sorry */

/*

 * Initialise the pool map for SVC_POOL_PERNODE mode.

 * Returns number of pools or <0 on error.

 some architectures (e.g. SN2) have cpuless nodes */

 nodes brought online later all get mapped to pool0, sorry */

/*

 * Add a reference to the global map of cpus to pools (and

 * vice versa).  Initialise the map if we're the first user.

 * Returns the number of pools.

 default, or memory allocation failure */

/*

 * Drop a reference to the global map of cpus to pools.

 * When the last reference is dropped, the map data is

 * freed; this allows the sysadmin to change the pool

 * mode using the pool_mode module option without

 * rebooting or re-loading sunrpc.ko.

/*

 * Set the given thread's cpus_allowed mask so that it

 * will only run on cpus in the given pool.

	/*

	 * The caller checks for sv_nrpools > 1, which

	 * implies that we've been initialized.

/*

 * Use the mapping mode to choose a pool for a given CPU.

 * Used when enqueueing an incoming RPC.  Always returns

 * a non-NULL pool pointer.

	/*

	 * An uninitialised map happens in a pure client when

	 * lockd is brought up, so silently treat it the

	 * same as SVC_POOL_GLOBAL.

 Remove any stale portmap registrations */

/*

 * Create an RPC service

npools*/1, ops);

/*

 * Destroy an RPC service. Should be called with appropriate locking to

 * protect the sv_nrthreads, sv_permsocks and sv_tempsocks.

	/*

	 * The last user is gone and thus all sockets have to be destroyed to

	 * the point. Check this.

/*

 * Allocate an RPC server's buffer space.

 * We allocate pages and place them in rq_pages.

 bc_xprt uses fore channel allocated buffers */

	pages = size / PAGE_SIZE + 1; /* extra page as we hold both request and reply.

				       * We assume one is at most one page

/*

 * Release an RPC server buffer

/*

 * Choose a pool in which to create a new thread, for svc_set_num_threads

/*

 * Choose a thread to kill, for svc_set_num_threads

 choose a pool in round-robin fashion */

		/*

		 * Remove from the pool->sp_all_threads list

		 * so we don't try to kill it again.

 create new threads */

 destroy old threads */

 destroy old threads */

/*

 * Create or destroy enough new threads to make the number

 * of threads the given number.  If `pool' is non-NULL, applies

 * only to threads in that pool, otherwise round-robins between

 * all pools.  Caller must ensure that mutual exclusion between this and

 * server startup or shutdown.

 *

 * Destroying threads relies on the service threads filling in

 * rqstp->rq_task, which only the nfs ones do.  Assumes the serv

 * has been created using svc_create_pooled().

 *

 * Based on code that used to be in nfsd_svc() but tweaked

 * to be pool-aware.

 The -1 assumes caller has done a svc_get() */

 destroy old threads */

 destroy old threads */

 The -1 assumes caller has done a svc_get() */

/**

 * svc_rqst_replace_page - Replace one page in rq_pages[]

 * @rqstp: svc_rqst with pages to replace

 * @page: replacement page

 *

 * When replacing a page in rq_pages, batch the release of the

 * replaced pages to avoid hammering the page allocator.

/*

 * Called from a server thread as it's exiting. Caller must hold the "service

 * mutex" for the service.

 Release the server */

/*

 * Register an "inet" protocol family netid with the local

 * rpcbind daemon via an rpcbind v4 SET request.

 *

 * No netconfig infrastructure is available in the kernel, so

 * we map IP_ protocol numbers to netids by hand.

 *

 * Returns zero on success; a negative errno value is returned

 * if any error occurs.

	/*

	 * User space didn't support rpcbind v4, so retry this

	 * registration request with the legacy rpcbind v2 protocol.

/*

 * Register an "inet6" protocol family netid with the local

 * rpcbind daemon via an rpcbind v4 SET request.

 *

 * No netconfig infrastructure is available in the kernel, so

 * we map IP_ protocol numbers to netids by hand.

 *

 * Returns zero on success; a negative errno value is returned

 * if any error occurs.

	/*

	 * User space didn't support rpcbind version 4, so we won't

	 * use a PF_INET6 listener.

 IS_ENABLED(CONFIG_IPV6) */

/*

 * Register a kernel RPC service via rpcbind version 4.

 *

 * Returns zero on success; a negative errno value is returned

 * if any error occurs.

	/*

	 * Don't register a UDP port if we need congestion

	 * control.

/**

 * svc_register - register an RPC service with the local portmapper

 * @serv: svc_serv struct for the service to register

 * @net: net namespace for the service to register

 * @family: protocol family of service's listener socket

 * @proto: transport protocol number to advertise

 * @port: port to advertise

 *

 * Service is registered for any address in the passed-in protocol family

/*

 * If user space is running rpcbind, it should take the v4 UNSET

 * and clear everything for this [program, version].  If user space

 * is running portmap, it will reject the v4 UNSET, but won't have

 * any "inet6" entries anyway.  So a PMAP_UNSET should be sufficient

 * in this case to clear all existing entries for [program, version].

	/*

	 * User space didn't support rpcbind v4, so retry this

	 * request with the legacy rpcbind v2 protocol.

/*

 * All netids, bind addresses and ports registered for [program, version]

 * are removed from the local rpcbind database (if the service is not

 * hidden) to make way for a new instance of the service.

 *

 * The result of unregistration is reported via dprintk for those who want

 * verification of the result, but is otherwise not important.

/*

 * dprintk the given error with the address of the client that caused it.

 compiler food */

	/*

	 * Some protocol versions (namely NFSv4) require some form of

	 * congestion control.  (See RFC 7530 section 3.1 paragraph 2)

	 * In other words, UDP is not allowed. We mark those when setting

	 * up the svc_xprt, and verify that here.

	 *

	 * The spec is not very clear about what error should be returned

	 * when someone tries to access a server that is listening on UDP

	 * for lower versions. RPC_PROG_MISMATCH seems to be the closest

	 * fit.

 Initialize storage for argp and resp */

 Bump per-procedure stats counter */

/*

 * Common routine for processing the RPC request.

 Will be turned off by GSS integrity and privacy services */

 Will be turned off only when NFSv4 Sessions are used */

 First words of reply: */

 REPLY */

 RPC version number */

 Save position in case we later decide to reject: */

 ACCEPT */

 program number */

 version number */

 procedure number */

	/*

	 * Decode auth data, and add verifier to reply buffer.

	 * We do this before anything else in order to get a decent

	 * auth verifier.

 Also give the program a chance to reject this call: */

 Should this check go into the dispatcher? */

 Syntactic check complete */

 Build the reply header. */

	/* un-reserve some of the out-queue now that we have a

	 * better idea of reply size

 Call the function that processes the request. */

 Check RPC status result */

 Caller can now send it */

 doesn't hurt to call this twice */

 REJECT */

 RPC_MISMATCH */

 Only RPCv2 supported */

 Restore write pointer to location of accept status: */

 REJECT */

 AUTH_ERROR */

 status */

/*

 * Process the RPC request.

	/*

	 * Setup response xdr_buf.

	 * Initially it has just one page

 direction != CALL */

 Returns 1 for send, 0 for drop */

/*

 * Process a backchannel RPC request that arrived over an existing

 * outbound connection

 Build the svc_rqst used by the common processing routine */

 Adjust the argument buffer length */

 reset result send buffer "put" position */

	/*

	 * Skip the next two words because they've already been

	 * processed in the transport

 XID */

 CALLDIR */

 Parse and execute the bc call */

 Processing error: drop the request */

 Finally, send the reply synchronously */

 CONFIG_SUNRPC_BACKCHANNEL */

/*

 * Return (transport-specific) limit on the rpc payload.

/**

 * svc_proc_name - Return RPC procedure name in string form

 * @rqstp: svc_rqst to operate on

 *

 * Return value:

 *   Pointer to a NUL-terminated string

/**

 * svc_encode_result_payload - mark a range of bytes as a result payload

 * @rqstp: svc_rqst to operate on

 * @offset: payload's byte offset in rqstp->rq_res

 * @length: size of payload, in bytes

 *

 * Returns zero on success, or a negative errno if a permanent

 * error occurred.

/**

 * svc_fill_write_vector - Construct data argument for VFS write call

 * @rqstp: svc_rqst to operate on

 * @payload: xdr_buf containing only the write data payload

 *

 * Fills in rqstp::rq_vec, and returns the number of elements.

	/* Some types of transport can present the write payload

	 * entirely in rq_arg.pages. In this case, @first is empty.

/**

 * svc_fill_symlink_pathname - Construct pathname argument for VFS symlink call

 * @rqstp: svc_rqst to operate on

 * @first: buffer containing first section of pathname

 * @p: buffer containing remaining section of pathname

 * @total: total length of the pathname argument

 *

 * The VFS symlink API demands a NUL-terminated pathname in mapped memory.

 * Returns pointer to a NUL-terminated string, or an ERR_PTR. Caller must free

 * the returned string.

	/* Sanity check: Linux doesn't allow the pathname argument to

	 * contain a NUL byte.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/net/sunrpc/xprt.c

 *

 *  This is a generic RPC call interface supporting congestion avoidance,

 *  and asynchronous calls.

 *

 *  The interface works like this:

 *

 *  -	When a process places a call, it allocates a request slot if

 *	one is available. Otherwise, it sleeps on the backlog queue

 *	(xprt_reserve).

 *  -	Next, the caller puts together the RPC message, stuffs it into

 *	the request struct, and calls xprt_transmit().

 *  -	xprt_transmit sends the message and installs the caller on the

 *	transport's wait list. At the same time, if a reply is expected,

 *	it installs a timer that is run after the packet's timeout has

 *	expired.

 *  -	When a packet arrives, the data_ready handler walks the list of

 *	pending requests for that transport. If a matching XID is found, the

 *	caller is woken up, and the timer removed.

 *  -	When no reply arrives within the timeout interval, the timer is

 *	fired by the kernel and runs xprt_timer(). It either adjusts the

 *	timeout values (minor timeout) or wakes up the caller with a status

 *	of -ETIMEDOUT.

 *  -	When the caller receives a notification from RPC that a reply arrived,

 *	it should release the RPC slot, and process the reply.

 *	If the call timed out, it may choose to retry the operation by

 *	adjusting the initial timeout value, and simply calling rpc_call

 *	again.

 *

 *  Support for async RPC is done through a set of RPC-specific scheduling

 *  primitives that `transparently' work for processes as well as async

 *  tasks that rely on callbacks.

 *

 *  Copyright (C) 1995-1997, Olaf Kirch <okir@monad.swb.de>

 *

 *  Transport switch API copyright (C) 2005, Chuck Lever <cel@netapp.com>

/*

 * Local variables

/*

 * Local functions

/**

 * xprt_register_transport - register a transport implementation

 * @transport: transport to register

 *

 * If a transport implementation is loaded as a kernel module, it can

 * call this interface to make itself known to the RPC client.

 *

 * Returns:

 * 0:		transport successfully registered

 * -EEXIST:	transport already registered

 * -EINVAL:	transport module being unloaded

 don't register the same transport class twice */

/**

 * xprt_unregister_transport - unregister a transport implementation

 * @transport: transport to unregister

 *

 * Returns:

 * 0:		transport successfully unregistered

 * -ENOENT:	transport never registered

/**

 * xprt_find_transport_ident - convert a netid into a transport identifier

 * @netid: transport to load

 *

 * Returns:

 * > 0:		transport identifier

 * -ENOENT:	transport module not available

/**

 * xprt_reserve_xprt - serialize write access to transports

 * @task: task that is requesting access to the transport

 * @xprt: pointer to the target transport

 *

 * This prevents mixing the payload of separate requests, and prevents

 * transport connects from colliding with writes.  No congestion control

 * is provided.

 Peek at head of queue to see if it can make progress */

/*

 * xprt_reserve_xprt_cong - serialize write access to transports

 * @task: task that is requesting access to the transport

 *

 * Same as xprt_reserve_xprt, but Van Jacobson congestion control is

 * integrated into the decision of whether a request is allowed to be

 * woken up and given access to the transport.

 * Note that the lock is only granted if we know there are free slots.

/**

 * xprt_release_xprt - allow other requests to use a transport

 * @xprt: transport with other tasks potentially waiting

 * @task: task that is releasing access to the transport

 *

 * Note that "task" can be NULL.  No congestion control is provided.

/**

 * xprt_release_xprt_cong - allow other requests to use a transport

 * @xprt: transport with other tasks potentially waiting

 * @task: task that is releasing access to the transport

 *

 * Note that "task" can be NULL.  Another task is awoken to use the

 * transport if the transport's congestion window allows it.

/*

 * Van Jacobson congestion avoidance. Check if the congestion window

 * overflowed. Put the task to sleep if this is the case.

/*

 * Adjust the congestion window, and wake up the next task

 * that has been sleeping due to congestion

/**

 * xprt_request_get_cong - Request congestion control credits

 * @xprt: pointer to transport

 * @req: pointer to RPC request

 *

 * Useful for transports that require congestion control.

/**

 * xprt_release_rqst_cong - housekeeping when request is complete

 * @task: RPC request that recently completed

 *

 * Useful for transports that require congestion control.

/*

 * Clear the congestion window wait flag and wake up the next

 * entry on xprt->sending

/**

 * xprt_adjust_cwnd - adjust transport congestion window

 * @xprt: pointer to xprt

 * @task: recently completed RPC request used to adjust window

 * @result: result code of completed RPC request

 *

 * The transport code maintains an estimate on the maximum number of out-

 * standing RPC requests, using a smoothed version of the congestion

 * avoidance implemented in 44BSD. This is basically the Van Jacobson

 * congestion algorithm: If a retransmit occurs, the congestion window is

 * halved; otherwise, it is incremented by 1/cwnd when

 *

 *	-	a reply is received and

 *	-	a full number of requests are outstanding and

 *	-	the congestion window hasn't been updated recently.

		/* The (cwnd >> 1) term makes sure

/**

 * xprt_wake_pending_tasks - wake all tasks on a transport's pending queue

 * @xprt: transport with waiting tasks

 * @status: result code to plant in each task before waking it

 *

/**

 * xprt_wait_for_buffer_space - wait for transport output buffer to clear

 * @xprt: transport

 *

 * Note that we only set the timer for the case of RPC_IS_SOFT(), since

 * we don't in general want to force a socket disconnection due to

 * an incomplete RPC call transmission.

/**

 * xprt_write_space - wake the task waiting for transport output buffer space

 * @xprt: transport with waiting tasks

 *

 * Can be called in a soft IRQ context, so xprt_write_space never sleeps.

/**

 * xprt_adjust_timeout - adjust timeout values for next retransmit

 * @req: RPC request containing parameters to use for the adjustment

 *

 Reset the RTT counters == "slow start" */

/**

 * xprt_disconnect_done - mark a transport as disconnected

 * @xprt: transport to flag for disconnect

 *

/**

 * xprt_schedule_autoclose_locked - Try to schedule an autoclose RPC call

 * @xprt: transport to disconnect

/**

 * xprt_force_disconnect - force a transport to disconnect

 * @xprt: transport to disconnect

 *

 Don't race with the test_bit() in xprt_clear_locked() */

/**

 * xprt_conditional_disconnect - force a transport to disconnect

 * @xprt: transport to disconnect

 * @cookie: 'connection cookie'

 *

 * This attempts to break the connection if and only if 'cookie' matches

 * the current transport 'connection cookie'. It ensures that we don't

 * try to break the connection more than once when we need to retransmit

 * a batch of RPC requests.

 *

 Don't race with the test_bit() in xprt_clear_locked() */

 Reset xprt->last_used to avoid connect/autodisconnect cycling */

/**

 * xprt_connect - schedule a transport connect operation

 * @task: RPC task that is requesting the connect

 *

 Race breaker */

/**

 * xprt_reconnect_delay - compute the wait before scheduling a connect

 * @xprt: transport instance

 *

/**

 * xprt_reconnect_backoff - compute the new re-establish timeout

 * @xprt: transport instance

 * @init_to: initial reestablish timeout

 *

/**

 * xprt_lookup_rqst - find an RPC request corresponding to an XID

 * @xprt: transport on which the original request was transmitted

 * @xid: RPC XID of incoming reply

 *

 * Caller holds xprt->queue_lock.

/**

 * xprt_pin_rqst - Pin a request on the transport receive list

 * @req: Request to pin

 *

 * Caller must ensure this is atomic with the call to xprt_lookup_rqst()

 * so should be holding xprt->queue_lock.

/**

 * xprt_unpin_rqst - Unpin a request on the transport receive list

 * @req: Request to pin

 *

 * Caller should be holding xprt->queue_lock.

/**

 * xprt_request_enqueue_receive - Add an request to the receive queue

 * @task: RPC task

 *

 Update the softirq receive buffer */

 Add request to the receive list */

 Turn off autodisconnect */

/**

 * xprt_request_dequeue_receive_locked - Remove a request from the receive queue

 * @task: RPC task

 *

 * Caller must hold xprt->queue_lock.

/**

 * xprt_update_rtt - Update RPC RTT statistics

 * @task: RPC request that recently completed

 *

 * Caller holds xprt->queue_lock.

/**

 * xprt_complete_rqst - called when reply processing is complete

 * @task: RPC request that recently completed

 * @copied: actual number of bytes received from the transport

 *

 * Caller holds xprt->queue_lock.

 Ensure all writes are done before we update */

 req->rq_reply_bytes_recvd */

/**

 * xprt_wait_for_reply_request_def - wait for reply

 * @task: pointer to rpc_task

 *

 * Set a request's retransmit timeout based on the transport's

 * default timeout parameters.  Used by transports that don't adjust

 * the retransmit timeout based on round-trip time estimation,

 * and put the task to sleep on the pending queue.

/**

 * xprt_wait_for_reply_request_rtt - wait for reply using RTT estimator

 * @task: pointer to rpc_task

 *

 * Set a request's retransmit timeout using the RTT estimator,

 * and put the task to sleep on the pending queue.

/**

 * xprt_request_wait_receive - wait for the reply to an RPC request

 * @task: RPC task about to send a request

 *

	/*

	 * Sleep on the pending queue if we're expecting a reply.

	 * The spinlock ensures atomicity between the test of

	 * req->rq_reply_bytes_recvd, and the call to rpc_sleep_on().

		/*

		 * Send an extra queue wakeup call if the

		 * connection was dropped in case the call to

		 * rpc_sleep_on() raced.

/**

 * xprt_request_enqueue_transmit - queue a task for transmission

 * @task: pointer to rpc_task

 *

 * Add a task to the transmission queue.

		/*

		 * Requests that carry congestion control credits are added

		 * to the head of the list to avoid starvation issues.

 Note: req is added _before_ pos */

 Note: req is added _before_ pos */

/**

 * xprt_request_dequeue_transmit_locked - remove a task from the transmission queue

 * @task: pointer to rpc_task

 *

 * Remove a task from the transmission queue

 * Caller must hold xprt->queue_lock

/**

 * xprt_request_dequeue_transmit - remove a task from the transmission queue

 * @task: pointer to rpc_task

 *

 * Remove a task from the transmission queue

/**

 * xprt_request_dequeue_xprt - remove a task from the transmit+receive queue

 * @task: pointer to rpc_task

 *

 * Remove a task from the transmit and receive queues, and ensure that

 * it is not pinned by the receive work item.

/**

 * xprt_request_prepare - prepare an encoded request for transport

 * @req: pointer to rpc_rqst

 *

 * Calls into the transport layer to do whatever is needed to prepare

 * the request for transmission or receive.

/**

 * xprt_request_need_retransmit - Test if a task needs retransmission

 * @task: pointer to rpc_task

 *

 * Test for whether a connection breakage requires the task to retransmit

/**

 * xprt_prepare_transmit - reserve the transport before sending a request

 * @task: RPC task about to send a request

 *

 Race breaker: someone may have transmitted us */

/**

 * xprt_request_transmit - send an RPC request on a transport

 * @req: pointer to request to transmit

 * @snd_task: RPC task that owns the transport lock

 *

 * This performs the transmission of a single request.

 * Note that if the request is not the same as snd_task, then it

 * does need to be pinned.

 * Returns '0' on success.

 Verify that our message lies in the RPCSEC_GSS window */

	/*

	 * Update req->rq_ntrans before transmitting to avoid races with

	 * xprt_update_rtt(), which needs to know that it is recording a

	 * reply to the first transmission.

/**

 * xprt_transmit - send an RPC request on a transport

 * @task: controlling RPC task

 *

 * Attempts to drain the transmit queue. On exit, either the transport

 * signalled an error that needs to be handled before transmission can

 * resume, or @task finished transmitting, and detected that it already

 * received a reply.

 Was @task transmitted, and has it received a reply? */

 mark unused */

 mark unused */

/**

 * xprt_reserve - allocate an RPC request slot

 * @task: RPC task requesting a slot allocation

 *

 * If the transport is marked as being congested, or if no more

 * slots are available, place the task on the transport's

 * backlog queue.

/**

 * xprt_retry_reserve - allocate an RPC request slot

 * @task: RPC task requesting a slot allocation

 *

 * If no more slots are available, place the task on the transport's

 * backlog queue.

 * Note that the only difference with xprt_reserve is that we now

 * ignore the value of the XPRT_CONGESTED flag.

/**

 * xprt_release - release an RPC request slot

 * @task: task which is finished with the slot

 *

	/*

	 * Set up the xdr_buf length.

	 * This also indicates that the buffer is XDR encoded already.

 CONFIG_SUNRPC_BACKCHANNEL */

/**

 * xprt_create_transport - create an RPC transport

 * @args: rpc transport creation arguments

 *

	/*

	 * Destroy any existing back channel

	/*

	 * Tear down transport state and free the rpc_xprt

/**

 * xprt_destroy - destroy an RPC transport, killing off all requests.

 * @xprt: transport to destroy

 *

	/*

	 * Exclude transport connect/disconnect handlers and autoclose

	/*

	 * Destroy sockets etc from the system workqueue so they can

	 * safely flush receive work running on rpciod.

/**

 * xprt_get - return a reference to an RPC transport.

 * @xprt: pointer to the transport

 *

/**

 * xprt_put - release a reference to an RPC transport.

 * @xprt: pointer to the transport

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/net/sunrpc/auth.c

 *

 * Generic RPC client authentication API.

 *

 * Copyright (C) 1996, Olaf Kirch <okir@monad.swb.de>

 others can be loadable modules */

/*

 * Return the machine_cred pointer to be used whenever

 * the a generic machine credential is needed.

/**

 * rpcauth_get_pseudoflavor - check if security flavor is supported

 * @flavor: a security flavor

 * @info: a GSS mech OID, quality of protection, and service value

 *

 * Verifies that an appropriate kernel module is available or already loaded.

 * Returns an equivalent pseudoflavor, or RPC_AUTH_MAXFLAVOR if "flavor" is

 * not supported locally.

/**

 * rpcauth_get_gssinfo - find GSS tuple matching a GSS pseudoflavor

 * @pseudoflavor: GSS pseudoflavor to match

 * @info: rpcsec_gss_info structure to fill in

 *

 * Returns zero and fills in "info" if pseudoflavor matches a

 * supported mechanism.

/*

 * On success, the caller is responsible for freeing the reference

 * held by the hashtable

/*

 * Initialize RPC credential cache

/*

 * Destroy a list of credentials

/*

 * Clear the RPC credential cache, and delete those credentials

 * that are not referenced.

 Note: We now hold a reference to cred */

/*

 * Destroy the RPC credential cache

/*

 * Remove stale credentials. Avoid sleeping inside the loop.

		/*

		 * Enforce a 60 second garbage collection moratorium

		 * Note that the cred_unused list must be time-ordered.

/*

 * Run memory cache shrinker.

 nothing left, don't come back */

/*

 * Look up a process' credentials in the authentication cache

 Task must use exactly this rpc_cred */

 If machine cred couldn't be bound, try a root cred */

 Race breaker */

/**

 * rpcauth_marshcred - Append RPC credential to end of @xdr

 * @task: controlling RPC task

 * @xdr: xdr_stream containing initial portion of RPC Call header

 *

 * On success, an appropriate verifier is added to @xdr, @xdr is

 * updated to point past the verifier, and zero is returned.

 * Otherwise, @xdr is in an undefined state and a negative errno

 * is returned.

/**

 * rpcauth_wrap_req_encode - XDR encode the RPC procedure

 * @task: controlling RPC task

 * @xdr: stream where on-the-wire bytes are to be marshalled

 *

 * On success, @xdr contains the encoded and wrapped message.

 * Otherwise, @xdr is in an undefined state.

/**

 * rpcauth_wrap_req - XDR encode and wrap the RPC procedure

 * @task: controlling RPC task

 * @xdr: stream where on-the-wire bytes are to be marshalled

 *

 * On success, @xdr contains the encoded and wrapped message,

 * and zero is returned. Otherwise, @xdr is in an undefined

 * state and a negative errno is returned.

/**

 * rpcauth_checkverf - Validate verifier in RPC Reply header

 * @task: controlling RPC task

 * @xdr: xdr_stream containing RPC Reply header

 *

 * On success, @xdr is updated to point past the verifier and

 * zero is returned. Otherwise, @xdr is in an undefined state

 * and a negative errno is returned.

/**

 * rpcauth_unwrap_resp_decode - Invoke XDR decode function

 * @task: controlling RPC task

 * @xdr: stream where the Reply message resides

 *

 * Returns zero on success; otherwise a negative errno is returned.

/**

 * rpcauth_unwrap_resp - Invoke unwrap and decode function for the cred

 * @task: controlling RPC task

 * @xdr: stream where the Reply message resides

 *

 * Returns zero on success; otherwise a negative errno is returned.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/net/sunrpc/clnt.c

 *

 *  This file contains the high-level RPC interface.

 *  It is modeled as a finite state machine to support both synchronous

 *  and asynchronous requests.

 *

 *  -	RPC header generation and argument serialization.

 *  -	Credential refresh.

 *  -	TCP connect handling.

 *  -	Retry of operation when it is suspected the operation failed because

 *	of uid squashing on the server, or when the credentials were stale

 *	and need to be refreshed, or when a packet was damaged in transit.

 *	This may be have to be moved to the VFS layer.

 *

 *  Copyright (C) 1992,1993 Rick Sladkey <jrs@world.std.com>

 *  Copyright (C) 1995,1996 Olaf Kirch <okir@monad.swb.de>

/*

 * All RPC clients are linked into this list

 save the nodename */

/**

 * rpc_create - create an RPC client and transport with one call

 * @args: rpc_clnt create argument structure

 *

 * Creates and initializes an RPC transport and an RPC client.

 *

 * It can ping the server in order to determine if it is up, and to see if

 * it supports this program and version.  RPC_CLNT_CREATE_NOPING disables

 * this behavior so asynchronous tasks can also use rpc_create.

	/*

	 * If the caller chooses not to specify a hostname, whip

	 * up a string representation of the passed-in address.

			/* caller wants default server name, but

	/*

	 * By default, kernel RPC client connects from a reserved port.

	 * CAP_NET_BIND_SERVICE will not be set for unprivileged requesters,

	 * but it is always enabled for rpciod, which handles the connect

	 * operation.

/*

 * This function clones the RPC client structure. It allows us to share the

 * same transport while varying parameters such as the authentication

 * flavour.

 Turn off autobind on clones */

/**

 * rpc_clone_client - Clone an RPC client structure

 *

 * @clnt: RPC client whose parameters are copied

 *

 * Returns a fresh RPC client or an ERR_PTR.

/**

 * rpc_clone_client_set_auth - Clone an RPC client structure and set its auth

 *

 * @clnt: RPC client whose parameters are copied

 * @flavor: security flavor for new client

 *

 * Returns a fresh RPC client or an ERR_PTR.

/**

 * rpc_switch_client_transport: switch the RPC transport on the fly

 * @clnt: pointer to a struct rpc_clnt

 * @args: pointer to the new transport arguments

 * @timeout: pointer to the new timeout parameters

 *

 * This function allows the caller to switch the RPC transport for the

 * rpc_clnt structure 'clnt' to allow it to connect to a mirrored NFS

 * server, for instance.  It assumes that the caller has ensured that

 * there are no active RPC tasks by using some form of locking.

 *

 * Returns zero if "clnt" is now using the new xprt.  Otherwise a

 * negative errno is returned, and "clnt" continues to use the old

 * xprt.

	/*

	 * A new transport was created.  "clnt" therefore

	 * becomes the root of a new cl_parent tree.  clnt's

	 * children, if it has any, still point to the old xprt.

	/*

	 * The old rpc_auth cache cannot be re-used.  GSS

	 * contexts in particular are between a single

	 * client and server.

/**

 * rpc_clnt_iterate_for_each_xprt - Apply a function to all transports

 * @clnt: pointer to client

 * @fn: function to apply

 * @data: void pointer to function data

 *

 * Iterates through the list of RPC transports currently attached to the

 * client and applies the function fn(clnt, xprt, data).

 *

 * On error, the iteration stops, and the function returns the error value.

/*

 * Kill all tasks for the given client.

 * XXX: kill their descendants as well?

	/*

	 * Spin lock all_tasks to prevent changes...

/*

 * Properly shut down an RPC client, terminating all outstanding

 * requests.

/*

 * Free an RPC client

	/* These might block on processes that might allocate memory,

	 * so they cannot be called in rpciod, so they are handled separately

	 * here.

/*

 * Free an RPC client

	/*

	 * Note: RPCSEC_GSS may need to send NULL RPC calls in order to

	 *       release remaining GSS contexts. This mechanism ensures

	 *       that it can do so safely.

/*

 * Release reference to the RPC client

/**

 * rpc_bind_new_program - bind a new RPC program to an existing client

 * @old: old rpc_client

 * @program: rpc program to set

 * @vers: rpc program version

 *

 * Clones the rpc client and sets up a new RPC program. This is mainly

 * of use for enabling different RPC programs to share the same transport.

 * The Sun NFSv2/v3 ACL protocol can do this.

 Remove from client task list */

 Add to the client's list of all tasks */

/*

 * Default callback for async RPC calls

/**

 * rpc_run_task - Allocate a new RPC task, then run rpc_execute against it

 * @task_setup_data: pointer to task initialisation data

/**

 * rpc_call_sync - Perform a synchronous RPC call

 * @clnt: pointer to RPC client

 * @msg: RPC call parameters

 * @flags: RPC call flags

/**

 * rpc_call_async - Perform an asynchronous RPC call

 * @clnt: pointer to RPC client

 * @msg: RPC call parameters

 * @flags: RPC call flags

 * @tk_ops: RPC call ops

 * @data: user call data

/**

 * rpc_run_bc_task - Allocate a new RPC task for backchannel use, then run

 * rpc_execute against it

 * @req: RPC request

	/*

	 * Create an rpc_task to send the data

 CONFIG_SUNRPC_BACKCHANNEL */

/**

 * rpc_prepare_reply_pages - Prepare to receive a reply data payload into pages

 * @req: RPC request to prepare

 * @pages: vector of struct page pointers

 * @base: offset in first page where receive should start, in bytes

 * @len: expected size of the upper layer data payload, in bytes

 * @hdrsize: expected size of upper layer reply header, in XDR words

 *

/**

 * rpc_peeraddr - extract remote peer address from clnt's xprt

 * @clnt: RPC client structure

 * @buf: target buffer

 * @bufsize: length of target buffer

 *

 * Returns the number of bytes that are actually in the stored address.

/**

 * rpc_peeraddr2str - return remote peer address in printable format

 * @clnt: RPC client structure

 * @format: address format

 *

 * NB: the lifetime of the memory referenced by the returned pointer is

 * the same as the rpc_xprt itself.  As long as the caller uses this

 * pointer, it must hold the RCU read lock.

/*

 * Try a getsockname() on a connected datagram socket.  Using a

 * connected datagram socket prevents leaving a socket in TIME_WAIT.

 * This conserves the ephemeral port number space.

 *

 * Returns zero and fills in "buf" if successful; otherwise, a

 * negative errno is returned.

/*

 * Scraping a connected socket failed, so we don't have a useable

 * local address.  Fallback: generate an address that will prevent

 * the server from calling us back.

 *

 * Returns zero and fills in "buf" if successful; otherwise, a

 * negative errno is returned.

/**

 * rpc_localaddr - discover local endpoint address for an RPC client

 * @clnt: RPC client structure

 * @buf: target buffer

 * @buflen: size of target buffer, in bytes

 *

 * Returns zero and fills in "buf" and "buflen" if successful;

 * otherwise, a negative errno is returned.

 *

 * This works even if the underlying transport is not currently connected,

 * or if the upper layer never previously provided a source address.

 *

 * The result of this function call is transient: multiple calls in

 * succession may give different results, depending on how local

 * networking configuration changes over time.

 Couldn't discover local address, return ANYADDR */

/**

 * rpc_net_ns - Get the network namespace for this RPC client

 * @clnt: RPC client to query

 *

/**

 * rpc_max_payload - Get maximum payload size for a transport, in bytes

 * @clnt: RPC client to query

 *

 * For stream transports, this is one RPC record fragment (see RFC

 * 1831), as we don't support multi-record requests yet.  For datagram

 * transports, this is the size of an IP packet minus the IP, UDP, and

 * RPC header sizes.

/**

 * rpc_max_bc_payload - Get maximum backchannel payload size, in bytes

 * @clnt: RPC client to query

/**

 * rpc_force_rebind - force transport to check that remote port is unchanged

 * @clnt: client to rebind

 *

/*

 * Restart an (async) RPC call. Usually called from within the

 * exit handler.

/*

 * Restart an (async) RPC call from the call_prepare state.

 * Usually called from within the exit handler.

/*

 * 0.  Initial state

 *

 *     Other FSM states can be visited zero or more times, but

 *     this state is visited exactly once for each RPC.

 Increment call count (version might not be valid for ping) */

/*

 * 1.	Reserve an RPC call slot

/*

 * 1b.	Grok the result of xprt_reserve()

	/*

	 * After a call to xprt_reserve(), we must have either

	 * a request slot or else an error status.

 woken up; retry */

/*

 * 1c.	Retry reserving an RPC call slot

/*

 * 2.	Bind and/or refresh the credentials

/*

 * 2a.	Process the results of a credential refresh

		/* Use rate-limiting and a max number of retries if refresh

		 * had status 0 but failed to update the cred.

/*

 * 2b.	Allocate the buffer. For details, see sched.c:rpc_malloc.

 *	(Note: buffer memory is freed in xprt_release).

	/*

	 * Calculate the size (in quads) of the RPC call

	 * and reply headers, and convert both values

	 * to byte sizes.

	/*

	 * Note: the reply buffer must at minimum allocate enough space

	 * for the 'struct accepted_reply' from RFC5531.

/*

 * 3.	Encode arguments of an RPC call

 Dequeue task from the receive queue while we're encoding */

 Encode here so that rpcsec_gss can use correct sequence number. */

 Did the encode result in an error condition? */

 Was the error nonfatal? */

 Add task to reply queue before transmission to avoid races */

 Check that the connection is OK */

/*

 * Helpers to check if the task was already transmitted, and

 * to take action when that is the case.

/*

 * 4.	Get the server port number if not yet set

/*

 * 4a.	Sort out bind result

 fail immediately if this is an RPC ping */

 server doesn't support any rpcbind version we know of */

 connection problems */

/*

 * 4b.	Connect to the RPC server

/*

 * 4c.	Sort out connect result

 A positive refusal suggests a rebind is needed. */

 retry with existing socket, after a delay */

 Check for timeouts before looping back to call_bind */

/*

 * 5.	Transmit the RPC request, and wait for reply

/*

 * 5a.	Handle cleanup after a transmission

	/*

	 * Common case: success.  Force the compiler to put this

	 * test first.

		/*

		 * Special cases: if we've been waiting on the

		 * socket's write_space() callback, or if the

		 * socket just returned a connection error,

		 * then hold onto the transport lock.

/*

 * 5b.	Send the backchannel RPC reply.  On error, drop the reply.  In

 * addition, disconnect on connectivity errors.

 Success */

		/*

		 * Problem reaching the server.  Disconnect and let the

		 * forechannel reestablish the connection.  The server will

		 * have to retransmit the backchannel request and we'll

		 * reprocess it.  Since these ops are idempotent, there's no

		 * need to cache our reply at this time.

		/*

		 * We were unable to reply and will have to drop the

		 * request.  The server should reconnect and retransmit.

 CONFIG_SUNRPC_BACKCHANNEL */

/*

 * 6.	Sort out the RPC call status

		/*

		 * Delay any retries for 3 seconds, then handle as if it

		 * were a timeout.

 shutdown or soft timeout */

 No allocated request or transport? return true */

		/*

		 * Once a "no retrans timeout" soft tasks (a.k.a NFSv4) has

		 * been sent, it should time out only if the transport

		 * connection gets terminally broken.

	/*

	 * Did our request time out due to an RPCSEC_GSS out-of-sequence

	 * event? RFC2203 requires the server to drop all such requests.

/*

 * 7.	Decode the RPC reply

	/*

	 * Did we ever call xprt_complete_rqst()? If not, we should assume

	 * the message is incomplete.

	/* Ensure that we see all writes made by xprt_complete_rqst()

	 * before it changed req->rq_reply_bytes_recvd.

 Check that the softirq receive buffer is valid */

 Ensure we obtain a new XID if we retry! */

	/* RFC-1014 says that the representation of XDR data must be a

	 * multiple of four bytes

	 * - if it isn't pointer subtraction in the NFS client may give

	 *   undefined results

 skip XID */

 possibly garbled cred/verf? */

/**

 * rpc_clnt_test_and_add_xprt - Test and add a new transport to a rpc_clnt

 * @clnt: pointer to struct rpc_clnt

 * @xps: pointer to struct rpc_xprt_switch,

 * @xprt: pointer struct rpc_xprt

 * @dummy: unused

/**

 * rpc_clnt_setup_test_and_add_xprt()

 *

 * This is an rpc_clnt_add_xprt setup() function which returns 1 so:

 *   1) caller of the test function must dereference the rpc_xprt_switch

 *   and the rpc_xprt.

 *   2) test function must call rpc_xprt_switch_add_xprt, usually in

 *   the rpc_call_done routine.

 *

 * Upon success (return of 1), the test function adds the new

 * transport to the rpc_clnt xprt switch

 *

 * @clnt: struct rpc_clnt to get the new transport

 * @xps:  the rpc_xprt_switch to hold the new transport

 * @xprt: the rpc_xprt to test

 * @data: a struct rpc_add_xprt_test pointer that holds the test function

 *        and test function call data

 Test the connection */

 rpc_xprt_switch and rpc_xprt are deferrenced by add_xprt_test() */

 so that rpc_clnt_add_xprt does not call rpc_xprt_switch_add_xprt */

/**

 * rpc_clnt_add_xprt - Add a new transport to a rpc_clnt

 * @clnt: pointer to struct rpc_clnt

 * @xprtargs: pointer to struct xprt_create

 * @setup: callback to test and/or set up the connection

 * @data: pointer to setup function data

 *

 * Creates a new transport using the parameters set in args and

 * adds it to clnt.

 * If ping is set, then test that connectivity succeeds before

 * adding the new transport.

 *

 CONFIG_SUNRPC_SWAP */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/net/sunrpc/svcauth.c

 *

 * The generic interface for RPC authentication on the server side.

 *

 * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>

 *

 * CHANGES

 * 19-Apr-2000 Chris Evans      - Security fix

/*

 * Table of authenticators

/* A request, which was authenticated, has now executed.

 * Time to finalise the credentials and verifier

 * and release and resources

/**************************************************

 * 'auth_domains' are stored in a hash table indexed by name.

 * When the last reference to an 'auth_domain' is dropped,

 * the object is unhashed and freed.

 * If auth_domain_lookup fails to find an entry, it will return

 * it's second argument 'new'.  If this is non-null, it will

 * have been atomically linked into the table.

/**

 * auth_domain_cleanup - check that the auth_domain table is empty

 *

 * On module unload the auth_domain_table must be empty.  To make it

 * easier to catch bugs which don't clean up domains properly, we

 * warn if anything remains in the table at cleanup time.

 *

 * Note that we cannot proactively remove the domains at this stage.

 * The ->release() function might be in a module that has already been

 * unloaded.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/net/sunrpc/xprtsock.c

 *

 * Client-side transport implementation for sockets.

 *

 * TCP callback races fixes (C) 1998 Red Hat

 * TCP send fixes (C) 1998 Red Hat

 * TCP NFS related read + write fixes

 *  (C) 1999 Dave Airlie, University of Limerick, Ireland <airlied@linux.ie>

 *

 * Rewrite of larges part of the code in order to stabilize TCP stuff.

 * Fix behaviour when socket buffer is full.

 *  (C) 1999 Trond Myklebust <trond.myklebust@fys.uio.no>

 *

 * IP socket transport implementation, (C) 2005 Chuck Lever <cel@netapp.com>

 *

 * IPv6 support contributed by Gilles Quillard, Bull Open Source, 2005.

 *   <gilles.quillard@bull.net>

/*

 * xprtsock tunables

/*

 * We can register our own files under /proc/sys/sunrpc by

 * calling register_sysctl_table() again.  The files in that

 * directory become the union of all files registered there.

 *

 * We simply need to make sure that we don't collide with

 * someone else's file names!

/*

 * FIXME: changing the UDP slot table size should also resize the UDP

 *        socket buffers for existing UDP transports

/*

 * Wait duration for a reply from the RPC portmapper.

/*

 * Delay if a UDP socket connect error occurs.  This is most likely some

 * kind of resource problem on the local host.

/*

 * The reestablish timeout allows clients to delay for a bit before attempting

 * to reconnect to a server that just dropped our connection.

 *

 * We implement an exponential backoff when trying to reestablish a TCP

 * transport connection with the server.  Some servers like to drop a TCP

 * connection when they are overworked, so we start with a short timeout and

 * increase over time if the server is down or not responding.

/*

 * TCP idle timeout; client drops the transport socket if it is idle

 * for this long.  Note that we also timeout UDP sockets to prevent

 * holding port numbers when there is no RPC traffic.

 NOP */

 Is this transport associated with the backchannel? */

 Look up and lock the request corresponding to the given XID */

 CONFIG_SUNRPC_BACKCHANNEL */

 CONFIG_SUNRPC_BACKCHANNEL */

 Look up and lock the request corresponding to the given XID */

/**

 * xs_nospace - handle transmit was incomplete

 * @req: pointer to RPC request

 *

 Protect against races with write_space */

 Don't race with disconnect */

 wait for more buffer space */

 Race breaker in case memory is freed before above code is called */

/*

 * Determine if the previous message in the stream was aborted before it

 * could complete transmission.

/*

 * Return the stream record marker field for a record of length < 2^31-1

/**

 * xs_local_send_request - write an RPC request to an AF_LOCAL socket

 * @req: pointer to RPC request

 *

 * Return values:

 *        0:	The request has been sent

 *   EAGAIN:	The socket was blocked, please call again later to

 *		complete the request

 * ENOTCONN:	Caller needs to invoke connect logic then call again

 *    other:	Some other error occurred, the request was not sent

 Close the stream if the previous transmission was incomplete */

/**

 * xs_udp_send_request - write an RPC request to a UDP socket

 * @req: pointer to RPC request

 *

 * Return values:

 *        0:	The request has been sent

 *   EAGAIN:	The socket was blocked, please call again later to

 *		complete the request

 * ENOTCONN:	Caller needs to invoke connect logic then call again

 *    other:	Some other error occurred, the request was not sent

 firewall is blocking us, don't return -EAGAIN or we end up looping */

 Still some bytes left; set up for a retry later. */

 Should we call xs_close() here? */

		/* When the server has died, an ICMP port unreachable message

/**

 * xs_tcp_send_request - write an RPC request to a TCP socket

 * @req: pointer to RPC request

 *

 * Return values:

 *        0:	The request has been sent

 *   EAGAIN:	The socket was blocked, please call again later to

 *		complete the request

 * ENOTCONN:	Caller needs to invoke connect logic then call again

 *    other:	Some other error occurred, the request was not sent

 *

 * XXX: In the case of soft timeouts, should we eventually give up

 *	if sendmsg is not able to make progress?

 Close the stream if the previous transmission was incomplete */

	/* Continue transmitting the packet/record. We must be careful

	 * to cope with writespace callbacks arriving _after_ we have

		/* If we've sent the entire packet, immediately

			/*

			 * Return EAGAIN if we're sure we're hitting the

			 * socket send buffer limits.

			/*

			 * Did we hit a memory allocation failure?

				/* Retry, knowing now that we're below the

				 * socket send buffer limit

 Should we call xs_close() here? */

/**

 * xs_error_report - callback to handle TCP socket state errors

 * @sk: socket

 *

 * Note: we don't call sock_error() since there may be a rpc_task

 * using the socket, and so we don't want to clear sk->sk_err.

 barrier ensures xprt_err is set before XPRT_SOCK_WAKE_ERROR */

 Reset stream record info */

/**

 * xs_close - close a socket

 * @xprt: transport

 *

 * This is used when all requests are complete; ie, no DRC state remains

 * on the server we want to save.

 *

 * The caller _must_ be holding XPRT_LOCKED in order to avoid issues with

 * xs_reset_transport() zeroing the socket from underneath a writer.

/**

 * xs_destroy - prepare to shutdown a transport

 * @xprt: doomed transport

 *

/**

 * xs_udp_data_read_skb - receive callback for UDP sockets

 * @xprt: transport

 * @sk: socket

 * @skb: skbuff

 *

 Copy the XID from the skb... */

 Look up and lock the request corresponding to the given XID */

 Suck it into the iovec, verify checksum if not done by hw. */

/**

 * xs_data_ready - "data ready" callback for UDP sockets

 * @sk: socket with data to read

 *

		/* Any data means we had a useful conversation, so

		 * then we don't need to delay the next reconnect

/*

 * Helper function to force a TCP close if the server is sending

 * junk and/or it has put us in CLOSE_WAIT

 CONFIG_SUNRPC_BACKCHANNEL */

/**

 * xs_tcp_state_change - callback to handle TCP socket state changes

 * @sk: socket whose state has changed

 *

 The client initiated a shutdown of the socket */

 The server initiated a shutdown of the socket */

		/*

		 * If the server closed down the connection, make sure that

		 * we back off before reconnecting

 Trigger the socket release */

/**

 * xs_udp_write_space - callback invoked when socket buffer space

 *                             becomes available

 * @sk: socket whose state has changed

 *

 * Called when more output buffer space is available for this socket.

 * We try not to wake our writers until they can make "significant"

 * progress, otherwise we'll waste resources thrashing kernel_sendmsg

 * with a bunch of small requests.

 from net/core/sock.c:sock_def_write_space */

/**

 * xs_tcp_write_space - callback invoked when socket buffer space

 *                             becomes available

 * @sk: socket whose state has changed

 *

 * Called when more output buffer space is available for this socket.

 * We try not to wake our writers until they can make "significant"

 * progress, otherwise we'll waste resources thrashing kernel_sendmsg

 * with a bunch of small requests.

 from net/core/stream.c:sk_stream_write_space */

/**

 * xs_udp_set_buffer_size - set send and receive limits

 * @xprt: generic transport

 * @sndsize: requested size of send buffer, in bytes

 * @rcvsize: requested size of receive buffer, in bytes

 *

 * Set socket send and receive buffer size limits.

/**

 * xs_udp_timer - called when a retransmit timeout occurs on a UDP transport

 * @xprt: controlling transport

 * @task: task that timed out

 *

 * Adjust the congestion window after a retransmit timeout has occurred.

/**

 * xs_set_port - reset the port number in the remote endpoint address

 * @xprt: generic transport

 * @port: new port number

 *

	/*

	 * If we are asking for any ephemeral port (i.e. port == 0 &&

	 * transport->xprt.resvport == 0), don't bind.  Let the local

	 * port selection happen implicitly when the socket is used

	 * (for example at connect time).

	 *

	 * This ensures that we can continue to establish TCP

	 * connections even when all local ephemeral ports are already

	 * a part of some TCP connection.  This makes no difference

	 * for UDP sockets, but also doesn't harm them.

	 *

	 * If we're asking for any reserved port (i.e. port == 0 &&

	 * transport->xprt.resvport == 1) xs_get_srcport above will

	 * ensure that port is non-zero and we will bind as needed.

/*

 * We don't support autobind on AF_LOCAL sockets

 Reset to new socket */

/**

 * xs_local_setup_socket - create AF_LOCAL socket, connect to a local endpoint

 * @transport: socket transport to connect

		/*

		 * We want the AF_LOCAL connect to be resolved in the

		 * filesystem namespace of the process making the rpc

		 * call.  Thus we connect synchronously.

		 *

		 * If we want to support asynchronous AF_LOCAL calls,

		 * we'll need to figure out how to pass a namespace to

		 * connect.

/*

 * Note that this should be called with XPRT_LOCKED held (or when we otherwise

 * know that we have exclusive access to the socket), to guard against

 * races with xs_reset_transport.

	/*

	 * If there's no sock, then we have nothing to set. The

	 * reconnecting process will get it for us.

/**

 * xs_enable_swap - Tag this transport as being used for swap.

 * @xprt: transport to tag

 *

 * Take a reference to this transport on behalf of the rpc_clnt, and

 * optionally mark it for swapping if it wasn't already.

/**

 * xs_disable_swap - Untag this transport as being used for swap.

 * @xprt: transport to tag

 *

 * Drop a "swapper" reference to this xprt on behalf of the rpc_clnt. If the

 * swapper refcount goes to 0, untag the socket as a memalloc socket.

 Reset to new socket */

/**

 * xs_tcp_shutdown - gracefully shut down a TCP socket

 * @xprt: transport

 *

 * Initiates a graceful shutdown of the TCP socket by calling the

 * equivalent of shutdown(SHUT_RDWR);

 TCP Keepalive options */

 TCP user timeout (see RFC5482) */

 Arbitrary lower limit */

		/* Avoid temporary address, they are bad for long-lived

		 * connections such as NFS mounts.

		 * RFC4941, section 3.6 suggests that:

		 *    Individual applications, which have specific

		 *    knowledge about the normal duration of connections,

		 *    MAY override this as appropriate.

 socket options */

 Reset to new socket */

 Tell the socket layer to start connecting... */

/**

 * xs_tcp_setup_socket - create a TCP socket and connect to a remote endpoint

 * @work: queued work item

 *

 * Invoked by a work queue tasklet.

 SYN_SENT! */

 Source port number is unavailable. Try a new one! */

		/* Happens, for instance, if the user specified a link

		 * local IPv6 address without a scope-id.

	/* xs_tcp_force_close() wakes tasks with a fixed error code.

	 * We need to wake them first to ensure the correct error code.

/**

 * xs_connect - connect a socket to a remote endpoint

 * @xprt: pointer to transport structure

 * @task: address of RPC task that manages state of connect request

 *

 * TCP: If the remote end dropped the connection, delay reconnecting.

 *

 * UDP socket connects are synchronous, but we use a work queue anyway

 * to guarantee that even unprivileged user processes can set up a

 * socket on a privileged port.

 *

 * If a UDP socket connect fails, the delay behavior here prevents

 * retry floods (hard mounts).

 Start by resetting any existing state */

/**

 * xs_local_print_stats - display AF_LOCAL socket-specific stats

 * @xprt: rpc_xprt struct containing statistics

 * @seq: output file

 *

/**

 * xs_udp_print_stats - display UDP socket-specific stats

 * @xprt: rpc_xprt struct containing statistics

 * @seq: output file

 *

/**

 * xs_tcp_print_stats - display TCP socket-specific stats

 * @xprt: rpc_xprt struct containing statistics

 * @seq: output file

 *

/*

 * Allocate a bunch of pages for a scratch buffer for the rpc code. The reason

 * we allocate pages instead doing a kmalloc like rpc_malloc is because we want

 * to use the server side send routines.

/*

 * Free the space allocated in the bc_alloc routine

/**

 * bc_send_request - Send a backchannel Call on a TCP socket

 * @req: rpc_rqst containing Call message to be sent

 *

 * xpt_mutex ensures @rqstp's whole message is written to the socket

 * without interruption.

 *

 * Return values:

 *   %0 if the message was sent successfully

 *   %ENOTCONN if the message was not sent

	/*

	 * Get the server socket associated with this callback xprt

	/*

	 * Grab the mutex to serialize data as the connection is shared

	 * with the fore channel

/*

 * The close routine. Since this is client initiated, we do nothing

/*

 * The xprt destroy routine. Again, because this connection is client

 * initiated, we do nothing

/*

 * The rpc_xprt_ops for the server backchannel

/**

 * xs_setup_local - Set up transport to use an AF_LOCAL socket

 * @args: rpc transport creation arguments

 *

 * AF_LOCAL is a "tpi_cots_ord" transport, just like TCP

/**

 * xs_setup_udp - Set up transport to use a UDP socket

 * @args: rpc transport creation arguments

 *

 XXX: header size can vary due to auth type, IPv6, etc. */

/**

 * xs_setup_tcp - Set up transport to use a TCP socket

 * @args: rpc transport creation arguments

 *

/**

 * xs_setup_bc_tcp - Set up transport to use a TCP backchannel socket

 * @args: rpc transport creation arguments

 *

 backchannel */

	/*

	 * Once we've associated a backchannel xprt with a connection,

	 * we want to keep it around as long as the connection lasts,

	 * in case we need to start using it for a backchannel again;

	 * this reference won't be dropped until bc_xprt is destroyed.

	/*

	 * Since we don't want connections for the backchannel, we set

	 * the xprt status to connected

/**

 * init_socket_xprt - set up xprtsock's sysctls, register with RPC client

 *

/**

 * cleanup_socket_xprt - remove xprtsock's sysctls, unregister

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 * In-kernel rpcbind client supporting versions 2, 3, and 4 of the rpcbind

 * protocol

 *

 * Based on RFC 1833: "Binding Protocols for ONC RPC Version 2" and

 * RFC 3530: "Network File System (NFS) version 4 Protocol"

 *

 * Original: Gilles Quillard, Bull Open Source, 2005 <gilles.quillard@bull.net>

 * Updated: Chuck Lever, Oracle Corporation, 2007 <chuck.lever@oracle.com>

 *

 * Descended from net/sunrpc/pmap_clnt.c,

 *  Copyright (C) 1996, Olaf Kirch <okir@monad.swb.de>

 alias for GETPORT */

 alias for CALLIT */

/*

 * r_owner

 *

 * The "owner" is allowed to unset a service in the rpcbind database.

 *

 * For AF_LOCAL SET/UNSET requests, rpcbind treats this string as a

 * UID which it maps to a local user name via a password lookup.

 * In all other cases it is ignored.

 *

 * For SET/UNSET requests, user space provides a value, even for

 * network requests, and GETADDR uses an empty string.  We follow

 * those precedents here.

/*

 * XDR data type sizes

/*

 * XDR argument and result sizes

/*

 * Note that RFC 1833 does not put any size restrictions on the

 * address string returned by the remote rpcbind database.

		/*

		 * cleanup_rpcb_clnt - remove xprtsock's sysctls, unregister

 Protected by rpcb_create_local_mutex */

/*

 * Returns zero on success, otherwise a negative errno value

 * is returned.

		/*

		 * We turn off the idle timeout to prevent the kernel

		 * from automatically disconnecting the socket.

		 * Otherwise, we'd have to cache the mount namespace

		 * of the caller and somehow pass that to the socket

		 * reconnect code.

	/*

	 * Because we requested an RPC PING at transport creation time,

	 * this works only if the user space portmapper is rpcbind, and

	 * it's listening on AF_LOCAL on the named socket.

/*

 * Returns zero on success, otherwise a negative errno value

 * is returned.

	/*

	 * This results in an RPC ping.  On systems running portmapper,

	 * the v4 ping will fail.  Proceed anyway, but disallow rpcb

	 * v4 upcalls.

/*

 * Returns zero on success, otherwise a negative errno value

 * is returned.

/**

 * rpcb_register - set or unset a port registration with the local rpcbind svc

 * @net: target network namespace

 * @prog: RPC program number to bind

 * @vers: RPC version number to bind

 * @prot: transport protocol to register

 * @port: port value to register

 *

 * Returns zero if the registration request was dispatched successfully

 * and the rpcbind daemon returned success.  Otherwise, returns an errno

 * value that reflects the nature of the error (request could not be

 * dispatched, timed out, or rpcbind returned an error).

 *

 * RPC services invoke this function to advertise their contact

 * information via the system's rpcbind daemon.  RPC services

 * invoke this function once for each [program, version, transport]

 * tuple they wish to advertise.

 *

 * Callers may also unregister RPC services that are no longer

 * available by setting the passed-in port to zero.  This removes

 * all registered transports for [program, version] from the local

 * rpcbind database.

 *

 * This function uses rpcbind protocol version 2 to contact the

 * local rpcbind daemon.

 *

 * Registration works over both AF_INET and AF_INET6, and services

 * registered via this function are advertised as available for any

 * address.  If the local rpcbind daemon is listening on AF_INET6,

 * services registered via this function will be advertised on

 * IN6ADDR_ANY (ie available for all AF_INET and AF_INET6

 * addresses).

/*

 * Fill in AF_INET family-specific arguments to register

/*

 * Fill in AF_INET6 family-specific arguments to register

/**

 * rpcb_v4_register - set or unset a port registration with the local rpcbind

 * @net: target network namespace

 * @program: RPC program number of service to (un)register

 * @version: RPC version number of service to (un)register

 * @address: address family, IP address, and port to (un)register

 * @netid: netid of transport protocol to (un)register

 *

 * Returns zero if the registration request was dispatched successfully

 * and the rpcbind daemon returned success.  Otherwise, returns an errno

 * value that reflects the nature of the error (request could not be

 * dispatched, timed out, or rpcbind returned an error).

 *

 * RPC services invoke this function to advertise their contact

 * information via the system's rpcbind daemon.  RPC services

 * invoke this function once for each [program, version, address,

 * netid] tuple they wish to advertise.

 *

 * Callers may also unregister RPC services that are registered at a

 * specific address by setting the port number in @address to zero.

 * They may unregister all registered protocol families at once for

 * a service by passing a NULL @address argument.  If @netid is ""

 * then all netids for [program, version, address] are unregistered.

 *

 * This function uses rpcbind protocol version 4 to contact the

 * local rpcbind daemon.  The local rpcbind daemon must support

 * version 4 of the rpcbind protocol in order for these functions

 * to register a service successfully.

 *

 * Supported netids include "udp" and "tcp" for UDP and TCP over

 * IPv4, and "udp6" and "tcp6" for UDP and TCP over IPv6,

 * respectively.

 *

 * The contents of @address determine the address family and the

 * port to be registered.  The usual practice is to pass INADDR_ANY

 * as the raw address, but specifying a non-zero address is also

 * supported by this API if the caller wishes to advertise an RPC

 * service on a specific network interface.

 *

 * Note that passing in INADDR_ANY does not create the same service

 * registration as IN6ADDR_ANY.  The former advertises an RPC

 * service on any IPv4 address, but not on IPv6.  The latter

 * advertises the service on all IPv4 and IPv6 addresses.

/*

 * In the case where rpc clients have been cloned, we want to make

 * sure that we use the program number/version etc of the actual

 * owner of the xprt. To do so, we walk back up the tree of parents

 * to find whoever created the transport and/or whoever has the

 * autobind flag set.

/**

 * rpcb_getport_async - obtain the port for a given RPC service on a given host

 * @task: task that is waiting for portmapper request

 *

 * This one can be called for an ongoing RPC request, and can be used in

 * an async (rpciod) context.

	/* Put self on the wait queue to ensure we get notified if

 Someone else may have bound if we slept */

 Parent transport's destination address */

 Don't ever use rpcbind v2 for AF_INET6 requests */

/*

 * Rpcbind child task calls this callback via tk_exit.

 Garbage reply: retry with a lesser rpcbind version */

 rpcbind server doesn't support this rpcbind protocol version */

 rpcbind server not available on remote host? */

 Requested RPC service wasn't registered on remote host */

 Succeeded */

/*

 * XDR functions for rpcbind

 truncate and hope for the best */

	/*

	 * If the returned universal address is a null string,

	 * the requested RPC service was not registered.

/*

 * Not all rpcbind procedures described in RFC 1833 are implemented

 * since the Linux kernel RPC code requires only these.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/net/sunrpc/sunrpc_syms.c

 *

 * Symbols exported by the sunrpc module.

 *

 * Copyright (C) 1997 Olaf Kirch <okir@monad.swb.de>

 svc sock transport */

 clnt sock transport */

 Wait for completion of call_rcu()'s */

 Ensure we're initialised before nfs */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/net/sunrpc/sched.c

 *

 * Scheduling for synchronous and asynchronous RPC requests.

 *

 * Copyright (C) 1996 Olaf Kirch, <okir@monad.swb.de>

 *

 * TCP NFS related read + write fixes

 * (C) 1999 Dave Airlie, University of Limerick, Ireland <airlied@linux.ie>

/*

 * RPC slabs and memory pools

/*

 * RPC tasks sit here while waiting for conditions to improve.

/*

 * rpciod-related stuff

/*

 * Disable the timer for a given RPC task. Should be called with

 * queue->lock and bh_disabled in order to avoid races within

 * rpc_run_timer().

/*

 * Set up a timer for the current task.

/*

 * Add a request to a queue list

 Cache the queue head in task->u.tk_wait.list */

/*

 * Remove request from a queue list

 Assume __rpc_list_enqueue_task() cached the queue head */

/*

 * Add new request to a priority queue.

/*

 * Add new request to wait queue.

 *

 * Swapper tasks always get inserted at the head of the queue.

 * This should avoid many nasty memory deadlocks and hopefully

 * improve overall performance.

 * Everyone else gets appended to the queue to ensure proper FIFO behavior.

 barrier matches the read in rpc_wake_up_task_queue_locked() */

/*

 * Remove request from a priority queue.

/*

 * Remove request from queue.

 * Note: must be called with spin lock held.

 Might be a task carrying a reverse-direction operation */

/*

 * Mark an RPC call as having completed by clearing the 'active' bit

 * and then waking up all tasks that were sleeping.

/*

 * Allow callers to wait for completion of an RPC call

 *

 * Note the use of out_of_line_wait_on_bit() rather than wait_on_bit()

 * to enforce taking of the wq->lock and hence avoid races with

 * rpc_complete_task().

/*

 * Make an RPC task runnable.

 *

 * Note: If the task is ASYNC, and is being made runnable after sitting on an

 * rpc_wait_queue, this must be called with the queue spinlock held to protect

 * the wait queue operation.

 * Note the ordering of rpc_test_and_set_running() and rpc_clear_queued(),

 * which is needed to ensure that __rpc_execute() doesn't loop (due to the

 * lockless RPC_IS_QUEUED() test) before we've had a chance to test

 * the RPC_TASK_RUNNING flag.

/*

 * Prepare for sleeping on a wait queue.

 * By always appending tasks to the list we ensure FIFO behavior.

 * NB: An RPC task will only receive interrupt-driven events as long

 * as it's on a wait queue.

 We shouldn't ever put an inactive task to sleep */

	/*

	 * Protect the queue operations.

	/*

	 * Protect the queue operations.

	/*

	 * Protect the queue operations.

	/*

	 * Protect the queue operations.

/**

 * __rpc_do_wake_up_task_on_wq - wake up a single rpc_task

 * @wq: workqueue on which to run task

 * @queue: wait queue

 * @task: task to be woken up

 *

 * Caller must hold queue->lock, and have cleared the task queued flag.

 Has the task been executed yet? If not, we cannot wake it up! */

/*

 * Wake up a queued task while the queue lock is being held

/*

 * Wake up a queued task while the queue lock is being held

/*

 * Wake up a task on a specific queue

/**

 * rpc_wake_up_queued_task_set_status - wake up a task and set task->tk_status

 * @queue: pointer to rpc_wait_queue

 * @task: pointer to rpc_task

 * @status: integer error value

 *

 * If @task is queued on @queue, then it is woken up, and @task->tk_status is

 * set to the value of @status.

/*

 * Wake up the next task on a priority queue.

	/*

	 * Service the privileged queue.

	/*

	 * Service a batch of tasks from a single owner.

	/*

	 * Service the next queue.

/*

 * Wake up the first task on the wait queue.

/*

 * Wake up the first task on the wait queue.

/*

 * Wake up the next task on the wait queue.

/**

 * rpc_wake_up_locked - wake up all rpc_tasks

 * @queue: rpc_wait_queue on which the tasks are sleeping

 *

/**

 * rpc_wake_up - wake up all rpc_tasks

 * @queue: rpc_wait_queue on which the tasks are sleeping

 *

 * Grabs queue->lock

/**

 * rpc_wake_up_status_locked - wake up all rpc_tasks and set their status value.

 * @queue: rpc_wait_queue on which the tasks are sleeping

 * @status: status value to set

/**

 * rpc_wake_up_status - wake up all rpc_tasks and set their status value.

 * @queue: rpc_wait_queue on which the tasks are sleeping

 * @status: status value to set

 *

 * Grabs queue->lock

/*

 * Run a task at a later time

/*

 * Helper to call task->tk_ops->rpc_call_prepare

 Initialize retry counters */

 starting timestamp */

/*

 * Helper that calls task->tk_ops->rpc_call_done if it exists

 Always release the RPC slot and buffer memory */

/*

 * This is the RPC `scheduler' (or rather, the finite state machine).

		/*

		 * Perform the next FSM step or a pending callback.

		 *

		 * tk_action may be NULL if the task has been killed.

		 * In particular, note that rpc_killall_tasks may

		 * do this at any time, so beware when dereferencing.

		/*

		 * Lockless check for whether task is sleeping or not.

		/*

		 * Signalled tasks should exit rather than sleep.

		/*

		 * The queue->lock protects against races with

		 * rpc_make_runnable().

		 *

		 * Note that once we clear RPC_TASK_RUNNING on an asynchronous

		 * rpc_task, rpc_make_runnable() can assign it to a

		 * different workqueue. We therefore cannot assume that the

		 * rpc_task pointer may still be dereferenced.

 sync task: sleep here */

			/*

			 * When a sync task receives a signal, it exits with

			 * -ERESTARTSYS. In order to catch any callbacks that

			 * clean up after sleeping on some queue, we don't

			 * break the loop here, but go around once more.

 Release all resources associated with the task */

/*

 * User-visible entry point to the scheduler.

 *

 * This may be called recursively if e.g. an async NFS task updates

 * the attributes and finds that dirty pages must be flushed.

 * NOTE: Upon exit of this function the task is guaranteed to be

 *	 released. In particular note that tk_release() will have

 *	 been called, so your task memory may have been freed.

/**

 * rpc_malloc - allocate RPC buffer resources

 * @task: RPC task

 *

 * A single memory region is allocated, which is split between the

 * RPC call and RPC reply that this task is being used for. When

 * this RPC is retired, the memory is released by calling rpc_free.

 *

 * To prevent rpciod from hanging, this allocator never sleeps,

 * returning -ENOMEM and suppressing warning if the request cannot

 * be serviced immediately. The caller can arrange to sleep in a

 * way that is safe for rpciod.

 *

 * Most requests are 'small' (under 2KiB) and can be serviced from a

 * mempool, ensuring that NFS reads and writes can always proceed,

 * and that there is good locality of reference for these buffers.

/**

 * rpc_free - free RPC buffer resources allocated via rpc_malloc

 * @task: RPC task

 *

/*

 * Creation and deletion of RPC task structures

 Initialize workqueue for async tasks */

/*

 * Create a new task for the specified client.

/*

 * rpc_free_task - release rpc task and perform cleanups

 *

 * Note that we free up the rpc_task _after_ rpc_release_calldata()

 * in order to work around a workqueue dependency issue.

 *

 * Tejun Heo states:

 * "Workqueue currently considers two work items to be the same if they're

 * on the same address and won't execute them concurrently - ie. it

 * makes a work item which is queued again while being executed wait

 * for the previous execution to complete.

 *

 * If a work function frees the work item, and then waits for an event

 * which should be performed by another work item and *that* work item

 * recycles the freed work item, it can create a false dependency loop.

 * There really is no reliable way to detect this short of verifying

 * every memory free."

 *

	/*

	 * Note: at this point we have been removed from rpc_clnt->cl_tasks,

	 * so it should be safe to use task->tk_count as a test for whether

	 * or not any other processes still hold references to our rpc_task.

 Wake up anyone who may be waiting for task completion */

/*

 * Start up the rpciod workqueue.

	/*

	 * Create the rpciod thread and wait for it to start.

	/*

	 * The following is not strictly a mempool initialisation,

	 * but there is no harm in doing it here

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/net/sunrpc/sysctl.c

 *

 * Sysctl interface to sunrpc module.

 *

 * I would prefer to register the sunrpc table below sys/net, but that's

 * impossible at the moment.

/*

 * Declare the debug flags here

 Display the RPC tasks on writing to rpc_debug */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/net/sunrpc/timer.c

 *

 * Estimate RPC request round trip time.

 *

 * Based on packet round-trip and variance estimator algorithms described

 * in appendix A of "Congestion Avoidance and Control" by Van Jacobson

 * and Michael J. Karels (ACM Computer Communication Review; Proceedings

 * of the Sigcomm '88 Symposium in Stanford, CA, August, 1988).

 *

 * This RTT estimator is used only for RPC over datagram protocols.

 *

 * Copyright (C) 2002 Trond Myklebust <trond.myklebust@fys.uio.no>

/**

 * rpc_init_rtt - Initialize an RPC RTT estimator context

 * @rt: context to initialize

 * @timeo: initial timeout value, in jiffies

 *

/**

 * rpc_update_rtt - Update an RPC RTT estimator context

 * @rt: context to update

 * @timer: timer array index (request type)

 * @m: recent actual RTT, in jiffies

 *

 * NB: When computing the smoothed RTT and standard deviation,

 *     be careful not to produce negative intermediate results.

 jiffies wrapped; ignore this one */

 Set lower bound on the variance */

/**

 * rpc_calc_rto - Provide an estimated timeout value

 * @rt: context to use for calculation

 * @timer: timer array index (request type)

 *

 * Estimate RTO for an NFS RPC sent via an unreliable datagram.  Use

 * the mean and mean deviation of RTT for the appropriate type of RPC

 * for frequently issued RPCs, and a fixed default for the others.

 *

 * The justification for doing "other" this way is that these RPCs

 * happen so infrequently that timer estimation would probably be

 * stale.  Also, since many of these RPCs are non-idempotent, a

 * conservative timeout is desired.

 *

 * getattr, lookup,

 * read, write, commit     - A+4D

 * other                   - timeo

 SPDX-License-Identifier: GPL-2.0

/*

 * debugfs interface for sunrpc

 *

 * (c) 2014 Jeff Layton <jlayton@primarydata.com>

 If there's another task on list, return it */

 enough for "../../rpc_xprt/ + 8 hex digits + NULL */

 enough for 8 hex digits + NULL */

 enough for 8 hex digits + NULL */

 make the per-client dir */

 make tasks file */

 8 hex digits + NULL term */

 make the per-client dir */

 make tasks file */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/net/sunrpc/auth_null.c

 *

 * AUTH_NULL authentication. Really :-)

 *

 * Copyright (C) 1996, Olaf Kirch <okir@monad.swb.de>

/*

 * Lookup NULL creds for current process

/*

 * Destroy cred handle.

/*

 * Match cred handle against current process

/*

 * Marshal credential.

 Credential */

 Verifier */

/*

 * Refresh credential. This is a no-op for AUTH_NULL

 SPDX-License-Identifier: GPL-2.0

/*

 * Multipath support for RPC

 *

 * Copyright (c) 2015, 2016, Primary Data, Inc. All rights reserved.

 *

 * Trond Myklebust <trond.myklebust@primarydata.com>

 *

/**

 * rpc_xprt_switch_add_xprt - Add a new rpc_xprt to an rpc_xprt_switch

 * @xps: pointer to struct rpc_xprt_switch

 * @xprt: pointer to struct rpc_xprt

 *

 * Adds xprt to the end of the list of struct rpc_xprt in xps.

/**

 * rpc_xprt_switch_remove_xprt - Removes an rpc_xprt from a rpc_xprt_switch

 * @xps: pointer to struct rpc_xprt_switch

 * @xprt: pointer to struct rpc_xprt

 *

 * Removes xprt from the list of struct rpc_xprt in xps.

/**

 * xprt_switch_alloc - Allocate a new struct rpc_xprt_switch

 * @xprt: pointer to struct rpc_xprt

 * @gfp_flags: allocation flags

 *

 * On success, returns an initialised struct rpc_xprt_switch, containing

 * the entry xprt. Returns NULL on failure.

/**

 * xprt_switch_get - Return a reference to a rpc_xprt_switch

 * @xps: pointer to struct rpc_xprt_switch

 *

 * Returns a reference to xps unless the refcount is already zero.

/**

 * xprt_switch_put - Release a reference to a rpc_xprt_switch

 * @xps: pointer to struct rpc_xprt_switch

 *

 * Release the reference to xps, and free it once the refcount is zero.

/**

 * rpc_xprt_switch_set_roundrobin - Set a round-robin policy on rpc_xprt_switch

 * @xps: pointer to struct rpc_xprt_switch

 *

 * Sets a round-robin default policy for iterators acting on xps.

 Exit loop if xprt_queuelen <= average queue length */

/*

 * xprt_iter_rewind - Resets the xprt iterator

 * @xpi: pointer to rpc_xprt_iter

 *

 * Resets xpi to ensure that it points to the first entry in the list

 * of transports.

/**

 * xprt_iter_init - Initialise an xprt iterator

 * @xpi: pointer to rpc_xprt_iter

 * @xps: pointer to rpc_xprt_switch

 *

 * Initialises the iterator to use the default iterator ops

 * as set in xps. This function is mainly intended for internal

 * use in the rpc_client.

/**

 * xprt_iter_init_listall - Initialise an xprt iterator

 * @xpi: pointer to rpc_xprt_iter

 * @xps: pointer to rpc_xprt_switch

 *

 * Initialises the iterator to iterate once through the entire list

 * of entries in xps.

/**

 * xprt_iter_xchg_switch - Atomically swap out the rpc_xprt_switch

 * @xpi: pointer to rpc_xprt_iter

 * @newswitch: pointer to a new rpc_xprt_switch or NULL

 *

 * Swaps out the existing xpi->xpi_xpswitch with a new value.

 Atomically swap out the old xpswitch */

/**

 * xprt_iter_destroy - Destroys the xprt iterator

 * @xpi: pointer to rpc_xprt_iter

/**

 * xprt_iter_xprt - Returns the rpc_xprt pointed to by the cursor

 * @xpi: pointer to rpc_xprt_iter

 *

 * Returns a pointer to the struct rpc_xprt that is currently

 * pointed to by the cursor.

 * Caller must be holding rcu_read_lock().

/**

 * xprt_iter_get_xprt - Returns the rpc_xprt pointed to by the cursor

 * @xpi: pointer to rpc_xprt_iter

 *

 * Returns a reference to the struct rpc_xprt that is currently

 * pointed to by the cursor.

/**

 * xprt_iter_get_next - Returns the next rpc_xprt following the cursor

 * @xpi: pointer to rpc_xprt_iter

 *

 * Returns a reference to the struct rpc_xprt that immediately follows the

 * entry pointed to by the cursor.

 Policy for always returning the first entry in the rpc_xprt_switch */

 Policy for round-robin iteration of entries in the rpc_xprt_switch */

 Policy for once-through iteration of entries in the rpc_xprt_switch */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * net/sunrpc/cache.c

 *

 * Generic code for various authentication-related caches

 * used by sunrpc clients and servers.

 *

 * Copyright (C) 2002 Neil Brown <neilb@cse.unsw.edu.au>

 ensure it isn't already expired */

 Must be called under cd->hash_lock */

	/* must fully initialise 'new', else

	 * we might get lose if we need to

	 * cache_put it soon.

 check if entry appeared while we slept */

 Didn't find anything, insert an empty entry */

 ensure it isn't immediately treated as expired */

 paired with smp_rmb() in cache_is_valid() */

	/* The 'old' entry is to be replaced by 'new'.

	 * If 'old' is not VALID, we update it directly,

	 * otherwise we need to replace it

 We need to insert a new entry */

 entry is valid */

			/*

			 * In combination with write barrier in

			 * sunrpc_cache_update, ensures that anyone

			 * using the cache entry after this sees the

			 * updated contents:

/*

 * This is the generic cache management routine for all

 * the authentication caches.

 * It checks the currency of a cache item and will (later)

 * initiate an upcall to fill it if needed.

 *

 *

 * Returns 0 if the cache_head can be used, or cache_puts it and returns

 * -EAGAIN if upcall is pending and request has been queued

 * -ETIMEDOUT if upcall failed or request could not be queue or

 *           upcall completed but item is still invalid (implying that

 *           the cache item has been replaced with a newer one).

 * -ENOENT if cache entry was negative

 First decide return status as best we can */

 now see if we want to start an upcall */

			/*

			 * Request was not deferred; handle it as best

			 * we can ourselves:

/*

 * caches need to be periodically cleaned.

 * For this we maintain a list of cache_detail and

 * a current pointer into that list and into the table

 * for that entry.

 *

 * Each time cache_clean is called it finds the next non-empty entry

 * in the current table and walks the list in that entry

 * looking for entries that can be removed.

 *

 * An entry gets removed if:

 * - The expiry is before current time

 * - The last_refresh time is before the flush_time for that cache

 *

 * later we might drop old entries with non-NEVER expiry if that table

 * is getting 'full' for some definition of 'full'

 *

 * The question of "how often to scan a table" is an interesting one

 * and is answered in part by the use of the "nextcheck" field in the

 * cache_detail.

 * When a scan of a table begins, the nextcheck field is set to a time

 * that is well into the future.

 * While scanning, if an expiry time is found that is earlier than the

 * current nextcheck time, nextcheck is set to that expiry time.

 * If the flush_time is ever set to a time earlier than the nextcheck

 * time, the nextcheck time is then set to that flush_time.

 *

 * A table is then only scanned if the current time is at least

 * the nextcheck time.

 *

 start the cleaning process */

 module must be being unloaded so its safe to kill the worker */

/* clean cache tries to find something to clean

 * and cleans it.

 * It returns 1 if it cleaned something,

 *            0 if it didn't find anything this time

 *           -1 if it fell off the end of the list.

 find a suitable table if we don't already have one */

 find a non-empty bucket in the table */

 find a cleanable entry in the bucket and clean it, or set to next bucket */

 Ok, now to clean this strand */

/*

 * We want to regularly clean the cache, so we need to schedule some work ...

/*

 * Clean all caches promptly.  This just calls cache_clean

 * repeatedly until we are sure that every cache has had a chance to

 * be fully cleaned

/*

 * Deferral and Revisiting of Requests.

 *

 * If a cache lookup finds a pending entry, we

 * need to defer the request and revisit it later.

 * All deferred requests are stored in a hash table,

 * indexed by "struct cache_head *".

 * As it may be wasteful to store a whole request

 * structure, we allow the request to provide a

 * deferred form, which must contain a

 * 'struct cache_deferred_req'

 * This cache_deferred_req contains a method to allow

 * it to be revisited when cache info is available

 ??? */

		/* The completion wasn't completed, so we need

		 * to clean up

			/* cache_revisit_request already removed

			 * this from the hash table, but hasn't

			 * called ->revisit yet.  It will very soon

			 * and we need to wait for it.

	/* Make sure we haven't exceed the limit of allowed deferred

	 * requests.

 Consider removing either the first or the last */

 Return true if and only if a deferred request is queued. */

		/* Bit could have been cleared before we managed to

		 * set up the deferral, so need to revisit just in case

/*

 * communicate with user-space

 *

 * We have a magic /proc file - /proc/net/rpc/<cachename>/channel.

 * On read, you get a full request, or block.

 * On write, an update request is processed.

 * Poll works if anything to read, and always allows write.

 *

 * Implemented by linked list of requests.  Each open file has

 * a ->private that also exists in this list.  New requests are added

 * to the end and may wakeup and preceding readers.

 * New readers are added to the head.  If, on read, an item is found with

 * CACHE_UPCALLING clear, we free it from the list.

 *

 if 0, then request */

 if non-0, we have a refcnt on next request */

	inode_lock(inode); /* protect against multiple concurrent

 need to find next request */

 need to release rq */

 32k is max userland buffer, lets check anyway */

 alway allow write */

	/* only find the length remaining in current request,

	 * or the length of the next request

 Lost a race and it is pending again */

/*

 * Support routines for text-based upcalls.

 * Fields are separated by spaces.

 * Fields are either mangled to quote space tab newline slosh with slosh

 * or a hexified with a leading \x

 * Record is terminated with newline.

 *

 This cache was never opened */

		/*

		 * We allow for the possibility that someone might

		 * restart a userspace daemon without restarting the

		 * server; but after 30 seconds, we give up.

/*

 * register an upcall request to user-space and queue it up for read() by the

 * upcall daemon.

 *

 * Each request is at most one page long.

 Too late to make an upcall */

 Lost a race, no longer PENDING, so don't enqueue */

/*

 * parse a message from user-space and pass it

 * to an appropriate cache

 * Messages are, like requests, separated into fields by

 * spaces and dequotes as \xHEXSTRING or embedded \nnn octal

 *

 * Message is

 *   reply cachename expiry key ... content....

 *

 * key and content are both parsed by cache

 return bytes copied, or -1 on error */

 HEX STRING */

 text with \nnn octal quoting */

/*

 * support /proc/net/rpc/$CACHENAME/content

 * as a seqfile.

 * We call ->cache_show passing NULL for the item to

 * get a header, then pass each real item in the cache

 cache_check does a cache_put on failure */

	/* Note that while we check that 'buf' holds a valid number,

	 * we always ignore the value and just flush everything.

	 * Making use of the number leads to races.

	/* Always flush everything, so behave like cache_purge()

	 * Do this by advancing flush_time to the current time,

	 * or by one second if it has already reached the current time.

	 * Newly added cache entries will always have ->last_refresh greater

	 * that ->flush_time, so they don't get flushed prematurely.

 for FIONREAD */

 CONFIG_PROC_FS */

 for FIONREAD */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/net/sunrpc/stats.c

 *

 * procfs-based user access to generic RPC statistics. The stats files

 * reside in /proc/net/rpc.

 *

 * The read routines assume that the buffer passed in is just big enough.

 * If you implement an RPC service that has its own stats routine which

 * appends the generic RPC stats, make sure you don't exceed the PAGE_SIZE

 * limit.

 *

 * Copyright (C) 1995, 1996, 1997 Olaf Kirch <okir@monad.swb.de>

/*

 * Get RPC client stats

/*

 * Get RPC server stats

/**

 * rpc_alloc_iostats - allocate an rpc_iostats structure

 * @clnt: RPC program, version, and xprt

 *

/**

 * rpc_free_iostats - release an rpc_iostats structure

 * @stats: doomed rpc_iostats structure

 *

/**

 * rpc_count_iostats_metrics - tally up per-task stats

 * @task: completed rpc_task

 * @op_metrics: stat structure for OP that will accumulate stats from @task

 kernel API: om_ops must never become larger than om_ntrans */

/**

 * rpc_count_iostats - tally up per-task stats

 * @task: completed rpc_task

 * @stats: array of stat structures

 *

 * Uses the statidx from @task

/*

 * Register/unregister RPC proc files

 SPDX-License-Identifier: GPL-2.0-only

/*

 * net/sunrpc/rpc_pipe.c

 *

 * Userland/kernel interface for rpcauth_gss.

 * Code shamelessly plagiarized from fs/nfsd/nfsctl.c

 * and fs/sysfs/inode.c

 *

 * Copyright (c) 2002, Trond Myklebust <trond.myklebust@fys.uio.no>

 *

/**

 * rpc_queue_upcall - queue an upcall message to userspace

 * @pipe: upcall pipe on which to queue given message

 * @msg: message to queue

 *

 * Call with an @inode created by rpc_mkpipe() to queue an upcall.

 * A userspace process may then later read the upcall by performing a

 * read on an open file for this inode.  It is up to the caller to

 * initialize the fields of @msg (other than @msg->list) appropriately.

 NOTE: it is up to the callback to update msg->copied */

/*

 * Description of fs contents.

/*

 * FIXME: This probably has races.

/**

 * rpc_mkpipe_dentry - make an rpc_pipefs file for kernel<->userspace

 *		       communication

 * @parent: dentry of directory to create new "pipe" in

 * @name: name of pipe

 * @private: private data to associate with the pipe, for the caller's use

 * @pipe: &rpc_pipe containing input parameters

 *

 * Data is made available for userspace to read by calls to

 * rpc_queue_upcall().  The actual reads will result in calls to

 * @ops->upcall, which will be called with the file pointer,

 * message, and userspace buffer to copy to.

 *

 * Writes can come at any time, and do not necessarily have to be

 * responses to upcalls.  They will result in calls to @msg->downcall.

 *

 * The @private argument passed here will be available to all these methods

 * from the file pointer, via RPC_I(file_inode(file))->private.

/**

 * rpc_unlink - remove a pipe

 * @dentry: dentry for the pipe, as returned from rpc_mkpipe

 *

 * After this call, lookups will no longer find the pipe, and any

 * attempts to read or write using preexisting opens of the pipe will

 * return -EPIPE.

/**

 * rpc_init_pipe_dir_head - initialise a struct rpc_pipe_dir_head

 * @pdh: pointer to struct rpc_pipe_dir_head

/**

 * rpc_init_pipe_dir_object - initialise a struct rpc_pipe_dir_object

 * @pdo: pointer to struct rpc_pipe_dir_object

 * @pdo_ops: pointer to const struct rpc_pipe_dir_object_ops

 * @pdo_data: pointer to caller-defined data

/**

 * rpc_add_pipe_dir_object - associate a rpc_pipe_dir_object to a directory

 * @net: pointer to struct net

 * @pdh: pointer to struct rpc_pipe_dir_head

 * @pdo: pointer to struct rpc_pipe_dir_object

 *

/**

 * rpc_remove_pipe_dir_object - remove a rpc_pipe_dir_object from a directory

 * @net: pointer to struct net

 * @pdh: pointer to struct rpc_pipe_dir_head

 * @pdo: pointer to struct rpc_pipe_dir_object

 *

/**

 * rpc_find_or_alloc_pipe_dir_object

 * @net: pointer to struct net

 * @pdh: pointer to struct rpc_pipe_dir_head

 * @match: match struct rpc_pipe_dir_object to data

 * @alloc: allocate a new struct rpc_pipe_dir_object

 * @data: user defined data for match() and alloc()

 *

/**

 * rpc_create_client_dir - Create a new rpc_client directory in rpc_pipefs

 * @dentry: the parent of new directory

 * @name: the name of new directory

 * @rpc_client: rpc client to associate with this directory

 *

 * This creates a directory at the given @path associated with

 * @rpc_clnt, which will contain a file named "info" with some basic

 * information about the client, together with any "pipes" that may

 * later be created using rpc_mkpipe().

/**

 * rpc_remove_client_dir - Remove a directory created with rpc_create_client_dir()

 * @rpc_client: rpc_client for the pipe

/*

 * populate the filesystem

/*

 * We have a single directory with 1 node in it.

/*

 * This call can be used only in RPC pipefs mount notification hooks.

/*

 * This call will be used for per network namespace operations calls.

 * Note: Function will be returned with pipefs_sb_lock taken if superblock was

 * found. This lock have to be released by rpc_put_sb_net() when all operations

 * will be completed.

/*

 * Here we present a bogus "info" file to keep rpc.gssd happy. We don't expect

 * that it will ever use this info to handle an upcall, but rpc.gssd expects

 * that this file will be there and have a certain format.

/**

 * rpc_gssd_dummy_populate - create a dummy gssd pipe

 * @root:	root of the rpc_pipefs filesystem

 * @pipe_data:	pipe data created when netns is initialized

 *

 * Create a dummy set of directories and a pipe that gssd can hold open to

 * indicate that it is up and running.

 We should never get this far if "gssd" doesn't exist */

/******************************************************************************



(c) 2007 Network Appliance, Inc.  All Rights Reserved.

(c) 2009 NetApp.  All Rights Reserved.



NetApp provides this source code under the GPL v2 License.

The GPL v2 license is available at

https://opensource.org/licenses/gpl-license.php.



THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR

CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,

EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,

PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR

PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF

LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING

NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS

SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.



/*

 * Helper routines that track the number of preallocation elements

 * on the transport.

/*

 * Free the preallocated rpc_rqst structure and the memory

 * buffers hanging off of it.

 Preallocate one XDR receive buffer */

 Pre-allocate one backchannel rpc_rqst */

 Preallocate one XDR receive buffer */

 Preallocate one XDR send buffer */

/*

 * Preallocate up to min_reqs structures and related buffers for use

 * by the backchannel.  This function can be called multiple times

 * when creating new sessions that use the same rpc_xprt.  The

 * preallocated buffers are added to the pool of resources used by

 * the rpc_xprt.  Any one of these resources may be used by an

 * incoming callback request.  It's up to the higher levels in the

 * stack to enforce that the maximum number of session slots is not

 * being exceeded.

 *

 * Some callback arguments can be large.  For example, a pNFS server

 * using multiple deviceids.  The list can be unbound, but the client

 * has the ability to tell the server the maximum size of the callback

 * requests.  Each deviceID is 16 bytes, so allocate one page

 * for the arguments to have enough room to receive a number of these

 * deviceIDs.  The NFS client indicates to the pNFS server that its

 * callback requests can be up to 4096 bytes in size.

	/*

	 * We use a temporary list to keep track of the preallocated

	 * buffers.  Once we're done building the list we splice it

	 * into the backchannel preallocation list off of the rpc_xprt

	 * struct.  This helps minimize the amount of time the list

	 * lock is held on the rpc_xprt struct.  It also makes cleanup

	 * easier in case of memory allocation errors.

 Pre-allocate one backchannel rpc_rqst */

 Add the allocated buffer to the tmp list */

	/*

	 * Add the temporary list to the backchannel preallocation list

	/*

	 * Memory allocation failed, free the temporary list

/**

 * xprt_destroy_backchannel - Destroys the backchannel preallocated structures.

 * @xprt:	the transport holding the preallocated strucures

 * @max_reqs:	the maximum number of preallocated structures to destroy

 *

 * Since these structures may have been allocated by multiple calls

 * to xprt_setup_backchannel, we only destroy up to the maximum number

 * of reqs specified by the caller.

/*

 * Return the preallocated rpc_rqst structure and XDR buffers

 * associated with this rpc_task.

	/*

	 * Return it to the list of preallocations so that it

	 * may be reused by a new callback request.

		/*

		 * The last remaining session was destroyed while this

		 * entry was in use.  Free the entry and don't attempt

		 * to add back to the list because there is no need to

		 * have anymore preallocated entries.

/*

 * One or more rpc_rqst structure have been preallocated during the

 * backchannel setup.  Buffer space for the send and private XDR buffers

 * has been preallocated as well.  Use xprt_alloc_bc_request to allocate

 * to this request.  Use xprt_free_bc_request to return it.

 *

 * We know that we're called in soft interrupt context, grab the spin_lock

 * since there is no need to grab the bottom half spin_lock.

 *

 * Return an available rpc_rqst, otherwise NULL if non are available.

/*

 * Add callback request to callback list.  The callback

 * service sleeps on the sv_cb_waitq waiting for new

 * requests.  Wake it up after adding enqueing the

 * request.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * AUTHUNIX and AUTHNULL credentials are both handled here.

 * AUTHNULL is treated just like AUTHUNIX except that the uid/gid

 * are always nobody (-2).  i.e. we do the same IP address checks for

 * AUTHNULL as for AUTHUNIX, and that is done here.

 other stuff later */

/**************************************************

 * cache for IP address to unix_domain

 * as needed by AUTH_UNIX

 e.g. "nfsd" */

 class ipaddress [domainname] */

	/* should be safe just to use the start of the input buffer

 class */

 ip address */

 Form a mapped IPv4 address in sin6 */

 domainname, or empty for NEGATIVE */

 IPv6 scope IDs are ignored for now */

 class addr domain */

				/*

				 * The entry has been invalidated since it was

				 * remembered, e.g. by a second mount from the

				 * same IP address.

 newly cached, keep the reference */

/****************************************************************************

 * auth.unix.gid cache

 * simple cache to map a UID to a list of GIDs

 * because AUTH_UNIX aka AUTH_SYS has a max of UNX_NGROUPS

 uid expiry Ngid gid0 gid1 ... gidN-1 */

 Signal that mapping to nobody uid/gid is required */

 kmalloc failure - client must retry */

 Put NULL verifier */

 don't drop */

 length */

 time stamp */

 machname length */

 skip machname */

	/*

	 * Note: we skip uid_valid()/gid_valid() checks here for

	 * backwards compatibility with clients that use -1 id's.

	 * Instead, -1 uid or gid is later mapped to the

	 * (export-specific) anonymous id by nfsd_setuser.

	 * Supplementary gid's will be left alone.

 uid */

 gid */

 gids length */

 Put NULL verifier */

	/* Verifier (such as it is) is already in place.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020 Anna Schumaker <Anna.Schumaker@Netapp.com>

	/* buf_len is the len until the first occurence of either

	 * '\n' or '\0'

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2009, Oracle.  All rights reserved.

 *

 * Convert socket addresses to presentation addresses and universal

 * addresses, and vice versa.

 *

 * Universal addresses are introduced by RFC 1833 and further refined by

 * recent RFCs describing NFSv4.  The universal address format is part

 * of the external (network) interface provided by rpcbind version 3

 * and 4, and by NFSv4.  Such an address is a string containing a

 * presentation format IP address followed by a port number in

 * "hibyte.lobyte" format.

 *

 * IPv6 addresses can also include a scope ID, typically denoted by

 * a '%' followed by a device name or a non-negative integer.  Refer to

 * RFC 4291, Section 2.2 for details on IPv6 presentation formats.

	/*

	 * RFC 4291, Section 2.2.2

	 *

	 * Shorthanded ANY address

	/*

	 * RFC 4291, Section 2.2.2

	 *

	 * Shorthanded loopback address

	/*

	 * RFC 4291, Section 2.2.3

	 *

	 * Special presentation address format for mapped v4

	 * addresses.

	/*

	 * RFC 4291, Section 2.2.1

 !IS_ENABLED(CONFIG_IPV6) */

 !IS_ENABLED(CONFIG_IPV6) */

/**

 * rpc_ntop - construct a presentation address in @buf

 * @sap: socket address

 * @buf: construction area

 * @buflen: size of @buf, in bytes

 *

 * Plants a %NUL-terminated string in @buf and returns the length

 * of the string, excluding the %NUL.  Otherwise zero is returned.

/**

 * rpc_pton - Construct a sockaddr in @sap

 * @net: applicable network namespace

 * @buf: C string containing presentation format IP address

 * @buflen: length of presentation address in bytes

 * @sap: buffer into which to plant socket address

 * @salen: size of buffer in bytes

 *

 * Returns the size of the socket address if successful; otherwise

 * zero is returned.

 *

 * Plants a socket address in @sap and returns the size of the

 * socket address, if successful.  Returns zero if an error

 * occurred.

/**

 * rpc_sockaddr2uaddr - Construct a universal address string from @sap.

 * @sap: socket address

 * @gfp_flags: allocation mode

 *

 * Returns a %NUL-terminated string in dynamically allocated memory;

 * otherwise NULL is returned if an error occurred.  Caller must

 * free the returned string.

/**

 * rpc_uaddr2sockaddr - convert a universal address to a socket address.

 * @net: applicable network namespace

 * @uaddr: C string containing universal address to convert

 * @uaddr_len: length of universal address string

 * @sap: buffer into which to plant socket address

 * @salen: size of buffer

 *

 * @uaddr does not have to be '\0'-terminated, but kstrtou8() and

 * rpc_pton() require proper string termination to be successful.

 *

 * Returns the size of the socket address if successful; otherwise

 * zero is returned.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/net/sunrpc/xdr.c

 *

 * Generic XDR support.

 *

 * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>

/*

 * XDR functions for basic NFS types

 zero trailing bytes */

/**

 * xdr_encode_opaque_fixed - Encode fixed length opaque data

 * @p: pointer to current position in XDR buffer.

 * @ptr: pointer to data to encode (or NULL)

 * @nbytes: size of data.

 *

 * Copy the array of data of length nbytes at ptr to the XDR buffer

 * at position p, then align to the next 32-bit boundary by padding

 * with zero bytes (see RFC1832).

 * Note: if ptr is NULL, only the padding is performed.

 *

 * Returns the updated current XDR buffer position

 *

/**

 * xdr_encode_opaque - Encode variable length opaque data

 * @p: pointer to current position in XDR buffer.

 * @ptr: pointer to data to encode (or NULL)

 * @nbytes: size of data.

 *

 * Returns the updated current XDR buffer position

/**

 * xdr_terminate_string - '\0'-terminate a string residing in an xdr_buf

 * @buf: XDR buffer where string resides

 * @len: length of string, in bytes

 *

/**

 * xdr_inline_pages - Prepare receive buffer for a large reply

 * @xdr: xdr_buf into which reply will be placed

 * @offset: expected offset where data payload will start, in bytes

 * @pages: vector of struct page pointers

 * @base: offset in first page where receive should start, in bytes

 * @len: expected size of the upper layer data payload, in bytes

 *

/*

 * Helper routines for doing 'memmove' like operations on a struct xdr_buf

/**

 * _shift_data_left_pages

 * @pages: vector of pages containing both the source and dest memory area.

 * @pgto_base: page vector address of destination

 * @pgfrom_base: page vector address of source

 * @len: number of bytes to copy

 *

 * Note: the addresses pgto_base and pgfrom_base are both calculated in

 *       the same way:

 *            if a memory area starts at byte 'base' in page 'pages[i]',

 *            then its address is given as (i << PAGE_CACHE_SHIFT) + base

 * Alse note: pgto_base must be < pgfrom_base, but the memory areas

 * 	they point to may overlap.

/**

 * _shift_data_right_pages

 * @pages: vector of pages containing both the source and dest memory area.

 * @pgto_base: page vector address of destination

 * @pgfrom_base: page vector address of source

 * @len: number of bytes to copy

 *

 * Note: the addresses pgto_base and pgfrom_base are both calculated in

 *       the same way:

 *            if a memory area starts at byte 'base' in page 'pages[i]',

 *            then its address is given as (i << PAGE_SHIFT) + base

 * Also note: pgfrom_base must be < pgto_base, but the memory areas

 * 	they point to may overlap.

 Are any pointers crossing a page boundary? */

/**

 * _copy_to_pages

 * @pages: array of pages

 * @pgbase: page vector address of destination

 * @p: pointer to source data

 * @len: length

 *

 * Copies data from an arbitrary memory location into an array of pages

 * The copy is assumed to be non-overlapping.

/**

 * _copy_from_pages

 * @p: pointer to destination

 * @pages: array of pages

 * @pgbase: offset of source data

 * @len: length

 *

 * Copies data into an arbitrary memory location from an array of pages

 * The copy is assumed to be non-overlapping.

/**

 * xdr_buf_pages_zero

 * @buf: xdr_buf

 * @pgbase: beginning offset

 * @len: length

 Expand the tail buffer */

 Shift data into head */

 Shift data into pages */

 Shift data into head */

/**

 * xdr_shrink_bufhead

 * @buf: xdr_buf

 * @len: new length of buf->head[0]

 *

 * Shrinks XDR buffer's header kvec buf->head[0], setting it to

 * 'len' bytes. The extra data is not lost, but is instead

 * moved into the inlined pages and/or the tail.

/**

 * xdr_shrink_pagelen - shrinks buf->pages to @len bytes

 * @buf: xdr_buf

 * @len: new page buffer length

 *

 * The extra data is not lost, but is instead moved into buf->tail.

 * Returns the actual number of bytes moved.

/**

 * xdr_stream_pos - Return the current offset from the start of the xdr_stream

 * @xdr: pointer to struct xdr_stream

/**

 * xdr_page_pos - Return the current offset from the start of the xdr pages

 * @xdr: pointer to struct xdr_stream

/**

 * xdr_init_encode - Initialize a struct xdr_stream for sending data.

 * @xdr: pointer to xdr_stream struct

 * @buf: pointer to XDR buffer in which to encode data

 * @p: current pointer inside XDR buffer

 * @rqst: pointer to controlling rpc_rqst, for debugging

 *

 * Note: at the moment the RPC client only passes the length of our

 *	 scratch buffer in the xdr_buf's header kvec. Previously this

 *	 meant we needed to call xdr_adjust_iovec() after encoding the

 *	 data. With the new scheme, the xdr_stream manages the details

 *	 of the buffer length, and takes care of adjusting the kvec

 *	 length for us.

/**

 * xdr_commit_encode - Ensure all data is written to buffer

 * @xdr: pointer to xdr_stream

 *

 * We handle encoding across page boundaries by giving the caller a

 * temporary location to write to, then later copying the data into

 * place; xdr_commit_encode does that copying.

 *

 * Normally the caller doesn't need to call this directly, as the

 * following xdr_reserve_space will do it.  But an explicit call may be

 * required at the end of encoding, or any other time when the xdr_buf

 * data might be read.

 Bigger buffers require special handling */

 Sorry, we're totally out of space */

	/*

	 * If the last encode didn't end exactly on a page boundary, the

	 * next one will straddle boundaries.  Encode into the next

	 * page, then copy it back later in xdr_commit_encode.  We use

	 * the "scratch" iov to track any temporarily unused fragment of

	 * space at the end of the previous buffer:

	/*

	 * Note this is where the next encode will start after we've

	 * shifted this one back:

/**

 * xdr_reserve_space - Reserve buffer space for sending

 * @xdr: pointer to xdr_stream

 * @nbytes: number of bytes to reserve

 *

 * Checks that we have enough buffer space to encode 'nbytes' more

 * bytes of data. If so, update the total xdr_buf length, and

 * adjust the length of the current kvec.

 align nbytes on the next 32-bit boundary */

/**

 * xdr_reserve_space_vec - Reserves a large amount of buffer space for sending

 * @xdr: pointer to xdr_stream

 * @vec: pointer to a kvec array

 * @nbytes: number of bytes to reserve

 *

 * Reserves enough buffer space to encode 'nbytes' of data and stores the

 * pointers in 'vec'. The size argument passed to xdr_reserve_space() is

 * determined based on the number of bytes remaining in the current page to

 * avoid invalidating iov_base pointers when xdr_commit_encode() is called.

	/*

	 * svcrdma requires every READ payload to start somewhere

	 * in xdr->pages.

/**

 * xdr_truncate_encode - truncate an encode buffer

 * @xdr: pointer to xdr_stream

 * @len: new length of buffer

 *

 * Truncates the xdr stream, so that xdr->buf->len == len,

 * and xdr->p points at offset len from the start of the buffer, and

 * head, tail, and page lengths are adjusted to correspond.

 *

 * If this means moving xdr->p to a different buffer, we assume that

 * the end pointer should be set to the end of the current page,

 * except in the case of the head buffer when we assume the head

 * buffer's current length represents the end of the available buffer.

 *

 * This is *not* safe to use on a buffer that already has inlined page

 * cache pages (as in a zero-copy server read reply), except for the

 * simple case of truncating from one position in the tail to another.

 *

 (otherwise assume xdr->end is already set) */

/**

 * xdr_restrict_buflen - decrease available buffer space

 * @xdr: pointer to xdr_stream

 * @newbuflen: new maximum number of bytes available

 *

 * Adjust our idea of how much space is available in the buffer.

 * If we've already used too much space in the buffer, returns -1.

 * If the available space is already smaller than newbuflen, returns 0

 * and does nothing.  Otherwise, adjusts xdr->buf->buflen to newbuflen

 * and ensures xdr->end is set at most offset newbuflen from the start

 * of the buffer.

/**

 * xdr_write_pages - Insert a list of pages into an XDR buffer for sending

 * @xdr: pointer to xdr_stream

 * @pages: list of pages

 * @base: offset of first byte

 * @len: length of data in bytes

 *

/**

 * xdr_init_decode - Initialize an xdr_stream for decoding data.

 * @xdr: pointer to xdr_stream struct

 * @buf: pointer to XDR buffer from which to decode data

 * @p: current pointer inside XDR buffer

 * @rqst: pointer to controlling rpc_rqst, for debugging

/**

 * xdr_init_decode_pages - Initialize an xdr_stream for decoding into pages

 * @xdr: pointer to xdr_stream struct

 * @buf: pointer to XDR buffer from which to decode data

 * @pages: list of pages to decode into

 * @len: length in bytes of buffer in pages

/**

 * xdr_inline_decode - Retrieve XDR data to decode

 * @xdr: pointer to xdr_stream struct

 * @nbytes: number of bytes of data to decode

 *

 * Check if the input buffer is long enough to enable us to decode

 * 'nbytes' more bytes of data starting at the current position.

 * If so return the current pointer, then update the current

 * pointer position.

 Realign pages to current pointer position */

 Truncate page data and move it into the tail */

/**

 * xdr_read_pages - align page-based XDR data to current pointer position

 * @xdr: pointer to xdr_stream struct

 * @len: number of bytes of page data

 *

 * Moves data beyond the current pointer position from the XDR head[] buffer

 * into the page list. Any data that lies beyond current position + @len

 * bytes is moved into the XDR tail[]. The xdr_stream current position is

 * then advanced past that data to align to the next XDR object in the tail.

 *

 * Returns the number of XDR encoded bytes now contained in the pages

 We only shift data left! */

 Move page data to the left */

 Could the hole be behind us? */

/**

 * xdr_enter_page - decode data from the XDR page

 * @xdr: pointer to xdr_stream struct

 * @len: number of bytes of page data

 *

 * Moves data beyond the current pointer position from the XDR head[] buffer

 * into the page list. Any data that lies beyond current position + "len"

 * bytes is moved into the XDR tail[]. The current pointer is then

 * repositioned at the beginning of the first XDR page.

	/*

	 * Position current pointer at beginning of tail, and

	 * set remaining message length.

/**

 * xdr_buf_subsegment - set subbuf to a portion of buf

 * @buf: an xdr buffer

 * @subbuf: the result buffer

 * @base: beginning of range in bytes

 * @len: length of range in bytes

 *

 * sets @subbuf to an xdr buffer representing the portion of @buf of

 * length @len starting at offset @base.

 *

 * @buf and @subbuf may be pointers to the same struct xdr_buf.

 *

 * Returns -1 if base of length are out of bounds.

/**

 * xdr_stream_subsegment - set @subbuf to a portion of @xdr

 * @xdr: an xdr_stream set up for decoding

 * @subbuf: the result buffer

 * @nbytes: length of @xdr to extract, in bytes

 *

 * Sets up @subbuf to represent a portion of @xdr. The portion

 * starts at the current offset in @xdr, and extends for a length

 * of @nbytes. If this is successful, @xdr is advanced to the next

 * XDR data item following that portion.

 *

 * Return values:

 *   %true: @subbuf has been initialized, and @xdr has been advanced.

 *   %false: a bounds error has occurred

 Extract @subbuf and bounds-check the fn arguments */

 Advance @xdr by @nbytes */

/**

 * xdr_buf_trim - lop at most "len" bytes off the end of "buf"

 * @buf: buf to be trimmed

 * @len: number of bytes to reduce "buf" by

 *

 * Trim an xdr_buf by the given number of bytes by fixing up the lengths. Note

 * that it's possible that we'll trim less than that amount if the xdr_buf is

 * too small, or if (for instance) it's all in the head and the parser has

 * already read too far into it.

 obj is assumed to point to allocated memory of size at least len: */

 obj is assumed to point to allocated memory of size at least len: */

 Returns 0 on success, or else a negative error code. */

 process head */

 align to start of pages */

 process pages array */

 align to start of tail */

 process tail */

/**

 * xdr_stream_decode_opaque - Decode variable length opaque

 * @xdr: pointer to xdr_stream

 * @ptr: location to store opaque data

 * @size: size of storage buffer @ptr

 *

 * Return values:

 *   On success, returns size of object stored in *@ptr

 *   %-EBADMSG on XDR buffer overflow

 *   %-EMSGSIZE on overflow of storage buffer @ptr

/**

 * xdr_stream_decode_opaque_dup - Decode and duplicate variable length opaque

 * @xdr: pointer to xdr_stream

 * @ptr: location to store pointer to opaque data

 * @maxlen: maximum acceptable object size

 * @gfp_flags: GFP mask to use

 *

 * Return values:

 *   On success, returns size of object stored in *@ptr

 *   %-EBADMSG on XDR buffer overflow

 *   %-EMSGSIZE if the size of the object would exceed @maxlen

 *   %-ENOMEM on memory allocation failure

/**

 * xdr_stream_decode_string - Decode variable length string

 * @xdr: pointer to xdr_stream

 * @str: location to store string

 * @size: size of storage buffer @str

 *

 * Return values:

 *   On success, returns length of NUL-terminated string stored in *@str

 *   %-EBADMSG on XDR buffer overflow

 *   %-EMSGSIZE on overflow of storage buffer @str

/**

 * xdr_stream_decode_string_dup - Decode and duplicate variable length string

 * @xdr: pointer to xdr_stream

 * @str: location to store pointer to string

 * @maxlen: maximum acceptable string length

 * @gfp_flags: GFP mask to use

 *

 * Return values:

 *   On success, returns length of NUL-terminated string stored in *@ptr

 *   %-EBADMSG on XDR buffer overflow

 *   %-EMSGSIZE if the size of the string would exceed @maxlen

 *   %-ENOMEM on memory allocation failure

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/net/sunrpc/socklib.c

 *

 * Common socket helper routines for RPC client and server

 *

 * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>

/*

 * Helper structure for copying from an sk_buff.

/**

 * xdr_skb_read_bits - copy some data bits from skb to internal buffer

 * @desc: sk_buff copy helper

 * @to: copy destination

 * @len: number of bytes to copy

 *

 * Possibly called several times to iterate over an sk_buff and copy

 * data out of it.

/**

 * xdr_skb_read_and_csum_bits - copy and checksum from skb to buffer

 * @desc: sk_buff copy helper

 * @to: copy destination

 * @len: number of bytes to copy

 *

 * Same as skb_read_bits, but calculate a checksum at the same time.

/**

 * xdr_partial_copy_from_skb - copy data out of an skb

 * @xdr: target XDR buffer

 * @base: starting offset

 * @desc: sk_buff copy helper

 * @copy_actor: virtual method for copying data

 *

		/* ACL likes to be lazy in allocating pages - ACLs

/**

 * csum_partial_copy_to_xdr - checksum and copy data

 * @xdr: target XDR buffer

 * @skb: source skb

 *

 * We have set things up such that we perform the checksum of the UDP

 * packet in parallel with the copies into the RPC client iovec.  -DaveM

/* Common case:

 *  - stream transport

 *  - sending from byte 0 of the message

 *  - the message is wholly contained in @xdr's head iovec

/**

 * xprt_sock_sendmsg - write an xdr_buf directly to a socket

 * @sock: open socket to send on

 * @msg: socket message metadata

 * @xdr: xdr_buf containing this request

 * @base: starting position in the buffer

 * @marker: stream record marker field

 * @sent_p: return the total number of bytes successfully queued for sending

 *

 * Return values:

 *   On success, returns zero and fills in @sent_p.

 *   %-ENOTSOCK if  @sock is not a struct socket.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/net/sunrpc/svc_xprt.c

 *

 * Author: Tom Tucker <tom@opengridcomputing.com>

/* apparently the "standard" is that clients close

 * idle connections after 5 minutes, servers after

 * 6 minutes

 *   http://nfsv4bat.org/Documents/ConnectAThon/1996/nfstcp.pdf

 List of registered transport classes */

/* SMP locking strategy:

 *

 *	svc_pool->sp_lock protects most of the fields of that pool.

 *	svc_serv->sv_lock protects sv_tempsocks, sv_permsocks, sv_tmpcnt.

 *	when both need to be taken (rare), svc_serv->sv_lock is first.

 *	The "service mutex" protects svc_serv->sv_nrthread.

 *	svc_sock->sk_lock protects the svc_sock->sk_deferred list

 *             and the ->sk_info_authunix cache.

 *

 *	The XPT_BUSY bit in xprt->xpt_flags prevents a transport being

 *	enqueued multiply. During normal transport processing this bit

 *	is set by svc_xprt_enqueue and cleared by svc_xprt_received.

 *	Providers should not manipulate this bit directly.

 *

 *	Some flags can be set to certain values at any time

 *	providing that certain rules are followed:

 *

 *	XPT_CONN, XPT_DATA:

 *		- Can be set or cleared at any time.

 *		- After a set, svc_xprt_enqueue must be called to enqueue

 *		  the transport for processing.

 *		- After a clear, the transport must be read/accepted.

 *		  If this succeeds, it must be set again.

 *	XPT_CLOSE:

 *		- Can set at any time. It is never cleared.

 *      XPT_DEAD:

 *		- Can only be set while XPT_BUSY is held which ensures

 *		  that no other thread will be using the transport or will

 *		  try to set XPT_DEAD.

 Make sure there isn't already a class with the same name */

/**

 * svc_print_xprts - Format the transport list for printing

 * @buf: target buffer for formatted address

 * @maxlen: length of target buffer

 *

 * Fills in @buf with a string containing a list of transport names, each name

 * terminated with '\n'. If the buffer is too small, some entries may be

 * missing, but it is guaranteed that all lines in the output buffer are

 * complete.

 *

 * Returns positive length of the filled-in string.

/**

 * svc_xprt_deferred_close - Close a transport

 * @xprt: transport instance

 *

 * Used in contexts that need to defer the work of shutting down

 * the transport to an nfsd thread.

 See comment on corresponding get in xs_setup_bc_tcp(): */

/*

 * Called by transport drivers to initialize the transport independent

 * portion of the transport instance.

/**

 * svc_xprt_received - start next receiver thread

 * @xprt: controlling transport

 *

 * The caller must hold the XPT_BUSY bit and must

 * not thereafter touch transport data.

 *

 * Note: XPT_DATA only gets cleared when a read-attempt finds no (or

 * insufficient) data.

	/* As soon as we clear busy, the xprt could be closed and

	 * 'put', so we need a reference to call svc_enqueue_xprt with:

	/* This errno is exposed to user space.  Provide a reasonable

/*

 * Copy the local and remote xprt addresses to the rqstp structure

	/*

	 * Destination address in request is needed for binding the

	 * source address in RPC replies/callbacks later.

/**

 * svc_print_addr - Format rq_addr field for printing

 * @rqstp: svc_rqst struct containing address to print

 * @buf: target buffer for formatted address

 * @len: length of target buffer

 *

 See smp_rmb() in svc_xprt_ready() */

	/*

	 * If another cpu has recently updated xpt_flags,

	 * sk_sock->flags, xpt_reserved, or xpt_nr_rqsts, we need to

	 * know about it; otherwise it's possible that both that cpu and

	 * this one could call svc_xprt_enqueue() without either

	 * svc_xprt_enqueue() recognizing that the conditions below

	 * are satisfied, and we could stall indefinitely:

	/* Mark transport as busy. It will remain in this state until

	 * the provider calls svc_xprt_received. We update XPT_BUSY

	 * atomically because it also guards against trying to enqueue

	 * the transport twice.

 find a thread for this xprt */

/*

 * Queue up a transport with data pending. If there are idle nfsd

 * processes, wake 'em up.

 *

/*

 * Dequeue the first transport, if there is one.

/**

 * svc_reserve - change the space reserved for the reply to a request.

 * @rqstp:  The request in question

 * @space: new max space to reserve

 *

 * Each request reserves some space on the output queue of the transport

 * to make sure the reply fits.  This function reduces that reserved

 * space to be the amount of space used already, plus @space.

 *

 See smp_rmb() in svc_xprt_ready() */

	/* Reset response buffer and release

	 * the reservation.

	 * But first, check that enough space was reserved

	 * for the reply, otherwise we have a bug!

/*

 * Some svc_serv's will have occasional work to do, even when a xprt is not

 * waiting to be serviced. This function is there to "kick" a task in one of

 * those services so that it can wake up and do that work. Note that we only

 * bother with pool 0 as we don't need to wake up more than one thread for

 * this purpose.

 skip any that aren't queued */

 No free entries available */

/*

 * Make sure that we don't have too many active connections. If we have,

 * something must be dropped. It's not clear what will happen if we allow

 * "too many" connections, but when dealing with network-facing software,

 * we have to code defensively. Here we do that by imposing hard limits.

 *

 * There's no point in trying to do random drop here for DoS

 * prevention. The NFS clients does 1 reconnect in 15 seconds. An

 * attacker can easily beat that.

 *

 * The only somewhat efficient mechanism would be if drop old

 * connections from the same IP first. But right now we don't even

 * record the client IP in svc_sock.

 *

 * single-threaded services that expect a lot of clients will probably

 * need to set sv_maxconn to override the default value which is based

 * on the number of threads

 Try to help the admin */

			/*

			 * Always select the oldest connection. It's not fair,

			 * but so is life

 use as many pages as possible */

 Made progress, don't sleep yet */

 this might be seen in nfsd_splice_actor() */

 Make arg->head point to first page and arg->pages point to rest */

 save at least one page for response */

 did someone call svc_wake_up? */

 was a socket queued? */

 are we shutting down? */

 are we freezing? */

 rq_xprt should be clear on entry */

	/*

	 * We have to be able to interrupt this wait

	 * to bring down the daemons ...

	/* Normally we will wait up to 5 seconds for any required

	 * cache information to be provided.

 setup timer to age temp transports */

 Leave XPT_BUSY set on the dead xprt: */

		/*

		 * We know this module_get will succeed because the

		 * listener holds a reference too

 XPT_DATA|XPT_DEFERRED case: */

/*

 * Receive the next request on any transport.  This code is carefully

 * organised not to touch any cachelines in the shared svc_serv

 * structure, only cachelines in the local svc_pool.

 No data, incomplete (TCP) read, or accept() */

/*

 * Drop request

/*

 * Return reply to client.

 calculate over-all length */

/*

 * Timer function to close old temporary transports, using

 * a mark-and-sweep algorithm.

 busy, try again 1 sec later */

		/* First time through, just mark it OLD. Second time

 a thread will dequeue and close it soon */

/* Close temporary transports whose xpt_local matches server_addr immediately

 * instead of waiting for them to be picked up by the timer.

 *

 * This is meant to be called from a notifier_block that runs when an ip

 * address is deleted.

/*

 * Remove a dead transport

 someone else will have to effect the close */

	/*

	 * We expect svc_close_xprt() to work even when no threads are

	 * running (e.g., while configuring the server before starting

	 * any threads), so if the transport isn't busy, we delete

	 * it ourself:

/*

 * Server threads may still be running (especially in the case where the

 * service is still running in other network namespaces).

 *

 * So we shut down sockets the same way we would on a running server, by

 * setting XPT_CLOSE, enqueuing, and letting a thread pick it up to do

 * the close.  In the case there are no such other threads,

 * threads running, svc_clean_up_xprts() does a simple version of a

 * server's main event loop, and in the case where there are other

 * threads, we may need to wait a little while and then check again to

 * see if they're done.

/*

 * Handle defer and revisit of requests

/*

 * Save the request off for later processing. The request buffer looks

 * like this:

 *

 * <xprt-header><rpc-header><rpc-pagelist><rpc-tail>

 *

 * This code can only handle requests that consist of an xprt-header

 * and rpc-header.

 if more than a page, give up FIXME */

 FIXME maybe discard if size too large */

 back up head to the start of the buffer and copy */

/*

 * recv data from a deferred request into an active one

 setup iov_base past transport header */

 The iov_len does not include the transport header bytes */

 The rq_arg.len includes the transport header bytes */

 Save off transport header len in case we get deferred again */

/**

 * svc_find_xprt - find an RPC transport instance

 * @serv: pointer to svc_serv to search

 * @xcl_name: C string containing transport's class name

 * @net: owner net pointer

 * @af: Address family of transport's local address

 * @port: transport's IP port number

 *

 * Return the transport instance pointer for the endpoint accepting

 * connections/peer traffic from the specified transport class,

 * address family and port.

 *

 * Specifying 0 for the address family or port is effectively a

 * wild-card, and will result in matching the first transport in the

 * service's list that has a matching class name.

 Sanity check the args */

/**

 * svc_xprt_names - format a buffer with a list of transport names

 * @serv: pointer to an RPC service

 * @buf: pointer to a buffer to be filled in

 * @buflen: length of buffer to be filled in

 *

 * Fills in @buf with a string containing a list of transport names,

 * each name terminated with '\n'.

 *

 * Returns positive length of the filled-in string on success; otherwise

 * a negative errno value is returned if an error occurs.

 Sanity check args */

----------------------------------------------------------------------------*/

----------------------------------------------------------------------------*/

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/net/sunrpc/svcsock.c

 *

 * These are the RPC server socket internals.

 *

 * The server scheduling algorithm does not always distribute the load

 * evenly when servicing a single client. May need to modify the

 * svc_xprt_enqueue procedure...

 *

 * TCP support is largely untested and may be a little slow. The problem

 * is that we currently do two separate recvfrom's, one for the 4-byte

 * record length, and the second for the actual record. This could possibly

 * be improved by always reading a minimum size of around 100 bytes and

 * tucking any superfluous bytes away in a temporary store. Still, that

 * leaves write requests out in the rain. An alternative may be to peek at

 * the first skb in the queue, and if it matches the next TCP sequence

 * number, to extract the record marker. Yuck.

 *

 * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>

/**

 * svc_tcp_release_rqst - Release transport-related resources

 * @rqstp: request structure with resources to be released

 *

/**

 * svc_udp_release_rqst - Release transport-related resources

 * @rqstp: request structure with resources to be released

 *

/*

 * Report socket names for nfsdfs

/*

 * Read from @rqstp's transport socket. The incoming message fills whole

 * pages in @rqstp's rq_pages array until the last page of the message

 * has been received into a partial page.

	/* If we read a full record, then assume there may be more

	 * data to read (stream based sockets only!)

/*

 * Set socket snd and rcv buffer lengths

/*

 * INET callback when data has been received on the socket.

 Refer to svc_setup_socket() for details. */

/*

 * INET callback when space is newly available on the socket.

 Refer to svc_setup_socket() for details. */

/*

 * See net/ipv6/ip_sockglue.c : ip_cmsg_recv_pktinfo

/*

 * See net/ipv6/datagram.c : ip6_datagram_recv_ctl

/*

 * Copy the UDP datagram's destination address to the rqstp structure.

 * The 'destination' address in this case is the address to which the

 * peer sent the datagram, i.e. our local address. For multihomed

 * hosts, this can change from msg to msg. Note that only the IP

 * address changes, the port number should remain the same.

/**

 * svc_udp_recvfrom - Receive a datagram from a UDP socket.

 * @rqstp: request structure into which to receive an RPC Call

 *

 * Called in a loop when XPT_DATA has been set.

 *

 * Returns:

 *   On success, the number of bytes in a received RPC Call, or

 *   %0 if a complete RPC Call message was not ready to return

	    /* udp sockets need large rcvbuf as all pending

	     * requests are still in that buffer.  sndbuf must

	     * also be large enough that there is enough space

	     * for one reply per thread.  We count all threads

	     * rather than threads in a particular pool, which

	     * provides an upper bound on the number of threads

	     * which will access the socket.

		/* Don't enable netstamp, sunrpc doesn't

 there may be more data... */

 we have to copy */

 we can use it in-place */

 possibly an icmp error */

/**

 * svc_udp_sendto - Send out a reply on a UDP socket

 * @rqstp: completed svc_rqst

 *

 * xpt_mutex ensures @rqstp's whole message is written to the socket

 * without interruption.

 *

 * Returns the number of bytes sent, or a negative errno.

 ICMP error on earlier request. */

	/*

	 * Set the SOCK_NOSPACE flag before checking the available

	 * sock space.

	/* initialise setting must have enough space to

	 * receive and respond to one request.

	 * svc_udp_recvfrom will re-adjust if necessary

 data might have come in before data_ready set up */

 make sure we get destination address info */

/*

 * A data_ready event on a listening socket means there's a connection

 * pending. Do not use state_change as a substitute for it.

 Refer to svc_setup_socket() for details. */

	/*

	 * This callback may called twice when a new connection

	 * is established as a child socket inherits everything

	 * from a parent LISTEN socket.

	 * 1) data_ready method of the parent socket will be called

	 *    when one of child sockets become ESTABLISHED.

	 * 2) data_ready method of the child socket may be called

	 *    when it receives data before the socket is accepted.

	 * In case of 2, we should ignore it silently.

/*

 * A state change on a connected socket means it's dying or dead.

 Refer to svc_setup_socket() for details. */

/*

 * Accept a TCP connection

 aborted connection or whatever */

 Reset the inherited callbacks before calling svc_setup_socket */

	/* make sure that a write doesn't block forever when

	 * low on memory

/*

 * Receive fragment record header into sk_marker.

	/* If we haven't gotten the record length yet,

	 * get the next four bytes.

 call again to read the remaining bytes */

	/*

	 * XXX!: cheating for now!  Only copying HEAD.

	 * But we know this is good enough for now (in fact, for any

	 * callback reply in the forseeable future).

 whatever; just giving up. */

 If we have more data, signal svc_xprt_enqueue() to try again */

/**

 * svc_tcp_recvfrom - Receive data from a TCP socket

 * @rqstp: request structure into which to receive an RPC Call

 *

 * Called in a loop when XPT_DATA has been set.

 *

 * Read the 4-byte stream record marker, then use the record length

 * in that marker to set up exactly the resources needed to receive

 * the next RPC message into @rqstp.

 *

 * Returns:

 *   On success, the number of bytes in a received RPC Call, or

 *   %0 if a complete RPC Call message was not ready to return

 *

 * The zero return case handles partial receives and callback Replies.

 * The state of a partial receive is preserved in the svc_sock for

 * the next call to svc_tcp_recvfrom.

 Reset TCP read info */

 record not complete */

/*

 * kernel_sendpage() is used exclusively to reduce the number of

 * copy operations in this path. Therefore the caller must ensure

 * that the pages backing @xdr are unchanging.

 *

 * In addition, the logic assumes that * .bv_len is never larger

 * than PAGE_SIZE.

/**

 * svc_tcp_sendto - Send out a reply on a TCP socket

 * @rqstp: completed svc_rqst

 *

 * xpt_mutex ensures @rqstp's whole message is written to the socket

 * without interruption.

 *

 * Returns the number of bytes sent, or a negative errno.

	/*

	 * The number of server threads has changed. Update

	 * rcvbuf and sndbuf accordingly on all sockets

/*

 * Initialize socket for RPC use and create svc_sock struct

 Register socket with portmapper */

	/*

	 * This barrier is necessary in order to prevent race condition

	 * with svc_data_ready(), svc_listen_data_ready() and others

	 * when calling callbacks above.

 Initialize the socket */

/**

 * svc_addsock - add a listener socket to an RPC service

 * @serv: pointer to RPC service to which to add a new listener

 * @fd: file descriptor of the new listener

 * @name_return: pointer to buffer to fill in with name of listener

 * @len: size of the buffer

 * @cred: credential

 *

 * Fills in socket name and returns positive length of name if successful.

 * Name is terminated with '\n'.  On error, returns a negative errno

 * value.

/*

 * Create socket for RPC service.

	/*

	 * If this is an PF_INET6 listener, we want to avoid

	 * getting requests from IPv4 remotes.  Those should

	 * be shunted to a PF_INET listener via rpcbind.

 allow address reuse */

/*

 * Detach the svc_sock from the socket so that no

 * more callbacks occur.

 put back the old socket callbacks */

/*

 * Disconnect the socket, and reset the callbacks

/*

 * Free the svc_sock's socket resources and the svc_sock itself.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/net/sunrpc/auth_unix.c

 *

 * UNIX-style authentication; no AUTH_SHORT support

 *

 * Copyright (C) 1996, Olaf Kirch <okir@monad.swb.de>

/*

 * Lookup AUTH_UNIX creds for current process

/*

 * Match credentials against current the auth_cred.

/*

 * Marshal credentials.

 * Maybe we should keep a cached credential for performance reasons.

 Credential */

 stamp */

 Verifier */

/*

 * Refresh credentials. This is a no-op for AUTH_UNIX

/*

 *  linux/net/sunrpc/gss_krb5_seqnum.c

 *

 *  Adapted from MIT Kerberos 5-1.2.1 lib/gssapi/krb5/util_seqnum.c

 *

 *  Copyright (c) 2000 The Regents of the University of Michigan.

 *  All rights reserved.

 *

 *  Andy Adamson   <andros@umich.edu>

/*

 * Copyright 1993 by OpenVision Technologies, Inc.

 *

 * Permission to use, copy, modify, distribute, and sell this software

 * and its documentation for any purpose is hereby granted without fee,

 * provided that the above copyright notice appears in all copies and

 * that both that copyright notice and this permission notice appear in

 * supporting documentation, and that the name of OpenVision not be used

 * in advertising or publicity pertaining to distribution of the software

 * without specific, written prior permission. OpenVision makes no

 * representations about the suitability of this software for any

 * purpose.  It is provided "as is" without express or implied warranty.

 *

 * OPENVISION DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,

 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO

 * EVENT SHALL OPENVISION BE LIABLE FOR ANY SPECIAL, INDIRECT OR

 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF

 * USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR

 * OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR

 * PERFORMANCE OF THIS SOFTWARE.

 SPDX-License-Identifier: GPL-2.0+

/*

 *  linux/net/sunrpc/gss_rpc_upcall.c

 *

 *  Copyright (C) 2012 Simo Sorce <simo@redhat.com>

/*

 * Encoding/Decoding functions

 Unused */

/*

 * Common transport functions

		/*

		 * Note we want connection to be done in the caller's

		 * filesystem namespace.  We therefore turn off the idle

		 * timeout, which would result in reconnections being

		 * done without the correct namespace:

 terminate and remove realm part */

 change service-hostname delimiter */

 not a service principal */

/*

 * Public functions

 numbers somewhat arbitrary but large enough for current needs */

		/*

		 * pass in the max length we expect for each of these

		 * buffers but let the xdr code kmalloc them:

 FIXME ? */

	/* we need to fetch all data even in case of error so

		/* Currently we only decode CREDS_VALUE, if we add

		 * anything else we'll have to loop and match on the

 steal group info from struct svc_cred */

 whether we use it or not, free data */

 convert to GSS_NT_HOSTBASED_SERVICE form and set into creds */

/*

 * Initialization stuff

 SPDX-License-Identifier: BSD-3-Clause

/*

 * linux/net/sunrpc/auth_gss/auth_gss.c

 *

 * RPCSEC_GSS client authentication.

 *

 *  Copyright (c) 2000 The Regents of the University of Michigan.

 *  All rights reserved.

 *

 *  Dug Song       <dugsong@monkey.org>

 *  Andy Adamson   <andros@umich.edu>

/* length of a krb5 verifier (48), plus data added before arguments when

	/*

	 * There are two upcall pipes; dentry[1], named "gssd", is used

	 * for the new text-based upcall; dentry[0] is named after the

	 * mechanism (for example, "krb5") and exists for

	 * backwards-compatibility with older gssd's.

 pipe_version >= 0 if and only if someone has a pipe open. */

/* gss_cred_set_ctx:

 * called by gss_upcall_callback and gss_create_upcall in order

 * to set the gss context. The actual exchange of an old context

 * and a new one is protected by the pipe->lock.

 NetApp 6.4R1 doesn't accept seq. no. 0 */

	/* First unsigned int gives the remaining lifetime in seconds of the

	 * credential - e.g. the remaining TGT lifetime for Kerberos or

	 * the -t value passed to GSSD.

	/* Sequence number window. Determines the maximum number of

	 * simultaneous requests

 gssd signals an error by passing ctx->gc_win = 0: */

		/*

		 * in which case, p points to an error code. Anything other

		 * than -EKEYEXPIRED gets converted to -EACCES.

 copy the opaque wire context */

 import the opaque security context */

 is there any trailing data? */

 pull in acceptor name (if there is one) */

/* XXX: Need some documentation about why UPCALL_BUF_LEN is so small.

 *	Is user space expecting no more than UPCALL_BUF_LEN bytes?

 *	Note that there are now _two_ NI_MAXHOST sized data items

 *	being passed in this string.

/* Try to add an upcall to the pipefs queue.

 * If an upcall owned by our uid already exists, then we return a reference

 * to that upcall instead of adding the new upcall.

	/*

	 * target= is a full service principal that names the remote

	 * identity that we are authenticating to.

	/*

	 * gssd uses service= and srchost= to select a matching key from

	 * the system's keytab to use as the source principal.

	 *

	 * service= is the service name part of the source principal,

	 * or "*" (meaning choose any).

	 *

	 * srchost= is the hostname part of the source principal. When

	 * not provided, gssd uses the local hostname.

		/* XXX: warning on the first, under the assumption we

 gss_upcall_callback will release the reference to gss_upcall_msg */

 if gssd is down, just skip upcalling altogether */

 Find a matching upcall */

 First open of any gss pipe determines the version: */

 Trying to open a pipe of a different version */

/*

 * NOTE: we have the opportunity to use different

 * parameters based on the input flavor (which must be a pseudoflavor)

 XXX? */

	/*

	 * Note: if we created the old pipe first, then someone who

	 * examined the directory at the right moment might conclude

	 * that we supported only the old pipe.  So we instead create

	 * the new pipe first.

/*

 * Auths may be shared between rpc clients that were cloned from a

 * common client with the same xprt, if they also share the flavor and

 * target_name.

 *

 * The auth is looked up from the oldest parent sharing the same

 * cl_xprt, and the auth itself references only that common parent

 * (which is guaranteed to last as long as any of its descendants).

 Find the original parent for this transport */

 Make a copy of the cred so that we can reference count it */

/*

 * gss_send_destroy_context will cause the RPCSEC_GSS to send a NULL RPC call

 * to the server with the GSS control procedure field set to

 * RPC_GSS_PROC_DESTROY. This should normally cause the server to release

 * all RPCSEC_GSS state associated with that context.

/* gss_destroy_cred (and gss_free_ctx) are used to clean up after failure

 * to create a new cred or context, so they check that things have been

/*

 * Lookup RPCSEC_GSS cred for the current process

	/*

	 * Note: in order to force a call to call_refresh(), we deliberately

	 * fail to flag the credential as RPCAUTH_CRED_UPTODATE.

 no point if there's no string */

 did the ctx disappear or was it replaced by one with no acceptor? */

	/*

	 * Did we find a new acceptor that's longer than the original? Allocate

	 * a longer buffer and try again.

/*

 * Returns -EACCES if GSS context is NULL or will expire within the

 * timeout (miliseconds)

 Don't match with creds that have expired. */

/*

 * Marshal credentials.

 *

 * The expensive part is computing the verifier. We can't cache a

 * pre-computed version of the verifier because the seqno, which

 * is different every time, is included in the MIC.

 Credential */

 Verifier */

	/* We compute the checksum for the verifier over the xdr-encoded bytes

/*

* Refresh credentials. XXX - finish

 Dummy refresh routine: used only when destroying the context */

	/* We leave it to unwrap to calculate au_rslack. For now we just

 Check that the trailing MIC fit in the buffer, after the fact */

	/*

	 * Move the tail into its own page, in case gss_wrap needs

	 * more space in the head when wrapping.

	 *

	 * Still... Why can't gss_wrap just slide the tail down?

 slack space should prevent this ever happening: */

	/* We're assuming that when GSS_S_CONTEXT_EXPIRED, the encryption was

 guess whether the pad goes into the head or the tail: */

		/* The spec seems a little ambiguous here, but I think that not

		 * wrapping context destruction requests makes the most sense.

/**

 * gss_update_rslack - Possibly update RPC receive buffer size estimates

 * @task: rpc_task for incoming RPC Reply being unwrapped

 * @cred: controlling rpc_cred for @task

 * @before: XDR words needed before each RPC Reply message

 * @after: XDR words needed following each RPC Reply message

 *

/*

 * RFC 2203, Section 5.3.2.2

 *

 *	struct rpc_gss_integ_data {

 *		opaque databody_integ<>;

 *		opaque checksum<>;

 *	};

 *

 *	struct rpc_gss_data_t {

 *		unsigned int seq_num;

 *		proc_req_arg_t arg;

 *	};

 opaque databody_integ<>; */

	/*

	 * The xdr_stream now points to the beginning of the

	 * upper layer payload, to be passed below to

	 * rpcauth_unwrap_resp_decode(). The checksum, which

	 * follows the upper layer payload in @rcv_buf, is

	 * located and parsed without updating the xdr_stream.

 opaque checksum<>; */

 gss_unwrap decrypted the sequence number */

	/* gss_unwrap redacts the opaque blob from the head iovec.

	 * rcv_buf has changed, thus the stream needs to be reset.

/*

 * Initialize RPCSEC_GSS module

 Wait for completion of call_rcu()'s */

/*

 *  linux/net/sunrpc/gss_krb5_seal.c

 *

 *  Adapted from MIT Kerberos 5-1.2.1 lib/gssapi/krb5/k5seal.c

 *

 *  Copyright (c) 2000-2008 The Regents of the University of Michigan.

 *  All rights reserved.

 *

 *  Andy Adamson	<andros@umich.edu>

 *  J. Bruce Fields	<bfields@umich.edu>

/*

 * Copyright 1993 by OpenVision Technologies, Inc.

 *

 * Permission to use, copy, modify, distribute, and sell this software

 * and its documentation for any purpose is hereby granted without fee,

 * provided that the above copyright notice appears in all copies and

 * that both that copyright notice and this permission notice appear in

 * supporting documentation, and that the name of OpenVision not be used

 * in advertising or publicity pertaining to distribution of the software

 * without specific, written prior permission. OpenVision makes no

 * representations about the suitability of this software for any

 * purpose.  It is provided "as is" without express or implied warranty.

 *

 * OPENVISION DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,

 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO

 * EVENT SHALL OPENVISION BE LIABLE FOR ANY SPECIAL, INDIRECT OR

 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF

 * USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR

 * OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR

 * PERFORMANCE OF THIS SOFTWARE.

/*

 * Copyright (C) 1998 by the FundsXpress, INC.

 *

 * All rights reserved.

 *

 * Export of this software from the United States of America may require

 * a specific license from the United States Government.  It is the

 * responsibility of any person or organization contemplating export to

 * obtain such a license before exporting.

 *

 * WITHIN THAT CONSTRAINT, permission to use, copy, modify, and

 * distribute this software and its documentation for any purpose and

 * without fee is hereby granted, provided that the above copyright

 * notice appear in all copies and that both that copyright notice and

 * this permission notice appear in supporting documentation, and that

 * the name of FundsXpress. not be used in advertising or publicity pertaining

 * to distribution of the software without specific, written prior

 * permission.  FundsXpress makes no representations about the suitability of

 * this software for any purpose.  It is provided "as is" without express

 * or implied warranty.

 *

 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND WITHOUT ANY EXPRESS OR

 * IMPLIED WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED

 * WARRANTIES OF MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE.

 ptr now at start of header described in rfc 1964, section 1.2.1: */

	/*

	 * signalg is stored as if it were converted from LE to host endian, even

	 * though it's an opaque pair of bytes according to the RFC.

	/* Per rfc 4121, sec 4.2.6.1, there is no header,

	/* Set up the sequence number. Now 64-bits in clear

/*

 *  linux/net/sunrpc/gss_generic_token.c

 *

 *  Adapted from MIT Kerberos 5-1.2.1 lib/gssapi/generic/util_token.c

 *

 *  Copyright (c) 2000 The Regents of the University of Michigan.

 *  All rights reserved.

 *

 *  Andy Adamson   <andros@umich.edu>

/*

 * Copyright 1993 by OpenVision Technologies, Inc.

 *

 * Permission to use, copy, modify, distribute, and sell this software

 * and its documentation for any purpose is hereby granted without fee,

 * provided that the above copyright notice appears in all copies and

 * that both that copyright notice and this permission notice appear in

 * supporting documentation, and that the name of OpenVision not be used

 * in advertising or publicity pertaining to distribution of the software

 * without specific, written prior permission. OpenVision makes no

 * representations about the suitability of this software for any

 * purpose.  It is provided "as is" without express or implied warranty.

 *

 * OPENVISION DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,

 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO

 * EVENT SHALL OPENVISION BE LIABLE FOR ANY SPECIAL, INDIRECT OR

 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF

 * USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR

 * OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR

 * PERFORMANCE OF THIS SOFTWARE.

 TWRITE_STR from gssapiP_generic.h */

/* XXXX this code currently makes the assumption that a mech oid will

   never be longer than 127 bytes.  This assumption is not inherent in

   the interfaces, so the code can be fixed if the OSI namespace

/* Each token looks like this:



0x60				tag for APPLICATION 0, SEQUENCE

					(constructed, definite-length)

	<length>		possible multiple bytes, need to parse/generate

	0x06			tag for OBJECT IDENTIFIER

		<moid_length>	compile-time constant string (assume 1 byte)

		<moid_bytes>	compile-time constant string

	<inner_bytes>		the ANY containing the application token

					bytes 0,1 are the token type

					bytes 2,n are the token data



For the purposes of this abstraction, the token "header" consists of

the sequence tag and length octets, the mech OID DER encoding, and the

first two inner bytes, which indicate the token type.  The token

"body" consists of everything else.



/* returns decoded length, or < 0 on failure.  Advances buf and

 returns the length of a token, given the mech oid and the body size */

 set body_size to sequence contents size */

 NEED overflow check */

/* fills in a buffer with the token header.  The buffer is assumed to

/*

 * Given a buffer containing a token, reads and verifies the token,

 * leaving buf advanced past the token header, and setting body_size

 * to the number of remaining bytes.  Returns 0 on success,

 * G_BAD_TOK_HEADER for a variety of errors, and G_WRONG_MECH if the

 * mechanism in the token does not match the mech argument.  buf and

 * *body_size are left unmodified on error.

   /* G_WRONG_MECH is not returned immediately because it's more important

 SPDX-License-Identifier: BSD-3-Clause

/*

 *  linux/net/sunrpc/gss_mech_switch.c

 *

 *  Copyright (c) 2001 The Regents of the University of Michigan.

 *  All rights reserved.

 *

 *  J. Bruce Fields   <bfields@umich.edu>

/**

 * gss_mech_register - register a GSS mechanism

 * @gm: GSS mechanism handle

 *

 * Returns zero if successful, or a negative errno.

/**

 * gss_mech_unregister - release a GSS mechanism

 * @gm: GSS mechanism handle

 *

/**

 * gss_svc_to_pseudoflavor - map a GSS service number to a pseudoflavor

 * @gm: GSS mechanism handle

 * @qop: GSS quality-of-protection value

 * @service: GSS service value

 *

 * Returns a matching security flavor, or RPC_AUTH_MAXFLAVOR if none is found.

/**

 * gss_mech_info2flavor - look up a pseudoflavor given a GSS tuple

 * @info: a GSS mech OID, quality of protection, and service value

 *

 * Returns a matching pseudoflavor, or RPC_AUTH_MAXFLAVOR if the tuple is

 * not supported.

/**

 * gss_mech_flavor2info - look up a GSS tuple for a given pseudoflavor

 * @pseudoflavor: GSS pseudoflavor to match

 * @info: rpcsec_gss_info structure to fill in

 *

 * Returns zero and fills in "info" if pseudoflavor matches a

 * supported mechanism.  Otherwise a negative errno is returned.

/* The mech could probably be determined from the token instead, but it's just

 gss_get_mic: compute a mic over message and return mic_token. */

 gss_verify_mic: check whether the provided mic_token verifies message. */

/*

 * This function is called from both the client and server code.

 * Each makes guarantees about how much "slack" space is available

 * for the underlying function in "buf"'s head and tail while

 * performing the wrap.

 *

 * The client and server code allocate RPC_MAX_AUTH_SIZE extra

 * space in both the head and tail which is available for use by

 * the wrap function.

 *

 * Underlying functions should verify they do not use more than

 * RPC_MAX_AUTH_SIZE of extra space in either the head or tail

 * when performing the wrap.

/* gss_delete_sec_context: free all resources associated with context_handle.

 * Note this differs from the RFC 2744-specified prototype in that we don't

 SPDX-License-Identifier: GPL-2.0+

/*

 * GSS Proxy upcall module

 *

 *  Copyright (C) 2012 Simo Sorce <simo@redhat.com>

 all we need to do is to write pages */

 we intentionally are not interested in this buffer */

 Contents of linux creds are all host-endian: */

 uid */

 gid */

 number of additional gid's */

 gid's */

 we recognize only 1 currently: CREDS_VALUE */

 option buffer */

 We have creds here. parse them */

 presence */

 consume uninteresting buffer */

 status->major_status */

 status->mech */

 status->minor_status */

 status->major_status_string */

 status->minor_status_string */

 status->server_ctx */

 we assume we have no options for now, so simply consume them */

 status->options */

 ctx->locale */

 ctx->server_ctx */

 we always want to ask for lucid contexts */

 ctx->options */

 we want a lucid_v1 context */

 ..and user creds */

 attr->attr */

 attr->value */

 attr->extensions */

 name->display_name */

 name->name_type */

 name->exported_name */

 name->exported_composite_name */

	/* leave name_attributes empty for now, will add once we have any

 name->name_attributes */

	/* leave options empty for now, will add once we have any options

 name->extensions */

 name->display_name */

 name->name_type */

 name->exported_name */

 name->exported_composite_name */

 we assume we have no attributes for now, so simply consume them */

 name->name_attributes */

 we assume we have no options for now, so simply consume them */

 name->extensions */

 cred->desired_name */

 cred->elements */

 cred->cred_handle_reference */

 cred->needs_release */

 ctx->exported_context_token */

 ctx->state */

 ctx->need_release */

 ctx->mech */

 ctx->src_name */

 ctx->targ_name */

 ctx->lifetime */

 ctx->ctx_flags */

 ctx->locally_initiated */

 ctx->open */

	/* leave options empty for now, will add once we have any options

 ctx->options */

 ctx->exported_context_token */

 ctx->state */

 ctx->need_release */

 ctx->mech */

 ctx->src_name */

 ctx->targ_name */

 ctx->lifetime */

 ctx->ctx_flags */

 ctx->locally_initiated */

 ctx->open */

 we assume we have no options for now, so simply consume them */

 ctx->options */

 cb->initiator_addrtype */

 cb->initiator_address */

 cb->acceptor_addrtype */

 cb->acceptor_address */

 cb->application_data */

 arg->context_handle */

 arg->cred_handle */

 arg->input_token */

 arg->input_cb */

	/* leave options empty for now, will add once we have any options

 arg->options */

 pretty arbitrary */,

 page base */, arg->npages * PAGE_SIZE);

 res->status */

 res->context_handle */

 res->output_token */

 res->delegated_cred_handle */

 we do not support upcall servers sending this data. */

 res->options */

 SPDX-License-Identifier: GPL-2.0

/*

 * Neil Brown <neilb@cse.unsw.edu.au>

 * J. Bruce Fields <bfields@umich.edu>

 * Andy Adamson <andros@umich.edu>

 * Dug Song <dugsong@monkey.org>

 *

 * RPCSEC_GSS server authentication.

 * This implements RPCSEC_GSS as defined in rfc2203 (rpcsec_gss) and rfc2078

 * (gssapi)

 *

 * The RPCSEC_GSS involves three stages:

 *  1/ context creation

 *  2/ data exchange

 *  3/ context destruction

 *

 * Context creation is handled largely by upcalls to user-space.

 *  In particular, GSS_Accept_sec_context is handled by an upcall

 * Data exchange is handled entirely within the kernel

 *  In particular, GSS_GetMIC, GSS_VerifyMIC, GSS_Seal, GSS_Unseal are in-kernel.

 * Context destruction is handled in-kernel

 *  GSS_Delete_sec_context is in-kernel

 *

 * Context creation is initiated by a RPCSEC_GSS_INIT request arriving.

 * The context handle and gss_token are used as a key into the rpcsec_init cache.

 * The content of this cache includes some of the outputs of GSS_Accept_sec_context,

 * being major_status, minor_status, context_handle, reply_token.

 * These are sent back to the client.

 * Sequence window management is handled by the kernel.  The window size if currently

 * a compile time constant.

 *

 * When user-space is happy that a context is established, it places an entry

 * in the rpcsec_context cache. The key for this cache is the context_handle.

 * The content includes:

 *   uid/gidlist - for determining access rights

 *   mechanism type

 *   mechanism specific information, such as a key

 *

/* The rpcsec_init cache is used for mapping RPCSEC_GSS_{,CONT_}INIT requests

 * into replies.

 *

 * Key is context handle (\x if empty) and gss_token.

 * Content is major_status minor_status (integers) context_handle, reply_token.

 *

 context token expiry major minor context token */

 handle */

 token */

 expiry */

 major/minor */

 out_handle */

 out_token */

/*

 * The rpcsec_context cache is used to store a context that is

 * used in data exchange.

 * The key is a context handle. The content is:

 *  uid, gidlist, mechanism, service-set, mech-specific-data

 highest seq number seen so far: */

	/* for i such that sd_max-GSS_SEQ_WIN < i <= sd_max, the i-th bit of

 contexthandle expiry [ uid gid N <n gids> mechname ...mechdata... ] */

 context handle */

 expiry */

 uid, or NEGATIVE */

		/*

		 * NOTE: we skip uid_valid()/gid_valid() checks here:

		 * instead, * -1 id's are later mapped to the

		 * (export-specific) anonymous id by nfsd_setuser.

		 *

		 * (But supplementary gid's get no such special

		 * treatment so are checked for validity here.)

 uid */

 gid */

 number of additional gid's */

 gid's */

 mech name */

 mech-specific data: */

 get client name */

/**

 * gss_check_seq_num - GSS sequence number window check

 * @rqstp: RPC Call to use when reporting errors

 * @rsci: cached GSS context state (updated on return)

 * @seq_num: sequence number to check

 *

 * Implements sequence number algorithm as specified in

 * RFC 2203, Section 5.3.3.1. "Context Management".

 *

 * Return values:

 *   %true: @rqstp's GSS sequence number is inside the window

 *   %false: @rqstp's GSS sequence number is outside the window

/*

 * Verify the checksum on the header and return SVC_OK on success.

 * Otherwise, return SVC_DROP (in the case of a bad sequence number)

 * or return SVC_DENIED and indicate error in rqstp->rq_auth_stat.

 data to compute the checksum over: */

 skip verification of revisited request */

 don't really need to check if head->iov_len > PAGE_SIZE ... */

/* It would be nice if this bit of code could be shared with the client.

 * Obstacles:

 *	The client shouldn't malloc(), would have to pass in own memory.

 *	The server uses base of head iovec as read pointer, while the

	/* NFS READ normally uses splice to send data in-place. However

	 * the data in cache can change after the reply's MIC is computed

	 * but before the RPC reply is sent. To prevent the client from

	 * rejecting the server-computed MIC in this somewhat rare case,

	 * do not use splice with the GSS integrity service.

 Did we already verify the signature on the original pass through? */

 copy out mic... */

 trim off the mic and padding at the end before returning */

		/* We need to adjust head and buf->len in tandem in this

		 * case to make svc_defer() work--it finds the original

		/* Already decrypted last time through! The sequence number

	/* buf->len is the number of bytes from the original start of the

	 * request to the end, where head[0].iov_len is just the bytes

	/* The upper layers assume the buffer is aligned on 4-byte boundaries.

	 * In the krb5p case, at least, the data ends up offset, so we need to

	/* XXX: This is very inefficient.  It would be better to either do

	 * this while we encrypt, or maybe in the receive code, if we can peak

 decoded gss client cred: */

	/* save a pointer to the beginning of the encoded verifier,

	/*

	 * A gss export can be specified either by:

	 * 	export	*(sec=krb5,rw)

	 * or by

	 * 	export gss/krb5(rw)

	 * The latter is deprecated; but for backwards compatibility reasons

	 * the nfsd code will still fall back on trying it if the former

	 * doesn't work; so we try to make both available to nfsd, below.

 Read the verifier; should be NULL: */

 Martial context handle and token for upcall: */

/*

 * Having read the cred already and found we're in the context

 * initiation case, read the verifier and initiate (or check the results

 * of) upcalls to userspace for help with context initiation.  If

 * the upcall results are available, write the verifier and result.

 * Otherwise, drop the request pending an answer to the upcall.

 Perform upcall, or find upcall result: */

 No upcall result: */

 Got an answer to the upcall; use it: */

 context handle */

	/* the handle needs to be just a unique id,

 make a copy for the caller */

 make a copy for the rsc cache */

 creds */

		/* userspace seem buggy, we should always get at least a

 steal creds */

 get mech handle from OID */

 mech-specific data: */

 Perform synchronous upcall to gss-proxy */

 Got an answer to the upcall; use it: */

/*

 * Try to set the sn->use_gss_proxy variable to a new value. We only allow

 * it to be changed if it's currently undefined (-1). If it's any other value

 * then return -EBUSY unless the type wouldn't have changed anyway.

 If use_gss_proxy is still undefined, then try to disable it */

 CONFIG_PROC_FS */

 CONFIG_PROC_FS */

/*

 * Accept an rpcsec packet.

 * If context establishment, punt to user space

 * If data exchange, verify/decrypt

 * If context destruction, handle here

 * In the context establishment and destruction case we encode

 * response here and return SVC_COMPLETE.

	/* start of rpc packet is 7 u32's back from here:

	 * xid direction rpcversion prog vers proc flavour

	/* credential is:

	 *   version(==1), proc(0,1,2,3), seq, service (1,2,3), handle

	 * at least 5 u32s, and is preceded by length, so that makes 6.

 Look up the context, and check the verifier: */

 now act upon the command: */

 Delete the entry from the cache_list and call cache_put */

 placeholders for length and seq. number: */

 placeholders for length and seq. number: */

 Restore write pointer to its original value: */

 If the reply stat is nonzero, don't wrap: */

 Skip the verifier: */

 move accept_stat to right place: */

 Also don't wrap if the accept stat is nonzero: */

 not strictly required: */

	/* XXX: Would be better to write some xdr helper functions for

	/*

	 * If there is currently tail data, make sure there is

	 * room for the head, tail, and 2 * RPC_MAX_AUTH_SIZE in

	 * the page, and move the current tail data such that

	 * there is RPC_MAX_AUTH_SIZE slack space available in

	 * both the head and tail.

	/*

	 * If there is no current tail data, make sure there is

	 * room for the head data, and 2 * RPC_MAX_AUTH_SIZE in the

	 * allotted page, and set up tail information such that there

	 * is RPC_MAX_AUTH_SIZE slack space available in both the

	 * head and tail.

 Release can be called twice, but we only wrap once. */

 normally not set till svc_send, but we need it here: */

	/* XXX: what for?  Do we mess it up the moment we call svc_putu32

	/*

	 * For any other gc_svc value, svcauth_gss_accept() already set

	 * the auth_error appropriately; just fall through:

/*

 *  linux/net/sunrpc/gss_krb5_unseal.c

 *

 *  Adapted from MIT Kerberos 5-1.2.1 lib/gssapi/krb5/k5unseal.c

 *

 *  Copyright (c) 2000-2008 The Regents of the University of Michigan.

 *  All rights reserved.

 *

 *  Andy Adamson   <andros@umich.edu>

/*

 * Copyright 1993 by OpenVision Technologies, Inc.

 *

 * Permission to use, copy, modify, distribute, and sell this software

 * and its documentation for any purpose is hereby granted without fee,

 * provided that the above copyright notice appears in all copies and

 * that both that copyright notice and this permission notice appear in

 * supporting documentation, and that the name of OpenVision not be used

 * in advertising or publicity pertaining to distribution of the software

 * without specific, written prior permission. OpenVision makes no

 * representations about the suitability of this software for any

 * purpose.  It is provided "as is" without express or implied warranty.

 *

 * OPENVISION DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,

 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO

 * EVENT SHALL OPENVISION BE LIABLE FOR ANY SPECIAL, INDIRECT OR

 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF

 * USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR

 * OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR

 * PERFORMANCE OF THIS SOFTWARE.

/*

 * Copyright (C) 1998 by the FundsXpress, INC.

 *

 * All rights reserved.

 *

 * Export of this software from the United States of America may require

 * a specific license from the United States Government.  It is the

 * responsibility of any person or organization contemplating export to

 * obtain such a license before exporting.

 *

 * WITHIN THAT CONSTRAINT, permission to use, copy, modify, and

 * distribute this software and its documentation for any purpose and

 * without fee is hereby granted, provided that the above copyright

 * notice appear in all copies and that both that copyright notice and

 * this permission notice appear in supporting documentation, and that

 * the name of FundsXpress. not be used in advertising or publicity pertaining

 * to distribution of the software without specific, written prior

 * permission.  FundsXpress makes no representations about the suitability of

 * this software for any purpose.  It is provided "as is" without express

 * or implied warranty.

 *

 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND WITHOUT ANY EXPRESS OR

 * IMPLIED WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED

 * WARRANTIES OF MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE.

/* read_token is a mic token, and message_buffer is the data that the mic was

 XXX sanity-check bodysize?? */

 it got through unscathed.  Make sure the context is unexpired */

 do sequencing checks */

 it got through unscathed.  Make sure the context is unexpired */

	/*

	 * NOTE: the sequence number at ptr + 8 is skipped, rpcsec_gss

	 * doesn't want it checked; see page 6 of rfc 2203.

/*

 * COPYRIGHT (c) 2008

 * The Regents of the University of Michigan

 * ALL RIGHTS RESERVED

 *

 * Permission is granted to use, copy, create derivative works

 * and redistribute this software and such derivative works

 * for any purpose, so long as the name of The University of

 * Michigan is not used in any advertising or publicity

 * pertaining to the use of distribution of this software

 * without specific, written prior authorization.  If the

 * above copyright notice or any other identification of the

 * University of Michigan is included in any copy of any

 * portion of this software, then the disclaimer below must

 * also be included.

 *

 * THIS SOFTWARE IS PROVIDED AS IS, WITHOUT REPRESENTATION

 * FROM THE UNIVERSITY OF MICHIGAN AS TO ITS FITNESS FOR ANY

 * PURPOSE, AND WITHOUT WARRANTY BY THE UNIVERSITY OF

 * MICHIGAN OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING

 * WITHOUT LIMITATION THE IMPLIED WARRANTIES OF

 * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE

 * REGENTS OF THE UNIVERSITY OF MICHIGAN SHALL NOT BE LIABLE

 * FOR ANY DAMAGES, INCLUDING SPECIAL, INDIRECT, INCIDENTAL, OR

 * CONSEQUENTIAL DAMAGES, WITH RESPECT TO ANY CLAIM ARISING

 * OUT OF OR IN CONNECTION WITH THE USE OF THE SOFTWARE, EVEN

 * IF IT HAS BEEN OR IS HEREAFTER ADVISED OF THE POSSIBILITY OF

 * SUCH DAMAGES.

/*

 * Copyright (C) 1998 by the FundsXpress, INC.

 *

 * All rights reserved.

 *

 * Export of this software from the United States of America may require

 * a specific license from the United States Government.  It is the

 * responsibility of any person or organization contemplating export to

 * obtain such a license before exporting.

 *

 * WITHIN THAT CONSTRAINT, permission to use, copy, modify, and

 * distribute this software and its documentation for any purpose and

 * without fee is hereby granted, provided that the above copyright

 * notice appear in all copies and that both that copyright notice and

 * this permission notice appear in supporting documentation, and that

 * the name of FundsXpress. not be used in advertising or publicity pertaining

 * to distribution of the software without specific, written prior

 * permission.  FundsXpress makes no representations about the suitability of

 * this software for any purpose.  It is provided "as is" without express

 * or implied warranty.

 *

 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND WITHOUT ANY EXPRESS OR

 * IMPLIED WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED

 * WARRANTIES OF MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE.

/*

 * This is the n-fold function as described in rfc3961, sec 5.1

 * Taken from MIT Kerberos and modified.

	/* the code below is more readable if I make these bytes

 first compute lcm(n,k) */

 now do the real work */

	/* this will end up cycling through k lcm(k,n)/k times, which

 compute the msbit in k which gets added into this byte */

			/* first, start with the msbit in the first,

			 /* then, for each byte, shift to the right

			 /* last, pick out the correct byte within

 pull out the byte value itself */

 do the addition */

 keep around the carry bit, if any */

 if there's a carry bit left over, add it back in */

 do the addition */

 keep around the carry bit, if any */

/*

 * This is the DK (derive_key) function as described in rfc3961, sec 5.1

 * Taken from MIT Kerberos and modified.

 allocate and set up buffers */

 initialize the input block */

 loop encrypting the blocks until enough key bytes are generated */

 postprocess the key */

 clean memory, free resources and exit */

/*

 * This is the des3 key derivation postprocess function

	/* take the seven bytes, move them around into the top 7 bits of the

/*

 * This is the aes key derivation postprocess function

/*

 * COPYRIGHT (c) 2008

 * The Regents of the University of Michigan

 * ALL RIGHTS RESERVED

 *

 * Permission is granted to use, copy, create derivative works

 * and redistribute this software and such derivative works

 * for any purpose, so long as the name of The University of

 * Michigan is not used in any advertising or publicity

 * pertaining to the use of distribution of this software

 * without specific, written prior authorization.  If the

 * above copyright notice or any other identification of the

 * University of Michigan is included in any copy of any

 * portion of this software, then the disclaimer below must

 * also be included.

 *

 * THIS SOFTWARE IS PROVIDED AS IS, WITHOUT REPRESENTATION

 * FROM THE UNIVERSITY OF MICHIGAN AS TO ITS FITNESS FOR ANY

 * PURPOSE, AND WITHOUT WARRANTY BY THE UNIVERSITY OF

 * MICHIGAN OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING

 * WITHOUT LIMITATION THE IMPLIED WARRANTIES OF

 * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE

 * REGENTS OF THE UNIVERSITY OF MICHIGAN SHALL NOT BE LIABLE

 * FOR ANY DAMAGES, INCLUDING SPECIAL, INDIRECT, INCIDENTAL, OR

 * CONSEQUENTIAL DAMAGES, WITH RESPECT TO ANY CLAIM ARISING

 * OUT OF OR IN CONNECTION WITH THE USE OF THE SOFTWARE, EVEN

 * IF IT HAS BEEN OR IS HEREAFTER ADVISED OF THE POSSIBILITY OF

 * SUCH DAMAGES.

	/* XXX: NOTE: we do not adjust the page lengths--they represent

	 * a range of data in the real filesystem page cache, and we need

	 * to know that range so the xdr code can properly place read data.

	 * However adjusting the head length, as we do above, is harmless.

	 * In the case of a request that fits into a single page, the server

	 * also uses length and head length together to determine the original

	 * start of the request to copy the request for deferal; so it's

	 * easier on the server if we adjust head and tail length in tandem.

	 * It's not really a problem that we don't fool with the page and

	 * tail lengths, though--at worst badly formed xdr might lead the

	 * server to attempt to parse the padding.

	 * XXX: Document all these weird requirements for gss mechanism

	/* rfc1964 claims this should be "random".  But all that's really

	 * necessary is that it be unique.  And not even that is necessary in

	 * our case since our "gssapi" implementation exists only to support

	 * rpcsec_gss, so we know that the only buffers we will ever encrypt

	 * already begin with a unique sequence number.  Just to hedge my bets

	 * I'll make a half-hearted attempt at something unique, but ensuring

	 * uniqueness would mean worrying about atomicity and rollover, and I

 initialize to random value */

/* Assumptions: the head and tail of inbuf are ours to play with.

 * The pages, however, may be real pages in the page cache and we replace

/* XXX: obviously the above should be documentation of wrap interface,

 XXX factor out common code with seal/unseal. */

 shift data to make room for header. */

 XXX Would be cleverer to encrypt while copying. */

 ptr now at header described in rfc 1964, section 1.2.1: */

	/*

	 * signalg and sealalg are stored as if they were converted from LE

	 * to host endian, even though they're opaque pairs of bytes according

	 * to the RFC.

 XXXJBF: UGH!: */

	/* XXX would probably be more efficient to compute checksum

 XXX sanity-check bodysize?? */

 get the sign and seal algorithms */

	/*

	 * Data starts after token header and checksum.  ptr points

	 * to the beginning of the token header

 it got through unscathed.  Make sure the context is unexpired */

 do sequencing checks */

	/* Copy the data back to the right position.  XXX: Would probably be

 slack must include room for krb5 padding */

 The GSS blob always precedes the RPC message payload */

/*

 * We can shift data by up to LOCAL_BUF_LEN bytes in a pass.  If we need

 * to do more than that, we shift repeatedly.  Kevin Coffman reports

 * seeing 28 bytes as the value used by Microsoft clients and servers

 * with AES, so this constant is chosen to allow handling 28 in one pass

 * without using too much stack space.

 *

 * If that proves to a problem perhaps we could use a more clever

 * algorithm.

 make room for gss token header */

 construct gss token header */

 We always do confidentiality in wrap tokens */

 "inner" token header always uses 0 for RRC */

	/*

	 * NOTE: the sequence number at ptr + 8 is skipped, rpcsec_gss

	 * doesn't want it checked; see page 6 of rfc 2203.

	/*

	 * Retrieve the decrypted gss token header and verify

	 * it against the original

 do sequencing checks */

 it got through unscathed.  Make sure the context is unexpired */

	/*

	 * Move the head data back to the right position in xdr_buf.

	 * We ignore any "ec" data since it might be in the head or

	 * the tail, and we really don't need to deal with it.

	 * Note that buf->head[0].iov_len may indicate the available

	 * head buffer space rather than that actually occupied.

 Trim off the trailing "extra count" and checksum blob */

 SPDX-License-Identifier: BSD-3-Clause

/*

 *  linux/net/sunrpc/gss_krb5_mech.c

 *

 *  Copyright (c) 2001-2008 The Regents of the University of Michigan.

 *  All rights reserved.

 *

 *  Andy Adamson <andros@umich.edu>

 *  J. Bruce Fields <bfields@umich.edu>

 forward declaration */

	/*

	 * DES (All DES enctypes are mapped to the same gss functionality)

 CONFIG_SUNRPC_DISABLE_INSECURE_ENCTYPES */

	/*

	 * 3DES

	/*

	 * AES128

	/*

	 * AES256

 Map all these key types to ENCTYPE_DES_CBC_RAW */

 Old format supports only DES!  Any other enctype uses new format */

	/* The downcall format was designed before we completely understood

	 * the uses of the context fields; so it includes some stuff we

	 * just give some minimal sanity-checking, and some we ignore

 unsigned 32-bit time overflows in year 2106 */

 seq uses the raw key */

 derive cksum */

 initiator seal encryption */

 acceptor seal encryption */

 initiator sign checksum */

 acceptor sign checksum */

 initiator seal integrity */

 acceptor seal integrity */

 unsigned 32-bit time overflows in year 2106 */

 set seq_send for use by "older" enctypes */

 Map ENCTYPE_DES3_CBC_SHA1 to ENCTYPE_DES3_CBC_RAW */

/*

 *  linux/net/sunrpc/gss_krb5_crypto.c

 *

 *  Copyright (c) 2000-2008 The Regents of the University of Michigan.

 *  All rights reserved.

 *

 *  Andy Adamson   <andros@umich.edu>

 *  Bruce Fields   <bfields@umich.edu>

/*

 * Copyright (C) 1998 by the FundsXpress, INC.

 *

 * All rights reserved.

 *

 * Export of this software from the United States of America may require

 * a specific license from the United States Government.  It is the

 * responsibility of any person or organization contemplating export to

 * obtain such a license before exporting.

 *

 * WITHIN THAT CONSTRAINT, permission to use, copy, modify, and

 * distribute this software and its documentation for any purpose and

 * without fee is hereby granted, provided that the above copyright

 * notice appear in all copies and that both that copyright notice and

 * this permission notice appear in supporting documentation, and that

 * the name of FundsXpress. not be used in advertising or publicity pertaining

 * to distribution of the software without specific, written prior

 * permission.  FundsXpress makes no representations about the suitability of

 * this software for any purpose.  It is provided "as is" without express

 * or implied warranty.

 *

 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND WITHOUT ANY EXPRESS OR

 * IMPLIED WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED

 * WARRANTIES OF MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE.

/*

 * checksum the plaintext data and hdrlen bytes of the token header

 * The checksum is performed over the first 8 bytes of the

 * gss token header and then over the data body

/*

 * checksum the plaintext data and hdrlen bytes of the token header

 * Per rfc4121, sec. 4.2.4, the checksum is performed over the data

 * body then over the first 16 octets of the MIC token

 * Inclusion of the header data in the calculation of the

 * checksum is optional.

 note that this truncates the hash */

	/* Worst case is 4 fragments: head, end of page 1, start

 pages are not in place: */

	/* Worst case is 4 fragments: head, end of page 1, start

 XXXJBF: */

/*

 * This function makes the assumption that it was ultimately called

 * from gss_wrap().

 *

 * The client auth_gss code moves any existing tail data into a

 * separate page before calling gss_wrap.

 * The server svcauth_gss code ensures that both the head and the

 * tail have slack space of RPC_MAX_AUTH_SIZE before calling gss_wrap.

 *

 * Even with that guarantee, this function may be called more than

 * once in the processing of gss_wrap().  The best we can do is

 * verify at compile-time (see GSS_KRB5_SLACK_CHECK) that the

 * largest expected shift will fit within RPC_MAX_AUTH_SIZE.

 * At run-time we can verify that a single invocation of this

 * function doesn't attempt to use more the RPC_MAX_AUTH_SIZE.

	/*

	 * For encryption, we want to read from the cleartext

	 * page cache pages, and write the encrypted data to

	 * the supplied xdr_buf pages.

 hide the gss token header and insert the confounder */

 copy plaintext gss token header after filler (if any) */

 Do the HMAC */

	/*

	 * When we are called, pages points to the real page cache

	 * data -- which we can't go and encrypt!  buf->pages points

	 * to scratch pages which we are going to send off to the

	 * client/server.  Swap in the plaintext pages to calculate

	 * the hmac.

 Make sure IV carries forward from any CBC results. */

 Now update buf to account for HMAC */

 create a segment skipping the header and leaving out the checksum */

 Make sure IV carries forward from any CBC results. */

 Calculate our hmac over the plaintext data */

 Get the packet's hmac value */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2018, 2019 Oracle. All rights reserved.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020 Oracle. All rights reserved.

/**

 * pcl_free - Release all memory associated with a parsed chunk list

 * @pcl: parsed chunk list

 *

/**

 * pcl_alloc_call - Construct a parsed chunk list for the Call body

 * @rctxt: Ingress receive context

 * @p: Start of an un-decoded Read list

 *

 * Assumptions:

 * - The incoming Read list has already been sanity checked.

 * - cl_count is already set to the number of segments in

 *   the un-decoded list.

 * - The list might not be in order by position.

 *

 * Return values:

 *       %true: Parsed chunk list was successfully constructed, and

 *              cl_count is updated to be the number of chunks (ie.

 *              unique positions) in the Read list.

 *      %false: Memory allocation failed.

 skip the list discriminator */

/**

 * pcl_alloc_read - Construct a parsed chunk list for normal Read chunks

 * @rctxt: Ingress receive context

 * @p: Start of an un-decoded Read list

 *

 * Assumptions:

 * - The incoming Read list has already been sanity checked.

 * - cl_count is already set to the number of segments in

 *   the un-decoded list.

 * - The list might not be in order by position.

 *

 * Return values:

 *       %true: Parsed chunk list was successfully constructed, and

 *              cl_count is updated to be the number of chunks (ie.

 *              unique position values) in the Read list.

 *      %false: Memory allocation failed.

 *

 * TODO:

 * - Check for chunk range overlaps

 skip the list discriminator */

/**

 * pcl_alloc_write - Construct a parsed chunk list from a Write list

 * @rctxt: Ingress receive context

 * @pcl: Parsed chunk list to populate

 * @p: Start of an un-decoded Write list

 *

 * Assumptions:

 * - The incoming Write list has already been sanity checked, and

 * - cl_count is set to the number of chunks in the un-decoded list.

 *

 * Return values:

 *       %true: Parsed chunk list was successfully constructed.

 *      %false: Memory allocation failed.

 skip the list discriminator */

/**

 * pcl_process_nonpayloads - Process non-payload regions inside @xdr

 * @pcl: Chunk list to process

 * @xdr: xdr_buf to process

 * @actor: Function to invoke on each non-payload region

 * @data: Arguments for @actor

 *

 * This mechanism must ignore not only result payloads that were already

 * sent via RDMA Write, but also XDR padding for those payloads that

 * the upper layer has added.

 *

 * Assumptions:

 *  The xdr->len and ch_position fields are aligned to 4-byte multiples.

 *

 * Returns:

 *   On success, zero,

 *   %-EMSGSIZE on XDR buffer overflow, or

 *   The return value of @actor

 No result payloads were generated */

 Process the region before the first result payload */

 Process the regions between each middle result payload */

 Process the region after the last result payload */

 SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause

/*

 * Copyright (c) 2015-2018 Oracle.  All rights reserved.

 * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the BSD-type

 * license below:

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions

 * are met:

 *

 *      Redistributions of source code must retain the above copyright

 *      notice, this list of conditions and the following disclaimer.

 *

 *      Redistributions in binary form must reproduce the above

 *      copyright notice, this list of conditions and the following

 *      disclaimer in the documentation and/or other materials provided

 *      with the distribution.

 *

 *      Neither the name of the Network Appliance, Inc. nor the names of

 *      its contributors may be used to endorse or promote products

 *      derived from this software without specific prior written

 *      permission.

 *

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT

 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,

 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT

 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,

 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY

 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 * Author: Tom Tucker <tom@opengridcomputing.com>

 RPC/RDMA parameters */

 historical default */

 Register RDMA with the SVC transport switch */

 SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause

/*

 * Copyright (c) 2015-2018 Oracle. All rights reserved.

 * Copyright (c) 2014 Open Grid Computing, Inc. All rights reserved.

 * Copyright (c) 2005-2007 Network Appliance, Inc. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the BSD-type

 * license below:

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions

 * are met:

 *

 *      Redistributions of source code must retain the above copyright

 *      notice, this list of conditions and the following disclaimer.

 *

 *      Redistributions in binary form must reproduce the above

 *      copyright notice, this list of conditions and the following

 *      disclaimer in the documentation and/or other materials provided

 *      with the distribution.

 *

 *      Neither the name of the Network Appliance, Inc. nor the names of

 *      its contributors may be used to endorse or promote products

 *      derived from this software without specific prior written

 *      permission.

 *

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT

 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,

 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT

 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,

 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY

 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 * Author: Tom Tucker <tom@opengridcomputing.com>

 QP event handler */

 These are considered benign events */

 These are considered fatal events */

	/*

	 * Note that this implies that the underlying transport support

	 * has some form of congestion control (see RFC 7530 section 3.1

	 * paragraph 2). For now, we assume that all supported RDMA

	 * transports are suitable here.

/*

 * This function handles the CONNECT_REQUEST event on a listening

 * endpoint. It is passed the cma_id for the _new_ connection. The context in

 * this cma_id is inherited from the listening cma_id and is the svc_xprt

 * structure for the listening endpoint.

 *

 * This function creates a new xprt for the new connection and enqueues it on

 * the accept queue for the listent xprt. When the listen thread is kicked, it

 * will call the recvfrom method on the listen xprt which will accept the new

 * connection.

 Create a new transport */

 Save client advertised inbound read limit for use later in accept. */

	/* The remote port is arbitrary and not under the control of the

	 * client ULP. Set it to a fixed value so that the DRC continues

	 * to be effective after a reconnect.

	/*

	 * Enqueue the new transport on the accept queue of the listening

	 * transport

/**

 * svc_rdma_listen_handler - Handle CM events generated on a listening endpoint

 * @cma_id: the server's listener rdma_cm_id

 * @event: details of the event

 *

 * Return values:

 *     %0: Do not destroy @cma_id

 *     %1: Destroy @cma_id (never returned here)

 *

 * NB: There is never a DEVICE_REMOVAL event for INADDR_ANY listeners.

/**

 * svc_rdma_cma_handler - Handle CM events on client connections

 * @cma_id: the server's listener rdma_cm_id

 * @event: details of the event

 *

 * Return values:

 *     %0: Do not destroy @cma_id

 *     %1: Destroy @cma_id (never returned here)

		/* Handle any requests that were received while

/*

 * Create a listening RDMA service endpoint.

	/* Allow both IPv4 and IPv6 sockets to bind a single port

	 * at the same time.

	/*

	 * We need to use the address from the cm_id in case the

	 * caller specified 0 for the port number.

/*

 * This is the xpo_recvfrom function for listening endpoints. Its

 * purpose is to accept incoming connections. The CMA callback handler

 * has already created a new transport and attached it to the new CMA

 * ID.

 *

 * There is a queue of pending connections hung on the listening

 * transport. This queue contains the new svc_xprt structure. This

 * function takes svc_xprt structures off the accept_q and completes

 * the connection.

 Get the next entry off the accept list */

	/* Qualify the transport resource defaults with the

 Transport header, head iovec, tail iovec */

 Add one SGE per page list entry */

 Construct RDMA-CM private message */

 Accept Connection */

 Take a reference in case the DTO handler runs */

 This call to put will destroy the transport */

 This blocks until the Completion Queues are empty */

 Destroy the QP if present (not a listener) */

 Destroy the CM ID */

	/*

	 * If there are already waiters on the SQ,

	 * return false.

 Otherwise return true. */

 SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause

/*

 * Copyright (c) 2014-2020, Oracle and/or its affiliates.

 * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the BSD-type

 * license below:

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions

 * are met:

 *

 *      Redistributions of source code must retain the above copyright

 *      notice, this list of conditions and the following disclaimer.

 *

 *      Redistributions in binary form must reproduce the above

 *      copyright notice, this list of conditions and the following

 *      disclaimer in the documentation and/or other materials provided

 *      with the distribution.

 *

 *      Neither the name of the Network Appliance, Inc. nor the names of

 *      its contributors may be used to endorse or promote products

 *      derived from this software without specific prior written

 *      permission.

 *

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT

 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,

 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT

 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,

 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY

 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

/*

 * rpc_rdma.c

 *

 * This file contains the guts of the RPC RDMA protocol, and

 * does marshaling/unmarshaling, etc. It is also where interfacing

 * to the Linux RPC framework lives.

/* Returns size of largest RPC-over-RDMA header in a Call message

 *

 * The largest Call header contains a full-size Read list and a

 * minimal Reply chunk.

 Fixed header fields and list discriminators */

 Maximum Read list size */

 Minimal Read chunk size */

 segment count */

 list discriminator */

/* Returns size of largest RPC-over-RDMA header in a Reply message

 *

 * There is only one Write list or one Reply chunk per Reply

 * message.  The larger list is the Write list.

 Fixed header fields and list discriminators */

 Maximum Write list size */

 segment count */

 list discriminator */

/**

 * rpcrdma_set_max_header_sizes - Initialize inline payload sizes

 * @ep: endpoint to initialize

 *

 * The max_inline fields contain the maximum size of an RPC message

 * so the marshaling code doesn't have to repeat this calculation

 * for every RPC.

/* The client can send a request inline as long as the RPCRDMA header

 * plus the RPC call fit under the transport's inline limit. If the

 * combined call message size exceeds that limit, the client must use

 * a Read chunk for this operation.

 *

 * A Read chunk is also required if sending the RPC call inline would

 * exceed this device's max_sge limit.

/* The client can't know how large the actual reply will be. Thus it

 * plans for the largest possible reply for that particular ULP

 * operation. If the maximum combined reply message size exceeds that

 * limit, the client must provide a write list or a reply chunk for

 * this request.

/* The client is required to provide a Reply chunk if the maximum

 * size of the non-payload part of the RPC Reply is larger than

 * the inline threshold.

/* ACL likes to be lazy in allocating pages. For TCP, these

 * pages can be allocated during receive processing. Not true

 * for RDMA, which must always provision receive buffers

 * up front.

/* Convert @vec to a single SGL element.

 *

 * Returns pointer to next available SGE, and bumps the total number

 * of SGEs consumed.

/* Convert @xdrbuf into SGEs no larger than a page each. As they

 * are registered, these SGEs are then coalesced into RDMA segments

 * when the selected memreg mode supports it.

 *

 * Returns positive number of SGEs consumed, or a negative errno.

 Item present */

/* Register and XDR encode the Read list. Supports encoding a list of read

 * segments that belong to a single read chunk.

 *

 * Encoding key for single-list chunks (HLOO = Handle32 Length32 Offset64):

 *

 *  Read chunklist (a linked list):

 *   N elements, position P (same P for all chunks of same arg!):

 *    1 - PHLOO - 1 - PHLOO - ... - 1 - PHLOO - 0

 *

 * Returns zero on success, or a negative errno if a failure occurred.

 * @xdr is advanced to the next position in the stream.

 *

 * Only a single @pos value is currently supported.

/* Register and XDR encode the Write list. Supports encoding a list

 * containing one array of plain segments that belong to a single

 * write chunk.

 *

 * Encoding key for single-list chunks (HLOO = Handle32 Length32 Offset64):

 *

 *  Write chunklist (a list of (one) counted array):

 *   N elements:

 *    1 - N - HLOO - HLOO - ... - HLOO - 0

 *

 * Returns zero on success, or a negative errno if a failure occurred.

 * @xdr is advanced to the next position in the stream.

 *

 * Only a single Write chunk is currently supported.

 Actual value encoded below */

 Update count of segments in this Write chunk */

/* Register and XDR encode the Reply chunk. Supports encoding an array

 * of plain segments that belong to a single write (reply) chunk.

 *

 * Encoding key for single-list chunks (HLOO = Handle32 Length32 Offset64):

 *

 *  Reply chunk (a counted array):

 *   N elements:

 *    1 - N - HLOO - HLOO - ... - HLOO

 *

 * Returns zero on success, or a negative errno if a failure occurred.

 * @xdr is advanced to the next position in the stream.

 Actual value encoded below */

 Update count of segments in the Reply chunk */

/**

 * rpcrdma_sendctx_unmap - DMA-unmap Send buffer

 * @sc: sendctx containing SGEs to unmap

 *

	/* The first two SGEs contain the transport header and

	 * the inline buffer. These are always left mapped so

	 * they can be cheaply re-used.

/* Prepare an SGE for the RPC-over-RDMA transport header.

/* The head iovec is straightforward, as it is usually already

 * DMA-mapped. Sync the content that has changed.

/* If there is a page list present, DMA map and prepare an

 * SGE for each page to be sent.

/* The tail iovec may include an XDR pad for the page list,

 * as well as additional content, and may not reside in the

 * same page as the head iovec.

/* Copy the tail to the end of the head buffer.

/* Copy pagelist content into the head buffer.

/* Copy the contents of @xdr into @rl_sendbuf and DMA sync it.

 * When the head, pagelist, and tail are small, a pull-up copy

 * is considerably less costly than DMA mapping the components

 * of @xdr.

 *

 * Assumptions:

 *  - the caller has already verified that the total length

 *    of the RPC Call body will fit into @rl_sendbuf.

 The whole RPC message resides in the head iovec now */

	/* If there is a Read chunk, the page list is being handled

	 * via explicit RDMA, and thus is skipped here.

 Do not include the tail if it is only an XDR pad */

		/* If the content in the page list is an odd length,

		 * xdr_write_pages() adds a pad at the beginning of

		 * the tail iovec. Force the tail's non-pad content to

		 * land at the next XDR position in the Send message.

/**

 * rpcrdma_prepare_send_sges - Construct SGEs for a Send WR

 * @r_xprt: controlling transport

 * @req: context of RPC Call being marshalled

 * @hdrlen: size of transport header, in bytes

 * @xdr: xdr_buf containing RPC Call

 * @rtype: chunk type being encoded

 *

 * Returns 0 on success; otherwise a negative errno is returned.

/**

 * rpcrdma_marshal_req - Marshal and send one RPC request

 * @r_xprt: controlling transport

 * @rqst: RPC request to be marshaled

 *

 * For the RPC in "rqst", this function:

 *  - Chooses the transfer mode (eg., RDMA_MSG or RDMA_NOMSG)

 *  - Registers Read, Write, and Reply chunks

 *  - Constructs the transport header

 *  - Posts a Send WR to send the transport header and request

 *

 * Returns:

 *	%0 if the RPC was sent successfully,

 *	%-ENOTCONN if the connection was lost,

 *	%-EAGAIN if the caller should call again with the same arguments,

 *	%-ENOBUFS if the caller should call again after a delay,

 *	%-EMSGSIZE if the transport header is too small,

 *	%-EIO if a permanent problem occurred while marshaling.

 Fixed header fields */

	/* When the ULP employs a GSS flavor that guarantees integrity

	 * or privacy, direct data placement of individual data items

	 * is not allowed.

	/*

	 * Chunks needed for results?

	 *

	 * o If the expected result is under the inline threshold, all ops

	 *   return as inline.

	 * o Large read ops return data as write chunk(s), header as

	 *   inline.

	 * o Large non-read ops return as a single reply chunk.

	/*

	 * Chunks needed for arguments?

	 *

	 * o If the total request is under the inline threshold, all ops

	 *   are sent as inline.

	 * o Large write ops transmit data as read chunk(s), header as

	 *   inline.

	 * o Large non-write ops are sent with the entire message as a

	 *   single read chunk (protocol 0-position special case).

	 *

	 * This assumes that the upper layer does not present a request

	 * that both has a data payload, and whose non-data arguments

	 * by themselves are larger than the inline threshold.

	/* This implementation supports the following combinations

	 * of chunk lists in one RPC-over-RDMA Call message:

	 *

	 *   - Read list

	 *   - Write list

	 *   - Reply chunk

	 *   - Read list + Reply chunk

	 *

	 * It might not yet support the following combinations:

	 *

	 *   - Read list + Write list

	 *

	 * It does not support the following combinations:

	 *

	 *   - Write list + Reply chunk

	 *   - Read list + Write list + Reply chunk

	 *

	 * This implementation supports only a single chunk in each

	 * Read or Write list. Thus for example the client cannot

	 * send a Call message with a Position Zero Read chunk and a

	 * regular Read chunk at the same time.

/**

 * rpcrdma_reset_cwnd - Reset the xprt's congestion window

 * @r_xprt: controlling transport instance

 *

 * Prepare @r_xprt for the next connection by reinitializing

 * its credit grant to one (see RFC 8166, Section 3.3.3).

/**

 * rpcrdma_inline_fixup - Scatter inline received data into rqst's iovecs

 * @rqst: controlling RPC request

 * @srcp: points to RPC message payload in receive buffer

 * @copy_len: remaining length of receive buffer content

 * @pad: Write chunk pad bytes needed (zero for pure inline)

 *

 * The upper layer has set the maximum number of bytes it can

 * receive in each component of rq_rcv_buf. These values are set in

 * the head.iov_len, page_len, tail.iov_len, and buflen fields.

 *

 * Unlike the TCP equivalent (xdr_partial_copy_from_skb), in

 * many cases this function simply updates iov_base pointers in

 * rq_rcv_buf to point directly to the received reply data, to

 * avoid copying reply data.

 *

 * Returns the count of bytes which had to be memcopied.

	/* The head iovec is redirected to the RPC reply message

	 * in the receive buffer, to avoid a memcopy.

	/* The contents of the receive buffer that follow

	 * head.iov_len bytes are copied into the page list.

		/* Implicit padding for the last segment in a Write

		 * chunk is inserted inline at the front of the tail

		 * iovec. The upper layer ignores the content of

		 * the pad. Simply ensure inline content in the tail

		 * that follows the Write chunk is properly aligned.

	/* The tail iovec is redirected to the remaining data

	 * in the receive buffer, to avoid a memcopy.

/* By convention, backchannel calls arrive via rdma_msg type

 * messages, and never populate the chunk lists. This makes

 * the RPC/RDMA header small and fixed in size, so it is

 * straightforward to check the RPC header's direction field.

 Peek at stream contents without advancing. */

 Chunk lists */

 RPC header */

	/* Now that we are sure this is a backchannel call,

	 * advance to the RPC header.

 CONFIG_SUNRPC_BACKCHANNEL */

 CONFIG_SUNRPC_BACKCHANNEL */

/* In RPC-over-RDMA Version One replies, a Read list is never

 * expected. This decoder is a stub that returns an error if

 * a Read list is present.

/* Supports only one Write chunk in the Write list

 Decode the chunk lists */

 RDMA_MSG sanity checks */

 Build the RPC reply's Payload stream in rqst->rq_rcv_buf */

 Decode the chunk lists */

 RDMA_NOMSG sanity checks */

 Reply chunk buffer already is the reply vector */

/**

 * rpcrdma_unpin_rqst - Release rqst without completing it

 * @rep: RPC/RDMA Receive context

 *

 * This is done when a connection is lost so that a Reply

 * can be dropped and its matching Call can be subsequently

 * retransmitted on a new connection.

/**

 * rpcrdma_complete_rqst - Pass completed rqst back to RPC

 * @rep: RPC/RDMA Receive context

 *

 * Reconstruct the RPC reply and complete the transaction

 * while @rqst is still pinned to ensure the rep, rqst, and

 * rq_task pointers remain stable.

/**

 * rpcrdma_reply_handler - Process received RPC/RDMA messages

 * @rep: Incoming rpcrdma_rep object to process

 *

 * Errors must result in the RPC task either being awakened, or

 * allowed to timeout, to discover the errors at that time.

	/* Any data means we had a useful conversation, so

	 * then we don't need to delay the next reconnect.

 Fixed transport header fields */

	/* Match incoming rpcrdma_rep to an rpcrdma_req to

	 * get context for handling any incoming chunks.

 don't deadlock */

 LocalInv completion will complete the RPC */

 SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause

/*

 * Copyright (c) 2014-2017 Oracle.  All rights reserved.

 * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the BSD-type

 * license below:

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions

 * are met:

 *

 *      Redistributions of source code must retain the above copyright

 *      notice, this list of conditions and the following disclaimer.

 *

 *      Redistributions in binary form must reproduce the above

 *      copyright notice, this list of conditions and the following

 *      disclaimer in the documentation and/or other materials provided

 *      with the distribution.

 *

 *      Neither the name of the Network Appliance, Inc. nor the names of

 *      its contributors may be used to endorse or promote products

 *      derived from this software without specific prior written

 *      permission.

 *

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT

 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,

 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT

 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,

 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY

 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

/*

 * verbs.c

 *

 * Encapsulates the major functions managing:

 *  o adapters

 *  o endpoints

 *  o connections

 *  o buffer memory

/*

 * Globals/Macros

/*

 * internal functions

/* Wait for outstanding transport work to finish. ib_drain_qp

 * handles the drains in the wrong order for us, so open code

 * them here.

	/* Wait for rpcrdma_post_recvs() to leave its critical

	 * section.

	/* Flush Receives, then wait for deferred Reply work

	 * to complete.

	/* Deferred Reply processing might have scheduled

	 * local invalidations.

/* Ensure xprt_force_disconnect() is invoked exactly once when a

 * connection is closed or lost. (The important thing is it needs

 * to be invoked "at least" once).

/**

 * rpcrdma_flush_disconnect - Disconnect on flushed completion

 * @r_xprt: transport to disconnect

 * @wc: work completion entry

 *

 * Must be called in process context.

/**

 * rpcrdma_wc_send - Invoked by RDMA provider for each polled Send WC

 * @cq:	completion queue

 * @wc:	WCE for a completed Send WR

 *

 WARNING: Only wr_cqe and status are reliable at this point */

/**

 * rpcrdma_wc_receive - Invoked by RDMA provider for each polled Receive WC

 * @cq:	completion queue

 * @wc:	WCE for a completed Receive WR

 *

 WARNING: Only wr_cqe and status are reliable at this point */

 status == SUCCESS means all fields in wc are trustworthy */

 Default settings for RPC-over-RDMA Version One */

/**

 * rpcrdma_cm_event_handler - Handle RDMA CM events

 * @id: rdma_cm_id on which an event has occurred

 * @event: details of the event

 *

 * Called with @id's mutex held. Returns 1 if caller should

 * destroy @id, otherwise 0.

/* Returns:

 *     %0 if @ep still has a positive kref count, or

 *     %1 if @ep was destroyed successfully.

 Initialize cma parameters */

 Prepare RDMA-CM private message */

 Client offers RDMA Read but does not initiate */

	/* Limit transport retries so client can detect server

	 * GID changes quickly. RPC layer handles re-establishing

	 * transport connection and retransmission.

	/* RPC-over-RDMA handles its own flow control. In addition,

	 * make all RNR NAKs visible so we know that RPC-over-RDMA

	 * flow control is working correctly (no NAKs should be seen).

/**

 * rpcrdma_xprt_connect - Connect an unconnected transport

 * @r_xprt: controlling transport instance

 *

 * Returns 0 on success or a negative errno.

	/* Bump the ep's reference count while there are

	 * outstanding Receives.

/**

 * rpcrdma_xprt_disconnect - Disconnect underlying transport

 * @r_xprt: controlling transport instance

 *

 * Caller serializes. Either the transport send lock is held,

 * or we're being called to destroy the transport.

 *

 * On return, @r_xprt is completely divested of all hardware

 * resources and prepared for the next ->connect operation.

/* Fixed-size circular FIFO queue. This implementation is wait-free and

 * lock-free.

 *

 * Consumer is the code path that posts Sends. This path dequeues a

 * sendctx for use by a Send operation. Multiple consumer threads

 * are serialized by the RPC transport lock, which allows only one

 * ->send_request call at a time.

 *

 * Producer is the code path that handles Send completions. This path

 * enqueues a sendctx that has been completed. Multiple producer

 * threads are serialized by the ib_poll_cq() function.

/* rpcrdma_sendctxs_destroy() assumes caller has already quiesced

 * queue activity, and rpcrdma_xprt_drain has flushed all remaining

 * Send requests.

	/* Maximum number of concurrent outstanding Send WRs. Capping

	 * the circular queue size stops Send Queue overflow by causing

	 * the ->send_request call to fail temporarily before too many

	 * Sends are posted.

/* The sendctx queue is not guaranteed to have a size that is a

 * power of two, thus the helpers in circ_buf.h cannot be used.

 * The other option is to use modulus (%), which can be expensive.

/**

 * rpcrdma_sendctx_get_locked - Acquire a send context

 * @r_xprt: controlling transport instance

 *

 * Returns pointer to a free send completion context; or NULL if

 * the queue is empty.

 *

 * Usage: Called to acquire an SGE array before preparing a Send WR.

 *

 * The caller serializes calls to this function (per transport), and

 * provides an effective memory barrier that flushes the new value

 * of rb_sc_head.

 ORDER: item must be accessed _before_ head is updated */

	/* Releasing the lock in the caller acts as a memory

	 * barrier that flushes rb_sc_head.

	/* The queue is "empty" if there have not been enough Send

	 * completions recently. This is a sign the Send Queue is

	 * backing up. Cause the caller to pause and try again.

/**

 * rpcrdma_sendctx_put_locked - Release a send context

 * @r_xprt: controlling transport instance

 * @sc: send context to release

 *

 * Usage: Called from Send completion to return a sendctxt

 * to the queue.

 *

 * The caller serializes calls to this function (per transport).

	/* Unmap SGEs of previously completed but unsignaled

	 * Sends by walking up the queue until @sc is found.

 ORDER: item must be accessed _before_ tail is updated */

 Paired with READ_ONCE */

/**

 * rpcrdma_mrs_refresh - Wake the MR refresh worker

 * @r_xprt: controlling transport instance

 *

	/* If there is no underlying connection, it's no use

	 * to wake the refresh worker.

		/* The work is scheduled on a WQ_MEM_RECLAIM

		 * workqueue in order to prevent MR allocation

		 * from recursing into NFS during direct reclaim.

/**

 * rpcrdma_req_create - Allocate an rpcrdma_req object

 * @r_xprt: controlling r_xprt

 * @size: initial size, in bytes, of send and receive buffers

 * @flags: GFP flags passed to memory allocators

 *

 * Returns an allocated and fully initialized rpcrdma_req or NULL.

/**

 * rpcrdma_req_setup - Per-connection instance setup of an rpcrdma_req object

 * @r_xprt: controlling transport instance

 * @req: rpcrdma_req object to set up

 *

 * Returns zero on success, and a negative errno on failure.

 Compute maximum header buffer size in bytes */

/* ASSUMPTION: the rb_allreqs list is stable for the duration,

 * and thus can be walked without holding rb_lock. Eg. the

 * caller is holding the transport send lock to exclude

 * device removal or disconnection.

 Credits are valid for only one connection */

/* ASSUMPTION: the rb_allreqs list is stable for the duration,

 * and thus can be walked without holding rb_lock. Eg. the

 * caller is holding the transport send lock to exclude

 * device removal or disconnection.

 Calls to llist_del_first are required to be serialized */

/**

 * rpcrdma_rep_put - Release rpcrdma_rep back to free list

 * @buf: buffer pool

 * @rep: rep to release

 *

/* Caller must ensure the QP is quiescent (RQ is drained) before

 * invoking this function, to guarantee rb_all_reps is not

 * changing.

 Mark this rep for destruction */

/**

 * rpcrdma_buffer_create - Create initial set of req/rep objects

 * @r_xprt: transport instance to (re)initialize

 *

 * Returns zero on success, otherwise a negative errno.

/**

 * rpcrdma_req_destroy - Destroy an rpcrdma_req object

 * @req: unused object to be destroyed

 *

 * Relies on caller holding the transport send lock to protect

 * removing req->rl_all from buf->rb_all_reqs safely.

/**

 * rpcrdma_mrs_destroy - Release all of a transport's MRs

 * @r_xprt: controlling transport instance

 *

 * Relies on caller holding the transport send lock to protect

 * removing mr->mr_list from req->rl_free_mrs safely.

/**

 * rpcrdma_buffer_destroy - Release all hw resources

 * @buf: root control block for resources

 *

 * ORDERING: relies on a prior rpcrdma_xprt_drain :

 * - No more Send or Receive completions can occur

 * - All MRs, reps, and reqs are returned to their free lists

/**

 * rpcrdma_mr_get - Allocate an rpcrdma_mr object

 * @r_xprt: controlling transport

 *

 * Returns an initialized rpcrdma_mr or NULL if no free

 * rpcrdma_mr objects are available.

/**

 * rpcrdma_reply_put - Put reply buffers back into pool

 * @buffers: buffer pool

 * @req: object to return

 *

/**

 * rpcrdma_buffer_get - Get a request buffer

 * @buffers: Buffer pool from which to obtain a buffer

 *

 * Returns a fresh rpcrdma_req, or NULL if none are available.

/**

 * rpcrdma_buffer_put - Put request/reply buffers back into pool

 * @buffers: buffer pool

 * @req: object to return

 *

/* Returns a pointer to a rpcrdma_regbuf object, or NULL.

 *

 * xprtrdma uses a regbuf for posting an outgoing RDMA SEND, or for

 * receiving the payload of RDMA RECV operations. During Long Calls

 * or Replies they may be registered externally via frwr_map.

/**

 * rpcrdma_regbuf_realloc - re-allocate a SEND/RECV buffer

 * @rb: regbuf to reallocate

 * @size: size of buffer to be allocated, in bytes

 * @flags: GFP flags

 *

 * Returns true if reallocation was successful. If false is

 * returned, @rb is left untouched.

/**

 * __rpcrdma_regbuf_dma_map - DMA-map a regbuf

 * @r_xprt: controlling transport instance

 * @rb: regbuf to be mapped

 *

 * Returns true if the buffer is now DMA mapped to @r_xprt's device

/**

 * rpcrdma_post_recvs - Refill the Receive Queue

 * @r_xprt: controlling transport instance

 * @needed: current credit grant

 * @temp: mark Receive buffers to be deleted after one use

 *

 fast path: all needed reps can be found on the free list */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2015, 2017 Oracle.  All rights reserved.

 * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.

/* Lightweight memory registration using Fast Registration Work

 * Requests (FRWR).

 *

 * FRWR features ordered asynchronous registration and invalidation

 * of arbitrarily-sized memory regions. This is the fastest and safest

 * but most complex memory registration mode.

/* Normal operation

 *

 * A Memory Region is prepared for RDMA Read or Write using a FAST_REG

 * Work Request (frwr_map). When the RDMA operation is finished, this

 * Memory Region is invalidated using a LOCAL_INV Work Request

 * (frwr_unmap_async and frwr_unmap_sync).

 *

 * Typically FAST_REG Work Requests are not signaled, and neither are

 * RDMA Send Work Requests (with the exception of signaling occasionally

 * to prevent provider work queue overflows). This greatly reduces HCA

 * interrupt workload.

/* Transport recovery

 *

 * frwr_map and frwr_unmap_* cannot run at the same time the transport

 * connect worker is running. The connect worker holds the transport

 * send lock, just as ->send_request does. This prevents frwr_map and

 * the connect worker from running concurrently. When a connection is

 * closed, the Receive completion queue is drained before the allowing

 * the connect worker to get control. This prevents frwr_unmap and the

 * connect worker from running concurrently.

 *

 * When the underlying transport disconnects, MRs that are in flight

 * are flushed and are likely unusable. Thus all MRs are destroyed.

 * New MRs are created on demand.

/**

 * frwr_mr_release - Destroy one MR

 * @mr: MR allocated by frwr_mr_init

 *

	/* The MR is returned to the req's MR free list instead

	 * of to the xprt's MR free list. No spinlock is needed.

/* frwr_reset - Place MRs back on the free list

 * @req: request to reset

 *

 * Used after a failed marshal. For FRWR, this means the MRs

 * don't have to be fully released and recreated.

 *

 * NB: This is safe only as long as none of @req's MRs are

 * involved with an ongoing asynchronous FAST_REG or LOCAL_INV

 * Work Request.

/**

 * frwr_mr_init - Initialize one MR

 * @r_xprt: controlling transport instance

 * @mr: generic MR to prepare for FRWR

 *

 * Returns zero if successful. Otherwise a negative errno

 * is returned.

/**

 * frwr_query_device - Prepare a transport for use with FRWR

 * @ep: endpoint to fill in

 * @device: RDMA device to query

 *

 * On success, sets:

 *	ep->re_attr

 *	ep->re_max_requests

 *	ep->re_max_rdma_segs

 *	ep->re_max_fr_depth

 *	ep->re_mrtype

 *

 * Return values:

 *   On success, returns zero.

 *   %-EINVAL - the device does not support FRWR memory registration

 *   %-ENOMEM - the device is not sufficiently capable for NFS/RDMA

	/* Quirk: Some devices advertise a large max_fast_reg_page_list_len

	 * capability, but perform optimally when the MRs are not larger

	 * than a page.

	/* Add room for frwr register and invalidate WRs.

	 * 1. FRWR reg WR for head

	 * 2. FRWR invalidate WR for head

	 * 3. N FRWR reg WRs for pagelist

	 * 4. N FRWR invalidate WRs for pagelist

	 * 5. FRWR reg WR for tail

	 * 6. FRWR invalidate WR for tail

	 * 7. The RDMA_SEND WR

	/* Calculate N if the device max FRWR depth is smaller than

	 * RPCRDMA_MAX_DATA_SEGS.

 FRWR reg + invalidate */

 for ib_drain_sq */

 for ib_drain_rq */

 Reply chunks require segments for head and tail buffers */

	/* Ensure the underlying device is capable of conveying the

	 * largest r/wsize NFS will ask for. This guarantees that

	 * failing over from one RDMA device to another will not

	 * break NFS I/O.

/**

 * frwr_map - Register a memory region

 * @r_xprt: controlling transport

 * @seg: memory region co-ordinates

 * @nsegs: number of segments remaining

 * @writing: true when RDMA Write will be used

 * @xid: XID of RPC using the registered memory

 * @mr: MR to fill in

 *

 * Prepare a REG_MR Work Request to register a memory region

 * for remote access via RDMA READ or RDMA WRITE.

 *

 * Returns the next segment or a negative errno pointer.

 * On success, @mr is filled in.

/**

 * frwr_wc_fastreg - Invoked by RDMA provider for a flushed FastReg WC

 * @cq: completion queue

 * @wc: WCE for a completed FastReg WR

 *

 * Each flushed MR gets destroyed after the QP has drained.

 WARNING: Only wr_cqe and status are reliable at this point */

/**

 * frwr_send - post Send WRs containing the RPC Call message

 * @r_xprt: controlling transport instance

 * @req: prepared RPC Call

 *

 * For FRWR, chain any FastReg WRs to the Send WR. Only a

 * single ib_post_send call is needed to register memory

 * and then post the Send WR.

 *

 * Returns the return code from ib_post_send.

 *

 * Caller must hold the transport send lock to ensure that the

 * pointers to the transport's rdma_cm_id and QP are stable.

/**

 * frwr_reminv - handle a remotely invalidated mr on the @mrs list

 * @rep: Received reply

 * @mrs: list of MRs to check

 *

 only one invalidated MR per RPC */

/**

 * frwr_wc_localinv - Invoked by RDMA provider for a LOCAL_INV WC

 * @cq: completion queue

 * @wc: WCE for a completed LocalInv WR

 *

 WARNING: Only wr_cqe and status are reliable at this point */

/**

 * frwr_wc_localinv_wake - Invoked by RDMA provider for a LOCAL_INV WC

 * @cq: completion queue

 * @wc: WCE for a completed LocalInv WR

 *

 * Awaken anyone waiting for an MR to finish being fenced.

 WARNING: Only wr_cqe and status are reliable at this point */

/**

 * frwr_unmap_sync - invalidate memory regions that were registered for @req

 * @r_xprt: controlling transport instance

 * @req: rpcrdma_req with a non-empty list of MRs to process

 *

 * Sleeps until it is safe for the host CPU to access the previously mapped

 * memory regions. This guarantees that registered MRs are properly fenced

 * from the server before the RPC consumer accesses the data in them. It

 * also ensures proper Send flow control: waking the next RPC waits until

 * this RPC has relinquished all its Send Queue entries.

	/* ORDER: Invalidate all of the MRs first

	 *

	 * Chain the LOCAL_INV Work Requests and post them with

	 * a single ib_post_send() call.

	/* Strong send queue ordering guarantees that when the

	 * last WR in the chain completes, all WRs in the chain

	 * are complete.

	/* Transport disconnect drains the receive CQ before it

	 * replaces the QP. The RPC reply handler won't call us

	 * unless re_id->qp is a valid pointer.

	/* The final LOCAL_INV WR in the chain is supposed to

	 * do the wake. If it was never posted, the wake will

	 * not happen, so don't wait in that case.

 On error, the MRs get destroyed once the QP has drained. */

	/* Force a connection loss to ensure complete recovery.

/**

 * frwr_wc_localinv_done - Invoked by RDMA provider for a signaled LOCAL_INV WC

 * @cq:	completion queue

 * @wc:	WCE for a completed LocalInv WR

 *

 WARNING: Only wr_cqe and status are reliable at this point */

 Ensure that @rep is generated before the MR is released */

/**

 * frwr_unmap_async - invalidate memory regions that were registered for @req

 * @r_xprt: controlling transport instance

 * @req: rpcrdma_req with a non-empty list of MRs to process

 *

 * This guarantees that registered MRs are properly fenced from the

 * server before the RPC consumer accesses the data in them. It also

 * ensures proper Send flow control: waking the next RPC waits until

 * this RPC has relinquished all its Send Queue entries.

	/* Chain the LOCAL_INV Work Requests and post them with

	 * a single ib_post_send() call.

	/* Strong send queue ordering guarantees that when the

	 * last WR in the chain completes, all WRs in the chain

	 * are complete. The last completion will wake up the

	 * RPC waiter.

	/* Transport disconnect drains the receive CQ before it

	 * replaces the QP. The RPC reply handler won't call us

	 * unless re_id->qp is a valid pointer.

 On error, the MRs get destroyed once the QP has drained. */

	/* The final LOCAL_INV WR in the chain is supposed to

	 * do the wake. If it was never posted, the wake does

	 * not happen. Unpin the rqst in preparation for its

	 * retransmission.

	/* Force a connection loss to ensure complete recovery.

/**

 * frwr_wp_create - Create an MR for padding Write chunks

 * @r_xprt: transport resources to use

 *

 * Return 0 on success, negative errno on failure.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2015-2020, Oracle and/or its affiliates.

 *

 * Support for reverse-direction RPCs on RPC/RDMA.

/**

 * xprt_rdma_bc_setup - Pre-allocate resources for handling backchannel requests

 * @xprt: transport associated with these backchannel resources

 * @reqs: number of concurrent incoming requests to expect

 *

 * Returns 0 on success; otherwise a negative errno

/**

 * xprt_rdma_bc_maxpayload - Return maximum backchannel message size

 * @xprt: transport

 *

 * Returns maximum size, in bytes, of a backchannel message

/**

 * xprt_rdma_bc_send_reply - marshal and send a backchannel reply

 * @rqst: RPC rqst with a backchannel RPC reply in rq_snd_buf

 *

 * Caller holds the transport's write lock.

 *

 * Returns:

 *	%0 if the RPC message has been sent

 *	%-ENOTCONN if the caller should reconnect and call again

 *	%-EIO if a permanent error occurred and the request was not

 *		sent. Do not try to send this message again.

/**

 * xprt_rdma_bc_destroy - Release resources for handling backchannel requests

 * @xprt: transport associated with these backchannel resources

 * @reqs: number of incoming requests to destroy; ignored

/**

 * xprt_rdma_bc_free_rqst - Release a backchannel rqst

 * @rqst: request to release

	/* Set a limit to prevent a remote from overrunning our resources.

/**

 * rpcrdma_bc_receive_call - Handle a reverse-direction Call

 * @r_xprt: transport receiving the call

 * @rep: receive buffer containing the call

 *

 * Operational assumptions:

 *    o Backchannel credits are ignored, just as the NFS server

 *      forechannel currently does

 *    o The ULP manages a replay cache (eg, NFSv4.1 sessions).

 *      No replay detection is done at the transport level

	/* The receive buffer has to be hooked to the rpcrdma_req

	 * so that it is not released while the req is pointing

	 * to its buffer, and so that it can be reposted after

	 * the Upper Layer is done decoding it.

 Queue rqst for ULP's callback service */

	/* This receive buffer gets reposted automatically

	 * when the connection is re-established.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2016-2018 Oracle.  All rights reserved.

 *

 * Use the core R/W API to move RPC-over-RDMA Read and Write chunks.

/* Each R/W context contains state for one chain of RDMA Read or

 * Write Work Requests.

 *

 * Each WR chain handles a single contiguous server-side buffer,

 * because scatterlist entries after the first have to start on

 * page alignment. xdr_buf iovecs cannot guarantee alignment.

 *

 * Each WR chain handles only one R_key. Each RPC-over-RDMA segment

 * from a client may contain a unique R_key, so each WR chain moves

 * up to one segment at a time.

 *

 * The scatterlist makes this data structure over 4KB in size. To

 * make it less likely to fail, and to handle the allocation for

 * smaller I/O requests without disabling bottom-halves, these

 * contexts are created on demand, but cached and reused until the

 * controlling svcxprt_rdma is destroyed.

/**

 * svc_rdma_destroy_rw_ctxts - Free accumulated R/W contexts

 * @rdma: transport about to be destroyed

 *

/**

 * svc_rdma_rw_ctx_init - Prepare a R/W context for I/O

 * @rdma: controlling transport instance

 * @ctxt: R/W context to prepare

 * @offset: RDMA offset

 * @handle: RDMA tag/handle

 * @direction: I/O direction

 *

 * Returns on success, the number of WQEs that will be needed

 * on the workqueue, or a negative errno.

/* A chunk context tracks all I/O for moving one Read or Write

 * chunk. This is a set of rdma_rw's that handle data movement

 * for all segments of one chunk.

 *

 * These are small, acquired with a single allocator call, and

 * no more than one is needed per chunk. They are allocated on

 * demand, and not cached.

/*

 * The consumed rw_ctx's are cleaned and placed on a local llist so

 * that only one atomic llist operation is needed to put them all

 * back on the free list.

/* State for sending a Write or Reply chunk.

 *  - Tracks progress of writing one chunk over all its segments

 *  - Stores arguments for the SGL constructor functions

 write state of this chunk */

 SGL constructor arguments */

/**

 * svc_rdma_write_done - Write chunk completion

 * @cq: controlling Completion Queue

 * @wc: Work Completion

 *

 * Pages under I/O are freed by a subsequent Send completion.

/* State for pulling a Read chunk.

/**

 * svc_rdma_wc_read_done - Handle completion of an RDMA Read ctx

 * @cq: controlling Completion Queue

 * @wc: Work Completion

 *

/* This function sleeps when the transport's Send Queue is congested.

 *

 * Assumptions:

 * - If ib_post_send() succeeds, only one completion is expected,

 *   even if one or more WRs are flushed. This is true when posting

 *   an rdma_rw_ctx or when posting a single signaled WR.

 If even one was posted, there will be a completion. */

/* Build and DMA-map an SGL that covers one kvec in an xdr_buf

/* Build and DMA-map an SGL that covers part of an xdr_buf's pagelist.

/* Construct RDMA Write WRs to send a portion of an xdr_buf containing

 * an RPC Reply.

/**

 * svc_rdma_iov_write - Construct RDMA Writes from an iov

 * @info: pointer to write arguments

 * @iov: kvec to write

 *

 * Returns:

 *   On success, returns zero

 *   %-E2BIG if the client-provided Write chunk is too small

 *   %-ENOMEM if a resource has been exhausted

 *   %-EIO if an rdma-rw error occurred

/**

 * svc_rdma_pages_write - Construct RDMA Writes from pages

 * @info: pointer to write arguments

 * @xdr: xdr_buf with pages to write

 * @offset: offset into the content of @xdr

 * @length: number of bytes to write

 *

 * Returns:

 *   On success, returns zero

 *   %-E2BIG if the client-provided Write chunk is too small

 *   %-ENOMEM if a resource has been exhausted

 *   %-EIO if an rdma-rw error occurred

/**

 * svc_rdma_xb_write - Construct RDMA Writes to write an xdr_buf

 * @xdr: xdr_buf to write

 * @data: pointer to write arguments

 *

 * Returns:

 *   On success, returns zero

 *   %-E2BIG if the client-provided Write chunk is too small

 *   %-ENOMEM if a resource has been exhausted

 *   %-EIO if an rdma-rw error occurred

/**

 * svc_rdma_send_write_chunk - Write all segments in a Write chunk

 * @rdma: controlling RDMA transport

 * @chunk: Write chunk provided by the client

 * @xdr: xdr_buf containing the data payload

 *

 * Returns a non-negative number of bytes the chunk consumed, or

 *	%-E2BIG if the payload was larger than the Write chunk,

 *	%-EINVAL if client provided too many segments,

 *	%-ENOMEM if rdma_rw context pool was exhausted,

 *	%-ENOTCONN if posting failed (connection is lost),

 *	%-EIO if rdma_rw initialization failed (DMA mapping, etc).

/**

 * svc_rdma_send_reply_chunk - Write all segments in the Reply chunk

 * @rdma: controlling RDMA transport

 * @rctxt: Write and Reply chunks from client

 * @xdr: xdr_buf containing an RPC Reply

 *

 * Returns a non-negative number of bytes the chunk consumed, or

 *	%-E2BIG if the payload was larger than the Reply chunk,

 *	%-EINVAL if client provided too many segments,

 *	%-ENOMEM if rdma_rw context pool was exhausted,

 *	%-ENOTCONN if posting failed (connection is lost),

 *	%-EIO if rdma_rw initialization failed (DMA mapping, etc).

/**

 * svc_rdma_build_read_segment - Build RDMA Read WQEs to pull one RDMA segment

 * @info: context for ongoing I/O

 * @segment: co-ordinates of remote memory to be read

 *

 * Returns:

 *   %0: the Read WR chain was constructed successfully

 *   %-EINVAL: there were not enough rq_pages to finish

 *   %-ENOMEM: allocating a local resources failed

 *   %-EIO: a DMA mapping error occurred

 Safety check */

/**

 * svc_rdma_build_read_chunk - Build RDMA Read WQEs to pull one RDMA chunk

 * @info: context for ongoing I/O

 * @chunk: Read chunk to pull

 *

 * Return values:

 *   %0: the Read WR chain was constructed successfully

 *   %-EINVAL: there were not enough resources to finish

 *   %-ENOMEM: allocating a local resources failed

 *   %-EIO: a DMA mapping error occurred

/**

 * svc_rdma_copy_inline_range - Copy part of the inline content into pages

 * @info: context for RDMA Reads

 * @offset: offset into the Receive buffer of region to copy

 * @remaining: length of region to copy

 *

 * Take a page at a time from rqstp->rq_pages and copy the inline

 * content from the Receive buffer into that page. Update

 * info->ri_pageno and info->ri_pageoff so that the next RDMA Read

 * result will land contiguously with the copied content.

 *

 * Return values:

 *   %0: Inline content was successfully copied

 *   %-EINVAL: offset or length was incorrect

/**

 * svc_rdma_read_multiple_chunks - Construct RDMA Reads to pull data item Read chunks

 * @info: context for RDMA Reads

 *

 * The chunk data lands in rqstp->rq_arg as a series of contiguous pages,

 * like an incoming TCP call.

 *

 * Return values:

 *   %0: RDMA Read WQEs were successfully built

 *   %-EINVAL: client provided too many chunks or segments,

 *   %-ENOMEM: rdma_rw context pool was exhausted,

 *   %-ENOTCONN: posting failed (connection is lost),

 *   %-EIO: rdma_rw initialization failed (DMA mapping, etc).

/**

 * svc_rdma_read_data_item - Construct RDMA Reads to pull data item Read chunks

 * @info: context for RDMA Reads

 *

 * The chunk data lands in the page list of rqstp->rq_arg.pages.

 *

 * Currently NFSD does not look at the rqstp->rq_arg.tail[0] kvec.

 * Therefore, XDR round-up of the Read chunk and trailing

 * inline content must both be added at the end of the pagelist.

 *

 * Return values:

 *   %0: RDMA Read WQEs were successfully built

 *   %-EINVAL: client provided too many chunks or segments,

 *   %-ENOMEM: rdma_rw context pool was exhausted,

 *   %-ENOTCONN: posting failed (connection is lost),

 *   %-EIO: rdma_rw initialization failed (DMA mapping, etc).

	/* Split the Receive buffer between the head and tail

	 * buffers at Read chunk's position. XDR roundup of the

	 * chunk is not included in either the pagelist or in

	 * the tail.

	/* Read chunk may need XDR roundup (see RFC 8166, s. 3.4.5.2).

	 *

	 * If the client already rounded up the chunk length, the

	 * length does not change. Otherwise, the length of the page

	 * list is increased to include XDR round-up.

	 *

	 * Currently these chunks always start at page offset 0,

	 * thus the rounded-up length never crosses a page boundary.

/**

 * svc_rdma_read_chunk_range - Build RDMA Read WQEs for portion of a chunk

 * @info: context for RDMA Reads

 * @chunk: parsed Call chunk to pull

 * @offset: offset of region to pull

 * @length: length of region to pull

 *

 * Return values:

 *   %0: RDMA Read WQEs were successfully built

 *   %-EINVAL: there were not enough resources to finish

 *   %-ENOMEM: rdma_rw context pool was exhausted,

 *   %-ENOTCONN: posting failed (connection is lost),

 *   %-EIO: rdma_rw initialization failed (DMA mapping, etc).

/**

 * svc_rdma_read_call_chunk - Build RDMA Read WQEs to pull a Long Message

 * @info: context for RDMA Reads

 *

 * Return values:

 *   %0: RDMA Read WQEs were successfully built

 *   %-EINVAL: there were not enough resources to finish

 *   %-ENOMEM: rdma_rw context pool was exhausted,

 *   %-ENOTCONN: posting failed (connection is lost),

 *   %-EIO: rdma_rw initialization failed (DMA mapping, etc).

/**

 * svc_rdma_read_special - Build RDMA Read WQEs to pull a Long Message

 * @info: context for RDMA Reads

 *

 * The start of the data lands in the first page just after the

 * Transport header, and the rest lands in rqstp->rq_arg.pages.

 *

 * Assumptions:

 *	- A PZRC is never sent in an RDMA_MSG message, though it's

 *	  allowed by spec.

 *

 * Return values:

 *   %0: RDMA Read WQEs were successfully built

 *   %-EINVAL: client provided too many chunks or segments,

 *   %-ENOMEM: rdma_rw context pool was exhausted,

 *   %-ENOTCONN: posting failed (connection is lost),

 *   %-EIO: rdma_rw initialization failed (DMA mapping, etc).

/**

 * svc_rdma_process_read_list - Pull list of Read chunks from the client

 * @rdma: controlling RDMA transport

 * @rqstp: set of pages to use as Read sink buffers

 * @head: pages under I/O collect here

 *

 * The RPC/RDMA protocol assumes that the upper layer's XDR decoders

 * pull each Read chunk as they decode an incoming RPC message.

 *

 * On Linux, however, the server needs to have a fully-constructed RPC

 * message in rqstp->rq_arg when there is a positive return code from

 * ->xpo_recvfrom. So the Read list is safety-checked immediately when

 * it is received, then here the whole Read list is pulled all at once.

 * The ingress RPC message is fully reconstructed once all associated

 * RDMA Reads have completed.

 *

 * Return values:

 *   %1: all needed RDMA Reads were posted successfully,

 *   %-EINVAL: client provided too many chunks or segments,

 *   %-ENOMEM: rdma_rw context pool was exhausted,

 *   %-ENOTCONN: posting failed (connection is lost),

 *   %-EIO: rdma_rw initialization failed (DMA mapping, etc).

 rq_respages starts after the last arg page */

 Ensure svc_rdma_recv_ctxt_put() does not try to release pages */

 SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause

/*

 * Copyright (c) 2014-2017 Oracle.  All rights reserved.

 * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the BSD-type

 * license below:

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions

 * are met:

 *

 *      Redistributions of source code must retain the above copyright

 *      notice, this list of conditions and the following disclaimer.

 *

 *      Redistributions in binary form must reproduce the above

 *      copyright notice, this list of conditions and the following

 *      disclaimer in the documentation and/or other materials provided

 *      with the distribution.

 *

 *      Neither the name of the Network Appliance, Inc. nor the names of

 *      its contributors may be used to endorse or promote products

 *      derived from this software without specific prior written

 *      permission.

 *

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT

 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,

 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT

 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,

 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY

 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

/*

 * transport.c

 *

 * This file contains the top-level implementation of an RPC RDMA

 * transport.

 *

 * Naming convention: functions beginning with xprt_ are part of the

 * transport switch. All others are RPC RDMA internal.

/*

 * tunables

/**

 * xprt_rdma_connect_worker - establish connection in the background

 * @work: worker thread context

 *

 * Requester holds the xprt's send lock to prevent activity on this

 * transport while a fresh connection is being established. RPC tasks

 * sleep on the xprt's pending queue waiting for connect to complete.

/**

 * xprt_rdma_inject_disconnect - inject a connection fault

 * @xprt: transport context

 *

 * If @xprt is connected, disconnect it to simulate spurious

 * connection loss. Caller must hold @xprt's send lock to

 * ensure that data structures and hardware resources are

 * stable during the rdma_disconnect() call.

/**

 * xprt_rdma_destroy - Full tear down of transport

 * @xprt: doomed transport context

 *

 * Caller guarantees there will be no more calls to us with

 * this @xprt.

 60 second timeout, no retries */

/**

 * xprt_setup_rdma - Set up transport to use RDMA

 *

 * @args: rpc transport arguments

 privileged port not needed */

	/*

	 * Set up RDMA-specific connect data.

	/* Ensure xprt->addr holds valid server TCP (not RDMA)

/**

 * xprt_rdma_close - close a transport connection

 * @xprt: transport context

 *

 * Called during autoclose or device removal.

 *

 * Caller holds @xprt's send lock to prevent activity on this

 * transport while the connection is torn down.

/**

 * xprt_rdma_set_port - update server port with rpcbind result

 * @xprt: controlling RPC transport

 * @port: new port value

 *

 * Transport connect status is unchanged.

/**

 * xprt_rdma_timer - invoked when an RPC times out

 * @xprt: controlling RPC transport

 * @task: RPC task that timed out

 *

 * Invoked when the transport is still connected, but an RPC

 * retransmit timeout occurs.

 *

 * Since RDMA connections don't have a keep-alive, forcibly

 * disconnect and retry to connect. This drives full

 * detection of the network path, and retransmissions of

 * all pending RPCs.

/**

 * xprt_rdma_set_connect_timeout - set timeouts for establishing a connection

 * @xprt: controlling transport instance

 * @connect_timeout: reconnect timeout after client disconnects

 * @reconnect_timeout: reconnect timeout after server disconnects

 *

/**

 * xprt_rdma_connect - schedule an attempt to reconnect

 * @xprt: transport state

 * @task: RPC scheduler context (unused)

 *

/**

 * xprt_rdma_alloc_slot - allocate an rpc_rqst

 * @xprt: controlling RPC transport

 * @task: RPC task requesting a fresh rpc_rqst

 *

 * tk_status values:

 *	%0 if task->tk_rqstp points to a fresh rpc_rqst

 *	%-EAGAIN if no rpc_rqst is available; queued on backlog

/**

 * xprt_rdma_free_slot - release an rpc_rqst

 * @xprt: controlling RPC transport

 * @rqst: rpc_rqst to release

 *

/**

 * xprt_rdma_allocate - allocate transport resources for an RPC

 * @task: RPC task

 *

 * Return values:

 *        0:	Success; rq_buffer points to RPC buffer to use

 *   ENOMEM:	Out of memory, call again later

 *      EIO:	A permanent error occurred, do not retry

/**

 * xprt_rdma_free - release resources allocated by xprt_rdma_allocate

 * @task: RPC task

 *

 * Caller guarantees rqst->rq_buffer is non-NULL.

	/* XXX: If the RPC is completing because of a signal and

	 * not because a reply was received, we ought to ensure

	 * that the Send completion has fired, so that memory

	 * involved with the Send is not still visible to the NIC.

/**

 * xprt_rdma_send_request - marshal and send an RPC request

 * @rqst: RPC message in rq_snd_buf

 *

 * Caller holds the transport's write lock.

 *

 * Returns:

 *	%0 if the RPC message has been sent

 *	%-ENOTCONN if the caller should reconnect and call again

 *	%-EAGAIN if the caller should call again

 *	%-ENOBUFS if the caller should call again after a delay

 *	%-EMSGSIZE if encoding ran out of buffer space. The request

 *		was not sent. Do not try to send this message again.

 *	%-EIO if an I/O error occurred. The request was not sent.

 *		Do not try to send this message again.

 CONFIG_SUNRPC_BACKCHANNEL */

 Must suppress retransmit to maintain credits */

	/* An RPC with no reply will throw off credit accounting,

	 * so drop the connection to reset the credit grant.

 need a local port? */

/*

 * Plumbing for rpc transport switch and kernel module

 sunrpc/xprt.c */

 ditto */

 ditto */

 sunrpc/rpcb_clnt.c */

 SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause

/*

 * Copyright (c) 2016-2018 Oracle. All rights reserved.

 * Copyright (c) 2014 Open Grid Computing, Inc. All rights reserved.

 * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the BSD-type

 * license below:

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions

 * are met:

 *

 *      Redistributions of source code must retain the above copyright

 *      notice, this list of conditions and the following disclaimer.

 *

 *      Redistributions in binary form must reproduce the above

 *      copyright notice, this list of conditions and the following

 *      disclaimer in the documentation and/or other materials provided

 *      with the distribution.

 *

 *      Neither the name of the Network Appliance, Inc. nor the names of

 *      its contributors may be used to endorse or promote products

 *      derived from this software without specific prior written

 *      permission.

 *

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT

 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,

 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT

 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,

 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY

 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 * Author: Tom Tucker <tom@opengridcomputing.com>

/* Operation

 *

 * The main entry point is svc_rdma_recvfrom. This is called from

 * svc_recv when the transport indicates there is incoming data to

 * be read. "Data Ready" is signaled when an RDMA Receive completes,

 * or when a set of RDMA Reads complete.

 *

 * An svc_rqst is passed in. This structure contains an array of

 * free pages (rq_pages) that will contain the incoming RPC message.

 *

 * Short messages are moved directly into svc_rqst::rq_arg, and

 * the RPC Call is ready to be processed by the Upper Layer.

 * svc_rdma_recvfrom returns the length of the RPC Call message,

 * completing the reception of the RPC Call.

 *

 * However, when an incoming message has Read chunks,

 * svc_rdma_recvfrom must post RDMA Reads to pull the RPC Call's

 * data payload from the client. svc_rdma_recvfrom sets up the

 * RDMA Reads using pages in svc_rqst::rq_pages, which are

 * transferred to an svc_rdma_recv_ctxt for the duration of the

 * I/O. svc_rdma_recvfrom then returns zero, since the RPC message

 * is still not yet ready.

 *

 * When the Read chunk payloads have become available on the

 * server, "Data Ready" is raised again, and svc_recv calls

 * svc_rdma_recvfrom again. This second call may use a different

 * svc_rqst than the first one, thus any information that needs

 * to be preserved across these two calls is kept in an

 * svc_rdma_recv_ctxt.

 *

 * The second call to svc_rdma_recvfrom performs final assembly

 * of the RPC Call message, using the RDMA Read sink pages kept in

 * the svc_rdma_recv_ctxt. The xdr_buf is copied from the

 * svc_rdma_recv_ctxt to the second svc_rqst. The second call returns

 * the length of the completed RPC Call message.

 *

 * Page Management

 *

 * Pages under I/O must be transferred from the first svc_rqst to an

 * svc_rdma_recv_ctxt before the first svc_rdma_recvfrom call returns.

 *

 * The first svc_rqst supplies pages for RDMA Reads. These are moved

 * from rqstp::rq_pages into ctxt::pages. The consumed elements of

 * the rq_pages array are set to NULL and refilled with the first

 * svc_rdma_recvfrom call returns.

 *

 * During the second svc_rdma_recvfrom call, RDMA Read sink pages

 * are transferred from the svc_rdma_recv_ctxt to the second svc_rqst.

/**

 * svc_rdma_recv_ctxts_destroy - Release all recv_ctxt's for an xprt

 * @rdma: svcxprt_rdma being torn down

 *

/**

 * svc_rdma_recv_ctxt_get - Allocate a recv_ctxt

 * @rdma: controlling svcxprt_rdma

 *

 * Returns a recv_ctxt or (rarely) NULL if none are available.

/**

 * svc_rdma_recv_ctxt_put - Return recv_ctxt to free list

 * @rdma: controlling svcxprt_rdma

 * @ctxt: object to return to the free list

 *

/**

 * svc_rdma_release_rqst - Release transport-specific per-rqst resources

 * @rqstp: svc_rqst being released

 *

 * Ensure that the recv_ctxt is released whether or not a Reply

 * was sent. For example, the client could close the connection,

 * or svc_process could drop an RPC, before the Reply is sent.

	/* Since we're destroying the xprt, no need to reset

/**

 * svc_rdma_post_recvs - Post initial set of Recv WRs

 * @rdma: fresh svcxprt_rdma

 *

 * Returns true if successful, otherwise false.

/**

 * svc_rdma_wc_receive - Invoked by RDMA provider for each polled Receive WC

 * @cq: Completion Queue context

 * @wc: Work Completion object

 *

 WARNING: Only wc->wr_cqe and wc->status are reliable */

	/* If receive posting fails, the connection is about to be

	 * lost anyway. The server will not be able to send a reply

	 * for this RPC, and the client will retransmit this RPC

	 * anyway when it reconnects.

	 *

	 * Therefore we drop the Receive, even if status was SUCCESS

	 * to reduce the likelihood of replayed requests once the

	 * client reconnects.

 All wc fields are now known to be valid */

 Note the unlock pairs with the smp_rmb in svc_xprt_ready: */

/**

 * svc_rdma_flush_recv_queues - Drain pending Receive work

 * @rdma: svcxprt_rdma being shut down

 *

/**

 * xdr_count_read_segments - Count number of Read segments in Read list

 * @rctxt: Ingress receive context

 * @p: Start of an un-decoded Read list

 *

 * Before allocating anything, ensure the ingress Read list is safe

 * to use.

 *

 * The segment count is limited to how many segments can fit in the

 * transport header without overflowing the buffer. That's about 40

 * Read segments for a 1KB inline threshold.

 *

 * Return values:

 *   %true: Read list is valid. @rctxt's xdr_stream is updated to point

 *	    to the first byte past the Read list. rc_read_pcl and

 *	    rc_call_pcl cl_count fields are set to the number of

 *	    Read segments in the list.

 *  %false: Read list is corrupt. @rctxt's xdr_stream is left in an

 *	    unknown state.

/* Sanity check the Read list.

 *

 * Sanity checks:

 * - Read list does not overflow Receive buffer.

 * - Chunk size limited by largest NFS data payload.

 *

 * Return values:

 *   %true: Read list is valid. @rctxt's xdr_stream is updated

 *	    to point to the first byte past the Read list.

 *  %false: Read list is corrupt. @rctxt's xdr_stream is left

 *	    in an unknown state.

 A bogus segcount causes this buffer overflow check to fail. */

/**

 * xdr_count_write_chunks - Count number of Write chunks in Write list

 * @rctxt: Received header and decoding state

 * @p: start of an un-decoded Write list

 *

 * Before allocating anything, ensure the ingress Write list is

 * safe to use.

 *

 * Return values:

 *       %true: Write list is valid. @rctxt's xdr_stream is updated

 *		to point to the first byte past the Write list, and

 *		the number of Write chunks is in rc_write_pcl.cl_count.

 *      %false: Write list is corrupt. @rctxt's xdr_stream is left

 *		in an indeterminate state.

/* Sanity check the Write list.

 *

 * Implementation limits:

 * - This implementation currently supports only one Write chunk.

 *

 * Sanity checks:

 * - Write list does not overflow Receive buffer.

 * - Chunk size limited by largest NFS data payload.

 *

 * Return values:

 *       %true: Write list is valid. @rctxt's xdr_stream is updated

 *		to point to the first byte past the Write list.

 *      %false: Write list is corrupt. @rctxt's xdr_stream is left

 *		in an unknown state.

/* Sanity check the Reply chunk.

 *

 * Sanity checks:

 * - Reply chunk does not overflow Receive buffer.

 * - Chunk size limited by largest NFS data payload.

 *

 * Return values:

 *       %true: Reply chunk is valid. @rctxt's xdr_stream is updated

 *		to point to the first byte past the Reply chunk.

 *      %false: Reply chunk is corrupt. @rctxt's xdr_stream is left

 *		in an unknown state.

/* RPC-over-RDMA Version One private extension: Remote Invalidation.

 * Responder's choice: requester signals it can handle Send With

 * Invalidate, and responder chooses one R_key to invalidate.

 *

 * If there is exactly one distinct R_key in the received transport

 * header, set rc_inv_rkey to that R_key. Otherwise, set it to zero.

/**

 * svc_rdma_xdr_decode_req - Decode the transport header

 * @rq_arg: xdr_buf containing ingress RPC/RDMA message

 * @rctxt: state of decoding

 *

 * On entry, xdr->head[0].iov_base points to first byte of the

 * RPC-over-RDMA transport header.

 *

 * On successful exit, head[0] points to first byte past the

 * RPC-over-RDMA header. For RDMA_MSG, this is the RPC message.

 *

 * The length of the RPC-over-RDMA header is returned.

 *

 * Assumptions:

 * - The transport header is entirely contained in the head iovec.

/* By convention, backchannel calls arrive via rdma_msg type

 * messages, and never populate the chunk lists. This makes

 * the RPC/RDMA header small and fixed in size, so it is

 * straightforward to check the RPC header's direction field.

 RPC call direction */

/**

 * svc_rdma_recvfrom - Receive an RPC call

 * @rqstp: request structure into which to receive an RPC Call

 *

 * Returns:

 *	The positive number of bytes in the RPC Call message,

 *	%0 if there were no Calls ready to return,

 *	%-EINVAL if the Read chunk data is too large,

 *	%-ENOMEM if rdma_rw context pool was exhausted,

 *	%-ENOTCONN if posting failed (connection is lost),

 *	%-EIO if rdma_rw initialization failed (DMA mapping, etc).

 *

 * Called in a loop when XPT_DATA is set. XPT_DATA is cleared only

 * when there are no remaining ctxt's to process.

 *

 * The next ctxt is removed from the "receive" lists.

 *

 * - If the ctxt completes a Read, then finish assembling the Call

 *   message and return the number of bytes in the message.

 *

 * - If the ctxt completes a Receive, then construct the Call

 *   message from the contents of the Receive buffer.

 *

 *   - If there are no Read chunks in this message, then finish

 *     assembling the Call message and return the number of bytes

 *     in the message.

 *

 *   - If there are Read chunks in this message, post Read WRs to

 *     pull that payload and return 0.

 No new incoming requests, terminate the loop */

 Unblock the transport for the next receive */

	/* Prevent svc_xprt_release from releasing pages in rq_pages

	 * if we return 0 or an error.

 SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause

/*

 * Copyright (c) 2016-2018 Oracle. All rights reserved.

 * Copyright (c) 2014 Open Grid Computing, Inc. All rights reserved.

 * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.

 *

 * This software is available to you under a choice of one of two

 * licenses.  You may choose to be licensed under the terms of the GNU

 * General Public License (GPL) Version 2, available from the file

 * COPYING in the main directory of this source tree, or the BSD-type

 * license below:

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions

 * are met:

 *

 *      Redistributions of source code must retain the above copyright

 *      notice, this list of conditions and the following disclaimer.

 *

 *      Redistributions in binary form must reproduce the above

 *      copyright notice, this list of conditions and the following

 *      disclaimer in the documentation and/or other materials provided

 *      with the distribution.

 *

 *      Neither the name of the Network Appliance, Inc. nor the names of

 *      its contributors may be used to endorse or promote products

 *      derived from this software without specific prior written

 *      permission.

 *

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT

 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,

 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT

 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,

 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY

 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 * Author: Tom Tucker <tom@opengridcomputing.com>

/* Operation

 *

 * The main entry point is svc_rdma_sendto. This is called by the

 * RPC server when an RPC Reply is ready to be transmitted to a client.

 *

 * The passed-in svc_rqst contains a struct xdr_buf which holds an

 * XDR-encoded RPC Reply message. sendto must construct the RPC-over-RDMA

 * transport header, post all Write WRs needed for this Reply, then post

 * a Send WR conveying the transport header and the RPC message itself to

 * the client.

 *

 * svc_rdma_sendto must fully transmit the Reply before returning, as

 * the svc_rqst will be recycled as soon as sendto returns. Remaining

 * resources referred to by the svc_rqst are also recycled at that time.

 * Therefore any resources that must remain longer must be detached

 * from the svc_rqst and released later.

 *

 * Page Management

 *

 * The I/O that performs Reply transmission is asynchronous, and may

 * complete well after sendto returns. Thus pages under I/O must be

 * removed from the svc_rqst before sendto returns.

 *

 * The logic here depends on Send Queue and completion ordering. Since

 * the Send WR is always posted last, it will always complete last. Thus

 * when it completes, it is guaranteed that all previous Write WRs have

 * also completed.

 *

 * Write WRs are constructed and posted. Each Write segment gets its own

 * svc_rdma_rw_ctxt, allowing the Write completion handler to find and

 * DMA-unmap the pages under I/O for that Write segment. The Write

 * completion handler does not release any pages.

 *

 * When the Send WR is constructed, it also gets its own svc_rdma_send_ctxt.

 * The ownership of all of the Reply's pages are transferred into that

 * ctxt, the Send WR is posted, and sendto returns.

 *

 * The svc_rdma_send_ctxt is presented when the Send WR completes. The

 * Send completion handler finally releases the Reply's pages.

 *

 * This mechanism also assumes that completions on the transport's Send

 * Completion Queue do not run in parallel. Otherwise a Write completion

 * and Send completion running at the same time could release pages that

 * are still DMA-mapped.

 *

 * Error Handling

 *

 * - If the Send WR is posted successfully, it will either complete

 *   successfully, or get flushed. Either way, the Send completion

 *   handler releases the Reply's pages.

 * - If the Send WR cannot be not posted, the forward path releases

 *   the Reply's pages.

 *

 * This handles the case, without the use of page reference counting,

 * where two different Write segments send portions of the same page.

/**

 * svc_rdma_send_ctxts_destroy - Release all send_ctxt's for an xprt

 * @rdma: svcxprt_rdma being torn down

 *

/**

 * svc_rdma_send_ctxt_get - Get a free send_ctxt

 * @rdma: controlling svcxprt_rdma

 *

 * Returns a ready-to-use send_ctxt, or NULL if none are

 * available and a fresh one cannot be allocated.

/**

 * svc_rdma_send_ctxt_put - Return send_ctxt to free list

 * @rdma: controlling svcxprt_rdma

 * @ctxt: object to return to the free list

	/* The first SGE contains the transport header, which

	 * remains mapped until @ctxt is destroyed.

/**

 * svc_rdma_wake_send_waiters - manage Send Queue accounting

 * @rdma: controlling transport

 * @avail: Number of additional SQEs that are now available

 *

/**

 * svc_rdma_wc_send - Invoked by RDMA provider for each polled Send WC

 * @cq: Completion Queue context

 * @wc: Work Completion object

 *

 * NB: The svc_xprt/svcxprt_rdma is pinned whenever it's possible that

 * the Send completion handler could be running.

/**

 * svc_rdma_send - Post a single Send WR

 * @rdma: transport on which to post the WR

 * @ctxt: send ctxt with a Send WR ready to post

 *

 * Returns zero if the Send WR was posted successfully. Otherwise, a

 * negative errno is returned.

 Sync the transport header buffer */

 If the SQ is full, wait until an SQ entry is available */

/**

 * svc_rdma_encode_read_list - Encode RPC Reply's Read chunk list

 * @sctxt: Send context for the RPC Reply

 *

 * Return values:

 *   On success, returns length in bytes of the Reply XDR buffer

 *   that was consumed by the Reply Read list

 *   %-EMSGSIZE on XDR buffer overflow

 RPC-over-RDMA version 1 replies never have a Read list. */

/**

 * svc_rdma_encode_write_segment - Encode one Write segment

 * @sctxt: Send context for the RPC Reply

 * @chunk: Write chunk to push

 * @remaining: remaining bytes of the payload left in the Write chunk

 * @segno: which segment in the chunk

 *

 * Return values:

 *   On success, returns length in bytes of the Reply XDR buffer

 *   that was consumed by the Write segment, and updates @remaining

 *   %-EMSGSIZE on XDR buffer overflow

/**

 * svc_rdma_encode_write_chunk - Encode one Write chunk

 * @sctxt: Send context for the RPC Reply

 * @chunk: Write chunk to push

 *

 * Copy a Write chunk from the Call transport header to the

 * Reply transport header. Update each segment's length field

 * to reflect the number of bytes written in that segment.

 *

 * Return values:

 *   On success, returns length in bytes of the Reply XDR buffer

 *   that was consumed by the Write chunk

 *   %-EMSGSIZE on XDR buffer overflow

/**

 * svc_rdma_encode_write_list - Encode RPC Reply's Write chunk list

 * @rctxt: Reply context with information about the RPC Call

 * @sctxt: Send context for the RPC Reply

 *

 * Return values:

 *   On success, returns length in bytes of the Reply XDR buffer

 *   that was consumed by the Reply's Write list

 *   %-EMSGSIZE on XDR buffer overflow

 Terminate the Write list */

/**

 * svc_rdma_encode_reply_chunk - Encode RPC Reply's Reply chunk

 * @rctxt: Reply context with information about the RPC Call

 * @sctxt: Send context for the RPC Reply

 * @length: size in bytes of the payload in the Reply chunk

 *

 * Return values:

 *   On success, returns length in bytes of the Reply XDR buffer

 *   that was consumed by the Reply's Reply chunk

 *   %-EMSGSIZE on XDR buffer overflow

 *   %-E2BIG if the RPC message is larger than the Reply chunk

/**

 * svc_rdma_page_dma_map - DMA map one page

 * @data: pointer to arguments

 * @page: struct page to DMA map

 * @offset: offset into the page

 * @len: number of bytes to map

 *

 * Returns:

 *   %0 if DMA mapping was successful

 *   %-EIO if the page cannot be DMA mapped

/**

 * svc_rdma_iov_dma_map - DMA map an iovec

 * @data: pointer to arguments

 * @iov: kvec to DMA map

 *

 * ib_dma_map_page() is used here because svc_rdma_dma_unmap()

 * handles DMA-unmap and it uses ib_dma_unmap_page() exclusively.

 *

 * Returns:

 *   %0 if DMA mapping was successful

 *   %-EIO if the iovec cannot be DMA mapped

/**

 * svc_rdma_xb_dma_map - DMA map all segments of an xdr_buf

 * @xdr: xdr_buf containing portion of an RPC message to transmit

 * @data: pointer to arguments

 *

 * Returns:

 *   %0 if DMA mapping was successful

 *   %-EIO if DMA mapping failed

 *

 * On failure, any DMA mappings that have been already done must be

 * unmapped by the caller.

/**

 * svc_rdma_xb_count_sges - Count how many SGEs will be needed

 * @xdr: xdr_buf containing portion of an RPC message to transmit

 * @data: pointer to arguments

 *

 * Returns:

 *   Number of SGEs needed to Send the contents of @xdr inline

/**

 * svc_rdma_pull_up_needed - Determine whether to use pull-up

 * @rdma: controlling transport

 * @sctxt: send_ctxt for the Send WR

 * @rctxt: Write and Reply chunks provided by client

 * @xdr: xdr_buf containing RPC message to transmit

 *

 * Returns:

 *   %true if pull-up must be used

 *   %false otherwise

 Resources needed for the transport header */

/**

 * svc_rdma_xb_linearize - Copy region of xdr_buf to flat buffer

 * @xdr: xdr_buf containing portion of an RPC message to copy

 * @data: pointer to arguments

 *

 * Returns:

 *   Always zero.

/**

 * svc_rdma_pull_up_reply_msg - Copy Reply into a single buffer

 * @rdma: controlling transport

 * @sctxt: send_ctxt for the Send WR; xprt hdr is already prepared

 * @rctxt: Write and Reply chunks provided by client

 * @xdr: prepared xdr_buf containing RPC message

 *

 * The device is not capable of sending the reply directly.

 * Assemble the elements of @xdr into the transport header buffer.

 *

 * Assumptions:

 *  pull_up_needed has determined that @xdr will fit in the buffer.

 *

 * Returns:

 *   %0 if pull-up was successful

 *   %-EMSGSIZE if a buffer manipulation problem occurred

/* svc_rdma_map_reply_msg - DMA map the buffer holding RPC message

 * @rdma: controlling transport

 * @sctxt: send_ctxt for the Send WR

 * @rctxt: Write and Reply chunks provided by client

 * @xdr: prepared xdr_buf containing RPC message

 *

 * Returns:

 *   %0 if DMA mapping was successful.

 *   %-EMSGSIZE if a buffer manipulation problem occurred

 *   %-EIO if DMA mapping failed

 *

 * The Send WR's num_sge field is set in all cases.

 Set up the (persistently-mapped) transport header SGE. */

	/* If there is a Reply chunk, nothing follows the transport

	 * header, and we're done here.

	/* For pull-up, svc_rdma_send() will sync the transport header.

	 * No additional DMA mapping is necessary.

/* Prepare the portion of the RPC Reply that will be transmitted

 * via RDMA Send. The RPC-over-RDMA transport header is prepared

 * in sc_sges[0], and the RPC xdr_buf is prepared in following sges.

 *

 * Depending on whether a Write list or Reply chunk is present,

 * the server may send all, a portion of, or none of the xdr_buf.

 * In the latter case, only the transport header (sc_sges[0]) is

 * transmitted.

 *

 * RDMA Send is the last step of transmitting an RPC reply. Pages

 * involved in the earlier RDMA Writes are here transferred out

 * of the rqstp and into the sctxt's page array. These pages are

 * DMA unmapped by each Write completion, but the subsequent Send

 * completion finally releases these pages.

 *

 * Assumptions:

 * - The Reply's transport header will never be larger than a page.

/**

 * svc_rdma_send_error_msg - Send an RPC/RDMA v1 error response

 * @rdma: controlling transport context

 * @sctxt: Send context for the response

 * @rctxt: Receive context for incoming bad message

 * @status: negative errno indicating error that occurred

 *

 * Given the client-provided Read, Write, and Reply chunks, the

 * server was not able to parse the Call or form a complete Reply.

 * Return an RDMA_ERROR message so the client can retire the RPC

 * transaction.

 *

 * The caller does not have to release @sctxt. It is released by

 * Send completion, or by this function on error.

 Remote Invalidation is skipped for simplicity. */

/**

 * svc_rdma_sendto - Transmit an RPC reply

 * @rqstp: processed RPC request, reply XDR already in ::rq_res

 *

 * Any resources still associated with @rqstp are released upon return.

 * If no reply message was possible, the connection is closed.

 *

 * Returns:

 *	%0 if an RPC reply has been successfully posted,

 *	%-ENOMEM if a resource shortage occurred (connection is lost),

 *	%-ENOTCONN if posting failed (connection is lost).

	/* Prevent svc_xprt_release() from releasing the page backing

	 * rq_res.head[0].iov_base. It's no longer being accessed by

/**

 * svc_rdma_result_payload - special processing for a result payload

 * @rqstp: svc_rqst to operate on

 * @offset: payload's byte offset in @xdr

 * @length: size of payload, in bytes

 *

 * Return values:

 *   %0 if successful or nothing needed to be done

 *   %-EMSGSIZE on XDR buffer overflow

 *   %-E2BIG if the payload was larger than the Write chunk

 *   %-EINVAL if client provided too many segments

 *   %-ENOMEM if rdma_rw context pool was exhausted

 *   %-ENOTCONN if posting failed (connection is lost)

 *   %-EIO if rdma_rw initialization failed (DMA mapping, etc)

 SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause

/*

 * Copyright (c) 2015, 2017 Oracle.  All rights reserved.

/* rpcrdma.ko module initialization

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2015-2018 Oracle.  All rights reserved.

 *

 * Support for reverse-direction RPCs on RPC/RDMA (server-side).

/**

 * svc_rdma_handle_bc_reply - Process incoming backchannel Reply

 * @rqstp: resources for handling the Reply

 * @rctxt: Received message

 *

 don't deadlock */

/* Send a reverse-direction RPC Call.

 *

 * Caller holds the connection's mutex and has already marshaled

 * the RPC/RDMA request.

 *

 * This is similar to svc_rdma_send_reply_msg, but takes a struct

 * rpc_rqst instead, does not support chunks, and avoids blocking

 * memory allocation.

 *

 * XXX: There is still an opportunity to block in svc_rdma_send()

 * if there are no SQ entries to post the Send. This may occur if

 * the adapter has a small maximum SQ depth.

	/* Bump page refcnt so Send completion doesn't release

	 * the rq_buffer before all retransmits are complete.

/* Server-side transport endpoint wants a whole page for its send

 * buffer. The client RPC code constructs the RPC header in this

 * buffer before it invokes ->send_request.

/**

 * xprt_rdma_bc_send_request - Send a reverse-direction Call

 * @rqst: rpc_rqst containing Call message to be sent

 *

 * Return values:

 *   %0 if the message was sent successfully

 *   %ENOTCONN if the message was not sent

/* It shouldn't matter if the number of backchannel session slots

 * doesn't match the number of RPC/RDMA credits. That just means

 * one or the other will have extra slots that aren't used.

 Final put for backchannel xprt is in __svc_rdma_free */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2017 Facebook

 We're done. */

 During iteration: we've been cancelled, abort. */

 During iteration: we need to reschedule between runs. */

 Do another round. */

	/* Clamp copy if the user has provided a size hint, but copy the full

	 * buffer if not to retain old behaviour.

/* Integer types of various sizes and pointer combinations cover variety of

 * architecture dependent calling conventions. 7+ can be supported in the

 * future.

 doesn't support data_in/out, ctx_out, duration, or repeat */

		/* smp_call_function_single() also checks cpu_online()

		 * after csd_lock(). However, since cpu is from user

		 * space, let's do an extra quick check to filter out

		 * invalid value before smp_call_function_single().

/**

 * range_is_zero - test whether buffer is initialized

 * @buf: buffer to check

 * @from: check from this position

 * @to: check up until (excluding) this position

 *

 * This function returns true if the there is a non-zero byte

 * in the buf in the range [from,to).

 make sure the fields we don't use are zeroed */

 mark is allowed */

 priority is allowed */

 ingress_ifindex is allowed */

 ifindex is allowed */

 cb is allowed */

 tstamp is allowed */

 wire_len is allowed */

 gso_segs is allowed */

 gso_size is allowed */

 hwtstamp is allowed */

 bpf program can never convert linear skb to non-linear */

		/* The device is now tracked in the xdp->rxq for later

		 * dev_put()

 There can't be user provided data before the meta data */

 Meta data is allocated from the headroom */

 XDP have extra tailroom as (most) drivers use full page */

	/* We convert the xdp_buff back to an xdp_md before checking the return

	 * code so the reference count of any held netdevice will be decremented

	 * even if the test run failed.

 make sure the fields we don't use are zeroed */

 flags is allowed */

 doesn't support data_in/out, ctx_out, duration, or repeat or flags */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2021. Huawei Technologies Co., Ltd

 A common type for test_N with return value in bpf_dummy_ops */

 args[0] is 0 means state argument of test_N will be NULL */

 state needs to be NULL if args[0] is 0 */

/*

 * The following program is used to generate the constants for

 * computing sched averages.

 *

 * ==============================================================

 *		C program (compile with -lm)

 * ==============================================================

 To silence -Wunused-but-set-variable warnings. */

 first period */

			/*

			 * This is the same as:

			 * max = max*y + 1024;

	printf("#define LOAD_AVG_MAX_N %d\n\n", n);

 Generated by Documentation/scheduler/sched-pelt; do not modify. */\n\n");

	calc_runnable_avg_yN_sum();

	calc_accumulated_sum_32();

 For building without an updated set of headers */

	/*

	 * check if dropping privileges is supported,

	 * bail on systems where the capability is not present

	/*

	 * Drop privileges but keep the ability to claim all

	 * free interfaces (i.e., those not used by kernel drivers)

/attr/exec

/{gpe,use_global_lock,io}

/gpe`):

ftp.suse.com/pub/people/trenn/sources/ec/

/QEMU0001:00/capability for MMIO

git.qemu.org/?p=qemu.git;a=blob_plain;f=docs/specs/pvpanic.txt

/QEMU0001:00/events

 Request EC info type 3 (EC firmware build date)

 Corresponds with sending type 0x00f0 with

 MBOX = [38, 00, 03, 00]

 View the result. The decoded ASCII result "12/21/18" is

 included after the raw hex.

 Corresponds with MBOX = [00, 00, 31, 32, 2f, 32, 31, 38, ...]

What:		/sys/devices/system/edac/mc/mc*/reset_counters

Date:		January 2006

Contact:	linux-edac@vger.kernel.org

Description:	This write-only control file will zero all the statistical

		counters for UE and CE errors on the given memory controller.

		Zeroing the counters will also reset the timer indicating how

		long since the last counter were reset. This is useful for

		computing errors/time.  Since the counters are always reset

		at driver initialization time, no module/kernel parameter

		is available.



What:		/sys/devices/system/edac/mc/mc*/mc_name

Date:		January 2006

Contact:	linux-edac@vger.kernel.org

Description:	This attribute file displays the type of memory controller

		that is being utilized.



What:		/sys/devices/system/edac/mc/mc*/ue_count

Date:		January 2006

Contact:	linux-edac@vger.kernel.org

Description:	This attribute file displays the total count of uncorrectable

		errors that have occurred on this memory controller. If

		panic_on_ue is set, this counter will not have a chance to

		increment, since EDAC will panic the system



What:		/sys/devices/system/edac/mc/mc*/ce_count

Date:		January 2006

Contact:	linux-edac@vger.kernel.org

Description:	This attribute file displays the total count of correctable

		errors that have occurred on this memory controller. This

		count is very important to examine. CEs provide early

		indications that a DIMM is beginning to fail. This count

		field should be monitored for non-zero values and report

		such information to the system administrator.



What:		/sys/devices/system/edac/mc/mc*/sdram_scrub_rate

Date:		February 2007

Contact:	linux-edac@vger.kernel.org

Description:	Read/Write attribute file that controls memory scrubbing.

		The scrubbing rate used by the memory controller is set by

		writing a minimum bandwidth in bytes/sec to the attribute file.

		The rate will be translated to an internal value that gives at

		least the specified rate.

		Reading the file will return the actual scrubbing rate employed.

		If configuration fails or memory scrubbing is not implemented,

		the value of the attribute file will be -1.



		For dimm*/size, this is the size, in MB of the DIMM memory

/interface_capabilities

/device_capabilities

/usb488_interface_capabilities

/usb488_device_capabilities

	* Install driver files into */drivers/scsi/lpfc instead of

/*

 * Nios2 TLB handling

 *

 * Copyright (C) 2009, Wind River Systems Inc

 *   Implemented by fredrik.markstrom@gmail.com and ivarholmqvist@gmail.com

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * This provides a PTEADDR value for addr that will cause a TLB miss

 * (fast TLB miss). TLB invalidation replaces entries with this value.

/*

 * This one is only used for pages with the global bit set so we don't care

 * much about the ASID.

 remember pid/way until we return. */

		/*

		 * There should be only a single entry that maps a

		 * particular {address,pid} so break after a match.

/*

 * This one is only used for pages with the global bit set so we don't care

 * much about the ASID.

 remember pid/way until we return. */

 remember pid/way until we return */

 remember pid/way until we return */

/*

 * All entries common to a mm share an asid.  To effectively flush these

 * entries, we just bump the asid.

 remember pid/way until we return */

 Start at way 0, way is auto-incremented after each TLBACC write */

	/* Map each TLB entry to physcal address 0 with no-access and a

 restore pid/way */

/*

 * MMU context handling.

 *

 * Copyright (C) 2011 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2009 Wind River Systems Inc

 *   Implemented by fredrik.markstrom@gmail.com and ivarholmqvist@gmail.com

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 The pids position and mask in context */

 The versions position and mask in context */

 Return the version part of a context */

 Return the pid part of a context */

 Value of the first context (version 1, pid 0) */

/*

 * Initialize MMU context management stuff.

	/* We need to set this here because the value depends on runtime data

/*

 * Set new context (pid), keep way

 Return the next pid */

	/* If the pid field wraps around we increase the version and

		/* Version is incremented since the pid increment above

	/* If the version wraps we start over with the first generation, we do

	/* If the process context we are swapping in has a different context

 Save the current pgd so the fast tlb handler can find it */

 Set the current context */

/*

 * After we have set current->mm to a new value, this activates

 * the context for the new mm so we see the new mappings.

/*

 * Copyright (C) 2013 Altera Corporation

 * Copyright (C) 2010 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2009 Wind River Systems Inc

 *   Implemented by fredrik.markstrom@gmail.com and ivarholmqvist@gmail.com

 * Copyright (C) 2004 Microtronix Datacom Ltd

 *

 * based on arch/m68k/mm/init.c

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License. See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * paging_init() continues the virtual memory environment setup which

 * was begun by the code in arch/head.S.

 * The parameters are pointers to where to stick the starting and ending

 * addresses of available kernel virtual memory.

 pass the memory from the bootmem allocator to the main allocator */

	unsigned long end_mem   = memory_end; /* this must not include

 this will put all memory onto the freelists */

 Copy kuser helpers */

 Map kuser helpers to user space address */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2009, Wind River Systems Inc

 * Implemented by fredrik.markstrom@gmail.com and ivarholmqvist@gmail.com

 Bail if we try to copy zero bytes  */

 Copy byte by byte for small copies and if src^dst != 0 */

  If 'to' is an odd address byte copy */

 If 'to' is not divideable by four copy halfwords */

 Copy words */

 Copy remaining bytes */

/*

 * Copyright (C) 2009 Wind River Systems Inc

 *   Implemented by fredrik.markstrom@gmail.com and ivarholmqvist@gmail.com

 *

 * based on arch/mips/mm/fault.c which is:

 *

 * Copyright (C) 1995-2000 Ralf Baechle

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 Supervisor only instruction address */

 Supervisor only data address */

 TLB permission violation (x) */

 TLB permission violation (r) */

 TLB permission violation (w) */

/*

 * This routine handles page faults.  It determines the address,

 * and the problem, and then passes it off to one of the appropriate

 * routines.

 Restart the instruction */

	/*

	 * We fault-in kernel-space virtual memory on-demand. The

	 * 'reference' page table is init_mm.pgd.

	 *

	 * NOTE! We MUST NOT take any locks for this case. We may

	 * be in an interrupt or a critical region, and should

	 * only copy the information from the master page table,

	 * nothing more.

	/*

	 * If we're in an interrupt or have no user

	 * context, we must not take the fault..

/*

 * Ok, we have a good vm_area for this memory access, so

 * we can handle it..

	/*

	 * If for any reason at all we couldn't handle the fault,

	 * make sure we exit gracefully rather than endlessly redo

	 * the fault.

			/*

			 * No need to mmap_read_unlock(mm) as we would

			 * have already released it in __lock_page_or_retry

			 * in mm/filemap.c.

/*

 * Something tried to access memory that isn't in our memory map..

 * Fix it, but check if it's kernel or user first..

 User mode accesses just cause a SIGSEGV */

 Are we prepared to handle this kernel fault? */

	/*

	 * Oops. The kernel tried to access some bad page. We'll have to

	 * terminate things with extreme prejudice.

/*

 * We ran out of memory, or some other thing happened to us that made

 * us unable to handle the page fault gracefully.

 Kernel mode? Handle exceptions or die */

		/*

		 * Synchronize this task's top level page-table

		 * with the 'reference' page table.

		 *

		 * Do _not_ use "tsk" here. We might be inside

		 * an interrupt in the middle of a task switch..

/*

 * Copyright (C) 2009 Wind River Systems Inc

 *   Implemented by fredrik.markstrom@gmail.com and ivarholmqvist@gmail.com

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/* pteaddr:

 *   ptbase | vpn* | zero

 *   31-22  | 21-2 | 1-0

 *

 *   *vpn is preserved on double fault

 *

 * tlbacc:

 *   IG   |*flags| pfn

 *   31-25|24-20 | 19-0

 *

 *   *crwxg

 *

 * tlbmisc:

 *   resv  |way   |rd | we|pid |dbl|bad|perm|d

 *   31-24 |23-20 |19 | 20|17-4|3  |2  |1   |0

 *

/*

 * Initialize a new pgd / pmd table with invalid pointers.

 Initialize the entire pgd.  */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2009, Wind River Systems Inc

 * Implemented by fredrik.markstrom@gmail.com and ivarholmqvist@gmail.com

 Outputs */

 Inputs  */ "r"(addr)

 : No clobber */);

 Outputs */

 Inputs  */ "r"(addr)

 : No clobber */);

 Outputs */

 Inputs  */ "r"(addr)

 : No clobber */);

	/*

	 * Writeback any data associated with the kernel mapping of this

	 * page.  This ensures that data in the physical page is mutually

	 * coherent with the kernels mapping.

	/*

	 * The zero page is never written to, so never has any dirty

	 * cache lines, and therefore never needs to be flushed.

 Flush this page if there are aliases. */

	/*

	* The zero page is never written to, so never has any dirty

	* cache lines, and therefore never needs to be flushed.

/*

 * Copyright (C) 2011 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2009 Wind River Systems Inc

 *  Implemented by fredrik.markstrom@gmail.com and ivarholmqvist@gmail.com

 *

 * Based on DMA code from MIPS.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

		/*

		 * We just need to flush the caches here , but Nios2 flush

		 * instruction will do both writeback and invalidate.

 flush and invalidate */

/*

 * Copyright (C) 2010, Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2009, Wind River Systems Inc

 *   Implemented by fredrik.markstrom@gmail.com and ivarholmqvist@gmail.com

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * Copyright (C) 2010 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2009 Wind River Systems Inc

 *   Implemented by fredrik.markstrom@gmail.com and ivarholmqvist@gmail.com

 * Copyright (C) 2004 Microtronix Datacom Ltd.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License. See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * Map some physical address range into the kernel address space.

 Don't allow wraparound or zero size */

 Don't allow anybody to remap normal RAM that we're using */

	/*

	 * Map uncached objects in the low part of address space to

	 * CONFIG_NIOS2_IO_REGION_BASE

 Mappings have to be page-aligned */

 Ok, go for it */

/*

 * iounmap unmaps nearly everything, so be careful

 * it doesn't free currently pointer/page tables anymore but it

 * wasn't used anyway and might be added later.

/*

 * Architecture-dependent parts of process handling.

 *

 * Copyright (C) 2013 Altera Corporation

 * Copyright (C) 2010 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2009 Wind River Systems Inc

 *   Implemented by fredrik.markstrom@gmail.com and ivarholmqvist@gmail.com

 * Copyright (C) 2004 Microtronix Datacom Ltd

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * The development boards have no way to pull a board reset. Just jump to the

 * cpu reset address and let the boot loader or the code in head.S take care of

 * resetting peripherals.

/*

 * There is no way to power off the development boards. So just spin for now. If

 * we ever have a way of resetting a board using a GPIO we should add that here.

 fn */

 Set the return value for the child. */

 Initialize tls register. */

/*

 *	Generic dumping code. Used for panic and debug.

 ;dgt2 */

 ;dgt2;tmp */

 ;dgt2;tmp */

/*

 * Do necessary setup to start up a newly executed thread.

 * Will startup in user mode (status_extension = 0).

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2013 Altera Corporation

 * Copyright (C) 2011 Tobias Klauser <tklauser@distanz.ch>

 *

 * Based on cpuinfo.c from microblaze

/*

 * Get CPU information for use by the procfs.

 CONFIG_PROC_FS */

/*

 * Hardware exception handling

 *

 * Copyright (C) 2010 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2004 Microtronix Datacom Ltd.

 * Copyright (C) 2001 Vic Phillips

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License.  See the file COPYING in the main directory of this

 * archive for more details.

	/*

	 * do_exit() should take care of panic'ing from an interrupt

	 * context so we don't handle it here

/*

 * The show_stack() is external API which we do not use ourselves.

		/*

		 * If the address is either in the text segment of the

		 * kernel, or in the region which contains vmalloc'ed

		 * memory, it *may* be the address of a calling

		 * routine; if so, print it so that someone tracing

		 * down the cause of the crash will be able to figure

		 * out the call path that was taken.

 Breakpoint handler */

	/*

	 * The breakpoint entry code has moved the PC on by 4 bytes, so we must

	 * move it back. This could be done on the host but we do it here

	 * because monitor.S of JTAG gdbserver does it too.

 Alignment exception handler */

 CONFIG_NIOS2_ALIGNMENT_TRAP */

 Illegal instruction handler */

 Supervisor instruction handler */

 Division error handler */

 Unhandled exception handler */

/*

 * Nios2-specific parts of system setup

 *

 * Copyright (C) 2010 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2004 Microtronix Datacom Ltd.

 * Copyright (C) 2001 Vic Phillips <vic@microtronix.com>

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License. See the file "COPYING" in the main directory of this archive

 * for more details.

 Copy a short hook instruction sequence to the exception address */

 The CPU exception address already points to the handler. */

 no output registers */

 Copy the fast TLB miss handler */

 no output registers */

/*

 * save args passed from u-boot, called from head.S

 *

 * @r4: NIOS magic

 * @r5: initrd start

 * @r6: initrd end or fdt

 * @r7: kernel command line

 r4 is magic NIOS */

 initramfs */

 CONFIG_BLK_DEV_INITRD */

 Keep a copy of command line */

 CONFIG_BLK_DEV_INITRD */

	/*

	 * Initialize MMU context handling here because data from cpuinfo is

	 * needed for this.

	/*

	 * get kmalloc into gear

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Device tree support

 *

 * Copyright (C) 2013, 2015 Altera Corporation

 * Copyright (C) 2010 Thomas Chou <thomas@wytron.com.tw>

 *

 * Based on MIPS support for CONFIG_OF device tree support

 *

 * Copyright (C) 2010 Cisco Systems Inc. <dediao@cisco.com>

/*

 *  linux/arch/nios2/kernel/misaligned.c

 *

 *  basic emulation for mis-aligned accesses on the NIOS II cpu

 *  modelled after the version for arm in arm/alignment.c

 *

 *  Brad Parker <brad@heeltoe.com>

 *  Copyright (C) 2010 Ambient Corporation

 *  Copyright (c) 2010 Altera Corporation, San Jose, California, USA.

 *  Copyright (c) 2010 Arrow Electronics, Inc.

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License.  See the file COPYING in the main directory of

 * this archive for more details.

 instructions we emulate */

 see arch/nios2/include/asm/ptrace.h */

 struct pt_regs */

 struct switch_stack */

/*

 * (mis)alignment handler

 back up one instruction */

 do fixup if in kernel or mode turned on */

 decompose instruction */

 do fixup to saved registers */

	/*

	 * kernel mode -

	 *  note exception and skip bad instruction (return)

 show_regs(fp); */

	/*

	 * user mode -

	 *  possibly warn,

	 *  possibly send SIGBUS signal to process

 else advance */

 pre-calc offsets of registers on sys call stack frame */

 struct pt_regs */

 struct switch_stack */

 default mode - silent fix */

/*

 * Copyright (C) 2013-2014 Altera Corporation

 * Copyright (C) 2010 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2004 Microtronix Datacom Ltd.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License. See the file "COPYING" in the main directory of this archive

 * for more details.

 Counter is counting down */

 Only read timer if it has been initialized */

	/* The timer's actual period is one cycle greater than the value

 stop counter */

 write new count */

 Clear the interrupt condition */

 clear pending interrupt */

 interrupt disable + continuous + start */

 Calibrate the delay loop directly */

/*

 * The first timer instance will use as a clockevent. If there are two or

 * more instances, the second one gets used as clocksource and all

 * others are unused.

/*

 * Copyright (C) 2013 Altera Corporation

 * Copyright (C) 2011-2012 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2004 Microtronix Datacom Ltd.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 sys_cacheflush -- flush the processor cache. */

 We only support op 0 now, return error if op is non-zero.*/

 Check for overflow */

	/*

	 * Verify that the specified address region actually belongs

	 * to this process.

/*

 * Copyright (C) 2004 Microtronix Datacom Ltd

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License. See the file COPYING in the main directory of this

 * archive for more details.

 string functions */

 memory management */

/*

 * libgcc functions - functions that are used internally by the

 * compiler...  (prototypes are not correct though, but that

 * doesn't really matter since they're not versioned).

/*

 * Copyright (C) 2013-2014 Altera Corporation

 * Copyright (C) 2011-2012 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2004 Microtronix Datacom Ltd

 * Copyright (C) 1991, 1992 Linus Torvalds

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file COPYING in the main directory of this archive

 * for more details.

/*

 * Do a signal return; undo the signal stack.

 *

 * Keep the return code on the stack quadword aligned!

 * That makes the cache flush below easier.

 Always make any pending restarted system calls return -EINTR */

 restore passed registers */

 gregs[23] is handled below */

	err |= __get_user(sw->fp, &gregs[24]);  /* Verify, should this be

	err |= __get_user(sw->gp, &gregs[25]);  /* Verify, should this be

	err |= __get_user(temp, &gregs[26]);  /* Not really necessary no user

 disable syscall checks */

 Verify, can we follow the stack back */

 Default to using normal stack.  */

 This is the X/Open sanctioned signal stack switching.  */

 Verify, is it 32 or 64 bit aligned */

 Create the ucontext.  */

	/* Set up to return from userspace; jump to fixed address sigreturn

 Set up registers for signal handler */

/*

 * OK, we're invoking a handler

 set up the stack frame */

	/*

	 * If we were from a system call, check for system call restarting...

		/*

		 * Prepare for system call restart. We do this here so that a

		 * debugger will see the already changed PC.

 handler */

	/*

	 * No handler present

	/*

	* If there's no signal to deliver, we just put the saved sigmask back.

	/*

	 * We want the common case to go fast, which is why we may in certain

	 * cases get here from kernel mode. Just return without doing anything

	 * if so.

			/*

			 * Restart without handlers.

			 * Deal with it without leaving

			 * the kernel space.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Nios2 KGDB support

 *

 * Copyright (C) 2015 Altera Corporation

 * Copyright (C) 2011 Tobias Klauser <tklauser@distanz.ch>

 *

 * Based on the code posted by Kazuyasu on the Altera Forum at:

 * http://www.alteraforum.com/forum/showpost.php?p=77003&postcount=20

 handle the optional parameters */

 this means that we do not want to exit from the handler */

	/*

	 * The breakpoint entry code has moved the PC on by 4 bytes, so we must

	 * move it back.  This could be done on the host but we do it here

 pass the first trap 30 code */

 Nothing to do */

 Breakpoint instruction: trap 30 */

/*

 * Copyright (C) 2014 Altera Corporation

 * Copyright (C) 2010 Tobias Klauser <tklauser@distanz.ch>

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License.  See the file COPYING in the main directory of this

 * archive for more details.

 R0

 R1..R7

 R8..R15

 R16..R23

 et and bt */

 PTR_BA

 use ea for PC */

/*

 * Set the thread state from a regset passed in via ptrace

 et and bt */

 use ea for PC */

/*

 * Define the register sets available on Nios2 under Linux

/*

 * Kernel module support for Nios II.

 *

 * Copyright (C) 2004 Microtronix Datacom Ltd.

 *   Written by Wentao Xu <xuwentao@microtronix.com>

 * Copyright (C) 2001, 2003 Rusty Russell

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License.  See the file COPYING in the main directory of this

 * archive for more details.

/*

 * Modules should NOT be allocated with kmalloc for (obvious) reasons.

 * But we do it for now to avoid relocation issues. CALL26/PCREL26 cannot reach

 * from 0x80000000 (vmalloc area) to 0xc00000000 (kernel) (kmalloc returns

 * addresses in 0xc0000000)

 Free memory returned from module_alloc */

 This is where to make the change */

		/* This is the symbol it is referring to.  Note that all

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright Altera Corporation (C) 2013. All rights reserved

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2013 Altera Corporation

 * Copyright (C) 2011 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2008 Thomas Chou <thomas@wytron.com.tw>

 *

 * based on irq.c from m68k which is:

 *

 * Copyright (C) 2007 Greg Ungerer <gerg@snapgear.com>

 Load the initial ienable value */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2011 Tobias Klauser <tklauser@distanz.ch>

 struct task_struct */

 struct thread_struct */

 struct pt_regs */

 struct switch_stack */

 struct thread_info */

/* Extracted from GLIBC memcpy.c and memcopy.h, which is:

   Copyright (C) 1991, 1992, 1993, 1997, 2004 Free Software Foundation, Inc.

   This file is part of the GNU C Library.

   Contributed by Torbjorn Granlund (tege@sics.se).



   The GNU C Library is free software; you can redistribute it and/or

   modify it under the terms of the GNU Lesser General Public

   License as published by the Free Software Foundation; either

   version 2.1 of the License, or (at your option) any later version.



   The GNU C Library is distributed in the hope that it will be useful,

   but WITHOUT ANY WARRANTY; without even the implied warranty of

   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU

   Lesser General Public License for more details.



   You should have received a copy of the GNU Lesser General Public

   License along with the GNU C Library; if not, see

www.gnu.org/licenses/>.  */

/* Type to use for aligned memory operations.

   This should normally be the biggest type supported by a single load

 Optimal type for storing bytes in registers.  */

/* Copy exactly NBYTES bytes from SRC_BP to DST_BP,

/* Copy *up to* NBYTES bytes from SRC_BP to DST_BP, with

   the assumption that DST_BP is aligned on an OPSIZ multiple.  If

   not all bytes could be easily copied, store remaining number of bytes

 extern void _wordcopy_fwd_aligned __P ((long int, long int, size_t)); */

 extern void _wordcopy_fwd_dest_aligned __P ((long int, long int, size_t)); */

 Threshold value for when to enter the unrolled loops.  */

/* _wordcopy_fwd_aligned -- Copy block beginning at SRCP to

   block beginning at DSTP with LEN `op_t' words (not LEN bytes!).

 stream-lined (read x8 + write x8) */

/* _wordcopy_fwd_dest_aligned -- Copy block beginning at SRCP to

   block beginning at DSTP with LEN `op_t' words (not LEN bytes!).

   DSTP should be aligned for memory operations on `op_t's, but SRCP must

 stream-lined (read x4 + write x4) */

	/* Calculate how to shift a word read at the memory operation

	/* Make SRCP aligned by rounding it down to the beginning of the `op_t'

 Copy from the beginning to the end.  */

 If there not too few bytes to copy, use word copy.  */

 Copy just a few bytes to make DSTP aligned.  */

		/* Copy whole pages from SRCP to DSTP by virtual address

 PAGE_COPY_FWD_MAYBE (dstp, srcp, len, len); */

		/* Copy from SRCP to DSTP taking advantage of the known

		   alignment of DSTP. Number of bytes remaining is put in the

		   third argument, i.e. in LEN.  This number may vary from

 Fall out and copy the tail. */

 There are just a few bytes to copy.  Use byte memory operations. */

/*

 * Copyright (C) 2011 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2004 Microtronix Datacom Ltd

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 fill8 %3, %5 (c & 0xff) */

 Word-align %0 (s) if necessary */

 Dword-align %0 (s) if necessary */

 %1 and %2 are how many more bytes to set */

 %2 is how many dwords to set */

 store residual word and/or byte if necessary */

 store residual byte if necessary */

 %0  Output */

 %1  Output */

 %2  Output */

 %3  Output */

 %4  Output only */

 %5  Input */

 %0  Input/Output */

 %1  Input/Output */

 clobbered */

/*

 * Copyright (C) 2011 Tobias Klauser <tklauser@distanz.ch>

 * Copyright (C) 2004 Microtronix Datacom Ltd

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright Altera Corporation (C) 2014. All rights reserved.

 2**32 / 1000000 (rounded up) */

 2**32 / 1000000000 (rounded up) */

/*

 * Copyright (C) 2013 Altera Corporation

 * Copyright (C) 2011 Thomas Chou

 * Copyright (C) 2011 Walter Goossens

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License. See the file COPYING in the main directory of this

 * archive for more details.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Copyright (C) 2008-2010 Thomas Chou <thomas@wytron.com.tw>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2009 Thomas Chou <thomas@wytron.com.tw>

 *

 * This is a collection of several routines from gzip-1.0.3

 * adapted for Linux.

 *

 * malloc by Hannu Savolainen 1993 and Matthias Urlichs 1994

 *

 * Adapted for SH by Stuart Menefy, Aug 1999

 *

 * Modified to use standard LinuxSH BIOS by Greg Banks 7Jul2000

 *

 * Based on arch/sh/boot/compressed/misc.c

/*

 * gzip declarations

 Window size must be at least 32k, */

 and a power of two */

 input buffer */

 Sliding window buffer */

 valid bytes in inbuf */

 index of next byte to be processed in inbuf */

 bytes in output buffer */

 gzip flag byte */

 bit 0 set: file probably ASCII text */

#define CONTINUATION	0x02 /* bit 1 set: continuation of multi-part gzip

 bit 2 set: extra field present */

 bit 3 set: original file name present */

 bit 4 set: file comment present */

 bit 5 set: file is encrypted */

 bit 6,7:   reserved */

/*

 * Fill the input buffer. This is called only when the buffer is empty

 * and at least one byte is really needed.

/*

 * Write the output window window[0..outcnt-1] and update crc and bytes_out.

 * (Used for the decompressed data only.)

 temporary variable */

 Halt */

/*

 * arch/sh/math-emu/math.c

 *

 * Copyright (C) 2006 Takashi YOSHII <takasi-y@ops.dti.ne.jp>

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 2 args instructions.

EQ*/) == 0)

 to process fmov's extension (odd n for DR access XD).

 1 arg instructions.

/**

 * fpu_init - Initialize FPU registers

 * @fpu: Pointer to software emulated FPU registers.

/**

 * do_fpu_inst - Handle reserved instructions for FPU emulation

 * @inst: instruction code.

 * @regs: registers on stack.

 initialize once. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic push-switch framework

 *

 * Copyright (C) 2006  Paul Mundt

 Workqueue API brain-damage */

 SPDX--License-Identifier: GPL-2.0

 When driver_override is set, only bind to the matching driver */

 Then try to match against the id table */

 fall-back to driver name match */

/**

 * sh_early_platform_driver_register - register early platform driver

 * @epdrv: sh_early_platform driver structure

 * @buf: string passed from early_param()

 *

 * Helper function for sh_early_platform_init() / sh_early_platform_init_buffer()

	/* Simply add the driver to the end of the global list.

	 * Drivers will by default be put on the list in compiled-in order.

	/* If the user has specified device then make sure the driver

	 * gets prioritized. The driver of the last device specified on

	 * command line will be put first on the list.

 Allow passing parameters after device name */

/**

 * sh_early_platform_add_devices - adds a number of early platform devices

 * @devs: array of early platform devices to add

 * @num: number of early platform devices in array

 *

 * Used by early architecture code to register early platform devices and

 * their platform data.

 simply add the devices to list */

/**

 * sh_early_platform_driver_register_all - register early platform drivers

 * @class_str: string to identify early platform driver class

 *

 * Used by architecture code to register all early platform drivers

 * for a certain class. If omitted then only early platform drivers

 * with matching kernel command line class parameters will be registered.

	/* The "class_str" parameter may or may not be present on the kernel

	 * command line. If it is present then there may be more than one

	 * matching parameter.

	 *

	 * Since we register our early platform drivers using early_param()

	 * we need to make sure that they also get registered in the case

	 * when the parameter is missing from the kernel command line.

	 *

	 * We use parse_early_options() to make sure the early_param() gets

	 * called at least once. The early_param() may be called more than

	 * once since the name of the preferred device may be specified on

	 * the kernel command line. sh_early_platform_driver_register() handles

	 * this case for us.

/**

 * sh_early_platform_match - find early platform device matching driver

 * @epdrv: early platform driver structure

 * @id: id to match against

/**

 * sh_early_platform_left - check if early platform driver has matching devices

 * @epdrv: early platform driver structure

 * @id: return true if id or above exists

/**

 * sh_early_platform_driver_probe_id - probe drivers matching class_str and id

 * @class_str: string to identify early platform driver class

 * @id: id to match against

 * @nr_probe: number of platform devices to successfully probe before exiting

 only use drivers matching our class_str */

 skip requested id */

			/*

			 * Set up a sensible init_name to enable

			 * dev_name() and others to be used before the

			 * rest of the driver core is initialized.

/**

 * sh_early_platform_driver_probe - probe a class of registered drivers

 * @class_str: string to identify early platform driver class

 * @nr_probe: number of platform devices to successfully probe before exiting

 * @user_only: only probe user specified early platform devices

 *

 * Used by architecture code to probe registered early platform drivers

 * within a certain class. For probe to happen a registered early platform

 * device matching a registered early platform driver is needed.

/**

 * early_platform_cleanup - clean up early platform code

 clean up the devres list used to chain devices */

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic heartbeat driver for regular LED banks

 *

 * Copyright (C) 2007 - 2010  Paul Mundt

 *

 * Most SH reference boards include a number of individual LEDs that can

 * be independently controlled (either via a pre-defined hardware

 * function or via the LED class, if desired -- the hardware tends to

 * encapsulate some of the same "triggers" that the LED class supports,

 * so there's not too much value in it).

 *

 * Additionally, most of these boards also have a LED bank that we've

 * traditionally used for strobing the load average. This use case is

 * handled by this driver, rather than giving each LED bit position its

 * own struct device.

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7760 DMABRG IRQ handling

 *

 * (c) 2007 MSC Vertriebsges.m.b.H, Manuel Lauss <mlau@msc-ge.com>

/*

 * The DMABRG is a special DMA unit within the SH7760. It does transfers

 * from USB-SRAM/Audio units to main memory (and also the LCDC; but that

 * part is sensibly placed  in the LCDC  registers and requires no irqs)

 * It has 3 IRQ lines which trigger 10 events, and works independently

 * from the traditional SH DMAC (although it blocks usage of DMAC 0)

 *

 * BRGIRQID   | component | dir | meaning      | source

 * -----------------------------------------------------

 *     0      | USB-DMA   | ... | xfer done    | DMABRGI1

 *     1      | USB-UAE   | ... | USB addr err.| DMABRGI0

 *     2      | HAC0/SSI0 | play| all done     | DMABRGI1

 *     3      | HAC0/SSI0 | play| half done    | DMABRGI2

 *     4      | HAC0/SSI0 | rec | all done     | DMABRGI1

 *     5      | HAC0/SSI0 | rec | half done    | DMABRGI2

 *     6      | HAC1/SSI1 | play| all done     | DMABRGI1

 *     7      | HAC1/SSI1 | play| half done    | DMABRGI2

 *     8      | HAC1/SSI1 | rec | all done     | DMABRGI1

 *     9      | HAC1/SSI1 | rec | half done    | DMABRGI2

 *

 * all can be enabled/disabled in the DMABRGCR register,

 * as well as checked if they occurred.

 *

 * DMABRGI0 services  USB  DMA  Address  errors,  but it still must be

 * enabled/acked in the DMABRGCR register.  USB-DMA complete indicator

 * is grouped together with the audio buffer end indicators, too bad...

 *

 * DMABRGCR:	Bits 31-24: audio-dma ENABLE flags,

 *		Bits 23-16: audio-dma STATUS flags,

 *		Bits  9-8:  USB error/xfer ENABLE,

 *		Bits  1-0:  USB error/xfer STATUS.

 *	Ack an IRQ by writing 0 to the STATUS flag.

 *	Mask IRQ by writing 0 to ENABLE flag.

 *

 * Usage is almost like with any other IRQ:

 *  dmabrg_request_irq(BRGIRQID, handler, data)

 *  dmabrg_free_irq(BRGIRQID)

 *

 * handler prototype:  void brgirqhandler(void *data)

/*

 * main DMABRG irq handler. It acks irqs and then

 * handles every set and unmasked bit sequentially.

 * No locking and no validity checks; it should be

 * as fast as possible (audio!)

 ack all */

 ignore masked */

 USB stuff, get it out of the way first */

 Audio */

 request DMAC channel 0 before anyone else can get it */

 enable DMABRG in DMAC 0 */

 enable DMABRG mode, enable the DMAC */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/dma/dma-g2.c

 *

 * G2 bus DMA support

 *

 * Copyright (C) 2003 - 2006  Paul Mundt

 G2 bus address */

 Root bus (SH-4) address */

 Size (in bytes), 32-byte aligned */

 Transfer direction */

 Transfer control */

 Channel enable */

 Transfer enable */

 Transfer status */

 Align the count */

 Fixup destination */

 Fixup direction */

	/*

	 * bit 0 - ???

	 * bit 1 - if set, generate a hardware event on transfer completion

	 * bit 2 - ??? something to do with suspend?

 ?? */

 debug cruft */

 Magic */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/dma/dma-pvr2.c

 *

 * NEC PowerVR 2 (Dreamcast) DMA support

 *

 * Copyright (C) 2003, 2004  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/dma/dma-sh.c

 *

 * SuperH On-chip DMAC Support

 *

 * Copyright (C) 2000 Takashi YOSHII

 * Copyright (C) 2003, 2004 Paul Mundt

 * Copyright (C) 2005 Andriy Skulysh

/*

 * Define the default configuration for dual address memory-memory transfer.

 * The 0x400 value represents auto-request, external->external.

 Normalize offset calculation */

/*

 * We determine the correct shift size based off of the CHCR transmit size

 * for the given channel. Since we know that it will take:

 *

 *	info->count >> ts_shift[transmit_size]

 *

 * iterations to complete the transfer.

/*

 * The transfer end interrupt must read the chcr register to end the

 * hardware interrupt active condition.

 * Besides that it needs to waken any waiting process, which should handle

 * setting up the next transfer.

	/*

	 * If we haven't pre-configured the channel with special flags, use

	 * the defaults.

	/*

	 * Single-address mode usage note!

	 *

	 * It's important that we don't accidentally write any value to SAR/DAR

	 * (this includes 0) that hasn't been directly specified by the user if

	 * we're in single-address mode.

	 *

	 * In this case, only one address can be defined, anything else will

	 * result in a DMA address error interrupt (at least on the SH-4),

	 * which will subsequently halt the transfer.

	 *

	 * Channel 2 on the Dreamcast is a special case, as this is used for

	 * cascading to the PVR2 DMAC. In this case, we still need to write

	 * SAR and DAR, regardless of value, in order for cascading to work.

/*

 * DMAOR handling

/*

 * DMAOR bases are broken out amongst channel groups. DMAOR0 manages

 * channels 0 - 5, DMAOR1 6 - 11 (optional).

 Try to clear the error flags first, incase they are set */

 See if we got an error again */

/*

 * DMAE handling

	/*

	 * Initialize DMAE, for parts that support it.

	/*

	 * Initialize DMAOR, and clean up any error flags that may have

	 * been set.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/dma/dma-api.c

 *

 * SuperH-specific DMA management API

 *

 * Copyright (C) 2003, 2004, 2005  Paul Mundt

	/*

	 * Look for each DMAC's range to determine who the owner of

	 * the channel is.

/**

 * request_dma_bycap - Allocate a DMA channel based on its capabilities

 * @dmac: List of DMA controllers to search

 * @caps: List of capabilities

 *

 * Search all channels of all DMA controllers to find a channel which

 * matches the requested capabilities. The result is the channel

 * number if a match is found, or %-ENODEV if no match is found.

 *

 * Note that not all DMA controllers export capabilities, in which

 * case they can never be allocated using this API, and so

 * request_dma() must be used specifying the channel number.

	/*

	 * Iterate over each registered DMAC

		/*

		 * Iterate over each channel

	/*

	 * Don't touch pre-configured channels

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/dma/dma-sysfs.c

 *

 * sysfs interface for SH DMA API

 *

 * Copyright (C) 2004 - 2006  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/superhyway/ops-sh4-202.c

 *

 * SuperHyway bus support for SH4-202

 *

 * Copyright (C) 2005  Paul Mundt

	/*

	 * XXX: Even though the SH4-202 Evaluation Device documentation

	 * indicates that VCRL is mapped first with VCRH at a + 0x04

	 * offset, the opposite seems to be true.

	 *

	 * Some modules (PBR and ePBR for instance) also appear to have

	 * VCRL/VCRH flipped in the documentation, but on the SH4-202

	 * itself it appears that these are all consistently mapped with

	 * VCRH preceding VCRL.

	 *

	 * Do not trust the documentation, for it is evil.

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic SH-4 / SH-4A PCIC operations (SH7751, SH7780).

 *

 * Copyright (C) 2002 - 2009  Paul Mundt

/*

 * Direct access to PCI hardware...

/*

 * Functions for accessing PCI configuration space with type 1 accesses

	/*

	 * PCIPDR may only be accessed as 32 bit words,

	 * so we must do byte alignment by hand

/*

 * Since SH4 only does 32bit access we'll have to do a read,

 * mask,write operation.

 * We'll allow an odd byte offset, though it should be illegal.

 Nothing to do. */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/pci/fixups-dreamcast.c

 *

 * PCI fixups for the Sega Dreamcast

 *

 * Copyright (C) 2001, 2002  M. R. Brown

 * Copyright (C) 2002, 2003, 2006  Paul Mundt

 *

 * This file originally bore the message (with enclosed-$):

 *	Id: pci.c,v 1.3 2003/05/04 19:29:46 lethal Exp

 *	Dreamcast PCI: Supports SEGA Broadband Adaptor only.

		/*

		 * We also assume that dev->devfn == 0

		/*

		 * This is not a normal BAR, prevent any attempts to move

		 * the BAR, as this will result in a bus lock.

		/*

		 * Redirect dma memory allocations to special memory window.

		 *

		 * If this GAPSPCI region were mapped by a BAR, the CPU

		 * phys_addr_t would be pci_resource_start(), and the bus

		 * address would be pci_bus_address(pci_resource_start()).

		 * But apparently there's no BAR mapping it, so we just

		 * "know" its CPU address is GAPSPCI_DMA_BASE.

	/*

	 * The interrupt routing semantics here are quite trivial.

	 *

	 * We basically only support one interrupt, so we only bother

	 * updating a device's interrupt line with this single shared

	 * interrupt. Keeps routing quite simple, doesn't it?

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/pci/fixups-r7780rp.c

 *

 * Highlander R7780RP-1 PCI fixups

 *

 * Copyright (C) 2003  Lineo uSolutions, Inc.

 * Copyright (C) 2004 - 2006  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * PCI support for the Sega Dreamcast

 *

 * Copyright (C) 2001, 2002  M. R. Brown

 * Copyright (C) 2002, 2003  Paul Mundt

 *

 * This file originally bore the message (with enclosed-$):

 *	Id: pci.c,v 1.3 2003/05/04 19:29:46 lethal Exp

 *	Dreamcast PCI: Supports SEGA Broadband Adaptor only.

/*

 * gapspci init

	/*

	 * FIXME: All of this wants documenting to some degree,

	 * even some basic register definitions would be nice.

	 *

	 * I haven't seen anything this ugly since.. maple.

 Setting Broadband Adapter */

 SPDX-License-Identifier: GPL-2.0

 eth0       */

 eth1       */

 PCI bridge */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/pci/fixups-sdk7780.c

 *

 * PCI fixups for the SDK7780SE03

 *

 * Copyright (C) 2003  Lineo uSolutions, Inc.

 * Copyright (C) 2004 - 2006  Paul Mundt

 * Copyright (C) 2006  Nobuhiro Iwamatsu

 IDSEL [16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31] */

 INTA */

 INTB */

 INTC */

 INTD */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/pci/ops-titan.c

 *

 * Ported to new API by Paul Mundt <lethal@linux-sh.org>

 *

 * Modified from ops-snapgear.c written by  David McCullough

 * Highly leveraged from pci-bigsur.c, written by Dustin McIntire.

 *

 * PCI initialization for the Titan boards

 SPDX-License-Identifier: GPL-2.0

 AMD Ethernet controller */

/*

 * Only long word accesses of the PCIC's internal local registers and the

 * configuration registers from the CPU is supported.

/*

 * Description:  This function sets up and initializes the pcic, sets

 * up the BARS, maps the DRAM into the address space etc, etc.

	/*

	* Initialize the slave bus controller on the pcic.  The values used

	* here should not be hardcoded, but they should be taken from the bsc

	* on the processor, to make this function as generic as possible.

	* (i.e. Another sbc may usr different SDRAM timing settings -- in order

	* for the pcic to work, its settings need to be exactly the same.)

 Enable Bit 19, BREQEN */

 Enable Bit 19 BREQEN, set PCIC to slave */

 PCIC BCR1 */

 PCIC BCR2 */

 PCIC WCR1 */

 PCIC WCR2 */

 PCIC WCR3 */

 PCIC MCR */

 Enable all interrupts, so we know what to fix */

 Set up standard PCI config registers */

 Bus Master, Mem & I/O access */

 PCI Class code & Revision ID */

 PCI I/O address (local regs) */

 PCI MEM address (local RAM)  */

 PCI MEM address (unused)     */

 PCI Subsystem ID & Vendor ID */

 MEM (full 64M exposed)       */

 MEM (unused)                 */

 MEM (direct map from PCI)    */

 MEM (unused)                 */

 Now turn it on... */

	/*

	* Set PCIMBR and PCIIOBR here, assuming a single window

	* (16M MEM, 256K IO) is enough.  If a larger space is

	* needed, the readx/writex and inx/outx functions will

	* have to do more (e.g. setting registers for each call).

	/*

	* Set the MBR so PCI address is one-to-one with window,

	* meaning all calls go straight through... use BUG_ON to

	* catch erroneous assumption.

 Set IOBR for window containing area specified in pci.h */

 All done, may as well say so... */

 SPDX-License-Identifier: GPL-2.0

/*

 * New-style PCI core.

 *

 * Copyright (c) 2004 - 2009  Paul Mundt

 * Copyright (c) 2002  M. R. Brown

 *

 * Modelled after arch/mips/pci/pci.c:

 *  Copyright (C) 2003, 04 Ralf Baechle (ralf@linux-mips.org)

/*

 * The PCI controller list.

	/* Don't allow 8-bit bus number overflow inside the hose -

/*

 * This interrupt-safe spinlock protects all accesses to PCI

 * configuration space.

	/*

	 * Do not panic here but later - this might happen before console init.

	/*

	 * Setup the ERR/PERR and SERR timers, if available.

	/*

	 * Scan the bus if it is register after the PCI subsystem

	 * initialization.

 Scan all of the recorded PCI controllers.  */

/*

 * We need to avoid collisions with `mirrored' VGA ports

 * and other strange ISA hardware, so we always want the

 * addresses to be allocated in the 0x000-0x0ff region

 * modulo 0x400.

		/*

                 * Put everything into 0x00-0xff region modulo 0x400.

/*

 * We can't use pci_find_device() here since we are

 * called from interrupt context.

		/*

		 * ignore host bridge - we handle

		 * that separately

 clear the status errors */

 CONFIG_GENERIC_IOMAP */

 SPDX-License-Identifier: GPL-2.0

/*

 * Low-Level PCI Support for the SH7751

 *

 *  Copyright (C) 2003 - 2009  Paul Mundt

 *  Copyright (C) 2001  Dustin McIntire

 *

 *  With cleanup by Paul van Gool <pvangool@mimotech.com>, 2003.

 check BCR for SDRAM in area */

 check BCR2 for 32bit SDRAM interface*/

 check for SH7751/SH7751R hardware */

 Set the BCR's to enable PCI access */

 Turn the clocks back on (not done in reset)*/

 Clear Powerdown IRQ's (not done in reset) */

	/* set the command/status bits to:

	 * Wait Cycle Control + Parity Enable + Bus Master +

	 * Mem space enable

 define this host as the host bridge */

	/* Set IO and Mem windows to local address

	 * Make PCI and local address the same for easy 1 to 1 mapping

 Set the values on window 0 PCI config registers */

	/* Set the local 16MB PCI memory space window to

	 * the lowest PCI mapped address

	/* Make sure the MSB's of IO window are set to access PCI space

 Set PCI WCRx, BCRx's, copy from BSC locations */

 check BCR for SDRAM in specified area */

 configure the wait control registers */

	/* NOTE: I'm ignoring the PCI error IRQs for now..

	 * TODO: add support for the internal error interrupts and

	 * DMA interrupts...

 SH7751 init done, set central function init complete */

 use round robin mode to stop a device starving/overruning */

 SPDX-License-Identifier: GPL-2.0

/*

 * Low-Level PCI Support for the SH7780

 *

 *  Copyright (C) 2005 - 2010  Paul Mundt

		/*

		 * 32-bit only resources must be last.

	/*

	 * Handle status errors.

	/*

	 * Handle arbiter errors.

	/*

	 * Handle the remaining PCI errors.

 Deassert SERR */

 Back off the IRQ for awhile */

 Clear out PCI arbiter IRQs */

 Clear all error conditions */

	/*

	 * The PCI ERR IRQ needs to be IRQF_SHARED since all of the power

	 * down IRQ vectors are routed through the ERR IRQ vector. We

	 * only request_irq() once as there is only a single masking

	 * source for multiple events.

 Unmask all of the arbiter IRQs. */

 Unmask all of the PCI IRQs */

 Enable register access */

 Enable 66MHz operation */

 Done */

 Enable CPU access to the PCIC registers. */

 Reset */

	/*

	 * Wait for it to come back up. The spec says to allow for up to

	 * 1 second after toggling the reset pin, but in practice 100ms

	 * is more than enough.

	/*

	 * Now throw it in to register initialization mode and

	 * start the real work.

	/*

	 * If there's more than 512MB of memory, we need to roll over to

	 * LAR1/LSR1.

		/*

		 * Otherwise just zero it out and disable it.

	/*

	 * LAR0/LSR0 covers up to the first 512MB, which is enough to

	 * cover all of lowmem on most platforms.

	/*

	 * Hook up the ERR and SERR IRQs.

	/*

	 * Disable the cache snoop controller for non-coherent DMA.

	/*

	 * Setup the memory BARs

		/*

		 * Make sure we're in the right physical addressing mode

		 * for dealing with the resource.

		/*

		 * The MBMR mask is calculated in units of 256kB, which

		 * keeps things pretty simple.

	/*

	 * And I/O.

	/*

	 * Initialization mode complete, release the control register and

	 * enable round robin mode to stop device overruns/starvation.

 SPDX-License-Identifier: GPL-2.0

/*

 * PCI operations for the Sega Dreamcast

 *

 * Copyright (C) 2001, 2002  M. R. Brown

 * Copyright (C) 2002, 2003  Paul Mundt

/*

 * The !gapspci_config_access case really shouldn't happen, ever, unless

 * someone implicitly messes around with the last devfn value.. otherwise we

 * only support a single device anyways, and if we didn't have a BBA, we

 * wouldn't make it terribly far through the PCI setup anyways.

 *

 * Also, we could very easily support both Type 0 and Type 1 configurations

 * here, but since it doesn't seem that there is any such implementation in

 * existence, we don't bother.

 *

 * I suppose if someone actually gets around to ripping the chip out of

 * the BBA and hanging some more devices off of it, then this might be

 * something to take into consideration. However, due to the cost of the BBA,

 * and the general lack of activity by DC hardware hackers, this doesn't seem

 * likely to happen anytime soon.

/*

 * We can also actually read and write in b/w/l sizes! Thankfully this part

 * was at least done right, and we don't have to do the stupid masking and

 * shifting that we do on the 7751! Small wonders never cease to amaze.

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic SH7786 PCI-Express operations.

 *

 *  Copyright (C) 2009 - 2010  Paul Mundt

	/*

	 * While each channel has its own memory-mapped extended config

	 * space, it's generally only accessible when in endpoint mode.

	 * When in root complex mode, the controller is unable to target

	 * itself with either type 0 or type 1 accesses, and indeed, any

	 * controller initiated target transfer to its own config space

	 * result in a completer abort.

	 *

	 * Each channel effectively only supports a single device, but as

	 * the same channel <-> device access works for any PCI_SLOT()

	 * value, we cheat a bit here and bind the controller's config

	 * space to devfn 0 in order to enable self-enumeration. In this

	 * case the regular PAR/PDR path is sidelined and the mangled

	 * config access itself is initiated as a SuperHyway transaction.

 Clear errors */

 Set the PIO address */

 Enable the configuration access */

 Check for errors */

 Check for master and target aborts */

 Disable the configuration access */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/pci/fixups-rts7751r2d.c

 *

 * RTS7751R2D / LBOXRE2 PCI fixups

 *

 * Copyright (C) 2003  Lineo uSolutions, Inc.

 * Copyright (C) 2004  Paul Mundt

 * Copyright (C) 2007  Nobuhiro Iwamatsu

 Enable Bit 19 BREQEN, set PCIC to slave */

 Enable all interrupts, so we known what to fix */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/pci/fixups-landisk.c

 *

 * PCI initialization for the I-O DATA Device, Inc. LANDISK board

 *

 * Copyright (C) 2006 kogiidena

 * Copyright (C) 2010 Nobuhiro Iwamatsu

	/*

	 * slot0: pin1-4 = irq5,6,7,8

	 * slot1: pin1-4 = irq6,7,8,5

	 * slot2: pin1-4 = irq7,8,5,6

	 * slot3: pin1-4 = irq8,5,6,7

 Enable Bit 19 BREQEN, set PCIC to slave */

 SPDX-License-Identifier: GPL-2.0

/*

 * These functions are used early on before PCI scanning is done

 * and all of the pci_dev and pci_bus structures have been created.

 Fake a parent bus structure. */

 check 66MHz capability */

/*

 * A simple handler for the regular PCI status errors, called from IRQ

 * context.

 Now back off of the IRQ for awhile */

 SPDX-License-Identifier: GPL-2.0

/*

 * SDK7786 FPGA PCIe mux handling

 *

 * Copyright (C) 2010  Paul Mundt

/*

 * The SDK7786 FPGA supports mangling of most of the slots in some way or

 * another. Slots 3/4 are special in that only one can be supported at a

 * time, and both appear on port 3 to the PCI bus scan. Enabling slot 4

 * (the horizontal edge connector) will disable slot 3 entirely.

 *

 * Misconfigurations can be detected through the FPGA via the slot

 * resistors to determine card presence. Hotplug remains unsupported.

	/*

	 * Enable slot #4 if it's been specified on the command line.

	 *

	 * Optionally reroute if slot #4 has a card present while slot #3

	 * does not, regardless of command line value.

	 *

	 * Card presence is logically inverted.

 Warn about forced rerouting if slot#3 is occupied */

 SPDX-License-Identifier: GPL-2.0

/*

 * Low-Level PCI Express Support for the SH7786

 *

 *  Copyright (C) 2009 - 2011  Paul Mundt

 100 MHz reference clock */

	/*

	 * Prevent enumeration of root complex resources.

 Set write data */

 Clear command */

	/*

	 * First register the fixed clock

	/*

	 * Grab the port's function clock, which the PHY clock depends

	 * on. clock lookups don't help us much at this point, since no

	 * dev_id is available this early. Lame.

	/*

	 * And now, set up the PHY clock

 Initialize the phy */

 Deassert Standby */

 Disable clock */

 Begin initialization */

	/*

	 * Initial header for port config space is type 1, set the device

	 * class to match. Hardware takes care of propagating the IDSETR

	 * settings, so there is no need to bother with a quirk.

 Initialize default capabilities. */

 Enable data link layer active state reporting */

 Enable extended sync and ASPM L0s support */

 Write out the physical slot number */

 Set the completion timer timeout to the maximum 32ms. */

	/*

	 * Set fast training sequences to the maximum 255,

	 * and enable MAC data scrambling.

	/*

	 * The start address must be aligned on its size. So we round

	 * it down, and then recalculate the size so that it covers

	 * the entire memory.

	/*

	 * If there's more than 512MB of memory, we need to roll over to

	 * LAR1/LAMR1.

		/*

		 * Otherwise just zero it out and disable it.

	/*

	 * LAR0/LAMR0 covers up to the first 512MB, which is enough to

	 * cover all of lowmem on most platforms.

 Finish initialization */

 Let things settle down a bit.. */

 Enable DL_Active Interrupt generation */

 Disable MAC data scrambling. */

	/*

	 * This will timeout if we don't have a link, but we permit the

	 * port to register anyways in order to support hotplug on future

	 * hardware.

		/*

		 * We can't use the 32-bit mode windows in legacy 29-bit

		 * mode, so just skip them entirely.

		/*

		 * The PAMR mask is calculated in units of 256kB, which

		 * keeps things pretty simple.

 Return the number of ports */

	/*

	 * Check if we are configured in endpoint or root complex mode,

	 * this is a fixed pin setting that applies to all PCIe ports.

	/*

	 * Setup clocks, needed both for PHY and PCIe registers.

 In the interest of preserving device ordering, synchronize */

	/*

	 * Fetch any optional platform clock associated with this block.

	 *

	 * This is a rather nasty hack for boards with spec-mocking FPGAs

	 * that have a secondary set of clocks outside of the on-chip

	 * ones that need to be accounted for before there is any chance

	 * of touching the existing MSTP bits or CPG clocks.

 Sane hardware should probably get a WARN_ON.. */

	/*

	 * Depending on the MMSELR register value, the PCIe0 MEM 1

	 * area may not be available. See Table 13.11 of the SH7786

	 * datasheet.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/drivers/pci/ops-snapgear.c

 *

 * Author:  David McCullough <davidm@snapgear.com>

 *

 * Ported to new API by Paul Mundt <lethal@linux-sh.org>

 *

 * Highly leveraged from pci-bigsur.c, written by Dustin McIntire.

 *

 * PCI initialization for the SnapGear boards

 the PCI bridge */ break;

 USB    */

 PCMCIA */

 eth0   */

 eth1   */

 safenet (unused) */

/*

 * debugfs ops for the L1 cache

 *

 *  Copyright (C) 2006  Paul Mundt

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

	/*

	 * Go uncached immediately so we don't skew the results any

	 * more than we already are..

	/*

	 * If the OC is already in RAM mode, we only have

	 * half of the entries to consider..

 Check the V bit, ignore invalid cachelines */

 U: Dirty, cache tag is 10 bits up */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/sh/mm/tlb-sh3.c

 *

 * SH-3 specific TLB operations

 *

 * Copyright (C) 1999  Niibe Yutaka

 * Copyright (C) 2002  Paul Mundt

	/*

	 * Handle debugger faulting in for debugee.

 Set PTEH register */

 Set PTEL register */

 drop software flags */

 conveniently, we want all the software flags to be 0 anyway */

 Load the TLB */

 no output */ : 
	/*

	 * NOTE: PTEH.ASID should be set to this MM

	 *       _AND_ we need to write ASID to the array.

	 *

	 * It would be simple if we didn't need to set PTEH.ASID...

 VALID bit is off */

 we already know the way .. */

	/*

	 * Flush all the TLB.

	 *

	 * Write to the MMU control register's bit:

	 *	TF-bit for SH-3, TI-bit for SH-4.

	 *      It's same position, bit #2.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/sh/mm/cache-j2.c

 *

 * Copyright (C) 2015-2016 Smart Energy Instruments, Inc.

/*

 * TLB miss handler for SH with an MMU.

 *

 *  Copyright (C) 1999  Niibe Yutaka

 *  Copyright (C) 2003 - 2012  Paul Mundt

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * Called with interrupts disabled.

	/*

	 * We don't take page faults for P1, P2, and parts of P4, these

	 * are always mapped, whether it be due to legacy behaviour in

	 * 29-bit mode, or due to PMB configuration in 32-bit mode.

	/*

	 * SH-4 does not set MMUCR.RC to the corresponding TLB entry in

	 * the case of an initial page write exception, so we need to

	 * flush it in order to avoid potential TLB entry duplication.

/*

 * arch/sh/mm/cache-sh4.c

 *

 * Copyright (C) 1999, 2000, 2002  Niibe Yutaka

 * Copyright (C) 2001 - 2009  Paul Mundt

 * Copyright (C) 2003  Richard Curnow

 * Copyright (c) 2007 STMicroelectronics (R&D) Ltd.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * The maximum number of pages we support up to when doing ranged dcache

 * flushing. Anything exceeding this will simply flush the dcache in its

 * entirety.

/*

 * Write back the range of D-cache, and purge the I-cache.

 *

 * Called from kernel/module.c:sys_init_module and routine for a.out format,

 * signal handler code and kprobes code

 If there are too many pages then just blow away the caches */

	/*

	 * Selectively flush d-cache then invalidate the i-cache.

	 * This is inefficient, so only use this for small ranges.

 Clear i-cache line valid-bit */

	/*

	 * All types of SH-4 require PC to be uncached to operate on the I-cache.

	 * Some types of SH-4 require PC to be uncached to operate on the D-cache.

/*

 * Write back & invalidate the D-cache of the page.

 * (To avoid "alias" issues)

 TODO: Selective icache invalidation through IC address array.. */

 Flush I-cache */

	/*

	 * back_to_cached() will take care of the barrier for us, don't add

	 * another one!

/*

 * Note : (RPC) since the caches are physically tagged, the only point

 * of flush_cache_mm for SH-4 is to get rid of aliases from the

 * D-cache.  The assumption elsewhere, e.g. flush_cache_range, is that

 * lines can stay resident so long as the virtual address they were

 * accessed with (hence cache set) is in accord with the physical

 * address (i.e. tag).  It's no different here.

 *

 * Caller takes mm->mmap_lock.

/*

 * Write back and invalidate I/D-caches for the page.

 *

 * ADDR: Virtual Address (U0 address)

 * PFN: Physical page number

 If the page isn't present, there is nothing to do here. */

		/*

		 * Use kmap_coherent or kmap_atomic to do flushes for

		 * another ASID than the current one.

/*

 * Write back and invalidate D-caches.

 *

 * START, END: Virtual Address (U0 address)

 *

 * NOTE: We need to flush the _physical_ page entry.

 * Flushing the cache lines for U0 only isn't enough.

 * We need to flush for P1 too, which may contain aliases.

	/*

	 * If cache is only 4k-per-way, there are never any 'aliases'.  Since

	 * the cache is physically tagged, the data can just be left in there.

/**

 * __flush_cache_one

 *

 * @addr:  address in memory mapped cache array

 * @phys:  P1 address to flush (has to match tags if addr has 'A' bit

 *         set i.e. associative write)

 * @exec_offset: set to 0x20000000 if flush has to be executed from P2

 *               region else 0x0

 *

 * The offset into the cache array implied by 'addr' selects the

 * 'colour' of the virtual address range that will be flushed.  The

 * operation (purge/write-back) is selected by the lower 2 bits of

 * 'phys'.

 Write this way for better assembly. */

	/*

	 * Apply exec_offset (i.e. branch to P2 if required.).

	 *

	 * FIXME:

	 *

	 *	If I write "=r" for the (temp_pc), it puts this in r6 hence

	 *	trashing exec_offset before it's been added on - why?  Hence

	 *	"=&r" as a 'workaround'

	/*

	 * We know there will be >=1 iteration, so write as do-while to avoid

	 * pointless nead-of-loop check for 0 iterations.

			/*

			 * Next line: intentionally not p+32, saves an add, p

			 * will do since only the cache tag bits need to

			 * match.

/*

 * SH-4 has virtually indexed and physically tagged cache.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/sh/mm/nommu.c

 *

 * Various helper routines and stubs for MMUless SH.

 *

 * Copyright (C) 2002 - 2009 Paul Mundt

/*

 * Nothing too terribly exciting here ..

/*

 * SRAM pool for tiny memories not otherwise managed.

 *

 * Copyright (C) 2010  Paul Mundt

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * This provides a standard SRAM pool for tiny memories that can be

 * added either by the CPU or the platform code. Typical SRAM sizes

 * to be inserted in to the pool will generally be less than the page

 * size, with anything more reasonably sized handled as a NUMA memory

 * node.

	/*

	 * This is a global pool, we don't care about node locality.

/*

 * arch/sh/mm/cache-shx3.c - SH-X3 optimized cache ops

 *

 * Copyright (C) 2010  Paul Mundt

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 Hardware-assisted synonym avoidance */

 ICBI broadcast */

	/*

	 * If we've got cache aliases, resolve them in hardware.

	/*

	 * Broadcast I-cache block invalidations by default.

/*

 * arch/sh/mm/tlb-pteaex.c

 *

 * TLB operations for SH-X3 CPUs featuring PTE ASID Extensions.

 *

 * Copyright (C) 2009 Paul Mundt

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

	/*

	 * Handle debugger faulting in for debugee.

 Set PTEH register */

 Set PTEAEX */

 Set PTEA register */

	/*

	 * For the extended mode TLB this is trivial, only the ESZ and

	 * EPR bits need to be written out to PTEA, with the remainder of

	 * the protection bits (with the exception of the compat-mode SZ

	 * and PR bits, which are cleared) being written out in PTEL.

 Set PTEL register */

 drop software flags */

 conveniently, we want all the software flags to be 0 anyway */

 Load the TLB */

 no output */ : 
/*

 * While SH-X2 extended TLB mode splits out the memory-mapped I/UTLB

 * data arrays, SH-X3 cores with PTEAEX split out the memory-mapped

 * address arrays. In compat mode the second array is inaccessible, while

 * in extended mode, the legacy 8-bit ASID field in address array 1 has

 * undefined behaviour.

	/*

	 * Flush all the TLB.

/*

 * arch/sh/mm/mmap.c

 *

 * Copyright (C) 2008 - 2009  Paul Mundt

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 Sane caches */

/*

 * To avoid cache aliases, we map the shared page with same color.

		/* We do not accept a shared mapping if it would violate

		 * cache aliasing constraints.

		/* We do not accept a shared mapping if it would violate

		 * cache aliasing constraints.

 requesting a specific address */

	/*

	 * A failed mmap() very likely causes application failure,

	 * so fall back to the bottom-up function here. This scenario

	 * can happen with large stack limits and large mmap()

	 * allocations.

 CONFIG_MMU */

/*

 * You really shouldn't be using read() or write() on /dev/mem.  This

 * might go away in the future.

/*

 * arch/sh/mm/tlb-urb.c

 *

 * TLB entry wiring helpers for URB-equipped parts.

 *

 * Copyright (C) 2010  Matt Fleming

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * Load the entry for 'addr' into the TLB and wire the entry.

	/*

	 * Make sure we're not trying to wire the last TLB entry slot.

	/*

	 * Insert this entry into the highest non-wired TLB slot (via

	 * the URC field).

 Load the entry into the TLB */

 ... and wire it up. */

/*

 * Unwire the last wired TLB entry.

 *

 * It should also be noted that it is not possible to wire and unwire

 * TLB entries in an arbitrary order. If you wire TLB entry N, followed

 * by entry N+1, you must unwire entry N+1 first, then entry N. In this

 * respect, it works like a stack or LIFO queue.

	/*

	 * Make sure we're not trying to unwire a TLB entry when none

	 * have been wired.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/sh/mm/kmap.c

 *

 * Copyright (C) 1999, 2000, 2002  Niibe Yutaka

 * Copyright (C) 2002 - 2009  Paul Mundt

 cache the first coherent kmap pte */

 XXX.. Kill this later, here for sanity at the moment.. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Write back the dirty D-caches, but not invalidate them.

 *

 * START: Virtual Address (U0, P1, or P3)

 * SIZE: Size of the region.

/*

 * Write back the dirty D-caches and invalidate them.

 *

 * START: Virtual Address (U0, P1, or P3)

 * SIZE: Size of the region.

/*

 * No write back please

/*

 * arch/sh/mm/tlb-debugfs.c

 *

 * debugfs ops for SH-4 ITLB/UTLBs.

 *

 * Copyright (C) 2010  Matt Fleming

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 Make the "entry >= urb" test fail. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/sh/mm/cache-sh3.c

 *

 * Copyright (C) 1999, 2000  Niibe Yutaka

 * Copyright (C) 2002 Paul Mundt

/*

 * Write back the dirty D-caches, but not invalidate them.

 *

 * Is this really worth it, or should we just alias this routine

 * to __flush_purge_region too?

 *

 * START: Virtual Address (U0, P1, or P3)

 * SIZE: Size of the region.

/*

 * Write back the dirty D-caches and invalidate them.

 *

 * START: Virtual Address (U0, P1, or P3)

 * SIZE: Size of the region.

 _Virtual_ address, ~U, ~V */

	/*

	 * No write back please

	 *

	 * Except I don't think there's any way to avoid the writeback.

	 * So we just alias it to sh3__flush_purge_region(). dwmw2.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/arch/sh/mm/init.c

 *

 *  Copyright (C) 1999  Niibe Yutaka

 *  Copyright (C) 2002 - 2011  Paul Mundt

 *

 *  Based on linux/arch/i386/mm/init.c:

 *   Copyright (C) 1995  Linus Torvalds

 Nothing to see here, move along. */

 CONFIG_MMU */

 Add active regions with valid PFNs. */

 All of system RAM sits in node 0 for the non-NUMA case */

	/*

	 * Partially used pages are not usable - thus

	 * we are rounding upwards:

	/*

	 * Reserve the kernel text and Reserve the bootmem bitmap. We do

	 * this in two steps (first step was init_bootmem()), because

	 * this catches the (definitely buggy) case of us accidentally

	 * initializing the bootmem allocator with an invalid RAM area.

	/*

	 * Reserve physical pages below CONFIG_ZERO_PAGE_OFFSET.

	/*

	 * Handle additional early reservations

	/*

	 * Once the early reservations are out of the way, give the

	 * platforms a chance to kick out some memory.

	/*

	 * Determine low and high memory ranges:

	/* We don't need to map the kernel through the TLB, as

	 * it is permanatly mapped using P1. So clear the

	/* Set an initial value for the MMU.TTB so we don't have to

	/*

	 * Populate the relevant portions of swapper_pg_dir so that

	 * we can use the fixmap entries without calling kmalloc.

	 * pte's will be filled in by __set_fixmap().

 Set this up early, so we can take care of the zero page */

 clear the zero-page */

 We only have ZONE_NORMAL, so this is easy.. */

 CONFIG_MEMORY_HOTPLUG */

 SPDX-License-Identifier: GPL-2.0

/*

 * This is the offset of the uncached section from its cached alias.

 *

 * Legacy platforms handle trivial transitions between cached and

 * uncached segments by making use of the 1:1 mapping relationship in

 * 512MB lowmem, others via a special uncached mapping.

 *

 * Default value only valid in 29 bit mode, in 32bit mode this will be

 * updated by the early PMB initialization code.

/*

 * Alignment access counters and corresponding user-space interfaces.

 *

 * Copyright (C) 2009 ST Microelectronics

 * Copyright (C) 2009 - 2010 Paul Mundt

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/* bitfield: 1: warn 2: fixup 4: signal -> combinations 2|4 && 1|2|4 are not

 0: no warning 1: print a warning message, disabled by default */

/*

 * This defaults to the global policy which can be set from the command

 * line, while processes can overload their preferences via prctl().

/*

 * This needs to be done after sysctl_init, otherwise sys/ will be

 * overwritten.  Actually, this shouldn't be in sys/ at all since

 * it isn't a sysctl, and it doesn't contain sysctl information.

 * We now locate it in /proc/cpu/alignment instead.

/*

 * arch/sh/mm/cache-sh7705.c

 *

 * Copyright (C) 1999, 2000  Niibe Yutaka

 * Copyright (C) 2004  Alex Song

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

/*

 * The 32KB cache on the SH7705 suffers from the same synonym problem

 * as SH4 CPUs

/*

 * Write back the range of D-cache, and purge the I-cache.

 *

 * Called from kernel/module.c:sys_init_module and routine for a.out format.

/*

 * Writeback&Invalidate the D-cache of the page

	/*

	 * Here, phys is the physical address of the page. We check all the

	 * tags in the cache for those with the same page number as this page

	 * (by masking off the lowest 2 bits of the 19-bit tag; these bits are

	 * derived from the offset within in the 4k page). Matching valid

	 * entries are invalidated.

	 *

	 * Since 2 bits of the cache index are derived from the virtual page

	 * number, knowing this would reduce the number of cache entries to be

	 * searched by a factor of 4. However this function exists to deal with

	 * potential cache aliasing, therefore the optimisation is probably not

	 * possible.

/*

 * Write back & invalidate the D-cache of the page.

 * (To avoid "alias" issues)

/*

 * Write back and invalidate I/D-caches for the page.

 *

 * ADDRESS: Virtual Address (U0 address)

/*

 * This is called when a page-cache page is about to be mapped into a

 * user process' address space.  It offers an opportunity for a

 * port to ensure d-cache/i-cache coherency if necessary.

 *

 * Not entirely sure why this is necessary on SH3 with 32K cache but

 * without it we get occasional "Memory fault" when loading a program.

/*

 * Copyright (C) 2004 - 2007  Paul Mundt

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 accept anything that begins with "memchunk." */

 strlen("memchunk.") */

/*

 * arch/sh/mm/numa.c - Multiple node support for SH machines

 *

 *  Copyright (C) 2007  Paul Mundt

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * On SH machines the conventional approach is to stash system RAM

 * in node 0, and other memory blocks in to node 1 and up, ordered by

 * latency. Each node's pgdat is node-local at the beginning of the node,

 * immediately followed by the node mem map.

 Don't allow bogus node assignment */

 Node-local pgdat */

 It's up */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/sh/mm/cache-sh2a.c

 *

 * Copyright (C) 2008 Yoshinori Sato

/*

 * The maximum number of pages we support up to when doing ranged dcache

 * flushing. Anything exceeding this will simply flush the dcache in its

 * entirety.

 Set associative bit to hit all ways */

/*

 * Write back the dirty D-caches, but not invalidate them.

 If there are too many pages then flush the entire cache */

/*

 * Write back the dirty D-caches and invalidate them.

/*

 * Invalidate the D-caches, but no write back please

 If there are too many pages then just blow the cache */

/*

 * Write back the range of D-cache, and purge the I-cache.

 I-Cache invalidate */

 If there are too many pages then just blow the cache */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/sh/mm/cache-sh2.c

 *

 * Copyright (C) 2002 Paul Mundt

 * Copyright (C) 2008 Yoshinori Sato

	/*

	 * SH-2 does not support individual line invalidation, only a

	 * global invalidate.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/sh/mm/cache.c

 *

 * Copyright (C) 1999, 2000, 2002  Niibe Yutaka

 * Copyright (C) 2002 - 2010  Paul Mundt

 Needing IPI for cross-core flush is SHX3-specific. */

	/*

	 * It's possible that this gets called early on when IRQs are

	 * still disabled due to ioremapping by the boot CPU, so don't

	 * even attempt IPIs unless there are other CPUs online.

 Make sure this page is cleared on other CPU's too before using it */

 XXX.. For now kunmap_coherent() does a purge */

 __flush_purge_region((void *)kaddr, PAGE_SIZE); */

 Nothing uses the VMA, so just pass the struct page along */

	/*

	 * Emit Secondary Cache parameters if the CPU has a probed L2.

	/*

	 * No flushing is necessary in the disabled cache case so we can

	 * just keep the noop functions in local_flush_..() and __flush_..()

/*

 * Page fault handler for SH with an MMU.

 *

 *  Copyright (C) 1999  Niibe Yutaka

 *  Copyright (C) 2003 - 2012  Paul Mundt

 *

 *  Based on linux/arch/i386/mm/fault.c:

 *   Copyright (C) 1995  Linus Torvalds

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * This is useful to dump out the page tables associated with

 * 'addr' in mm 'mm'.

 We must not map this if we have highmem enabled */

		/*

		 * The page tables are fully synchronised so there must

		 * be another reason for the fault. Return NULL here to

		 * signal that we have not taken care of the fault.

/*

 * Handle a fault on the vmalloc or module mapping area

 Make sure we are in vmalloc/module/P3 area: */

	/*

	 * Synchronize this task's top level page-table

	 * with the 'reference' page table.

	 *

	 * Do _not_ use "current" here. We might be inside

	 * an interrupt in the middle of a task switch..

 Are we prepared to handle this kernel fault?  */

	/*

	 * Oops. The kernel tried to access some bad page. We'll have to

	 * terminate things with extreme prejudice.

 User mode accesses just cause a SIGSEGV */

		/*

		 * It's possible to have interrupts off here:

	/*

	 * Something tried to access memory that isn't in our memory map..

	 * Fix it, but check if it's kernel or user first..

 Kernel mode? Handle exceptions or die: */

	/*

	 * Pagefault was interrupted by SIGKILL. We have no reason to

	 * continue pagefault.

 Release mmap_lock first if necessary */

 Kernel mode? Handle exceptions or die: */

		/*

		 * We ran out of memory, call the OOM killer, and return the

		 * userspace (which will retry the fault, or kill us if we got

		 * oom-killed):

 write, present and write, not present: */

 ITLB miss on NX page */

 read, not present: */

/*

 * This routine handles page faults.  It determines the address,

 * and the problem, and then passes it off to one of the appropriate

 * routines.

	/*

	 * We fault-in kernel-space virtual memory on-demand. The

	 * 'reference' page table is init_mm.pgd.

	 *

	 * NOTE! We MUST NOT take any locks for this case. We may

	 * be in an interrupt or a critical region, and should

	 * only copy the information from the master page table,

	 * nothing more.

 Only enable interrupts if they were on before the fault */

	/*

	 * If we're in an interrupt, have no user context or are running

	 * with pagefaults disabled then we must not take the fault:

	/*

	 * Ok, we have a good vm_area for this memory access, so

	 * we can handle it..

	/*

	 * If for any reason at all we couldn't handle the fault,

	 * make sure we exit gracefully rather than endlessly redo

	 * the fault.

			/*

			 * No need to mmap_read_unlock(mm) as we would

			 * have already released it in __lock_page_or_retry

			 * in mm/filemap.c.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/mm/hugetlbpage.c

 *

 * SuperH HugeTLB page support.

 *

 * Cloned from sparc64 by Paul Mundt.

 *

 * Copyright (C) 2002, 2003 David S. Miller (davem@redhat.com)

 SPDX-License-Identifier: GPL-2.0

 PAGETABLE_LEVELS > 2 */

/*

 * arch/sh/mm/ioremap.c

 *

 * (C) Copyright 1995 1996 Linus Torvalds

 * (C) Copyright 2005 - 2010  Paul Mundt

 *

 * Re-map IO memory to kernel address space so that we can access it.

 * This is needed for high PCI addresses that aren't mapped in the

 * 640k-1MB IO memory area on PC's

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License. See the file "COPYING" in the main directory of this

 * archive for more details.

/*

 * On 32-bit SH, we traditionally have the whole physical address space mapped

 * at all times (as MIPS does), so "ioremap()" and "iounmap()" do not need to do

 * anything but place the address in the proper segment.  This is true for P1

 * and P2 addresses, as well as some P3 ones.  However, most of the P3 addresses

 * and newer cores using extended addressing need to map through page tables, so

 * the ioremap() implementation becomes a bit more complicated.

	/*

	 * For P1 and P2 space this is trivial, as everything is already

	 * mapped. Uncached access for P1 addresses are done through P2.

	 * In the P3 case or for addresses outside of the 29-bit space,

	 * mapping must be done by the PMB or by using page tables.

		/*

		 * Anything using the legacy PTEA space attributes needs

		 * to be kicked down to page table mappings.

 P4 above the store queues are always mapped. */

 CONFIG_29BIT */

/*

 * Remap an arbitrary physical address space into the kernel virtual

 * address space. Needed when the kernel wants to access high addresses

 * directly.

 *

 * NOTE! We need to allow non-page-aligned mappings too: we will obviously

 * have to convert them into an offset in a page-aligned mapping, but the

 * caller shouldn't need to know that small detail.

 Don't allow wraparound or zero size */

	/*

	 * If we can't yet use the regular approach, go the fixmap route.

	/*

	 * First try to remap through the PMB.

	 * PMB entries are all pre-faulted.

	/*

	 * Mappings have to be page-aligned

	/*

	 * Ok, go for it..

/*

 * Simple checks for non-translatable mappings.

	/*

	 * In 29-bit mode this includes the fixed P1/P2 areas, as well as

	 * parts of P3.

	/*

	 * Nothing to do if there is no translatable mapping.

	/*

	 * There's no VMA if it's from an early fixed mapping.

	/*

	 * If the PMB handled it, there's nothing else to do.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/mm/extable.c

 *  Taken from:

 *   linux/arch/i386/mm/extable.c

/*

 * arch/sh/mm/pmb.c

 *

 * Privileged Space Mapping Buffer (PMB) Support.

 *

 * Copyright (C) 2005 - 2011  Paul Mundt

 * Copyright (C) 2010  Matt Fleming

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

	/*

	 * 0 .. NR_PMB_ENTRIES for specific entry selection, or

	 * PMB_NO_ENTRY to search for a free one

 Adjacent entry link for contiguous multi-entry mappings */

/*

 * Ensure that the PMB entries match our cache configuration.

 *

 * When we are in 32-bit address extended mode, CCR.CB becomes

 * invalid, so care must be taken to manually adjust cacheable

 * translations.

/*

 * Convert typical pgprot value to the PMB equivalent

		/*

		 * See if VPN and PPN are bounded by an existing mapping.

		/*

		 * Now see if we're in range of a simple mapping.

		/*

		 * Finally for sizes that involve compound mappings, walk

		 * the chain.

		/*

		 * Nothing else to do if the range requirements are met.

/*

 * Must be run uncached.

 Set V-bit */

 Clear V-bit */

 CONFIG_PM */

			/*

			 * Link adjacent entries that span multiple PMB

			 * entries for easier tear-down.

			/*

			 * Instead of trying smaller sizes on every

			 * iteration (even if we succeed in allocating

			 * space), try using pmb_sizes[i].size again.

	/*

	 * Small mappings need to go through the TLB.

	/*

	 * XXX: This should really start from uncached_end, but this

	 * causes the MMU to reset, so for now we restrict it to the

	 * 0xb000...0xc000 range.

		/*

		 * We may be called before this pmb_entry has been

		 * entered into the PMB table via set_pmb_entry(), but

		 * that's OK because we've allocated a unique slot for

		 * this entry in pmb_alloc() (even if we haven't filled

		 * it yet).

		 *

		 * Therefore, calling __clear_pmb_entry() is safe as no

		 * other mapping can be using that slot.

/*

 * Sync our software copy of the PMB mappings with those in hardware. The

 * mappings in the hardware PMB were either set up by the bootloader or

 * very early on by the kernel.

	/*

	 * Run through the initial boot mappings, log the established

	 * ones, and blow away anything that falls outside of the valid

	 * PPN range. Specifically, we only care about existing mappings

	 * that impact the cached/uncached sections.

	 *

	 * Note that touching these can be a bit of a minefield; the boot

	 * loader can establish multi-page mappings with the same caching

	 * attributes, so we need to ensure that we aren't modifying a

	 * mapping that we're presently executing from, or may execute

	 * from in the case of straddling page boundaries.

	 *

	 * In the future we will have to tidy up after the boot loader by

	 * jumping between the cached and uncached mappings and tearing

	 * down alternating mappings while executing from the other.

		/*

		 * Skip over any bogus entries

		/*

		 * Only preserve in-range mappings.

			/*

			 * Invalidate anything out of bounds.

		/*

		 * Update the caching attributes if necessary

			/*

			 * Compare the previous entry against the current one to

			 * see if the entries span a contiguous mapping. If so,

			 * setup the entry links accordingly. Compound mappings

			 * are later coalesced.

 This is the end of the line.. */

	/*

	 * The merged page size must be valid.

		/*

		 * We're only interested in compound mappings

		/*

		 * Nothing to do if it already uses the largest possible

		 * page size.

	/*

	 * If the uncached mapping was constructed by the kernel, it will

	 * already be a reasonable size.

		/*

		 * Found it, now resize it.

 Synchronize software state */

 Attempt to combine compound mappings */

 Resize initial mappings, if necessary */

 Log them */

 Flush out the TLB */

 02: V 0x88 0x08 128MB C CB  B */

 SPDX-License-Identifier: GPL-2.0

/*

 * Re-map IO memory to kernel address space so that we can access it.

 *

 * These functions should only be used when it is necessary to map a

 * physical address space into the kernel address space before ioremap()

 * can be used, e.g. early in boot before paging_init().

 *

 * Copyright (C) 2009  Matt Fleming

	/*

	 * Mappings have to be page-aligned

	/*

	 * Mappings have to fit in the FIX_IOREMAP area.

	/*

	 * Ok, go for it..

	/*

	 * If we don't match, it's not for us.

/*

 * debugfs ops for process ASIDs

 *

 *  Copyright (C) 2000, 2001  Paolo Alberelli

 *  Copyright (C) 2003 - 2008  Paul Mundt

 *  Copyright (C) 2003, 2004  Richard Curnow

 *

 * Provides a debugfs file that lists out the ASIDs currently associated

 * with the processes.

 *

 * In the SH-5 case, if the DM.PC register is examined through the debug

 * link, this shows ASID + PC. To make use of this, the PID->ASID

 * relationship needs to be known. This is primarily for debugging.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/sh/mm/tlb-sh4.c

 *

 * SH-4 specific TLB operations

 *

 * Copyright (C) 1999  Niibe Yutaka

 * Copyright (C) 2002 - 2007 Paul Mundt

	/*

	 * Handle debugger faulting in for debugee.

 Set PTEH register */

 Set PTEA register */

	/*

	 * For the extended mode TLB this is trivial, only the ESZ and

	 * EPR bits need to be written out to PTEA, with the remainder of

	 * the protection bits (with the exception of the compat-mode SZ

	 * and PR bits, which are cleared) being written out in PTEL.

		/* The last 3 bits and the first one of pteval contains

		 * the PTEA timing control and space attribute bits

 Set PTEL register */

 drop software flags */

 conveniently, we want all the software flags to be 0 anyway */

 Load the TLB */

 no output */ : 
	/*

	 * NOTE: PTEH.ASID should be set to this MM

	 *       _AND_ we need to write ASID to the array.

	 *

	 * It would be simple if we didn't need to set PTEH.ASID...

 VALID bit is off */

	/*

	 * Flush all the TLB.

/*

 * TLB flushing operations for SH with an MMU.

 *

 *  Copyright (C) 1999  Niibe Yutaka

 *  Copyright (C) 2003  Paul Mundt

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 Too many TLB to flush */

 Too many TLB to flush */

 Invalidate all TLB of this process. */

 Instead of invalidating each TLB, we get new MMU context. */

	/*

	 * This is the most destructive of the TLB flushing options,

	 * and will tear down all of the UTLB/ITLB mappings, including

	 * wired entries.

 SPDX-License-Identifier: GPL-2.0

/*

 * this gets called so that we can store lazy state into memory and copy the

 * current task into the new thread.

 SPDX-License-Identifier: GPL-2.0

/*

 * 'traps.c' handles hardware traps and faults after we have saved some

 * state in 'entry.S'.

 *

 *  SuperH version: Copyright (C) 1999 Niibe Yutaka

 *                  Copyright (C) 2000 Philipp Rumpf

 *                  Copyright (C) 2000 David Howells

 *                  Copyright (C) 2002 - 2010 Paul Mundt

/*

 * handle an instruction that does an unaligned memory access by emulating the

 * desired behaviour

 * - note that PC _may not_ point to the faulting instruction

 *   (if that instruction is in a branch delay slot)

 * - return 0 if emulation okay, -EFAULT on existential error

 0x0F00 */

 0x00F0 */

 mov.[bwl] to/from memory via r0+rn */

 from memory */

 to memory */

 mov.l Rm,@(disp,Rn) */

 mov.[bwl] to memory, possibly with pre-decrement */

 mov.l @(disp,Rm),Rn */

 mov.[bwl] from memory, possibly with post-increment */

 mov.w R0,@(disp,Rn) */

 called Rn in the spec */

 mov.w @(disp,Rm),R0 */

 mov.w @(disp,PC),Rn */

 mov.l @(disp,PC),Rn */

	/* Argh. Address not only misaligned but also non-existent.

	 * Raise an EFAULT and see if it's trapped

/*

 * emulate the instruction in the delay slot

 * - fetches the instruction from PC+2

 the instruction-fetch faulted */

 kernel */

/*

 * handle an instruction that does an unaligned memory access

 * - have to be careful of branch delay-slot instructions that fault

 *  SH3:

 *   - if the branch would be taken PC points to the branch

 *   - if the branch would not be taken, PC points to delay-slot

 *  SH4:

 *   - PC always points to delayed branch

 * - return 0 if handled, -EFAULT if failed (may not return if in kernel)

 Macros to determine offset from current PC for branch instructions */

 Explicit type coercion is used to force sign extension where needed */

	/*

	 * XXX: We can't handle mixed 16/32-bit instructions yet

 0x0F00 */

	/*

	 * Log the unexpected fixups, and then pass them on to perf.

	 *

	 * We intentionally don't report the expected cases to perf as

	 * otherwise the trapped I/O case will skew the results too much

	 * to be useful.

 rts */

 braf @Rm */

 bsrf @Rm */

 mov.[bwl] to/from memory via r0+rn */

 mov.l Rm,@(disp,Rn) */

 mov.[bwl] to memory, possibly with pre-decrement */

 jmp @Rm */

 jsr @Rm */

 mov.[bwl] to/from memory via r0+rn */

 mov.l @(disp,Rm),Rn */

 mov.[bwl] from memory, possibly with post-increment */

 bf lab, bf/s lab, bt lab, bt/s lab */

 mov.w R0,@(disp,Rm) */

 mov.w @(disp,Rm),R0 */

 bf   lab - no delayslot*/

 bf/s lab */

 next after slot */

 bt   lab - no delayslot */

 bt/s lab */

 next after slot */

 mov.w @(disp,Rm),Rn */

 bra label */

 bsr label */

 mov.l @(disp,Rm),Rn */

 handle non-delay-slot instruction */

/*

 * Handle various address error exceptions:

 *  - instruction address error:

 *       misaligned PC

 *       PC >= 0x80000000 in user mode

 *  - data address error (read and write)

 *       misaligned data access

 *       access to >= 0x80000000 is user mode

 * Unfortuntaly we can't distinguish between instruction address error

 * and data address errors caused by read accesses.

 Intentional ifdef */

 shout about userspace fixups */

 ignore */

 bad PC is not something we can fix */

 sorted */

			/* Argh. Fault on the instruction itself.

			   This should never happen non-SMP

/*

 *	SH-DSP support gerg@snapgear.com.

	/*

	 * Safe guard if DSP mode is already enabled or we're lacking

	 * the DSP altogether.

 Check for any type of DSP or support instruction */

 CONFIG_SH_DSP */

 Let gcc know unhandled cases don't make it past here */

 not a FPU inst. */

 Check if it's a DSP instruction */

 Enable DSP mode, and restart instruction. */

 Save DSP mode */

	/*

	 * bfs: 8fxx: PC+=d*2+4;

	 * bts: 8dxx: PC+=d*2+4;

	 * bra: axxx: PC+=D*2+4;

	 * bsr: bxxx: PC+=D*2+4  after PR=PC+4;

	 * braf:0x23: PC+=Rn*2+4;

	 * bsrf:0x03: PC+=Rn*2+4 after PR=PC+4;

	 * jmp: 4x2b: PC=Rn;

	 * jsr: 4x0b: PC=Rn      after PR=PC+4;

	 * rts: 000b: PC=PR;

 bsr */

 bsrf */

 jsr */

 bfs, bts */

 bra, bsr */

 braf, bsrf */

 jmp, jsr */

 rts */

 fault in branch.*/

 not a FPU inst. */

	/* NOTE: The VBR value should be at P1

	   (or P2, virtural "fixed" address space).

 no output */

 disable exception blocking now when the vbr has been setup */

	/*

	 * For SH-4 lacking an FPU, treat floating point instructions as

	 * reserved. They'll be handled in the math-emu case, or faulted on

	 * otherwise.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/stacktrace.c

 *

 * Stack trace management functions

 *

 *  Copyright (C) 2006 - 2008  Paul Mundt

/*

 * Save stack-backtrace addresses into a stack_trace buffer.

 SPDX-License-Identifier: GPL-2.0

 print_modules */

/*

 * try and fix up kernelspace address errors

 * - userspace errors just cause EFAULT to be returned, resulting in SEGV

 * - kernel/userspace interfaces cause a jump to an appropriate handler

 * - other kernel errors are bad

 Switch unwinders when unwind_stack() is called */

/*

 * Generic trap handler.

 Rewind */

/*

 * Special handler for BUG() traps.

 Rewind */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/setup.c

 *

 * This file handles the architecture-dependent parts of initialization

 *

 *  Copyright (C) 1999  Niibe Yutaka

 *  Copyright (C) 2002 - 2010 Paul Mundt

/*

 * Initialize loops_per_jiffy as 10000000 (1000MIPS).

 * This value will be used at the very early stage of serial setup.

 * The bigger value means no problem.

/*

 * The machine vector. First entry in .machvec.init, or clobbered by

 * sh_mv= on the command line, prior to .machvec.init teardown.

	/*

	 * Check for the rare cases where boot loaders adhere to the boot

	 * ABI.

	/*

	 * If we got this far in spite of the boot loader's best efforts

	 * to the contrary, assume we actually have a valid initrd and

	 * fix up the root dev.

	/*

	 * Address sanitization

 max one active range per node for now */

	/*

	 * We don't know which RAM region contains kernel data or

	 * the reserved crashkernel region, so try it repeatedly

	 * and let the resource manager test it.

	/*

	 * Also make sure that there is a PMB mapping that covers this

	 * range before we attempt to activate it, to avoid reset by MMU.

	 * We can hit this path with NUMA or memory hot-add.

 Avoid calling an __init function on secondary cpus. */

 Save unparsed command line copy for /proc/cmdline */

 Let earlyprintk output early console messages */

 Perform the machine specific initialisation */

 processor boot mode configuration */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/io.c - Machine independent I/O functions.

 *

 * Copyright (C) 2000 - 2009  Stuart Menefy

 * Copyright (C) 2005  Paul Mundt

/*

 * Copy data from IO memory space to "real" memory space.

	/*

	 * Would it be worthwhile doing byte and long transfers first

	 * to try and get aligned?

/*

 * Copy data from "real" memory space to IO memory space.

/*

 * "memset" on IO memory space.

 * This needs to be optimized.

 SPDX-License-Identifier: GPL-2.0

/*

 * Trapped io support

 *

 * Copyright (C) 2008 Magnus Damm

 *

 * Intercept io operations by trapping.

 structure must be page aligned */

 support IORESOURCE_IO _or_ MEM, not both */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/sh/kernel/signal.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  1997-11-28  Modified for POSIX.1b signals by Richard Henderson

 *

 *  SuperH version:  Copyright (C) 1999, 2000  Niibe Yutaka & Kaz Kojima

 *

/*

 * The following define adds a 64 byte gap between the signal

 * stack frame and previous contents of the stack.  This allows

 * frame unwinding in a function epilogue but only if a frame

 * pointer is used in the function.  This is necessary because

 * current gcc compilers (<4.3) do not generate unwind info on

 * SH for function epilogues.

/*

 * Do a signal return; undo the signal stack.

 Move mem word at PC+n to R3 */

 Syscall w/no args (NR in R3) */

 Syscall w/no args (NR in R3) */

 or r0,r0 (insert to avoid hardware bug) */

	/* This will cause a "finit" to be triggered by the next

	   attempted FPU operation by the 'current' process.

 CONFIG_SH_FPU */

 Release FPU */

 disable syscall checks */

 Always make any pending restarted system calls return -EINTR */

 Always make any pending restarted system calls return -EINTR */

/*

 * Set up a signal frame.

 non-iBCS2 extensions.. */

/*

 * Determine which stack to use..

/* These symbols are defined with the addresses in the vsyscall page.

	/* Set up to return from userspace.  If provided, use a stub

 Generate return code (system call to sigreturn) */

 Set up registers for signal handler */

 Arg for signal handler */

 Create the ucontext.  */

	/* Set up to return from userspace.  If provided, use a stub

 Generate return code (system call to rt_sigreturn) */

 Set up registers for signal handler */

 Arg for signal handler */

 If we're not from a syscall, bail out */

 check for system call restart.. */

/*

 * OK, we're invoking a handler

 Set up the stack frame */

/*

 * Note that 'init' is a special process: it doesn't get signals it doesn't

 * want to handle. Thus you cannot kill init even with a SIGKILL even by

 * mistake.

 *

 * Note that we go through the signals twice: once to check the signals that

 * the kernel can handle, and then we build all the user-level signal handling

 * stack-frames in one go after that.

	/*

	 * We want the common case to go fast, which

	 * is why we may in certain cases get here from

	 * kernel mode. Just return without doing anything

	 * if so.

 Whee!  Actually deliver the signal.  */

 Did we come from a system call? */

 Restart the system call - no handlers present */

	/*

	 * If there's no signal to deliver, we just put the saved sigmask

	 * back.

 deal with pending signal delivery */

 SPDX-License-Identifier: GPL-2.0

/*

 *  arch/sh/kernel/time.c

 *

 *  Copyright (C) 1999  Tetsuya Okada & Niibe Yutaka

 *  Copyright (C) 2000  Philipp Rumpf <prumpf@tux.org>

 *  Copyright (C) 2002 - 2009  Paul Mundt

 *  Copyright (C) 2002  M. R. Brown  <mrbrown@linux-sh.org>

	/*

	 * Make sure all compiled-in early timers register themselves.

	 *

	 * Run probe() for two "earlytimer" devices, these will be the

	 * clockevents and clocksource devices respectively. In the event

	 * that only a clockevents device is available, we -ENODEV on the

	 * clocksource and the jiffies clocksource is used transparently

	 * instead. No error handling is necessary here.

 SPDX-License-Identifier: GPL-2.0

/*

 * sys_pipe() is the normal C calling standard for creating

 * a pipe. It's not the way Unix traditionally does this, though.

 SPDX-License-Identifier: GPL-2.0

/*

 * The idle loop for all SuperH platforms.

 *

 *  Copyright (C) 2002 - 2009  Paul Mundt

 Isn't this racy ? */

	/*

	 * If a platform has set its own idle routine, leave it alone.

 SPDX-License-Identifier: GPL-2.0

/*

 * Disassemble SuperH instructions.

 *

 * Copyright (C) 1999 kaz Kojima

 * Copyright (C) 2008 Paul Mundt

/*

 * Format of an instruction in memory.

 Only used for argument parsing */

 Only used for argument parsing */

 SPDX-License-Identifier: GPL-2.0

/*

 * SuperH process tracing

 *

 * Copyright (C) 1999, 2000  Kaz Kojima & Niibe Yutaka

 * Copyright (C) 2002 - 2009  Paul Mundt

 *

 * Audit support by Yuichi Nakamura <ynakam@hitachisoft.jp>

/*

 * This routine will get a word off of the process kernel stack.

/*

 * This routine will put a word on the process kernel stack.

	/*

	 * Disable the breakpoint request here since ptrace has defined a

	 * one-shot behaviour for breakpoint exceptions.

 reenable breakpoint */

/*

 * Called by kernel/ptrace.c when detaching..

 *

 * Make sure single step bits etc are not set.

/*

 * These are our native regset flavours.

	/*

	 * Format is:

	 *	R0 --> R15

	 *	PC, PR, SR, GBR, MACH, MACL, TRA

 read the word at location addr in the USER area. */

 write the word at location addr in the USER area */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/hw_breakpoint.c

 *

 * Unified kernel/user-space hardware breakpoint facility for the on-chip UBC.

 *

 * Copyright (C) 2009 - 2010  Paul Mundt

/*

 * Stores the breakpoints currently in use on each breakpoint address

 * register for each cpus

/*

 * A dummy placeholder for early accesses until the CPUs get a chance to

 * register their UBCs later in the boot process.

/*

 * Install a perf counter breakpoint.

 *

 * We seek a free UBC channel and use it for this breakpoint.

 *

 * Atomic: we hold the counter->ctx->lock and we only handle variables

 * and registers local to this cpu.

/*

 * Uninstall the breakpoint contained in the given counter.

 *

 * First we search the debug address register it uses and then we disable

 * it.

 *

 * Atomic: we hold the counter->ctx->lock and we only handle variables

 * and registers local to this cpu.

/*

 * Check for virtual address in kernel space.

 Len */

 Type */

 Len */

 Type */

/*

 * Validate the arch-specific HW Breakpoint register settings

	/*

	 * Check that the low-order bits of the address are appropriate

	 * for the alignment implied by len.

/*

 * Release the user breakpoints used by ptrace

	/*

	 * Do an early return if none of the channels triggered.

	/*

	 * By default, resume all of the active channels.

	/*

	 * Disable breakpoints during exception handling.

		/*

		 * The counter may be concurrently released but that can only

		 * occur from a call_rcu() path. We can then safely fetch

		 * the breakpoint, use its callback, touch its counter

		 * while we are in an rcu_read_lock() path.

		/*

		 * Reset the condition match flag to denote completion of

		 * exception handling.

		/*

		 * bp can be NULL due to concurrent perf counter

		 * removing.

		/*

		 * Don't restore the channel if the breakpoint is from

		 * ptrace, as it always operates in one-shot mode.

 Deliver the signal to userspace */

/*

 * Handle debug exception notifications.

	/*

	 * If the breakpoint hasn't been triggered by the UBC, it's

	 * probably from a debugger, so don't do anything more here.

	 *

	 * This also permits the UBC interface clock to remain off for

	 * non-UBC breakpoints, as we don't need to check the triggered

	 * or active channel masks.

 TODO */

 Bail if it's already assigned */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2009 Matt Fleming <matt@console-pimps.org>

 *

 * This is an implementation of a DWARF unwinder. Its main purpose is

 * for generating stacktrace information. Based on the DWARF 3

 * specification from http://www.dwarfstd.org.

 *

 * TODO:

 *	- DWARF64 doesn't work.

 *	- Registers with DWARF_VAL_OFFSET rules aren't handled properly.

 #define DEBUG */

 Reserve enough memory for two stack frames */

 ... with 4 registers per frame. */

/**

 *	dwarf_frame_alloc_reg - allocate memory for a DWARF register

 *	@frame: the DWARF frame whose list of registers we insert on

 *	@reg_num: the register number

 *

 *	Allocate space for, and initialise, a dwarf reg from

 *	dwarf_reg_pool and insert it onto the (unsorted) linked-list of

 *	dwarf registers for @frame.

 *

 *	Return the initialised DWARF reg.

		/*

		 * Let's just bomb hard here, we have no way to

		 * gracefully recover.

/**

 *	dwarf_frame_reg - return a DWARF register

 *	@frame: the DWARF frame to search in for @reg_num

 *	@reg_num: the register number to search for

 *

 *	Lookup and return the dwarf reg @reg_num for this frame. Return

 *	NULL if @reg_num is an register invalid number.

/**

 *	dwarf_read_addr - read dwarf data

 *	@src: source address of data

 *	@dst: destination address to store the data to

 *

 *	Read 'n' bytes from @src, where 'n' is the size of an address on

 *	the native machine. We return the number of bytes read, which

 *	should always be 'n'. We also have to be careful when reading

 *	from @src and writing to @dst, because they can be arbitrarily

 *	aligned. Return 'n' - the number of bytes read.

/**

 *	dwarf_read_uleb128 - read unsigned LEB128 data

 *	@addr: the address where the ULEB128 data is stored

 *	@ret: address to store the result

 *

 *	Decode an unsigned LEB128 encoded datum. The algorithm is taken

 *	from Appendix C of the DWARF 3 spec. For information on the

 *	encodings refer to section "7.6 - Variable Length Data". Return

 *	the number of bytes read.

/**

 *	dwarf_read_leb128 - read signed LEB128 data

 *	@addr: the address of the LEB128 encoded data

 *	@ret: address to store the result

 *

 *	Decode signed LEB128 data. The algorithm is taken from Appendix

 *	C of the DWARF 3 spec. Return the number of bytes read.

 The number of bits in a signed integer. */

/**

 *	dwarf_read_encoded_value - return the decoded value at @addr

 *	@addr: the address of the encoded value

 *	@val: where to write the decoded value

 *	@encoding: the encoding with which we can decode @addr

 *

 *	GCC emits encoded address in the .eh_frame FDE entries. Decode

 *	the value at @addr using @encoding. The decoded value is written

 *	to @val and the number of bytes read is returned.

/**

 *	dwarf_entry_len - return the length of an FDE or CIE

 *	@addr: the address of the entry

 *	@len: the length of the entry

 *

 *	Read the initial_length field of the entry and store the size of

 *	the entry in @len. We return the number of bytes read. Return a

 *	count of 0 on error.

	/*

	 * An initial length field value in the range DW_LEN_EXT_LO -

	 * DW_LEN_EXT_HI indicates an extension, and should not be

	 * interpreted as a length. The only extension that we currently

	 * understand is the use of DWARF64 addresses.

		/*

		 * The 64-bit length field immediately follows the

		 * compulsory 32-bit length field.

/**

 *	dwarf_lookup_cie - locate the cie

 *	@cie_ptr: pointer to help with lookup

	/*

	 * We've cached the last CIE we looked up because chances are

	 * that the FDE wants this CIE.

/**

 *	dwarf_lookup_fde - locate the FDE that covers pc

 *	@pc: the program counter

/**

 *	dwarf_cfa_execute_insns - execute instructions to calculate a CFA

 *	@insn_start: address of the first instruction

 *	@insn_end: address of the last instruction

 *	@cie: the CIE for this function

 *	@fde: the FDE for this function

 *	@frame: the instructions calculate the CFA for this frame

 *	@pc: the program counter of the address we're interested in

 *

 *	Execute the Call Frame instruction sequence starting at

 *	@insn_start and ending at @insn_end. The instructions describe

 *	how to calculate the Canonical Frame Address of a stackframe.

 *	Store the results in @frame.

		/*

		 * Firstly, handle the opcodes that embed their operands

		 * in the instructions.

 NOTREACHED */

 NOTREACHED */

 NOTREACHED */

		/*

		 * Secondly, handle the opcodes that don't embed their

		 * operands in the instruction.

/**

 *	dwarf_free_frame - free the memory allocated for @frame

 *	@frame: the frame to free

/**

 *	dwarf_unwind_stack - unwind the stack

 *

 *	@pc: address of the function to unwind

 *	@prev: struct dwarf_frame of the previous stackframe on the callstack

 *

 *	Return a struct dwarf_frame representing the most recent frame

 *	on the callstack. Each of the lower (older) stack frames are

 *	linked via the "prev" member.

	/*

	 * If we've been called in to before initialization has

	 * completed, bail out immediately.

	/*

	 * If we're starting at the top of the stack we need get the

	 * contents of a physical register to get the CFA in order to

	 * begin the virtual unwinding of the stack.

	 *

	 * NOTE: the return address is guaranteed to be setup by the

	 * time this function makes its first function call.

	/*

	 * If our stack has been patched by the function graph tracer

	 * then we might see the address of return_to_handler() where we

	 * expected to find the real return address.

		/*

		 * We currently have no way of tracking how many

		 * return_to_handler()'s we've seen. If there is more

		 * than one patched return address on our stack,

		 * complain loudly.

		/*

		 * This is our normal exit path. There are two reasons

		 * why we might exit here,

		 *

		 *	a) pc has no asscociated DWARF frame info and so

		 *	we don't know how to unwind this frame. This is

		 *	usually the case when we're trying to unwind a

		 *	frame that was called from some assembly code

		 *	that has no DWARF info, e.g. syscalls.

		 *

		 *	b) the DEBUG info for pc is bogus. There's

		 *	really no way to distinguish this case from the

		 *	case above, which sucks because we could print a

		 *	warning here.

 CIE initial instructions */

 FDE instructions */

 Calculate the CFA */

			/*

			 * Again, we're starting from the top of the

			 * stack. We need to physically read

			 * the contents of a register in order to get

			 * the Canonical Frame Address for this

			 * function.

	/*

	 * If we haven't seen the return address register or the return

	 * address column is undefined then we must assume that this is

	 * the end of the callstack.

	/*

	 * Ah, the joys of unwinding through interrupts.

	 *

	 * Interrupts are tricky - the DWARF info needs to be _really_

	 * accurate and unfortunately I'm seeing a lot of bogus DWARF

	 * info. For example, I've seen interrupts occur in epilogues

	 * just after the frame pointer (r14) had been restored. The

	 * problem was that the DWARF info claimed that the CFA could be

	 * reached by using the value of the frame pointer before it was

	 * restored.

	 *

	 * So until the compiler can be trusted to produce reliable

	 * DWARF info when it really matters, let's stop unwinding once

	 * we've calculated the function that was interrupted.

	/*

	 * Record the offset into the .eh_frame section

	 * for this CIE. It allows this CIE to be

	 * quickly and easily looked up from the

	 * corresponding FDE.

	/*

	 * Which column in the rule table contains the

	 * return address?

		/*

		 * "L" indicates a byte showing how the

		 * LSDA pointer is encoded. Skip it.

			/*

			 * "R" indicates a byte showing

			 * how FDE addresses are

			 * encoded.

			/*

			 * "R" indicates a personality

			 * routine in the CIE

			 * augmentation.

			/*

			 * Unknown augmentation. Assume

			 * 'z' augmentation.

 Add to list */

	/*

	 * In a .eh_frame section the CIE pointer is the

	 * delta between the address within the FDE

 Call frame instructions. */

 Add to list. */

	/*

	 * Deallocate all the memory allocated for the DWARF unwinder.

	 * Traverse all the FDE/CIE lists and remove and free all the

	 * memory associated with those data structures.

/**

 *	dwarf_parse_section - parse DWARF section

 *	@eh_frame_start: start address of the .eh_frame section

 *	@eh_frame_end: end address of the .eh_frame section

 *	@mod: the kernel module containing the .eh_frame section

 *

 *	Parse the information in a .eh_frame section.

			/*

			 * We read a bogus length field value. There is

			 * nothing we can do here apart from disabling

			 * the DWARF unwinder. We can't even skip this

			 * entry and move to the next one because 'len'

			 * tells us where our next entry is.

 initial length does not include itself */

 Alloc bit cleared means "ignore it." */

 Did we find the .eh_frame section? */

/**

 *	module_dwarf_cleanup - remove FDE/CIEs associated with @mod

 *	@mod: the module that is being unloaded

 *

 *	Remove any FDEs and CIEs from the global lists that came from

 *	@mod's .eh_frame section because @mod is being unloaded.

 CONFIG_MODULES */

/**

 *	dwarf_unwinder_init - initialise the dwarf unwinder

 *

 *	Build the data structures describing the .dwarf_frame section to

 *	make it easier to lookup CIE and FDE entries. Because the

 *	.eh_frame section is packed as tightly as possible it is not

 *	easy to lookup the FDE for a given PC, so we build a list of FDE

 *	and CIE entries that make it easier.

 SPDX-License-Identifier: GPL-2.0

/*

 * machine_kexec.c - handle transition of Linux booting another kernel

 * Copyright (C) 2002-2003 Eric Biederman  <ebiederm@xmission.com>

 *

 * GameCube/ppc32 port Copyright (C) 2004 Albert Herranz

 * LANDISK/sh4 supported by kogiidena

 Nothing to do for UP, but definitely broken for SMP.. */

/*

 * Do what every setup is needed on image and the

 * reboot code buffer to allow us to avoid allocations

 * later.

/*

 * Do not allocate memory (or fail in any way) in machine_kexec().

 * We are past the point of no return, committed to rebooting now.

	/*

	 * Nicked from the mips version of machine_kexec():

	 * The generic kexec code builds a page list with physical

	 * addresses. Use phys_to_virt() to convert them to virtual.

 Interrupts aren't acceptable while we reboot */

 we need both effective and real address here */

 copy our kernel relocation code to the control code page */

 now call it */

 Convert page list back to physical addresses, what a mess. */

	/*

	 * Crash kernel trumps memory limit

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/iomap.c

 *

 * Copyright (C) 2000  Niibe Yutaka

 * Copyright (C) 2005 - 2007 Paul Mundt

/*

 * These are the "repeat MMIO read/write" functions.

 * Note the "__raw" accesses, since we don't want to

 * convert to CPU byte order. We write in "IO byte

 * order" (we also don't have IO barriers).

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/topology.c

 *

 *  Copyright (C) 2007  Paul Mundt

	/*

	 * Presently all SH-X3 SMP cores are multi-cores, so just keep it

	 * simple until we have a method for determining topology..

	/*

	 * In the UP case, make sure the CPU association is still

	 * registered under each node. Without this, sysfs fails

	 * to make the connection between nodes other than node0

	 * and cpu0.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2008 Matt Fleming <matt@console-pimps.org>

 * Copyright (C) 2008 Paul Mundt <lethal@linux-sh.org>

 *

 * Code for replacing ftrace calls with jumps.

 *

 * Copyright (C) 2007-2008 Steven Rostedt <srostedt@redhat.com>

 *

 * Thanks goes to Ingo Molnar, for suggesting the idea.

 * Mathieu Desnoyers, for suggesting postponing the modifications.

 * Arjan van de Ven, for keeping me straight, and explaining to me

 * the dangers of modifying code on the run.

/*

 * If we're trying to nop out a call to a function, we instead

 * place a call to the address after the memory table.

 *

 * 8c011060 <a>:

 * 8c011060:       02 d1           mov.l   8c01106c <a+0xc>,r1

 * 8c011062:       22 4f           sts.l   pr,@-r15

 * 8c011064:       02 c7           mova    8c011070 <a+0x10>,r0

 * 8c011066:       2b 41           jmp     @r1

 * 8c011068:       2a 40           lds     r0,pr

 * 8c01106a:       09 00           nop

 * 8c01106c:       68 24           .word 0x2468     <--- ip

 * 8c01106e:       1d 8c           .word 0x8c1d

 * 8c011070:       26 4f           lds.l   @r15+,pr <--- ip + MCOUNT_INSN_SIZE

 *

 * We write 0x8c011070 to 0x8c01106c so that on entry to a() we branch

 * past the _mcount call and continue executing code like normal.

 Place the address in the memory table. */

	/*

	 * No locking needed, this must be called via kstop_machine

	 * which in essence is like running on a uniprocessor machine.

/*

 * Modifying code must take extra care. On an SMP machine, if

 * the code being modified is also being executed on another CPU

 * that CPU will have undefined results and possibly take a GPF.

 * We use kstop_machine to stop other CPUS from executing code.

 * But this does not stop NMIs from happening. We still need

 * to protect against that. We separate out the modification of

 * the code to take care of this.

 *

 * Two buffers are added: An IP buffer and a "code" buffer.

 *

 * 1) Put the instruction pointer into the IP buffer

 *    and the new code into the "code" buffer.

 * 2) Wait for any running NMIs to finish and set a flag that says

 *    we are modifying code, it is done in an atomic operation.

 * 3) Write the code

 * 4) clear the flag.

 * 5) Wait for any running NMIs to finish.

 *

 * If an NMI is executed, the first thing it does is to call

 * "ftrace_nmi_enter". This will check if the flag is set to write

 * and if it is, it will write what is in the IP and "code" buffers.

 *

 * The trick is, it does not matter if everyone is writing the same

 * content to the code location. Also, if a CPU is executing code

 * it is OK to write to that code location if the contents being written

 * are the same as what exists.

 set when NMI should do the write */

 holds return value of text write */

 holds the IP to write to */

 holds the text to write to the IP */

	/*

	 * Yes, more than one CPU process can be writing to mod_code_status.

	 *    (and the code itself)

	 * But if one were to fail, then they all should, and if one were

	 * to succeed, then they all should.

 if we fail, then kill any new writers */

 Must have previous changes seen before executions */

 Finish all executions before clearing nmi_running */

 The buffers need to be visible before we let NMIs write them */

 Make sure all running NMIs have finished before we write the code */

 Make sure the write happens before clearing the bit */

	/*

	 * Note:

	 * We are paranoid about modifying text, as if a bug was to happen, it

	 * could cause us to read or write to someplace that could cause harm.

	 * Carefully read and modify the code with probe_kernel_*(), and make

	 * sure what we read is what we expected it to be before modifying it.

 read the text we want to modify */

 Make sure it is what we expect it to be */

 replace the text with the new text */

 CONFIG_DYNAMIC_FTRACE */

 CONFIG_DYNAMIC_FTRACE */

/*

 * Hook the return address and push it in the stack of return addrs

 * in the current thread info.

 *

 * This is the main routine for the function graph tracer. The function

 * graph tracer essentially works like this:

 *

 * parent is the stack address containing self_addr's return address.

 * We pull the real return address out of parent and store it in

 * current's ret_stack. Then, we replace the return address on the stack

 * with the address of return_to_handler. self_addr is the function that

 * called mcount.

 *

 * When self_addr returns, it will jump to return_to_handler which calls

 * ftrace_return_to_handler. ftrace_return_to_handler will pull the real

 * return address off of current's ret_stack and jump to it.

	/*

	 * Protect against fault, even if it shouldn't

	 * happen. This tool is too much intrusive to

	 * ignore such a protection.

 CONFIG_FUNCTION_GRAPH_TRACER */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2004 - 2007  Paul Mundt

 invalidate only */

 writeback only */

 writeback and invalidate */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *  Copyright (C) 2000, 2001, 2002 Andi Kleen, SuSE Labs

 *  Copyright (C) 2009  Matt Fleming

 *  Copyright (C) 2002 - 2012  Paul Mundt

/*

 * Print one address/symbol entries per line.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/machvec.c

 *

 * The SuperH machine vector setup handlers, yanked from setup.c

 *

 *  Copyright (C) 1999  Niibe Yutaka

 *  Copyright (C) 2002 - 2007 Paul Mundt

 Boot with the generic vector */

	/*

	 * Only overload the machvec if one hasn't been selected on

	 * the command line with sh_mv=

		/*

		 * Sanity check for machvec section alignment. Ensure

		 * __initmv hasn't been misused.

		/*

		 * If the machvec hasn't been preselected, use the first

		 * vector (usually the only one) from .machvec.init.

	/*

	 * Manually walk the vec, fill in anything that the board hasn't yet

	 * by hand, wrapping to the generic implementation.

 SPDX-License-Identifier: GPL-2.0

/*

 * Performance event support framework for SuperH hardware counters.

 *

 *  Copyright (C) 2009  Paul Mundt

 *

 * Heavily based on the x86 and PowerPC implementations.

 *

 * x86:

 *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>

 *  Copyright (C) 2008-2009 Red Hat, Inc., Ingo Molnar

 *  Copyright (C) 2009 Jaswinder Singh Rajput

 *  Copyright (C) 2009 Advanced Micro Devices, Inc., Robert Richter

 *  Copyright (C) 2008-2009 Red Hat, Inc., Peter Zijlstra

 *  Copyright (C) 2009 Intel Corporation, <markus.t.metzger@intel.com>

 *

 * ppc:

 *  Copyright 2008-2009 Paul Mackerras, IBM Corporation.

 Number of perf_events counting hardware events */

 Used to avoid races in calling reserve/release_pmc_hardware */

/*

 * Stub these out for now, do something more profound later.

/*

 * Release the PMU if this is the last perf_event.

 unpack config */

	/*

	 * See if we need to reserve the counter.

	 *

	 * If no events are currently in use, then we have to take a

	 * mutex to ensure that we don't race with another task doing

	 * reserve_pmc_hardware or release_pmc_hardware.

	/*

	 * Depending on the counter configuration, they may or may not

	 * be chained, in which case the previous counter value can be

	 * updated underneath us if the lower-half overflows.

	 *

	 * Our tactic to handle this is to first atomically read and

	 * exchange a new raw count - then add that new-prev delta

	 * count to the generic counter atomically.

	 *

	 * As there is no interrupt associated with the overflow events,

	 * this is the simplest approach for maintaining consistency.

	/*

	 * Now we have the new raw value and have updated the prev

	 * timestamp already. We can now calculate the elapsed delta

	 * (counter-)time and add that to the generic counter.

	 *

	 * Careful, not all hw sign-extends above the physical width

	 * of the count.

 does not support taken branch sampling */

	/*

	 * All of the on-chip counters are "limited", in that they have

	 * no interrupts, and are therefore unable to do sampling without

	 * further work and timer assistance.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/kernel/sys_sh.c

 *

 * This file contains various random system calls that

 * have a non-standard calling sequence on the Linux/SuperH

 * platform.

 *

 * Taken from i386 version.

	/*

	 * The shift for mmap2 is constant, regardless of PAGE_SIZE

	 * setting.

 sys_cacheflush -- flush (part of) the processor cache.  */

	/*

	 * Verify that the specified address region actually belongs

	 * to this process.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/smp.c

 *

 * SMP support for the SuperH processors.

 *

 * Copyright (C) 2002 - 2010 Paul Mundt

 * Copyright (C) 2006 - 2007 Akio Idehara

 Map physical to logical */

 Map logical to physical */

 State of each CPU */

	/*

	 * Take this CPU offline.  Once we clear this, we can't return,

	 * and we must not schedule until we're ready to give up the cpu.

	/*

	 * OK - migrate IRQs away from this CPU

	/*

	 * Flush user cache and TLB mappings, and then remove this CPU

	 * from the vm mask set of all processes.

 ... !CONFIG_HOTPLUG_CPU */

 We said "no" in __cpu_disable */

 Fill in data in head.S for secondary cpus */

 don't clear bss for secondary cpus */

 Not really SMP stuff ... */

/*

 * The following tlb flush calls are invoked when old translations are

 * being torn down, or pte attributes are changing. For single threaded

 * address spaces, a new context is obtained on the current cpu, and tlb

 * context on other cpus are invalidated to force a new context allocation

 * at switch_mm time, should the mm ever be used on other cpus. For

 * multithreaded address spaces, intercpu interrupts have to be sent.

 * Another case where intercpu interrupts are required is when the target

 * mm might be active on another cpu (eg debuggers doing the flushes on

 * behalf of debugees, kswapd stealing pages from another process etc).

 * Kanoj 07/00.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * SuperH KGDB support

 *

 * Copyright (C) 2008 - 2012  Paul Mundt

 *

 * Single stepping taken from the old stub by Henry Bell and Jeremy Siegel.

 Macros for single step instruction identification */

 Calculate the new address for after a step */

 BT */

 BTS */

 Not in delay slot */

 BF */

 BFS */

 Not in delay slot */

 BRA */

 BRAF */

 BSR */

 BSRF */

 JMP */

 JSR */

 RTS */

 RTE */

 Other */

/*

 * Replace the instruction immediately after the current instruction

 * (i.e. next in the expected flow of control) with a trap instruction,

 * so that returning will cause only a single instruction to be executed.

 * Note that this model is slightly broken for instructions with delay

 * slots (e.g. B[TF]S, BSR, BRA etc), where both the branch and the

 * instruction in the delay slot will be executed.

 Determine where the target instruction will send us to */

 Replace it */

 Flush and return */

 Undo a single step */

 If we have stepped, put back the old instruction */

 Use stepped_address in case we stopped elsewhere */

 Initialize to zero */

	/*

	 * Copy out GP regs 8 to 14.

	 *

	 * switch_to() relies on SR.RB toggling, so regs 0->7 are banked

	 * and need privileged instructions to get to. The r15 value we

	 * fetch from the thread info directly.

	/*

	 * Additional registers we have context for

 Undo any stepping we may have done */

 try to read optional parameter, pc unchanged if no parm */

 this means that we do not want to exit from the handler: */

/*

 * The primary entry points for the kgdb debug trap table entries.

		/*

		 * This means a user thread is single stepping

		 * a system call which should be ignored

	/*

	 * Lowest-prio notifier priority, we want to be notified last:

 Breakpoint instruction: trapa #0x3c */

 SPDX-License-Identifier: GPL-2.0

/**

 * regs_query_register_offset() - query register offset from its name

 * @name:	the name of a register

 *

 * regs_query_register_offset() returns the offset of a register in struct

 * pt_regs from its name. If the name is invalid, this returns -EINVAL;

/**

 * regs_query_register_name() - query register name from its offset

 * @offset:	the offset of a register in struct pt_regs.

 *

 * regs_query_register_name() returns the name of a register from its

 * offset in struct pt_regs. If the @offset is invalid, this returns NULL;

 SPDX-License-Identifier: GPL-2.0

/*

 * swsusp.c - SuperH hibernation support

 *

 * Copyright (C) 2009 Magnus Damm

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/process.c

 *

 * This file handles the architecture-dependent parts of process handling..

 *

 *  Copyright (C) 1995  Linus Torvalds

 *

 *  SuperH version:  Copyright (C) 1999, 2000  Niibe Yutaka & Kaz Kojima

 *		     Copyright (C) 2006 Lineo Solutions Inc. support SH4A UBC

 *		     Copyright (C) 2002 - 2008  Paul Mundt

 Forget lazy FPU state */

 do nothing */

		/* We can use the __save_dsp or just copy the struct:

		 * __save_dsp(p);

		 * p->thread.dsp_status.status |= SR_DSP

 Set return value for child */

/*

 *	switch_to(x,y) should switch tasks from x to y.

 *

 we're going to use this soon, after a few expensive things */

	/*

	 * Restore the kernel mode register

	 *	k7 (r7_bank1)

 no output */

	/*

	 * If the task has used fpu the last 5 timeslices, just do a full

	 * restore of the math state immediately to avoid the trap; the

	 * chances of needing FPU soon are obviously high now

	/*

	 * The same comment as on the Alpha applies here, too ...

 SPDX-License-Identifier: GPL-2.0

/*

 * SHcompact irqflags support

 *

 * Copyright (C) 2006 - 2009 Paul Mundt

 no inputs */

 no inputs */

 SPDX-License-Identifier: GPL-2.0

/*

 *  C interface for trapping into the standard LinuxSH BIOS.

 *

 *  Copyright (C) 2000 Greg Banks, Mitch Davis

 *  Copyright (C) 1999, 2000  Niibe Yutaka

 *  Copyright (C) 2002  M. R. Brown

 *  Copyright (C) 2004 - 2010  Paul Mundt

/*

 * Read the old value of the VBR register to initialise the vector

 * through which debug and BIOS traps are delegated by the Linux trap

 * handler.

/**

 * sh_bios_vbr_reload - Re-load the system VBR from the BIOS vector.

 *

 * This can be used by save/restore code to reinitialize the system VBR

 * from the fixed BIOS VBR. A no-op if no BIOS VBR is known.

/*

 *	Print a string through the BIOS

/*

 *	Setup initial baud/bits/parity. We do two things here:

 *	- construct a cflag setting for the first rs_open()

 *	- initialize the serial port

 *	Return non-zero if we didn't find a serial port.

	/*

	 *	Now construct a cflag setting.

	 *	TODO: this is a totally bogus cflag, as we have

	 *	no idea what serial settings the BIOS is using, or

	 *	even if its using the serial port at all.

no parity*/0;

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Atmel Corporation

 SPDX-License-Identifier: GPL-2.0

/*

 * Kernel probes (kprobes) for SuperH

 *

 * Copyright (C) 2007 Chris Smith <chris.smith@st.com>

 * Copyright (C) 2006 Lineo Solutions, Inc.

 Bad breakpoint */

/**

 * If an illegal slot instruction exception occurs for an address

 * containing a kprobe, remove the probe.

 *

 * Returns 0 if the exception was handled successfully, 1 otherwise.

/*

 * Singlestep is implemented by disabling the current kprobe and setting one

 * on the next instruction, following branches. Two probes are set if the

 * branch is conditional.

 case 1 */

 case 2 */

 case 1 */

 case 2 */

 Called with kretprobe_lock held */

 Replace the return addr with trampoline addr */

	/*

	 * We don't want to be preempted for the entire

	 * duration of kprobe processing

 Check we're not actually recursing */

			/* We have reentered the kprobe_handler(), since

			 * another probe was hit while within the handler.

			 * We here save the original kprobes variables and

			 * just single step on the instruction of the new probe

			 * without calling any user handlers.

 Not one of ours: let kernel handle it */

			/*

			 * The breakpoint instruction was removed right

			 * after we hit it. Another cpu has removed

			 * either a probepoint or a debugger breakpoint

			 * at this address. In either case, no further

			 * handling of this interrupt is appropriate.

 handler has already set things up, so skip ss setup */

/*

 * For function-return probes, init_kprobes() establishes a probepoint

 * here. When a retprobed function returns, this probe is hit and

 * trampoline_probe_handler() runs, calling the kretprobe's handler.

/*

 * Called when we hit the probe point at __kretprobe_trampoline

 Restore back the original saved kprobes variables and continue. */

		/*

		 * We are here because the instruction being single

		 * stepped caused a page fault. We reset the current

		 * kprobe, point the pc back to the probe address

		 * and allow the page fault handler to continue as a

		 * normal page fault.

		/*

		 * In case the user-specified fault handler returned

		 * zero, try to fix up.

		/*

		 * fixup_exception() could not handle it,

		 * Let do_page_fault() fix it.

/*

 * Wrapper routine to for handling exceptions.

 Not a kprobe trap */

 SPDX-License-Identifier: GPL-2.0

/*

 *	crash_dump.c - Memory preserving reboot related code.

 *

 *	Created by: Hariprasad Nellitheertha (hari@in.ibm.com)

 *	Copyright (C) IBM Corporation, 2004. All rights reserved

/**

 * copy_oldmem_page - copy one page from "oldmem"

 * @pfn: page frame number to be copied

 * @buf: target memory address for the copy; this can be in kernel address

 *	space or user address space (see @userbuf)

 * @csize: number of bytes to copy

 * @offset: offset in bytes into the page (based on pfn) to begin the copy

 * @userbuf: if set, @buf is in user address space, use copy_to_user(),

 *	otherwise @buf is in kernel address space, use memcpy().

 *

 * Copy a page from "oldmem". For this page, there is no pte mapped

 * in the current kernel. We stitch up a pte, similar to kmap_atomic.

 SPDX-License-Identifier: GPL-2.0+

/*  Kernel module help for SH.



    SHcompact version by Kaz Kojima and Paul Mundt.



    SHmedia bits:



	Copyright 2004 SuperH (UK) Ltd

	Author: Richard Curnow



	Based on the sh version, and on code from the sh64-specific parts of

	modutils, originally written by Richard Curnow and Ben Gaster.

 This is where to make the change */

		/* This is the symbol it is referring to.  Note that all

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/return_address.c

 *

 * Copyright (C) 2009  Matt Fleming

 * Copyright (C) 2009  Paul Mundt

 Failed to unwind the stack to the specified depth. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2009  Matt Fleming

 *

 * Based, in part, on kernel/time/clocksource.c.

 *

 * This file provides arbitration code for stack unwinders.

 *

 * Multiple stack unwinders can be available on a system, usually with

 * the most accurate unwinder being the currently active one.

/*

 * This is the most basic stack unwinder an architecture can

 * provide. For architectures without reliable frame pointers, e.g.

 * RISC CPUs, it can be implemented by looking through the stack for

 * addresses that lie within the kernel text section.

 *

 * Other CPUs, e.g. x86, can use their frame pointer register to

 * construct more accurate stack traces.

/*

 * "curr_unwinder" points to the stack unwinder currently in use. This

 * is the unwinder with the highest rating.

 *

 * "unwinder_list" is a linked-list of all available unwinders, sorted

 * by rating.

 *

 * All modifications of "curr_unwinder" and "unwinder_list" must be

 * performed whilst holding "unwinder_lock".

/**

 * select_unwinder - Select the best registered stack unwinder.

 *

 * Private function. Must hold unwinder_lock when called.

 *

 * Select the stack unwinder with the best rating. This is useful for

 * setting up curr_unwinder.

/*

 * Enqueue the stack unwinder sorted by rating.

 Keep track of the place, where to insert */

/**

 * unwinder_register - Used to install new stack unwinder

 * @u: unwinder to be registered

 *

 * Install the new stack unwinder on the unwinder list, which is sorted

 * by rating.

 *

 * Returns -EBUSY if registration fails, zero otherwise.

/*

 * Unwind the call stack and pass information to the stacktrace_ops

 * functions. Also handle the case where we need to switch to a new

 * stack dumper because the current one faulted unexpectedly.

	/*

	 * The problem with unwinders with high ratings is that they are

	 * inherently more complicated than the simple ones with lower

	 * ratings. We are therefore more likely to fault in the

	 * complicated ones, e.g. hitting BUG()s. If we fault in the

	 * code for the current stack unwinder we try to downgrade to

	 * one with a lower rating.

	 *

	 * Hopefully this will give us a semi-reliable stacktrace so we

	 * can diagnose why curr_unwinder->dump() faulted.

 Make sure no one beat us to changing the unwinder */

 SPDX-License-Identifier: GPL-2.0

/*

 * Performance event callchain support - SuperH architecture code

 *

 * Copyright (C) 2009  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

 Destroy all of the TLBs in preparation for reset by MMU */

 Address error with SR.BL=1 first. */

 If that fails or is unsupported, go for the watchdog next. */

	/*

	 * Give up and sleep.

 stop other cpus */

 stop this cpu */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/ioport.c

 *

 * Copyright (C) 2000  Niibe Yutaka

 * Copyright (C) 2005 - 2007 Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/kernel/irq.c

 *

 *	Copyright (C) 1992, 1998 Linus Torvalds, Ingo Molnar

 *

 *

 * SuperH version:  Copyright (C) 1999  Niibe Yutaka

/*

 * 'what should we do if we get a hw irq event on an illegal vector'.

 * each architecture has to answer this themselves, it doesn't deserve

 * a generic callback i think.

/*

 * /proc/interrupts printing for arch specific interrupts

/*

 * per-CPU IRQ handling contexts (thread information and stack)

	/*

	 * this is where we switch to the IRQ stack. However, if we are

	 * already using the IRQ stack (because we interrupted a hardirq

	 * handler) we can't do that and just have to keep using the

	 * current stack (which is the irq stack already after all)

		/*

		 * Copy the softirq bits in preempt_count so that the

		 * softirq checks work in the hardirq context.

 switch to the irq stack */

 restore the stack (ring zero) */

 no outputs */

/*

 * allocate per-cpu stacks for hardirq and for softirq processing

 build the stack frame on the softirq stack */

 switch to the softirq stack */

 restore the thread stack */

 no outputs */

 Perform the machine specific initialisation */

/*

 * The CPU has been marked offline.  Migrate IRQs off this CPU.  If

 * the affinity settings do not allow other CPUs, force them onto any

 * available CPU.

 SPDX-License-Identifier: GPL-2.0

 need in pfn_valid macro */

 SPDX-License-Identifier: GPL-2.0

/*

 * This program is used to generate definitions needed by

 * assembly language modules.

 *

 * We use the technique used in the OSF Mach kernel code:

 * generate asm statements containing #defines,

 * compile this file to assembler, and then extract the

 * #defines from the assembly-language output.

 offsets into the thread_info struct */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/vsyscall/vsyscall.c

 *

 *  Copyright (C) 2006 Paul Mundt

 *

 * vDSO randomization

 * Copyright(C) 2005-2006, Red Hat, Inc., Ingo Molnar

/*

 * Should the kernel map a VDSO page into processes and pass its

 * address down to glibc upon exec()?

/*

 * These symbols are defined by vsyscall.o to mark the bounds

 * of the ELF DSO images included therein.

	/*

	 * XXX: Map this page to a fixmap entry if we get around

	 * to adding the page to ELF core dumps

 Setup a VMA at program startup for the vsyscall page */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH Pin Function Control Initialization

 *

 * Copyright (C) 2012  Renesas Solutions Corp.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/clock.c - SuperH clock framework

 *

 *  Copyright (C) 2005 - 2009  Paul Mundt

 *

 * This clock framework is derived from the OMAP version by:

 *

 *	Copyright (C) 2004 - 2008 Nokia Corporation

 *	Written by Tuukka Tikkanen <tuukka.tikkanen@elektrobit.com>

 *

 *  Modified for omap shared clock framework by Tony Lindgren <tony@atomide.com>

 Kick the child clocks.. */

 Enable the necessary init clocks */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Memory allocation at the first usage of the FPU and other state.

		/*

		 * does a slab alloc which can sleep

			/*

			 * ran out of memory!

 CONFIG_SH_FPU */

 SPDX-License-Identifier: GPL-2.0

/*

 * The ordering of these clocks matters, do not change it.

 main clocks */

/*

 * Placeholder for compatibility, until the lazy CPUs do this

 * on their own.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/kernel/adc.c -- SH3 on-chip ADC support

 *

 *  Copyright (C) 2004  Andriy Skulysh <askulysh@image.kiev.ua>

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/init.c

 *

 * CPU init code

 *

 * Copyright (C) 2002 - 2009  Paul Mundt

 * Copyright (C) 2003  Richard Curnow

/*

 * Generic wrapper for command line arguments to disable on-chip

 * peripherals (nofpu, nodsp, and so forth).

 Clear RABD */

 Flush the update */

	/*

	 * Future proofing.

	 *

	 * Disable support for slottable sleep instruction, non-nop

	 * instructions in the rte delay slot, and associative writes to

	 * the memory-mapped cache array.

 2nd-level cache init */

/*

 * Generic first-level cache init

	/*

	 * At this point we don't know whether the cache is enabled or not - a

	 * bootloader may have enabled it.  There are at least 2 things that

	 * could be dirty in the cache at this point:

	 * 1. kernel command line set up by boot loader

	 * 2. spilled registers from the prolog of this function

	 * => before re-initialising the cache, we must do a purge of the whole

	 * cache out to memory for safety.  As long as nothing is spilled

	 * during the loop to lines that have already been done, this is safe.

	 * - RPC

		/*

		 * If the OC is already in RAM mode, we only have

		 * half of the entries to flush..

 If EMODE is not set, we only have 1 way to flush. */

	/*

	 * Default CCR values .. enable the caches

	 * and invalidate them immediately..

 Force EMODE if possible */

 Write-through */

 Write-back */

 Off */

 No S-cache */

 Disable the FPU */

 Clear SR.DSP bit */

	/*

	 * Set the SR.DSP bit, wait for one instruction, and then read

	 * back the SR value.

 If the DSP bit is still set, this CPU has a DSP */

 Disable the DSP */

 Now that we've determined the DSP status, clear the DSP bit. */

 CONFIG_SH_DSP */

/**

 * cpu_init

 *

 * This is our initial entry point for each CPU, and is invoked on the

 * boot CPU prior to calling start_kernel(). For SMP, a combination of

 * this and start_secondary() will bring up each processor to a ready

 * state prior to hand forking the idle loop.

 *

 * We do all of the basic processor init here, including setting up

 * the caches, FPU, DSP, etc. By the time start_kernel() is hit (and

 * subsequently platform_setup()) things like determining the CPU

 * subtype and initial configuration will all be done.

 *

 * Each processor family is still responsible for doing its own probing

 * and cache configuration in cpu_probe().

 First, probe the CPU */

 First setup the rest of the I-cache info */

 And the D-cache too */

 Init the cache */

 Boot CPU sets the cache shape */

	/*

	 * Initialize the per-CPU ASID cache very early, since the

	 * TLB flushing routines depend on this being setup.

 Do the rest of the boot processor setup */

 Save off the BIOS VBR, if there is one */

		/*

		 * Setup VBR for boot CPU. Secondary CPUs do this through

		 * start_secondary().

		/*

		 * Boot processor to setup the FP and extended state

		 * context info.

 SPDX-License-Identifier: GPL-2.0

 Symbolic CPU flags, keep in sync with asm/cpu-features.h */

/*

 *	Get CPU information for use by the procfs.

	/*

	 * Check for what type of cache we have, we support both the

	 * unified cache on the SH-2 and SH-3, as well as the harvard

	 * style cache on the SH-4.

 Optional secondary cache */

 CONFIG_PROC_FS */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/shmobile/pm.c

 *

 * Power management support code for SuperH Mobile

 *

 *  Copyright (C) 2009 Magnus Damm

/*

 * Notifier lists for pre/post sleep notification

/*

 * Sleep modes available on SuperH Mobile:

 *

 * Sleep mode is just plain "sleep" instruction

 * Sleep Self-Refresh mode is above plus RAM put in Self-Refresh

 * Standby Self-Refresh mode is above plus stopped clocks

 /*

  * U-standby mode is unsupported since it needs bootloader hacks

 RSMEM */

 ILRAM */

 code located directly after data structure */

 flush the caches if MMU flag is set */

 Let assembly snippet in on-chip memory handle the rest */

 part 0: data area */

 STBCR */

 BAR */

 PTEH */

 PTEL */

 TTB */

 TEA */

 MMUCR */

 PTEA */

 PASCR */

 IRMCR */

 CCR */

 RAMCR */

 part 1: common code to enter sleep mode */

 part 2: board specific code to enter self-refresh mode */

 part 3: board specific code to resume from self-refresh mode */

 part 4: common code to resume from sleep mode */

 located at interrupt vector */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/shmobile/cpuidle.c

 *

 * Cpuidle support code for SuperH Mobile

 *

 *  Copyright (C) 2009 Magnus Damm

 regular sleep mode */

 sleep mode + self refresh */

 software standby mode + self refresh */

 convert allowed mode to allowed state */

	/* take the following into account for sleep mode selection:

	 * - allowed_state: best mode allowed by hardware (clock deps)

	 * - requested_state: best mode allowed by software (latencies)

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7619 Setup

 *

 *  Copyright (C) 2006  Yoshinori Sato

 *  Copyright (C) 2009  Paul Mundt

 interrupt sources */

 IPRA */ { IRQ0, IRQ1, IRQ2, IRQ3 } },

 IPRB */ { IRQ4, IRQ5, IRQ6, IRQ7 } },

 IPRC */ { WDT, EDMAC, CMT0, CMT1 } },

 IPRD */ { SCIF0, SCIF1, SCIF2 } },

 IPRE */ { HIF_HIFI, HIF_HIFBI } },

 IPRF */ { DMAC0, DMAC1, DMAC2, DMAC3 } },

 IPRG */ { SIOF } },

 enable CMT clock */

 SPDX-License-Identifier: GPL-2.0

/*

 * SMP support for J2 processor

 *

 * Copyright (C) 2015-2016 Smart Energy Instruments, Inc.

	/* Disable any cpus past max_cpus, or all secondaries if we didn't

	/* There is only one IPI interrupt shared by all messages, so

 Generate the actual interrupt by writing to CCRn bit 28. */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh2/clock-sh7619.c

 *

 * SH7619 support for the clock framework

 *

 *  Copyright (C) 2006  Yoshinori Sato

 *

 * Based on clock-sh4.c

 *  Copyright (C) 2005  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh2/probe.c

 *

 * CPU Subtype Probing for SH-2.

 *

 * Copyright (C) 2002 Paul Mundt

	/* These defaults are appropriate for the original/current

	 * J2 cache. Once there is a proper framework for getting cache

	/*

	 * SH-2 doesn't have separate caches

 SPDX-License-Identifier: GPL-2.0

/*

 * SH4-202 Setup

 *

 *  Copyright (C) 2006  Paul Mundt

 *  Copyright (C) 2009  Magnus Damm

 interrupt sources */

 only IRLM mode supported */

 IPRA */ { TMU0, TMU1, TMU2, RTC } },

 IPRB */ { WDT, 0, 0, 0 } },

 IPRC */ { 0, 0, SCIF, HUDI } },

 IPRD */ { IRL0, IRL1, IRL2, IRL3 } },

 individual interrupt mode for IRL3-0 */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7760 Setup

 *

 *  Copyright (C) 2006  Paul Mundt

 interrupt sources */

 interrupt groups */

 0xf80 according to data sheet */

 INTMSK00 / INTMSKCLR00 */

 INTMSK04 / INTMSKCLR04 */

 IPRA */ { TMU0, TMU1, TMU2 } },

 IPRB */ { WDT, REF, 0, 0 } },

 IPRC */ { GPIOI, DMAC, 0, HUDI } },

 IPRD */ { IRL0, IRL1, IRL2, IRL3 } },

 INTPRI00 */ { IRQ4, IRQ5, IRQ6, IRQ7 } },

 INTPRI04 */ { HCAN20, HCAN21, SSI0, SSI1,

 INTPRI08 */ { USB, LCDC, DMABRG, SCIF0,

 INTPRI0C */ { 0, 0, MMCIF, 0,

	/*

	 * This is actually a SIM card module serial port, based on an SCI with

	 * additional registers. The sh-sci driver doesn't support the SIM port

	 * type, declare it as a SCI. Don't declare the additional registers in

	 * the memory resource or the driver will compute an incorrect regshift

	 * value.

/*

 * Floating point emulation support for subnormalised numbers on SH4

 * architecture This file is derived from the SoftFloat IEC/IEEE

 * Floating-point Arithmetic Package, Release 2 the original license of

 * which is reproduced below.

 *

 * ========================================================================

 *

 * This C source file is part of the SoftFloat IEC/IEEE Floating-point

 * Arithmetic Package, Release 2.

 *

 * Written by John R. Hauser.  This work was made possible in part by the

 * International Computer Science Institute, located at Suite 600, 1947 Center

 * Street, Berkeley, California 94704.  Funding was partially provided by the

 * National Science Foundation under grant MIP-9311980.  The original version

 * of this code was written as part of a project to build a fixed-point vector

 * processor in collaboration with the University of California at Berkeley,

 * overseen by Profs. Nelson Morgan and John Wawrzynek.  More information

 * is available through the web page `http://HTTP.CS.Berkeley.EDU/~jhauser/

 * arithmetic/softfloat.html'.

 *

 * THIS SOFTWARE IS DISTRIBUTED AS IS, FOR FREE.  Although reasonable effort

 * has been made to avoid it, THIS SOFTWARE MAY CONTAIN FAULTS THAT WILL AT

 * TIMES RESULT IN INCORRECT BEHAVIOR.  USE OF THIS SOFTWARE IS RESTRICTED TO

 * PERSONS AND ORGANIZATIONS WHO CAN AND WILL TAKE FULL RESPONSIBILITY FOR ANY

 * AND ALL LOSSES, COSTS, OR OTHER PROBLEMS ARISING FROM ITS USE.

 *

 * Derivative works are acceptable, even for commercial purposes, so long as

 * (1) they include prominent notice that the work is derivative, and (2) they

 * include prominent notice akin to these three paragraphs for those parts of

 * this code that are retained.

 *

 * ========================================================================

 *

 * SH4 modifications by Ismail Dhaoui <ismail.dhaoui@st.com>

 * and Kamel Khelifi <kamel.khelifi@st.com>

 in fpu.c */

 in fpu.c */

 SH4 has only 2 rounding modes - round to nearest and round to zero */

 SH4 has only 2 rounding modes - round to nearest and round to zero */

/*

 * -------------------------------------------------------------------------------

 *  Returns the result of converting the double-precision floating-point value

 *  `a' to the single-precision floating-point format.  The conversion is

 *  performed according to the IEC/IEEE Standard for Binary Floating-point

 *  Arithmetic.

 *  -------------------------------------------------------------------------------

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4/clock-sh4.c

 *

 * Generic SH-4 support for the clock framework

 *

 *  Copyright (C) 2005  Paul Mundt

 *

 * FRQCR parsing hacked out of arch/sh/kernel/time.c

 *

 *  Copyright (C) 1999  Tetsuya Okada & Niibe Yutaka

 *  Copyright (C) 2000  Philipp Rumpf <prumpf@tux.org>

 *  Copyright (C) 2002, 2003, 2004  Paul Mundt

 *  Copyright (C) 2002  M. R. Brown  <mrbrown@linux-sh.org>

 Same */

 SPDX-License-Identifier: GPL-2.0

/*

 * Save/restore floating point context for signal handlers.

 *

 * Copyright (C) 1999, 2000  Kaz Kojima & Niibe Yutaka

 * Copyright (C) 2006  ST Microelectronics Ltd. (denorm support)

 *

 * FIXME! These routines have not been tested for big endian case.

/* The PR (precision) bit in the FP Status Register must be clear when

 * an frchg instruction is executed, otherwise the instruction is undefined.

 * Executing frchg with PR set causes a trap on some SH4 implementations.

/*

 * Save FPU registers onto task structure.

/**

 *      denormal_to_double - Given denormalized float number,

 *                           store double float

 *

 *      @fpu: Pointer to sh_fpu_hard structure

 *      @n: Index to FP register

/**

 *	ieee_fpe_handler - Handle denormalized number exception

 *

 *	@regs: Pointer to register structure

 *

 *	Returns 1 when it's handled (should not cause exception).

 bsr & jsr */

 bra & bsr */

 bt/s */

 bf/s */

 jmp & jsr */

 braf & bsrf */

 rts */

 fcnvsd */

 FPU error */

 fmul */

 FPU error because of denormal (doubles) */

 FPU error because of denormal (floats) */

 fadd, fsub */

 FPU error because of denormal (doubles) */

 FPU error because of denormal (floats) */

 fdiv */

 FPU error because of denormal (doubles) */

 FPU error because of denormal (floats) */

 fcnvds - double to single precision convert */

 subnormal double to float conversion */

		/* Set the FPSCR flag as well as cause bits - simply

 SPDX-License-Identifier: GPL-2.0

/*

 * Performance events support for SH7750-style performance counters

 *

 *  Copyright (C) 2009  Paul Mundt

 16-bit */

 32-bit */

/*

 * There are a number of events supported by each counter (33 in total).

 * Since we have 2 counters, each counter will take the event code as it

 * corresponds to the PMCR PMM setting. Each counter can be configured

 * independently.

 *

 *	Event Code	Description

 *	----------	-----------

 *

 *	0x01		Operand read access

 *	0x02		Operand write access

 *	0x03		UTLB miss

 *	0x04		Operand cache read miss

 *	0x05		Operand cache write miss

 *	0x06		Instruction fetch (w/ cache)

 *	0x07		Instruction TLB miss

 *	0x08		Instruction cache miss

 *	0x09		All operand accesses

 *	0x0a		All instruction accesses

 *	0x0b		OC RAM operand access

 *	0x0d		On-chip I/O space access

 *	0x0e		Operand access (r/w)

 *	0x0f		Operand cache miss (r/w)

 *	0x10		Branch instruction

 *	0x11		Branch taken

 *	0x12		BSR/BSRF/JSR

 *	0x13		Instruction execution

 *	0x14		Instruction execution in parallel

 *	0x15		FPU Instruction execution

 *	0x16		Interrupt

 *	0x17		NMI

 *	0x18		trapa instruction execution

 *	0x19		UBCA match

 *	0x1a		UBCB match

 *	0x21		Instruction cache fill

 *	0x22		Operand cache fill

 *	0x23		Elapsed time

 *	0x24		Pipeline freeze by I-cache miss

 *	0x25		Pipeline freeze by D-cache miss

 *	0x27		Pipeline freeze by branch instruction

 *	0x28		Pipeline freeze by CPU register

 *	0x29		Pipeline freeze by FPU

 I-cache */

 I-cache */

	/*

	 * Make sure this CPU actually has perf counters.

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7091/SH7750/SH7750S/SH7750R/SH7751/SH7751R Setup

 *

 *  Copyright (C) 2006  Paul Mundt

 *  Copyright (C) 2006  Jamie Lenehan

 Shared Period/Carry/Alarm IRQ */

 SH7750R, SH7751 and SH7751R all have two extra timer channels */

 interrupt sources */

 only IRLM mode supported */

 interrupt groups */

 IPRA */ { TMU0, TMU1, TMU2, RTC } },

 IPRB */ { WDT, REF, SCI1, 0 } },

 IPRC */ { GPIOI, DMAC, SCIF, HUDI } },

 IPRD */ { IRL0, IRL1, IRL2, IRL3 } },

 INTPRI00 */ { 0, 0, 0, 0,

 SH7750, SH7750S, SH7751 and SH7091 all have 4-channel DMA controllers */

 SH7750R and SH7751R both have 8-channel DMA controllers */

 SH7750R, SH7751 and SH7751R all have two extra timer channels */

 INTMSK00 / INTMSKCLR00 */

 SH7750S, SH7750R, SH7751 and SH7751R all have IRLM priority registers */

 SH7751 and SH7751R both have PCI */

	/*

	 * same vectors for SH7750, SH7750S and SH7091 except for IRLM,

	 * see below..

 impossible to mask interrupts on SH7750 and SH7091 */

 individual interrupt mode for IRL3-0 */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4/sq.c

 *

 * General management API for SH-4 integrated Store Queues

 *

 * Copyright (C) 2001 - 2006  Paul Mundt

 * Copyright (C) 2001, 2002  M. R. Brown

/**

 * sq_flush_range - Flush (prefetch) a specific SQ range

 * @start: the store queue address to start flushing from

 * @len: the length to flush

 *

 * Flushes the store queue cache from @start to @start + @len in a

 * linear fashion.

 Flush the queues */

 Wait for completion */

	/*

	 * Without an MMU (or with it turned off), this is much more

	 * straightforward, as we can just load up each queue's QACR with

	 * the physical address appropriately masked.

/**

 * sq_remap - Map a physical address through the Store Queues

 * @phys: Physical address of mapping.

 * @size: Length of mapping.

 * @name: User invoking mapping.

 * @prot: Protection bits.

 *

 * Remaps the physical address @phys through the next available store queue

 * address of @size length. @name is logged at boot time as well as through

 * the sysfs interface.

 Don't allow wraparound or zero size */

 Don't allow anyone to remap normal memory.. */

/**

 * sq_unmap - Unmap a Store Queue allocation

 * @vaddr: Pre-allocated Store Queue mapping.

 *

 * Unmaps the store queue allocation @map that was previously created by

 * sq_remap(). Also frees up the pte that was previously inserted into

 * the kernel page table and discards the UTLB translation.

		/*

		 * Tear down the VMA in the MMU case.

/*

 * Needlessly complex sysfs interface. Unfortunately it doesn't seem like

 * there is any other easy way to add things on a per-cpu basis without

 * putting the directory entries somewhere stupid and having to create

 * links in sysfs by hand back in to the per-cpu directories.

 *

 * Some day we may want to have an additional abstraction per store

 * queue, but considering the kobject hell we already have to deal with,

 * it's simply not worth the trouble.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4/clock-sh4-202.c

 *

 * Additional SH4-202 support for the clock framework

 *

 *  Copyright (C) 2005  Paul Mundt

 Safe fallback */

	/*

	 * For some reason, the shoc_clk seems to be set to some really

	 * insane value at boot (values outside of the allowable frequency

	 * range for instance). We deal with this by scaling it back down

	 * to something sensible just in case.

	 *

	 * Start scaling from the high end down until we find something

	 * that passes rate verification..

 Undefined clock */

 Make sure we have something sensible to switch to */

 main clocks */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4/probe.c

 *

 * CPU Subtype Probing for SH-4.

 *

 * Copyright (C) 2001 - 2007  Paul Mundt

 * Copyright (C) 2003  Richard Curnow

	/*

	 * Setup some sane SH-4 defaults for the icache

	/*

	 * And again for the dcache ..

 We don't know the chip cut */

	/*

	 * Setup some generic flags we can probe on SH-4A parts

 And some SH-4 defaults.. */

 FPU detection works for almost everyone */

 Mask off the upper chip ID */

	/*

	 * Probe the underlying processor version/revision and

	 * adjust cpu_data setup accordingly.

 yon-ten-go */

 0x4E0 */

 SH7733/SH7734 */

 1st cut */

 2nd cut */

	/*

	 * On anything that's not a direct-mapped cache, look to the CVR

	 * for I/D-cache specifics.

 And the rest of the D-cache */

	/*

	 * SH-4A's have an optional PIPT L2.

		/*

		 * Verify that it really has something hooked up, this

		 * is the safety net for CPUs that have optional L2

		 * support yet do not implement it.

			/*

			 * Silicon and specifications have clearly never

			 * met..

			/*

			 * Size calculation is much more sensible

			 * than it is for the L1.

			 *

			 * Sizes are 128KB, 256KB, 512KB, and 1MB.

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7203 and SH7263 Setup

 *

 *  Copyright (C) 2007 - 2009  Paul Mundt

 interrupt sources */

 ROM-DEC, SDHI, SRC, and IEB are SH7263 specific */

 interrupt groups */

 SH7263-specific trash */

 IPR01 */ { IRQ0, IRQ1, IRQ2, IRQ3 } },

 IPR02 */ { IRQ4, IRQ5, IRQ6, IRQ7 } },

 IPR05 */ { PINT, 0, 0, 0 } },

 IPR06 */ { DMAC0, DMAC1, DMAC2, DMAC3 } },

 IPR07 */ { DMAC4, DMAC5, DMAC6, DMAC7 } },

 IPR08 */ { USB, LCDC, CMT0, CMT1 } },

 IPR09 */ { BSC, WDT, MTU0_ABCD, MTU0_VEF } },

 IPR10 */ { MTU1_AB, MTU1_VU, MTU2_AB,

 IPR11 */ { MTU3_ABCD, MTU2_TCI3V, MTU4_ABCD,

 IPR12 */ { ADC_ADI, IIC30, IIC31, IIC32 } },

 IPR13 */ { IIC33, SCIF0, SCIF1, SCIF2 } },

 IPR14 */ { SCIF3, SSU0, SSU1, SSI0_SSII } },

 IPR15 */ { SSI1_SSII, SSI2_SSII,

 IPR16 */ { FLCTL, 0, RTC, RCAN0 } },

 IPR17 */ { RCAN1, 0, 0, 0 } },

 IPR15 */ { SSI1_SSII, SSI2_SSII,

 IPR16 */ { FLCTL, SDHI, RTC, RCAN0 } },

 IPR17 */ { RCAN1, SRC, IEBI, 0 } },

 PINTER */

 Shared Period/Carry/Alarm IRQ */

 enable CMT clock */

 enable MTU2 clock */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7264 Setup

 *

 * Copyright (C) 2012  Renesas Electronics Europe Ltd

 interrupt sources */

 interrupt groups */

 IPR01 */ { IRQ0, IRQ1, IRQ2, IRQ3 } },

 IPR02 */ { IRQ4, IRQ5, IRQ6, IRQ7 } },

 IPR05 */ { PINT, 0, 0, 0 } },

 IPR06 */ { DMAC0,  DMAC1,  DMAC2,  DMAC3 } },

 IPR07 */ { DMAC4,  DMAC5,  DMAC6,  DMAC7 } },

 IPR08 */ { DMAC8,  DMAC9,

 IPR09 */ { DMAC12, DMAC13,

 IPR10 */ { USB, VDC3, CMT0, CMT1 } },

 IPR11 */ { BSC, WDT, MTU0_ABCD, MTU0_VEF } },

 IPR12 */ { MTU1_AB, MTU1_VU,

 IPR13 */ { MTU3_ABCD, MTU3_TCI3V,

 IPR14 */ { PWMT1, PWMT2, ADC_ADI, 0 } },

 IPR15 */ { SSIF0, SSII1, SSII2, SSII3 } },

 IPR16 */ { RSPDIF, IIC30, IIC31, IIC32 } },

 IPR17 */ { SCIF0, SCIF1, SCIF2, SCIF3 } },

 IPR18 */ { SCIF4, SCIF5, SCIF6, SCIF7 } },

 IPR19 */ { SIO_FIFO, 0, RSPIC0, RSPIC1, } },

 IPR20 */ { RCAN0, RCAN1, IEBC, CD_ROMD } },

 IPR21 */ { NFMC, SDHI, RTC, 0 } },

 IPR22 */ { SRCC0, SRCC1, 0, DCOMU } },

 PINTER */

 Shared Period/Carry/Alarm IRQ */

 USB Host */

 Initialise UACS25 */

  not use dma */

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas MX-G (R8A03022BG) Setup

 *

 *  Copyright (C) 2008, 2009  Paul Mundt

 interrupt sources */

 interrupt groups */

 IPR01 */ { IRQ0, IRQ1, IRQ2, IRQ3 } },

 IPR02 */ { IRQ4, IRQ5, IRQ6, IRQ7 } },

 IPR03 */ { IRQ8, IRQ9, IRQ10, IRQ11 } },

 IPR04 */ { IRQ12, IRQ13, IRQ14, IRQ15 } },

 IPR05 */ { PINT, 0, 0, 0 } },

 IPR06 */ { } },

 IPR07 */ { } },

 IPR08 */ { } },

 IPR09 */ { } },

 IPR10 */ { } },

 IPR11 */ { } },

 IPR12 */ { } },

 IPR13 */ { } },

 IPR14 */ { 0, 0, 0, SCIF0 } },

 IPR15 */

 IPR16 */

 PINTER */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh2a/clock-sh7269.c

 *

 * SH7269 clock framework support

 *

 * Copyright (C) 2012  Phil Edworthy

 SH7269 registers */

 Fixed 32 KHz root clock for RTC */

/*

 * Default rate for the root input clock, reset this with clk_set_rate()

 * from the platform code.

 The mask field specifies the div2 entries that are valid */

 CMT */

 USB */

 SCIF0 */

 SCIF1 */

 SCIF2 */

 SCIF3 */

 SCIF4 */

 SCIF5 */

 SCIF6 */

 SCIF7 */

 MTU2 */

 ADC */

 RTC */

 main clocks */

 DIV4 clocks */

 MSTP clocks */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7264 Pinmux

 *

 *  Copyright (C) 2012  Renesas Electronics Europe Ltd

 SPDX-License-Identifier: GPL-2.0

/*

 *  SH7201 setup

 *

 *  Copyright (C) 2008  Peter Griffin pgriffin@mpc-data.co.uk

 *  Copyright (C) 2009  Paul Mundt

 interrupt sources */

 interrupt groups */

 IPR01 */ { IRQ0, IRQ1, IRQ2, IRQ3 } },

 IPR02 */ { IRQ4, IRQ5, IRQ6, IRQ7 } },

 IPR05 */ { PINT, 0, ADC_ADI, 0 } },

 IPR06 */ { 0, MTU20_ABCD, MTU20_VEF, MTU21_AB } },

 IPR07 */ { MTU21_VU, MTU22_AB, MTU22_VU,  MTU23_ABCD } },

 IPR08 */ { MTU2_TCI3V, MTU24_ABCD, MTU2_TCI4V, MTU25_UVW } },

 IPR09 */ { RTC, WDT, IIC30, 0 } },

 IPR10 */ { IIC31, IIC32, DMAC0_DMINT0, DMAC1_DMINT1 } },

 IPR11 */ { DMAC2_DMINT2, DMAC3_DMINT3, SCIF0, SCIF1 } },

 IPR12 */ { SCIF2, SCIF3, SCIF4, SCIF5 } },

 IPR13 */ { SCIF6, SCIF7, DMAC0_DMINTA, DMAC4_DMINT4  } },

 IPR14 */ { DMAC5_DMINT5, DMAC6_DMINT6, DMAC7_DMINT7, 0 } },

 IPR15 */ { 0, RCAN0, RCAN1, 0 } },

 IPR16 */ { SSI0_SSII, SSI1_SSII, TMR0, TMR1 } },

 PINTER */

 Shared Period/Carry/Alarm IRQ */

 enable MTU2 clock */

 SPDX-License-Identifier: GPL-2.0

/*

 * Save/restore floating point context for signal handlers.

 *

 * Copyright (C) 1999, 2000  Kaz Kojima & Niibe Yutaka

 *

 * FIXME! These routines can be optimized in big endian case.

/* The PR (precision) bit in the FP Status Register must be clear when

 * an frchg instruction is executed, otherwise the instruction is undefined.

 * Executing frchg with PR set causes a trap on some SH4 implementations.

/*

 * Save FPU registers onto task structure.

/*

 *	Emulate arithmetic ops on denormalized number for some FPU insns.

 denormalized float * float */

 FIXME: use guard bits */

 denormalized double * double */

 FIXME: use guard bits */

 ix - iy where iy: denormal and ix, iy >= 0 */

 ix + iy where iy: denormal and ix, iy >= 0 */

 ix - iy where iy: denormal and ix, iy >= 0 */

 ix + iy where iy: denormal and ix, iy >= 0 */

/**

 *	denormal_to_double - Given denormalized float number,

 *	                     store double float

 *

 *	@fpu: Pointer to sh_fpu_hard structure

 *	@n: Index to FP register

/**

 *	ieee_fpe_handler - Handle denormalized number exception

 *

 *	@regs: Pointer to register structure

 *

 *	Returns 1 when it's handled (should not cause exception).

 bsr & jsr */

 bra & bsr */

 bt/s */

 bf/s */

 jmp & jsr */

 braf & bsrf */

 rts */

 fcnvsd */

 FPU error */

 fmul */

 FPU error because of denormal */

 FPU error because of denormal */

 fadd, fsub */

 FPU error because of denormal */

 FPU error because of denormal */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7269 Setup

 *

 * Copyright (C) 2012  Renesas Electronics Europe Ltd

 * Copyright (C) 2012  Phil Edworthy

 interrupt sources */

 interrupt groups */

 IPR01 */ { IRQ0, IRQ1, IRQ2, IRQ3 } },

 IPR02 */ { IRQ4, IRQ5, IRQ6, IRQ7 } },

 IPR05 */ { PINT, 0, 0, 0 } },

 IPR06 */ { DMAC0,  DMAC1, DMAC2,  DMAC3 } },

 IPR07 */ { DMAC4,  DMAC5, DMAC6,  DMAC7 } },

 IPR08 */ { DMAC8,  DMAC9,

 IPR09 */ { DMAC12, DMAC13,

 IPR10 */ { USB, VDC4, VDC4, VDC4 } },

 IPR11 */ { 0, 0, 0, 0 } },

 IPR12 */ { CMT0, CMT1, BSC, WDT } },

 IPR13 */ { MTU0_ABCD, MTU0_VEF,

 IPR14 */ { MTU2_AB, MTU2_VU,

 IPR15 */ { MTU4_ABCD, MTU4_TCI4V,

 IPR16 */ { 0, 0, 0, 0 } },

 IPR17 */ { ADC_ADI, SSIF0, SSII1, SSII2 } },

 IPR18 */ { SSII3, SSII4, SSII5,  RSPDIF} },

 IPR19 */ { IIC30, IIC31, IIC32, IIC33 } },

 IPR20 */ { SCIF0, SCIF1, SCIF2, SCIF3 } },

 IPR21 */ { SCIF4, SCIF5, SCIF6, SCIF7 } },

 IPR22 */ { 0, RCAN0, RCAN1, RCAN2 } },

 IPR23 */ { RSPIC0, RSPIC1, 0, 0 } },

 IPR24 */ { IEBC, CD_ROMD, NFMC, 0 } },

 IPR25 */ { SDHI0, SDHI1, RTC, 0 } },

 IPR26 */ { SRCC0, SRCC1, SRCC2, 0 } },

 PINTER */

 Shared Period/Carry/Alarm IRQ */

 USB Host */

  not use dma */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh2a/clock-sh7201.c

 *

 * SH7201 support for the clock framework

 *

 *  Copyright (C) 2008 Peter Griffin  <pgriffin@mpc-data.co.uk>

 *

 * Based on clock-sh4.c

 *  Copyright (C) 2005  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh2a/clock-sh7203.c

 *

 * SH7203 support for the clock framework

 *

 *  Copyright (C) 2007 Kieran Bingham (MPC-Data Ltd)

 *

 * Based on clock-sh7263.c

 *  Copyright (C) 2006  Yoshinori Sato

 *

 * Based on clock-sh4.c

 *  Copyright (C) 2005  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7269 Pinmux

 *

 * Copyright (C) 2012  Renesas Electronics Europe Ltd

 * Copyright (C) 2012  Phil Edworthy

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh2a/clock-sh7206.c

 *

 * SH7206 support for the clock framework

 *

 *  Copyright (C) 2006  Yoshinori Sato

 *

 * Based on clock-sh4.c

 *  Copyright (C) 2005  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh2a/clock-sh7264.c

 *

 * SH7264 clock framework support

 *

 * Copyright (C) 2012  Phil Edworthy

 SH7264 registers */

 Fixed 32 KHz root clock for RTC */

/*

 * Default rate for the root input clock, reset this with clk_set_rate()

 * from the platform code.

 The mask field specifies the div2 entries that are valid */

 SCIF */

 VDC */

 CMT */

 USB */

 MTU2 */

 SDHI0 */

 SDHI1 */

 ADC */

 RTC */

 main clocks */

 DIV4 clocks */

 MSTP clocks */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7206 Setup

 *

 *  Copyright (C) 2006  Yoshinori Sato

 *  Copyright (C) 2009  Paul Mundt

 interrupt sources */

 interrupt groups */

 IPR01 */ { IRQ0, IRQ1, IRQ2, IRQ3 } },

 IPR02 */ { IRQ4, IRQ5, IRQ6, IRQ7 } },

 IPR05 */ { PINT, 0, ADC_ADI0, ADC_ADI1 } },

 IPR06 */ { DMAC0, DMAC1, DMAC2, DMAC3 } },

 IPR07 */ { DMAC4, DMAC5, DMAC6, DMAC7 } },

 IPR08 */ { CMT0, CMT1, BSC, WDT } },

 IPR09 */ { MTU0_ABCD, MTU0_VEF,

 IPR10 */ { MTU2_AB, MTU2_VU,

 IPR11 */ { MTU4_ABCD, MTU2_TCI4V,

 IPR12 */ { MTU3S_ABCD, MTU2S_TCI3V,

 IPR13 */ { MTU5S, POE2_OEI3, IIC3, 0 } },

 IPR14 */ { SCIF0, SCIF1, SCIF2, SCIF3 } },

 PINTER */

 enable CMT clock */

 enable MTU2 clock */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7203 Pinmux

 *

 *  Copyright (C) 2008  Magnus Damm

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh2a/probe.c

 *

 * CPU Subtype Probing for SH-2A.

 *

 * Copyright (C) 2004 - 2007  Paul Mundt

 All SH-2A CPUs have support for 16 and 32-bit opcodes.. */

	/*

	 * The icache is the same as the dcache as far as this setup is

	 * concerned. The only real difference in hardware is that the icache

	 * lacks the U bit that the dcache has, none of this has any bearing

	 * on the cache info.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh2a/opcode_helper.c

 *

 * Helper for the SH-2A 32-bit opcodes.

 *

 *  Copyright (C) 2007  Paul Mundt

/*

 * Instructions on SH are generally fixed at 16-bits, however, SH-2A

 * introduces some 32-bit instructions. Since there are no real

 * constraints on their use (and they can be mixed and matched), we need

 * to check the instruction encoding to work out if it's a true 32-bit

 * instruction or not.

 *

 * Presently, 32-bit opcodes have only slight variations in what the

 * actual encoding looks like in the first-half of the instruction, which

 * makes it fairly straightforward to differentiate from the 16-bit ones.

 *

 * First 16-bits of encoding		Used by

 *

 *	0011nnnnmmmm0001	mov.b, mov.w, mov.l, fmov.d,

 *				fmov.s, movu.b, movu.w

 *

 *	0011nnnn0iii1001        bclr.b, bld.b, bset.b, bst.b, band.b,

 *				bandnot.b, bldnot.b, bor.b, bornot.b,

 *				bxor.b

 *

 *	0000nnnniiii0000        movi20

 *	0000nnnniiii0001        movi20s

 Look for the common cases */

 movi20 */

 movi20s */

 32-bit mov/fmov/movu variants */

 And the special cases.. */

 32-bit b*.b bit operations */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH3 Setup code for SH7710, SH7712

 *

 *  Copyright (C) 2006 - 2009  Paul Mundt

 *  Copyright (C) 2007  Nobuhiro Iwamatsu

 interrupt sources */

 IRQ0->5 are handled in setup-sh3.c */

 IPRA */ { TMU0, TMU1, TMU2, RTC } },

 IPRB */ { WDT, REF, 0, 0 } },

 IPRC */ { IRQ3, IRQ2, IRQ1, IRQ0 } },

 IPRD */ { 0, 0, IRQ5, IRQ4 } },

 IPRE */ { DMAC1, SCIF0, SCIF1 } },

 IPRF */ { IPSEC, DMAC2 } },

 IPRG */ { EDMAC0, EDMAC1, EDMAC2 } },

 IPRH */ { 0, 0, 0, SIOF0 } },

 IPRI */ { 0, 0, SIOF1 } },

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh3/clock-sh7709.c

 *

 * SH7709 support for the clock framework

 *

 *  Copyright (C) 2005  Andriy Skulysh

 *

 * Based on arch/sh/kernel/cpu/sh3/clock-sh7705.c

 *  Copyright (C) 2005  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh3/clock-sh7705.c

 *

 * SH7705 support for the clock framework

 *

 *  Copyright (C) 2005  Paul Mundt

 *

 * FRQCR parsing hacked out of arch/sh/kernel/time.c

 *

 *  Copyright (C) 1999  Tetsuya Okada & Niibe Yutaka

 *  Copyright (C) 2000  Philipp Rumpf <prumpf@tux.org>

 *  Copyright (C) 2002, 2003, 2004  Paul Mundt

 *  Copyright (C) 2002  M. R. Brown  <mrbrown@linux-sh.org>

/*

 * SH7705 uses the same divisors as the generic SH-3 case, it's just the

 * FRQCR layout that is a bit different..

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh3/clock-sh7710.c

 *

 * SH7710 support for the clock framework

 *

 *  Copyright (C) 2005  Paul Mundt

 *

 * FRQCR parsing hacked out of arch/sh/kernel/time.c

 *

 *  Copyright (C) 1999  Tetsuya Okada & Niibe Yutaka

 *  Copyright (C) 2000  Philipp Rumpf <prumpf@tux.org>

 *  Copyright (C) 2002, 2003, 2004  Paul Mundt

 *  Copyright (C) 2002  M. R. Brown  <mrbrown@linux-sh.org>

 SPDX-License-Identifier: GPL-2.0

 We need to set SCPCR to enable RTS/CTS */

 Clear out SCP7MD1,0, SCP6MD1,0, SCP4MD1,0*/

 We need to set SCPCR to enable RTS/CTS */

		/* Clear out SCP7MD1,0, SCP4MD1,0,

 Set /RTS2 (bit6) = 0 */

 SPDX-License-Identifier: GPL-2.0

/*

 * Shared SH3 Setup code

 *

 *  Copyright (C) 2008  Magnus Damm

 All SH3 devices are equipped with IRQ0->5 (except sh7708) */

 interrupt sources */

 IPRC */ { IRQ3, IRQ2, IRQ1, IRQ0 } },

 IPRD */ { 0, 0, IRQ5, IRQ4 } },

 IRR0 */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh3/clock-sh7706.c

 *

 * SH7706 support for the clock framework

 *

 *  Copyright (C) 2006  Takashi YOSHII

 *

 * Based on arch/sh/kernel/cpu/sh3/clock-sh7709.c

 *  Copyright (C) 2005  Andriy Skulysh

 SPDX-License-Identifier: GPL-2.0

/*

 * SH3 Setup code for SH7706, SH7707, SH7708, SH7709

 *

 *  Copyright (C) 2007  Magnus Damm

 *  Copyright (C) 2009  Paul Mundt

 *

 * Based on setup-sh7709.c

 *

 *  Copyright (C) 2006  Paul Mundt

 interrupt sources */

 IRQ0->5 are handled in setup-sh3.c */

 IPRA */ { TMU0, TMU1, TMU2, RTC } },

 IPRB */ { WDT, REF, SCI, 0 } },

 IPRC */ { IRQ3, IRQ2, IRQ1, IRQ0 } },

 IPRD */ { 0, 0, IRQ5, IRQ4 } },

 IPRE */ { DMAC, 0, SCIF2, ADC_ADI } },

 IPRD */ { PINT07, PINT815, } },

 IPRE */ { 0, SCIF0 } },

 IPRF */ { 0, LCDC, PCC0, PCC1, } },

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh3/clock-sh3.c

 *

 * Generic SH-3 support for the clock framework

 *

 *  Copyright (C) 2005  Paul Mundt

 *

 * FRQCR parsing hacked out of arch/sh/kernel/time.c

 *

 *  Copyright (C) 1999  Tetsuya Okada & Niibe Yutaka

 *  Copyright (C) 2000  Philipp Rumpf <prumpf@tux.org>

 *  Copyright (C) 2002, 2003, 2004  Paul Mundt

 *  Copyright (C) 2002  M. R. Brown  <mrbrown@linux-sh.org>

 SPDX-License-Identifier: GPL-2.0

 enable RTS/CTS */

 SCIF0 */

 Clear PTCR bit 9-2; enable all scif pins but sck */

 SCIF1 */

 Clear PVCR bit 9-2 */

 SCIF0 */

 Clear PTCR bit 5-2; enable only tx and rx  */

 SCIF1 */

 Clear PVCR bit 5-2 */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7720 Pinmux

 *

 *  Copyright (C) 2008  Magnus Damm

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh3/clock-sh7712.c

 *

 * SH7712 support for the clock framework

 *

 *  Copyright (C) 2007  Andrew Murray <amurray@mpc-data.co.uk>

 *

 * Based on arch/sh/kernel/cpu/sh3/clock-sh3.c

 *  Copyright (C) 2005  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * Setup code for SH7720, SH7721.

 *

 *  Copyright (C) 2007  Markus Brunner, Mark Jonas

 *  Copyright (C) 2009  Paul Mundt

 *

 *  Based on arch/sh/kernel/cpu/sh4/setup-sh7750.c:

 *

 *  Copyright (C) 2006  Paul Mundt

 *  Copyright (C) 2006  Jamie Lenehan

 Shared Period/Carry/Alarm IRQ */

 interrupt sources */

 IRQ0->5 are handled in setup-sh3.c */

 H_UDI cannot be masked */  INTC_VECT(TMU_SUNI, 0x6c0),

 IPRA */ { TMU0, TMU1, TMU2, RTC } },

 IPRB */ { WDT, REF_RCMI, SIM, 0 } },

 IPRC */ { IRQ3, IRQ2, IRQ1, IRQ0 } },

 IPRD */ { USBF_SPD, TMU_SUNI, IRQ5, IRQ4 } },

 IPRE */ { DMAC1, 0, LCDC, SSL } },

 IPRF */ { ADC, DMAC2, USBFI, CMT } },

 IPRG */ { SCIF0, SCIF1, 0, 0 } },

 IPRH */ { PINT07, PINT815, TPU, IIC } },

 IPRI */ { SIOF0, SIOF1, MMC, PCC } },

 IPRJ */ { 0, USBHI, 0, AFEIF } },

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7705 Setup

 *

 *  Copyright (C) 2006 - 2009  Paul Mundt

 *  Copyright (C) 2007  Nobuhiro Iwamatsu

 interrupt sources */

 IRQ0->5 are handled in setup-sh3.c */

 IPRA */ { TMU0, TMU1, TMU2, RTC } },

 IPRB */ { WDT, REF_RCMI, 0, 0 } },

 IPRC */ { IRQ3, IRQ2, IRQ1, IRQ0 } },

 IPRD */ { PINT07, PINT815, IRQ5, IRQ4 } },

 IPRE */ { DMAC, SCIF0, SCIF2, ADC_ADI } },

 IPRF */ { 0, 0, USB } },

 IPRG */ { TPU0, TPU1 } },

 IPRH */ { TPU2, TPU3 } },

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh3/probe.c

 *

 * CPU Subtype Probing for SH-3.

 *

 * Copyright (C) 1999, 2000  Niibe Yutaka

 * Copyright (C) 2002  Paul Mundt

	/*

	 * Check if the entry shadows or not.

	 * When shadowed, it's 128-entry system.

	 * Otherwise, it's 256-entry system.

 First, write back & invalidate */

 Next, check if there's shadow or not */

 Lastly, invaliate them. */

	/*

	 * 7709A/7729 has 16K cache (256-entry), while 7702 has only

	 * 2K(direct) 7702 is not supported (yet)

 Shadow */

 7709A or 7729  */

	/*

	 * SH-3 doesn't have separate caches

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7757 (B0 step) Pinmux

 *

 *  Copyright (C) 2009-2010  Renesas Solutions Corp.

 *

 *  Author : Yoshihiro Shimoda <shimoda.yoshihiro@renesas.com>

 *

 * Based on SH7723 Pinmux

 *  Copyright (C) 2008  Magnus Damm

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/clock-sh7734.c

 *

 * Clock framework for SH7734

 *

 * Copyright (C) 2011, 2012 Nobuhiro Iwamatsu <nobuhiro.iwamatsu.yj@renesas.com>

 * Copyright (C) 2011, 2012 Renesas Solutions Corp.

 IIC */

 SCIF */

 HSCIF */

 TMU / TIMER */

 SSI */

 HSPI */

 ADMAC */

 GETHER */

 DMAC */

 VIDEOIN1 */

 VIDEOIN0 */

 RGPVBG */

 2DG */

 VIEW */

 USB */

 MMC */

 MIMLB */

 SDHI0 */

 SDHI1 */

 SDHI2 */

 RQSPI */

 SRC0 */

 SRC1 */

 RSPI */

 RCAN0 */

 RCAN1 */

 FLTCL */

 ADC */

 MTU */

 IE-BUS */

 RTC */

 HIF */

 STIF0 */

 STIF1 */

 MSTPCR0 */

 MSTPCR1 */

 MSTPCR3 */

 main clocks */

 clocks */

 MSTP32 clocks */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7763 Setup

 *

 *  Copyright (C) 2006  Paul Mundt

 *  Copyright (C) 2007  Yoshihiro Shimoda

 *  Copyright (C) 2008, 2009  Nobuhiro Iwamatsu

 Shared Period/Carry/Alarm IRQ */

 interrupt sources */

 interrupt groups */

 INT2MSKR / INT2MSKCR */

 INT2MSKR1 / INT2MSKCR1 */

 INT2PRI0 */ { TMU0, TMU1,

 INT2PRI1 */ { TMU3, TMU4, TMU5, RTC } },

 INT2PRI2 */ { SCIF0, SCIF1, WDT } },

 INT2PRI3 */ { HUDI, DMAC, ADC } },

 INT2PRI4 */ { CMT, HAC,

 INT2PRI5 */ { PCIINTB, PCIINTC,

 INT2PRI6 */ { SIOF0, USBF, MMCIF, SSI0 } },

 INT2PRI7 */ { SCIF2, GPIO } },

 INT2PRI8 */ { SSI3, SSI2, SSI1, 0 } },

 INT2PRI9 */ { LCDC, 0, IIC1, IIC0 } },

 INT2PRI10 */ { TPU, SIM, SIOF2, SIOF1 } },

 INT2PRI11 */ { PCC } },

 INT2PRI12 */ { 0, 0, USBH, GETHER } },

 INT2PRI13 */ { 0, 0, STIF1, STIF0 } },

 Support for external interrupt pins in IRQ mode */

 INTMSK0 / INTMSKCLR0 */

 INTPRI */ { IRQ0, IRQ1, IRQ2, IRQ3,

 ICR1 */   { IRQ0, IRQ1, IRQ2, IRQ3,

 INTREQ */

 External interrupt pins in IRL mode */

 INTMSK2 / INTMSKCLR2 */

 INTMSK2 / INTMSKCLR2 */

 disable IRQ7-0 */

 disable IRL3-0 + IRL7-4 */

 select IRQ mode for IRL3-0 + IRL7-4 */

 enable IRL7-4 but don't provide any masking */

 enable IRL0-3 but don't provide any masking */

 enable IRL7-4 and mask using cpu intc controller */

 enable IRL0-3 and mask using cpu intc controller */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7723 Pinmux

 *

 *  Copyright (C) 2008  Magnus Damm

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/clock-sh7785.c

 *

 * SH7785 support for the clock framework

 *

 *  Copyright (C) 2007 - 2010  Paul Mundt

/*

 * Default rate for the root input clock, reset this with clk_set_rate()

 * from the platform code.

 MSTPCR0 */

 MSTPCR1 */

 main clocks */

 DIV4 clocks */

 MSTP32 clocks */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/clock-sh7786.c

 *

 * SH7786 support for the clock framework

 *

 *  Copyright (C) 2010  Paul Mundt

/*

 * Default rate for the root input clock, reset this with clk_set_rate()

 * from the platform code.

	/*

	 * Clock modes 0, 1, and 2 use an x64 multiplier against PLL1,

	 * while modes 3, 4, and 5 use an x32.

 MSTPCR0 */

 MSTPCR1 */

 main clocks */

 DIV4 clocks */

 MSTP32 clocks */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7786 Setup

 *

 * Copyright (C) 2009 - 2011  Renesas Solutions Corp.

 * Kuninori Morimoto <morimoto.kuninori@renesas.com>

 * Paul Mundt <paul.mundt@renesas.com>

 *

 * Based on SH7785 Setup

 *

 *  Copyright (C) 2007  Paul Mundt

/*

 * The rest of these all have multiplexed IRQs

 Placeholders, see sh7786_devices_setup() */

 Resource order important! */

 Channel registers and DMAOR */

 DMARSx */

 IRQ for channels 0-5 */

/*

 * Please call this function if your platform board

 * use external clock for USB

	/*

	 * USB initial settings

	 *

	 * The following settings are necessary

	 * for using the USB modules.

	 *

	 * see "USB Initial Settings" for detail

	/*

	 * Set the PHY and PLL enable bit

 Set the PHY RST bit */

 interrupt sources */

 Muxed sub-events */

 INTPRI */   { IRQ0, IRQ1, IRQ2, IRQ3,

 INT2PRI0 */ { 0, 0, 0, WDT } },

 INT2PRI1 */ { TMU0_0, TMU0_1,

 INT2PRI2 */ { TMU1_0, TMU1_1,

 INT2PRI3 */ { DMAC0_0, DMAC0_1,

 INT2PRI4 */ { DMAC0_4, DMAC0_5,

 INT2PRI5 */ { HUDI0, DMAC1_0,

 INT2PRI6 */ { DMAC1_3, HPB_0,

 INT2PRI7 */ { SCIF0_0, SCIF0_1,

 INT2PRI8 */ { SCIF1, TMU2, TMU3, 0 } },

 INT2PRI9 */ { 0, 0, SCIF2, SCIF3 } },

 INT2PRI10 */ { SCIF4, SCIF5,

 INT2PRI11 */ { 0, 0, 0, 0 } },

 INT2PRI12 */ { 0, 0, 0, 0 } },

 INT2PRI13 */ { 0, 0, 0, 0 } },

 INT2PRI14 */ { 0, 0, 0, PCIeC0_0 } },

 INT2PRI15 */ { PCIeC0_1, PCIeC0_2,

 INT2PRI16 */ { PCIeC1_2, USB, 0, 0 } },

 INT2PRI17 */ { 0, 0, 0, 0 } },

 INT2PRI18 */ { 0, 0, I2C0, I2C1 } },

 INT2PRI19 */ { DU, SSI0, SSI1, SSI2 } },

 INT2PRI20 */ { SSI3, PCIeC2_0,

 INT2PRI21 */ { HAC0, HAC1, FLCTL, 0 } },

 INT2PRI22 */ { HSPI, GPIO0,

 INT2PRI23 */ { 0, 0, 0, 0 } },

 INT2PRI24 */ { 0, 0, 0, 0 } },

 CnICIPRI / CnICIPRICLR */

 Support for external interrupt pins in IRQ mode */

 ICR1 */   { IRQ0, IRQ1, IRQ2, IRQ3,

 INTREQ */

 External interrupt pins in IRL mode */

 disable IRQ3-0 + IRQ7-4 */

 disable IRL3-0 + IRL7-4 */

 select IRL mode for IRL3-0 + IRL7-4 */

 select IRQ mode for IRL7-4 */

 select IRQ mode for IRL3-0 */

 enable IRL7-4 but don't provide any masking */

 enable IRL0-3 but don't provide any masking */

 enable IRL7-4 and mask using cpu intc controller */

 enable IRL0-3 and mask using cpu intc controller */

	/*

	 * De-mux SCIF1 IRQs if possible

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/setup-sh7734.c

 *

 * SH7734 Setup

 *

 * Copyright (C) 2011,2012 Nobuhiro Iwamatsu <nobuhiro.iwamatsu.yj@renesas.com>

 * Copyright (C) 2011,2012 Renesas Solutions Corp.

 SCIF */

 RTC */

 I2C 0 */

 TMU */

 interrupt sources */

 Group */

 Mask */

 Priority */

 Common */

 Common */

 Mask group */

 22 */

 21 */

 19 */

 18 */

 17 */

 16 */

 14 */

 13 */

 12 */

 10 */

 2 */

 1 */

 Priority group*/

 INT2PRI5 */

 INT2PRI5 */

 INT2MSKRG / INT2MSKCR */

 SDHI 0-2 */

 STIF 0,1 */

 GPIO 0-5*/

 HPBDMAC 0_3 - 23_25_27_28 */

 LBSCDMAC 0 - 2 */

 RCAN, IEBUS */

 SRC 0,1 */

 SCIF 0-5, HSCIF */

 LCDC, MIMLB */

 2DG, RGPVG */

 HSPI, RSPI, QSPI */

 VIN0, 1 */

 SSI 0-3 */

 I2C */ 
 TMU30 - TMU80 */

 TMU00 - TMU21 */

 INT2PRI0 */

 INT2PRI1 */

 INT2PRI2 */

 INT2PRI3 */

 INT2PRI4 */

 INT2PRI5 */

 INT2PRI6 */

 INT2PRI7 */

 INT2PRI8 */

 ADIF */, VIN1, RESET_WDT, HIF } },

 INT2PRI9 */

 INT2PRI10 */

 INT2PRI11 */

 Support for external interrupt pins in IRQ mode */

 ICR1 */

 INTREQ */

 INTMSK0 / INTMSKCLR0 */

 INTPRI */

 External interrupt pins in IRL mode */

 disable IRQ3-0 */

 disable IRL3-0 */

 select IRL mode for IRL3-0 */

 disable holding function, ie enable "SH-4 Mode (LVLMODE)" */

 select IRQ mode for IRL3-0 */

 enable IRL0-3 but don't provide any masking */

 enable IRL0-3 and mask using cpu intc controller */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7366 Setup

 *

 *  Copyright (C) 2008 Renesas Solutions

 *

 * Based on linux/arch/sh/kernel/cpu/sh4a/setup-sh7722.c

 "i2c0" clock */

 place holder for contiguous memory */

 place holder for contiguous memory */

 place holder for contiguous memory */

 interrupt sources */

 interrupt groups */

 IMR0 / IMCR0 */

 IMR1 / IMCR1 */

 IMR2 / IMCR2 */

 IMR3 / IMCR3 */

 IMR4 / IMCR4 */

 IMR5 / IMCR5 */

 IMR6 / IMCR6 */

 IMR7 / IMCR7 */

 IMR8 / IMCR8 */

 IMR9 / IMCR9 */

 IMR10 / IMCR10 */

 IMR11 / IMCR11 */

 INTMSK00 / INTMSKCLR00 */

 IPRA */ { TMU0, TMU1, TMU2 } },

 IPRB */ { VEU2, LCDC, ICB } },

 IPRC */ { } },

 IPRD */ { } },

 IPRE */ { DMAC0123, VIOVOU, MFI, VPU } },

 IPRF */ { 0, DMAC45, USB, CMT } },

 IPRG */ { SCIF, SCIFA1, SCIFA2, DENC } },

 IPRH */ { MSIOF, 0, FLCTL, I2C } },

 IPRI */ { 0, 0, TSIF, } },

 IPRJ */ { 0, 0, SIU } },

 IPRK */ { 0, MMC, 0, SDHI } },

 IPRL */ { } },

 INTPRI00 */

 ICR1 */

 INTREQ00 */

 TODO: Register Node 1 */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/clock-sh7722.c

 *

 * SH7722 clock framework support

 *

 * Copyright (C) 2009 Magnus Damm

 SH7722 registers */

 Fixed 32 KHz root clock for RTC and Power Management purposes */

/*

 * Default rate for the root input clock, reset this with clk_set_rate()

 * from the platform code.

 The dll block multiplies the 32khz r_clk, may be used instead of extal */

 main clocks */

 DIV4 clocks */

 DIV6 clocks */

 MSTP clocks */

 autodetect extal or dll configuration */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/clock-sh7763.c

 *

 * SH7763 support for the clock framework

 *

 *  Copyright (C) 2005  Paul Mundt

 *  Copyright (C) 2007  Yoshihiro Shimoda

/*

 * Additional SH7763-specific on-chip clocks that aren't already part of the

 * clock framework

 main clocks */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4/clock-sh7757.c

 *

 * SH7757 support for the clock framework

 *

 *  Copyright (C) 2009-2010  Renesas Solutions Corp.

/*

 * Default rate for the root input clock, reset this with clk_set_rate()

 * from the platform code.

	/*

	 * P clock is always enable, because some P clock modules is used

	 * by Host PC.

 MSTPCR0 */

 MSTPCR1 */

 MSTPCR2 */

 main clocks */

 DIV4 clocks */

 MSTP32 clocks */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7722 Setup

 *

 *  Copyright (C) 2006 - 2008  Paul Mundt

 Channel registers and DMAOR */

 DMARSx */

 IRQ for channels 0-3 */

 IRQ for channels 4-5 */

 Serial */

 Period IRQ */

 Carry IRQ */

 Alarm IRQ */

 "usbf0" clock */

 "i2c0" clock */

 place holder for contiguous memory */

 place holder for contiguous memory */

 place holder for contiguous memory */

 interrupt sources */

 interrupt groups */

 IMR0 / IMCR0 */

 IMR1 / IMCR1 */

 IMR2 / IMCR2 */

 IMR3 / IMCR3 */

 IMR4 / IMCR4 */

 IMR5 / IMCR5 */

 IMR6 / IMCR6 */

 IMR7 / IMCR7 */

 IMR8 / IMCR8 */

 IMR9 / IMCR9 */

 IMR10 / IMCR10 */

 IMR11 / IMCR11 */

 INTMSK00 / INTMSKCLR00 */

 IPRA */ { TMU0, TMU1, TMU2, IRDA } },

 IPRB */ { JPU, LCDC, SIM } },

 IPRC */ { } },

 IPRD */ { } },

 IPRE */ { DMAC0123, VIOVOU, 0, VPU } },

 IPRF */ { KEYSC, DMAC45, USB, CMT } },

 IPRG */ { SCIF0, SCIF1, SCIF2 } },

 IPRH */ { SIOF0, SIOF1, FLCTL, I2C } },

 IPRI */ { SIO, 0, TSIF, RTC } },

 IPRJ */ { 0, 0, SIU } },

 IPRK */ { 0, 0, 0, SDHI } },

 IPRL */ { TWODG, 0, TPU } },

 INTPRI00 */

 ICR1 */

 INTREQ00 */

 Register the URAM space as Node 1 */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7785 Setup

 *

 *  Copyright (C) 2007  Paul Mundt

 DMA */

 Channel registers and DMAOR */

 DMARSx */

		/*

		 * Real DMA error vector is 0x6e0, and channel

		 * vectors are 0x620-0x6c0

 Channel registers and DMAOR */

 DMAC1 has no DMARS */

		/*

		 * Real DMA error vector is 0x940, and channel

		 * vectors are 0x880-0x920

 interrupt sources */

 interrupt groups */

 INTMSK0 / INTMSKCLR0 */

 INTMSK2 / INTMSKCLR2 */

 INT2MSKR / INT2MSKCR */

 INTPRI */   { IRQ0, IRQ1, IRQ2, IRQ3,

 INT2PRI0 */ { TMU0, TMU1,

 INT2PRI1 */ { TMU3, TMU4, TMU5, } },

 INT2PRI2 */ { SCIF0, SCIF1,

 INT2PRI3 */ { SCIF4, SCIF5, WDT, } },

 INT2PRI4 */ { HUDI, DMAC0, DMAC1, } },

 INT2PRI5 */ { HAC0, HAC1,

 INT2PRI6 */ { PCIINTB, PCIINTC,

 INT2PRI7 */ { SIOF, HSPI, MMCIF, } },

 INT2PRI8 */ { FLCTL, GPIO, SSI0, SSI1, } },

 INT2PRI9 */ { DU, GDTA, } },

 Support for external interrupt pins in IRQ mode */

 ICR1 */   { IRQ0, IRQ1, IRQ2, IRQ3,

 INTREQ */

 External interrupt pins in IRL mode */

 disable IRQ3-0 + IRQ7-4 */

 disable IRL3-0 + IRL7-4 */

 select IRL mode for IRL3-0 + IRL7-4 */

 disable holding function, ie enable "SH-4 Mode" */

 select IRQ mode for IRL7-4 */

 select IRQ mode for IRL3-0 */

 enable IRL7-4 but don't provide any masking */

 enable IRL0-3 but don't provide any masking */

 enable IRL7-4 and mask using cpu intc controller */

 enable IRL0-3 and mask using cpu intc controller */

 Register the URAM space as Node 1 */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH-X3 SMP

 *

 *  Copyright (C) 2007 - 2010  Paul Mundt

 *  Copyright (C) 2007  Magnus Damm

 C0INITICI..CnINTICI */

 C0INTICICLR..CnINTICICLR */

 Enable light sleep for the boot CPU */

	/*

	 * Do this stupidly for now.. we don't have an easy way to probe

	 * for the total number of cores.

 Start up secondary processor by sending a reset */

 CPIDR */

 C0INTICI..CnINTICI */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7770 Setup

 *

 *  Copyright (C) 2006 - 2008  Paul Mundt

 interrupt sources */

 interrupt groups */

 INT2MSKR / INT2MSKCR */

 INT2PRI0 */ { GPIO, TMU0, 0, HAC } },

 INT2PRI1 */ { IPI, SPDIF, HUDI, I2C } },

 INT2PRI2 */ { DMAC, I2S, SRC, DU } },

 INT2PRI3 */ { VIDEO_IN, REMOTE, YUV, USB } },

 INT2PRI4 */ { ATAPI, CAN, GPS, GFX2D } },

 INT2PRI5 */ { 0, GFX3D, EXBUS_ATA, SPI } },

 INT2PRI6 */ { SCIF1234, SCIF567, SCIF089 } },

 INT2PRI7 */ { ADC, 0, 0, BBDMAC_0_3 } },

 INT2PRI8 */

 INT2PRI9 */

 INT2PRI10 */

 INT2PRI11 */

 INT2PRI12 */

 INT2PRI13 */

 Support for external interrupt pins in IRQ mode */

 INTMSK0 / INTMSKCLR0 */

 INTPRI */ { IRQ0, IRQ1, IRQ2, IRQ3,

 ICR1 */   { IRQ0, IRQ1, IRQ2, IRQ3,

 External interrupt pins in IRL mode */

 INTMSK2 / INTMSKCLR2 */

 INTMSK2 / INTMSKCLR2 */

 disable IRQ7-0 */

 disable IRL3-0 + IRL7-4 */

 select IRL mode for IRL3-0 + IRL7-4 */

 disable holding function, ie enable "SH-4 Mode" */

 select IRQ mode for IRL3-0 + IRL7-4 */

 enable IRL7-4 but don't provide any masking */

 enable IRL0-3 but don't provide any masking */

 enable IRL7-4 and mask using cpu intc controller */

 enable IRL0-3 and mask using cpu intc controller */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/clock-sh7780.c

 *

 * SH7780 support for the clock framework

 *

 *  Copyright (C) 2005  Paul Mundt

/*

 * Additional SH7780-specific on-chip clocks that aren't already part of the

 * clock framework

 main clocks */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4/clock-shx3.c

 *

 * SH-X3 support for the clock framework

 *

 *  Copyright (C) 2006-2007  Renesas Technology Corp.

 *  Copyright (C) 2006-2007  Renesas Solutions Corp.

 *  Copyright (C) 2006-2010  Paul Mundt

/*

 * Default rate for the root input clock, reset this with clk_set_rate()

 * from the platform code.

 PLL1 has a fixed x72 multiplier.  */

 MSTPCR0 */

 MSTPCR1 */

 main clocks */

 DIV4 clocks */

 MSTP32 clocks */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/clock-sh7366.c

 *

 * SH7366 clock framework support

 *

 * Copyright (C) 2009 Magnus Damm

 SH7366 registers */

 Fixed 32 KHz root clock for RTC and Power Management purposes */

/*

 * Default rate for the root input clock, reset this with clk_set_rate()

 * from the platform code.

 The dll block multiplies the 32khz r_clk, may be used instead of extal */

 See page 52 of Datasheet V0.40: Overview -> Block Diagram */

 main clocks */

 DIV4 clocks */

 DIV6 clocks */

 MSTP32 clocks */

 autodetect extal or dll configuration */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH-X3 Prototype Setup

 *

 *  Copyright (C) 2007 - 2010  Paul Mundt

/*

 * This intentionally only registers SCIF ports 0, 1, and 3. SCIF 2

 * INTEVT values overlap with the FPU EXPEVT ones, requiring special

 * demuxing in the exception dispatch path.

 *

 * As this overlap is something that never should have made it in to

 * silicon in the first place, we just refuse to deal with the port at

 * all rather than adding infrastructure to hack around it.

 interrupt sources */

 interrupt groups */

 CnINTMSK0 / CnINTMSKCLR0 */

 CnINTMSK1 / CnINTMSKCLR1 */

 CnINT2MSK0 / CnINT2MSKCLR0 */

 HUDI bits ignored */

 CnINT2MSK1 / CnINT2MSKCLR1 */

 IRM bits ignored */

 CnINT2MSK2 / CnINT2MSKCLR2 */

 INTPRI */ { IRQ0, IRQ1, IRQ2, IRQ3 } },

 INT2PRI0 */ { 0, HUDII, TMU5, TMU4,

 INT2PRI1 */ { DTU3, DTU2, DTU1, DTU0,

 INT2PRI2 */ { DMAC1, DMAC0,

 INT2PRI3 */ { FE1, FE0, ATAPI, VCORE0,

 INT2PRI4 */ { 0, 0, PAM, GPIO3,

 CnICIPRI / CnICIPRICLR */

 Support for external interrupt pins in IRQ mode */

 ICR1 */   { IRQ0, IRQ1, IRQ2, IRQ3 } },

 External interrupt pins in IRL mode */

 Register CPU#0 URAM space as Node 1 */

 CPU0 */

 XXX: Not yet.. */

 CPU1 */

 CPU2 */

 CPU3 */

 CSM */

 SPDX-License-Identifier: GPL-2.0

/*

 * Performance events support for SH-4A performance counters

 *

 *  Copyright (C) 2009, 2010  Paul Mundt

/*

 * The PMCAT location for SH-X3 CPUs was quietly moved, while the CCBR

 * and PMCTR locations remains tentatively constant. This change remains

 * wholly undocumented, and was simply found through trial and error.

 *

 * Early cuts of SH-X3 still appear to use the SH-X/SH-X2 locations, and

 * it's unclear when this ceased to be the case. For now we always use

 * the new location (if future parts keep up with this trend then

 * scanning for them at runtime also remains a viable option.)

 *

 * The gap in the register space also suggests that there are other

 * undocumented counters, so this will need to be revisited at a later

 * point in time.

/*

 * Supported raw event codes:

 *

 *	Event Code	Description

 *	----------	-----------

 *

 *	0x0000		number of elapsed cycles

 *	0x0200		number of elapsed cycles in privileged mode

 *	0x0280		number of elapsed cycles while SR.BL is asserted

 *	0x0202		instruction execution

 *	0x0203		instruction execution in parallel

 *	0x0204		number of unconditional branches

 *	0x0208		number of exceptions

 *	0x0209		number of interrupts

 *	0x0220		UTLB miss caused by instruction fetch

 *	0x0222		UTLB miss caused by operand access

 *	0x02a0		number of ITLB misses

 *	0x0028		number of accesses to instruction memories

 *	0x0029		number of accesses to instruction cache

 *	0x002a		instruction cache miss

 *	0x022e		number of access to instruction X/Y memory

 *	0x0030		number of reads to operand memories

 *	0x0038		number of writes to operand memories

 *	0x0031		number of operand cache read accesses

 *	0x0039		number of operand cache write accesses

 *	0x0032		operand cache read miss

 *	0x003a		operand cache write miss

 *	0x0236		number of reads to operand X/Y memory

 *	0x023e		number of writes to operand X/Y memory

 *	0x0237		number of reads to operand U memory

 *	0x023f		number of writes to operand U memory

 *	0x0337		number of U memory read buffer misses

 *	0x02b4		number of wait cycles due to operand read access

 *	0x02bc		number of wait cycles due to operand write access

 *	0x0033		number of wait cycles due to operand cache read miss

 *	0x003b		number of wait cycles due to operand cache write miss

/*

 * Special reserved bits used by hardware emulators, read values will

 * vary, but writes must always be 0.

 I-cache */

 I-cache */

	/*

	 * Make sure this CPU actually has perf counters.

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7780 Setup

 *

 *  Copyright (C) 2006  Paul Mundt

 Shared Period/Carry/Alarm IRQ */

 DMA */

 Channel registers and DMAOR */

 DMARSx */

		/*

		 * Real DMA error vector is 0x6c0, and channel

		 * vectors are 0x640-0x6a0, 0x780-0x7a0

 Channel registers and DMAOR */

 DMAC1 has no DMARS */

		/*

		 * Real DMA error vector is 0x6c0, and channel

		 * vectors are 0x7c0-0x7e0, 0xd80-0xde0

 interrupt sources */

 interrupt groups */

 INT2MSKR / INT2MSKCR */

 INT2PRI0 */ { TMU0, TMU1,

 INT2PRI1 */ { TMU3, TMU4, TMU5, RTC } },

 INT2PRI2 */ { SCIF0, SCIF1, WDT } },

 INT2PRI3 */ { HUDI, DMAC0, DMAC1 } },

 INT2PRI4 */ { CMT, HAC,

 INT2PRI5 */ { PCIINTB, PCIINTC,

 INT2PRI6 */ { SIOF, HSPI, MMCIF, SSI } },

 INT2PRI7 */ { FLCTL, GPIO } },

 Support for external interrupt pins in IRQ mode */

 INTMSK0 / INTMSKCLR0 */

 INTPRI */ { IRQ0, IRQ1, IRQ2, IRQ3,

 ICR1 */   { IRQ0, IRQ1, IRQ2, IRQ3,

 INTREQ */

 External interrupt pins in IRL mode */

 INTMSK2 / INTMSKCLR2 */

 INTMSK2 / INTMSKCLR2 */

 disable IRQ7-0 */

 disable IRL3-0 + IRL7-4 */

 select IRL mode for IRL3-0 + IRL7-4 */

 disable holding function, ie enable "SH-4 Mode" */

 select IRQ mode for IRL3-0 + IRL7-4 */

 enable IRL7-4 but don't provide any masking */

 enable IRL0-3 but don't provide any masking */

 enable IRL7-4 and mask using cpu intc controller */

 enable IRL0-3 and mask using cpu intc controller */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7724 Pinmux

 *

 * Copyright (C) 2009 Renesas Solutions Corp.

 *

 * Kuninori Morimoto <morimoto.kuninori@renesas.com>

 *

 * Based on SH7723 Pinmux

 *  Copyright (C) 2008  Magnus Damm

 SPDX-License-Identifier: GPL-2.0

/*

 * Shared support for SH-X3 interrupt controllers.

 *

 *  Copyright (C) 2009 - 2010  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7786 Pinmux

 *

 * Copyright (C) 2008, 2009  Renesas Solutions Corp.

 * Kuninori Morimoto <morimoto.kuninori@renesas.com>

 *

 *  Based on SH7785 pinmux

 *

 *  Copyright (C) 2008  Magnus Damm

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7734 processor support - PFC hardware block

 *

 * Copyright (C) 2012  Renesas Solutions Corp.

 * Copyright (C) 2012  Nobuhiro Iwamatsu <nobuhiro.iwamatsu.yj@renesas.com>

 PFC */

 GPIO */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/clock-sh7723.c

 *

 * SH7723 clock framework support

 *

 * Copyright (C) 2009 Magnus Damm

 SH7723 registers */

 Fixed 32 KHz root clock for RTC and Power Management purposes */

/*

 * Default rate for the root input clock, reset this with clk_set_rate()

 * from the platform code.

 The dll multiplies the 32khz r_clk, may be used instead of extal */

 See page 60 of Datasheet V1.0: Overview -> Block Diagram */

 main clocks */

 DIV4 clocks */

 DIV6 clocks */

 MSTP clocks */

 autodetect extal or dll configuration */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7785 Pinmux

 *

 *  Copyright (C) 2008  Magnus Damm

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/clock-sh7724.c

 *

 * SH7724 clock framework support

 *

 * Copyright (C) 2009 Magnus Damm

 SH7724 registers */

 Fixed 32 KHz root clock for RTC and Power Management purposes */

/*

 * Default rate for the root input clock, reset this with clk_set_rate()

 * from the platform code.

 The fll multiplies the 32khz r_clk, may be used instead of extal */

 A fixed divide-by-3 block use by the div6 clocks */

 External input clock (pin name: FSIMCKA/FSIMCKB/DV_CLKI ) */

 set KICK bit in FRQCRA to update hardware setting */

 Indices are important - they are the actual src selecting values */

 main clocks */

 DIV4 clocks */

 DIV6 clocks */

 MSTP clocks */

 autodetect extal or fll configuration */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/clock-sh7770.c

 *

 * SH7770 support for the clock framework

 *

 *  Copyright (C) 2005  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7757 Setup

 *

 * Copyright (C) 2009, 2011  Renesas Solutions Corp.

 *

 *  based on setup-sh7785.c : Copyright (C) 2007  Paul Mundt

 SCIF2 */

 SCIF3 */

 SCIF4 */

 DMA */

 channel 0 to 5 */

 Channel registers and DMAOR */

 DMARSx */

 channel 6 to 11 */

 Channel registers and DMAOR */

 DMARSx */

 IRQ for channels 4 */

 IRQ for channels 5 */

 IRQ for channels 6 */

 IRQ for channels 7 */

 IRQ for channels 8 */

 IRQ for channels 9 */

 IRQ for channels 10 */

 IRQ for channels 11 */

 channel 12 to 17 */

 Channel registers and DMAOR */

 DMARSx */

 IRQ for channels 12 to 16 */

 IRQ for channel 17 */

 channel 18 to 23 */

 Channel registers and DMAOR */

 DMARSx */

 IRQ for channels 18 to 22 */

 IRQ for channel 23 */

 interrupt sources */

 interrupt groups */

 INTMSK0 / INTMSKCLR0 */

 INTMSK2 / INTMSKCLR2 */

 INT2MSKR / INT2MSKCR */

 INT2MSKR1 / INT2MSKCR1 */

 INT2MSKR2 / INT2MSKCR2 */

 INT2MSKR3 / INT2MSKCR3 */

 INT2MSKR4 / INT2MSKCR4 */

 INT2MSKR5 / INT2MSKCR5 */

 ICR2 */   { IRQ15, IRQ14, IRQ13, IRQ12,

 Support for external interrupt pins in IRQ mode */

 ICR1 */   { IRQ0, IRQ1, IRQ2, IRQ3,

 INTREQ */

 External interrupt pins in IRL mode */

 disable IRQ3-0 + IRQ7-4 */

 disable IRL3-0 + IRL7-4 */

 select IRL mode for IRL3-0 + IRL7-4 */

 disable holding function, ie enable "SH-4 Mode" */

 select IRQ mode for IRL7-4 */

 select IRQ mode for IRL3-0 */

 enable IRL7-4 but don't provide any masking */

 enable IRL0-3 but don't provide any masking */

 enable IRL7-4 and mask using cpu intc controller */

 enable IRL0-3 and mask using cpu intc controller */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7343 Setup

 *

 *  Copyright (C) 2006  Paul Mundt

 Serial */

 "i2c0" clock */

 "i2c1" clock */

 place holder for contiguous memory */

 place holder for contiguous memory */

 place holder for contiguous memory */

 interrupt sources */

 interrupt groups */

 IMR1 / IMCR1 */

 IMR2 / IMCR2 */

 IMR3 / IMCR3 */

 IMR4 / IMCR4 */

 IMR5 / IMCR5 */

 IMR6 / IMCR6 */

 IMR7 / IMCR7 */

 IMR8 / IMCR8 */

 IMR9 / IMCR9 */

 IMR10 / IMCR10 */

 IMR11 / IMCR11 */

 INTMSK00 / INTMSKCLR00 */

 IPRA */ { TMU0, TMU1, TMU2 } },

 IPRB */ { JPU, LCDC, SIM } },

 IPRE */ { DMAC0123, VIOVOU, MFI, VPU } },

 IPRF */ { KEYSC, DMAC45, USB, CMT } },

 IPRG */ { SCIF, SCIF1, SCIF2, SCIF3 } },

 IPRH */ { SIOF0, SIOF1, FLCTL, I2C0 } },

 IPRI */ { SIO, 0, TSIF, I2C1 } },

 IPRJ */ { Z3D4, 0, SIU } },

 IPRK */ { 0, MMC, 0, SDHI } },

 IPRL */ { 0, 0, TPU } },

 INTPRI00 */

 ICR1 */

 INTREQ00 */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7724 Setup

 *

 * Copyright (C) 2009 Renesas Solutions Corp.

 *

 * Kuninori Morimoto <morimoto.kuninori@renesas.com>

 *

 * Based on SH7723 Setup

 * Copyright (C) 2008  Paul Mundt

 DMA */

 Resource order important! */

 Channel registers and DMAOR */

 DMARSx */

 IRQ for channels 0-3 */

 IRQ for channels 4-5 */

 Resource order important! */

 Channel registers and DMAOR */

 DMARSx */

 IRQ for channels 0-3 */

 IRQ for channels 4-5 */

 Serial */

 RTC */

 Period IRQ */

 Carry IRQ */

 Alarm IRQ */

 I2C0 */

 "i2c0" clock */

 I2C1 */

 "i2c1" clock */

 VPU */

 place holder for contiguous memory */

 VEU0 */

 place holder for contiguous memory */

 VEU1 */

 place holder for contiguous memory */

 BEU0 */

 place holder for contiguous memory */

 BEU1 */

 place holder for contiguous memory */

 JPU */

 place holder for contiguous memory */

 SPU2DSP0 */

 place holder for contiguous memory */

 SPU2DSP1 */

 place holder for contiguous memory */

 Enable L2 cache */

 interrupt sources */

 interrupt groups */

 IMR0 / IMCR0 */

 IMR1 / IMCR1 */

 IMR2 / IMCR2 */

 IMR3 / IMCR3 */

 IMR4 / IMCR4 */

 IMR5 / IMCR5 */

 IMR6 / IMCR6 */

 IMR7 / IMCR7 */

 IMR8 / IMCR8 */

 IMR9 / IMCR9 */

 IMR10 / IMCR10 */

 IMR11 / IMCR11 */

 IMR12 / IMCR12 */

 INTMSK00 / INTMSKCLR00 */

 IPRA */ { TMU0_TUNI0, TMU0_TUNI1,

 IPRB */ { JPU, LCDC, DMAC1A, BEU1 } },

 IPRC */ { TMU1_TUNI0, TMU1_TUNI1,

 IPRD */ { 0, MMCIF, 0, ATAPI } },

 IPRE */ { DMAC0A, VIO, SCIFA3, VPU } },

 IPRF */ { KEYSC, DMAC0B, USB, CMT } },

 IPRG */ { SCIF_SCIF0, SCIF_SCIF1,

 IPRH */ { MSIOF_MSIOFI0, MSIOF_MSIOFI1,

 IPRI */ { SCIFA4, ICB, TSIF, _2DG } },

 IPRJ */ { CEU1, ETHI, FSI, SDHI1 } },

 IPRK */ { RTC, DMAC1B, 0, SDHI0 } },

 IPRL */ { SCIFA5, 0, TPU, _2DDMAC } },

 INTPRI00 */

 ICR1 */

 INTREQ00 */

 BSC */

 INTC */

 RWDT */

 CPG */

 BCR */

 MMSELR */

 CS0BCR */

 CS4BCR */

 CS5ABCR */

 CS5BBCR */

 CS6ABCR */

 CS6BBCR */

 CS4WCR */

 CS5AWCR */

 CS5BWCR */

 CS6AWCR */

 CS6BWCR */

 INTC */

 IPRA */

 IPRB */

 IPRC */

 IPRD */

 IPRE */

 IPRF */

 IPRG */

 IPRH */

 IPRI */

 IPRJ */

 IPRK */

 IPRL */

 IMR0 */

 IMR1 */

 IMR2 */

 IMR3 */

 IMR4 */

 IMR5 */

 IMR6 */

 IMR7 */

 IMR8 */

 IMR9 */

 IMR10 */

 IMR11 */

 IMR12 */

 RWDT */

 RWTCNT */

 RWTCSR */

 CPG */

 IRDACLKCR */

 SPUCLKCR */

 BCR */

 MMSELR */

 CS0BCR */

 CS4BCR */

 CS5ABCR */

 CS5BBCR */

 CS6ABCR */

 CS6BBCR */

 CS4WCR */

 CS5AWCR */

 CS5BWCR */

 CS6AWCR */

 CS6BWCR */

 INTC */

 IPRA */

 IPRB */

 IPRC */

 IPRD */

 IPRE */

 IPRF */

 IPRG */

 IPRH */

 IPRI */

 IPRJ */

 IPRK */

 IPRL */

 IMR0 */

 IMR1 */

 IMR2 */

 IMR3 */

 IMR4 */

 IMR5 */

 IMR6 */

 IMR7 */

 IMR8 */

 IMR9 */

 IMR10 */

 IMR11 */

 IMR12 */

 RWDT */

 RWTCNT */

 RWTCSR */

 CPG */

 IRDACLKCR */

 SPUCLKCR */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH-X3 prototype CPU pinmux

 *

 * Copyright (C) 2010  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/ubc.c

 *

 * On-chip UBC support for SH-4A CPUs.

 *

 * Copyright (C) 2009 - 2010  Paul Mundt

 CRR */

 CBR */

	/*

	 * The UBC MSTP bit is optional, as not all platforms will have

	 * it. Just ignore it if we can't find it.

 dummy read for write posting */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH7723 Setup

 *

 *  Copyright (C) 2008  Paul Mundt

 Serial */

 place holder for contiguous memory */

 place holder for contiguous memory */

 place holder for contiguous memory */

 Period IRQ */

 Carry IRQ */

 Alarm IRQ */

  not use dma */

 "i2c0" clock */

 Enable L2 cache */

 interrupt sources */

 interrupt groups */

 IMR0 / IMCR0 */

 IMR1 / IMCR1 */

 IMR2 / IMCR2 */

 IMR3 / IMCR3 */

 IMR4 / IMCR4 */

 IMR5 / IMCR5 */

 IMR6 / IMCR6 */

 IMR7 / IMCR7 */

 IMR8 / IMCR8 */

 IMR9 / IMCR9 */

 IMR10 / IMCR10 */

 IMR11 / IMCR11 */

 IMR12 / IMCR12 */

 INTMSK00 / INTMSKCLR00 */

 IPRA */ { TMU0_TUNI0, TMU0_TUNI1, TMU0_TUNI2, IRDA_IRDAI } },

 IPRB */ { VEU2H1_VEU2HI, LCDC_LCDCI, DMAC1A, 0} },

 IPRC */ { TMU1_TUNI0, TMU1_TUNI1, TMU1_TUNI2, 0} },

 IPRD */ { } },

 IPRE */ { DMAC0A, VIO, SCIFA_SCIFA0, VPU_VPUI } },

 IPRF */ { KEYSC_KEYI, DMAC0B, USB_USI0, CMT_CMTI } },

 IPRG */ { SCIF_SCIF0, SCIF_SCIF1, SCIF_SCIF2,0 } },

 IPRH */ { MSIOF_MSIOFI0,MSIOF_MSIOFI1, FLCTL, I2C } },

 IPRI */ { SCIFA_SCIFA1,0,TSIF_TSIFI,_2DG } },

 IPRJ */ { ADC_ADI,0,SIU_SIUI,SDHI1 } },

 IPRK */ { RTC,DMAC1B,0,SDHI0 } },

 IPRL */ { SCIFA_SCIFA2,0,TPU_TPUI,ATAPI_ATAPII } },

 INTPRI00 */

 ICR1 */

 INTREQ00 */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/sh4a/clock-sh7343.c

 *

 * SH7343 clock framework support

 *

 * Copyright (C) 2009 Magnus Damm

 SH7343 registers */

 Fixed 32 KHz root clock for RTC and Power Management purposes */

/*

 * Default rate for the root input clock, reset this with clk_set_rate()

 * from the platform code.

 The dll block multiplies the 32khz r_clk, may be used instead of extal */

 main clocks */

 DIV4 clocks */

 DIV6 clocks */

 MSTP32 clocks */

 autodetect extal or dll configuration */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/kernel/cpu/irq/imask.c

 *

 * Copyright (C) 1999, 2000  Niibe Yutaka

 *

 * Simple interrupt handling using IMASK of SR register.

 *

 NOTE: Will not work on level 15 */

 Bitmap of IRQ masked */

 SPDX-License-Identifier: GPL-2.0

/*

 * Interrupt handling for IPR-based IRQ.

 *

 * Copyright (C) 1999  Niibe Yutaka & Takeshi Yaegashi

 * Copyright (C) 2000  Kazumoto Kojima

 * Copyright (C) 2003  Takashi Kusuda <kusuda-takashi@hitachi-ul.co.jp>

 * Copyright (C) 2006  Paul Mundt

 *

 * Supported system:

 *	On-chip supporting modules (TMU, RTC, etc.).

 *	On-chip supporting modules for SH7709/SH7709A/SH7729.

 *	Hitachi SolutionEngine external I/O:

 *		MS7709SE01, MS7709ASE01, and MS7750SE01

 Set the priority in IPR to 0 */

 Read back to flush write posting */

 Set priority in IPR back to original value */

/*

 * The shift value is now the number of bits to shift, not the number of

 * bits/4. This is to make it easier to read the value directly from the

 * datasheets. The IPR address is calculated using the ipr_offset table.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/lib/io.c - SH32 optimized I/O routines

 *

 * Copyright (C) 2000  Stuart Menefy

 * Copyright (C) 2005  Paul Mundt

 *

 * Provide real functions which expand to whatever the header file defined.

 * Also definitions of machine independent IO functions.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic __div64_32 wrapper for __xdiv64_32.

 SPDX-License-Identifier: GPL-2.0

 w.s.high = 1..1 or 0..0 */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 *	Precise Delay Loops for SuperH

 *

 *	Copyright (C) 1999 Niibe Yutaka & Kaz Kojima

		/*

		 * ST40-300 appears to have an issue with this code,

		 * normally taking two cycles each loop, as with all

		 * other SH variants. If however the branch and the

		 * delay slot straddle an 8 byte boundary, this increases

		 * to 3 cycles.

		 * This align directive ensures this doesn't occur.

 2**32 / 1000000 */

 SPDX-License-Identifier: GPL-2.0

/*

 *	Copyright (C) 2000 YAEGASHI Takeshi

 *	Hitachi HD64461 companion chip support

 This belongs in cpu specific */

 Should be at processor specific part.. */

 SPDX-License-Identifier: GPL-2.0

/*

 * June 2006 Steve Glendinning <steve.glendinning@shawell.net>

 *

 * Polaris-specific resource declaration

 *

 Dummy supplies, where voltage doesn't matter */

 Configure area 5 with 2 wait states */

 Configure area 5 for 32-bit access */

 External IRQs */

 IRQ0 */

 IRQ1 */

 Disable all interrupts */

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas Technology Corp. R0P7785LC0011RL Support.

 *

 * Copyright (C) 2008  Yoshihiro Shimoda

 * Copyright (C) 2009  Paul Mundt

/*

 * NOTE: This board has 2 physical memory maps.

 *	 Please look at include/asm-sh/sh7785lcr.h or hardware manual.

 28MHz */

 25MHz */

 Initialize IRQ setting */

 Initialize the board */

 sm501 DRAM configuration */

 Return the board specific boot mode pin configuration */

	/* These are the factory default settings of S1 and S2.

	 * If you change these dip switches then you will need to

	 * adjust the values below as well.

 Clock Mode 16 */

 32-bit Area0 bus width */

 32-bit Area0 bus width */

 Area 0 SRAM interface [fixed] */

 Little Endian */

 Master Mode */

 No PLL step-up */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/shmin/setup.c

 *

 * Copyright (C) 2006 Takashi YOSHII

 *

 * SHMIN Support.

 IRQ0-3=IRQ

 IRQ0-3=IRQ-mode,Low-active.

 SPDX-License-Identifier: GPL-2.0

/*

 * Data Technology Inc. ESPT-GIGA board support

 *

 * Copyright (C) 2008, 2009 Renesas Solutions Corp.

 * Copyright (C) 2008, 2009 Nobuhiro Iwamatsu <iwamatsu.nobuhiro@renesas.com>

 NOR Flash */

 Read-only */

 SH-Ether */

 use eth1 */

 TSU */

 irq number */

 SPDX-License-Identifier: GPL-2.0

/*

 * SH generic board support, using device tree

 *

 * Copyright (C) 2015-2016 Smart Energy Instruments, Inc.

	/* FIXME: eventually this should not be used at all;

 Disabled pending move to COMMON_CLK framework. */

 replaced by DT root's model */

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas Technology Corp. SH7786 Urquell Support.

 *

 * Copyright (C) 2008  Kuninori Morimoto <morimoto.kuninori@renesas.com>

 * Copyright (C) 2009, 2010  Paul Mundt

 *

 * Based on board-sh7785lcr.c

 * Copyright (C) 2008  Yoshihiro Shimoda

/*

 * bit  1234 5678

 *----------------------------

 * SW1  0101 0010  -> Pck 33MHz version

 *     (1101 0010)    Pck 66MHz version

 * SW2  0x1x xxxx  -> little endian

 *                    29bit mode

 * SW47 0001 1000  -> CS0 : on-board flash

 *                    CS1 : SRAM, registers, LAN, PCMCIA

 *                    38400 bps for SCIF1

 *

 * Address

 * 0x00000000 - 0x04000000  (CS0)     Nor Flash

 * 0x04000000 - 0x04200000  (CS1)     SRAM

 * 0x05000000 - 0x05800000  (CS1)     on board register

 * 0x05800000 - 0x06000000  (CS1)     LAN91C111

 * 0x06000000 - 0x06400000  (CS1)     PCMCIA

 * 0x08000000 - 0x10000000  (CS2-CS3) DDR3

 * 0x10000000 - 0x14000000  (CS4)     PCIe

 * 0x14000000 - 0x14800000  (CS5)     Core0 LRAM/URAM

 * 0x14800000 - 0x15000000  (CS5)     Core1 LRAM/URAM

 * 0x18000000 - 0x1C000000  (CS6)     ATA/NAND-Flash

 * 0x1C000000 -             (CS7)     SH7786 Control register

 HeartBeat */

 LAN91C111 */

 Nor Flash */

 Read-only */

 Read-only */

 USB */

 enable LAN */

	/*

	 * Only handle the EXTAL case, anyone interfacing a crystal

	 * resonator will need to provide their own input clock.

 Initialize the board */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * ALPHAPROJECT AP-SH4A-3A Support.

 *

 * Copyright (C) 2010 ALPHAPROJECT Co.,Ltd.

 * Copyright (C) 2008  Yoshihiro Shimoda

 * Copyright (C) 2009  Paul Mundt

 Dummy supplies, where voltage doesn't matter */

 Initialize the board */

 Return the board specific boot mode pin configuration */

	/* These are the factory default settings of SW1 and SW2.

	 * If you change these dip switches then you will need to

	 * adjust the values below as well.

 Clock Mode 16 */

 16-bit Area0 bus width */

 Area 0 SRAM interface */

 Little Endian */

 Master Mode */

 Crystal resonator */

 Display Unit */

 29-bit address mode */

 No PLL step-up */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002  David McCullough <davidm@snapgear.com>

 * Copyright (C) 2003  Paul Mundt <lethal@linux-sh.org>

 *

 * Based on files with the following comments:

 *

 *           Copyright (C) 2000  Kazumoto Kojima

 *

 *           Modified for 7751 Solution Engine by

 *           Ian da Silva and Jeremy Siegel, 2001.

/*

 * EraseConfig handling functions

 Setup "EraseConfig" switch on external IRQ 0 */

/*

 * Initialize IRQ setting

 *

 * IRL0 = erase switch

 * IRL1 = eth0

 * IRL2 = eth1

 * IRL3 = crypto

 enable individual interrupt mode for externals */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/renesas/edosk7705/setup.c

 *

 * Copyright (C) 2000  Kazumoto Kojima

 *

 * Hitachi SolutionEngine Support.

 *

 * Modified for edosk7705 development

 * board by S. Dunn, 2003.

 eth initialization functions */

 platform init code */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * SH-2007 board support.

 *

 * Copyright (C) 2003, 2004  SUGIOKA Toshinobu

 * Copyright (C) 2010  Hitoshi Mitake <mitake@dcl.info.waseda.ac.jp>

 Dummy supplies, where voltage doesn't matter */

/*

 * Initialize the board

 setup wait control registers for area 5 */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0+

/*

 * Renesas Europe EDOSK7760 Board Support

 *

 * Copyright (C) 2008 SPES Societa' Progettazione Elettronica e Software Ltd.

 * Author: Luca Santini <luca.santini@spesonline.com>

 Bus state controller registers for CS4 area */

 NOR flash */

 Read-only */

 i2c initialization functions */

 eth initialization functions */

 platform init code */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas R0P7757LC0012RL Support.

 *

 * Copyright (C) 2009 - 2010  Renesas Solutions Corp.

 PUDR */

 Fast Ethernet */

 TSU */

 TSU */

 Fixed 3.3V regulator to be used by SDHI0, MMCIF */

 SH_MMCIF */

 SDHI0 */

 RGMII (PTA) */

 ONFI (PTB, PTZ) */

 IRQ8 to 0 (PTB, PTC) */

 SPI0 (PTD) */

 RMII 0/1 (PTE, PTF) */

 eMMC (PTG) */

 LPC (PTG, PTH, PTQ, PTU) */

 SPI1 (PTH) */

 SDHI (PTI) */

 SCIF3/4 (PTJ, PTW) */

 SERMUX (PTK, PTL, PTO, PTV) */

 SMR0: SerMux mode 0 */

 IIC (PTM, PTR, PTS) */

 USB (PTN) */

 SGPIO1/0 (PTN, PTO) */

 WDT (PTN) */

 System (PTT) */

 PWMX (PTT) */

 R-SPI (PTV) */

 EVC (PTV, PTW) */

 LED for heartbeat */

 control for MDIO of Gigabit Ethernet */

 control for eMMC */

 eMMC_RST# */

 eMMC_INDEX# */

 eMMC_PRST# */

 register SPI device information */

 General platform */

 Initialize IRQ setting */

 Initialize the board */

	/* These are the factory default settings of S3 (Low active).

	 * If you change these dip switches then you will need to

	 * adjust the values below as well.

 Clock Mode: 1 */

 The Machine Vector */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/magicpanel/setup.c

 *

 *  Copyright (C) 2007  Markus Brunner, Mark Jonas

 *

 *  Magic Panel Release 2 board setup

 Dummy supplies, where voltage doesn't matter */

 Wait until reset finished. Timeout is 100ms. */

 PMDR: LAN_RESET=on */

 PMDR: LAN_RESET=off */

 CS2: LAN (0x08000000 - 0x0bffffff) */

 no idle cycles, normal space, 8 bit data bus */

 (SW:1.5 WR:3 HW:1.5), ext. wait */

 CS4: CAN1 (0xb0000000 - 0xb3ffffff) */

 no idle cycles, normal space, 8 bit data bus */

 (SW:1.5 WR:3 HW:1.5), ext. wait */

 CS5a: CAN2 (0xb4000000 - 0xb5ffffff) */

 no idle cycles, normal space, 8 bit data bus */

 (SW:1.5 WR:3 HW:1.5), ext. wait */

 CS5b: CAN3 (0xb6000000 - 0xb7ffffff) */

 no idle cycles, normal space, 8 bit data bus */

 (SW:1.5 WR:3 HW:1.5), ext. wait */

 CS6a: Rotary (0xb8000000 - 0xb9ffffff) */

 no idle cycles, normal space, 8 bit data bus */

 (SW:1.5 WR:3 HW:1.5), no ext. wait */

	/* A7 GPO(LED8);     A6 GPO(LED7);     A5 GPO(LED6);	  A4 GPO(LED5);

	 * A3 GPO(LED4);     A2 GPO(LED3);     A1 GPO(LED2);	  A0 GPO(LED1);

 01 01 01 01 01 01 01 01 */

	/* B7 GPO(RST4);   B6 GPO(RST3);  B5 GPO(RST2);    B4 GPO(RST1);

	 * B3 GPO(PB3);	   B2 GPO(PB2);	  B1 GPO(PB1);	   B0 GPO(PB0);

 01 01 01 01 01 01 01 01 */

	/* C7 GPO(PC7);	  C6 GPO(PC6);	  C5 GPO(PC5);	   C4 GPO(PC4);

	 * C3 LCD_DATA3;  C2 LCD_DATA2;   C1 LCD_DATA1;	   C0 LCD_DATA0;

 01 01 01 01 00 00 00 00 */

	/* D7 GPO(PD7);	D6 GPO(PD6);	D5 GPO(PD5);	   D4 GPO(PD4);

	 * D3 GPO(PD3);	D2 GPO(PD2);	D1 GPO(PD1);	   D0 GPO(PD0);

 01 01 01 01 01 01 01 01 */

	/* E7 (x);	  E6 GPI(nu);	 E5 GPI(nu);	  E4 LCD_M_DISP;

	 * E3 LCD_CL1;	  E2 LCD_CL2;	 E1 LCD_DON;	  E0 LCD_FLM;

 00 11 11 00 00 00 00 00 */

	/* F7 (x);	     F6 DA1(VLCD);     F5 DA0(nc);	  F4 AN3;

	 * F3 AN2(MID_AD);   F2 AN1(EARTH_AD); F1 AN0(TEMP);	  F0 GPI+(nc);

 00 00 00 00 00 00 00 10 */

	/* G7 (x);	  G6 IRQ5(TOUCH_BUSY); G5 IRQ4(TOUCH_IRQ); G4 GPI(KEY2);

	 * G3 GPI(KEY1);  G2 GPO(LED11);	G1 GPO(LED10);     G0 GPO(LED9);

 00 00 00 11 11 01 01 01 */

	/* H7 (x);	      H6 /RAS(BRAS);	  H5 /CAS(BCAS); H4 CKE(BCKE);

	 * H3 GPO(EARTH_OFF); H2 GPO(EARTH_TEST); H1 USB2_PWR;	 H0 USB1_PWR;

 00 00 00 00 01 01 00 00 */

	/* J7 (x);	  J6 AUDCK;	   J5 ASEBRKAK;	    J4 AUDATA3;

	 * J3 AUDATA2;	  J2 AUDATA1;	   J1 AUDATA0;	    J0 AUDSYNC;

 00 00 00 00 00 00 00 00 */

	/* K7 (x);	    K6 (x);	     K5 (x);	   K4 (x);

	 * K3 PINT7(/PWR2); K2 PINT6(/PWR1); K1 PINT5(nu); K0 PINT4(FLASH_READY)

 00 00 00 00 11 11 11 11 */

	/* L7 TRST;	   L6 TMS;	     L5 TDO;		  L4 TDI;

	 * L3 TCK;	   L2 (x);	     L1 (x);		  L0 (x);

 00 00 00 00 00 00 00 00 */

	/* M7 GPO(CURRENT_SINK);    M6 GPO(PWR_SWITCH);     M5 GPO(LAN_SPEED);

	 * M4 GPO(LAN_RESET);       M3 GPO(BUZZER);	    M2 GPO(LCD_BL);

	 * M1 CS5B(CAN3_CS);	    M0 GPI+(nc);

 01 01 01 01 01 01 00 10 */

	/* CURRENT_SINK=off,	PWR_SWITCH=off, LAN_SPEED=100MBit,

	 * LAN_RESET=off,	BUZZER=off,	LCD_BL=off

	/* P7 (x);	       P6 (x);		  P5 (x);

	 * P4 GPO(nu);	       P3 IRQ3(LAN_IRQ);  P2 IRQ2(CAN3_IRQ);

	 * P1 IRQ1(CAN2_IRQ);  P0 IRQ0(CAN1_IRQ)

 00 00 00 01 00 00 00 00 */

	/* R7 A25;	     R6 A24;	     R5 A23;		  R4 A22;

	 * R3 A21;	     R2 A20;	     R1 A19;		  R0 A0;

	/* S7 (x);		S6 (x);        S5 (x);	     S4 GPO(EEPROM_CS2);

	 * S3 GPO(EEPROM_CS1);  S2 SIOF0_TXD;  S1 SIOF0_RXD; S0 SIOF0_SCK;

 00 00 00 01 01 00 00 00 */

	/* T7 (x);	   T6 (x);	  T5 (x);	  T4 COM1_CTS;

	 * T3 COM1_RTS;	   T2 COM1_TXD;	  T1 COM1_RXD;	  T0 GPO(WDOG)

 00 00 00 00 00 00 00 01 */

	/* U7 (x);	     U6 (x);	   U5 (x);	  U4 GPI+(/AC_FAULT);

	 * U3 GPO(TOUCH_CS); U2 TOUCH_TXD; U1 TOUCH_RXD;  U0 TOUCH_SCK;

 00 00 00 10 01 00 00 00 */

	/* V7 (x);	  V6 (x);	V5 (x);		  V4 GPO(MID2);

	 * V3 GPO(MID1);  V2 CARD_TxD;	V1 CARD_RxD;	  V0 GPI+(/BAT_FAULT);

 00 00 00 01 01 00 00 10 */

	/* set Pin Select Register A:

	 * /PCC_CD1, /PCC_CD2,  PCC_BVD1, PCC_BVD2,

	 * /IOIS16,  IRQ4,	IRQ5,	  USB1d_SUSPEND

	/* set Pin Select Register B:

	 * /SCIF0_RTS, /SCIF0_CTS, LCD_VCPWC,

	 * LCD_VEPWC,  IIC_SDA,    IIC_SCL, Reserved

	/* set Pin Select Register C:

	 * SIOF1_SCK, SIOF1_RxD, SCIF1_RxD, SCIF1_TxD, Reserved

	/* set Pin Select Register D: Reserved, SIOF1_TxD, Reserved, SIOF1_MCLK,

	 * Reserved, SIOF1_SYNC, Reserved, SCIF1_SCK, Reserved

 set USB TxRx Control: Reserved, DRV, Reserved, USB_TRANS, USB_SEL */

 set USB Clock Control: USSCS, USSTB, Reserved (HighByte always A5) */

 Reserved for bootloader, read-only */

 Reserved for kernel image */

 Rest is used for Flash FS */

/*

 * Add all resources to the platform_device

/*

 * Initialize IRQ setting

 install handlers for IRQ0-5 */

 IRQ0 CAN1 */

 IRQ1 CAN2 */

 IRQ2 CAN3 */

 IRQ3 SMSC9115 */

 IRQ4 touchscreen */

 IRQ5 touchscreen */

 IRQ0 CAN1 */

 IRQ0 CAN2 */

 IRQ0 CAN3 */

 IRQ3 SMSC9115 */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/titan/setup.c - Setup for Titan

 *

 *  Copyright (C) 2006  Jamie Lenehan

 enable individual interrupt mode for externals */

 SPDX-License-Identifier: GPL-2.0

/*

 * ALPHAPROJECT AP-SH4AD-0A Support.

 *

 * Copyright (C) 2010 ALPHAPROJECT Co.,Ltd.

 * Copyright (C) 2010  Matt Fleming

 * Copyright (C) 2010  Paul Mundt

 Dummy supplies, where voltage doesn't matter */

	/* These are the factory default settings of SW1 and SW2.

	 * If you change these dip switches then you will need to

	 * adjust the values below as well.

 Clock Mode 3 */

 16-bit Area0 bus width  */

 Normal mode */

 Little Endian */

 Crystal resonator */

 29-bit address mode */

 PCI-E Root port */

 4 lane + 1 lane */

 AUD Enable */

 Normal Operation */

 Initialize the board */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas - AP-325RXA

 * (Compatible with Algo System ., LTD. - AP-320A)

 *

 * Copyright (C) 2008 Renesas Solutions Corp.

 * Author : Yusuke Goda <goda.yuske@renesas.com>

 Dummy supplies, where voltage doesn't matter */

/*

 * AP320 and AP325RXA has CPLD data in NOR Flash(0xA80000-0xABFFFF).

 * If this area erased, this board can not boot.

 Read-only */

 Read-only */

 ASD AP-320/325 LCD ON */

 ASD AP-320/325 LCD OFF */

 hsync and vsync are active low */

 7.0 inch */

 P4-only space */

 Powerdown/reset gpios for CEU image sensors */

 [0] = ov7725  */

 "ceu.0" clock */

 Fixed 3.3V regulators to be used by SDHI0, SDHI1 */

 "sdhi0" clock */

 "sdhi1" clock */

 register board specific self-refresh code */

 LD3 and LD4 LEDs */

 RUN */

 INDICATOR */

 SW1 input */

 MODE */

 LCDC */

 LCD backlight */

 CEU */

 OE_CAM */

 STBY_CAM */

 RST_CAM */

 SADDR */

 FLCTL */

 SDHI0 - CN3 - SD CARD */

 SDHI1 - CN7 - MICRO SD CARD */

 Add a clock alias for ov7725 xclk source. */

 Register RSTB gpio for ov7725 camera sensor. */

 Initialize CEU platform device separately to map memory first */

 Return the board specific boot mode pin configuration */

	/* MD0=0, MD1=0, MD2=0: Clock Mode 0

	 * MD3=0: 16-bit Area0 Bus Width

	 * MD5=1: Little Endian

	 * TSTMD=1, MD8=1: Test Mode Disabled

 Reserve a portion of memory for CEU buffers */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/sh03/setup.c

 *

 * Copyright (C) 2004  Interface Co.,Ltd. Saito.K

 *

 open I/O area window */

 IDE cmd address : 0x1f0-0x1f7 and 0x3f6 */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/sh03/rtc.c -- CTP/PCI-SH03 on-chip RTC support

 *

 *  Copyright (C) 2004  Saito.K & Jeanne(ksaito@interface.co.jp)

 *

 gets recalled with irq locally disabled */

 may take up to 1 second... */

 correct for half hour time zone */

 SPDX-License-Identifier: GPL-2.0

/*

 * KFR2R09 board support code

 *

 * Copyright (C) 2009 Magnus Damm

 set VIO_CKO clock to 25MHz */

 Read-only */

 KEYOUT0->4, KEYIN0->4 */

 "keysc0" clock */

 set 1s delay to encourage fsync() */

 P4-only space */

  not use dma */

 [0] = rj54n1cb0c */

 "ceu0" clock */

 Fixed 3.3V regulator to be used by SDHI0 */

 set bit 1 (the second bit) of chip at 0x09, register 0x13 */

 set bit 6 (the 7th bit) of chip at 0x09, register 0x13 */

 USB_DET */

 no cable plugged in */

 unable to configure using i2c */

 R-standby disables USB clock */

 USBCLK_ON */

 USBCLK_ON = H */

 wait 20ms to let the clock settle */

 register board specific self-refresh code */

 enable SCIF1 serial port for YC401 console support */

 ECONTMSK(bit6=L10ONEN) set 1 */

 HPON_ON */

 HPON_ON = H */

 setup NOR flash at CS0 */

 setup NAND flash at CS4 */

 setup KEYSC pins */

 setup LCDC pins for SYS panel */

 LCD_RS */

 LCD_CS/ */

 LCD_RD/ */

 LCD_WR/ */

 LCD_VSYNC */

 LCD_RST/ */

 PROTECT/ */

 LEDSTDBY/ */

 setup USB function */

 CEU */

 SDHI0 connected to yc304 */

 Set camera clock frequency and register and alias for rj54n1. */

	/* set DRVCRB

	 *

	 * use 1.8 V for VccQ_VIO

	 * use 2.85V for VccQ_SR

 Initialize CEU platform device separately to map memory first */

 Return the board specific boot mode pin configuration */

	/* MD0=1, MD1=1, MD2=0: Clock Mode 3

	 * MD3=0: 16-bit Area0 Bus Width

	 * MD5=1: Little Endian

	 * MD8=1: Test Mode Disabled

 Reserve a portion of memory for CEU buffers */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * KFR2R09 LCD panel support

 *

 * Copyright (C) 2009 Magnus Damm

 *

 * Register settings based on the out-of-tree t33fb.c driver

 * Copyright (C) 2008 Lineo Solutions, Inc.

/* The on-board LCD module is a Hitachi TX07D34VM0AAA. This module is made

 * up of a 240x400 LCD hooked up to a R61517 driver IC. The driver IC is

 * communicating with the main port of the LCDC using an 18-bit SYS interface.

 *

 * The device code for this LCD module is 0x01221517.

 WEMODE: 1=cont, 0=one-shot */

 EPF, DFM */

 RIM[1] : 1 (18bpp) */

 400 lines */

 PTH4/LCDRS High [param, 17:0] */

 PTH4/LCDRS Low [cmd, 7:0] */

 access protect OFF */

 deep standby OFF */

 device code command */

 dummy read */

 read device code */

 write start */

 paint it black */

 access protect off */

 exit deep standby mode */

 frame memory I/F */

 display mode and frame memory write mode */

 DBI, internal clock */

 panel */

 timing (normal) */

 timing (partial) */

 timing (idle) */

 timing (source/VCOM/gate driving) */

 gamma (red) */

 gamma (green) */

 gamma (blue) */

 power (common) */

 VCOM */

 power (normal) */

 power (partial) */

 power (idle) */

 TE signal */

 TE signal line */

 column address */

 page address */

 exit sleep mode */

 clear vram */

 display ON */

 power on */

 PROTECT/ -> L */

 LCD_RST/ -> L */

 PROTECT/ -> H */

 LCD_RST/ -> H */

 PROTECT/ -> L */

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas Technology Sales RTS7751R2D Support.

 *

 * Copyright (C) 2002 - 2006 Atom Create Engineering Co., Ltd.

 * Copyright (C) 2004 - 2007 Paul Mundt

 For R2D-1 polling is preferred */

 Single Epson RTC-9701JE attached on CS0 */

/*

 * The CF is connected with a 16-bit bus where 8-bit operations are

 * unsupported. The linux ata driver is however using 8-bit operations, so

 * insert a trapped io filter to convert 8-bit operations into 16-bit.

/*

 * Initialize the board

	/* sm501 dram configuration:

	 * ColSizeX = 11 - External Memory Column Size: 256 words.

	 * APX = 1 - External Memory Active to Pre-Charge Delay: 7 clocks.

	 * RstX = 1 - External Memory Reset: Normal.

	 * Rfsh = 1 - Local Memory Refresh to Command Delay: 12 clocks.

	 * BwC =  1 - Local Memory Block Write Cycle Time: 2 clocks.

	 * BwP =  1 - Local Memory Block Write to Pre-Charge Delay: 1 clock.

	 * AP = 1 - Internal Memory Active to Pre-Charge Delay: 7 clocks.

	 * Rst = 1 - Internal Memory Reset: Normal.

	 * RA = 1 - Internal Memory Remain in Active State: Do not remain.

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/renesas/rts7751r2d/irq.c

 *

 * Copyright (C) 2007  Magnus Damm

 * Copyright (C) 2000  Kazumoto Kojima

 *

 * Renesas Technology Sales RTS7751R2D Support, R2D-PLUS and R2D-1.

 *

 * Modified for RTS7751R2D by

 * Atom Create Engineering Co., Ltd. 2002.

 board specific interrupt sources (R2D-1 and R2D-PLUS) */

 EXT_INT0-3 */

 Real Time Clock */

 Ethernet controller (R2D-1 board) */

 Key input (R2D-PLUS board) */

 SD Card */

 CF Card Detect + CF IDE */

 SM501 aka Voyager */

 Ethernet controller */

 Cardbus/PCMCIA bridge */

 Ethernet controller with HUB (R2D-PLUS board) */

 PCI Slot 3.3v (R2D-1 board) */

 PCI Slot 3.3v */

 Touch Panel */

 Vectors for R2D-1 */

 ng */

 IRLMSK mask register layout for R2D-1 */

 IRLMSK */

 IRLn to IRQ table for R2D-1 */

 CONFIG_RTS7751R2D_1 */

 Vectors for R2D-PLUS */

 IRLMSK mask register layout for R2D-PLUS */

 IRLMSK */

 IRLn to IRQ table for R2D-PLUS */

 CONFIG_RTS7751R2D_PLUS */

/*

 * Initialize IRQ setting

 according to manual */

 in reality */

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas Technology Europe RSK+ 7203 Support.

 *

 * Copyright (C) 2008 - 2010  Paul Mundt

 default to 50ms */

 Select pins for SCIF0 */

 Setup LAN9118: CS1 in 16-bit Big Endian Mode, IRQ0 at Port B */

 CS1BCR */

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas Technology Europe RSK+ Support.

 *

 * Copyright (C) 2008 Paul Mundt

 * Copyright (C) 2008 Peter Griffin <pgriffin@mpc-data.co.uk>

 Dummy supplies, where voltage doesn't matter */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * RSK+SH7264 Support.

 *

 * Copyright (C) 2012 Renesas Electronics Europe

 SPDX-License-Identifier: GPL-2.0

/*

 * RSK+SH7269 Support

 *

 * Copyright (C) 2012  Renesas Electronics Europe Ltd

 * Copyright (C) 2012  Phil Edworthy

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/dreamcast/setup.c

 *

 * Hardware support for the Sega Dreamcast.

 *

 * Copyright (c) 2001, 2002 M. R. Brown <mrbrown@linuxdc.org>

 * Copyright (c) 2002, 2003, 2004 Paul Mundt <lethal@linux-sh.org>

 *

 * This file is part of the LinuxDC project (www.linuxdc.org)

 *

 * This file originally bore the message (with enclosed-$):

 *	Id: setup_dc.c,v 1.5 2001/05/24 05:09:16 mrbrown Exp

 *	SEGA Dreamcast support

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/dreamcast/rtc.c

 *

 * Dreamcast AICA RTC routines.

 *

 * Copyright (c) 2001, 2002 M. R. Brown <mrbrown@0xd6.org>

 * Copyright (c) 2002 Paul Mundt <lethal@chaoticdreams.org>

/* The AICA RTC has an Epoch of 1/1/1950, so we must subtract 20 years (in

   seconds) to get the standard Unix Epoch when getting the time, and add

/* The AICA RTC is represented by a 32-bit seconds counter stored in 2 16-bit

/**

 * aica_rtc_gettimeofday - Get the time from the AICA RTC

 * @dev: the RTC device (ignored)

 * @tm: pointer to resulting RTC time structure

 *

 * Grabs the current RTC seconds counter and adjusts it to the Unix Epoch.

 normalize to 1970..2106 time range */

/**

 * aica_rtc_settimeofday - Set the AICA RTC to the current time

 * @dev: the RTC device (ignored)

 * @tm: pointer to new RTC time structure

 *

 * Adjusts the given @tv to the AICA Epoch and sets the RTC seconds counter.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/dreamcast/irq.c

 *

 * Holly IRQ support for the Sega Dreamcast.

 *

 * Copyright (c) 2001, 2002 M. R. Brown <mrbrown@0xd6.org>

 *

 * This file is part of the LinuxDC project (www.linuxdc.org)

/*

 * Dreamcast System ASIC Hardware Events -

 *

 * The Dreamcast's System ASIC (a.k.a. Holly) is responsible for receiving

 * hardware events from system peripherals and triggering an SH7750 IRQ.

 * Hardware events can trigger IRQs 13, 11, or 9 depending on which bits are

 * set in the Event Mask Registers (EMRs).  When a hardware event is

 * triggered, its corresponding bit in the Event Status Registers (ESRs)

 * is set, and that bit should be rewritten to the ESR to acknowledge that

 * event.

 *

 * There are three 32-bit ESRs located at 0xa05f6900 - 0xa05f6908.  Event

 * types can be found in arch/sh/include/mach-dreamcast/mach/sysasic.h.

 * There are three groups of EMRs that parallel the ESRs.  Each EMR group

 * corresponds to an IRQ, so 0xa05f6910 - 0xa05f6918 triggers IRQ 13,

 * 0xa05f6920 - 0xa05f6928 triggers IRQ 11, and 0xa05f6930 - 0xa05f6938

 * triggers IRQ 9.

 *

 * In the kernel, these events are mapped to virtual IRQs so that drivers can

 * respond to them as they would a normal interrupt.  In order to keep this

 * mapping simple, the events are mapped as:

 *

 * 6900/6910 - Events  0-31, IRQ 13

 * 6904/6924 - Events 32-63, IRQ 11

 * 6908/6938 - Events 64-95, IRQ  9

 *

 Base event status register */

 Base event mask register */

/*

 * Helps us determine the EMR group that this event belongs to: 0 = 0x6910,

 * 1 = 0x6920, 2 = 0x6930; also determine the event offset.

 Return the hardware event's bit position within the EMR/ESR */

/*

 * For each of these *_irq routines, the IRQ passed in is the virtual IRQ

 * (logically mapped to the corresponding bit for the hardware event).

 Disable the hardware event by masking its bit in its EMR */

 Enable the hardware event by setting its bit in its EMR */

 Acknowledge a hardware event by writing its bit back to its ESR */

/*

 * Map the hardware event indicated by the processor IRQ to a virtual IRQ.

 Mask the ESR to filter any spurious, unwanted interrupts */

 Now scan and find the first set bit as the event to map */

 Not reached */

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas Technology Europe SDK7786 Support.

 *

 * Copyright (C) 2010  Matt Fleming

 * Copyright (C) 2010  Paul Mundt

 Dummy supplies, where voltage doesn't matter */

	/*

	 * Hand over I2C control to the FPGA.

/*

 * FPGA-driven PCIe clocks

 *

 * Historically these include the oscillator, clock B (slots 2/3/4) and

 * clock A (slot 1 and the CPU clock). Newer revs of the PCB shove

 * everything under a single PCIe clocks enable bit that happens to map

 * to the same bit position as the oscillator bit for earlier FPGA

 * versions.

 *

 * Given that the legacy clocks have the side-effect of shutting the CPU

 * off through the FPGA along with the PCI slots, we simply leave them in

 * their initial state and don't bother registering them with the clock

 * framework.

	/*

	 * Only handle the EXTAL case, anyone interfacing a crystal

	 * resonator will need to provide their own input clock.

	/*

	 * Setup the FPGA clocks.

	/*

	 * It can take up to 20us for the R8C to do its job, back off and

	 * wait a bit until we've been shut off. Even though newer FPGA

	 * versions don't set the ACK bit, the latency issue remains.

 Initialize the board */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * SDK7786 FPGA SRAM Support.

 *

 * Copyright (C) 2010  Paul Mundt

 Enable FPGA SRAM */

	/*

	 * FPGA_SEL determines the area mapping

	/*

	 * The memory itself occupies a 2KiB range at the top of the area

	 * immediately below the system registers.

	/*

	 * The FPGA SRAM resides in translatable physical space, so set

	 * up a mapping prior to inserting it in to the pool.

 SPDX-License-Identifier: GPL-2.0

/*

 * SDK7786 FPGA USRGPIR Support.

 *

 * Copyright (C) 2010  Paul Mundt

 always in */

 don't care */

 SPDX-License-Identifier: GPL-2.0

/*

 * SDK7786 FPGA Support.

 *

 * Copyright (C) 2010  Paul Mundt

/*

 * The FPGA can be mapped in any of the generally available areas,

 * so we attempt to scan for it using the fixed SRSTR read magic.

 *

 * Once the FPGA is located, the rest of the mapping data for the other

 * components can be determined dynamically from its section mapping

 * registers.

	/*

	 * Iterate over all of the areas where the FPGA could be mapped.

	 * The possible range is anywhere from area 0 through 6, area 7

	 * is reserved.

 Failed to remap this area, move along. */

 Found it! */

 SPDX-License-Identifier: GPL-2.0

/*

 * SDK7786 FPGA NMI Support.

 *

 * Copyright (C) 2010  Paul Mundt

/*

 * Default to the manual NMI switch.

 Set the NMI source */

 And the IRQ masking */

 SPDX-License-Identifier: GPL-2.0

/*

 * SDK7786 FPGA IRQ Controller Support.

 *

 * Copyright (C) 2010  Matt Fleming

 * Copyright (C) 2010  Paul Mundt

 Enable priority encoding for all IRLs */

 Clear FPGA interrupt status registers */

 Unmask FPGA interrupts */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/se/7619/setup.c

 *

 * Copyright (C) 2006 Yoshinori Sato

 *

 * Hitachi SH7619 SolutionEngine Support.

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/se/7780/setup.c

 *

 * Copyright (C) 2006,2007  Nobuhiro Iwamatsu

 *

 * Hitachi UL SolutionEngine 7780 Support.

 Heartbeat */

 SMC91x */

 don't use dma */

 "SH-Linux" on LED Display */

	/*

	 * PCI REQ/GNT setting

	 *   REQ0/GNT0 -> USB

	 *   REQ1/GNT1 -> PC Card

	 *   REQ2/GNT2 -> Serial ATA

	 *   REQ3/GNT3 -> PCI slot

 GPIO setting */

 iVDR Power ON */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/se/7780/irq.c

 *

 * Copyright (C) 2006,2007  Nobuhiro Iwamatsu

 *

 * Hitachi UL SolutionEngine 7780 Support.

/*

 * Initialize IRQ setting

 enable all interrupt at FPGA */

 mask SM501 interrupt */

 enable all interrupt at FPGA */

 set FPGA INTSEL register */

 FPGA + 0x06 */

 FPGA + 0x08 */

 FPGA + 0x0A */

 install handlers for IRQ0-7 */

 ICR1: detect low level(for 2ndcut) */

	/*

	 * FPGA PCISEL register initialize

	 *

	 *  CPU  || SLOT1 | SLOT2 | S-ATA | USB

	 *  -------------------------------------

	 *  INTA || INTA  | INTD  |  --   | INTB

	 *  -------------------------------------

	 *  INTB || INTB  | INTA  |  --   | INTC

	 *  -------------------------------------

	 *  INTC || INTC  | INTB  | INTA  |  --

	 *  -------------------------------------

	 *  INTD || INTD  | INTC  |  --   | INTA

	 *  -------------------------------------

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/se/7751/setup.c

 *

 * Copyright (C) 2000  Kazumoto Kojima

 *

 * Hitachi SolutionEngine Support.

 *

 * Modified for 7751 Solution Engine by

 * Ian da Silva and Jeremy Siegel, 2001.

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/se/7751/irq.c

 *

 * Copyright (C) 2000  Kazumoto Kojima

 *

 * Hitachi SolutionEngine Support.

 *

 * Modified for 7751 Solution Engine by

 * Ian da Silva and Jeremy Siegel, 2001.

 Add additional entries here as drivers are added and tested. */

/*

 * Initialize IRQ setting

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/se/7724/setup.c

 *

 * Copyright (C) 2009 Renesas Solutions Corp.

 *

 * Kuninori Morimoto <morimoto.kuninori@renesas.com>

/*

 * SWx    1234 5678

 * ------------------------------------

 * SW31 : 1001 1100    : default

 * SW32 : 0111 1111    : use on board flash

 *

 * SW41 : abxx xxxx  -> a = 0 : Analog  monitor

 *                          1 : Digital monitor

 *                      b = 0 : VGA

 *                          1 : 720p

/*

 * about 720p

 *

 * When you use 1280 x 720 lcdc output,

 * you should change OSC6 lcdc clock from 25.175MHz to 74.25MHz,

 * and change SW41 to use 720p

/*

 * about sound

 *

 * This setup.c supports FSI slave mode.

 * Please change J20, J21, J22 pin to 1-2 connection.

 Heartbeat */

 LAN91C111 */

 MTD */

 Read-only */

 LCDC */

 hsync and vsync are active low */

 hsync and vsync are active low */

 7.0 inch */

 CEU0 */

 "ceu.0" clock */

 CEU1 */

 "ceu.1" clock */

 FSI */

 change J20, J21, J22 pin to 1-2 connection to use slave mode */

 KEYSC in SoC (Needs SW33-2 set to ON) */

 "keysc0" clock */

 SH Eth */

 SMSC LAN8187 */

  not use dma */

 USB1 */

  not use dma */

 Fixed 3.3V regulator to be used by SDHI0, SDHI1 */

 IrDA */

 With open J18 jumper address is 0x21 */

 I2C device */

 check EEPROM status */

 read MAC addr from EEPROM */

 read */

 enable I2C device */

 select camera, monitor */

 register board specific self-refresh code */

 Reset Release */

 bit4: NTSC_PDN, bit5: NTSC_RESET */

 LAN */

 AK8813 PDN */

 AK8813 RESET */

 VIDEO DAC */

 AK4643 */

 IrDA */

 USB0 */

 RMII */

 AK8813 RESET */

 turn on USB clocks, use external clock */

 Let LED9 show STATUS2 */

 Lit LED10 show STATUS0 */

 Lit LED11 show PDSTATUS */

 enable USB0 port */

 enable USB1 port */

 enable IRQ 0,1,2 */

 enable SCIFA3 */

 enable LCDC */

 enable CEU0 */

 enable CEU1 */

 KEYSC */

 enable FSI */

 set SPU2 clock to 83.4 MHz */

 change parent of FSI A */

 48kHz dummy clock was used to make sure 1/1 divide */

 SDHI0 connected to cn7 */

 SDHI1 connected to cn8 */

 enable IrDA */

	/*

	 * enable SH-Eth

	 *

	 * please remove J33 pin from your board !!

	 *

	 * ms7724 board should not use GPIO_FN_LNKSTA pin

	 * So, This time PTX5 is set to input pin

 720p */

 VGA */

 Digital monitor */

 Analog monitor */

 VOU */

 Initialize CEU platform devices separately to map memory first */

 Reserve a portion of memory for CEU 0 and CEU 1 buffers */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/se/7724/irq.c

 *

 * Copyright (C) 2009 Renesas Solutions Corp.

 *

 * Kuninori Morimoto <morimoto.kuninori@renesas.com>

 *

 * Based on  linux/arch/sh/boards/se/7722/irq.c

 * Copyright (C) 2007  Nobuhiro Iwamatsu

 *

 * Hitachi UL SolutionEngine 7724 Support.

/*

 * Initialize IRQ setting

 mask all */

 mask all */

 mask all */

 clear irq */

 clear irq */

 clear irq */

 set irq type */

 SPDX-License-Identifier: GPL-2.0

 Filled in later */

 Wire-up dynamic vectors */

/*

 * Initialize the board

 FPGA */

 PORT E 1 = IRQ5 */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * Hitachi UL SolutionEngine 7343 FPGA IRQ Support.

 *

 * Copyright (C) 2008  Yoshihiro Shimoda

 * Copyright (C) 2012  Paul Mundt

 *

 * Based on linux/arch/sh/boards/se/7343/irq.c

 * Copyright (C) 2007  Nobuhiro Iwamatsu

 CPLD Interrupt status register */

 CPLD Interrupt mask register */

/*

 * Initialize IRQ setting

	/*

	 * All FPGA IRQs disabled by default

 mrshpc irq enable */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/se/770x/setup.c

 *

 * Copyright (C) 2000  Kazumoto Kojima

 *

 * Hitachi SolutionEngine Support.

 *

/*

 * Configure the Super I/O chip

 XXX: Another candidate for a more generic cchip machine vector */

 FDC */

 IRQ6 */

 AUXIO (GPIO): to use IDE1 */

 nIOROP */

 nIOWOP */

 COM1 */

 IRQ4 */

 COM2 */

 IRQ3 */

 RTC */

 IRQ8 */

 XXX: PARPORT, KBD, and MOUSE will come here... */

 SH771X Ethernet driver */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/se/770x/irq.c

 *

 * Copyright (C) 2000  Kazumoto Kojima

 * Copyright (C) 2006  Nobuhiro Iwamatsu

 *

 * Hitachi SolutionEngine Support.

 *

	/*

	* Super I/O (Just mimic PC):

	*  1: keyboard

	*  3: serial 0

	*  4: serial 1

	*  5: printer

	*  6: floppy

	*  8: rtc

	* 12: mouse

	* 14: ide0

 This is default value */

 LAN */

 PCIRQ2 */

 PCIRQ1 */

 PCIRQ0 */

 ST NIC */

 LAN */

 MRSHPC IRQs setting */

 PCIRQ3 */

 PCIRQ2 */

 PCIRQ1 */

 PCIRQ0 */

 #2, #13 are allocated for SLOT IRQ #1 and #2 (for now) */

 NOTE: #2 and #13 are not used on PC */

 SLOTIRQ2 */

 SLOTIRQ1 */

/*

 * Initialize IRQ setting

 Disable all interrupts */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/se/7722/setup.c

 *

 * Copyright (C) 2007 Nobuhiro Iwamatsu

 * Copyright (C) 2012 Paul Mundt

 *

 * Hitachi UL SolutionEngine 7722 Support.

 Heartbeat */

 SMC91x */

 Filled in later */

 don't use dma */

 Filled in later */

 KEYOUT0->5, KEYIN0->4 */

 SW1 -> SW30 */

 life */

 "keysc0" clock */

 Wire-up dynamic vectors */

 FPGA */

 PORT E 1 = IRQ5 ,E 0 = BS */

 PORT J 1 = IRQ1,J 0 =IRQ0 */

 LCDC I/O */

 SIOF1*/

 LCDC */

 LCDC,CS6A */

 KEYSC */

 BS,SHHID2 */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * Hitachi UL SolutionEngine 7722 FPGA IRQ Support.

 *

 * Copyright (C) 2007  Nobuhiro Iwamatsu

 * Copyright (C) 2012  Paul Mundt

/*

 * Initialize FPGA IRQs

	/*

	 * All FPGA IRQs disabled by default

 mrshpc irq enable */

 SPDX-License-Identifier: GPL-2.0

/*

 *

 * linux/arch/sh/boards/se/7206/setup.c

 *

 * Copyright (C) 2006  Yoshinori Sato

 * Copyright (C) 2007 - 2008  Paul Mundt

 *

 * Hitachi 7206 SolutionEngine Support.

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/se/7206/irq.c

 *

 * Copyright (C) 2005,2006 Yoshinori Sato

 *

 * Hitachi SolutionEngine Support.

 *

 Set the priority in IPR to 0 */

 FPGA mask set */

 Set priority in IPR back to original value */

 FPGA mask reset */

 FPGA isr clear */

/*

 * Initialize IRQ setting

 SMC91C111 */

 ATA */

 SLOT / PCM */

 ICR1 */

 FPGA System register setup*/

 Clear INTSTS0 */

 Clear INTSTS1 */

 IRQ0=LAN, IRQ1=ATA, IRQ3=SLT,PCM */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/se/7721/setup.c

 *

 * Copyright (C) 2008 Renesas Solutions Corp.

 *

 * Hitachi UL SolutionEngine 7721 Support.

 for USB */

 PGCR */

 PHCR */

 PPCR */

 PSELA */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/se/7721/irq.c

 *

 * Copyright (C) 2008  Renesas Solutions Corp.

 board specific interrupt sources */

 IRLMSK */

/*

 * Initialize IRQ setting

 PPCR */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/renesas/r7780rp/psw.c

 *

 * push switch support for RDBRP-1/RDBREVRP-1 debug boards.

 *

 * Copyright (C) 2006  Paul Mundt

 Nothing to do if there's no state change */

 Figure out who raised it */

 debounce */

 Clear the switch IRQs */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/renesas/r7780rp/setup.c

 *

 * Renesas Solutions Highlander Support.

 *

 * Copyright (C) 2002 Atom Create Engineering Co., Ltd.

 * Copyright (C) 2005 - 2008 Paul Mundt

 *

 * This contains support for the R7780RP-1, R7780MP, and R7785RP

 * Highlander modules.

 irq number */

 don't use dma */

 irq number */

 don't use dma */

 R7785RP has a slightly more sensible FPGA.. */

 This config is flash board for mass production. */

/*

 * The CF is connected using a 16-bit bus where 8-bit operations are

 * unsupported. The linux ata driver is however using 8-bit operations, so

 * insert a trapped io filter to convert 8-bit operations into 16-bit.

/*

 * Platform specific clocks

 main clocks */

/*

 * Initialize the board

	/*

	 * Enable the important clocks right away..

 Clear LED. */

 SD Power ON */

 Si13112 */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas Solutions Highlander R7785RP Support.

 *

 * Copyright (C) 2002  Atom Create Engineering Co., Ltd.

 * Copyright (C) 2006 - 2008  Paul Mundt

 * Copyright (C) 2007  Magnus Damm

 FPGA specific interrupt sources */

 Compact Flash */

 SMBUS */

 Touch panel */

 RTC Alarm */

 Temperature sensor */

 Ethernet controller */

 external bus connector */

 IRLMCR1 */

 IRLMCR2 */

 FPGA IRLSSR1(CF_CD clear) */

 Setup the FPGA IRL */

 FPGA IRLA */

 FPGA IRLB */

 FPGA IRLC */

 FPGA IRLD */

 FPGA IRLE */

 FPGA IRLF */

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas Solutions Highlander R7780RP-1 Support.

 *

 * Copyright (C) 2002  Atom Create Engineering Co., Ltd.

 * Copyright (C) 2006  Paul Mundt

 * Copyright (C) 2008  Magnus Damm

 board specific interrupt sources */

 Ethernet controller */

 Push Switch */

 Compact Flash */

 dirty: overwrite cpu vectors for pci */

 IRLMSK */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2008 Paul Mundt

 SCIF0 */

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas Solutions Highlander R7780MP Support.

 *

 * Copyright (C) 2002  Atom Create Engineering Co., Ltd.

 * Copyright (C) 2006  Paul Mundt

 * Copyright (C) 2007  Magnus Damm

 board specific interrupt sources */

 Compact Flash */

 Touch panel */

 FPGA SCIF1 */

 FPGA SCIF0 */

 SMBUS */

 RTC Alarm */

 Ethernet controller */

 Push Switch */

 external bus connector */

 IRLMSK */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/superh/microdev/setup.c

 *

 * Copyright (C) 2003 Sean McGoogan (Sean.McGoogan@superh.com)

 * Copyright (C) 2003, 2004 SuperH, Inc.

 * Copyright (C) 2004, 2005 Paul Mundt

 *

 * SuperH SH4-202 MicroDev board support.

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/superh/microdev/io.c

 *

 * Copyright (C) 2003 Sean McGoogan (Sean.McGoogan@superh.com)

 * Copyright (C) 2003, 2004 SuperH, Inc.

 * Copyright (C) 2004 Paul Mundt

 *

 * SuperH SH4-202 MicroDev board support.

	/*

	 *	we need to have a 'safe' address to re-direct all I/O requests

	 *	that we do not explicitly wish to handle. This safe address

	 *	must have the following properies:

	 *

	 *		* writes are ignored (no exception)

	 *		* reads are benign (no side-effects)

	 *		* accesses of width 1, 2 and 4-bytes are all valid.

	 *

	 *	The Processor Version Register (PVR) has these properties.

 Processor Version Register */

 I/O base for SMSC FDC37C93xAPM IDE #2 */

 I/O base for SMSC FDC37C93xAPM IDE #1 */

 I/O port for Philips ISP1161x USB chip */

 I/O base for SMSC FDC37C93xAPM Serial #2 */

 I/O base for SMSC LAN91C111 Ethernet chip */

 I/O misc for SMSC FDC37C93xAPM IDE #2 */

 I/O base for SMSC FDC37C93xAPM SuperIO chip */

 I/O misc for SMSC FDC37C93xAPM IDE #1 */

 I/O base for SMSC FDC37C93xAPM Serial #1 */

 I/O extent for Philips ISP1161x USB chip */

 I/O extent for SMSC LAN91C111 Ethernet chip */

 I/O extent for SMSC FDC37C93xAPM SuperIO chip */

 I/O extent for IDE Task Register set */

 Physical address of SMSC LAN91C111 Ethernet chip */

 Physical address of Philips ISP1161x USB chip */

 Physical address of SMSC FDC37C93xAPM SuperIO chip */

/*

 * map I/O ports to memory-mapped addresses

			/*

			 *	SMSC LAN91C111 Ethernet chip

			/*

			 *	SMSC FDC37C93xAPM SuperIO chip

			 *

			 *	Configuration Registers

			/*

			 *	SMSC FDC37C93xAPM SuperIO chip

			 *

			 *	IDE #1

			/*

			 *	SMSC FDC37C93xAPM SuperIO chip

			 *

			 *	IDE #2

			/*

			 *	SMSC FDC37C93xAPM SuperIO chip

			 *

			 *	Serial #1

			/*

			 *	SMSC FDC37C93xAPM SuperIO chip

			 *

			 *	Serial #2

			/*

			 *	Philips USB ISP1161x chip

			/*

			 *	safe default.

 SPDX-License-Identifier: GPL-2.0

/*

 * Setup for the SMSC FDC37C93xAPM

 *

 * Copyright (C) 2003 Sean McGoogan (Sean.McGoogan@superh.com)

 * Copyright (C) 2003, 2004 SuperH, Inc.

 * Copyright (C) 2004, 2005 Paul Mundt

 *

 * SuperH SH4-202 MicroDev board support.

 Logical Device Number */

 Device ID */

 Device Revision */

 Activate */

 Primary Base Address */

 Secondary Base Address */

 Primary Interrupt Select */

 Secondary Interrupt Select */

 HDCS0 Address Decoder */

 HDCS1 Address Decoder */

 IDE #1 logical device */

 IDE #2 logical device */

 Parallel Port logical device */

 Serial #1 logical device */

 Serial #2 logical device */

 Keyboard logical device */

 Configuration Registers (Aux I/O) */

 Task File Registe base for IDE #1 */

 Miscellaneous AT registers for IDE #1 */

 Task File Registe base for IDE #2 */

 Miscellaneous AT registers for IDE #2 */

 General-Purpose base address on CPU-board FPGA */

 Initially the chip is in run state */

 Put it into configuration state */

 Read device ID info */

 Select the keyboard device */

 enable it */

 enable the interrupts */

 Select the Serial #1 device */

 enable it */

 program with port addresses */

 enable the interrupts */

 Select the Serial #2 device */

 enable it */

 program with port addresses */

 enable the interrupts */

 Select the IDE#1 device */

 enable it */

 program with port addresses */

 select the interrupt */

 Select the IDE#2 device */

 enable it */

 program with port addresses */

 select the interrupt */

 Select the configuration registers */

		/* enable the appropriate GPIO pins for IDE functionality:

		 * bit[0]   In/Out		1==input;  0==output

		 * bit[1]   Polarity		1==invert; 0==no invert

		 * bit[2]   Int Enb #1		1==Enable Combined IRQ #1; 0==disable

		 * bit[3:4] Function Select	00==original; 01==Alternate Function #1

 GP42 = nIDE1_OE */

 GP45 = IDE1_IRQ */

 GP46 = nIOROP */

 GP47 = nIOWOP */

 GP20 = nIDE2_OE */

 Exit the configuration state */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/superh/microdev/irq.c

 *

 * Copyright (C) 2003 Sean McGoogan (Sean.McGoogan@superh.com)

 *

 * SuperH SH4-202 MicroDev board support.

 IRL0 .. IRL15 */

 IRQ #0	IRL=15	0x200  */

 IRQ #1	IRL=14	0x220  */

 IRQ #2	IRL=13	0x240  */

 IRQ #3	IRL=12	0x260  */

 IRQ #4	IRL=11	0x280  */

 IRQ #5	IRL=10	0x2a0  */

 IRQ #6	IRL=9	0x2c0  */

 IRQ #7	IRL=8	0x2e0  */

 IRQ #8	IRL=7	0x300  */

 IRQ #9	IRL=6	0x320  */

 IRQ #10	IRL=5	0x340  */

 IRQ #11	IRL=4	0x360  */

 IRQ #12	IRL=3	0x380  */

 IRQ #13	IRL=2	0x3a0  */

 IRQ #14	IRL=1	0x3c0  */

 IRQ #15	IRL=0	0x3e0  */

 disable interrupts on the FPGA INTC register */

 set priority for the interrupt */

 enable interrupts on the FPGA INTC register */

 This function sets the desired irq handler to be a MicroDev type */

 disable interrupts on the FPGA INTC register */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/lbox/setup.c

 *

 * Copyright (C) 2007 Nobuhiro Iwamatsu

 *

 * NTT COMWARE L-BOX RE2 Support

 Boot CF base address */

 open I/O area window */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/lboxre2/irq.c

 *

 * Copyright (C) 2007 Nobuhiro Iwamatsu

 *

 * NTT COMWARE L-BOX RE2 Support.

/*

 * Initialize IRQ setting

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas System Solutions Asia Pte. Ltd - Migo-R

 *

 * Copyright (C) 2008 Magnus Damm

/* Address     IRQ  Size  Bus  Description

 * 0x00000000       64MB  16   NOR Flash (SP29PL256N)

 * 0x0c000000       64MB  64   SDRAM (2xK4M563233G)

 * 0x10000000  IRQ0       16   Ethernet (SMC91C111)

 * 0x14000000  IRQ4       16   USB 2.0 Host Controller (M66596)

 * 0x18000000       8GB    8   NAND Flash (K9K8G08U0A)

 IRQ0 */

 KEYOUT0->4, KEYIN1->5 */

 "keysc0" clock */

 Read-only */

 NAND_RBn */

 7.0 inch */

 2.4 inch */

 set 1s delay to encourage fsync() */

 P4-only space */

 [0] = ov772x */

 [1] = tw9910 */

 ceu.0 */

 Powerdown/reset gpios for CEU image sensors */

 Fixed 3.3V regulator to be used by SDHI0 */

 IRQ6 */

 register board specific self-refresh code */

 Let D11 LED show STATUS0 */

 Lit D12 LED show PDSTATUS */

 SMC91C111 - Enable IRQ0, Setup CS4 for 16-bit fast access */

 KEYSC */

 NAND Flash */

 SDHI */

 Touch Panel */

 LCD Panel */

 LCDC - QVGA - Enable SYS Interface signals */

 LCD_DON */

 LCDC - WVGA - Enable RGB Interface signals */

 CEU */

 D15->D8 */

 SIU: Port B */

	/*

	 * The original driver sets SIUB OLR/OBT, ILR/IBT, and SIUA OLR/OBT to

	 * output. Need only SIUB, set to output for master mode (table 34.2)

	 /*

	  * Use 10 MHz VIO_CKO instead of 24 MHz to work around signal quality

	  * issues on Panel Board V2.1.

 Add a clock alias for ov7725 xclk source. */

 Register GPIOs for video sources. */

 Initialize CEU platform device separately to map memory first */

 Return the board specific boot mode pin configuration */

	/* MD0=1, MD1=1, MD2=0: Clock Mode 3

	 * MD3=0: 16-bit Area0 Bus Width

	 * MD5=1: Little Endian

	 * TSTMD=1, MD8=0: Test Mode Disabled

 Reserve a portion of memory for CEU buffers */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * Support for SuperH MigoR Quarter VGA LCD Panel

 *

 * Copyright (C) 2008 Magnus Damm

 *

 * Based on lcd_powertip.c from Kenati Technologies Pvt Ltd.

 * Copyright (c) 2007 Ujjwal Pande <ujjwal@kenati.com>,

/* LCD Module is a PH240320T according to board schematics. This module

 * is made up of a 240x320 LCD hooked up to a R61505U (or HX8347-A01?)

 * Driver IC. This IC is connected to the SH7722 built-in LCDC using a

 * SYS-80 interface configured in 16 bit mode.

 *

 * Index 0: "Device Code Read" returns 0x1505.

 DB0-DB7 are connected to D1-D8, and DB8-DB15 to D10-D17 */

 clear GRAM to avoid displaying garbage */

 horiz addr */

 vert addr */

 yes, 256 words per line */

 reset horiz addr */

 reset vert addr */

 enable display */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/mach-x3proto/setup.c

 *

 * Renesas SH-X3 Prototype Board Support.

 *

 * Copyright (C) 2007 - 2010  Paul Mundt

 Filled in by ilsel */

 Filled in by ilsel */

 don't use dma */

 Filled in by ilsel */

 don't use dma */

 Set ICR0.LVLMODE */

	/*

	 * IRLs are only needed for ILSEL mappings, so flip over the INTC

	 * pins at a later point to enable the GPIOs to settle.

	/*

	 * Now that ILSELs are available, set up the baseboard GPIOs.

	/*

	 * Propagate dynamic GPIOs for the baseboard button device.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/mach-x3proto/gpio.c

 *

 * Renesas SH-X3 Prototype Baseboard GPIO Support.

 *

 * Copyright (C) 2010 - 2012  Paul Mundt

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/mach-x3proto/ilsel.c

 *

 * Helper routines for SH-X3 proto board ILSEL.

 *

 * Copyright (C) 2007 - 2010  Paul Mundt

/*

 * ILSEL is split across:

 *

 *	ILSEL0 - 0xb8100004 [ Levels  1 -  4 ]

 *	ILSEL1 - 0xb8100006 [ Levels  5 -  8 ]

 *	ILSEL2 - 0xb8100008 [ Levels  9 - 12 ]

 *	ILSEL3 - 0xb810000a [ Levels 13 - 15 ]

 *

 * With each level being relative to an ilsel_source_t.

/*

 * ILSEL level map, in descending order from the highest level down.

 *

 * Supported levels are 1 - 15 spread across ILSEL0 - ILSEL4, mapping

 * directly to IRLs. As the IRQs are numbered in reverse order relative

 * to the interrupt level, the level map is carefully managed to ensure a

 * 1:1 mapping between the bit position and the IRQ number.

 *

 * This careful constructions allows ilsel_enable*() to be referenced

 * directly for hooking up an ILSEL set and getting back an IRQ which can

 * subsequently be used for internal accounting in the (optional) disable

 * path.

/**

 * ilsel_enable - Enable an ILSEL set.

 * @set: ILSEL source (see ilsel_source_t enum in include/asm-sh/ilsel.h).

 *

 * Enables a given non-aliased ILSEL source (<= ILSEL_KEY) at the highest

 * available interrupt level. Callers should take care to order callsites

 * noting descending interrupt levels. Aliasing FPGA and external board

 * IRQs need to use ilsel_enable_fixed().

 *

 * The return value is an IRQ number that can later be taken down with

 * ilsel_disable().

/**

 * ilsel_enable_fixed - Enable an ILSEL set at a fixed interrupt level

 * @set: ILSEL source (see ilsel_source_t enum in include/asm-sh/ilsel.h).

 * @level: Interrupt level (1 - 15)

 *

 * Enables a given ILSEL source at a fixed interrupt level. Necessary

 * both for level reservation as well as for aliased sources that only

 * exist on special ILSEL#s.

 *

 * Returns an IRQ number (as ilsel_enable()).

/**

 * ilsel_disable - Disable an ILSEL set

 * @irq: Bit position for ILSEL set value (retval from enable routines)

 *

 * Disable a previously enabled ILSEL set.

 SPDX-License-Identifier: GPL-2.0

/*

 * hp6x0 Power Management Routines

 *

 * Copyright (c) 2006 Andriy Skulysh <askulsyh@gmail.com>

 set wdt */

 disable PLL1 */

 enable standby */

 set self-refresh */

 set interrupt handler */

 enable PLL1 */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/hp6xx/setup.c

 *

 * Copyright (C) 2002 Andriy Skulysh

 * Copyright (C) 2007 Kristoffer Ericson <Kristoffer_e1@hotmail.com>

 *

 * Setup code for HP620/HP660/HP680/HP690 (internal peripherials only)

 CF Slot */

 HP Jornada 680/690 speaker on */

 HP Palmtop 620lx/660lx speaker on */

 HP Jornada 680/690 speaker off */

 HP Palmtop 620lx/660lx speaker off */

 Gets touchscreen and powerbutton IRQ working */

 Enable IRQ0 -> IRQ3 in IRQ_MODE */

 SPDX-License-Identifier: GPL-2.0

/*

 * bios-less APM driver for hp680

 *

 * Copyright 2005 (c) Andriy Skulysh <askulysh@gmail.com>

 * Copyright 2008 (c) Kristoffer Ericson <kristoffer.ericson@gmail.com>

 percentage values */

 resonably sane values */

 % of full battery */

 We want our estimates in minutes */

 Extremely(!!) rough estimate, we will replace this with a datalist later on */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/renesas/sdk7780/setup.c

 *

 * Renesas Solutions SH7780 SDK Support

 * Copyright (C) 2008 Nicholas Beck <nbeck@mpc-data.co.uk>

 Heartbeat */

 SMC91x */

 don't use dma */

 Setup pin mux'ing for PCIC */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/renesas/sdk7780/irq.c

 *

 * Renesas Technology Europe SDK7780 Support.

 *

 * Copyright (C) 2008  Nicholas Beck <nbeck@mpc-data.co.uk>

 board specific interrupt sources */

 Ethernet controller */

 Setup IRL 0-3 */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/renesas/sh7763rdp/setup.c

 *

 * Renesas Solutions sh7763rdp board

 *

 * Copyright (C) 2008 Renesas Solutions Corp.

 * Copyright (C) 2008 Nobuhiro Iwamatsu <iwamatsu.nobuhiro@renesas.com>

 NOR Flash */

 Read-only */

/*

 * SH-Ether

 *

 * SH Ether of SH7763 has multi IRQ handling.

 * (0x920,0x940,0x960 -> 0x920)

 use eth1 */

 TSU */

 irq number */

 SH7763 LCDC */

 Board version check */

 USB pin select bits (clear bit 5-2 to 0) */

 USBH setup port I controls to other (clear bits 4-9 to 0) */

 Select USB Host controller */

 For LCD */

 set PTJ7-1, bits 15-2 of PJCR to 0 */

 set PTI5, bits 11-10 of PICR to 0 */

 set PSEL2 bits 14-8, 5-4, of PSEL2 to 0 */

 set PSEL3 bits 14-12, 6-4, 2-0 of PSEL3 to 0 */

 For HAC */

 bit3-0  0100:HAC & SSI1 enable */

 bit14      1:SSI_HAC_CLK enable */

 SH-Ether */

 MMC */

selects SCIF and MMC other functions */

 MMC clock operates */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/sh/boards/renesas/sh7763rdp/irq.c

 *

 * Renesas Solutions SH7763RDP Support.

 *

 * Copyright (C) 2008 Renesas Solutions Corp.

 * Copyright (C) 2008  Nobuhiro Iwamatsu <iwamatsu.nobuhiro@renesas.com>

/*

 * Initialize IRQ setting

 GPIO enabled */

 enable GPIO interrupts */

 USBH enabled */

 GETHER enabled */

 DMAC enabled */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2009 Renesas Solutions Corp.

 *

 * Kuninori Morimoto <morimoto.kuninori@renesas.com>

/*

 *  Address      Interface        BusWidth

 *-----------------------------------------

 *  0x0000_0000  uboot            16bit

 *  0x0004_0000  Linux romImage   16bit

 *  0x0014_0000  MTD for Linux    16bit

 *  0x0400_0000  Internal I/O     16/32bit

 *  0x0800_0000  DRAM             32bit

 *  0x1800_0000  MFI              16bit

/* SWITCH

 *------------------------------

 * DS2[1] = FlashROM write protect  ON     : write protect

 *                                  OFF    : No write protect

 * DS2[2] = RMII / TS, SCIF         ON     : RMII

 *                                  OFF    : TS, SCIF3

 * DS2[3] = Camera / Video          ON     : Camera

 *                                  OFF    : NTSC/PAL (IN)

 * DS2[5] = NTSC_OUT Clock          ON     : On board OSC

 *                                  OFF    : SH7724 DV_CLK

 * DS2[6-7] = MMC / SD              ON-OFF : SD

 *                                  OFF-ON : MMC

/*

 * FSI - DA7210

 *

 * it needs amixer settings for playing

 *

 * amixer set 'HeadPhone' 80

 * amixer set 'Out Mixer Left DAC Left' on

 * amixer set 'Out Mixer Right DAC Right' on

 Heartbeat */

 PTG */

 MTD */

 force read-only */

 SH Eth */

 SMSC LAN8700 */

 USB0 host */

  not use dma */

 USB1 host/function */

 .name will be added in arch_setup */

  not use dma */

/*

 * USBHS

 enable vbus if HOST */

  not use dma */

 LCDC and backlight */

 hsync and vsync are active low */

 hsync and vsync are active low */

 7.0 inch */

 CEU0 */

 [0] = mt9t112  */

 [1] = tw9910  */

 ceu.0 */

 CEU1 */

 [0] = mt9t112  */

 ceu.1 */

 Power up/down GPIOs for camera devices and video decoder */

 I2C device */

 for 24MHz */

 for 24MHz */

 1st camera */

 2nd camera */

 KEYSC */

 keysc0 clock */

 TouchScreen */

 Offset 7 on port B */

 SDHI0 */

 Offset 6 on port B */

 Card detect */

 SDHI1 */

 Card detect */

 CONFIG_MMC_SH_MMCIF */

 MMC SPI */

 3.3V only */

 device "mmc_spi" @ CS0 */

 Card detect */

 Write protect */

 MSIOF0 */

 MSIOF0 */

 FSI */

 IrDA */

 SH_MMCIF */

 MMC2I */

 MMC3I */

 SH7724: Max Pclk/2 */

 read MAC address from EEPROM */

 register board specific self-refresh code */

 enable STATUS0, STATUS2 and PDSTATUS */

 enable SCIFA0 */

 enable debug LED */

 enable SH-Eth */

 enable USB */

 enable LCDC */

 I/O buffer drive ability is high */

 DVI */

 No backlight */

 Panel */

		/* FIXME

		 *

		 * LCDDON control is needed for Panel,

		 * but current sh_mobile_lcdc driver doesn't control it.

		 * It is temporary correspondence

 enable TouchScreen */

 enable CEU0 */

 enable CEU1 */

 enable KEYSC */

 enable user debug switch */

 SD-card slot CN11 */

 enable SDHI0 on CN11 (needs DS2.4 set to ON) */

 enable MSIOF0 on CN11 (needs DS2.4 set to OFF) */

 3.3V power control */

 disable power by default */

 MMC/SD-card slot CN12 */

 enable MMCIF (needs DS2.6,7 set to OFF,ON) */

 enable SDHI1 on CN12 (needs DS2.6,7 set to ON,OFF) */

 I/O buffer drive ability is high for CN12 */

 enable FSI */

 set SPU2 clock to 83.4 MHz */

 change parent of FSI B */

 48kHz dummy clock was used to make sure 1/1 divide */

 enable motion sensor */

 set VPU clock to 166 MHz */

 enable IrDA */

 Register gpio lookup tables for cameras and video decoder */

 enable I2C device */

 VOU */

 AK8813 power / reset sequence */

 Reset */

 Power down */

 Power up, reset */

 Remove reset */

 Initialize CEU platform devices separately to map memory first */

 Reserve a portion of memory for CEU 0 and CEU 1 buffers */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/landisk/psw.c

 *

 * push switch support for LANDISK and USL-5P

 *

 * Copyright (C) 2006-2007  Paul Mundt

 * Copyright (C) 2007  kogiidena

 Nothing to do if there's no state change */

 Figure out who raised it */

 Clear the switch IRQs */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/landisk/setup.c

 *

 * I-O DATA Device, Inc. LANDISK Support.

 *

 * Copyright (C) 2000 Kazumoto Kojima

 * Copyright (C) 2002 Paul Mundt

 * Copylight (C) 2002 Atom Create Engineering Co., Ltd.

 * Copyright (C) 2005-2007 kogiidena

 open I/O area window */

 IDE cmd address : 0x1f0-0x1f7 and 0x3f6 */

 I/O port identity mapping */

 LED ON */

/*

 * The Machine Vector

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/landisk/gio.c - driver for landisk

 *

 * This driver will also support the I-O DATA Device, Inc. LANDISK Board.

 * LANDISK and USL-5P Button, LED and GIO driver drive function.

 *

 *   Copylight (C) 2006 kogiidena

 *   Copylight (C) 2002 Atom Create Engineering Co., Ltd. *

 GIO minor no. */

 write */

 address set */

 write byte */

 write word */

 write long */

 read byte */

 read word */

 read long */

 read */

 open */

 release */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boards/mach-landisk/irq.c

 *

 * I-O DATA Device, Inc. LANDISK Support

 *

 * Copyright (C) 2005-2007 kogiidena

 * Copyright (C) 2011 Nobuhiro Iwamatsu

 *

 * Copyright (C) 2001  Ian da Silva, Jeremy Siegel

 * Based largely on io_se.c.

 PCI int A */

 PCI int B */

 PCI int C */

 PCI int D */

 ATA */

 CF */

 Power switch */

 Button switch */

 Vectors for LANDISK */

 IRLMSK mask register layout for LANDISK */

 IRLMSK */

/*

 * Initialize IRQ setting

/*

 * sh7724 MMCIF loader

 *

 * Copyright (C) 2010 Magnus Damm

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/* SH7724 specific MMCIF loader

 *

 * loads the romImage from an MMC card starting from block 512

 * use the following line to write the romImage to an MMC card

 * # dd if=arch/sh/boot/romImage of=/dev/sdx bs=512 seek=512

 enable clock to the MMCIF hardware block */

 setup pins D7-D0 */

 setup pins MMC_CLK, MMC_CMD */

 select D3-D0 pin function */

 select D7-D4 pin function */

 disable Hi-Z for the MMC pins */

 high drive capability for MMC pins */

 setup MMCIF hardware */

 load kernel via MMCIF interface */

 disable clock to the MMCIF hardware block */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/sh/boot/compressed/misc.c

 *

 * This is a collection of several routines from gzip-1.0.3

 * adapted for Linux.

 *

 * malloc by Hannu Savolainen 1993 and Matthias Urlichs 1994

 *

 * Adapted for SH by Stuart Menefy, Aug 1999

 *

 * Modified to use standard LinuxSH BIOS by Greg Banks 7Jul2000

/*

 * gzip declarations

 cache.c */

 Defined in vmlinux.lds.S */

 This should be updated to use the sh-sci routines */

 Halt */

 Needed because vmlinux.lds.h references this */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Xtensa KASAN shadow map initialization

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2017 Cadence Design Systems Inc.

	/*

	 * Replace shadow map pages that cover addresses from VMALLOC area

	 * start to the end of KSEG with clean writable pages.

	/*

	 * Write protect kasan_early_shadow_page and zero-initialize it again.

 At this point kasan is fully initialized. Enable error messages. */

/*

 * High memory support for Xtensa architecture

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License.  See the file "COPYING" in the main directory of

 * this archive for more details.

 *

 * Copyright (C) 2014 Cadence Design Systems Inc.

	/*

	 * The fixmap operates top down, so the color offset needs to be

	 * reverse as well.

	/* Check if this memory layout is broken because PKMAP overlaps

	 * page table.

/*

 * arch/xtensa/mm/tlb.c

 *

 * Logic that manipulates the Xtensa MMU.  Derived from MIPS.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001 - 2003 Tensilica Inc.

 *

 * Joe Taylor

 * Chris Zankel	<chris@zankel.net>

 * Marc Gauthier

/* If mm is current, we simply assign the current task a new ASID, thus,

 * invalidating all previous tlb entries. If mm is someone else's user mapping,

 * wie invalidate the context, thus, when that user mapping is swapped in,

 * a new context will be assigned to it.

/*

 * Check that TLB entries with kernel ASID (1) have kernel VMA (>= TASK_SIZE),

 * and TLB entries with user ASID (>=4) have VMA < TASK_SIZE.

 *

 * Check that valid TLB entries either have the same PA as the PTE, or PTE is

 * marked as non-present. Non-present PTE and the page with non-zero refcount

 * and zero mapcount is normal for batched TLB flush operation. Zero refcount

 * means that the page was freed prematurely. Non-zero mapcount is unusual,

 * but does not necessary means an error, thus marked as suspicious.

 CONFIG_DEBUG_TLB_SANITY */

/*

 * arch/xtensa/mm/init.c

 *

 * Derived from MIPS, PPC.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001 - 2005 Tensilica Inc.

 * Copyright (C) 2014 - 2016 Cadence Design Systems Inc.

 *

 * Chris Zankel	<chris@zankel.net>

 * Joe Taylor	<joe@tensilica.com, joetylr@yahoo.com>

 * Marc Gauthier

 * Kevin Chea

/*

 * Initialize the bootmem system and give it all low memory we have available.

	/* Reserve all memory below PHYS_OFFSET, as memory

	 * accounting doesn't work for pages below that address.

	 *

	 * If PHYS_OFFSET is zero reserve page at address 0:

	 * successfull allocations should never return NULL.

 All pages are DMA-able, so we put them all in the DMA zone. */

 set highmem page free */

 Ignore complete lowmem entries */

 Truncate partial highmem entries */

/*

 * Initialize memory pages.

/*

 * arch/xtensa/mm/cache.c

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001-2006 Tensilica Inc.

 *

 * Chris Zankel	<chris@zankel.net>

 * Joe Taylor

 * Marc Gauthier

 *

/* 

 * Note:

 * The kernel provides one architecture bit PG_arch_1 in the page flags that 

 * can be used for cache coherency.

 *

 * I$-D$ coherency.

 *

 * The Xtensa architecture doesn't keep the instruction cache coherent with

 * the data cache. We use the architecture bit to indicate if the caches

 * are coherent. The kernel clears this bit whenever a page is added to the

 * page cache. At that time, the caches might not be in sync. We, therefore,

 * define this flag as 'clean' if set.

 *

 * D-cache aliasing.

 *

 * With cache aliasing, we have to always flush the cache when pages are

 * unmapped (see tlb_start_vma(). So, we use this flag to indicate a dirty

 * page.

 * 

 *

 *

/*

 * Any time the kernel writes to a user page cache page, or it is about to

 * read from a page cache page this routine is called.

 *

	/*

	 * If we have a mapping but the page is not mapped to user-space

	 * yet, we simply mark this page dirty and defer flushing the 

	 * caches until update_mmu().

		/* 

		 * Flush the page in kernel space and user space.

		 * Note that we can omit that step if aliasing is not

		 * an issue, but we do have to synchronize I$ and D$

		 * if we have a mapping.

 There shouldn't be an entry in the cache for this page anymore. */

/*

 * For now, flush the whole cache. FIXME??

/* 

 * Remove any entry in the cache for this page. 

 *

 * Note that this function is only called for user pages, so use the

 * alias versions of the cache flush functions.

 Note that we have to use the 'alias' address to avoid multi-hit */

 DCACHE_WAY_SIZE > PAGE_SIZE */

 Invalidate old entry in TLBs */

/*

 * access_process_vm() has called get_user_pages(), which has done a

 * flush_dcache_page() on the page.

 Flush and invalidate user page if aliased. */

 Copy data */

	/*

	 * Flush and invalidate kernel page if aliased and synchronize 

	 * data and instruction caches for executable pages. 

	/*

	 * Flush user page if aliased. 

	 * (Note: a simply flush would be sufficient) 

 TODO VM_EXEC flag work-around, cache aliasing

/*

 * arch/xtensa/mm/fault.c

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001 - 2010 Tensilica Inc.

 *

 * Chris Zankel <chris@zankel.net>

 * Joe Taylor	<joe@tensilica.com, joetylr@yahoo.com>

/*

 * This routine handles page faults.  It determines the address,

 * and the problem, and then passes it off to one of the appropriate

 * routines.

 *

 * Note: does not handle Miss and MultiHit.

	/* We fault-in kernel-space virtual memory on-demand. The

	 * 'reference' page table is init_mm.pgd.

	/* If we're in an interrupt or have no user

	 * context, we must not take the fault..

	/* Ok, we have a good vm_area for this memory access, so

	 * we can handle it..

 Allow read even from write-only pages. */

	/* If for any reason at all we couldn't handle the fault,

	 * make sure we exit gracefully rather than endlessly redo

	 * the fault.

			 /* No need to mmap_read_unlock(mm) as we would

			 * have already released it in __lock_page_or_retry

			 * in mm/filemap.c.

	/* Something tried to access memory that isn't in our memory map..

	 * Fix it, but check if it's kernel or user first..

	/* We ran out of memory, or some other thing happened to us that made

	 * us unable to handle the page fault gracefully.

	/* Send a sigbus, regardless of whether we were in kernel

	 * or user mode.

 Kernel mode? Handle exceptions or die */

		/* Synchronize this task's top level page-table

		 * with the 'reference' page table.

 Are we prepared to handle this kernel fault?  */

	/* Oops. The kernel tried to access some bad page. We'll have to

	 * terminate things with extreme prejudice.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ioremap implementation.

 *

 * Copyright (C) 2015 Cadence Design Systems Inc.

 SPDX-License-Identifier: GPL-2.0

/*

 * xtensa mmu stuff

 *

 * Extracted from init.c

/*

 * Flush the mmu and reset associated register to default values.

	/*

	 * Writing zeros to the instruction and data TLBCFG special

	 * registers ensure that valid values exist in the register.

	 *

	 * For existing PGSZID<w> fields, zero selects the first element

	 * of the page-size array.  For nonexistent PGSZID<w> fields,

	 * zero is the best value to write.  Also, when changing PGSZID<w>

	 * fields, the corresponding TLB must be flushed.

 Set rasid register to a known value. */

	/* Set PTEVADDR special register to the start of the page

	 * table, which is in kernel mappable space (ie. not

	 * statically mapped).  This register's value is undefined on

	 * reset.

	/*

	 * Update the IO area mapping in case xtensa_kio_paddr has changed

/*

 * arch/xtensa/kernel/process.c

 *

 * Xtensa Processor version.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001 - 2005 Tensilica Inc.

 *

 * Joe Taylor <joe@tensilica.com, joetylr@yahoo.com>

 * Chris Zankel <chris@zankel.net>

 * Marc Gauthier <marc@tensilica.com, marc@alumni.uwaterloo.ca>

 * Kevin Chea

 Make sure we don't switch tasks during this operation. */

 Walk through all cp owners and release it for the requested one. */

/*

 * Powermanagement idle function, if any is provided by the platform.

/*

 * This is called when the thread calls exit().

/*

 * Flush thread state. This is called when a thread does an execve()

 * Note that we flush coprocessor registers for the case execve fails.

/*

 * this gets called so that we can store coprocessor state into memory and

 * copy the current task into the new thread.

/*

 * Copy thread.

 *

 * There are two modes in which this function is called:

 * 1) Userspace thread creation,

 *    regs != NULL, usp_thread_fn is userspace stack pointer.

 *    It is expected to copy parent regs (in case CLONE_VM is not set

 *    in the clone_flags) and set up passed usp in the childregs.

 * 2) Kernel thread creation,

 *    regs == NULL, usp_thread_fn is the function to run in the new thread

 *    and thread_fn_arg is its parameter.

 *    childregs are not used for the kernel threads.

 *

 * The stack layout for the new thread looks like this:

 *

 *	+------------------------+

 *	|       childregs        |

 *	+------------------------+ <- thread.sp = sp in dummy-frame

 *	|      dummy-frame       |    (saved in dummy-frame spill-area)

 *	+------------------------+

 *

 * We create a dummy frame to return to either ret_from_fork or

 *   ret_from_kernel_thread:

 *   a0 points to ret_from_fork/ret_from_kernel_thread (simulating a call4)

 *   sp points to itself (thread.sp)

 *   a2, a3 are unused for userspace threads,

 *   a2 points to thread_fn, a3 holds thread_fn arg for kernel threads.

 *

 * Note: This is a pristine frame, so we don't need any spill region on top of

 *       childregs.

 *

 * The fun part:  if we're keeping the same VM (i.e. cloning a thread,

 * not an entire process), we're normally given a new usp, and we CANNOT share

 * any live address register windows.  If we just copy those live frames over,

 * the two threads (parent and child) will overflow the same frames onto the

 * parent stack at different times, likely corrupting the parent stack (esp.

 * if the parent returns from functions that called clone() and calls new

 * ones, before the child overflows its now old copies of its parent windows).

 * One solution is to spill windows to the parent stack, but that's fairly

 * involved.  Much simpler to just not copy those live frames across.

 Create a call4 dummy-frame: a0 = 0, a1 = childregs. */

 Reserve 16 bytes for the _switch_to stack frame. */

		/* This does not copy all the regs.

		 * In a bout of brilliance or madness,

		 * ARs beyond a0-a15 exist past the end of the struct.

		/* When sharing memory with the parent thread, the child

		   usually starts on a pristine stack, so we have to reset

		   windowbase, windowstart and wmask.

		   (Note that such a new thread is required to always create

		   an initial call4 frame)

		   The exception is vfork, where the new thread continues to

		   run on the parent's stack until it calls execve. This could

		   be a call8 or call12, which requires a legal stack frame

		   of the previous caller for the overflow handlers to work.

		   (Note that it's always legal to overflow live registers).

		   In this case, ensure to spill at least the stack pointer

 check that caller window is live and same stack */

 pass parameters to ret_from_kernel_thread: */

		/*

		 * a2 = thread_fn, a3 = thread_fn arg.

		 * Window underflow will load registers from the

		 * spill slots on the stack on return from _switch_to.

		/*

		 * a12 = thread_fn, a13 = thread_fn arg.

		 * _switch_to epilogue will load registers from the stack.

		/* Childregs are only used when we're going to userspace

		 * in which case start_thread will set them up.

/*

 * These bracket the sleeping functions..

 Stack layout: sp-4: ra, sp-3: sp' */

/*

 * arch/xtensa/kernel/syscall.c

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001 - 2005 Tensilica Inc.

 * Copyright (C) 2000 Silicon Graphics, Inc.

 * Copyright (C) 1995 - 2000 by Ralf Baechle

 *

 * Joe Taylor <joe@tensilica.com, joetylr@yahoo.com>

 * Marc Gauthier <marc@tensilica.com, marc@alumni.uwaterloo.ca>

 * Chris Zankel <chris@zankel.net>

 * Kevin Chea

 *

 FIXME __cacheline_aligned */= {

		/* We do not accept a shared mapping if it would violate

		 * cache aliasing constraints.

 At this point:  (!vmm || addr < vmm->vm_end). */

/*

 * Kernel and userspace stack tracing.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001 - 2013 Tensilica Inc.

 * Copyright (C) 2015 Cadence Design Systems Inc.

/* Address of common_exception_return, used to check the

 * transition from kernel to user space.

	/* Two steps:

	 *

	 * 1. Look through the register window for the

	 * previous PCs in the call trace.

	 *

	 * 2. Look on the stack.

 Step 1.  */

	/* Rotate WINDOWSTART to move the bit corresponding to

	 * the current window to the bit #0.

	/* Look for bits that are set, they correspond to

	 * valid windows.

 Get the PC from a0 and a1. */

			/* Read a0 and a1 from the

			 * corresponding position in AREGs.

 Step 2. */

	/* We are done with the register window, we need to

	 * look through the stack.

 Start from the a1 register. */

 a1 = regs->areg[1]; */

 Check if the region is OK to access. */

 Copy a1, a0 from user space stack frame. */

 Spill the register window to the stack first. */

	/* Read the stack frames one by one and create the PC

	 * from the a0 and a1 registers saved there.

/*

 * level == 0 is for the return address from the caller of this function,

 * not from this function itself.

/*

 * arch/xtensa/kernel/traps.c

 *

 * Exception handling.

 *

 * Derived from code with the following copyrights:

 * Copyright (C) 1994 - 1999 by Ralf Baechle

 * Modified for R3000 by Paul M. Antoine, 1995, 1996

 * Complete output from die() by Ulf Carlsson, 1998

 * Copyright (C) 1999 Silicon Graphics, Inc.

 *

 * Essentially rewritten for the Xtensa architecture port.

 *

 * Copyright (C) 2001 - 2013 Tensilica Inc.

 *

 * Joe Taylor	<joe@tensilica.com, joetylr@yahoo.com>

 * Chris Zankel	<chris@zankel.net>

 * Marc Gauthier<marc@tensilica.com, marc@alumni.uwaterloo.ca>

 * Kevin Chea

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * Machine specific interrupt handlers

/*

 * The vector table must be preceded by a save area (which

 * implies it must be in RAM, unless one places RAM immediately

 * before a ROM and puts the vector at the start of the ROM (!))

 EXCCAUSE_INSTRUCTION_FETCH unhandled */

 EXCCAUSE_LOAD_STORE_ERROR unhandled*/

 EXCCAUSE_INTEGER_DIVIDE_BY_ZERO unhandled */

 EXCCAUSE_PRIVILEGED unhandled */

 EXCCAUSE_SIZE_RESTRICTION unhandled */

 EXCCAUSE_DTLB_SIZE_RESTRICTION unhandled */

 CONFIG_MMU */

 XCCHAL_EXCCAUSE_FLOATING_POINT unhandled */

/* The exception table <exc_table> serves two functions:

 * 1. it contains three dispatch tables (fast_user, fast_kernel, default-c)

 * 2. it is a temporary memory buffer for the exception handlers.

/*

 * Unhandled Exceptions. Kill user task or panic if in kernel space.

 If in user mode, send SIGILL signal to current process */

/*

 * Multi-hit exception. This if fatal!

/*

 * IRQ handler.

 clear lowest pending irq in the unhandled mask */

/*

 * Illegal instruction. Fatal if in kernel space.

 If in user mode, send SIGILL signal to current process. */

/*

 * Handle unaligned memory accesses from user space. Kill task.

 *

 * If CONFIG_UNALIGNED_USER is not set, we don't allow unaligned memory

 * accesses causes from user space.

/* Handle debug events.

 * When CONFIG_HAVE_HW_BREAKPOINT is on this handler is called with

 * preemption disabled to avoid rescheduling and keep mapping of hardware

 * breakpoint structures to debug registers intact, so that

 * DEBUGCAUSE.DBNUM could be used in case of data breakpoint hit.

 If in user mode, send SIGTRAP signal to current process */

 Set exception C handler - for temporary use when probing exceptions */

/*

 * Initialize dispatch tables.

 *

 * The exception vectors are stored compressed the __init section in the

 * dispatch_init_table. This function initializes the following three tables

 * from that compressed table:

 * - fast user		first dispatch table for user exceptions

 * - fast kernel	first dispatch table for kernel exceptions

 * - default C-handler	C-handler called by the default fast handler.

 *

 * See vectors.S for more details.

 Setup default vectors. */

 Setup specific handlers. */

 Initialize EXCSAVE_1 to hold the address of the exception table. */

/*

 * This function dumps the current valid window frame and other base registers.

/*

 * arch/xtensa/kernel/setup.c

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1995  Linus Torvalds

 * Copyright (C) 2001 - 2005  Tensilica Inc.

 * Copyright (C) 2014 - 2016  Cadence Design Systems Inc.

 *

 * Chris Zankel	<chris@zankel.net>

 * Joe Taylor	<joe@tensilica.com, joetylr@yahoo.com>

 * Kevin Chea

 * Marc Gauthier<marc@tensilica.com> <marc@alumni.uwaterloo.ca>

 Command line specified as configuration option. */

/*

 * Boot parameter parsing.

 *

 * The Xtensa port uses a list of variable-sized tags to pass data to

 * the kernel. The first tag must be a BP_TAG_FIRST tag for the list

 * to be recognised. The list is terminated with a zero-sized

 * BP_TAG_LAST tag.

 parse current tag */

 CONFIG_BLK_DEV_INITRD */

 CONFIG_USE_OF */

 Boot parameters must start with a BP_TAG_FIRST tag. */

 Parse all tags. */

 round down to nearest 256MB boundary */

 CONFIG_USE_OF */

/*

 * Initialize architecture. (Early stage)

 Initialize MMU. */

 Initialize initial KASAN shadow map */

 Parse boot parameters */

 Early hook for platforms */

/*

 * Initialize system. Setup memory and reserve regions.

 Reserve some memory regions */

 CONFIG_VECTORS_ADDR */

	/*

	 * We have full MMU: all autoload ways, ways 7, 8 and 9 of DTLB must

	 * be flushed.

	 * Way 4 is not currently used by linux.

	 * Ways 5 and 6 shall not be touched on MMUv2 as they are hardwired.

	 * Way 5 shall be flushed and way 6 shall be set to identity mapping

	 * on MMUv3.

 MMU v3 */

		/*

		 * Find a place for the temporary mapping. It must not be

		 * in the same 512MB region with vaddr or paddr, otherwise

		 * there may be multihit exception either on entry to the

		 * temporary mapping, or on entry to the identity mapping.

		 * (512MB is the biggest page size supported by TLB.)

 Invalidate mapping in the selected temporary area */

		/*

		 * Map two consecutive pages starting at the physical address

		 * of this function to the temporary mapping area.

 Reinitialize TLB */

				      /*

				       * No literal, data or stack access

				       * below this point

 Initialize *tlbcfg */

 Invalidate TLB way 5 */

 Initialize TLB way 6 */

 Jump to identity mapping */

 Complete way 6 initialization */

 Invalidate temporary mapping */

/*

 * Display some core information through /proc/cpuinfo.

 high-level stuff */

 Registers. */

 Interrupt. */

 Cache */

/*

 * We show only CPU #0 info.

 CONFIG_PROC_FS */

/*

 * arch/xtensa/kernel/time.c

 *

 * Timer and clock support.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2005 Tensilica Inc.

 *

 * Chris Zankel <chris@zankel.net>

 ccount Hz */

/*

 * There is no way to disable the timer interrupt at the device level,

 * only at the intenable register itself. Since enable_irq/disable_irq

 * calls are nested, we need to make sure that these calls are

 * balanced.

 Allow platform to do something useful (Wdog). */

/*

 * Xtensa hardware breakpoints/watchpoints handling functions

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2016 Cadence Design Systems Inc.

 Breakpoint currently in use for each IBREAKA. */

 Watchpoint currently in use for each DBREAKA. */

/*

 * Construct an arch_hw_breakpoint from a perf_event.

 Type */

 Len */

 Address */

	/* We don't have indexed wsr and creating instruction dynamically

	 * doesn't seem worth it given how small XCHAL_NUM_IBREAK and

	 * XCHAL_NUM_DBREAK are. Thus the switch. In case build breaks here

	 * the switch below needs to be extended.

 Breakpoint */

 Watchpoint */

 Breakpoint */

 Watchpoint */

/*

 * Set ptrace breakpoint pointers to zero for this task.

 * This is required in order to prevent child processes from unregistering

 * breakpoints held by their parent.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Cadence Design Systems Inc.

 Jump only works within 128K of the J instruction. */

/*

 * S32C1I selftest.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2016 Cadence Design Systems Inc.

/*

 * Basic atomic compare-and-swap, that records PC of S32C1I for probing.

 *

 * If *v == cmp, set *v = set.  Return previous *v.

 Handle probed exception */

 exception on s32c1i ? */

 skip the s32c1i instruction */

 Simple test of S32C1I (soc bringup assist) */

 temporarily saved handlers */

 First try an S32C1I that does not store: */

 took exception? */

 unclean exception? */

 Then an S32C1I that stores: */

 unclean exception? */

 Verify consistency of exceptions: */

		/* If emulation of S32C1I upon bus error gets implemented,

		 * we can get rid of this panic for single core (not SMP)

 XCHAL_HAVE_S32C1I */

/* This condition should not occur with a commercially deployed processor.

 * Display reminder for early engr test or demo chips / FPGA bitstreams

 XCHAL_HAVE_S32C1I */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * DMA coherent memory allocation.

 *

 * Copyright (C) 2002 - 2005 Tensilica Inc.

 * Copyright (C) 2015 Cadence Design Systems Inc.

 *

 * Based on version for i386.

 *

 * Chris Zankel <chris@zankel.net>

 * Joe Taylor <joe@tensilica.com, joetylr@yahoo.com>

/*

 * Memory caching is platform-dependent in noMMU xtensa configurations.

 * This function should be implemented in platform code in order to enable

 * coherent DMA memory operations when CONFIG_MMU is not enabled.

 CONFIG_MMU */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * arch/xtensa/kernel/pci.c

 *

 * PCI bios-type initialisation for PCI machines

 *

 * Copyright (C) 2001-2005 Tensilica Inc.

 *

 * Based largely on work from Cort (ppc/kernel/pci.c)

 * IO functions copied from sparc.

 *

 * Chris Zankel <chris@zankel.net>

/*

 * We need to avoid collisions with `mirrored' VGA ports

 * and other strange ISA hardware, so we always want the

 * addresses to be allocated in the 0x000-0x0ff region

 * modulo 0x400.

 *

 * Why? Because some silly external IO cards only decode

 * the low 10 bits of the IO address. The 0x00-0xff region

 * is reserved for motherboard devices that decode all 16

 * bits, so it's ok to allocate at, say, 0x2800-0x28ff,

 * but we want to try to avoid allocating at 0x2900-0x2bff

 * which might have be mirrored at 0x0100-0x03ff..

 This is a subordinate bridge */

/*

 * Platform support for /proc/bus/pci/X/Y mmap()s.

 *  -- paulus.

 should never happen */

 Convert to an offset within this PCI controller */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Xtensa Performance Monitor Module driver

 * See Tensilica Debug User's Guide for PMU registers documentation.

 *

 * Copyright (C) 2015 Cadence Design Systems Inc.

 Global control/status for all perf counters */

 Perf counter values */

 Perf counter control registers */

 Perf counter status registers */

 Array of events currently on this core */

 Bitmap of used hardware counters */

 Taken and non-taken branches + taken loop ends */

 Instruction-related + other global stall cycles */

 Data-related global stall cycles */

 Not 'previous counter' select */

/*

 * Starts/Stops a counter present on the PMU. The PMI handler

 * should stop the counter when perf_event_overflow() returns

 * !0. ->start() will be used to continue.

/*

 * Adds/Removes a counter to/from the PMU, can be done inside

 * a transaction, see the ->*_txn() methods.

/*

 * Xtensa SMP support functions.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 - 2013 Tensilica Inc.

 *

 * Chris Zankel <chris@zankel.net>

 * Joe Taylor <joe@tensilica.com>

 * Pete Delaney <piet@tensilica.com

 IPI (Inter Process Interrupt) */

 Bits 18..21 of SYSCFGID contain the core count minus 1. */

 Bits 0...18 of SYSCFGID contain the core id  */

 Set with xt-gdb via .xt-gdb */

 Init EXCSAVE1 */

 All kernel threads share the same mm context. */

 Pairs with the third memw in the cpu_restart */

			/*

			 * Pairs with the first two memws in the

			 * .Lboot_secondary.

/*

 * __cpu_disable runs on the processor to be shutdown.

	/*

	 * Take this CPU offline.  Once we clear this, we can't return,

	 * and we must not schedule until we're ready to give up the cpu.

	/*

	 * OK - migrate IRQs away from this CPU

	/*

	 * Flush user cache and TLB mappings, and then remove this CPU

	 * from the vm mask set of all processes.

/*

 * called on the thread which is asking for a CPU to be shutdown -

 * waits until shutdown has completed, or it is timed out.

 Pairs with the second memw in the cpu_restart */

/*

 * Called from the idle thread for the CPU which has been shutdown.

 *

 * Note that we disable IRQs here, but do not re-enable them

 * before returning to the caller. This is also the behaviour

 * of the other hotplug-cpu capable cores, so presumably coming

 * out of idle fixes this.

 CONFIG_HOTPLUG_CPU */

 TLB flush functions */

 Cache flush functions */

 ------------------------------------------------------------------------- */

/*

 * arch/xtensa/kernel/signal.c

 *

 * Default platform functions.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2005, 2006 Tensilica Inc.

 * Copyright (C) 1991, 1992  Linus Torvalds

 * 1997-11-28  Modified for POSIX.1b signals by Richard Henderson

 *

 * Chris Zankel <chris@zankel.net>

 * Joe Taylor <joe@tensilica.com>

/*

 * Flush register windows stored in pt_regs to stack.

 * Returns 1 for errors.

 Return if no other frames. */

 Rotate windowmask and skip empty frames. */

 For call8 or call12 frames, we need the previous stack pointer. */

 Spill frames to stack. */

 Save registers a4..a7 (call8) or a4...a11 (call12) */

 call4 */

 call8 */

 call12 */

 Save current frame a0..a3 under next SP */

 Get current stack pointer for next loop iteration. */

/*

 * Note: We don't copy double exception 'regs', we have to finish double exc. 

 * first before we return to signal handler! This dbl.exc.handler might cause 

 * another double exception, but I think we are fine as the situation is the 

 * same as if we had returned to the signal handerl and got an interrupt 

 * immediately...

 All registers were flushed to stack. Start with a pristine frame. */

 disable syscall checks */

	/* For PS, restore only PS.CALLINC.

	 * Assume that all other bits are either the same as for the signal

	 * handler, or the user mode value doesn't matter (e.g. PS.OWB).

 Additional corruption checks */

	/* The signal handler may have used coprocessors in which

	 * case they are still enabled.  We disable them to force a

	 * reloading of the original task's CP state by the lazy

	 * context-switching mechanisms of CP exception handling.

	 * Also, we essentially discard any coprocessor state that the

/*

 * Do a signal return; undo the signal stack.

 Always make any pending restarted system calls return -EINTR */

/*

 * Set up a signal frame.

	/*

	 * The 12-bit immediate is really split up within the 24-bit MOVI

	 * instruction.  As long as the above system call numbers fit within

	 * 8-bits, the following code works fine. See the Xtensa ISA for

	 * details.

 Big Endian version */

 Generate instruction:  MOVI a2, __NR_rt_sigreturn */

 Generate instruction:  SYSCALL */

 Little Endian version */

 Generate instruction:  MOVI a2, __NR_rt_sigreturn */

 Generate instruction:  SYSCALL */

 Flush generated code out of the data cache */

 Create the user context.  */

 Create sys_rt_sigreturn syscall in stack frame */

	/* 

	 * Create signal handler execution context.

	 * Return context not modified until this point.

 Set up registers for signal handler; preserve the threadptr */

 Set up a stack frame for a call4 if userspace uses windowed ABI */

/*

 * Note that 'init' is a special process: it doesn't get signals it doesn't

 * want to handle. Thus you cannot kill init even with a SIGKILL even by

 * mistake.

 *

 * Note that we go through the signals twice: once to check the signals that

 * the kernel can handle, and then we build all the user-level signal handling

 * stack-frames in one go after that.

 Are we from a system call? */

 If so, check system call restarting.. */

 nothing to do */

 Whee!  Actually deliver the signal.  */

 Set up the stack frame */

 Did we come from a system call? */

 Restart the system call - no handlers present */

 If there's no signal to deliver, we just restore the saved mask.  */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001 - 2007  Tensilica Inc.

 *

 * Joe Taylor	<joe@tensilica.com, joetylr@yahoo.com>

 * Chris Zankel <chris@zankel.net>

 * Scott Foehner<sfoehner@yahoo.com>,

 * Kevin Chea

 * Marc Gauthier<marc@tensilica.com> <marc@alumni.uwaterloo.ca>

 Flush all coprocessor registers to memory. */

 Flush all coprocessors before we overwrite them. */

/*

 * Called by kernel/ptrace.c when detaching to disable single stepping.

 Nothing to do.. */

 Default return value. */

		/* Note: PS.EXCM is not set while user task is running;

		 * its being set in regs is for exception handling

		 * convenience.

 tmp = 0 */

 Initialise fields to sane defaults. */

/*

 * Address bit 0 choose instruction (0) or data (1) break register, bits

 * 31..1 are the register number.

 * Both PTRACE_GETHBPREGS and PTRACE_SETHBPREGS transfer two 32-bit words:

 * address (0) and control (1).

 * Instruction breakpoint contorl word is 0 to clear breakpoint, 1 to set.

 * Data breakpoint control word bit 31 is 'trigger on store', bit 30 is

 * 'trigger on load, bits 29..0 are length. Length 0 is used to clear a

 * breakpoint. To set a breakpoint length must be a power of 2 in the range

 * 1..64 and the address must be length-aligned.

 read register specified by addr. */

 write register specified by addr. */

/*

 * arch/xtensa/kernel/platform.c

 *

 * Default platform functions.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2005 Tensilica Inc.

 *

 * Chris Zankel <chris@zankel.net>

/*

 * Default functions that are used if no platform specific function is defined.

 * (Please, refer to include/asm-xtensa/platform.h for more information)

/*

 * arch/xtensa/kernel/module.c

 *

 * Module support.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001 - 2006 Tensilica Inc.

 *

 * Chris Zankel <chris@zankel.net>

 *

			/* FIXME: Ignore any other opcodes.  The Xtensa

			   assembler currently assumes that the linker will

			   always do relaxation and so all PC-relative

			   operands need relocations.  (The assembler also

			   writes out the tentative PC-relative values,

			   assuming no link-time relaxation, so it is usually

			   safe to ignore the relocations.)  If the

			   assembler's "--no-link-relax" flag can be made to

			   work, and if all kernel modules can be assembled

			   with that flag, then unexpected relocations could

/*

 * arch/xtensa/kernel/xtensa_ksyms.c

 *

 * Export Xtensa-specific functions for loadable modules.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001 - 2005  Tensilica Inc.

 *

 * Joe Taylor <joe@tensilica.com>

 CONFIG_NET */

/*

 * String functions

/*

 * gcc internal math functions

/*

 * Networking support

/*

 * Architecture-specific symbols

/*

 * Kernel hacking ...

 FIXME EXPORT_SYMBOL(screen_info);

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/xtensa/kernel/irq.c

 *

 * Xtensa built-in interrupt controller and some generic functions copied

 * from i386.

 *

 * Copyright (C) 2002 - 2013 Tensilica, Inc.

 * Copyright (C) 1992, 1998 Linus Torvalds, Ingo Molnar

 *

 *

 * Chris Zankel <chris@zankel.net>

 * Kevin Chea

 *

 Debugging check for stack overflow: is there less than 1KB free? */

 XCHAL_INTTYPE_MASK_WRITE_ERROR */

 XCHAL_INTTYPE_MASK_NMI */

/*

 * The CPU has been marked offline.  Migrate IRQs off this CPU.  If

 * the affinity settings do not allow other CPUs, force them onto any

 * available CPU.

 CONFIG_HOTPLUG_CPU */

/*

 * arch/xtensa/kernel/asm-offsets.c

 *

 * Generates definitions from c-type structures used by assembly sources.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2005 Tensilica Inc.

 *

 * Chris Zankel <chris@zankel.net>

 struct pt_regs */

 struct task_struct */

 offsets in thread_info struct */

 struct thread_info (offset from start_struct) */

 struct mm_struct */

 struct page */

 constants */

 struct debug_table */

 struct exc_table */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * arch/xtensa/lib/pci-auto.c

 *

 * PCI autoconfiguration library

 *

 * Copyright (C) 2001 - 2005 Tensilica Inc.

 *

 * Chris Zankel <zankel@tensilica.com, cez@zankel.net>

 *

 * Based on work from Matt Porter <mporter@mvista.com>

/*

 *

 * Setting up a PCI

 *

 * pci_ctrl->first_busno = <first bus number (0)>

 * pci_ctrl->last_busno = <last bus number (0xff)>

 * pci_ctrl->ops = <PCI config operations>

 * pci_ctrl->map_irq = <function to return the interrupt number for a device>

 *

 * pci_ctrl->io_space.start = <IO space start address (PCI view)>

 * pci_ctrl->io_space.end = <IO space end address (PCI view)>

 * pci_ctrl->io_space.base = <IO space offset: address 0 from CPU space>

 * pci_ctrl->mem_space.start = <MEM space start address (PCI view)>

 * pci_ctrl->mem_space.end = <MEM space end address (PCI view)>

 * pci_ctrl->mem_space.base = <MEM space offset: address 0 from CPU space>

 *

 * pcibios_init_resource(&pci_ctrl->io_resource, <IO space start>,

 * 			 <IO space end>, IORESOURCE_IO, "PCI host bridge");

 * pcibios_init_resource(&pci_ctrl->mem_resources[0], <MEM space start>,

 * 			 <MEM space end>, IORESOURCE_MEM, "PCI host bridge");

 *

 * pci_ctrl->last_busno = pciauto_bus_scan(pci_ctrl,pci_ctrl->first_busno);

 *

 * int __init pciauto_bus_scan(struct pci_controller *pci_ctrl, int current_bus)

 *

/*

 * Helper functions

 Initialize the bars of a PCI device.  */

 Tickle the BAR and get the size */

 If BAR is not implemented go to the next BAR */

 Check the BAR type and set our address mask */

 Allocate a base address (bar_size is negative!) */

 Write it out and update our limit */

		/*

		 * If we are a 64-bit decoder then increment to the

		 * upper 32 bits of the bar and force it to locate

		 * in the lower 4GB of memory.

 Initialize the interrupt number. */

 Fix illegal pin numbers. */

 Configure bus number registers */

 Round memory allocator to 1MB boundary */

 Round I/O allocator to 4KB boundary */

 Set up memory and I/O filter limits, assume 32-bit I/O space */

 Configure bus number registers */

	/*

	 * Round memory allocator to 1MB boundary.

	 * If no space used, allocate minimum.

 Allocate 1MB for pre-fretch */

 Round I/O allocator to 4KB boundary */

 Enable memory and I/O accesses, enable bus master */

/*

 * Scan the current PCI bus.

	/*

	 * Fetch our I/O and memory space upper boundaries used

	 * to allocated base addresses on this pci_controller.

 Skip our host bridge */

 If config space read fails from this device, move on */

 Allocate PCI I/O and/or memory space */

		/*

		 * Found a peripheral, enable some standard

		 * settings

 Allocate PCI I/O and/or memory space */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * arch/xtensa/platforms/xt2000/setup.c

 *

 * Platform specific functions for the XT2000 board.

 *

 * Authors:	Chris Zankel <chris@zankel.net>

 *		Joe Taylor <joe@tensilica.com>

 *

 * Copyright 2001 - 2004 Tensilica Inc.

 Assumes s points to an 8-chr string.  No checking for NULL. */

	/* Flush and reset the mmu, simulate a processor reset, and

 control never gets here */

 early initialization */

 Heartbeat. Let the LED blink. */

#define RS_TABLE_SIZE 2

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * arch/xtensa/platform-iss/setup.c

 *

 * Platform specific initialization.

 *

 * Authors: Chris Zankel <chris@zankel.net>

 *          Joe Taylor <joe@tensilica.com>

 *

 * Copyright 2001 - 2005 Tensilica Inc.

 * Copyright 2017 Cadence Design Systems Inc.

	/* Flush and reset the mmu, simulate a processor reset, and

 control never gets here */

/*

 * arch/xtensa/platforms/iss/console.c

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001-2005 Tensilica Inc.

 *   Authors	Christian Zankel, Joe Taylor

 see drivers/char/serialX.c to reference original version */

 Let's say iss can always accept 2K characters.. */

 Stub, once again.. */

 Stub, once again.. */

 Initialize the tty_driver structure */

/* We use `late_initcall' instead of just `__initcall' as a workaround for

 * the fact that (1) simcons_tty_init can't be called before tty_init,

 * (2) tty_init is called via `module_init', (3) if statically linked,

 * module_init == device_init, and (4) there's no ordering of init lists.

 * We can do this easily because simcons is always statically linked, but

 * other tty drivers that depend on tty_init and which must use

 * `module_init' to declare their init routines are likely to be broken.

 CONFIG_SERIAL_CONSOLE */

/*

 * arch/xtensa/platforms/iss/simdisk.c

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001-2013 Tensilica Inc.

 *   Authors	Victor Prupis

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * arch/xtensa/platforms/iss/network.c

 *

 * Platform specific initialization.

 *

 * Authors: Chris Zankel <chris@zankel.net>

 * Based on work form the UML team.

 *

 * Copyright 2005 Tensilica Inc.

 ------------------------------------------------------------------------- */

 We currently only support the TUNTAP transport protocol. */

 ------------------------------------------------------------------------- */

 This structure contains out private information for the driver. */

 ================================ HELPERS ================================ */

 Set Ethernet address of the specified device. */

 ======================= TUNTAP TRANSPORT INTERFACE ====================== */

 O_RDWR */

/*

 * ethX=tuntap,[mac address],device name

 Transport should be 'tuntap': ethX=tuntap,mac,dev_name */

 ================================ ISS NET ================================ */

 Check if there is any new data. */

 Try to allocate memory, if it fails, try again next round. */

 Setup skb */

 FIXME reactivate_fd(lp->fd, ISS_ETH_IRQ); */

	/* clear buffer - it can happen that the host side of the interface

	 * is full when we get here. In this case, new data is never queued,

	 * SIGIOs never arrive, and the net never works.

 this is normally done in the interrupt when tx finishes */

 Initialize private element. */

	/*

	 * If this name ends up conflicting with an existing registered

	 * netdevice, that is OK, register_netdev{,ice}() will notice this

	 * and fail.

	/*

	 * Try all transport protocols.

	 * Note: more protocols can be added by adding '&& !X_init(lp, eth)'.

 sysfs register */

 XXX: should we call ->remove() here? */

 FIXME: unregister; free, etc.. */

 ------------------------------------------------------------------------- */

 Filled in during early boot */

 init string */

/*

 * Parse the command line and look for 'ethX=...' fields, and register all

 * those fields. They will be later initialized in iss_net_init.

/*

 * Initialize all ISS Ethernet devices previously registered in iss_net_setup.

 Walk through all Ethernet devices specified in the command line. */

/*

 * Driver for the LCD display on the Tensilica XTFPGA board family.

 * http://www.mytechcorp.com/cfdata/productFile/File1/MOC-16216B-B-A0A04.pdf

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001, 2006 Tensilica Inc.

 * Copyright (C) 2015 Cadence Design Systems Inc.

 LCD instruction and data addresses. */

 8bit and 2 lines display */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * arch/xtensa/platform/xtavnet/setup.c

 *

 * ...

 *

 * Authors:	Chris Zankel <chris@zankel.net>

 *		Joe Taylor <joe@tensilica.com>

 *

 * Copyright 2001 - 2006 Tensilica Inc.

 Try software reset first. */

	/* If software reset did not work, flush and reset the mmu,

	 * simulate a processor reset, and jump to the reset vector.

 control never gets here */

/*----------------------------------------------------------------------------

 *  Ethernet -- OpenCores Ethernet MAC (ethoc driver)

 register space */

 buffer space */

 IRQ number */

	/*

	 * The MAC address for these boards is 00:50:c2:13:6f:xx.

	 * The last byte (here as zero) is read from the DIP switches on the

	 * board.

/*----------------------------------------------------------------------------

 *  USB Host/Device -- Cypress CY7C67300

 register space */

 IRQ number */

/*----------------------------------------------------------------------------

 *  UART

 set in xtavnet_init() */

 platform devices */

 Ethernet MAC address.  */

	/* Clock rate varies among FPGA bitstreams; board specific FPGA register

	 * reports the actual clock rate.

 register platform devices */

	/* ETHOC driver is a bit quiet; at least display Ethernet MAC, so user

	 * knows whether they set it correctly on the DIP switches.

/*

 * Register to be done during do_initcalls().

 CONFIG_USE_OF */

 SPDX-License-Identifier: GPL-2.0

 bits taken from ppc */

puts("oops... out of memory\n");

pause();

 skip header */

puts("bad gzipped data\n");

puts("gunzip: ran out of data in header\n");

puts("inflateInit2 returned "); puthex(r); puts("\n");

puts("inflate returned "); puthex(r); puts("\n");

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * MPIC timer driver

 *

 * Copyright 2013 Freescale Semiconductor, Inc.

 * Author: Dongsheng Wang <Dongsheng.Wang@freescale.com>

 *	   Li Yang <leoli@freescale.com>

/* Clock Ratio

 * Divide by 64 0x00000300

 * Divide by 32 0x00000200

 * Divide by 16 0x00000100

 * Divide by  8 0x00000000 (Hardware default div)

 TCR register: CASC & ROVR value */

 cascade map */

 cascade control timer */

 cascade timer 0 and 1 */

 cascade timer 1 and 2 */

 cascade timer 2 and 3 */

 the time set by the user is converted to "ticks" */

 prevent u64 overflow */

 detect whether there is a cascade timer available */

 set timer busy */

 set group tcr reg for cascade */

 Two cascade timers: Support the maximum time */

 detect idle timer */

 set ticks to timer */

 one timer: Reverse allocation */

 set timer busy */

 set ticks & stop timer */

/**

 * mpic_start_timer - start hardware timer

 * @handle: the timer to be started.

 *

 * It will do ->fn(->dev) callback from the hardware interrupt at

 * the 'time64_t' point in the future.

/**

 * mpic_stop_timer - stop hardware timer

 * @handle: the timer to be stoped

 *

 * The timer periodically generates an interrupt. Unless user stops the timer.

/**

 * mpic_get_remain_time - get timer time

 * @handle: the timer to be selected.

 * @time: time for timer

 *

 * Query timer remaining time.

/**

 * mpic_free_timer - free hardware timer

 * @handle: the timer to be removed.

 *

 * Free the timer.

 *

 * Note: can not be used in interrupt context.

/**

 * mpic_request_timer - get a hardware timer

 * @fn: interrupt handler function

 * @dev: callback function of the data

 * @time: time for timer

 *

 * This executes the "request_irq", returning NULL

 * else "handle" on success.

 Set timer idle */

 Init FSL timer hardware */

 Init FSL timer hardware */

/*

 *  Driver for ePAPR Embedded Hypervisor PIC

 *

 *  Copyright 2008-2011 Freescale Semiconductor, Inc.

 *

 *  Author: Ashish Kalra <ashish.kalra@freescale.com>

 *

 * This file is licensed under the terms of the GNU General Public License

 * version 2.  This program is licensed "as is" without any warranty of any

 * kind, whether express or implied.

/*

 * Linux descriptor level callbacks

 Now convert sense value */

	/*

	 * TODO : Add specific interface call for platform to set

	 * individual interrupt priorities.

	 * platform currently using static/default priority for all ints

 Return an interrupt vector or 0 if no interrupt is pending. */

 if core int mode */

 legacy mode */

 0xFFFF --> no irq is pending */

	/*

	 * this will also setup revmap[] in the slow path for the first

	 * time, next calls will always use fast path by indexing revmap

 Exact match, unless ehv_pic node is NULL */

 Default chip */

	/*

	 * using handle_fasteoi_irq as our irq handler, this will

	 * only call the eoi callback and suitable for the MPIC

	 * controller which set ISR/IPR automatically and clear the

	 * highest priority active interrupt in ISR/IPR when we do

	 * a specific eoi

 Set default irq type */

	/*

	 * interrupt sense values coming from the guest device tree

	 * interrupt specifiers can have four possible sense and

	 * level encoding information and they need to

	 * be translated between firmware type & linux type.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Freescale MPC85xx/MPC86xx RapidIO RMU support

 *

 * Copyright 2009 Sysgo AG

 * Thomas Moll <thomas.moll@sysgo.com>

 * - fixed maintenance access routines, check for aligned access

 *

 * Copyright 2009 Integrated Device Technology, Inc.

 * Alex Bounine <alexandre.bounine@idt.com>

 * - Added Port-Write message handling

 * - Added Machine Check exception handling

 *

 * Copyright (C) 2007, 2008, 2010, 2011 Freescale Semiconductor, Inc.

 * Zhang Wei <wei.zhang@freescale.com>

 * Lian Minghuan-B31939 <Minghuan.Lian@freescale.com>

 * Liu Gang <Gang.Liu@freescale.com>

 *

 * Copyright 2005 MontaVista Software, Inc.

 * Matt Porter <mporter@kernel.crashing.org>

 RapidIO definition irq, which read from OF-tree */

 EPWISR Error match value */

/**

 * fsl_rio_tx_handler - MPC85xx outbound message interrupt handler

 * @irq: Linux interrupt number

 * @dev_instance: Pointer to interrupt-specific data

 *

 * Handles outbound message interrupts. Executes a register outbound

 * mailbox event handler and acks the interrupt occurrence.

 Ack the end-of-message interrupt */

/**

 * fsl_rio_rx_handler - MPC85xx inbound message interrupt handler

 * @irq: Linux interrupt number

 * @dev_instance: Pointer to interrupt-specific data

 *

 * Handles inbound message interrupts. Executes a registered inbound

 * mailbox event handler and acks the interrupt occurrence.

 XXX Need to check/dispatch until queue empty */

		/*

		* Can receive messages for any mailbox/letter to that

		* mailbox destination. So, make the callback with an

		* unknown/invalid mailbox number argument.

 Ack the queueing interrupt */

/**

 * fsl_rio_dbell_handler - MPC85xx doorbell interrupt handler

 * @irq: Linux interrupt number

 * @dev_instance: Pointer to interrupt-specific data

 *

 * Handles doorbell interrupts. Parses a list of registered

 * doorbell event handlers and executes a matching event handler.

 XXX Need to check/dispatch until queue empty */

XXX: Error recovery is not implemented, we just clear errors */

/**

 * fsl_rio_port_write_handler - MPC85xx port write interrupt handler

 * @irq: Linux interrupt number

 * @dev_instance: Pointer to interrupt-specific data

 *

 * Handles port write interrupts. Parses a list of registered

 * port write event handlers and executes a matching event handler.

 Schedule deferred processing if PW was received */

		/* Save PW message (if there is room in FIFO),

		 * otherwise discard it.

		/* Clear interrupt and issue Clear Queue command. This allows

		 * another port-write to be received.

		/* Clear Transaction Error: port-write controller should be

		 * disabled when clearing this error

	/*

	 * Process port-write messages

 Pass the port-write message to RIO core for processing */

/**

 * fsl_rio_pw_enable - enable/disable port-write interface init

 * @mport: Master port implementing the port write unit

 * @enable:    1=enable; 0=disable port-write message handling

/**

 * fsl_rio_port_write_init - MPC85xx port write interface init

 * @mport: Master port implementing the port write unit

 *

 * Initializes port write unit hardware and DMA buffer

 * ring. Called from fsl_rio_setup(). Returns %0 on success

 * or %-ENOMEM on failure.

 Following configurations require a disabled port write controller */

 Initialize port write */

 Point dequeue/enqueue pointers at first entry */

 Clear interrupt status IPWSR */

	/* Configure port write controller for snooping enable all reporting,

 Hook up port-write handler */

 Enable Error Interrupt */

/**

 * fsl_rio_doorbell_send - Send a MPC85xx doorbell message

 * @mport: RapidIO master port info

 * @index: ID of RapidIO interface

 * @destid: Destination ID of target device

 * @data: 16-bit info field of RapidIO doorbell message

 *

 * Sends a MPC85xx doorbell message. Returns %0 on success or

 * %-EINVAL on failure.

	/* In the serial version silicons, such as MPC8548, MPC8641,

	 * below operations is must be.

/**

 * fsl_add_outb_message - Add message to the MPC85xx outbound message queue

 * @mport: Master port with outbound message queue

 * @rdev: Target of outbound message

 * @mbox: Outbound mailbox

 * @buffer: Message to add to outbound queue

 * @len: Length of message

 *

 * Adds the @buffer message to the MPC85xx outbound message queue. Returns

 * %0 on success or %-EINVAL on failure.

 Copy and clear rest of buffer */

 Set mbox field for message, and set destid */

 Enable EOMI interrupt and priority */

 Set transfer size aligned to next power of 2 (in double words) */

 Set snooping and source buffer address */

 Increment enqueue pointer */

 Go to next descriptor */

/**

 * fsl_open_outb_mbox - Initialize MPC85xx outbound mailbox

 * @mport: Master port implementing the outbound message unit

 * @dev_id: Device specific pointer to pass on event

 * @mbox: Mailbox to open

 * @entries: Number of entries in the outbound mailbox ring

 *

 * Initializes buffer ring, request the outbound message interrupt,

 * and enables the outbound message unit. Returns %0 on success and

 * %-EINVAL or %-ENOMEM on failure.

 Initialize shadow copy ring */

 Initialize outbound message descriptor ring */

 Point dequeue/enqueue pointers at first entry in ring */

 Configure for snooping */

 Clear interrupt status */

 Hook up outbound message handler */

	/*

	 * Configure outbound message unit

	 *      Snooping

	 *      Interrupts (all enabled, except QEIE)

	 *      Chaining mode

	 *      Disable

 Set number of entries */

 Now enable the unit */

/**

 * fsl_close_outb_mbox - Shut down MPC85xx outbound mailbox

 * @mport: Master port implementing the outbound message unit

 * @mbox: Mailbox to close

 *

 * Disables the outbound message unit, free all buffers, and

 * frees the outbound message interrupt.

 Disable inbound message unit */

 Free ring */

 Free interrupt */

/**

 * fsl_open_inb_mbox - Initialize MPC85xx inbound mailbox

 * @mport: Master port implementing the inbound message unit

 * @dev_id: Device specific pointer to pass on event

 * @mbox: Mailbox to open

 * @entries: Number of entries in the inbound mailbox ring

 *

 * Initializes buffer ring, request the inbound message interrupt,

 * and enables the inbound message unit. Returns %0 on success

 * and %-EINVAL or %-ENOMEM on failure.

 Initialize client buffer ring */

 Initialize inbound message ring */

 Point dequeue/enqueue pointers at first entry in ring */

 Clear interrupt status */

 Hook up inbound message handler */

	/*

	 * Configure inbound message unit:

	 *      Snooping

	 *      4KB max message size

	 *      Unmask all interrupt sources

	 *      Disable

 Set number of queue entries */

 Now enable the unit */

/**

 * fsl_close_inb_mbox - Shut down MPC85xx inbound mailbox

 * @mport: Master port implementing the inbound message unit

 * @mbox: Mailbox to close

 *

 * Disables the inbound message unit, free all buffers, and

 * frees the inbound message interrupt.

 Disable inbound message unit */

 Free ring */

 Free interrupt */

/**

 * fsl_add_inb_buffer - Add buffer to the MPC85xx inbound message queue

 * @mport: Master port implementing the inbound message unit

 * @mbox: Inbound mailbox number

 * @buf: Buffer to add to inbound queue

 *

 * Adds the @buf buffer to the MPC85xx inbound message queue. Returns

 * %0 on success or %-EINVAL on failure.

/**

 * fsl_get_inb_message - Fetch inbound message from the MPC85xx message unit

 * @mport: Master port implementing the inbound message unit

 * @mbox: Inbound mailbox number

 *

 * Gets the next available inbound message from the inbound message queue.

 * A pointer to the message is returned on success or NULL on failure.

 If no more messages, then bail out */

 Copy max message size, caller is expected to allocate that big */

 Clear the available buffer */

/**

 * fsl_rio_doorbell_init - MPC85xx doorbell interface init

 * @mport: Master port implementing the inbound doorbell unit

 *

 * Initializes doorbell unit hardware and inbound DMA buffer

 * ring. Called from fsl_rio_setup(). Returns %0 on success

 * or %-ENOMEM on failure.

 Initialize inbound doorbells */

 Point dequeue/enqueue pointers at first entry in ring */

 Clear interrupt status */

 Hook up doorbell handler */

 Configure doorbells for snooping, 512 entries, and enable */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2006-2008, Michael Ellerman, IBM Corporation.

/**

 * msi_bitmap_reserve_dt_hwirqs - Reserve irqs specified in the device tree.

 * @bmp: pointer to the MSI bitmap.

 *

 * Looks in the device tree to see if there is a property specifying which

 * irqs can be used for MSI. If found those irqs reserved in the device tree

 * are reserved in the bitmap.

 *

 * Returns 0 for success, < 0 if there was an error, and > 0 if no property

 * was found in the device tree.

 Format is: (<u32 start> <u32 count>)+ */

 the bitmap won't be freed from memblock allocator */

 We zalloc'ed the bitmap, so all irqs are free by default */

 Can't allocate a bitmap of 0 irqs */

 of_node may be NULL */

 Should all be free by default */

 With no node, there's no msi-available-ranges, so expect > 0 */

 Should all still be free */

 Check we can fill it up and then no more */

 Should all be allocated */

 And if we free one we can then allocate another */

 Free most of them for the alignment tests */

 Check we get a naturally aligned offset */

 Clients may WARN_ON bitmap == NULL for "not-allocated" */

 There should really be a struct device_node allocator */

 No msi-available-ranges, so expect > 0 */

 Should all still be free */

 Now create a fake msi-available-ranges property */

 There should really .. oh whatever */

 msi-available-ranges, so expect == 0 */

 Check we got the expected result */

 CONFIG_MSI_BITMAP_SELFTEST */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Functions for setting up and using a MPC106 northbridge

 * Extracted from arch/powerpc/platforms/powermac/pci.c.

 *

 * Copyright (C) 2003 Benjamin Herrenschmuidt (benh@kernel.crashing.org)

 * Copyright (C) 1997 Paul Mackerras (paulus@samba.org)

/* N.B. this is called before bridges is initialized, so we can't

 Disabled for now, HW problems ??? */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * tsi108/109 device setup code

 *

 * Maintained by Roy Zang < tie-fei.zang@freescale.com >

		/* Some boards with the TSI108 bridge (e.g. Holly)

		 * have a miswiring of the ethernet PHYs which

		 * requires a workaround.  The special

		 * "txc-rxc-delay-disable" property enables this

		 * workaround.  FIXME: Need to port the tsi108_eth

		 * driver itself to phylib and use a non-misleading

		 * name for the workaround flag - it's not actually to

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * MPIC timer wakeup driver

 *

 * Copyright 2013 Freescale Semiconductor, Inc.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Freescale MPC85xx/MPC86xx RapidIO support

 *

 * Copyright 2009 Sysgo AG

 * Thomas Moll <thomas.moll@sysgo.com>

 * - fixed maintenance access routines, check for aligned access

 *

 * Copyright 2009 Integrated Device Technology, Inc.

 * Alex Bounine <alexandre.bounine@idt.com>

 * - Added Port-Write message handling

 * - Added Machine Check exception handling

 *

 * Copyright (C) 2007, 2008, 2010, 2011 Freescale Semiconductor, Inc.

 * Zhang Wei <wei.zhang@freescale.com>

 *

 * Copyright 2005 MontaVista Software, Inc.

 * Matt Porter <mporter@kernel.crashing.org>

 Port-Write debugging */

 Accept All ID */

 Check if we are prepared to handle this fault */

/**

 * fsl_local_config_read - Generate a MPC85xx local config space read

 * @mport: RapidIO master port info

 * @index: ID of RapdiIO interface

 * @offset: Offset into configuration space

 * @len: Length (in bytes) of the maintenance transaction

 * @data: Value to be read into

 *

 * Generates a MPC85xx local configuration space read. Returns %0 on

 * success or %-EINVAL on failure.

/**

 * fsl_local_config_write - Generate a MPC85xx local config space write

 * @mport: RapidIO master port info

 * @index: ID of RapdiIO interface

 * @offset: Offset into configuration space

 * @len: Length (in bytes) of the maintenance transaction

 * @data: Value to be written

 *

 * Generates a MPC85xx local configuration space write. Returns %0 on

 * success or %-EINVAL on failure.

/**

 * fsl_rio_config_read - Generate a MPC85xx read maintenance transaction

 * @mport: RapidIO master port info

 * @index: ID of RapdiIO interface

 * @destid: Destination ID of transaction

 * @hopcount: Number of hops to target device

 * @offset: Offset into configuration space

 * @len: Length (in bytes) of the maintenance transaction

 * @val: Location to be read into

 *

 * Generates a MPC85xx read maintenance transaction. Returns %0 on

 * success or %-EINVAL on failure.

 16MB maintenance window possible */

 allow only aligned access to maintenance registers */

/**

 * fsl_rio_config_write - Generate a MPC85xx write maintenance transaction

 * @mport: RapidIO master port info

 * @index: ID of RapdiIO interface

 * @destid: Destination ID of transaction

 * @hopcount: Number of hops to target device

 * @offset: Offset into configuration space

 * @len: Length (in bytes) of the maintenance transaction

 * @val: Value to be written

 *

 * Generates an MPC85xx write maintenance transaction. Returns %0 on

 * success or %-EINVAL on failure.

 16MB maintenance windows possible */

 allow only aligned access to maintenance registers */

 close inbound windows */

 check if addresses are aligned with the window size */

 check for conflicting ranges */

 find unused atmu */

 skip default window */

XXX: Error recovery is not implemented, we just clear errors */

 Serial phy */

 Parallel phy */

/**

 * fsl_rio_setup - Setup Freescale PowerPC RapidIO interface

 * @dev: platform_device pointer

 *

 * Initializes MPC85xx RapidIO hardware interface, configures

 * master port with system-specific info, and registers the

 * master port with the RapidIO subsystem.

set up doobell node*/

set up port write node*/

set up ports node*/

 Get node address wide */

 Get node size wide */

 Get parent address wide wide */

 Checking the port training status */

 Disable ports */

 Set 1x lane */

 Enable ports */

 Set to receive packets with any dest ID */

 Configure maintenance transaction window */

/* The probe function for RapidIO peer-to-peer network.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Freescale General-purpose Timers Module

 *

 * Copyright (c) Freescale Semiconductor, Inc. 2006.

 *               Shlomi Gridish <gridish@freescale.com>

 *               Jerry Huang <Chang-Ming.Huang@freescale.com>

 * Copyright (c) MontaVista Software, Inc. 2008.

 *               Anton Vorontsov <avorontsov@ru.mvista.com>

 Timer 1, Timer 2 global config register */

 Timer 3, timer 4 global config register */

 Timer 1 mode register */

 Timer 2 mode register */

 Timer 1 reference register */

 Timer 2 reference register */

 Timer 1 capture register */

 Timer 2 capture register */

 Timer 1 counter */

 Timer 2 counter */

 Timer 3 mode register */

 Timer 4 mode register */

 Timer 3 reference register */

 Timer 4 reference register */

 Timer 3 capture register */

 Timer 4 capture register */

 Timer 3 counter */

 Timer 4 counter */

 Timer 1 event register */

 Timer 2 event register */

 Timer 3 event register */

 Timer 4 event register */

 Timer 1 prescale register */

 Timer 2 prescale register */

 Timer 3 prescale register */

 Timer 4 prescale register */

/**

 * gtm_get_timer - request GTM timer to use it with the rest of GTM API

 * Context:	non-IRQ

 *

 * This function reserves GTM timer for later use. It returns gtm_timer

 * structure to use with the rest of GTM API, you should use timer->irq

 * to manage timer interrupt.

/**

 * gtm_get_specific_timer - request specific GTM timer

 * @gtm:	specific GTM, pass here GTM's device_node->data

 * @timer:	specific timer number, Timer1 is 0.

 * Context:	non-IRQ

 *

 * This function reserves GTM timer for later use. It returns gtm_timer

 * structure to use with the rest of GTM API, you should use timer->irq

 * to manage timer interrupt.

/**

 * gtm_put_timer16 - release 16 bits GTM timer

 * @tmr:	pointer to the gtm_timer structure obtained from gtm_get_timer

 * Context:	any

 *

 * This function releases GTM timer so others may request it.

/*

 * This is back-end for the exported functions, it's used to reset single

 * timer in reference mode.

 CPM2 doesn't have primary prescaler */

	/*

	 * We have two 8 bit prescalers -- primary and secondary (psr, sps),

	 * plus "slow go" mode (clk / 16). So, total prescale value is

	 * 16 * (psr + 1) * (sps + 1). Though, for CPM2 GTMs we losing psr.

	/*

	 * Properly reset timers: stop, reset, set up prescalers, reference

	 * value and clear event register.

 Let it be. */

/**

 * gtm_set_timer16 - (re)set 16 bit timer with arbitrary precision

 * @tmr:	pointer to the gtm_timer structure obtained from gtm_get_timer

 * @usec:	timer interval in microseconds

 * @reload:	if set, the timer will reset upon expiry rather than

 *         	continue running free.

 * Context:	any

 *

 * This function (re)sets the GTM timer so that it counts up to the requested

 * interval value, and fires the interrupt when the value is reached. This

 * function will reduce the precision of the timer as needed in order for the

 * requested timeout to fit in a 16-bit register.

 quite obvious, frequency which is enough for Sec precision */

/**

 * gtm_set_exact_utimer16 - (re)set 16 bits timer

 * @tmr:	pointer to the gtm_timer structure obtained from gtm_get_timer

 * @usec:	timer interval in microseconds

 * @reload:	if set, the timer will reset upon expiry rather than

 *         	continue running free.

 * Context:	any

 *

 * This function (re)sets GTM timer so that it counts up to the requested

 * interval value, and fires the interrupt when the value is reached. If reload

 * flag was set, timer will also reset itself upon reference value, otherwise

 * it continues to increment.

 *

 * The _exact_ bit in the function name states that this function will not

 * crop precision of the "usec" argument, thus usec is limited to 16 bits

 * (single timer width).

 quite obvious, frequency which is enough for Sec precision */

	/*

	 * We can lower the frequency (and probably power consumption) by

	 * dividing both frequency and usec by 2 until there is no remainder.

	 * But we won't bother with this unless savings are measured, so just

	 * run the timer as is.

/**

 * gtm_stop_timer16 - stop single timer

 * @tmr:	pointer to the gtm_timer structure obtained from gtm_get_timer

 * Context:	any

 *

 * This function simply stops the GTM timer.

/**

 * gtm_ack_timer16 - acknowledge timer event (free-run timers only)

 * @tmr:	pointer to the gtm_timer structure obtained from gtm_get_timer

 * @events:	events mask to ack

 * Context:	any

 *

 * Thus function used to acknowledge timer interrupt event, use it inside the

 * interrupt handler.

	/*

	 * Yeah, I don't like this either, but timers' registers a bit messed,

	 * so we have to provide shortcuts to write timer independent code.

	 * Alternative option is to create gt*() accessors, but that will be

	 * even uglier and cryptic.

 CPM2 doesn't have primary prescaler */

 We don't want to lose the node and its ->data */

/*

 * General Purpose functions for the global management of the

 * 8260 Communication Processor Module.

 * Copyright (c) 1999-2001 Dan Malek <dan@embeddedalley.com>

 * Copyright (c) 2000 MontaVista Software, Inc (source@mvista.com)

 *	2.3.99 Updates

 *

 * 2006 (c) MontaVista Software, Inc.

 * Vitaly Bordug <vbordug@ru.mvista.com>

 * 	Merged to arch/powerpc from arch/ppc/syslib/cpm2_common.c

 *

 * This file is licensed under the terms of the GNU General Public License

 * version 2. This program is licensed "as is" without any warranty of any

 * kind, whether express or implied.

/*

 *

 * In addition to the individual control of the communication

 * channels, there are a few functions that globally affect the

 * communication processor.

 *

 * Buffer descriptors must be allocated from the dual ported memory

 * space.  The allocator for that is here.  When the communication

 * process is reset, we reclaim the memory available.  There is

 * currently no deallocator for this memory.

 Pointer to comm processor space */

/* We allocate this here because it is used almost exclusively for

 * the communication processor devices.

#define CPM_MAP_SIZE	(0x40000)	/* 256k - the PQ3 reserve this amount

					   of space for CPM as it is larger

	/* Tell everyone where the comm processor resides.

	/* Reset the CPM.

/* Set a baud rate generator.  This needs lots of work.  There are

 * eight BRGs, which can be connected to the CPM channels or output

 * as clocks.  The BRGs are in two different block of internal

 * memory mapped space.

 * The baud rate clock is the system clock divided by something.

 * It was set up long ago during the initial boot phase and is

 * is given to us.

 * Baud rate clocks are zero-based in the driver code (as that maps

 * to port numbers).  Documentation uses 1-based numbering.

	/* This is good enough to get SMCs running.....

 Round the clock divider to the nearest integer. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * (c) Copyright 2006 Benjamin Herrenschmidt, IBM Corp.

 *                    <benh@kernel.crashing.org>

 host.type == DCR_HOST_INVALID */

 host.type == DCR_HOST_INVALID */

 host.type == DCR_HOST_INVALID */

 defined(CONFIG_PPC_DCR_NATIVE) && defined(CONFIG_PPC_DCR_MMIO) */

 Stride is not properly defined yet, default to 0x10 for Axon */

 XXX FIXME: Which property name is to use of the 2 following ? */

 Maybe could do some better range checking here */

 defined(CONFIG_PPC_DCR_MMIO) */

 defined(CONFIG_PPC_DCR_NATIVE) */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2006-2007, Michael Ellerman, IBM Corporation.

 The mpic calls this even when there is no allocator setup */

	/* Reserve source numbers we know are reserved in the HW.

	 *

	 * This is a bit of a mix of U3 and U4 reserves but that's going

	 * to work fine, we have plenty enugh numbers left so let's just

	 * mark anything we don't like reserved.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2007-2011 Freescale Semiconductor, Inc.

 *

 * Author: Tony Li <tony.li@freescale.com>

 *	   Jason Jin <Jason.jin@freescale.com>

 *

 * The hwirq alloc and free code reuse from sysdev/mpic_msi.c

 Offset of MSIIR, relative to start of MSIR bank */

/*

 * We do not need this actually. The MSIR register has been read once

 * in the cascade interrupt. So, this MSI interrupt has been acked

	/*

	 * Reserve all the hwirqs

	 * The available hwirqs will be released in fsl_msi_setup_hwirq()

 Physical address of the MSIIR */

 If the msi-address-64 property exists, then use it */

	/*

	 * MPIC version 2.0 has erratum PIC1. It causes

	 * that neither MSI nor MSI-X can work fine.

	 * This is a workaround to allow MSI-X to function

	 * properly. It only works for MSI-X, we prevent

	 * MSI on buggy chips in fsl_setup_msi_irqs().

		/*

		 * MPIC version 2.0 has erratum PIC1. For now MSI

		 * could not work. So check to prevent MSI from

		 * being used on the board with this erratum.

	/*

	 * If the PCI node has an fsl,msi property, then we need to use it

	 * to find the specific MSI.

		/*

		 * Loop over all the MSI devices until we find one that has an

		 * available interrupt.

			/*

			 * If the PCI node has an fsl,msi property, then we

			 * restrict our search to the corresponding MSI node.

			 * The simplest way is to skip over MSI nodes with the

			 * wrong phandle. Under the Freescale hypervisor, this

			 * has the additional benefit of skipping over MSI

			 * nodes that are not mapped in the PAMU.

 chip_data is msi_data via host->hostdata in host->map() */

 free by the caller of this function */

 Release the hwirqs corresponding to this MSI register */

	/*

	 * Under the Freescale hypervisor, the msi nodes don't have a 'reg'

	 * property.  Instead, we use hypercalls to access the MSI.

		/*

		 * First read the MSIIR/MSIIR1 offset from dts

		 * On failure use the hardcode MSIIR offset

 For erratum PIC1 on MPIC version 2.0*/

	/*

	 * Remember the phandle, so that we can match with any PCI nodes

	 * that have an "fsl,msi" property.

	/*

	 * Apply the MSI ops to all the controllers.

	 * It doesn't hurt to reassign the same ops,

	 * but bail out if we find another MSI driver.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * FSL SoC setup code

 *

 * Maintained by Kumar Gala (see MAINTAINERS for contact information)

 *

 * 2006 (c) MontaVista Software, Inc.

 * Vitaly Bordug <vbordug@ru.mvista.com>

 For the Freescale hypervisor */

 Legacy device binding -- will go away when no users are left. */

 CONFIG_CPM2 */

 set reset control register */

 HRESET_REQ */

/*

 * Restart the current partition

 *

 * This function should be assigned to the ppc_md.restart function pointer,

 * to initiate a partition restart when we're running under the Freescale

 * hypervisor.

/*

 * Halt the current partition

 *

 * This function should be assigned to the pm_power_off and ppc_md.halt

 * function pointers, to shut down the partition when we're running under

 * the Freescale hypervisor.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * MPC83xx/85xx/86xx PCI/PCIE support routing.

 *

 * Copyright 2007-2012 Freescale Semiconductor, Inc.

 * Copyright 2008-2009 MontaVista Software, Inc.

 *

 * Initial author: Xianghua Xiao <x.xiao@freescale.com>

 * Recode: ZHANG WEI <wei.zhang@freescale.com>

 * Rewrite the routing for Frescale PCI and PCI Express

 * 	Roy Zang <tie-fei.zang@freescale.com>

 * MPC83xx PCI-Express support:

 * 	Tony Li <tony.li@freescale.com>

 * 	Anton Vorontsov <avorontsov@ru.mvista.com>

 if we aren't a PCIe don't bother */

 if we aren't in host mode don't bother */

 for PCIe IP rev 3.0 or greater use CSR0 for link state */

	/*

	 * Fix up PCI devices that are able to DMA to the large inbound

	 * mapping that allows addressing any RAM address from across PCI.

 enable & mem R/W */

 enable relaxed ordering */

 atmu setup for fsl pci/pcie controller */

	/*

	 * If this is kdump, we don't want to trigger a bunch of PCI

	 * errors by closing the window on in-flight DMA.

	 *

	 * We still run most of the function's logic so that things like

	 * hose->dma_window_size still get set.

		/*

		 * BSC9132 Rev1.0 has an issue where all the PEX inbound

		 * windows have implemented the default target value as 0xf

		 * for CCSR space.In all Freescale legacy devices the target

		 * of 0xf is reserved for local memory space. 9132 Rev1.0

		 * now has local mempry space mapped to target 0x0 instead of

		 * 0xf. Hence adding a workaround to remove the target 0xf

		 * defined for memory space from Inbound window attributes.

 Disable all windows (except powar0 since it's ignored) */

 Setup outbound MEM window */

 We assume all memory resources have the same offset */

 Setup outbound IO window */

 Enable, IO R/W */

 convert to pci address space */

 setup PCSRBAR/PEXCSRBAR */

 Setup inbound mem window */

	/*

	 * The msi-address-64 property, if it exists, indicates the physical

	 * address of the MSIIR register.  Normally, this register is located

	 * inside CCSR, so the ATMU that covers all of CCSR is used. But if

	 * this property exists, then we normally need to create a new ATMU

	 * for it.  For now, however, we cheat.  The only entity that creates

	 * this property is the Freescale hypervisor, and the address is

	 * specified in the partition configuration.  Typically, the address

	 * is located in the page immediately after the end of DDR.  If so, we

	 * can avoid allocating a new ATMU by extending the DDR ATMU by one

	 * page.

 TODO: Create a new ATMU for MSIIR */

 PCIe can overmap inbound & outbound since RX & TX are separated */

 Size window to exact size if power-of-two or one size up */

 Setup inbound memory window */

		/*

		 * if we have >4G of memory setup second PCI inbound window to

		 * let devices that are 64-bit address capable to work w/o

		 * SWIOTLB and access the full range of memory

 Size window up if we dont fit in exact power-of-2 */

 Setup inbound memory window */

			/*

			 * install our own dma_set_mask handler to fixup dma_ops

			 * and dma_offset

 Setup inbound memory window */

 adjusting outbound windows could reclaim space in mem map */

	/* The root complex bridge comes up with bogus resources,

	 * we copy the PHB ones in.

	 *

	 * With the current generic PCI code, the PHB bus no longer

	 * has bus->resource[0..4] set, so things are a bit more

	 * tricky.

 Fetch host bridge registers address */

 Get bus range if any */

 set platform device as the parent */

 use fsl_indirect_read_config for PCIe */

 For PCIE read HEADER_TYPE to identify controller mode */

 For PCI read PROG to identify controller mode */

 check PCI express link status */

		/*

		 * Set PBFR(PCI Bus Function Register)[10] = 1 to

		 * disable the combining of crossing cacheline

		 * boundary requests into one burst transaction.

		 * PCI-X operation is not affected.

		 * Fix erratum PCI 5 on MPC8548

 Master disable streaming */

 Interpret the "ranges" property */

 This also maps the I/O region and sets isa_io/mem_base */

 Setup PEX window registers */

 Set up controller operations */

 unmap cfg_data & cfg_addr separately if not on same page */

 CONFIG_FSL_SOC_BOOKE || CONFIG_PPC_86xx */

/*

 * With the convention of u-boot, the PCIE outbound window 0 serves

 * as configuration transactions outbound.

	/*

	 * Workaround for the HW bug: for Type 0 configure transactions the

	 * PCI-E controller does not check the device number bits and just

	 * assumes that the device number bits are 0.

 Type 0 */

 PPC_INDIRECT_TYPE_SURPRESS_PRIMARY_BUS */

 PCI-E isn't configured. */

 Fetch host bridge registers address */

		/*

		 * MPC83xx supports up to two host controllers

		 * 	one at 0x8500 has config space registers at 0x8300

		 * 	one at 0x8600 has config space registers at 0x8380

	/*

	 * Controller at offset 0x8500 is primary

 Get bus range if any */

 Interpret the "ranges" property */

 This also maps the I/O region and sets isa_io/mem_base */

 CONFIG_PPC_83xx */

 Walk the Root Complex Inbound windows to match IMMR base */

 not enabled, skip */

		/*

		 * For PEXCSRBAR, bit 3-0 indicate prefetchable and

		 * address type. So when getting base address, these

		 * bits should be masked

 Let KVM/QEMU deal with the exception */

	/*

	 * The following entries are for compatibility with older device

	 * trees.

 Callers can specify the primary bus using other means. */

 If a PCI host bridge contains an ISA node, it's primary. */

	/*

	 * If there's no PCI host bridge with ISA, arbitrarily

	 * designate one as primary.  This can go away once

	 * various bugs with primary-less systems are fixed.

 Get hose's pci_dev */

 PME Disable */

 Enable PTOD, ENL23D & EXL23D */

 PME Enable */

 Send PME_Turn_Off Message Request */

 Wait trun off done */

 Send Exit L2 State Message */

 Wait exit done */

/*

 * Setup code for PC-style Real-Time Clock.

 *

 * Author: Wade Farnsworth <wfarnsworth@mvista.com>

 *

 * 2007 (c) MontaVista Software, Inc. This file is licensed under

 * the terms of the GNU General Public License version 2. This program

 * is licensed "as is" without any warranty of any kind, whether express

 * or implied.

	/*

	 * RTC_PORT(x) is hardcoded in asm/mc146818rtc.h.  Verify that the

	 * address provided by the device node matches.

		/*

		 * Use a fixed interrupt value of 8 since on PPC if we are

		 * using this its off an i8259 which we ensure has interrupt

		 * numbers 0..15.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2011-2012, Meador Inge, Mentor Graphics Corporation.

 *

 * Some ideas based on un-pushed work done by Vivek Mahajan, Jason Jin, and

 * Mingkai Hu from Freescale Semiconductor, Inc.

 Assume busy until proven otherwise.  */

/* The following three functions are used to compute the order and number of

 * the message register blocks.  They are clearly very inefficent.  However,

 * they are called *only* a few times during device initialization.

/* The probe function for a single message register block.

	/* Allocate the message register array upon the first device

	 * registered.

 IO map the message register block. */

 Ensure the block has a defined order. */

	/* Grab the receive mask which specifies what registers can receive

	 * interrupts.

 Build up the appropriate message register data structures. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * A udbg backend which logs messages and reads input from in memory

 * buffers.

 *

 * The console output can be read from memcons_output which is a

 * circular buffer whose next write position is stored in memcons.output_pos.

 *

 * Input may be passed by writing into the memcons_input buffer when it is

 * empty. The input buffer is empty when both input_pos == input_start and

 * *input_start == '\0'.

 *

 * Copyright (C) 2003-2005 Anton Blanchard and Milton Miller, IBM Corp

 * Copyright (C) 2013 Alistair Popple, IBM Corp

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2009-2010 Freescale Semiconductor, Inc.

 *

 * Simple memory allocator abstraction for QorIQ (P1/P2) based Cache-SRAM

 *

 * Author: Vivek Mahajan <vivek.mahajan@freescale.com>

 *

 * This file is derived from the original work done

 * by Sylvain Munaut for the Bestcomm SRAM allocator.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * RCPM(Run Control/Power Management) support

 *

 * Copyright 2012-2015 Freescale Semiconductor Inc.

 *

 * Author: Chenhui Zhao <chenhui.zhao@freescale.com>

 one bit corresponds to one thread for PH10 of 6500 */

 if both threads are offline, put the cpu in PH20 */

 if only one thread is offline, disable the thread */

 Upon resume, wait for RCPM_POWMGTCSR_SLP bit to be clear. */

 clear previous LPM20 status */

 enter LPM20 status */

 At this point, the device is in LPM20 status. */

 resume ... */

 read back to push the previous write */

 support sleep by default */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2009-2010, 2012 Freescale Semiconductor, Inc.

 *

 * QorIQ (P1/P2) L2 controller init for Cache-SRAM instantiation

 *

 * Author: Vivek Mahajan <vivek.mahajan@freescale.com>

 fall back to L2 cache only */

	/*

	 * Write bits[0-17] to srbar0

	/*

	 * Write bits[18-21] to srbare0

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * memory mapped NVRAM

 *

 * (C) Copyright IBM Corp. 2005

 *

 * Authors : Utz Bacher <utz.bacher@de.ibm.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * arch/powerpc/sysdev/dart_iommu.c

 *

 * Copyright (C) 2004 Olof Johansson <olof@lixom.net>, IBM Corporation

 * Copyright (C) 2005 Benjamin Herrenschmidt <benh@kernel.crashing.org>,

 *                    IBM Corporation

 *

 * Based on pSeries_iommu.c:

 * Copyright (C) 2001 Mike Corrigan & Dave Engebretsen, IBM Corporation

 * Copyright (C) 2004 Olof Johansson <olof@lixom.net>, IBM Corporation

 *

 * Dynamic DMA mapping support, Apple U3, U4 & IBM CPC925 "DART" iommu.

 DART table address and size */

 Mapped base address for the dart */

 Dummy val that entries are set to when unused */

	/* To invalidate the DART, set the DARTCNTL_FLUSHTLB bit in the

	 * control register and wait for it to clear.

	 *

	 * Gotcha: Sometimes, the DART won't detect that the bit gets

	 * set. If so, clear it and set it again.

	/*

	 * We add 1 to the number of entries to flush, following a

	 * comment in Darwin indicating that the memory controller

	 * can prefetch unmapped memory under some circumstances.

 Perform a standard cache flush */

	/*

	 * Perform the sequence described in the CPC925 manual to

	 * ensure all the data gets to a point the cache incoherent

	 * DART hardware will see.

	/* On U3, all memory is contiguous, so we can move this

	 * out of the loop.

	/* We don't worry about flushing the TLB cache. The only drawback of

	 * not doing it is that we won't catch buggy device drivers doing

	 * bad DMAs, but then no 32-bit architecture ever does either.

 512 pages (2MB) is max DART tablesize. */

	/*

	 * 16MB (1 << 24) alignment. We allocate a full 16Mb chuck since we

	 * will blow up an entire large page anyway in the kernel mapping.

 There is no point scanning the DART space for leaks*/

	/* Allocate a spare page to map all invalid DART pages. We need to do

	 * that to work around what looks like a problem with the HT bridge

	 * prefetching into invalid pages and corrupting data

 IOMMU disabled by the user ? bail out */

	/*

	 * Only use the DART if the machine has more than 1GB of RAM

	 * or if requested with iommu=on on cmdline.

	 *

	 * 1GB of RAM is picked as limit because some default devices

	 * (i.e. Airport Extreme) have 30 bit address range limits.

 Get DART registers */

 Map in DART registers */

 Allocate the DART and dummy page */

 Fill initial table */

 Push to memory */

 Initialize DART with table base and enable it. */

 Invalidate DART to get rid of possible stale TLBs */

 it_size is in number of entries */

 Initialize the common IOMMU code */

	/* Reserve the last page of the DART to avoid possible prefetch

	 * past the DART mapped area

 Find the DART in the device-tree */

 use default direct_dma_ops */

 Initialize the DART HW */

	/*

	 * U4 supports a DART bypass, we use it for 64-bit capable devices to

	 * improve performance.  However, that only works for devices connected

	 * to the U4 own PCIe interface, not bridged through hypertransport.

	 * We need the device to support at least 40 bits of addresses.

 Setup pci_dma ops */

 CONFIG_PM */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Freescale Semiconductor, Inc.

 *

 * Author: Varun Sethi <varun.sethi@freescale.com>

 allocate interrupt vectors for error interrupts */

 Mask all error interrupts */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Common routines for Tundra Semiconductor TSI108 host bridge.

 *

 * 2004-2005 (c) Tundra Semiconductor Corp.

 * Author: Alex Bounine (alexandreb@tundra.com)

 * Author: Roy Zang (tie-fei.zang@freescale.com)

 * 	   Add pci interrupt router host

	/*

	 * Quietly clear PB and PCI error flags set as result

	 * of PCI/X configuration read requests.

 Read PB Error Log Registers */

 Clear error flag */

 Clear read error reported in PB_ISR */

 Clear PCI/X bus cfg errors if applicable */

 PCI Config mapping */

 Fetch host bridge registers address */

 Get bus range if any */

 Interpret the "ranges" property */

 This also maps the I/O region and sets isa_io/mem_base */

/*

 * Low level utility functions

 INTx_DIR = output */

 INTx_TYPE = unused */

 Read PCI/X block interrupt status register */

 Process Interrupt from PCI bus INTA# - INTD# lines */

 Disable interrupts from PCI block */

 end of DEBUG */

/*

 * Linux descriptor level callbacks

 Enable interrupts from PCI block */

/*

 * Interrupt controller descriptor for cascaded PCI interrupt controller.

/*

 * Exported functions

/*

 * The Tsi108 PCI interrupts initialization routine.

 *

 * The INTA# - INTD# interrupts on the PCI bus are reported by the PCI block

 * to the MPIC using single interrupt source (IRQ_TSI108_PCI). Therefore the

 * PCI block has to be treated as a cascaded interrupt controller connected

 * to the MPIC.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * arch/powerpc/sysdev/ipic.c

 *

 * IPIC routines implementations.

 *

 * Copyright 2005 Freescale Semiconductor, Inc.

	/* mb() can't guarantee that masking is finished.  But it does finish

	/* mb() can't guarantee that ack is finished.  But it does finish

	/* mb() can't guarantee that ack is finished.  But it does finish

	/* ipic supports only low assertion and high-to-low change senses

 ipic supports only edge mode on external interrupts */

	/* only EXT IRQ senses are programmable on ipic

	 * internal IRQ senses are LEVEL_LOW

 level interrupts and edge interrupts have different ack operations */

 Exact match, unless ipic node is NULL */

 Set default irq type */

 init hw */

	/* default priority scheme is grouped. If spread mode is required

 handle MCP route */

 handle routing of IRQ0 to MCP */

 Return an interrupt vector or 0 if no interrupt is pending. */

 0 --> no irq is pending */

		/* In deep sleep, make sure there can be no

		 * pending interrupts, as this can cause

		 * problems on 831x.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Instantiate mmio-mapped RTC chips based on device tree information

 *

 * Copyright 2007 David Gibson <dwg@au1.ibm.com>, IBM Corporation.

/*

 * Platform information definitions.

 *

 * Copied from arch/ppc/syslib/cpm2_pic.c with minor subsequent updates

 * to make in work in arch/powerpc/. Original (c) belongs to Dan Malek.

 *

 * Author:  Vitaly Bordug <vbordug@ru.mvista.com>

 *

 * 1999-2001 (c) Dan Malek <dan@embeddedalley.com>

 * 2006 (c) MontaVista Software, Inc.

 *

 * This file is licensed under the terms of the GNU General Public License

 * version 2. This program is licensed "as is" without any warranty of any

 * kind, whether express or implied.

/* The CPM2 internal interrupt controller.  It is usually

 * the only interrupt controller.

 * There are two 32-bit registers (high/low) for up to 64

 * possible interrupts.

 *

 * Now, the fun starts.....Interrupt Numbers DO NOT MAP

 * in a simple arithmetic fashion to mask or pending registers.

 * That is, interrupt 4 does not map to bit position 4.

 * We create two tables, indexed by vector number, to indicate

 * which register to use and which bit in the register to use.

 External IRQS */

 Port C IRQS */

 2 32-bit registers */

/* bit numbers do not match the docs, these are precomputed so the bit for

	/*

	 * Work around large numbers of spurious IRQs on PowerPC 82xx

	 * systems.

	/* Port C interrupts are either IRQ_TYPE_EDGE_FALLING or

	 * IRQ_TYPE_EDGE_BOTH (default).  All others are IRQ_TYPE_EDGE_FALLING

	 * or IRQ_TYPE_LEVEL_LOW (default)

	/* internal IRQ senses are LEVEL_LOW

	 * EXT IRQ and Port C IRQ senses are programmable

       /* For CPM2, read the SIVEC register and shift the bits down

	/* Clear the CPM IRQ controller, in case it has any bits set

	 * from the bootloader

 Mask out everything */

 Ack everything */

 Dummy read of the vector */

	/* Initialize the default interrupt mapping priorities,

	 * in case the boot rom changed something on us.

 create a legacy host */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2006, Segher Boessenkool, IBM Corporation.

 * Copyright 2006-2007, Michael Ellerman, IBM Corporation.

 A bit ugly, can we get this from the pci_dev somehow? */

	/* U4 PCIe MSIs need to write to the special register in

	 * the bridge that generates interrupts. There should be

	 * theorically a register at 0xf8005000 where you just write

	 * the MSI number and that triggers the right interrupt, but

	 * unfortunately, this is busted in HW, the bridge endian swaps

	 * the value and hits the wrong nibble in the register.

	 *

	 * So instead we use another register set which is used normally

	 * for converting HT interrupts to MPIC interrupts, which decodes

	 * the interrupt number as part of the low address bits

	 *

	 * This will not work if we ever use more than one legacy MSI in

	 * a block but we never do. For one MSI or multiple MSI-X where

	 * each interrupt address can be specified separately, it works

	 * just fine.

 If we can't find a magic address then MSI ain't gonna work */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Freescale LBC and UPM routines.

 *

 * Copyright  2007-2008  MontaVista Software, Inc.

 * Copyright  2010 Freescale Semiconductor

 *

 * Author: Anton Vorontsov <avorontsov@ru.mvista.com>

 * Author: Jack Lan <Jack.Lan@freescale.com>

 * Author: Roy Zang <tie-fei.zang@freescale.com>

/**

 * fsl_lbc_addr - convert the base address

 * @addr_base:	base address of the memory bank

 *

 * This function converts a base address of lbc into the right format for the

 * BR register. If the SOC has eLBC then it returns 32bit physical address

 * else it convers a 34bit local bus physical address to correct format of

 * 32bit address for BR register (Example: MPC8641).

/**

 * fsl_lbc_find - find Localbus bank

 * @addr_base:	base address of the memory bank

 *

 * This function walks LBC banks comparing "Base address" field of the BR

 * registers with the supplied addr_base argument. When bases match this

 * function returns bank number (starting with 0), otherwise it returns

 * appropriate errno value.

/**

 * fsl_upm_find - find pre-programmed UPM via base address

 * @addr_base:	base address of the memory bank controlled by the UPM

 * @upm:	pointer to the allocated fsl_upm structure

 *

 * This function fills fsl_upm structure so you can use it with the rest of

 * UPM API. On success this function returns 0, otherwise it returns

 * appropriate errno value.

/**

 * fsl_upm_run_pattern - actually run an UPM pattern

 * @upm:	pointer to the fsl_upm structure obtained via fsl_upm_find

 * @io_base:	remapped pointer to where memory access should happen

 * @mar:	MAR register content during pattern execution

 *

 * This function triggers dummy write to the memory specified by the io_base,

 * thus UPM pattern actually executed. Note that mar usage depends on the

 * pre-programmed AMX bits in the UPM RAM.

 clear event registers */

 Set the monitor timeout value to the maximum for erratum A001 */

/*

 * NOTE: This interrupt is used to report localbus events of various kinds,

 * such as transaction errors on the chipselects.

/*

 * fsl_lbc_ctrl_probe

 *

 * called by device layer when it finds a device matching

 * one our driver can handled. This code allocates all of

 * the resources needed for the controller only.  The

 * resources for the NAND banks themselves are allocated

 * in the chip probe function.

 Enable interrupts for any detected events */

 save lbc registers */

 restore lbc registers */

 CONFIG_SUSPEND */

 SPDX-License-Identifier: GPL-2.0

/*

 * Common CPM GPIO wrapper for the CPM GPIO ports

 *

 * Author: Christophe Leroy <christophe.leroy@c-s.fr>

 *

 * Copyright 2017 CS Systemes d'Information.

 *

 Port E uses CPM2 layout */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Common CPM code

 *

 * Author: Scott Wood <scottwood@freescale.com>

 *

 * Copyright 2007-2008,2010 Freescale Semiconductor, Inc.

 *

 * Some parts derived from commproc.c/cpm2_common.c, which is:

 * Copyright (c) 1997 Dan error_act (dmalek@jlc.net)

 * Copyright (c) 1999-2001 Dan Malek <dan@embeddedalley.com>

 * Copyright (c) 2000 MontaVista Software, Inc (source@mvista.com)

 * 2006 (c) MontaVista Software, Inc.

 * Vitaly Bordug <vbordug@ru.mvista.com>

 shadowed data register to clear/set bits safely */

 CONFIG_CPM2 || CONFIG_8xx_GPIO */

/*

 *  arch/powerpc/kernel/mpic.c

 *

 *  Driver for interrupt controllers following the OpenPIC standard, the

 *  common implementation being IBM's MPIC. This driver also can deal

 *  with various broken implementations of this HW.

 *

 *  Copyright (C) 2004 Benjamin Herrenschmidt, IBM Corp.

 *  Copyright 2010-2012 Freescale Semiconductor, Inc.

 *

 *  This file is subject to the terms and conditions of the GNU General Public

 *  License.  See the file COPYING in the main directory of this archive

 *  for more details.

 XXX for now */

 Original OpenPIC compatible MPIC */

 Tsi108/109 PIC */

 CONFIG_MPIC_WEIRD */

 CONFIG_MPIC_WEIRD */

/*

 * Register accessor functions

/*

 * Low level utility functions

 CONFIG_PPC_DCR */

 !CONFIG_PPC_DCR */

/* Check if we have one of those nice broken MPICs with a flipped endian on

 * reads from IPI registers

/* Test if an interrupt is sourced from HyperTransport (used on broken U3s)

 * to force the edge setting on the MPIC and do the ack workaround.

 Enable and configure */

	/* use the lowest bit inverted to the actual HW,

 Disable */

	/* use the lowest bit inverted to the actual HW,

 mask it , will be unmasked later */

 Apple HT PIC has a non-standard way of doing EOIs */

 Allocate fixups array */

 Init spinlock */

	/* Map U3 config space. We assume all IO-APICs are on the primary bus

	 * so we only need to map 64kB.

	/* Now we scan all slots. We do a very quick scan, we read the header

	 * type, vendor ID and device ID only, that's plenty enough

 If no device, skip */

 Check if is supports capability lists */

 next device, if function 0 */

 CONFIG_MPIC_U3_HT_IRQS */

 CONFIG_MPIC_U3_HT_IRQS */

 Find an mpic associated with a given linux interrupt */

 Determine if the linux irq is an IPI */

 Determine if the linux irq is a timer */

 Convert a cpu mask from logical to physical cpu numbers. */

 Get the mpic structure from the IPI number */

 Get the mpic structure from the irq number */

 Get the mpic structure from the irq data */

 Send an EOI */

/*

 * Linux descriptor level callbacks

 make sure mask gets to controller before we return to user */

 make sure mask gets to controller before we return to user */

	/* We always EOI on end_irq() even for edge interrupts since that

	 * should only lower the priority, the MPIC should have properly

	 * latched another edge interrupt coming in anyway

	/* We always EOI on end_irq() even for edge interrupts since that

	 * should only lower the priority, the MPIC should have properly

	 * latched another edge interrupt coming in anyway

 !CONFIG_MPIC_U3_HT_IRQS */

 NEVER disable an IPI... that's just plain wrong! */

	/*

	 * IPIs are marked IRQ_PER_CPU. This has the side effect of

	 * preventing the IRQ_PENDING/IRQ_INPROGRESS logic from

	 * applying to them. We EOI them late to avoid re-entering.

 CONFIG_SMP */

 Now convert sense value */

 We don't support "none" type */

 Default: read HW settings */

 Apply to irq desc */

 Apply to HW */

 CONFIG_SMP */

 CONFIG_MPIC_U3_HT_IRQS */

 Exact match, unless mpic node is NULL */

 CONFIG_SMP */

 Default chip */

 Check for HT interrupts, override vecpri */

 CONFIG_MPIC_U3_HT_IRQS */

 Set default irq type */

	/* If the MPIC was reset, then all vectors have already been

	 * initialized.  Otherwise, a per source lazy initialization

	 * is done here.

		/*

		 * Freescale MPIC with extended intspec:

		 * First two cells are as usual.  Third specifies

		 * an "interrupt type".  Fourth is type-specific data.

		 *

		 * See Documentation/devicetree/bindings/powerpc/fsl/mpic.txt

		/* Apple invented a new race of encoding on machines with

		 * an HT APIC. They encode, among others, the index within

		 * the HT APIC. We don't care about it here since thankfully,

		 * it appears that they have the APIC already properly

		 * configured, and thus our current fixup code that reads the

		 * APIC config works fine. However, we still need to mask out

		 * bits in the specifier to make sure we only get bit 0 which

		 * is the level/edge bit (the only sense bit exposed by Apple),

		 * as their bit 1 means something else.

 IRQ handler for a secondary MPIC cascaded from another IRQ controller */

/*

 * Exported functions

 Default MPIC search parameters */

	/*

	 * If we were not passed a device-tree node, then perform the default

	 * search for standardized a standardized OpenPIC.

 Pick the physical address from the device tree if unspecified */

 Check if it is DCR-based */

 Read extra device-tree properties into the flags variable */

 CONFIG_MPIC_U3_HT_IRQS */

 CONFIG_SMP */

 so far */

 Look for protected sources */

 Allocate a bitmap with one bit per interrupt */

 default register type */

	/*

	 * An MPIC with a "dcr-reg" property must be accessed that way, but

	 * only if the kernel includes DCR support.

 Map the global registers */

		/*

		 * Yes, Freescale really did put global registers in the

		 * magic per-cpu area -- and they don't even show up in the

		 * non-magic per-cpu copies that this driver normally uses.

		/* Error interrupt mask register (EIMR) is required for

		 * handling individual device error interrupts. EIMR

		 * was added in MPIC version 4.1.

		 *

		 * Over here we reserve vector number space for error

		 * interrupt vectors. This space is stolen from the

		 * global vector number space, as in case of ipis

		 * and timer interrupts.

		 *

		 * Available vector space = intvec_top - 13, where 13

		 * is the number of vectors which have been consumed by

		 * ipis, timer interrupts and spurious.

	/*

	 * EPR is only available starting with v4.0.  To support

	 * platforms that don't know the MPIC version at compile-time,

	 * such as qemu-e500, turn off coreint if this MPIC doesn't

	 * support it.  Note that we never enable it if it wasn't

	 * requested in the first place.

	 *

	 * This is done outside the MPIC_FSL check, so that we

	 * also disable coreint if the MPIC node doesn't have

	 * an "fsl,mpic" compatible at all.  This will be the case

	 * with device trees generated by older versions of QEMU.

	 * fsl_version will be zero if MPIC_FSL is not set.

 Reset */

	/* When using a device-node, reset requests are only honored if the MPIC

	 * is allowed to reset.

 CoreInt */

	/*

	 * The MPIC driver will crash if there are more cores than we

	 * can initialize, so we may as well catch that problem here.

 Map the per-CPU registers */

	/*

	 * Read feature register.  For non-ISU MPICs, num sources as well. On

	 * ISU MPICs, sources are counted as ISUs are added

	/*

	 * By default, the last source number comes from the MPIC, but the

	 * device-tree and board support code can override it on buggy hw.

	 * If we get passed an isu_size (multi-isu MPIC) then we use that

	 * as a default instead of the value read from the HW.

 Initialize main ISU if none provided */

	/*

	 * FIXME: The code leaks the MPIC object and mappings here; this

	 * is very unlikely to fail but it ought to be fixed anyways.

 Display version */

 Set current processor priority to max */

		/*

		 * Timer group B is present at the latest in MPIC 3.1 (e.g.

		 * mpc8536).  It is not present in MPIC 2.0 (e.g. mpc8544).

		 * I don't know about the status of intermediate versions (or

		 * whether they even exist).

 Initialize timers to our reserved vectors and mask them for now */

 Initialize IPIs to our reserved vectors and mark them disabled for now */

 Do the HT PIC fixups on U3 broken mpic */

 start with vector = source number, and masked */

 check if protected */

 init hw */

 Init spurious vector */

 Disable 8259 passthrough, if supported */

 Set current processor priority to 0 */

 allocate memory to save mpic state */

 Check if this MPIC is chained from a parent interrupt controller */

 FSL mpic error interrupt initialization */

 	/* let the mpic know we want intrs. default affinity is 0xffffffff

	 * until changed via /proc. That's how it's done on x86. If we want

	 * it differently, then we should make sure we also change the default

	 * values of irq_desc[].affinity in irq.c.

 Set current processor priority to 0 */

 CONFIG_SMP */

 let the mpic know we don't want intrs.  */

 Set current processor priority to max */

	/* We need to EOI the IPI since not all platforms reset the MPIC

	 * on boot and new interrupts wouldn't get delivered otherwise.

 make sure we're sending something that translates to an IPI */

 Set target bit for core reset */

 Restore target bit after reset complete */

	/* Perform 15 EOI on each reset core to clear pending interrupts.

 CONFIG_SMP */

 we use the lowest bit in an inverted meaning */

 Enable and configure */

 end for loop */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * i8259 interrupt controller driver.

 RO, gives us the irq vector */

/*

 * Acknowledge the IRQ using either the PCI host bridge's interrupt

 * acknowledge feature or poll.  How i8259_init() is called determines

 * which is called.  It should be noted that polling is broken on some

 * IBM and Motorola PReP boxes so we must use the int-ack feature on them.

 Either int-ack or poll for the IRQ */

 Perform an interrupt acknowledge cycle on controller 1. */

 prepare for poll */

			/*

			 * Interrupt is cascaded so perform interrupt

			 * acknowledge on controller 2.

 prepare for poll */

		/*

		 * This may be a spurious interrupt.

		 *

		 * Read the interrupt status register (ISR). If the most

		 * significant bit is not set then there is no valid

		 * interrupt.

 ISR register */

 DUMMY */

 Non-specific EOI */

 Non-specific EOI to cascade */

 DUMMY */

 Non-specific EOI */

 We block the internal cascade */

	/* We use the level handler only for now, we might want to

	 * be more cautious here but that works for now

/**

 * i8259_init - Initialize the legacy controller

 * @node: device node of the legacy PIC (can be NULL, but then, it will match

 *        all interrupts, so beware)

 * @intack_addr: PCI interrupt acknowledge (real) address which will return

 *             	 the active irq from the 8259

 initialize the controller */

 Mask all first */

 init master interrupt controller */

 Start init sequence */

 Vector base */

 edge triggered, Cascade (slave) on IRQ2 */

 Select 8086 mode */

 init slave interrupt controller */

 Start init sequence */

 Vector base */

 edge triggered, Cascade (slave) on IRQ2 */

 Select 8086 mode */

 That thing is slow */

 always read ISR */

 Unmask the internal cascade */

 Set interrupt masks */

 create a legacy host */

 reserve our resources */

	/* XXX should we continue doing that ? it seems to cause problems

	 * with further requesting of PCI IO resources for that range...

	 * need to look into it.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Suspend/resume support

 *

 * Copyright 2009  MontaVista Software, Inc.

 *

 * Author: Anton Vorontsov <avorontsov@ru.mvista.com>

 At this point, the CPU is asleep. */

 Upon resume, wait for SLP bit to be clear. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support for indirect PCI bridges.

 *

 * Copyright (C) 1998 Gabriel Paubert.

	/*

	 * Note: the caller has already checked that offset is

	 * suitably aligned and that len is 1, 2 or 4.

 suppress setting of PCI_PRIMARY_BUS */

 Workaround for PCI_28 Errata in 440EPx/GRx */

	/*

	 * Note: the caller has already checked that offset is

	 * suitably aligned and that len is 1, 2 or 4.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * pmi driver

 *

 * (C) Copyright IBM Deutschland Entwicklung GmbH 2005

 *

 * PMI (Platform Management Interrupt) is a way to communicate

 * with the BMC (Baseboard Management Controller) via interrupts.

 * Unlike IPMI it is bidirectional and has a low latency.

 *

 * Author: Christian Krafft <krafft@de.ibm.com>

 SPDX-License-Identifier: GPL-2.0

/**

 * 	mpc5xxx_get_bus_frequency - Find the bus frequency for a device

 * 	@node:	device node

 *

 * 	Returns bus frequency (IPS on MPC512x, IPB on MPC52xx),

 * 	or 0 if the bus frequency cannot be found.

/*

 * Interrupt handling for GE FPGA based PIC

 *

 * Author: Martyn Welch <martyn.welch@ge.com>

 *

 * 2008 (c) GE Intelligent Platforms Embedded Systems, Inc.

 *

 * This file is licensed under the terms of the GNU General Public License

 * version 2.  This program is licensed "as is" without any warranty of any

 * kind, whether express or implied.

 Interrupt Controller Interface Registers */

/*

 * Interrupt Controller Handling

 *

 * The interrupt controller handles interrupts for most on board interrupts,

 * apart from PCI interrupts. For example on SBC610:

 *

 * 17:31 RO Reserved

 * 16    RO PCI Express Doorbell 3 Status

 * 15    RO PCI Express Doorbell 2 Status

 * 14    RO PCI Express Doorbell 1 Status

 * 13    RO PCI Express Doorbell 0 Status

 * 12    RO Real Time Clock Interrupt Status

 * 11    RO Temperature Interrupt Status

 * 10    RO Temperature Critical Interrupt Status

 * 9     RO Ethernet PHY1 Interrupt Status

 * 8     RO Ethernet PHY3 Interrupt Status

 * 7     RO PEX8548 Interrupt Status

 * 6     RO Reserved

 * 5     RO Watchdog 0 Interrupt Status

 * 4     RO Watchdog 1 Interrupt Status

 * 3     RO AXIS Message FIFO A Interrupt Status

 * 2     RO AXIS Message FIFO B Interrupt Status

 * 1     RO AXIS Message FIFO C Interrupt Status

 * 0     RO AXIS Message FIFO D Interrupt Status

 *

 * Interrupts can be forwarded to one of two output lines. Nothing

 * clever is done, so if the masks are incorrectly set, a single input

 * interrupt could generate interrupts on both output lines!

 *

 * The dual lines are there to allow the chained interrupts to be easily

 * passed into two different cores. We currently do not use this functionality

 * in this driver.

 *

 * Controller can also be configured to generate Machine checks (MCP), again on

 * two lines, to be attached to two different cores. It is suggested that these

 * should be masked out.

	/*

	 * See if we actually have an interrupt, call generic handling code if

	 * we do.

	/* Don't think we actually have to do anything to ack an interrupt,

	 * we just need to clear down the devices interrupt and it will go away

/* When an interrupt is being configured, this call allows some flexibilty

 * in deciding which irq_chip structure is used

 All interrupts are LEVEL sensitive */

/*

 * Initialisation of PIC, this should be called in BSP

 Map the devices registers into memory */

 Initialise everything as masked. */

 Map controller */

 Setup an irq_domain structure */

 Chain with parent controller */

/*

 * This is called when we receive an interrupt with apparently comes from this

 * chip - check, returning the highest interrupt generated or return 0.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2016,2017 IBM Corporation.

 Based on the similar routines in RTAS */

 seems appropriate for XIVE hcalls */

/*

 * Note: this call has a partition wide scope and can take a while to

 * complete. If it returns H_LONG_BUSY_* it should be retried

 * periodically.

 unused */

	/*

	 * No chip-id for the sPAPR backend. This has an impact how we

	 * pick a target. See xive_pick_irq_target().

	/*

	 * When the H_INT_ESB flag is set, the H_INT_ESB hcall should

	 * be used for interrupt management. Skip the remapping of the

	 * ESB pages which are not available.

 Full function page supports trigger */

 This can be called multiple time to change a queue configuration */

 If there's an actual queue page, clean it */

 Initialize the rest of the fields */

 TODO: add support for the notification page */

 Default is to always notify */

 Configure and enable the queue in HW */

 Ignore cascaded controllers for the moment */

 CONFIG_SMP */

/*

 * Perform an "ack" cycle on the current thread. Grab the pending

 * active priorities and update the CPPR to the most favored one.

	/*

	 * Perform the "Acknowledge O/S to Register" cycle.

	 *

	 * Let's speedup the access to the TIMA using the raw I/O

	 * accessor as we don't need the synchronisation routine of

	 * the higher level ones

 Synchronize subsequent queue accesses */

	/*

	 * Grab the CPPR and the "NSR" field which indicates the source

	 * of the interrupt (if any)

 Mark the priority pending */

		/*

		 * A new interrupt should never have a CPPR less favored

		 * than our current one.

 Update our idea of what the CPPR is */

 Only some debug on the TIMA settings */

 Nothing to do */;

 Specs are unclear on what this is doing */

 CONFIG_SMP */

/*

 * get max priority from "/ibm,plat-res-int-priorities"

	/* HW supports priorities in the range [0-7] and 0xFF is a

	 * wildcard priority used to mask. We scan the ranges reserved

	 * by the hypervisor to find the lowest priority we can use.

 Hypervisor only supports XIVE */

 Resource 1 is the OS ring TIMA */

 Feed the IRQ number allocator with the ranges given in the DT */

 Iterate the EQ sizes and pick one */

 Initialize XIVE core with our backend */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2016,2017 IBM Corporation.

 This can be called multiple time to change a queue configuration */

 If there's an actual queue page, clean it */

 Initialize the rest of the fields */

 Default flags */

 Escalation needed ? */

 Configure and enable the queue in HW */

		/*

		 * KVM code requires all of the above to be visible before

		 * q->qpage is set due to how it manages IPI EOIs

 Disable the queue in HW */

	/*

	 * We use the variant with no iounmap as this is called on exec

	 * from an IPI and iounmap isn't safe

	/*

	 * Old versions of skiboot can incorrectly return 0xffffffff to

	 * indicate no space, fix it up here.

 Allocate an IPI and populate info about it */

 CONFIG_SMP */

 Free the IPI */

 CONFIG_SMP */

 Switch the XIVE to emulation mode */

/*

 * Perform an "ack" cycle on the current thread, thus

 * grabbing the pending active priorities and updating

 * the CPPR to the most favored one.

 Perform the acknowledge hypervisor to register cycle */

 Synchronize subsequent queue accesses */

	/*

	 * Grab the CPPR and the "HE" field which indicates the source

	 * of the hypervisor interrupt (if any)

 Nothing to see here */

 Physical thread interrupt */

 Mark the priority pending */

		/*

		 * A new interrupt should never have a CPPR less favored

		 * than our current one.

 Update our idea of what the CPPR is */

 HV Pool interrupt (unused) */

 Legacy FW LSI (unused) */

 Check if pool VP already active, if it is, pull it */

 Enable the pool VP */

 Grab it's CAM value */

 Push it on the CPU (set LSMFB to 0xff to skip backlog scan) */

 Pull the pool VP from the CPU */

 Disable it */

 CONFIG_SMP */

 Allocate a pool big enough */

 Resource 1 is HV window */

 Read number of priorities */

 Iterate the EQ sizes and pick one */

 Do we support single escalation */

 Configure Thread Management areas for KVM */

 Resource 2 is OS window */

 Grab size of provisionning pages */

 Switch the XIVE to exploitation mode */

 Setup some dummy HV pool VPs */

 Initialize XIVE core with our backend */

		/*

		 * XXX TODO: Try to make the allocation local to the node where

		 * the chip resides.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2016,2017 IBM Corporation.

 We use only one priority for now */

 TIMA exported to KVM */

 Backend ops */

 Our global interrupt domain */

 The IPIs use the same logical irq number when on the same chip */

/*

 * Use early_cpu_to_node() for hot-plugged CPUs

 Xive state for each CPU */

 An invalid CPU target */

/*

 * Read the next entry in a queue, return its content if it's valid

 * or 0 if there is no new entry.

 *

 * The queue pointer is moved forward unless "just_peek" is set

 Check valid bit (31) vs current toggle polarity */

 If consuming from the queue ... */

 Next entry */

 Wrap around: flip valid toggle */

 Mask out the valid bit (31) */

/*

 * Scans all the queue that may have interrupts in them

 * (based on "pending_prio") in priority order until an

 * interrupt is found or all the queues are empty.

 *

 * Then updates the CPPR (Current Processor Priority

 * Register) based on the most favored interrupt found

 * (0xff if none) and return what was found (0 if none).

 *

 * If just_peek is set, return the most favored pending

 * interrupt if any but don't update the queue pointers.

 *

 * Note: This function can operate generically on any number

 * of queues (up to 8). The current implementation of the XIVE

 * driver only uses a single queue however.

 *

 * Note2: This will also "flush" "the pending_count" of a queue

 * into the "count" when that queue is observed to be empty.

 * This is used to keep track of the amount of interrupts

 * targetting a queue. When an interrupt is moved away from

 * a queue, we only decrement that queue count once the queue

 * has been observed empty to avoid races.

 Find highest pending priority */

 Try to fetch */

 Found something ? That's it */

			/*

			 * We should never get here; if we do then we must

			 * have failed to synchronize the interrupt properly

			 * when shutting it down.

 Clear pending bits */

		/*

		 * Check if the queue count needs adjusting due to

		 * interrupts being moved away. See description of

		 * xive_dec_target_count()

 If nothing was found, set CPPR to 0xff */

 Update HW CPPR to match if necessary */

/*

 * This is used to perform the magic loads from an ESB

 * described in xive-regs.h

 CONFIG_XMON */

	/*

	 * This can be called either as a result of a HW interrupt or

	 * as a "replay" because EOI decided there was still something

	 * in one of the queues.

	 *

	 * First we perform an ACK cycle in order to update our mask

	 * of pending priorities. This will also have the effect of

	 * updating the CPPR to the most favored pending interrupts.

	 *

	 * In the future, if we have a way to differentiate a first

	 * entry (on HW interrupt) from a replay triggered by EOI,

	 * we could skip this on replays unless we soft-mask tells us

	 * that a new HW interrupt occurred.

 Scan our queue(s) for interrupts */

 Return pending interrupt if any */

/*

 * After EOI'ing an interrupt, we need to re-check the queue

 * to see if another interrupt is pending since multiple

 * interrupts can coalesce into a single notification to the

 * CPU.

 *

 * If we find that there is indeed more in there, we call

 * force_external_irq_replay() to make Linux synthetize an

 * external interrupt on the next call to local_irq_restore().

/*

 * EOI an interrupt at the source. There are several methods

 * to do this depending on the HW version and source type

 If the XIVE supports the new "store EOI facility, use it */

	/*

	 * For LSIs, we use the "EOI cycle" special load rather than

	 * PQ bits, as they are automatically re-triggered in HW when

	 * still pending.

	/*

	 * Otherwise, we use the special MMIO that does a clear of

	 * both P and Q and returns the old Q. This allows us to then

	 * do a re-trigger if Q was set rather than synthesizing an

	 * interrupt in software

 Re-trigger if needed */

 irq_chip eoi callback, called with irq descriptor lock held */

	/*

	 * EOI the source if it hasn't been disabled and hasn't

	 * been passed-through to a KVM guest

	/*

	 * Clear saved_p to indicate that it's no longer occupying

	 * a queue slot on the target queue

 Check for more work in the queue */

/*

 * Helper used to mask and unmask an interrupt source.

	/*

	 * If the interrupt had P set, it may be in a queue.

	 *

	 * We need to make sure we don't re-enable it until it

	 * has been fetched from that queue and EOId. We keep

	 * a copy of that P state and use it to restore the

	 * ESB accordingly on unmask.

/*

 * Try to chose "cpu" as a new interrupt target. Increments

 * the queue accounting for that target if it's not already

 * full.

	/*

	 * Calculate max number of interrupts in that queue.

	 *

	 * We leave a gap of 1 just in case...

/*

 * Un-account an interrupt for a target CPU. We don't directly

 * decrement q->count since the interrupt might still be present

 * in the queue.

 *

 * Instead increment a separate counter "pending_count" which

 * will be substracted from "count" later when that CPU observes

 * the queue to be empty.

	/*

	 * We increment the "pending count" which will be used

	 * to decrement the target queue count whenever it's next

	 * processed and found empty. This ensure that we don't

	 * decrement while we still have the interrupt there

	 * occupying a slot.

 Find a tentative CPU target in a CPU mask */

 Pick up a starting point CPU in the mask based on  fuzz */

 Locate it */

 Sanity check */

 Remember first one to handle wrap-around */

	/*

	 * Now go through the entire mask until we find a valid

	 * target.

		/*

		 * We re-check online as the fallback case passes us

		 * an untested affinity mask

 Wrap around */

/*

 * Pick a target CPU for an interrupt. This is done at

 * startup or if the affinity is changed in a way that

 * invalidates the current target.

	/*

	 * If we have chip IDs, first we try to build a mask of

	 * CPUs matching the CPU and find a target in there

 Build a mask of matching chip IDs */

 Try to find a target */

 No chip IDs, fallback to using the affinity mask */

 Pick a target */

 Try again breaking affinity */

 Sanity check */

	/*

	 * Configure the logical number to be the Linux IRQ number

	 * and set the target queue

 Unmask the ESB */

 called with irq descriptor lock held */

 Mask the interrupt at the source */

	/*

	 * Mask the interrupt in HW in the IVT/EAS and set the number

	 * to be the "bad" IRQ number

 Is this valid ? */

	/*

	 * If existing target is already in the new mask, and is

	 * online then do nothing.

 Pick a new target */

 No target found */

 Sanity check */

	/*

	 * Only configure the irq if it's not currently passed-through to

	 * a KVM guest

 Give up previous target */

	/*

	 * We only support these. This has really no effect other than setting

	 * the corresponding descriptor bits mind you but those will in turn

	 * affect the resend function when re-enabling an edge interrupt.

	 *

	 * Set set the default to edge as explained in map().

	/*

	 * Double check it matches what the FW thinks

	 *

	 * NOTE: We don't know yet if the PAPR interface will provide

	 * the LSI vs MSI information apart from the device-tree so

	 * this check might have to move into an optional backend call

	 * that is specific to the native backend

 This should be only for MSIs */

	/*

	 * To perform a retrigger, we first set the PQ bits to

	 * 11, then perform an EOI.

/*

 * Caller holds the irq descriptor lock, so this won't be called

 * concurrently with xive_get_irqchip_state on the same interrupt.

	/*

	 * This is called by KVM with state non-NULL for enabling

	 * pass-through or NULL for disabling it

 Set it to PQ=10 state to prevent further sends */

 No target ? nothing to do */

			/*

			 * An untargetted interrupt should have been

			 * also masked at the source

		/*

		 * If P was set, adjust state to PQ=11 to indicate

		 * that a resend is needed for the interrupt to reach

		 * the guest. Also remember the value of P.

		 *

		 * This also tells us that it's in flight to a host queue

		 * or has already been fetched but hasn't been EOIed yet

		 * by the host. This it's potentially using up a host

		 * queue slot. This is important to know because as long

		 * as this is the case, we must not hard-unmask it when

		 * "returning" that interrupt to the host.

		 *

		 * This saved_p is cleared by the host EOI, when we know

		 * for sure the queue slot is no longer in use.

			/*

			 * Sync the XIVE source HW to ensure the interrupt

			 * has gone through the EAS before we change its

			 * target to the guest. That should guarantee us

			 * that we *will* eventually get an EOI for it on

			 * the host. Otherwise there would be a small window

			 * for P to be seen here but the interrupt going

			 * to the guest queue.

 No host target ? hard mask and return */

		/*

		 * Sync the XIVE source HW to ensure the interrupt

		 * has gone through the EAS before we change its

		 * target to the host.

		/*

		 * By convention we are called with the interrupt in

		 * a PQ=10 or PQ=11 state, ie, it won't fire and will

		 * have latched in Q whether there's a pending HW

		 * interrupt or not.

		 *

		 * First reconfigure the target.

		/*

		 * Then if saved_p is not set, effectively re-enable the

		 * interrupt with an EOI. If it is set, we know there is

		 * still a message in a host queue somewhere that will be

		 * EOId eventually.

		 *

		 * Note: We don't check irqd_irq_disabled(). Effectively,

		 * we *will* let the irq get through even if masked if the

		 * HW is still firing it in order to deal with the whole

		 * saved_p business properly. If the interrupt triggers

		 * while masked, the generic code will re-mask it anyway.

 Called with irq descriptor lock held. */

		/*

		 * The esb value being all 1's means we couldn't get

		 * the PQ state of the interrupt through mmio. It may

		 * happen, for example when querying a PHB interrupt

		 * while the PHB is in an error state. We consider the

		 * interrupt to be inactive in that case.

	/*

	 * Turn OFF by default the interrupt being mapped. A side

	 * effect of this check is the mapping the ESB page of the

	 * interrupt in the Linux address space. This prevents page

	 * fault issues in the crash handler which masks all

	 * interrupts.

 Handle possible race with unplug and drop stale IPIs */

	/*

	 * Nothing to do, we never mask/unmask IPIs, but the callback

	 * has to exist for the struct irq_chip.

/*

 * IPIs are marked per-cpu. We use separate HW interrupts under the

 * hood but associated with the same "linux" interrupt

		/*

		 * Map one IPI interrupt per node for all cpus of that node.

		 * Since the HW interrupt number doesn't have any meaning,

		 * simply use the node number.

 Check if we are already setup */

 Register the IPI */

 Grab an IPI from the backend, this will populate xc->hw_ipi */

	/*

	 * Populate the IRQ data in the xive_cpu structure and

	 * configure the HW / enable the IPIs.

 Unmask it */

 Disable the IPI and free the IRQ data */

 Already cleaned up ? */

 TODO: clear IPI mapping */

 Mask the IPI */

	/*

	 * Note: We don't call xive_cleanup_irq_data() to free

	 * the mappings as this is called from an IPI on kexec

	 * which is not a safe environment to call iounmap()

 Deconfigure/mask in the backend */

 Free the IPIs in the backend */

 Register the IPI */

 Allocate and setup IPI for the boot CPU */

 CONFIG_SMP */

	/*

	 * Mark interrupts as edge sensitive by default so that resend

	 * actually works. Will fix that up below if needed.

	/*

	 * If intsize is at least 2, we look for the type in the second cell,

	 * we assume the LSB indicates a level interrupt.

 No IRQ domain level information. To be done */

 TODO: call xive_irq_domain_map() */

		/*

		 * Mark interrupts as edge sensitive by default so that resend

		 * actually works. Will fix that up below if needed.

 allocates and sets handler data */

 We setup 1 queues for now with a 64k page */

 Setup EQs if not already */

 The backend might have additional things to do */

 Set CPPR to 0xff to enable flow of interrupts */

 This will have already been done on the boot CPU */

 Allocate per-CPU data and queues */

 Allocate and setup IPI for the new CPU */

 We assume local irqs are disabled */

 Check what's already in the CPU queue */

		/*

		 * We need to re-route that interrupt to its new destination.

		 * First get and lock the descriptor

		/*

		 * Ignore anything that isn't a XIVE irq and ignore

		 * IPIs, so can just be dropped.

		/*

		 * The IRQ should have already been re-routed, it's just a

		 * stale in the old queue, so re-trigger it in order to make

		 * it reach is new destination.

		/*

		 * Clear saved_p to indicate that it's no longer pending

		/*

		 * For LSIs, we EOI, this will cause a resend if it's

		 * still asserted. Otherwise do an MSI retrigger.

 Migrate interrupts away from the CPU */

 Set CPPR to 0 to disable flow of interrupts */

 Flush everything still in the queue */

 Re-enable CPPR  */

 Called if an interrupt occurs while the CPU is hot unplugged */

 CONFIG_HOTPLUG_CPU */

 CONFIG_SMP */

 Set CPPR to 0 to disable flow of interrupts */

 Get rid of IPI */

 Disable and free the queues */

 Allocate per-CPU data and queues */

 Get ready for interrupts */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * ICS backend for OPAL managed interrupts.

 *

 * Copyright 2011 IBM Corp.

#define DEBUG

	/*

	 * The generic MSI code returns with the interrupt disabled on the

	 * card, using the MSI mask bits. Firmware doesn't appear to unmask

	 * at that level, so we do it here by hand.

 unmask it */

 Patched at init time */

 Register ourselves */

	/* We need to patch our irq chip's EOI to point to the

	 * right ICP

 Find native ICS in the device-tree */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2011 IBM Corporation.

 Make sure all previous accesses are ordered before IPI sending */

 Clear any pending IPI */

	/* We take the ipi irq but and never return so we

	 * need to EOI the IPI, but want to leave our priority 0

	 *

	 * should we check all the other interrupts too?

	 * should we be flagging idle loop instead?

	 * or creating some task to be scheduled?

 We don't have a linux mapping, so have rtas mask it. */

 We might learn about it later, so EOI it */

 CONFIG_SMP */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2011 IBM Corporation.

 Globals common to all ICP/ICS implementations */

 Find the server numbers for the boot cpu. */

	/* Global interrupt distribution server is specified in the last

	 * entry of "ibm,ppc-interrupt-gserver#s" property. Get the last

	 * entry fom this property for current boot cpu id and use it as

	 * default distribution server

/* GIQ stuff, currently only supported on RTAS setups, will have

 * to be sorted properly for bare metal

	/*

	 * IPIs are marked IRQF_PERCPU. The handler was set in map.

 Register all the IPIs */

 Setup cause_ipi callback based on which ICP is used */

 CONFIG_SMP */

	/*

	 * we have to reset the cppr index to 0 because we're

	 * not going to return from the IPI

	/*

	 * Some machines need to have at least one cpu in the GIQ,

	 * so leave the master cpu in the group.

 Interrupts are disabled. */

 If we used to be the default server, move to the new "boot_cpuid" */

 Reject any interrupt that was queued to us... */

 Remove ourselves from the global interrupt queue */

 We can't set affinity on ISA interrupts */

 We only need to migrate enabled IRQS */

 We need a mapping in the XICS IRQ domain */

 We need to get IPIs still. */

 Locate interrupt server */

		/* We only support delivery to all cpus or to one cpu.

		 * The irq has to be migrated only in the single cpu

		 * case.

 This is expected during cpu offline. */

 Reset affinity to all cpus */

 Allow "sufficient" time to drop any inflight IRQ's */

	/*

	 * Allow IPIs again. This is done at the very end, after migrating all

	 * interrupts, the expectation is that we'll only get woken up by an IPI

	 * interrupt beyond this point, but leave externals masked just to be

	 * safe. If we're using icp-opal this may actually allow all

	 * interrupts anyway, but that should be OK.

 CONFIG_HOTPLUG_CPU */

/*

 * For the moment we only implement delivery to all cpus or one cpu.

 *

 * If the requested affinity is cpu_all_mask, we set global affinity.

 * If not we set it to the first cpu in the mask, even if multiple cpus

 * are set. This is so things like irqbalance (which set core and package

 * wide affinities) do the right thing.

 *

 * We need to fix this to implement support for the links

	/*

	 * Workaround issue with some versions of JS20 firmware that

	 * deliver interrupts to cpus which haven't been started. This

	 * happens when using the maxcpus= boot option.

 CONFIG_SMP */

 Dummies */

 Patched at init time */

	/*

	 * Mark interrupts as edge sensitive by default so that resend

	 * actually works. The device-tree parsing will turn the LSIs

	 * back to level.

 Don't call into ICS for IPIs */

 Let the ICS be the chip data for the XICS domain. For ICS native */

	/*

	 * If intsize is at least 2, we look for the type in the second cell,

	 * we assume the LSB indicates a level interrupt.

	/*

	 * We only support these. This has really no effect other than setting

	 * the corresponding descriptor bits mind you but those will in turn

	 * affect the resend function when re-enabling an edge interrupt.

	 *

	 * Set set the default to edge as explained in map().

	/*

	 * We need to push a dummy CPPR when retriggering, since the subsequent

	 * EOI will try to pop it. Passing 0 works, as the function hard codes

	 * the priority value anyway.

 Tell the core to do a soft retrigger */

	/* We fetch the interrupt server size from the first ICS node

	 * we find if any

 Fist locate ICP */

 Copy get_irq callback over to ppc_md */

 Patch up IPI chip EOI */

 Now locate ICS */

 Initialize common bits */

 SPDX-License-Identifier: GPL-2.0

 RTAS service tokens */

 Now unmask the interrupt (often a no-op) */

 unmask it */

 Have to set XIVE to 0xff to be able to remove a slot */

 Patched at init time */

 Check if RTAS knows about this interrupt */

	/* IBM machines have interrupt parents of various funky types for things

	 * like vdevices, events, etc... The trick we use here is to match

	 * everything here except the legacy 8259 which is compatible "chrp,iic"

 Only one global & state struct ics */

	/* We enable the RTAS "ICS" if RTAS is present with the

	 * appropriate tokens

	/* We need to patch our irq chip's EOI to point to the

	 * right ICP

 Register ourselves */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2011 IBM Corporation.

 Handled an interrupt latched by KVM */

 Clear any pending IPI */

	/* We take the ipi irq but and never return so we

	 * need to EOI the IPI, but want to leave our priority 0

	 *

	 * should we check all the other interrupts too?

	 * should we be flagging idle loop instead?

	 * or creating some task to be scheduled?

 We don't have a linux mapping, so have rtas mask it. */

 We might learn about it later, so EOI it */

	/*

	 * Currently not used to send IPIs to another CPU

	 * on the same core. Only caller is KVM real mode.

	 * Need the physical address of the XICS to be

	 * previously saved in kvm_hstate in the paca.

	/*

	 * Just like the cause_ipi functions, it is required to

	 * include a full barrier before causing the IPI.

/*

 * Called when an interrupt is received on an off-line CPU to

 * clear the interrupt, so that the CPU can go back to nap mode.

 Clear pending IPI */

 EOI the interrupt */

 CONFIG_SMP */

	/* This may look gross but it's good enough for now, we don't quite

	 * have a hard -> linux processor id matching.

	/* Fail, skip that CPU. Don't print, it's normal, some XICS come up

	 * with way more entries in there than you have CPUs

	/* This code does the theorically broken assumption that the interrupt

	 * server numbers are the same as the hard CPU numbers.

	 * This happens to be the case so far but we are playing with fire...

	 * should be fixed one of these days. -BenH.

	/* Do that ever happen ? we'll know soon enough... but even good'old

	 * f80 does have that property ..

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * ICS backend for OPAL managed interrupts.

 *

 * Copyright 2011 IBM Corp.

 No link for now */

 No link for now */

 Have to set XIVE to 0xff to be able to remove a slot */

 Patched at init time */

 Check if HAL knows about this interrupt */

 Check if HAL knows about this interrupt */

 Check if HAL knows about this interrupt */

 Only one global & state struct ics */

	/* We need to patch our irq chip's EOI to point to the

	 * right ICP

 Register ourselves */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2016 IBM Corporation.

 Clear any pending IPI */

	/*

	 * We take the ipi irq but and never return so we need to EOI the IPI,

	 * but want to leave our priority 0.

	 *

	 * Should we check all the other interrupts too?

	 * Should we be flagging idle loop instead?

	 * Or creating some task to be scheduled?

 Handle an interrupt latched by KVM first */

 Then ask OPAL */

 We don't have a linux mapping, so have rtas mask it. */

 We might learn about it later, so EOI it */

	/*

	 * Here be dragons. The caller has asked to allow only IPI's and not

	 * external interrupts. But OPAL XIVE doesn't support that. So instead

	 * of allowing no interrupts allow all. That's still not right, but

	 * currently the only caller who does this is xics_migrate_irqs_away()

	 * and it works in that case.

	/*

	 * EOI tells us whether there are more interrupts to fetch.

	 *

	 * Some HW implementations might not be able to send us another

	 * external interrupt in that case, so we force a replay.

/*

 * Called when an interrupt is received on an off-line CPU to

 * clear the interrupt, so that the CPU can go back to nap mode.

 Clear pending IPI */

 EOI the interrupt */

 CONFIG_SMP */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * arch/powerpc/math-emu/math_efp.c

 *

 * Copyright (C) 2006-2008, 2010 Freescale Semiconductor, Inc.

 *

 * Author: Ebony Zhu,	<ebony.zhu@freescale.com>

 *         Yu Liu,	<yu.liu@freescale.com>

 *

 * Derived from arch/alpha/math-emu/math.c

 *              arch/powerpc/math-emu/math.c

 *

 * Description:

 * This file is the exception handler to make E500 SPE instructions

 * fully comply with IEEE-754 floating point standard.

 not an spe instruction */

	/*

	 * If the "invalid" exception sticky bit was set by the

	 * processor for non-finite input, but was not set before the

	 * instruction being emulated, clear it.  Likewise for the

	 * "underflow" bit, which may have been set by the processor

	 * for exact underflow, not just inexact underflow when the

	 * flag should be set for IEEE 754 semantics.  Other sticky

	 * exceptions will only be set by the processor when they are

	 * correct according to IEEE 754 semantics, and we must not

	 * clear sticky bits that were already set before the emulated

	 * instruction as they represent the user-visible sticky

	 * exception status.  "inexact" traps to kernel are not

	 * required for IEEE semantics and are not enabled by default,

	 * so the "inexact" sticky bit may have been set by a previous

	 * instruction without the kernel being aware of it.

 according to e500 cpu a005 erratum, reissue efp inst */

 not an spe instruction */

 No need to round if the result is exact */

		/*

		 * These instructions always round to zero,

		 * independent of the rounding mode.

 Recover the sign of a zero result if possible.  */

 Recover the sign of a zero result if possible.  */

 Recover the sign of a zero result if possible.  */

	/* Since SPE instructions on E500 core can handle round to nearest

	 * and round toward zero with IEEE-754 complied, we just need

	 * to handle round toward +Inf and round toward -Inf by software.

 Z > 0, choose Z1 */

 round to -Inf */

 Z < 0, choose Z2 */

 Z < 0, choose Z2 */

 Z > 0, choose Z1 */

 Z > 0, choose Z1 */

 round to -Inf */

 Z < 0, choose Z2 */

 Z < 0, choose Z2 */

 Z_low > 0, choose Z1 */

 Z_high word > 0, choose Z1 */

 round to -Inf */

 Z_low < 0, choose Z2 */

 Z_low < 0, choose Z2 */

 Z_high < 0, choose Z2 */

 Z_high < 0, choose Z2 */

		/*

		 * E500 revision below 1.1, 2.3, 3.1, 4.1, 5.1

		 * need cpu a005 errata workaround

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1999  Eddie C. Dost  (ecd@atecom.com)

 The instructions list which may be not implemented by a hardware FPU */

 Optional */

   31 */

   48 */

   49 */

   50 */

   51 */

   52 */

   53 */

   54 */

   55 */

   59 */

   63 */

 Opcode 31: */

 X-Form: */

  535 */

  567 */

  599 */

  631 */

  663 */

  695 */

  727 */

  759 */

  983 */

 Opcode 59: */

 A-Form: */

   18 */

   20 */

   21 */

   22 */

   24 */

   25 */

   26 */

   28 */

   29 */

   30 */

   31 */

 Opcode 63: */

 A-Form: */

   18 */

   20 */

   21 */

   22 */

   23 */

   24 */

   25 */

   26 */

   28 */

   29 */

   30 */

   31 */

 X-Form: */

    0	*/

   12 */

   14 */

   15 */

   32 */

   38 */

   40 */

   64 */

   70 */

   72 */

  134 */

  136 */

  264 */

  583 */

  711 */

	fpscr &= ~(FPSCR_VX);

	/*

	 * If we support a HW FPU, we need to ensure the FP state

	 * is flushed into the thread_struct before attempting

	 * emulation

 SPDX-License-Identifier: GPL-2.0

	/* The bit order of exception enables and exception status

	 * is the same. Simply shift and mask to check for enabled

	 * exceptions.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 This has so very few changes over libgcc2's __udivmoddi4 it isn't funny.  */

 0q = nn / 0D */

 Remainder in n0.  */

 qq = NN / 0d */

 Divide intentionally by zero.  */

 Remainder in n0.  */

 UDIV_NEEDS_NORMALIZATION */

 0q = nn / 0D */

	      /* Normalize, i.e. make the most significant bit of the

 Remainder in n0 >> bm.  */

 qq = NN / 0d */

 Divide intentionally by zero.  */

	      /* From (n1 >= d0) /\ (the most significant bit of d0 is set),

		 conclude (the most significant bit of n1 is set) /\ (the

		 leading quotient digit q1 = 1).



		 This special case is necessary, not an optimization.

 Normalize.  */

 n1 != d0...  */

 Remainder in n0 >> bm.  */

 UDIV_NEEDS_NORMALIZATION */

 00 = nn / DD */

 Remainder in n1n0.  */

 0q = NN / dd */

	      /* From (n1 >= d1) /\ (the most significant bit of d1 is set),

		 conclude (the most significant bit of n1 is set) /\ (the

		 quotient digit q0 = 0 or 1).



	      /* The condition on the next line takes advantage of that

 Normalize.  */

 Remainder in (n1n0 - m1m0) >> bm.  */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Hypervisor supplied "24x7" performance counter support

 *

 * Author: Cody P Schafer <cody@linux.vnet.ibm.com>

 * Copyright 2014 IBM Corporation.

 Version of the 24x7 hypervisor API that we should use in this machine. */

 Whether we have to aggregate result data for some domains. */

 fall through */

/*

 * The Processor Module Information system parameter allows transferring

 * of certain processor module information from the platform to the OS.

 * Refer PAPR+ document to get parameter token value as '43'.

 Physical sockets */

 Physical chips per socket*/

 Physical cores per chip */

/*

 * read_24x7_sys_info()

 * Retrieve the number of sockets and chips per socket and cores per

 * chip details through the get-system-parameter rtas call.

	/*

	 * Making system parameter: chips and sockets and cores per chip

	 * default to 1.

 Domains for which more than one result element are returned for each event. */

 POWER8 doesn't support virtual domains. */

/*

 * TODO: Merging events:

 * - Think of the hcall as an interface to a 4d array of counters:

 *   - x = domains

 *   - y = indexes in the domain (core, chip, vcpu, node, etc)

 *   - z = offset into the counter space

 *   - w = lpars (guest vms, "logical partitions")

 * - A single request is: x,y,y_last,z,z_last,w,w_last

 *   - this means we can retrieve a rectangle of counters in y,z for a single x.

 *

 * - Things to consider (ignoring w):

 *   - input  cost_per_request = 16

 *   - output cost_per_result(ys,zs)  = 8 + 8 * ys + ys * zs

 *   - limited number of requests per hcall (must fit into 4K bytes)

 *     - 4k = 16 [buffer header] - 16 [request size] * request_count

 *     - 255 requests per hcall

 *   - sometimes it will be more efficient to read extra data and discard

/*

 * Example usage:

 *  perf stat -e 'hv_24x7/domain=2,offset=8,vcpu=0,lpar=0xffffffff/'

 u3 0-6, one of HV_24X7_PERF_DOMAIN */

 u16 */

 u32, see "data_offset" */

 u16 */

 .attrs is set in init */

 .attrs is set in init */

 .attrs is set in init */

/*

 * request_buffer and result_buffer are not required to be 4k aligned,

 * but are not allowed to cross any 4k boundary. Aligning them to 4k is

 * the simplest way to ensure that.

/*

 * Things we don't check:

 *  - padding for desc, name, and long/detailed desc is required to be '\0'

 *    bytes.

 *

 *  Return NULL if we pass end,

 *  Otherwise return the address of the byte just following the event.

/*

 * Each event we find in the catalog, will have a sysfs entry. Format the

 * data for this sysfs entry based on the event's domain.

 *

 * Events belonging to the Chip domain can only be monitored in that domain.

 * i.e the domain for these events is a fixed/knwon value.

 *

 * Events belonging to the Core domain can be monitored either in the physical

 * core or in one of the virtual CPU domains. So the domain value for these

 * events must be specified by the user (i.e is a required parameter). Format

 * the Core events with 'domain=?' so the perf-tool can error check required

 * parameters.

 *

 * NOTE: For the Core domain events, rather than making domain a required

 *	 parameter we could default it to PHYS_CORE and allowe users to

 *	 override the domain to one of the VCPU domains.

 *

 *	 However, this can make the interface a little inconsistent.

 *

 *	 If we set domain=2 (PHYS_CHIP) and allow user to override this field

 *	 the user may be tempted to also modify the "offset=x" field in which

 *	 can lead to confusing usage. Consider the HPM_PCYC (offset=0x18) and

 *	 HPM_INST (offset=0x20) events. With:

 *

 *		perf stat -e hv_24x7/HPM_PCYC,offset=0x20/

 *

 *	we end up monitoring HPM_INST, while the command line has HPM_PCYC.

 *

 *	By not assigning a default value to the domain for the Core events,

 *	we can have simple guidelines:

 *

 *		- Specifying values for parameters with "=?" is required.

 *

 *		- Specifying (i.e overriding) values for other parameters

 *		  is undefined.

 Avoid trusting fw to NUL terminate strings */

/*

 * Allocate and initialize strings representing event attributes.

 *

 * NOTE: The strings allocated here are never destroyed and continue to

 *	 exist till shutdown. This is to allow us to create as many events

 *	 from the catalog as possible, even if we encounter errors with some.

 *	 In case of changes to error paths in future, these may need to be

 *	 freed by the caller.

 If there isn't a description, don't create the sysfs file */

 If there isn't a description, don't create the sysfs file */

 */

 Figure out where to put new node */

 Add new node and rebalance tree. */

 data->ct */

	/*

	 * the strings we point to are in the giant block of memory filled by

	 * the catalog, and are freed separately.

/*

 * ensure the event structure's sizes are self consistent and don't cause us to

 * read outside of the event

 *

 * On success, return the event length in bytes.

 * Otherwise, return -1 (and print as appropriate).

/*

 * Return true incase of invalid or dummy events with names like RESERVED*

	/*

	 * event data can span several pages, events can cross between these

	 * pages. Use vmalloc to make this easier.

	/*

	 * using vmalloc_to_phys() like this only works if PAGE_SIZE is

	 * divisible by 4096

	/*

	 * scan the catalog to determine the number of attributes we need, and

	 * verify it at the same time.

 Iterate over the catalog filling in the attribute vector */

		/*

		 * these are the only "bad" events that are intermixed and that

		 * we can ignore without issue. make sure to skip them here

 real length varies */);

/*

 * Start the process for a new H_GET_24x7_DATA hcall.

 memset above set request_buffer->num_requests to 0 */

/*

 * Commit (i.e perform) the H_GET_24x7_DATA hcall using the data collected

 * by 'init_24x7_request()' and 'add_event_to_24x7_request()'.

	/*

	 * NOTE: Due to variable number of array elements in request and

	 *	 result buffer(s), sizeof() is not reliable. Use the actual

	 *	 allocated buffer size, H24x7_DATA_BUFFER_SIZE.

/*

 * Add the given @event to the next slot in the 24x7 request_buffer.

 *

 * Note that H_GET_24X7_DATA hcall allows reading several counters'

 * values in a single HCALL. We expect the caller to add events to the

 * request buffer one by one, make the HCALL and process the results.

/**

 * get_count_from_result - get event count from all result elements in result

 *

 * If the event corresponding to this result needs aggregation of the result

 * element values, then this function does that.

 *

 * @event:	Event associated with @res.

 * @resb:	Result buffer containing @res.

 * @res:	Result to work on.

 * @countp:	Output variable containing the event count.

 * @next:	Optional output variable pointing to the next result in @resb.

	/*

	 * We can bail out early if the result is empty.

	/*

	 * Since we always specify 1 as the maximum for the smallest resource

	 * we're requesting, there should to be only one element per result.

	 * Except when an event needs aggregation, in which case there are more.

 Go through the result elements in the result. */

 The next result is after the last result element. */

 process result from hcall */

 Not our event */

 Unused areas must be 0 */

 no branch sampling */

 offset must be 8 byte aligned */

 Physical domains & other lpars require extra capabilities */

 Get the initial value of the counter for this event */

 We checked this in event init, shouldn't fail here... */

	/*

	 * If in a READ transaction, add this counter to the list of

	 * counters to read during the next HCALL (i.e commit_txn()).

	 * If not in a READ transaction, go ahead and make the HCALL

	 * to read this counter by itself.

			/*

			 * Associate the event with the HCALL request index,

			 * so ->commit_txn() can quickly find/update count.

/*

 * 24x7 counters only support READ transactions. They are

 * always counting and dont need/support ADD transactions.

 * Cache the flags, but otherwise ignore transactions that

 * are not PERF_PMU_TXN_READ.

 We should not be called if we are already in a txn */

/*

 * Clean up transaction state.

 *

 * NOTE: Ignore state of request and result buffers for now.

 *	 We will initialize them during the next read/txn.

/*

 * 24x7 counters only support READ transactions. They are always counting

 * and dont need/support ADD transactions. Clear ->txn_flags but otherwise

 * ignore transactions that are not of type PERF_PMU_TXN_READ.

 *

 * For READ transactions, submit all pending 24x7 requests (i.e requests

 * that were queued by h_24x7_event_read()), to the hypervisor and update

 * the event counts.

 Go through results in the result buffer to update event counts. */

/*

 * 24x7 counters only support READ transactions. They are always counting

 * and dont need/support ADD transactions. However, regardless of type

 * of transaction, all we need to do is cleanup, so we don't have to check

 * the type of transaction.

 Check if exiting cpu is used for collecting 24x7 events */

 Find a new cpu to collect 24x7 events */

 Migrate 24x7 events to the new target */

 POWER8 only supports v1, while POWER9 only supports v2. */

 SMT8 in POWER9 needs to aggregate result elements. */

 sampling not supported */

 init cpuhotplug */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0+



 Copyright 2019 Madhavan Srinivasan, IBM Corporation.

/*

 * Raw event encoding:

 *

 *        60        56        52        48        44        40        36        32

 * | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - |

 *

 *        28        24        20        16        12         8         4         0

 * | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - |

 *                                 [ pmc ]                       [    pmcxsel    ]

/*

 * Event codes defined in ISA v3.0B

 Cycles, alternate code */

 One or more instructions completed in a cycle */

 Floating-point instruction completed */

 Instruction ERAT/L1-TLB miss */

 All instructions completed and none available */

 A load-type instruction completed (ISA v3.0+) */

 Instruction completed, alternate code (ISA v3.0+) */

 A store-type instruction completed */

 Instruction Dispatched */

 Run_cycles */

 Data ERAT/L1-TLB miss/reload */

 Taken branch completed */

 Demand iCache Miss */

 L1 Dcache reload from memory */

 L1 Dcache store miss */

 Alternate code for PM_INST_DISP */

 Branch direction or target mispredicted */

 Data TLB miss/reload */

 Demand LD - L3 Miss (not L2 hit and not L3 hit) */

 L1 Dcache load miss */

 Cycle when instruction(s) dispatched */

 Branch or branch target mispredicted */

 Instructions completed with run latch set */

 Instruction TLB miss/reload */

 Load data not cached */

 Instructions */

 Cycles */

 Table of alternatives, sorted in increasing order of column 0 */

 Note that in each row, column 0 must be the smallest */

/*

 * Table of generalized cache-related events.

 * 0 means not supported, -1 means nonsensical, other values

 * are event codes.

/*

 * We set MMCR0[CC5-6RUN] so we can use counters 5 and 6 for

 * PM_INST_CMPL and PM_CYC.

	/*

	 * From ISA v2.07 on, PMU features are architected;

	 * we require >= v3.0 because (a) that has PM_LD_CMPL and

	 * PM_INST_CMPL_ALT, which v2.07 doesn't have, and

	 * (b) we don't expect any non-IBM Power ISA

	 * implementations that conform to v2.07 but not v3.0.

 Tell userspace that EBB is supported */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter support for POWER9 processors.

 *

 * Copyright 2009 Paul Mackerras, IBM Corporation.

 * Copyright 2013 Michael Ellerman, IBM Corporation.

 * Copyright 2016 Madhavan Srinivasan, IBM Corporation.

/*

 * Raw event encoding for Power9:

 *

 *        60        56        52        48        44        40        36        32

 * | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - |

 *   | | [ ]                       [ ] [      thresh_cmp     ]   [  thresh_ctl   ]

 *   | |  |                         |                                     |

 *   | |  *- IFM (Linux)            |	               thresh start/stop -*

 *   | *- BHRB (Linux)              *sm

 *   *- EBB (Linux)

 *

 *        28        24        20        16        12         8         4         0

 * | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - |

 *   [   ] [  sample ]   [cache]   [ pmc ]   [unit ]   []    m   [    pmcxsel    ]

 *     |        |           |                          |     |

 *     |        |           |                          |     *- mark

 *     |        |           *- L1/L2/L3 cache_sel      |

 *     |        |                                      |

 *     |        *- sampling mode for marked events     *- combine

 *     |

 *     *- thresh_sel

 *

 * Below uses IBM bit numbering.

 *

 * MMCR1[x:y] = unit    (PMCxUNIT)

 * MMCR1[24]   = pmc1combine[0]

 * MMCR1[25]   = pmc1combine[1]

 * MMCR1[26]   = pmc2combine[0]

 * MMCR1[27]   = pmc2combine[1]

 * MMCR1[28]   = pmc3combine[0]

 * MMCR1[29]   = pmc3combine[1]

 * MMCR1[30]   = pmc4combine[0]

 * MMCR1[31]   = pmc4combine[1]

 *

 * if pmc == 3 and unit == 0 and pmcxsel[0:6] == 0b0101011

 *	MMCR1[20:27] = thresh_ctl

 * else if pmc == 4 and unit == 0xf and pmcxsel[0:6] == 0b0101001

 *	MMCR1[20:27] = thresh_ctl

 * else

 *	MMCRA[48:55] = thresh_ctl   (THRESH START/END)

 *

 * if thresh_sel:

 *	MMCRA[45:47] = thresh_sel

 *

 * if thresh_cmp:

 *	MMCRA[9:11] = thresh_cmp[0:2]

 *	MMCRA[12:18] = thresh_cmp[3:9]

 *

 * MMCR1[16] = cache_sel[2]

* MMCR1[17] = cache_sel[3]

 *

 * if mark:

 *	MMCRA[63]    = 1		(SAMPLE_ENABLE)

 *	MMCRA[57:59] = sample[0:2]	(RAND_SAMP_ELIG)

*	MMCRA[61:62] = sample[3:4]	(RAND_SAMP_MODE)

 *

 * if EBB and BHRB:

 *	MMCRA[32:33] = IFM

 *

 * MMCRA[SDAR_MODE]  = sm

/*

 * Some power9 event codes.

 MMCRA IFM bits - POWER9 */

 Nasty Power9 specific hack */

 PowerISA v2.07 format attribute structure*/

 Table of alternatives, sorted by column 0 */

	/* BHRB and regular PMU events share the same privilege state

	 * filter configuration. BHRB is always recorded along with a

	 * regular PMU event. As the privilege state filter is handled

	 * in the basic PMC configuration of the accompanying regular

	 * PMU event, we ignore any separate BHRB specific request.

 No branch filter requested */

 Invalid branch filter options - HW does not support */

 Every thing else is unsupported */

 Enable BHRB filter in PMU */

/*

 * Table of generalized cache-related events.

 * 0 means not supported, -1 means nonsensical, other values

 * are event codes.

 Comes from cpu_specs[] */

 Blacklist events */

 Set the PERF_REG_EXTENDED_MASK here */

 Tell userspace that EBB is supported */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance event support - PPC 8xx

 *

 * Copyright 2016 Christophe Leroy, CS Systemes d'Information

 If it was the last user, stop counting to avoid useles overhead */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter support for POWER7 processors.

 *

 * Copyright 2009 Paul Mackerras, IBM Corporation.

/*

 * Bits in event code for POWER7

 PMC number (1-based) for direct events */

 TTMMUX number and setting - unit select */

 Combined event bit */

 L2 event select */

/*

 * Bits in MMCR1 for POWER7

/*

 * Power7 event codes.

/*

 * Layout of constraint bits:

 * 6666555555555544444444443333333333222222222211111111110000000000

 * 3210987654321098765432109876543210987654321098765432109876543210

 *                                              < ><  ><><><><><><>

 *                                              L2  NC P6P5P4P3P2P1

 *

 * L2 - 16-18 - Required L2SEL value (select field)

 *

 * NC - number of counters

 *     15: NC error 0x8000

 *     12-14: number of events needing PMC1-4 0x7000

 *

 * P6

 *     11: P6 error 0x800

 *     10-11: Count of events needing PMC6

 *

 * P1..P5

 *     0-9: Count of events needing PMC1..PMC5

 need a counter from PMC1-4 set */

 L2SEL must be identical across events */

 at most 2 alternatives for any event */

 PM_INST_DISP */

 PM_RUN_CYC */

 PM_RUN_INST_CMPL */

/*

 * Scan the alternatives table for a match and return the

 * index into the alternatives table if found, else -1.

 this only handles the 4x decode events */

		/*

		 * We're only counting in RUN state,

		 * so PM_CYC is equivalent to PM_RUN_CYC

		 * and PM_INST_CMPL === PM_RUN_INST_CMPL.

		 * This doesn't include alternatives that don't provide

		 * any extra flexibility in assigning PMCs.

 PM_CYC */

 PM_RUN_CYC */

 PM_RUN_CYC */

 PM_PPC_CMPL */

 PM_RUN_INST_CMPL */

 PM_RUN_INST_CMPL */

 PM_PPC_CMPL */

/*

 * Returns 1 if event counts things relating to marked instructions

 * and thus needs the MMCRA_SAMPLE_ENABLE bit set, or 0 if not.

 trim off edge/level bit */

 First pass to count resource use */

 Second pass: assign PMCs, set all MMCR1 fields */

 Bus event or any-PMC direct event */

 Direct or decoded event */

 L2 events */

 Return MMCRx values */

/*

 * Table of generalized cache-related events.

 * 0 means not supported, -1 means nonsensical, other values

 * are event codes.

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter support for e6500 family processors.

 *

 * Author: Priyanka Jain, Priyanka.Jain@freescale.com

 * Based on e500-pmu.c

 * Copyright 2013 Freescale Semiconductor, Inc.

 * Copyright 2008-2009 Paul Mackerras, IBM Corporation.

/*

 * Map of generic hardware event types to hardware events

 * Zero if unsupported

/*

 * Table of generalized cache-related events.

 * 0 means not supported, -1 means nonsensical, other values

 * are event codes.

RESULT_ACCESS		RESULT_MISS */

RESULT_ACCESS		RESULT_MISS */

	/*

	 * Assuming LL means L2, it's not a good match for this model.

	 * It does not have separate read/write events (but it does have

	 * separate instruction/data events).

RESULT_ACCESS		RESULT_MISS */

	/*

	 * There are data/instruction MMU misses, but that's a miss on

	 * the chip's internal level-one TLB which is probably not

	 * what the user wants.  Instead, unified level-two TLB misses

	 * are reported here.

RESULT_ACCESS		RESULT_MISS */

RESULT_ACCESS		RESULT_MISS */

 RESULT_ACCESS	RESULT_MISS */

 Upper half of event id is PMLCb, for threshold events */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * In-Memory Collection (IMC) Performance Monitor counter support.

 *

 * Copyright (C) 2017 Madhavan Srinivasan, IBM Corporation.

 *           (C) 2017 Anju T Sudhakar, IBM Corporation.

 *           (C) 2017 Hemant K Shaw, IBM Corporation.

 Nest IMC data structures and variables */

/*

 * Used to avoid races in counting the nest-pmu units during hotplug

 * register and unregister

 Core IMC data structures and variables */

 Thread IMC data structures and variables */

 Trace IMC data structures */

/*

 * Global data structure used to avoid races between thread,

 * core and trace-imc

 Format attribute for imc trace-mode */

 Get the cpumask printed to a buffer "buf" */

 device_str_attr_create : Populate event "name" and string "str" in attribute */

 Add the base_reg value to the "reg" */

/*

 * imc_free_events: Function to cleanup the events list, having

 * 		    "nr_entries".

 Nothing to clean, return */

/*

 * update_events_in_group: Update the "events" information in an attr_group

 *                         and assign the attr_group to the pmu "pmu".

 Did not find any node with a given phandle */

 Get a count of number of child nodes */

 Get the event prefix */

 Get a global unit and scale data if available */

 "reg" property gives out the base offset of the counters data */

 Allocate memory for the events */

 Parse the events and update the struct */

 Allocate memory for attribute group */

	/*

	 * Allocate memory for attributes.

	 * Since we have count of events for this pmu, we also allocate

	 * memory for the scale and unit attribute for now.

	 * "ct" has the total event structs added from the events-parent node.

	 * So allocate three times the "ct" (this includes event, event_scale and

	 * event_unit).

 Save the event attribute */

 get_nest_pmu_ref: Return the imc_pmu_ref struct for the given node */

	/*

	 * Check in the designated list for this cpu. Dont bother

	 * if not one of them.

	/*

	 * Check whether nest_imc is registered. We could end up here if the

	 * cpuhotplug callback registration fails. i.e, callback invokes the

	 * offline path for all successfully registered nodes. At this stage,

	 * nest_imc pmu will not be registered and we should return here.

	 *

	 * We return with a zero since this is not an offline failure. And

	 * cpuhp_setup_state() returns the actual failure reason to the caller,

	 * which in turn will call the cleanup routine.

	/*

	 * Now that this cpu is one of the designated,

	 * find a next cpu a) which is online and b) in same chip.

	/*

	 * If this(target) is the last cpu in the cpumask for this chip,

	 * check for any possible online cpu in the chip.

	/*

	 * Update the cpumask with the target cpu and

	 * migrate the context if needed

		/*

		 * If this is the last cpu in this chip then, skip the reference

		 * count mutex lock and make the reference count on this chip zero.

 Get the cpumask of this node */

	/*

	 * If this is not the first online CPU on this node, then

	 * just return.

	/*

	 * If this is the first online cpu on this node

	 * disable the nest counters by making an OPAL call.

 Make this CPU the designated target for counter collection */

	/*

	 * See if we need to disable the nest PMU.

	 * If no events are currently in use, then we have to take a

	 * mutex to ensure that we don't race with another task doing

	 * enable or disable the nest counters.

 Take the mutex lock for this node and then decrement the reference count */

		/*

		 * The scenario where this is true is, when perf session is

		 * started, followed by offlining of all cpus in a given node.

		 *

		 * In the cpuhotplug offline path, ppc_nest_imc_cpu_offline()

		 * function set the ref->count to zero, if the cpu which is

		 * about to offline is the last cpu in a given node and make

		 * an OPAL call to disable the engine in that node.

		 *

 Sampling not supported */

 Sanity check for config (event offset) */

	/*

	 * Nest HW counter memory resides in a per-chip reserve-memory (HOMER).

	 * Get the base memory addresss for this cpu.

 Return, if chip_id is not valid */

	/*

	 * Add the event offset to the base address.

	/*

	 * Get the imc_pmu_ref struct for this node.

	 * Take the mutex lock and then increment the count of nest pmu events

	 * inited.

/*

 * core_imc_mem_init : Initializes memory for the current core.

 *

 * Uses alloc_pages_node() and uses the returned address as an argument to

 * an opal call to configure the pdbar. The address sent as an argument is

 * converted to physical address before the opal call is made. This is the

 * base address at which the core imc counters are populated.

	/*

	 * alloc_pages_node() will allocate memory for core in the

	 * local node only.

 We need only vbase for core counters */

 Init the mutex */

 Get the cpumask for this core */

 If a cpu for this core is already set, then, don't do anything */

 set the cpu in the mask */

	/*

	 * clear this cpu out of the mask, if not present in the mask,

	 * don't bother doing anything.

	/*

	 * Check whether core_imc is registered. We could end up here

	 * if the cpuhotplug callback registration fails. i.e, callback

	 * invokes the offline path for all sucessfully registered cpus.

	 * At this stage, core_imc pmu will not be registered and we

	 * should return here.

	 *

	 * We return with a zero since this is not an offline failure.

	 * And cpuhp_setup_state() returns the actual failure reason

	 * to the caller, which inturn will call the cleanup routine.

 Find any online cpu in that core except the current "cpu" */

		/*

		 * If this is the last cpu in this core then, skip taking refernce

		 * count mutex lock for this core and directly zero "refc" for

		 * this core.

		/*

		 * Reduce the global reference count, if this is the

		 * last cpu in this core and core-imc event running

		 * in this cpu.

		/*

		 * If no other thread is running any

		 * event for this domain(thread/core/trace),

		 * set the global id to zero.

	/*

	 * See if we need to disable the IMC PMU.

	 * If no events are currently in use, then we have to take a

	 * mutex to ensure that we don't race with another task doing

	 * enable or disable the core counters.

 Take the mutex lock and decrement the refernce count for this core */

		/*

		 * The scenario where this is true is, when perf session is

		 * started, followed by offlining of all cpus in a given core.

		 *

		 * In the cpuhotplug offline path, ppc_core_imc_cpu_offline()

		 * function set the ref->count to zero, if the cpu which is

		 * about to offline is the last cpu in a given core and make

		 * an OPAL call to disable the engine in that core.

		 *

 Sampling not supported */

 Sanity check for config (event offset) */

 Get the core_imc mutex for this core */

	/*

	 * Core pmu units are enabled only when it is used.

	 * See if this is triggered for the first time.

	 * If yes, take the mutex lock and enable the core counters.

	 * If not, just increment the count in core_imc_refc struct.

	/*

	 * Since the system can run either in accumulation or trace-mode

	 * of IMC at a time, core-imc events are allowed only if no other

	 * trace/thread imc events are enabled/monitored.

	 *

	 * Take the global lock, and check the refc.id

	 * to know whether any other trace/thread imc

	 * events are running.

		/*

		 * No other trace/thread imc events are running in

		 * the system, so set the refc.id to core-imc.

/*

 * Allocates a page of memory for each of the online cpus, and load

 * LDBAR with 0.

 * The physical base address of the page allocated for a cpu will be

 * written to the LDBAR for that cpu, when the thread-imc event

 * is added.

 *

 * LDBAR Register Layout:

 *

 *  0          4         8         12        16        20        24        28

 * | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - |

 *   | |       [   ]    [                   Counter Address [8:50]

 *   | * Mode    |

 *   |           * PB Scope

 *   * Enable/Disable

 *

 *  32        36        40        44        48        52        56        60

 * | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - |

 *           Counter Address [8:50]              ]

 *

		/*

		 * This case could happen only once at start, since we dont

		 * free the memory in cpu offline path.

	/*

	 * Set the bit 0 of LDBAR to zero.

	 *

	 * If bit 0 of LDBAR is unset, it will stop posting

	 * the counter data to memory.

	 * For thread-imc, bit 0 of LDBAR will be set to 1 in the

	 * event_add function. So reset this bit here, to stop the updates

	 * to memory in the cpu_offline path.

 Reduce the refc if thread-imc event running on this cpu */

 Sampling not supported */

 Sanity check for config offset */

	/*

	 * Check if any other trace/core imc events are running in the

	 * system, if not set the global id to thread-imc.

	/*

	 * In-Memory Collection (IMC) counters are free flowing counters.

	 * So we take a snapshot of the counter value on enable and save it

	 * to calculate the delta at later stage to present the event counter

	 * value.

 Update the delta to the event count */

	/*

	 * In Memory Counters are free flowing counters. HW or the microcode

	 * keeps adding to the counter offset in memory. To get event

	 * counter value, we snapshot the value here and we calculate

	 * delta at later point.

	/*

	 * Take a snapshot and calculate the delta and update

	 * the event counter values.

	/*

	 * imc pmus are enabled only when it is used.

	 * See if this is triggered for the first time.

	 * If yes, take the mutex lock and enable the counters.

	 * If not, just increment the count in ref count struct.

 Set bit 0 of LDBAR to zero, to stop posting updates to memory */

	/*

	 * Take a snapshot and calculate the delta and update

	 * the event counter values.

/*

 * Allocate a page of memory for each cpu, and load LDBAR with 0.

 Initialise the counters for trace mode */

 Init the mutex, if not already */

	/*

	 * No need to set bit 0 of LDBAR to zero, as

	 * it is set to zero for imc trace-mode

	 *

	 * Reduce the refc if any trace-imc event running

	 * on this cpu.

/*

 * Function to parse trace-imc data obtained

 * and to prepare the perf sample.

 Sanity checks for a valid record */

 Prepare perf sample */

 when MSR HV and PR not set in the trace-record */

 MSR HV is 0 and PR is 1 */

 MSR HV is 1 and PR is 0 */

 MSR HV is 1 and PR is 1 */

 Exit, if not a valid record */

 If this is a valid record, create the sample */

 Set trace-imc bit in ldbar and load ldbar with per-thread memory address */

 trace-imc reference count */

 Return if this is a couting event */

	/*

	 * Take the global lock, and make sure

	 * no other thread is running any core/thread imc

	 * events

		/*

		 * No core/thread imc events are running in the

		 * system, so set the refc.id to trace-imc.

 update_pmu_ops : Populate the appropriate operations for "pmu" */

 init_nest_pmu_ref: Initialize the imc_pmu_ref struct for all the nodes */

		/*

		 * Mutex lock to avoid races while tracking the number of

		 * sessions using the chip's nest pmu units.

		/*

		 * Loop to init the "id" with the node_id. Variable "i" initialized to

		 * 0 and will be used as index to the array. "i" will not go off the

		 * end of the array since the "for_each_node" loops for "N_POSSIBLE"

		 * nodes only.

	/*

	 * Loop to init the per_cpu "local_nest_imc_refc" with the proper

	 * "nest_imc_refc" index. This makes get_nest_pmu_ref() alot simple.

 mem_info will never be NULL */

	/*

	 * By setting 0th bit of LDBAR to zero, we disable thread-imc

	 * updates to memory.

 Function to free the attr_groups which are dynamically allocated */

/*

 * Common function to unregister cpu hotplug callback and

 * free the memory.

 * TODO: Need to handle pmu unregistering, which will be

 * done in followup series.

 Free core_imc memory */

 Free thread_imc memory */

/*

 * Function to unregister thread-imc if core-imc

 * is not registered.

/*

 * imc_mem_init : Function to support memory allocation for core imc.

 Update the pmu name */

 Needed for hotplug/migration */

 Update the pmu name */

 Update the pmu name */

 Update the pmu name */

/*

 * init_imc_pmu : Setup and register the IMC pmu device.

 *

 * @parent:	Device tree unit node

 * @pmu_ptr:	memory allocated for this pmu

 * @pmu_idx:	Count of nest pmc registered

 *

 * init_imc_pmu() setup pmu cpumask and registers for a cpu hotplug callback.

 * Handles failure cases and accordingly frees memory.

		/*

		* Nest imc pmu need only one cpu per chip, we initialize the

		* cpumask for the first nest imc pmu and use the same for the

		* rest. To handle the cpuhotplug callback unregister, we track

		* the number of nest pmus in "nest_pmus".

 Register for cpu hotplug notification. */

 Unknown domain */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Hypervisor supplied "gpci" ("get performance counter info") performance

 * counter support

 *

 * Author: Cody P Schafer <cody@linux.vnet.ibm.com>

 * Copyright 2014 IBM Corporation.

/*

 * Example usage:

 *  perf stat -e 'hv_gpci/counter_info_version=3,offset=0,length=8,

 *		  secondary_index=0,starting_index=0xffffffff,request=0x10/' ...

 u32 */

 u32 */

/*

 * Note that starting_index, phys_processor_idx, sibling_part_id,

 * hw_chip_id, partition_id all refer to the same bit range. They

 * are basically aliases for the starting_index. The specific alias

 * used depends on the event. See REQUEST_IDX_KIND in hv-gpci-requests.h

 u16 */

 u8 */

 u8, bytes of data (1-8) */

 u32, byte offset */

	/*

	 * we verify offset and length are within the zeroed buffer at event

	 * init.

 Not our event */

 config2 is unused */

 no branch sampling */

 last byte within the buffer? */

 check if the request works... */

 Check if exiting cpu is used for collecting gpci events */

 Find a new cpu to collect gpci events */

 Migrate gpci events to the new target */

 init cpuhotplug */

 sampling not supported */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter support for POWER5+/++ (not POWER5) processors.

 *

 * Copyright 2009 Paul Mackerras, IBM Corporation.

/*

 * Bits in event code for POWER5+ (POWER5 GS) and POWER5++ (POWER5 GS DD3)

 PMC number (1-based) for direct events */

 TTMMUX number and setting - unit select */

 Byte number of event bus to use */

 Storage subsystem mux select */

 Set if event uses event bus */

 Values in PM_UNIT field */

/*

 * Bits in MMCR1 for POWER5+

/*

 * Layout of constraint bits:

 * 6666555555555544444444443333333333222222222211111111110000000000

 * 3210987654321098765432109876543210987654321098765432109876543210

 *             [  ><><>< ><> <><>[  >  <  ><  ><  ><  ><><><><><><>

 *             NC  G0G1G2 G3 T0T1 UC    B0  B1  B2  B3 P6P5P4P3P2P1

 *

 * NC - number of counters

 *     51: NC error 0x0008_0000_0000_0000

 *     48-50: number of events needing PMC1-4 0x0007_0000_0000_0000

 *

 * G0..G3 - GRS mux constraints

 *     46-47: GRS_L2SEL value

 *     44-45: GRS_L3SEL value

 *     41-44: GRS_MCSEL value

 *     39-40: GRS_FABSEL value

 *	Note that these match up with their bit positions in MMCR1

 *

 * T0 - TTM0 constraint

 *     36-37: TTM0SEL value (0=FPU, 2=IFU, 3=ISU1) 0x30_0000_0000

 *

 * T1 - TTM1 constraint

 *     34-35: TTM1SEL value (0=IDU, 3=GRS) 0x0c_0000_0000

 *

 * UC - unit constraint: can't have all three of FPU|IFU|ISU1, ISU0, IDU|GRS

 *     33: UC3 error 0x02_0000_0000

 *     32: FPU|IFU|ISU1 events needed 0x01_0000_0000

 *     31: ISU0 events needed 0x01_8000_0000

 *     30: IDU|GRS events needed 0x00_4000_0000

 *

 * B0

 *     24-27: Byte 0 event source 0x0f00_0000

 *	      Encoding as for the event code

 *

 * B1, B2, B3

 *     20-23, 16-19, 12-15: Byte 1, 2, 3 event sources

 *

 * P6

 *     11: P6 error 0x800

 *     10-11: Count of events needing PMC6

 *

 * P1..P5

 *     0-9: Count of events needing PMC1..PMC5

 Masks and values for using events from the various units */

 Map LSU1 low word (bytes 4-7) to unit LSU1+1 */

 Set byte lane select field */

 need a counter from PMC1-4 set */

 at most 3 alternatives for any event */

 PM_GCT_FULL_CYC */

 PM_GRP_DISP_REJECT */

 PM_BR_PRED_CR */

 PM_BR_PRED_TA */

 PM_THRD_L2MISS_BOTH_CYC */

 PM_DTLB_MISS */

 PM_MRK_DTLB_MISS */

 PM_RUN_CYC */

 PM_INST_CMPL */

 PM_LSU_LMQ_SRQ_EMPTY_CYC */

 PM_INST_DISP */

/*

 * Scan the alternatives table for a match and return the

 * index into the alternatives table if found, else -1.

 PMC 1 */	{ 0x21, 0x23, 0x25, 0x27 },

 PMC 2 */	{ 0x07, 0x17, 0x0e, 0x1e },

 PMC 3 */	{ 0x20, 0x22, 0x24, 0x26 },

 PMC 4 */	{ 0x07, 0x17, 0x0e, 0x1e }

/*

 * Some direct events for decodes of event bus byte 3 have alternative

 * PMCSEL values on other counters.  This returns the alternative

 * event code for those that do, or -1 otherwise.  This also handles

 * alternative PCMSEL values for add events.

 1 <-> 4, 2 <-> 3 */

 new decode alternatives for power5+ */

 alternative add event encodings */

		/*

		 * We're only counting in RUN state,

		 * so PM_CYC is equivalent to PM_RUN_CYC

		 * and PM_INST_CMPL === PM_RUN_INST_CMPL.

		 * This doesn't include alternatives that don't provide

		 * any extra flexibility in assigning PMCs (e.g.

		 * 0x100005 for PM_RUN_CYC vs. 0xf for PM_CYC).

		 * Note that even with these additional alternatives

		 * we never end up with more than 3 alternatives for any event.

 PM_CYC */

 PM_RUN_CYC */

 PM_RUN_CYC */

 PM_INST_CMPL */

 PM_RUN_INST_CMPL */

 PM_RUN_INST_CMPL */

 PM_INST_CMPL */

 remove the limited PMC events */

 remove all but the limited PMC events */

/*

 * Map of which direct events on which PMCs are marked instruction events.

 * Indexed by PMCSEL value, bit i (LE) set if PMC i is a marked event.

 * Bit 0 is set if it is marked for all PMCs.

 * The 0x80 bit indicates a byte decode PMCSEL value.

 00 */

 01 PM_IOPS_CMPL */

 02 PM_MRK_GRP_DISP */

 03 PM_MRK_ST_CMPL, PM_MRK_ST_GPS, PM_MRK_ST_CMPL_INT */

 04 */

 05 PM_MRK_BRU_FIN, PM_MRK_INST_FIN, PM_MRK_CRU_FIN */

 06 */

 07 */

 08 - 0a */

 0b PM_THRESH_TIMEO, PM_MRK_GRP_TIMEO */

 0c */

 0d */

 0e */

 0f */

 10 */

 11 PM_MRK_GRP_BR_REDIR, PM_MRK_GRP_IC_MISS */

 12 */

 13 PM_MRK_GRP_CMPL */

 14 PM_GRP_MRK, PM_MRK_{FXU,FPU,LSU}_FIN */

 15 PM_MRK_GRP_ISSUED */

 16 */

 17 */

 1d */

 1e */

 1f */

 20 */

 21 */

 22 */

 23 */

 24 */

 25 */

 26 */

 27 */

/*

 * Returns 1 if event counts things relating to marked instructions

 * and thus needs the MMCRA_SAMPLE_ENABLE bit set, or 0 if not.

 byte 1 bits 0-7, byte 2 bits 0,2-4,6 */

 byte 5 bits 6-7, byte 6 bits 0,4, byte 7 bits 0-4,6 */

 First pass to count resource use */

	/*

	 * Assign resources and set multiplexer selects.

	 *

	 * PM_ISU0 can go either on TTM0 or TTM1, but that's the only

	 * choice we have to deal with.

 move ISU to TTM1 */

 Set TTM[01]SEL fields. */

 Set byte lane select fields, TTM[23]SEL and GRS_*SEL. */

 get ISU0 through TTM1 rather than TTM0 */

 select lower word of LSU1 for this byte */

 Second pass: assign PMCs, set PMCxSEL and PMCx_ADDER_SEL fields */

 Bus event or any-PMC direct event */

 Direct event */

 add events on higher-numbered bus */

 Instructions or run cycles on PMC5/6 */

 select alternate byte lane */

 Return MMCRx values */

 LD_REF_L1 */

 LD_MISS_L1 */

 BR_ISSUED */

 BR_MPRED_CR */

/*

 * Table of generalized cache-related events.

 * 0 means not supported, -1 means nonsensical, other values

 * are event codes.

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter support for PPC970-family processors.

 *

 * Copyright 2008-2009 Paul Mackerras, IBM Corporation.

/*

 * Bits in event code for PPC970

 PMC number (1-based) for direct events */

 TTMMUX number and setting - unit select */

 Byte number of event bus to use */

 Values in PM_UNIT field */

/*

 * Bits in MMCR0 for PPC970

/*

 * Bits in MMCR1 for PPC970

/*

 * Layout of constraint bits:

 * 6666555555555544444444443333333333222222222211111111110000000000

 * 3210987654321098765432109876543210987654321098765432109876543210

 *               <><><>[  >[  >[  ><  ><  ><  ><  ><><><><><><><><>

 *               SPT0T1 UC  PS1 PS2 B0  B1  B2  B3 P1P2P3P4P5P6P7P8

 *

 * SP - SPCSEL constraint

 *     48-49: SPCSEL value 0x3_0000_0000_0000

 *

 * T0 - TTM0 constraint

 *     46-47: TTM0SEL value (0=FPU, 2=IFU, 3=VPU) 0xC000_0000_0000

 *

 * T1 - TTM1 constraint

 *     44-45: TTM1SEL value (0=IDU, 3=STS) 0x3000_0000_0000

 *

 * UC - unit constraint: can't have all three of FPU|IFU|VPU, ISU, IDU|STS

 *     43: UC3 error 0x0800_0000_0000

 *     42: FPU|IFU|VPU events needed 0x0400_0000_0000

 *     41: ISU events needed 0x0200_0000_0000

 *     40: IDU|STS events needed 0x0100_0000_0000

 *

 * PS1

 *     39: PS1 error 0x0080_0000_0000

 *     36-38: count of events needing PMC1/2/5/6 0x0070_0000_0000

 *

 * PS2

 *     35: PS2 error 0x0008_0000_0000

 *     32-34: count of events needing PMC3/4/7/8 0x0007_0000_0000

 *

 * B0

 *     28-31: Byte 0 event source 0xf000_0000

 *	      Encoding as for the event code

 *

 * B1, B2, B3

 *     24-27, 20-23, 16-19: Byte 1, 2, 3 event sources

 *

 * P1

 *     15: P1 error 0x8000

 *     14-15: Count of events needing PMC1

 *

 * P2..P8

 *     0-13: Count of events needing PMC2..PMC8

 PMC1: PM_MRK_GRP_DISP, PM_MRK_ST_CMPL */

 PMC2: PM_THRESH_TIMEO, PM_MRK_BRU_FIN */

 PMC3: PM_MRK_ST_CMPL_INT, PM_MRK_VMX_FIN */

 PMC4: PM_MRK_GRP_CMPL, PM_MRK_CRU_FIN */

 PMC5: PM_GRP_MRK, PM_MRK_GRP_TIMEO */

 PMC6: PM_MRK_ST_STS, PM_MRK_FXU_FIN, PM_MRK_GRP_ISSUED */

 PMC7: PM_MRK_FPU_FIN, PM_MRK_INST_FIN */

 PMC8: PM_MRK_LSU_FIN */

/*

 * Returns 1 if event counts things relating to marked instructions

 * and thus needs the MMCRA_SAMPLE_ENABLE bit set, or 0 if not.

 add events */

 decode events */

 byte 0 bits 2,3,6 */

 byte 2 bits 0,2,3,4,6; all of byte 1 */

 byte 3 bits 4,6 */

 Masks and values for using events from the various units */

		/*

		 * Bus events on bytes 0 and 2 can be counted

		 * on PMC1/2/5/6; bytes 1 and 3 on PMC3/4/7/8.

 Set byte lane select field */

 increment PMC1/2/5/6 field */

 increment PMC3/4/7/8 field */

 2 alternatives for LSU empty */

 First pass to count resource use */

 count 1/2/5/6 vs 3/4/7/8 use */

	/*

	 * Assign resources and set multiplexer selects.

	 *

	 * PM_ISU can go either on TTM0 or TTM1, but that's the only

	 * choice we have to deal with.

 move ISU to TTM1 */

 Set TTM[01]SEL fields. */

 Check only one unit per TTMx */

 Set byte lane select fields and TTM3SEL. */

 Second pass: assign PMCs, set PMCxSEL and PMCx_ADDER_SEL fields */

 8 means don't count */

 Bus event or any-PMC direct event */

 Direct event */

 add events on higher-numbered bus */

 mark only one IOP per PPC instruction */

 Return MMCRx values */

	/*

	 * Setting the PMCxSEL field to 0x08 disables PMC x.

 PM_LD_REF_L1 */

 PM_LD_MISS_L1 */

 PM_BR_ISSUED */

 PM_GRP_BR_MPRED */

/*

 * Table of generalized cache-related events.

 * 0 means not supported, -1 means nonsensical, other values

 * are event codes.

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter support for POWER5 (not POWER5++) processors.

 *

 * Copyright 2009 Paul Mackerras, IBM Corporation.

/*

 * Bits in event code for POWER5 (not POWER5++)

 PMC number (1-based) for direct events */

 TTMMUX number and setting - unit select */

 Byte number of event bus to use */

 Storage subsystem mux select */

 Set if event uses event bus */

 Values in PM_UNIT field */

/*

 * Bits in MMCR1 for POWER5

/*

 * Layout of constraint bits:

 * 6666555555555544444444443333333333222222222211111111110000000000

 * 3210987654321098765432109876543210987654321098765432109876543210

 *         <><>[  ><><>< ><> [  >[ >[ ><  ><  ><  ><  ><><><><><><>

 *         T0T1 NC G0G1G2 G3  UC PS1PS2 B0  B1  B2  B3 P6P5P4P3P2P1

 *

 * T0 - TTM0 constraint

 *     54-55: TTM0SEL value (0=FPU, 2=IFU, 3=ISU1) 0xc0_0000_0000_0000

 *

 * T1 - TTM1 constraint

 *     52-53: TTM1SEL value (0=IDU, 3=GRS) 0x30_0000_0000_0000

 *

 * NC - number of counters

 *     51: NC error 0x0008_0000_0000_0000

 *     48-50: number of events needing PMC1-4 0x0007_0000_0000_0000

 *

 * G0..G3 - GRS mux constraints

 *     46-47: GRS_L2SEL value

 *     44-45: GRS_L3SEL value

 *     41-44: GRS_MCSEL value

 *     39-40: GRS_FABSEL value

 *	Note that these match up with their bit positions in MMCR1

 *

 * UC - unit constraint: can't have all three of FPU|IFU|ISU1, ISU0, IDU|GRS

 *     37: UC3 error 0x20_0000_0000

 *     36: FPU|IFU|ISU1 events needed 0x10_0000_0000

 *     35: ISU0 events needed 0x08_0000_0000

 *     34: IDU|GRS events needed 0x04_0000_0000

 *

 * PS1

 *     33: PS1 error 0x2_0000_0000

 *     31-32: count of events needing PMC1/2 0x1_8000_0000

 *

 * PS2

 *     30: PS2 error 0x4000_0000

 *     28-29: count of events needing PMC3/4 0x3000_0000

 *

 * B0

 *     24-27: Byte 0 event source 0x0f00_0000

 *	      Encoding as for the event code

 *

 * B1, B2, B3

 *     20-23, 16-19, 12-15: Byte 1, 2, 3 event sources

 *

 * P1..P6

 *     0-11: Count of events needing PMC1..PMC6

 Masks and values for using events from the various units */

 Map LSU1 low word (bytes 4-7) to unit LSU1+1 */

		/*

		 * Bus events on bytes 0 and 2 can be counted

		 * on PMC1/2; bytes 1 and 3 on PMC3/4.

 Set byte lane select field */

 increment PMC1/2 field */

 increment PMC3/4 field */

 need a counter from PMC1-4 set */

 at most 3 alternatives for any event */

 PM_GRP_DISP_REJECT */

 PM_THRD_L2MISS_BOTH_CYC */

 PM_RUN_CYC */

 PM_INST_CMPL */

 PM_INST_DISP */

/*

 * Scan the alternatives table for a match and return the

 * index into the alternatives table if found, else -1.

 PMC 1 */	{ 0x21, 0x23, 0x25, 0x27 },

 PMC 2 */	{ 0x07, 0x17, 0x0e, 0x1e },

 PMC 3 */	{ 0x20, 0x22, 0x24, 0x26 },

 PMC 4 */	{ 0x07, 0x17, 0x0e, 0x1e }

/*

 * Some direct events for decodes of event bus byte 3 have alternative

 * PMCSEL values on other counters.  This returns the alternative

 * event code for those that do, or -1 otherwise.

 1 <-> 4, 2 <-> 3 */

/*

 * Map of which direct events on which PMCs are marked instruction events.

 * Indexed by PMCSEL value, bit i (LE) set if PMC i is a marked event.

 * Bit 0 is set if it is marked for all PMCs.

 * The 0x80 bit indicates a byte decode PMCSEL value.

 00 */

 01 PM_IOPS_CMPL */

 02 PM_MRK_GRP_DISP */

 03 PM_MRK_ST_CMPL, PM_MRK_ST_GPS, PM_MRK_ST_CMPL_INT */

 04 */

 05 PM_MRK_BRU_FIN, PM_MRK_INST_FIN, PM_MRK_CRU_FIN */

 06 */

 07 */

 08 - 0a */

 0b PM_THRESH_TIMEO, PM_MRK_GRP_TIMEO */

 0c */

 0d */

 0e */

 0f */

 10 */

 11 PM_MRK_GRP_BR_REDIR, PM_MRK_GRP_IC_MISS */

 12 */

 13 PM_MRK_GRP_CMPL */

 14 PM_GRP_MRK, PM_MRK_{FXU,FPU,LSU}_FIN */

 15 PM_MRK_GRP_ISSUED */

 16 */

 17 */

 1d */

 1e */

 1f */

 20 */

 21 */

 22 */

 23 */

 24 */

 25 */

 26 */

 27 */

/*

 * Returns 1 if event counts things relating to marked instructions

 * and thus needs the MMCRA_SAMPLE_ENABLE bit set, or 0 if not.

 byte 1 bits 0-7, byte 2 bits 0,2-4,6 */

 byte 4 bits 1,3,5,7, byte 5 bits 6-7, byte 7 bits 0-4,6 */

 First pass to count resource use */

 count 1/2 vs 3/4 use */

	/*

	 * Assign resources and set multiplexer selects.

	 *

	 * PM_ISU0 can go either on TTM0 or TTM1, but that's the only

	 * choice we have to deal with.

 move ISU to TTM1 */

 Set TTM[01]SEL fields. */

 Set byte lane select fields, TTM[23]SEL and GRS_*SEL. */

 get ISU0 through TTM1 rather than TTM0 */

 select lower word of LSU1 for this byte */

 Second pass: assign PMCs, set PMCxSEL and PMCx_ADDER_SEL fields */

 Bus event or any-PMC direct event */

 Direct event */

 add events on higher-numbered bus */

 Instructions or run cycles on PMC5/6 */

 Return MMCRx values */

 LD_REF_L1 */

 LD_MISS_L1 */

 BR_ISSUED */

 BR_MPRED_CR */

/*

 * Table of generalized cache-related events.

 * 0 means not supported, -1 means nonsensical, other values

 * are event codes.

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter support for MPC7450-family processors.

 *

 * Copyright 2008-2009 Paul Mackerras, IBM Corporation.

 Number of hardware counters */

 Maximum number of event alternative codes */

/*

 * Bits in event code for MPC7450 family

/*

 * Classify events according to how specific their PMC requirements are.

 * Result is:

 *	0: can go on any PMC

 *	1: can go on PMCs 1-4

 *	2: can go on PMCs 1,2,4

 *	3: can go on PMCs 1 or 2

 *	4: can only go on one PMC

 *	-1: event code is invalid

/*

 * Events using threshold and possible threshold scale:

 *	code	scale?	name

 *	11e	N	PM_INSTQ_EXCEED_CYC

 *	11f	N	PM_ALTV_IQ_EXCEED_CYC

 *	128	Y	PM_DTLB_SEARCH_EXCEED_CYC

 *	12b	Y	PM_LD_MISS_EXCEED_L1_CYC

 *	220	N	PM_CQ_EXCEED_CYC

 *	30c	N	PM_GPR_RB_EXCEED_CYC

 *	30d	?	PM_FPR_IQ_EXCEED_CYC ?

 *	311	Y	PM_ITLB_SEARCH_EXCEED

 *	410	N	PM_GPR_IQ_EXCEED_CYC

/*

 * Return use of threshold and threshold scale bits:

 * 0 = uses neither, 1 = uses threshold, 2 = uses both

/*

 * Layout of constraint bits:

 * 33222222222211111111110000000000

 * 10987654321098765432109876543210

 *  |<    ><  > < > < ><><><><><><>

 *  TS TV   G4   G3  G2P6P5P4P3P2P1

 *

 * P1 - P6

 *	0 - 11: Count of events needing PMC1 .. PMC6

 *

 * G2

 *	12 - 14: Count of events needing PMC1 or PMC2

 *

 * G3

 *	16 - 18: Count of events needing PMC1, PMC2 or PMC4

 *

 * G4

 *	20 - 23: Count of events needing PMC1, PMC2, PMC3 or PMC4

 *

 * TV

 *	24 - 29: Threshold value requested

 *

 * TS

 *	30: Threshold scale value requested

 PMC1 mask, value: P1,G2,G3,G4 */

 PMC2: P2,G2,G3,G4 */

 PMC3: P3,G4 */

 PMC4: P4,G3,G4 */

 PMC5: P5 */

 PMC6: P6 */

 class 0: no constraint */

 class 1: G4 */

 class 2: G3 */

 class 3: G2 */

 PM_L1_DCACHE_MISS */

 PM_SNOOP_RETRY */

 PM_L2_HIT */

 PM_L3_HIT */

 PM_L2_ICACHE_MISS */

 PM_L3_ICACHE_MISS */

 PM_L2_DCACHE_MISS */

 PM_L3_DCACHE_MISS */

 PM_LD_HIT_L3 */

 PM_ST_HIT_L3 */

 PM_L2_TOUCH_HIT */

 PM_L3_TOUCH_HIT */

 PM_INT_LOCAL */

 PM_L2_MISS */

 PM_L3_MISS */

/*

 * Scan the alternatives table for a match and return the

 * index into the alternatives table if found, else -1.

/*

 * Bitmaps of which PMCs each class can use for classes 0 - 3.

 * Bit i is set if PMC i+1 is usable.

 Bit position and width of each PMCSEL field */

/*

 * Compute MMCR0/1/2 values for a set of events.

 First pass: count usage in each class */

 Second pass: allocate PMCs from most specific event to least */

 Find a suitable PMC */

 Return MMCRx values */

	/*

	 * 32-bit doesn't have an MMCRA and uses SPRN_MMCR2 to define

	 * SPRN_MMCRA. So assign mmcra of cpu_hw_events with `mmcr2`

	 * value to ensure that any write to this SPRN_MMCRA will

	 * use mmcr2 value.

/*

 * Disable counting by a PMC.

 * Note that the pmc argument is 0-based here, not 1-based.

 PM_L1_DCACHE_MISS */

 PM_BR_CMPL */

 PM_BR_MPRED */

/*

 * Table of generalized cache-related events.

 * 0 means not supported, -1 means nonsensical, other values

 * are event codes.

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter support for POWER8 processors.

 *

 * Copyright 2009 Paul Mackerras, IBM Corporation.

 * Copyright 2013 Michael Ellerman, IBM Corporation.

/*

 * Some power8 event codes.

 MMCRA IFM bits - POWER8 */

/*

 * Raw event encoding for PowerISA v2.07 (Power8):

 *

 *        60        56        52        48        44        40        36        32

 * | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - |

 *   | | [ ]                           [      thresh_cmp     ]   [  thresh_ctl   ]

 *   | |  |                                                              |

 *   | |  *- IFM (Linux)                 thresh start/stop OR FAB match -*

 *   | *- BHRB (Linux)

 *   *- EBB (Linux)

 *

 *        28        24        20        16        12         8         4         0

 * | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - |

 *   [   ] [  sample ]   [cache]   [ pmc ]   [unit ]   c     m   [    pmcxsel    ]

 *     |        |           |                          |     |

 *     |        |           |                          |     *- mark

 *     |        |           *- L1/L2/L3 cache_sel      |

 *     |        |                                      |

 *     |        *- sampling mode for marked events     *- combine

 *     |

 *     *- thresh_sel

 *

 * Below uses IBM bit numbering.

 *

 * MMCR1[x:y] = unit    (PMCxUNIT)

 * MMCR1[x]   = combine (PMCxCOMB)

 *

 * if pmc == 3 and unit == 0 and pmcxsel[0:6] == 0b0101011

 *	# PM_MRK_FAB_RSP_MATCH

 *	MMCR1[20:27] = thresh_ctl   (FAB_CRESP_MATCH / FAB_TYPE_MATCH)

 * else if pmc == 4 and unit == 0xf and pmcxsel[0:6] == 0b0101001

 *	# PM_MRK_FAB_RSP_MATCH_CYC

 *	MMCR1[20:27] = thresh_ctl   (FAB_CRESP_MATCH / FAB_TYPE_MATCH)

 * else

 *	MMCRA[48:55] = thresh_ctl   (THRESH START/END)

 *

 * if thresh_sel:

 *	MMCRA[45:47] = thresh_sel

 *

 * if thresh_cmp:

 *	MMCRA[22:24] = thresh_cmp[0:2]

 *	MMCRA[25:31] = thresh_cmp[3:9]

 *

 * if unit == 6 or unit == 7

 *	MMCRC[53:55] = cache_sel[1:3]      (L2EVENT_SEL)

 * else if unit == 8 or unit == 9:

 *	if cache_sel[0] == 0: # L3 bank

 *		MMCRC[47:49] = cache_sel[1:3]  (L3EVENT_SEL0)

 *	else if cache_sel[0] == 1:

 *		MMCRC[50:51] = cache_sel[2:3]  (L3EVENT_SEL1)

 * else if cache_sel[1]: # L1 event

 *	MMCR1[16] = cache_sel[2]

*	MMCR1[17] = cache_sel[3]

 *

 * if mark:

 *	MMCRA[63]    = 1		(SAMPLE_ENABLE)

 *	MMCRA[57:59] = sample[0:2]	(RAND_SAMP_ELIG)

*	MMCRA[61:62] = sample[3:4]	(RAND_SAMP_MODE)

 *

 * if EBB and BHRB:

 *	MMCRA[32:33] = IFM

 *

 PowerISA v2.07 format attribute structure*/

 Table of alternatives, sorted by column 0 */

	/* BHRB and regular PMU events share the same privilege state

	 * filter configuration. BHRB is always recorded along with a

	 * regular PMU event. As the privilege state filter is handled

	 * in the basic PMC configuration of the accompanying regular

	 * PMU event, we ignore any separate BHRB specific request.

 No branch filter requested */

 Invalid branch filter options - HW does not support */

 Every thing else is unsupported */

 Enable BHRB filter in PMU */

/*

 * Table of generalized cache-related events.

 * 0 means not supported, -1 means nonsensical, other values

 * are event codes.

 Tell userspace that EBB is supported */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Common Performance counter support functions for PowerISA v2.07 processors.

 *

 * Copyright 2009 Paul Mackerras, IBM Corporation.

 * Copyright 2013 Michael Ellerman, IBM Corporation.

 * Copyright 2016 Madhavan Srinivasan, IBM Corporation.

 Only check pmc, unit and pmcxsel, ignore the edge bit (0) */

 PM_MRK_FAB_RSP_MATCH & PM_MRK_FAB_RSP_MATCH_CYC */

	/*

	 * MMCRA[SDAR_MODE] specifices how the SDAR should be updated in

	 * continous sampling mode.

	 *

	 * Incase of Power8:

	 * MMCRA[SDAR_MODE] will be programmed as "0b01" for continous sampling

	 * mode and will be un-changed when setting MMCRA[63] (Marked events).

	 *

	 * Incase of Power9/power10:

	 * Marked event: MMCRA[SDAR_MODE] will be set to 0b00 ('No Updates'),

	 *               or if group already have any marked events.

	 * For rest

	 *	MMCRA[SDAR_MODE] will be set from event code.

	 *      If sdar_mode from event is zero, default to 0b01. Hardware

	 *      requires that we set a non-zero value.

	/*

	 * Incase of P10, thresh_cmp value is not part of raw event code

	 * and provided via attr.config1 parameter. To program threshold in MMCRA,

	 * take a 18 bit number N and shift right 2 places and increment

	 * the exponent E by 1 until the upper 10 bits of N are zero.

	 * Write E to the threshold exponent and write the lower 8 bits of N

	 * to the threshold mantissa.

	 * The max threshold that can be written is 261120.

		/*

		 * Note that it is invalid to write a mantissa with the

		 * upper 2 bits of mantissa being zero, unless the

		 * exponent is also zero.

	/*

	 * Since location of threshold compare bits in MMCRA

	 * is different for p8, using different shift value.

	/*

	 * Check the mantissa upper two bits are not zero, unless the

	 * exponent is also zero. See the THRESH_CMP_MANTISSA doc.

 Nothing to do */

 Skip if no SIER support */

		/*

		 * Type 0b111 denotes either larx or stcx instruction. Use the

		 * MMCRA sampling bits [57:59] along with the type value

		 * to determine the exact instruction type. If the sampling

		 * criteria is neither load or store, set the type as default

		 * to NA.

	/*

	 * Use 64 bit weight field (full) if sample type is

	 * WEIGHT.

	 *

	 * if sample type is WEIGHT_STRUCT:

	 * - store memory latency in the lower 32 bits.

	 * - For ISA v3.1, use remaining two 16 bit fields of

	 *   perf_sample_weight to store cycle counter values

	 *   from sier2.

 Ignore Linux defined bits when checking event below */

		/*

		 * PMC5 and PMC6 are used to count cycles and instructions and

		 * they do not support most of the constraint bits. Add a check

		 * to exclude PMC5/6 from most of the constraints except for

		 * EBB/BHRB.

		/*

		 * Add to number of counters in use. Note this includes events with

		 * a PMC of 0 - they still need a PMC, it's just assigned later.

		 * Don't count events on PMC 5 & 6, there is only one valid event

		 * on each of those counters, and they are handled above.

			/*

			 * L2/L3 events contain a cache selector field, which is

			 * supposed to be programmed into MMCRC. However MMCRC is only

			 * HV writable, and there is no API for guest kernels to modify

			 * it. The solution is for the hypervisor to initialise the

			 * field to zeroes, and for us to only ever allow events that

			 * have a cache selector of zero. The bank selector (bit 3) is

			 * irrelevant, as long as the rest of the value is 0.

		/*

		 * Special case for PM_MRK_FAB_RSP_MATCH and PM_MRK_FAB_RSP_MATCH_CYC,

		 * the threshold control bits are used for the match value.

 EBB events must specify the PMC */

 Only EBB events can request BHRB */

	/*

	 * All events must agree on EBB, either all request it or none.

	 * EBB events are pinned & exclusive, so this should never actually

	 * hit, but we leave it as a fallback in case.

 First pass to count resource use */

	/*

	 * Disable bhrb unless explicitly requested

	 * by setting MMCRA (BHRBRD) bit.

 Second pass: assign PMCs, set all MMCR1 fields */

 In continuous sampling mode, update SDAR on TLB miss */

 Set RADIX_SCOPE_QUAL bit */

		/*

		 * PM_MRK_FAB_RSP_MATCH and PM_MRK_FAB_RSP_MATCH_CYC,

		 * the threshold bits are used for the match value.

 set MMCRA (BHRBRD) to 0 if there is user request for BHRB */

 Return MMCRx values */

 pmc_inuse is 1-based */

 If we're not using PMC 5 or 6, freeze them */

	/*

	 * Set mmcr0 (PMCCEXT) for p10 which

	 * will restrict access to group B registers

	 * when MMCR0 PMCC=0b00.

 Filter out the original event, it's already in alt[0] */

		/*

		 * We're only counting in RUN state, so PM_CYC is equivalent to

		 * PM_RUN_CYC and PM_INST_CMPL === PM_RUN_INST_CMPL.

 PMC_CYC */

 PM_RUN_CYC */

 PM_INST_CMPL */

 PM_RUN_INST_CMPL */

	/*

	 * MMCRA[61:62] is Random Sampling Mode (SM).

	 * value of 0b11 is reserved.

	/*

	 * Check for all reserved value

	 * Source: Performance Monitoring Unit User Guide

	/*

	 * MMCRA[48:51]/[52:55]) Threshold Start/Stop

	 * Events Selection.

	 * 0b11110000/0b00001111 is reserved.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter support for POWER10 processors.

 *

 * Copyright 2020 Madhavan Srinivasan, IBM Corporation.

 * Copyright 2020 Athira Rajeev, IBM Corporation.

/*

 * Raw event encoding for Power10:

 *

 *        60        56        52        48        44        40        36        32

 * | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - |

 *   | | [ ]   [ src_match ] [  src_mask ]   | [ ] [ l2l3_sel ]  [  thresh_ctl   ]

 *   | |  |                                  |  |                         |

 *   | |  *- IFM (Linux)                     |  |        thresh start/stop -*

 *   | *- BHRB (Linux)                       |  src_sel

 *   *- EBB (Linux)                          *invert_bit

 *

 *        28        24        20        16        12         8         4         0

 * | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - | - - - - |

 *   [   ] [  sample ]   [ ] [ ]   [ pmc ]   [unit ]   [ ] |  m   [    pmcxsel    ]

 *     |        |        |    |                        |   |  |

 *     |        |        |    |                        |   |  *- mark

 *     |        |        |    *- L1/L2/L3 cache_sel    |   |*-radix_scope_qual

 *     |        |        sdar_mode                     |

 *     |        *- sampling mode for marked events     *- combine

 *     |

 *     *- thresh_sel

 *

 * Below uses IBM bit numbering.

 *

 * MMCR1[x:y] = unit    (PMCxUNIT)

 * MMCR1[24]   = pmc1combine[0]

 * MMCR1[25]   = pmc1combine[1]

 * MMCR1[26]   = pmc2combine[0]

 * MMCR1[27]   = pmc2combine[1]

 * MMCR1[28]   = pmc3combine[0]

 * MMCR1[29]   = pmc3combine[1]

 * MMCR1[30]   = pmc4combine[0]

 * MMCR1[31]   = pmc4combine[1]

 *

 * if pmc == 3 and unit == 0 and pmcxsel[0:6] == 0b0101011

 *	MMCR1[20:27] = thresh_ctl

 * else if pmc == 4 and unit == 0xf and pmcxsel[0:6] == 0b0101001

 *	MMCR1[20:27] = thresh_ctl

 * else

 *	MMCRA[48:55] = thresh_ctl   (THRESH START/END)

 *

 * if thresh_sel:

 *	MMCRA[45:47] = thresh_sel

 *

 * if l2l3_sel:

 * MMCR2[56:60] = l2l3_sel[0:4]

 *

 * MMCR1[16] = cache_sel[0]

 * MMCR1[17] = cache_sel[1]

 * MMCR1[18] = radix_scope_qual

 *

 * if mark:

 *	MMCRA[63]    = 1		(SAMPLE_ENABLE)

 *	MMCRA[57:59] = sample[0:2]	(RAND_SAMP_ELIG)

 *	MMCRA[61:62] = sample[3:4]	(RAND_SAMP_MODE)

 *

 * if EBB and BHRB:

 *	MMCRA[32:33] = IFM

 *

 * MMCRA[SDAR_MODE]  = sdar_mode[0:1]

/*

 * Some power10 event codes.

 MMCRA IFM bits - POWER10 */

 Table of alternatives, sorted by column 0 */

	/* BHRB and regular PMU events share the same privilege state

	 * filter configuration. BHRB is always recorded along with a

	 * regular PMU event. As the privilege state filter is handled

	 * in the basic PMC configuration of the accompanying regular

	 * PMU event, we ignore any separate BHRB specific request.

 No branch filter requested */

 Invalid branch filter options - HW does not support */

 Every thing else is unsupported */

 Enable BHRB filter in PMU */

/*

 * Table of generalized cache-related events.

 * 0 means not supported, -1 means nonsensical, other values

 * are event codes.

/*

 * Set the MMCR0[CC56RUN] bit to enable counting for

 * PMC5 and PMC6 regardless of the state of CTRL[RUN],

 * so that we can use counters 5 and 6 as PM_INST_CMPL and

 * PM_CYC.

 Comes from cpu_specs[] */

 Add the ppmu flag for power10 DD1 */

 Set the PERF_REG_EXTENDED_MASK here */

 Tell userspace that EBB is supported */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2016 Anju T, IBM Corporation.

 Function to return the extended register values */

	/*

	 * If the idx is referring to value beyond the

	 * supported registers, return 0 with a warning

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter callchain support - powerpc architecture code

 *

 * Copyright  2009 Paul Mackerras, IBM Corporation.

/*

 * On 64-bit we don't want to invoke hash_page on user addresses from

 * interrupt context, so if the access faults, we read the page tables

 * to find which page (if any) is mapped and access it directly. Radix

 * has no need for this so it doesn't use read_user_stack_slow.

 align address to page boundary */

/*

 * 64-bit user processes use the same stack frame for RT and non-RT signals.

/*

 * Do some sanity checking on the signal frame pointed to by sp.

 * We check the pinfo and puc pointers in the frame.

		/*

		 * Note: the next_sp - sp >= signal frame size check

		 * is true when next_sp < sp, which can happen when

		 * transitioning from an alternate signal stack to the

		 * normal stack.

			/*

			 * This looks like an signal frame

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter callchain support - powerpc architecture code

 *

 * Copyright  2009 Paul Mackerras, IBM Corporation.

 CONFIG_PPC64 */

 CONFIG_PPC64 */

/*

 * Layout for non-RT signal frames

/*

 * Layout for RT signal frames

	/*

	 * Note: the next_sp - sp >= signal frame size check

	 * is true when next_sp < sp, for example, when

	 * transitioning from an alternate signal stack to the

	 * normal stack.

			/*

			 * This looks like an signal frame, so restart

			 * the stack trace with the values in it.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance event support - Freescale Embedded Performance Monitor

 *

 * Copyright 2008-2009 Paul Mackerras, IBM Corporation.

 * Copyright 2010 Freescale Semiconductor, Inc.

 Number of perf_events counting hardware events */

 Used to avoid races in calling reserve/release_pmc_hardware */

/*

 * Read one performance monitor counter (PMC).

/*

 * Write one PMC.

/*

 * Write one local control A register

/*

 * Write one local control B register

	/*

	 * Performance monitor interrupts come even when interrupts

	 * are soft-disabled, as long as interrupts are hard-enabled.

	 * Therefore we treat them like NMIs.

 The counters are only 32 bits wide */

/*

 * Disable all events to prevent PMU interrupts and to allow

 * events to be added or removed.

		/*

		 * Check if we ever enabled the PMU on this cpu.

			/*

			 * Set the 'freeze all counters' bit, and disable

			 * interrupts.  The barrier is to make sure the

			 * mtpmr has been executed and the PMU has frozen

			 * the events before we return.

/*

 * Re-enable all events if disable == 0.

 * If we were previously disabled and events were added, then

 * put the new config on the PMU.

 context locked on entry */

	/*

	 * Allocate counters from top-down, so that restricted-capable

	 * counters are kept free as long as possible.

 context locked on entry */

	/*

	 * TODO: if at least one restricted event exists, and we

	 * just freed up a non-restricted-capable counter, and

	 * there is a restricted-capable counter occupied by

	 * a non-restricted event, migrate that event to the

	 * vacated counter.

/*

 * Release the PMU if this is the last perf_event.

/*

 * Translate a generic cache event_id config to a raw event_id code.

 unpack config */

	/*

	 * If this is in a group, check if it can go on with all the

	 * other hardware events in the group.  We assume the event

	 * hasn't been linked into its leader's sibling list at this point.

	/*

	 * See if we need to reserve the PMU.

	 * If no events are currently in use, then we have to take a

	 * mutex to ensure that we don't race with another task doing

	 * reserve_pmc_hardware or release_pmc_hardware.

/*

 * A counter has overflowed; update its count and record

 * things if requested.  Note that interrupts are hard-disabled

 * here so there is no possibility of being interrupted.

 we don't have to worry about interrupts here */

	/*

	 * See if the total period for this event has expired,

	 * and update for the next period.

	/*

	 * Finally record data if requested.

 event has overflowed */

				/*

				 * Disabled counter is negative,

				 * reset it just in case.

 PMM will keep counters frozen until we return from the interrupt. */

 something's already registered */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter support for e500 family processors.

 *

 * Copyright 2008-2009 Paul Mackerras, IBM Corporation.

 * Copyright 2010 Freescale Semiconductor, Inc.

/*

 * Map of generic hardware event types to hardware events

 * Zero if unsupported

 Data L1 cache reloads */

/*

 * Table of generalized cache-related events.

 * 0 means not supported, -1 means nonsensical, other values

 * are event codes.

	/*

	 * D-cache misses are not split into read/write/prefetch;

	 * use raw event 41.

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

	/*

	 * Assuming LL means L2, it's not a good match for this model.

	 * It allocates only on L1 castout or explicit prefetch, and

	 * does not have separate read/write events (but it does have

	 * separate instruction/data events).

 	RESULT_ACCESS	RESULT_MISS */

	/*

	 * There are data/instruction MMU misses, but that's a miss on

	 * the chip's internal level-one TLB which is probably not

	 * what the user wants.  Instead, unified level-two TLB misses

	 * are reported here.

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 Upper half of event id is PMLCb, for threshold events */

 Threshold requested on non-threshold event */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter support for POWER6 processors.

 *

 * Copyright 2008-2009 Paul Mackerras, IBM Corporation.

/*

 * Bits in event code for POWER6

 PMC number (1-based) for direct events */

 Unit event comes (TTMxSEL encoding) */

 Load lookahead match value */

 Load lookahead match enable */

 Byte of event bus to use */

 Subunit event comes from (NEST_SEL enc.) */

 PMCxSEL value */

/*

 * Bits in MMCR1 for POWER6

/*

 * Map of which direct events on which PMCs are marked instruction events.

 * Indexed by PMCSEL value >> 1.

 * Bottom 4 bits are a map of which PMCs are interesting,

 * top 4 bits say what sort of event:

 *   0 = direct marked event,

 *   1 = byte decode event,

 *   4 = add/and event (PMC1 -> bits 0 & 4),

 *   5 = add/and event (PMC1 -> bits 1 & 5),

 *   6 = add/and event (PMC1 -> bits 2 & 6),

 *   7 = add/and event (PMC1 -> bits 3 & 7).

 00 */

 02 */

 04 */

 06 PM_MRK_ST_CMPL, PM_MRK_ST_GPS, PM_MRK_ST_CMPL_INT */

 08 PM_MRK_DFU_FIN */

 0a PM_MRK_IFU_FIN, PM_MRK_INST_FIN */

 0c */

 0e */

 10 PM_MRK_INST_DISP */

 12 PM_MRK_LSU_DERAT_MISS */

 14 */

 16 */

 18 PM_THRESH_TIMEO, PM_MRK_INST_FIN */

 1a PM_MRK_INST_DISP, PM_MRK_{FXU,FPU,LSU}_FIN */

 1c PM_MRK_INST_ISSUED */

 1e */

 20 */

 22 */

 24 */

 26 */

 28 PM_MRK_DATA_FROM_L2MISS, PM_MRK_DATA_FROM_L3MISS */

 2a */

 2c */

 2e */

 30 */

 32 */

 34 */

 36 */

 38 */

 3a */

 3c */

 3e PM_MRK_INST_TIMEO */

 40 */

 42 */

 44 */

 46 */

 48 */

 4a */

 4c */

 4e */

 50 */

 52 PM_MRK_BR_TAKEN, PM_MRK_BR_MPRED */

 54 PM_MRK_PTEG_FROM_L3MISS, PM_MRK_PTEG_FROM_L2MISS */

 56 PM_MRK_LD_MISS_L1 */

 58 */

 5a */

 5c */

 5e */

/*

 * Masks showing for each unit which bits are marked events.

 * These masks are in LE order, i.e. 0x00000001 is byte 0, bit 0.

 direct events set 1: byte 3 bit 0 */

 direct events set 2: byte 2 bit 0 */

 IDU, IFU, nest: nothing */

 VMX set 1: byte 0 bits 3, 7 */

 VMX set 2: byte 0 bits 4-7 */

 LSU set 1: byte 2 bit 0, byte 3 bit 2 */

 LSU set 2: byte 2 bit 0, all of byte 3 */

 LSU set 3 */

 VMX set 3: byte 0 bit 4 */

 BFP set 1 */

 BFP set 2: byte 0 bits 1, 5 */

/*

 * Returns 1 if event counts things relating to marked instructions

 * and thus needs the MMCRA_SAMPLE_ENABLE bit set, or 0 if not.

 drop edge/level bit */

/*

 * Assign PMC numbers and compute MMCR1 value for a set of events

 collision! */

 can go on any PMC; find a free one */

 this event uses the event bus */

 check for conflict on this byte of event bus */

 Nest events have a further mux */

 these need the PMCx_ADDR_SEL bits */

 bus select values are different for PMC3/4 */

/*

 * Layout of constraint bits:

 *

 *	0-1	add field: number of uses of PMC1 (max 1)

 *	2-3, 4-5, 6-7, 8-9, 10-11: ditto for PMC2, 3, 4, 5, 6

 *	12-15	add field: number of uses of PMC1-4 (max 4)

 *	16-19	select field: unit on byte 0 of event bus

 *	20-23, 24-27, 28-31 ditto for bytes 1, 2, 3

 *	32-34	select field: nest (subunit) event selector

 add field for count of PMC1-4 uses */

 at most 4 alternatives for any event */

 PM_PTEG_RELOAD_VALID */

 PM_LD_MISS_L1 */

 PM_ST_MISS_L1 */

 PM_RUN_CYC */

 PM_RUN_COUNT */

 PM_PURR */

 PM_FLUSH */

 PM_MRK_INST_DISP */

 PM_TB_BIT_TRANS */

 PM_ST_FIN */

 PM_L1_ICACHE_MISS */

 PM_INST_IMC_MATCH_CMPL */

 PM_GCT_EMPTY_CYC */

 PM_LSU_DERAT_MISS_CYC */

 PM_LSU_DERAT_MISS */

 PM_INST_DISP */

 PM_INST_DISP */

 PM_EXT_INT */

 PM_DATA_FROM_L2MISS */

 PM_MRK_FPU_FIN */

 PM_MRK_INST_FIN */

 PM_L1_DCACHE_RELOAD_VALID */

 PM_DATA_FROM_L3MISS */

/*

 * This could be made more efficient with a binary search on

 * a presorted list, if necessary

 check the alternatives table */

 copy out alternatives from list */

 Check for alternative ways of computing sum events */

 PMCSEL 0x32 counter N == PMCSEL 0x34 counter 5-N */

 ignore edge bit */

 PMCSEL 0x38 counter N == PMCSEL 0x3a counter N+/-2 */

		/*

		 * We're only counting in RUN state,

		 * so PM_CYC is equivalent to PM_RUN_CYC,

		 * PM_INST_CMPL === PM_RUN_INST_CMPL, PM_PURR === PM_RUN_PURR.

		 * This doesn't include alternatives that don't provide

		 * any extra flexibility in assigning PMCs (e.g.

		 * 0x10000a for PM_RUN_CYC vs. 0x1e for PM_CYC).

		 * Note that even with these additional alternatives

		 * we never end up with more than 4 alternatives for any event.

 PM_CYC */

 PM_RUN_CYC */

 PM_RUN_CYC */

 PM_CYC */

 PM_INST_CMPL */

 PM_RUN_INST_CMPL */

 PM_RUN_INST_CMPL */

 PM_INST_CMPL */

 PM_PURR */

 PM_RUN_PURR */

 PM_RUN_PURR */

 PM_PURR */

 remove the limited PMC events */

 remove all but the limited PMC events */

 Set PMCxSEL to 0 to disable PMCx */

 LD_REF_L1 */

 LD_MISS_L1 */

 BR_PRED */

 BR_MPRED */

/*

 * Table of generalized cache-related events.

 * 0 means not supported, -1 means nonsensical, other values

 * are event codes.

 * The "DTLB" and "ITLB" events relate to the DERAT and IERAT.

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 	RESULT_ACCESS	RESULT_MISS */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance counter callchain support - powerpc architecture code

 *

 * Copyright  2009 Paul Mackerras, IBM Corporation.

/*

 * Is sp valid as the address of the next kernel stack frame after prev_sp?

 * The next frame may be in a different stack area but should not go

 * back down in the same stack area.

 must be 16-byte aligned */

	/*

	 * sp could decrease when we jump off an interrupt stack

	 * back to the regular process stack.

			/*

			 * This looks like an interrupt frame for an

			 * interrupt that occurred in the kernel

			/*

			 * We can't tell which of the first two addresses

			 * we get are valid, but we can filter out the

			 * obviously bogus ones here.  We replace them

			 * with 0 rather than removing them entirely so

			 * that userspace can tell which is which.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Performance event support - powerpc architecture code

 *

 * Copyright 2008-2009 Paul Mackerras, IBM Corporation.

 BHRB bits */

 BHRB HW branch filter */

 Store the PMC values */

/*

 * Normally, to ignore kernel events we set the FCS (freeze counters

 * in supervisor mode) bit in MMCR0, but if the kernel runs with the

 * hypervisor bit set in the MSR, or if we are running on a processor

 * where the hypervisor bit is forced to 1 (as on Apple G5 processors),

 * then we need to use the FCHV bit to ignore kernel events.

/*

 * 32-bit doesn't have MMCRA but does have an MMCR2,

 * and a few other names are different.

 * Also 32-bit doesn't have MMCR3, SIER2 and SIER3.

 * Define them as zero knowing that any code path accessing

 * these registers (via mtspr/mfspr) are done under ppmu flag

 * check for PPMU_ARCH_31 and we will not enter that code path

 * for 32-bit.

 CONFIG_PPC32 */

/*

 * Return PMC value corresponding to the

 * index passed.

	/*

	 * When we take a performance monitor exception the regs are setup

	 * using perf_read_regs() which overloads some fields, in particular

	 * regs->result to tell us whether to use SIAR.

	 *

	 * However if the regs are from another exception, eg. a syscall, then

	 * they have not been setup using perf_read_regs() and so regs->result

	 * is something random.

/*

 * Things that are specific to 64-bit implementations.

/*

 * The user wants a data address recorded.

 * If we're not doing instruction sampling, give them the SDAR

 * (sampled data address).  If we are doing instruction sampling, then

 * only give them the SDAR if it corresponds to the instruction

 * pointed to by SIAR; this is indicated by the [POWER6_]MMCRA_SDSYNC, the

 * [POWER7P_]MMCRA_SDAR_VALID bit in MMCRA, or the SDAR_VALID bit in SIER.

	/*

	 * Check the address in SIAR to identify the

	 * privilege levels since the SIER[MSR_HV, MSR_PR]

	 * bits are not set for marked events in power10

	 * DD1.

	/*

	 * If we don't have flags in MMCRA, rather than using

	 * the MSR, we intuit the flags from the address in

	 * SIAR which should give slightly more reliable

	 * results

 PR has priority over HV, so order below is important */

/*

 * Overload regs->dsisr to store MMCRA so we only need to read it once

 * on each interrupt.

 * Overload regs->dar to store SIER if we have it.

 * Overload regs->result to specify whether we should use the MSR (result

 * is zero) or the SIAR (result is non zero).

	/*

	 * If this isn't a PMU exception (eg a software event) the SIAR is

	 * not valid. Use pt_regs.

	 *

	 * If it is a marked event use the SIAR.

	 *

	 * If the PMU doesn't update the SIAR for non marked events use

	 * pt_regs.

	 *

	 * If regs is a kernel interrupt, always use SIAR. Some PMUs have an

	 * issue with regs_sipr not being in synch with SIAR in interrupt entry

	 * and return sequences, which can result in regs_sipr being true for

	 * kernel interrupts and SIAR, which has the effect of causing samples

	 * to pile up at mtmsrd MSR[EE] 0->1 or pending irq replay around

	 * interrupt entry/exit.

	 *

	 * If the PMU has HV/PR flags then check to see if they

	 * place the exception in userspace. If so, use pt_regs. In

	 * continuous sampling mode the SIAR and the PMU exception are

	 * not synchronised, so they may be many instructions apart.

	 * This can result in confusing backtraces. We still want

	 * hypervisor samples as well as samples in the kernel with

	 * interrupts off hence the userspace check.

/*

 * On processors like P7+ that have the SIAR-Valid bit, marked instructions

 * must be sampled only if the SIAR-valid bit is set.

 *

 * For unmarked instructions and for processors that don't have the SIAR-Valid

 * bit, assume that SIAR is valid.

		/*

		 * SIER[SIAR_VALID] is not set for some

		 * marked events on power10 DD1, so drop

		 * the check for SIER[SIAR_VALID] and return true.

 Reset all possible BHRB entries */

 Clear BHRB if we changed task context to avoid data leaks */

		/* BHRB cannot be turned off when other

		 * events are active on the PMU.

 avoid stale pointer */

/* Called from ctxsw to prevent one process's branch entries to

 * mingle with the other process's entries during context switch.

 Calculate the to address for a branch */

 Userspace: need copy instruction here then translate it */

 Translate relative branch target from kernel to user address */

 Processing BHRB entries */

 Assembly read function */

 Terminal marker: End of valid BHRB entries */

 invalid entry */

			/*

			 * BHRB rolling buffer could very much contain the kernel

			 * addresses at this point. Check the privileges before

			 * exporting it to userspace (avoid exposure of regions

			 * where we could have speculative execution)

			 * Incase of ISA v3.1, BHRB will capture only user-space

			 * addresses, hence include a check before filtering code

			/* Branches are read most recent first (ie. mfbhrb 0 is

			 * the most recent branch).

			 * There are two types of valid entries:

			 * 1) a target entry which is the to address of a

			 *    computed goto like a blr,bctr,btar.  The next

			 *    entry read from the bhrb will be branch

			 *    corresponding to this target (ie. the actual

			 *    blr/bctr/btar instruction).

			 * 2) a from address which is an actual branch.  If a

			 *    target entry proceeds this, then this is the

			 *    matching branch for that target.  If this is not

			 *    following a target entry, then this is a branch

			 *    where the target is given as an immediate field

			 *    in the instruction (ie. an i or b form branch).

			 *    In this case we need to read the instruction from

			 *    memory to determine the target/to address.

				/* Target branches use two entries

				 * (ie. computed gotos/XL form)

 Get from address in next entry */

					/* Shouldn't have two targets in a

				/* Branches to immediate field 

	/*

	 * This could be a per-PMU callback, but we'd rather avoid the cost. We

	 * check that the PMU supports EBB, meaning those that don't can still

	 * use bit 63 of the event code for something else if they wish.

 Event and group leader must agree on EBB */

	/*

	 * IFF this is the first time we've added an EBB event, set

	 * PMXE in the user MMCR0 so we can detect when it's cleared by

	 * userspace. We need this so that we can context switch while

	 * userspace is in the EBB handler (where PMXE is 0).

 Enable EBB and read/write to all 6 PMCs and BHRB for userspace */

	/*

	 * Add any bits from the user MMCR0, FC or PMAO. This is compatible

	 * with pmao_restore_workaround() because we may add PMAO but we never

	 * clear it here.

	/*

	 * Be careful not to set PMXE if userspace had it cleared. This is also

	 * compatible with pmao_restore_workaround() because it has already

	 * cleared PMXE and we leave PMAO alone.

	/*

	 * Merge the kernel & user values of MMCR2. The semantics we implement

	 * are that the user MMCR2 can set bits, ie. cause counters to freeze,

	 * but not clear bits. If a task wants to be able to clear bits, ie.

	 * unfreeze counters, it should not set exclude_xxx in its events and

	 * instead manage the MMCR2 entirely by itself.

	/*

	 * On POWER8E there is a hardware defect which affects the PMU context

	 * switch logic, ie. power_pmu_disable/enable().

	 *

	 * When a counter overflows PMXE is cleared and FC/PMAO is set in MMCR0

	 * by the hardware. Sometime later the actual PMU exception is

	 * delivered.

	 *

	 * If we context switch, or simply disable/enable, the PMU prior to the

	 * exception arriving, the exception will be lost when we clear PMAO.

	 *

	 * When we reenable the PMU, we will write the saved MMCR0 with PMAO

	 * set, and this _should_ generate an exception. However because of the

	 * defect no exception is generated when we write PMAO, and we get

	 * stuck with no counters counting but no exception delivered.

	 *

	 * The workaround is to detect this case and tweak the hardware to

	 * create another pending PMU exception.

	 *

	 * We do that by setting up PMC6 (cycles) for an imminent overflow and

	 * enabling the PMU. That causes a new exception to be generated in the

	 * chip, but we don't take it yet because we have interrupts hard

	 * disabled. We then write back the PMU state as we want it to be seen

	 * by the exception handler. When we reenable interrupts the exception

	 * handler will be called and see the correct state.

	 *

	 * The logic is the same for EBB, except that the exception is gated by

	 * us having interrupts hard disabled as well as the fact that we are

	 * not in userspace. The exception is finally delivered when we return

	 * to userspace.

 Only if PMAO is set and PMAO_SYNC is clear */

 If we're doing EBB, only if BESCR[GE] is set */

	/*

	 * We are already soft-disabled in power_pmu_enable(). We need to hard

	 * disable to actually prevent the PMU exception from firing.

	/*

	 * This is a bit gross, but we know we're on POWER8E and have 6 PMCs.

	 * Using read/write_pmc() in a for loop adds 12 function calls and

	 * almost doubles our code size.

 Ensure all freeze bits are unset */

 Set up PMC6 to overflow in one cycle */

 Enable exceptions and unfreeze PMC6 */

 Now we need to refreeze and restore the PMCs */

 CONFIG_PPC64 */

/*

 * Read one performance monitor counter (PMC).

 CONFIG_PPC64 */

/*

 * Write one PMC.

 CONFIG_PPC64 */

 Called from sysrq_handle_showregs() */

/*

 * Check if a set of events can all go on the PMU at once.

 * If they can't, this will look at alternative codes for the events

 * and see if any combination of alternative codes is feasible.

 * The feasible set is returned in event_id[].

 First see if the events will go on as-is */

 all OK */

 doesn't work, gather alternatives... */

 enumerate all possibilities and see if any will work */

 we're backtracking, restore context */

		/*

		 * See if any alternative k for event_id i,

		 * where k > j, will satisfy the constraints.

			/*

			 * No feasible alternative, backtrack

			 * to event_id i-1 and continue enumerating its

			 * alternatives from where we got up to.

			/*

			 * Found a feasible alternative for event_id i,

			 * remember where we got up to with this event_id,

			 * go on to the next event_id, and start with

			 * the first alternative for it.

 OK, we have a feasible combination, tell the caller the solution */

/*

 * Check if newly-added events have consistent settings for

 * exclude_{user,kernel,hv} with each other and any previously

 * added events.

	/*

	 * If the PMU we're on supports per event exclude settings then we

	 * don't need to do any of this logic. NB. This assumes no PMU has both

	 * per event exclude and limited PMCs.

	/*

	 * POWER7 can roll back counter values, if the new value is smaller

	 * than the previous value it will cause the delta and the counter to

	 * have bogus values unless we rolled a counter over.  If a coutner is

	 * rolled back, it will be smaller, but within 256, which is the maximum

	 * number of events to rollback at once.  If we detect a rollback

	 * return 0.  This can lead to a small lack of precision in the

	 * counters.

	/*

	 * Performance monitor interrupts come even when interrupts

	 * are soft-disabled, as long as interrupts are hard-enabled.

	 * Therefore we treat them like NMIs.

	/*

	 * A number of places program the PMC with (0x80000000 - period_left).

	 * We never want period_left to be less than 1 because we will program

	 * the PMC with a value >= 0x800000000 and an edge detected PMC will

	 * roll around to 0 before taking an exception. We have seen this

	 * on POWER8.

	 *

	 * To fix this, clamp the minimum value of period_left to 1.

/*

 * On some machines, PMC5 and PMC6 can't be written, don't respect

 * the freeze conditions, and don't generate interrupts.  This tells

 * us if `event' is using such a PMC.

/*

 * Since limited events don't respect the freeze conditions, we

 * have to read them immediately after freezing or unfreezing the

 * other events.  We try to keep the values from the limited

 * events as consistent as possible by keeping the delay (in

 * cycles and instructions) between freezing/unfreezing and reading

 * the limited events as small and consistent as possible.

 * Therefore, if any limited events are in use, we read them

 * both, and always in the same order, to minimize variability,

 * and do it inside the same asm that writes MMCR0.

	/*

	 * Write MMCR0, then read PMC5 and PMC6 immediately.

	 * To ensure we don't get a performance monitor interrupt

	 * between writing MMCR0 and freezing/thawing the limited

	 * events, we first write MMCR0 with the event overflow

	 * interrupt enable bits turned off.

	/*

	 * Write the full MMCR0 including the event overflow interrupt

	 * enable bits, if necessary.

/*

 * Disable all events to prevent PMU interrupts and to allow

 * events to be added or removed.

		/*

		 * Check if we ever enabled the PMU on this cpu.

		/*

		 * Set the 'freeze counters' bit, clear EBE/BHRBA/PMCC/PMAO/FC56

 Set mmcr0 PMCCEXT for p10 */

		/*

		 * The barrier is to make sure the mtspr has been

		 * executed and the PMU has frozen the events etc.

		 * before we return.

		/*

		 * Disable instruction sampling if it was enabled

 Disable BHRB via mmcra (BHRBRD) for p10 */

		/*

		 * Write SPRN_MMCRA if mmcra has either disabled

		 * instruction sampling or BHRB.

		/*

		 * These are readable by userspace, may contain kernel

		 * addresses and are not switched by context switch, so clear

		 * them now to avoid leaking anything to userspace in general

		 * including to another process.

/*

 * Re-enable all events if disable == 0.

 * If we were previously disabled and events were added, then

 * put the new config on the PMU.

	/*

	 * EBB requires an exclusive group and all events must have the EBB

	 * flag set, or not set, so we can just check a single event. Also we

	 * know we have at least one event.

	/*

	 * If we didn't change anything, or only removed events,

	 * no need to recalculate MMCR* settings and reset the PMCs.

	 * Just reenable the PMU with the current MMCR* settings

	 * (possibly updated for removal of events).

	/*

	 * Clear all MMCR settings and recompute them for the new set of events.

 shouldn't ever get here */

		/*

		 * Add in MMCR0 freeze bits corresponding to the attr.exclude_*

		 * bits for the first event. We have already checked that all

		 * events have the same value for these bits as the first event.

	/*

	 * Write the new configuration to MMCR* with the freeze

	 * bit set and set the hardware events to their initial values.

	 * Then unfreeze the events.

	/*

	 * Read off any pre-existing events that need to move

	 * to another PMC.

	/*

	 * Initialize the PMCs for all the new and moved events.

	/*

	 * Enable instruction sampling if necessary

/*

 * Add an event to the PMU.

 * If all events are not already frozen, then we disable and

 * re-enable the PMU in order to get hw_perf_enable to do the

 * actual work of reconfiguring the PMU.

	/*

	 * Add the event to the list (if there is room)

	 * and check whether the total set is still feasible.

	/*

	 * This event may have been disabled/stopped in record_and_restart()

	 * because we exceeded the ->event_limit. If re-starting the event,

	 * clear the ->hw.state (STOPPED and UPTODATE flags), so the user

	 * notification is re-enabled.

	/*

	 * If group events scheduling transaction was started,

	 * skip the schedulability test here, it will be performed

	 * at commit time(->commit_txn) as a whole

/*

 * Remove an event from the PMU.

 disable exceptions if no events are running */

/*

 * POWER-PMU does not support disabling individual counters, hence

 * program their cycle counter to their max value and ignore the interrupts.

/*

 * Start group events scheduling transaction

 * Set the flag to make pmu::enable() not perform the

 * schedulability test, it will be performed at commit time

 *

 * We only support PERF_PMU_TXN_ADD transactions. Save the

 * transaction flags but otherwise ignore non-PERF_PMU_TXN_ADD

 * transactions.

 txn already in flight */

/*

 * Stop group events scheduling transaction

 * Clear the flag and pmu::enable() will perform the

 * schedulability test.

 no txn in flight */

/*

 * Commit group events scheduling transaction

 * Perform the group schedulability test as a whole

 * Return 0 if success

 no txn in flight */

/*

 * Return 1 if we might be able to put event on a limited PMC,

 * or 0 if not.

 * An event can only go on a limited PMC if it counts something

 * that a limited PMC can count, doesn't require interrupts, and

 * doesn't exclude any processor mode.

	/*

	 * The requested event_id isn't on a limited PMC already;

	 * see if any alternative code goes on a limited PMC.

/*

 * Find an alternative event_id that goes on a normal PMC, if possible,

 * and return the event_id code, or 0 if there is no such alternative.

 * (Note: event_id code 0 is "don't count" on all machines.)

 Number of perf_events counting hardware events */

 Used to avoid races in calling reserve/release_pmc_hardware */

/*

 * Release the PMU if this is the last perf_event.

/*

 * Translate a generic cache event_id config to a raw event_id code.

 unpack config */

 PMU has BHRB enabled */

	/*

	 * PMU config registers have fields that are

	 * reserved and some specific values for bit fields are reserved.

	 * For ex., MMCRA[61:62] is Randome Sampling Mode (SM)

	 * and value of 0b11 to this field is reserved.

	 * Check for invalid values in attr.config.

	/*

	 * If we are not running on a hypervisor, force the

	 * exclude_hv bit to 0 so that we don't care what

	 * the user set it to.

	/*

	 * If this is a per-task event, then we can use

	 * PM_RUN_* events interchangeably with their non RUN_*

	 * equivalents, e.g. PM_RUN_CYC instead of PM_CYC.

	 * XXX we should check if the task is an idle task.

	/*

	 * If this machine has limited events, check whether this

	 * event_id could go on a limited event.

			/*

			 * The requested event_id is on a limited PMC,

			 * but we can't use a limited PMC; see if any

			 * alternative goes on a normal PMC.

 Extra checks for EBB */

	/*

	 * If this is in a group, check if it can go on with all the

	 * other hardware events in the group.  We assume the event

	 * hasn't been linked into its leader's sibling list at this point.

	/*

	 * For EBB events we just context switch the PMC value, we don't do any

	 * of the sample_period logic. We use hw.prev_count for this.

	/*

	 * See if we need to reserve the PMU.

	 * If no events are currently in use, then we have to take a

	 * mutex to ensure that we don't race with another task doing

	 * reserve_pmc_hardware or release_pmc_hardware.

/*

 * A counter has overflowed; update its count and record

 * things if requested.  Note that interrupts are hard-disabled

 * here so there is no possibility of being interrupted.

 we don't have to worry about interrupts here */

	/*

	 * See if the total period for this event has expired,

	 * and update for the next period.

			/*

			 * If address is not requested in the sample via

			 * PERF_SAMPLE_IP, just record that sample irrespective

			 * of SIAR valid check.

	/*

	 * Due to hardware limitation, sometimes SIAR could sample a kernel

	 * address even when freeze on supervisor state (kernel) is set in

	 * MMCR2. Check attr.exclude_kernel and address to drop the sample in

	 * these cases.

	/*

	 * Finally record data if requested.

 Account for interrupt in case of invalid SIAR */

/*

 * Called from generic code to get the misc flags (i.e. processor mode)

 * for an event_id.

/*

 * Called from generic code to get the instruction pointer

 * for an event_id.

	/*

	 * Events on POWER7 can roll back if a speculative event doesn't

	 * eventually complete. Unfortunately in some rare cases they will

	 * raise a performance monitor exception. We need to catch this to

	 * ensure we reset the PMC. In all cases the PMC will be 256 or less

	 * cycles from overflow.

	 *

	 * We only do this if the first pass fails to find any overflowing

	 * PMCs because a user might set a period of less than 256 and we

	 * don't want to mistakenly reset them.

/*

 * Performance monitor interrupt stuff

 Read all the PMCs since we'll need them a bunch of times */

 Try to find what caused the IRQ */

 these won't generate IRQs */

		/*

		 * We've found one that's overflowed.  For active

		 * counters we need to log this.  For inactive

		 * counters, we need to reset it anyway

 reset non active counters that have overflowed */

 check active counters for special buggy p7 overflow */

 event has overflowed in a buggy way*/

	/*

	 * Reset MMCR0 to its normal value.  This will set PMXE and

	 * clear FC (freeze counters) and PMAO (perf mon alert occurred)

	 * and thus allow interrupts to occur again.

	 * XXX might want to use MSR.PM to keep the events frozen until

	 * we get back out of this interrupt.

 Clear the cpuhw->pmcs */

 something's already registered */

	/*

	 * Use FCHV to ignore kernel events if MSR.HV is set.

 CONFIG_PPC64 */

 run through all the pmu drivers one at a time */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright 2010 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

#define DEBUG_RESIZE_HPT	1

 These fields read-only after init */

 These fields protected by kvm->arch.mmu_setup_lock */

	/* Possible values and their usage:

	 *  <0     an error occurred during allocation,

	 *  -EBUSY allocation is in the progress,

	 *  0      allocation made successfuly.

	/* Private to the work thread, until error != -EBUSY,

	 * then protected by kvm->arch.mmu_setup_lock.

 HPTEs are 2**4 bytes long */

 Allocate reverse map array */

 order mmu_ready vs. vcpus_running */

 We already have a suitable HPT */

 Set the entire HPT to 0, i.e. invalid HPTEs */

		/*

		 * Reset all the reverse-mapping chains for all memslots

 Ensure that each vcpu will flush its TLB on next entry. */

 Bits in first HPTE dword for pagesize 4k, 64k or 16M */

 Bits in second HPTE dword for pagesize 4k, 64k or 16M */

 VRMA can't be > 1TB */

 Can't use more than 1 HPTE per HPTEG */

 can't use hpt_hash since va > 64 bits */

		/*

		 * We assume that the hash table is empty and no

		 * vcpus are using it at this stage.  Since we create

		 * at most one HPTE per HPTEG, we just assume entry 7

		 * is available and use it.

 POWER8 and above have 12-bit LPIDs (10-bit in POWER7) */

 rsvd_lpid is reserved for use in partition switching */

 this can't happen */

 or something */

 Get SLB entry */

 real mode access */

 Find the HPTE in the hash table */

 Get PP bits and key for permission check */

 Calculate permissions */

 Storage key permission check for POWER7 */

 Get the guest physical address */

/*

 * Quick test for whether an instruction is a load or a store.

 * If the instruction is a load or a store, then this will indicate

 * which it is, at least on server processors.  (Embedded processors

 * have some external PID instructions that don't follow the rule

 * embodied here.)  If the instruction isn't a load or store, then

 * this doesn't return anything useful.

 major opcode 31 */

	/*

	 * Fast path - check if the guest physical address corresponds to a

	 * device on the FAST_MMIO_BUS, if so we can avoid loading the

	 * instruction all together, then we can just handle it and return.

	/*

	 * If we fail, we just return to the guest and try executing it again.

	/*

	 * WARNING: We do not know for sure whether the instruction we just

	 * read from memory is the same that caused the fault in the first

	 * place.  If the instruction we read is neither an load or a store,

	 * then it can't access memory, so we don't need to worry about

	 * enforcing access permissions.  So, assuming it is a load or

	 * store, we just check that its direction (load or store) is

	 * consistent with the original fault, since that's what we

	 * checked the access permissions against.  If there is a mismatch

	 * we just return and retry the instruction.

	/*

	 * Emulated accesses are emulated by looking at the hash for

	 * translation once, then performing the access later. The

	 * translation could be invalidated in the meantime in which

	 * point performing the subsequent memory access on the old

	 * physical address could possibly be a security hole for the

	 * guest (but not the host).

	 *

	 * This is less of an issue for MMIO stores since they aren't

	 * globally visible. It could be an issue for MMIO loads to

	 * a certain extent but we'll ignore it for now.

	/*

	 * Real-mode code has already searched the HPT and found the

	 * entry we're interested in.  Lock the entry and check that

	 * it hasn't changed.  If it has, just return and re-execute the

	 * instruction.

 Translate the logical address and get the page */

 No memslot means it's an emulated MMIO region */

	/*

	 * This should never happen, because of the slot_is_aligned()

	 * check in kvmppc_do_h_enter().

 used to check for invalidations in progress */

 If writing != 0, then the HPTE must allow writing, if we get here */

	/*

	 * Do a fast check first, since __gfn_to_pfn_memslot doesn't

	 * do it with !atomic && !async, which is how we call it.

	 * We always ask for write permission since the common case

	 * is that the page is writable.

 Call KVM generic code to do the slow-path check */

	/*

	 * Read the PTE from the process' radix tree and use that

	 * so we get the shift and attribute bits.

	/*

	 * If the PTE disappeared temporarily due to a THP

	 * collapse, just return and let the guest try again.

 Check WIMG vs. the actual page we're accessing */

		/*

		 * Allow guest to map emulated device memory as

		 * uncacheable, but actually make it cacheable.

	/*

	 * Set the HPTE to point to hpa.

	 * Since the hpa is at PAGE_SIZE granularity, make sure we

	 * don't mask out lower-order bits if psize < PAGE_SIZE.

	/*

	 * If the HPT is being resized, don't update the HPTE,

	 * instead let the guest retry after the resize operation is complete.

	 * The synchronization for mmu_ready test vs. set is provided

	 * by the HPTE lock.

 HPTE has been changed under us; let the guest retry */

 Always put the HPTE in the rmap chain for the page base address */

 Check if we might have been invalidated; let the guest retry if so */

 Only set R/C in real HPTE if set in both *rmap and guest_rpte */

 HPTE was previously valid, so we need to invalidate it */

 don't lose previous R and C bits */

 Mutual exclusion with kvm_unmap_hva_range etc. */

		/*

		 * This assumes it is acceptable to lose reference and

		 * change bits across a reset.

 Must be called with both HPTE and rmap locked */

 chain is now empty */

 remove i from chain */

 Now check and modify the HPTE */

 Harvest R and C */

		/*

		 * To avoid an ABBA deadlock with the HPTE lock bit,

		 * we can't spin on the HPTE lock while holding the

		 * rmap chain lock.

 unlock rmap before spinning on the HPTE lock */

		/*

		 * Testing the present bit without locking is OK because

		 * the memslot has been marked invalid already, and hence

		 * no new HPTEs referencing this page can be created,

		 * thus the present bit can't go from 0 to 1.

 If this HPTE isn't referenced, ignore it */

 unlock rmap before spinning on the HPTE lock */

 Now check and modify the HPTE */

/*

 * Returns the number of system pages that are dirty.

 * This can be more than 1 if we find a huge-page HPTE.

		/*

		 * Checking the C (changed) bit here is racy since there

		 * is no guarantee about when the hardware writes it back.

		 * If the HPTE is not writable then it is stable since the

		 * page can't be written to, and we would have done a tlbie

		 * (which forces the hardware to complete any writeback)

		 * when making the HPTE read-only.

		 * If vcpus are running then this call is racy anyway

		 * since the page could get dirtied subsequently, so we

		 * expect there to be a further call which would pick up

		 * any delayed C bit writeback.

		 * Otherwise we need to do the tlbie even if C==0 in

		 * order to pick up any delayed writeback of C.

 unlock rmap before spinning on the HPTE lock */

 Now check and modify the HPTE */

 need to make it temporarily absent so C is stable */

		/*

		 * Note that if npages > 0 then i must be a multiple of npages,

		 * since we always put huge-page HPTEs in the rmap chain

		 * corresponding to their page base address.

 We need to mark this page dirty in the memslot dirty_bitmap, if any */

/*

 * HPT resizing

	/* Guest is stopped, so new HPTEs can't be added or faulted

	 * in, only unmapped or altered by host actions.  So, it's

 nothing to do */

 Nothing to do */

 Unmap */

 Reload PTE after unmap */

 We only have 28 - 23 bits of offset in avpn */

 We can find more bits from the pteg value */

 We only have 40 - 23 bits of seg_off in avpn */

 Bolted collision, nothing we can do */

 Discard the new HPTE */

 Discard the previous HPTE */

 No need for a barrier, since new HPT isn't active */

	/* Exchange the pending tables in the resize structure with

 Request is still current? */

		/* We may request large allocations here:

		 * do not sleep with kvm->arch.mmu_setup_lock held for a while.

		/* We have strict assumption about -EBUSY

		 * when preparing for HPT resize.

		/* It is possible that kvm->arch.resize_hpt != resize

		 * after we grab kvm->arch.mmu_setup_lock again.

 Suitable resize in progress? */

 estimated time in ms */

 not suitable, cancel it */

 nothing to do */

 start new resize */

 estimated time in ms */

 Nothing to do, just force a KVM exit */

 This shouldn't be possible */

 Stop VCPUs from running while we mess with the HPT */

	/* Boot all CPUs out of the guest so they re-read

 Let VCPUs run again */

/*

 * Functions for reading and writing the hash table via reads and

 * writes on a file descriptor.

 *

 * Reads return the guest view of the hash table, which has to be

 * pieced together from the real hash table and the guest_rpte

 * values in the revmap array.

 *

 * On writes, each HPTE written is considered in turn, and if it

 * is valid, it is written to the HPT as if an H_ENTER with the

 * exact flag set was done.  When the invalid count is non-zero

 * in the header written to the stream, the kernel will make

 * sure that that many HPTEs are invalid, and invalidate them

 * if not.

/*

 * Returns 1 if this HPT entry has been modified or has pending

 * R/C bit changes.

 Also need to consider changes in reference and changed bits */

 Unmodified entries are uninteresting except on the first pass */

 lock the HPTE so it's stable and read it */

 re-evaluate valid and dirty from synchronized HPTE value */

 Harvest R and C into guest view if necessary */

 only clear modified if this is the right sort of entry */

 Initialize header */

 Skip uninteresting entries, i.e. clean on not-first pass */

 Grab a series of valid entries */

 valid entry, write it out */

 Now skip invalid entries while we can */

 found an invalid entry */

 write back the header */

 Check if we've wrapped around the hash table */

 lock out vcpus from running while we're doing this */

 temporarily */

 order mmu_ready vs. vcpus_running */

 Order HPTE updates vs. mmu_ready */

 reject flags we don't recognize */

 make sure kvmppc_do_h_enter etc. see the increment */

 lock the HPTE so it's stable and read it */

 POWER7/POWER8 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2009. SUSE Linux Products GmbH. All rights reserved.

 *

 * Authors:

 *    Alexander Graf <agraf@suse.de>

 *    Kevin Wolf <mail@kevin-wolf.de>

 *    Paul Mackerras <paulus@samba.org>

 *

 * Description:

 * Functions relating to running KVM on Book 3S processors where

 * we don't have access to hypervisor mode, and we run the guest

 * in problem state (user mode).

 *

 * This file is derived from arch/powerpc/kvm/44x.c,

 * by Hollis Blanchard <hollisb@us.ibm.com>.

 #define EXIT_DEBUG */

 #define DEBUG_EXT */

 Some compatibility defines */

 We are in DR only split real mode */

 We have not fixed up the guest already */

 The code is in fixupable address space */

 If transactional, change to suspend mode on IRQ delivery */

 Disable AIL if supported */

 Enable AIL if supported */

 Copy data needed by real-mode code from vcpu to shadow vcpu */

	/*

	 * Now also save the current time base value. We use this

	 * to find the guest purr and spurr value.

 Guest MSR values */

 Process MSR values */

 External providers the guest reserved */

 64-bit Process MSR values */

	/*

	 * in guest privileged state, we want to fail all TM transactions.

	 * So disable MSR TM bit so that all tbegin. will be able to be

	 * trapped into host.

 Copy data touched by real-mode code from shadow vcpu back to vcpu */

	/*

	 * Maybe we were already preempted and synced the svcpu from

	 * our preempt notifiers. Don't bother touching this svcpu then.

	/*

	 * Update purr and spurr using time base on exit.

	/*

	 * Unlike other MSR bits, MSR[TS]bits can be changed at guest without

	 * notifying host:

	 *  modified by unprivileged instructions like "tbegin"/"tend"/

	 * "tresume"/"tsuspend" in PR KVM guest.

	 *

	 * It is necessary to sync here to calculate a correct shadow_msr.

	 *

	 * privileged guest's tbegin will be failed at present. So we

	 * only take care of problem state guest.

/* loadup math bits which is enabled at kvmppc_get_msr() but not enabled at

 * hardware.

 Indicate we want to get back into the guest */

	/* We misuse TLB_FLUSH to indicate that we want to clear

************ MMU Notifiers *************/

 XXX could be more clever ;) */

 XXX could be more clever ;) */

 The page will get remapped properly on its next fault */

****************************************/

 For PAPR guest, make sure MSR reflects guest mode */

	/* We should never target guest MSR to TS=10 && PR=0,

	 * since we always fail transaction for guest privilege

	 * state.

 Unset POW bit after we woke up */

 Preload magic page segment when in kernel mode */

	/*

	 * When switching from 32 to 64-bit, we may have a stale 32-bit

	 * magic page around, we need to flush it. Typically 32-bit magic

	 * page will be instantiated when calling into RTAS. Note: We

	 * assume that such transition only happens while in kernel mode,

	 * ie, we never transition from user 32-bit to kernel 64-bit with

	 * a 32-bit magic page around.

 going from RTAS to normal kernel code */

 Preload FPU if it's enabled */

	/* If we are in hypervisor level on 970, we can tell the CPU to

	/* Cell performs badly if MSR_FEx are set. So let's hope nobody

	/*

	 * If they're asking for POWER6 or later, set the flag

	 * indicating that we can do multiple large page sizes

	 * and 1TB segments.

	 * Also set the flag that indicates that tlbie has the large

	 * page bit in the RB operand instead of the instruction.

 32 bit Book3S always has 32 byte dcbz */

 On some CPUs we can execute paired single operations natively */

 lonestar 2.0 */

 lonestar 2.2 */

 gekko 1.0 */

 gekko 2.0 */

 gekko 2.3a */

 gekko 2.3b */

 gekko 2.4 */

 gekko 2.4e (8SE) - retail HW2 */

 broadway */

 Enable HID2.PSE - in case we need it later */

/* Book3s_32 CPUs always have 32 bytes cache line size, which Linux assumes. To

 * make Book3s_32 Linux work on Book3s_64, we have to make sure we trap dcbz to

 * emulate 32 bytes dcbz length.

 *

 * The Book3s_64 inventors also realized this case and implemented a special bit

 * in the HID5 register, which is a hypervisor ressource. Thus we can't use it.

 *

 * My approach here is to patch the dcbz instruction on executing pages.

 patch dcbz into reserved instruction, so we trap */

 Resolve real address if translation turned on */

		/*

		 * If we do the dcbz hack, we have to NX on every execution,

		 * so we can patch the executing code. This renders our guest

		 * NX-less.

 Page not found in guest PTE entries, or protection fault */

 Page not found in guest SLB */

			/*

			 * There is already a host HPTE there, presumably

			 * a read-only one for a page the guest thinks

			 * is writable, so get rid of it first.

 The guest's PTE is not mapped yet. Map on the host */

 Exit KVM if mapping failed */

 MMIO */

 Give up external provider (FPU, Altivec, VSX) */

	/*

	 * VSX instructions can access FP and vector registers, so if

	 * we are giving up VSX, make sure we give up FP and VMX as well.

		/*

		 * Note that on CPUs with VSX, giveup_fpu stores

		 * both the traditional FP registers and the added VSX

		 * registers into thread.fp_state.fpr[].

 Give up facility (TAR / EBB / DSCR) */

 Facility not available to the guest, ignore giveup request*/

 Handle external providers (FPU, Altivec, VSX) */

 When we have paired singles, we emulate in software */

 No VSX?  Give an illegal instruction interrupt */

		/*

		 * We have to load up all the FP and VMX registers before

		 * we can let the guest use VSX instructions.

 See if we already own all the ext(s) needed */

/*

 * Kernel code using FP or VMX could have flushed guest state to

 * the thread_struct; if so, get it back now.

 Inject the Interrupt Cause field and trigger a guest interrupt */

 Couldn't emulate, trigger interrupt in guest */

 Enable facilities (TAR, EBB, DSCR) for the guest */

	/*

	 * Not every facility is enabled by FSCR bits, check whether the

	 * guest has this facility enabled at all.

 Facility not enabled by the guest */

 TAR switching isn't lazy in Linux yet */

	/* Since we disabled MSR_TM at privilege state, the mfspr instruction

	 * for TM spr can trigger TM fac unavailable. In this case, the

	 * emulation is handled by kvmppc_emulate_fac(), which invokes

	 * kvmppc_emulate_mfspr() finally. But note the mfspr can include

	 * RT for NV registers. So it need to restore those NV reg to reflect

	 * the update.

 TAR got dropped, drop it in shadow too */

	/*

	 * shadow_srr1 only contains valid flags if we came here via a program

	 * exception. The other exceptions (emulation assist, FP unavailable,

	 * etc.) do not provide flags in SRR1, so use an illegal-instruction

	 * exception when injecting a program interrupt into the guest.

 We get here with MSR.EE=1 */

		/* We set segments as unused segments when invalidating them. So

 only care about PTEG not found errors, but leave NX alone */

			/*

			 * XXX If we do the dcbz hack we use the NX bit to flush&patch the page,

			 *     so we can't use the NX bit inside the guest. Let's cross our fingers,

			 *     that no guest that needs the dcbz hack does NX.

		/* We set segments as unused segments when invalidating them. So

		/*

		 * We need to handle missing shadow PTEs, and

		 * protection faults due to us mapping a page read-only

		 * when the guest thinks it is writable.

 We're good on these - the host merely wanted to get our attention */

 Get last sc for papr */

 The sc instuction points SRR0 to the next inst */

 SC 1 papr hypercalls */

 MOL hypercalls */

 KVM PV hypercalls */

 Guest syscalls */

 Do paired single instruction emulation */

 Enable external provider */

 Ugh - bork here! What did we get? */

		/* To avoid clobbering exit_reason, only check for signals if

		 * we aren't already exiting to userspace for some other

		/*

		 * Interrupts could be timers for the guest which we have to

		 * inject again, so let's postpone them until we're in the guest

		 * and if we really did time things so badly, then we just exit

		 * again due to a host external interrupt.

 interrupts now hard-disabled */

 Flush all SLB entries */

 Flush the MMU after messing with the segments */

		/*

		 * We are only interested in the LPCR_ILE bit

 Always start the shared struct in native endian mode */

	/*

	 * Default to the same as the host if we're on sufficiently

	 * recent machine that we have 1TB segments;

	 * otherwise default to PPC970FX.

 default to book3s_32 (750) */

 Check if we can run the vcpu at all */

	/*

	 * Interrupts could be timers for the guest which we have to inject

	 * again, so let's postpone them until we're in the guest and if we

	 * really did time things so badly, then we just exit again due to

	 * a host external interrupt.

 interrupts now hard-disabled */

 Save FPU, Altivec and VSX state */

 Preload FPU if it's enabled */

	/* No need for guest_exit. It's done in handle_exit.

 Make sure we save the guest FPU/Altivec/VSX state */

 Make sure we save the guest TAR/EBB/DSCR state */

/*

 * Get (and clear) the dirty memory log for a memory slot.

 If nothing is dirty, don't bother messing with page tables. */

 SLB is always 64 entries */

 Standard 4k base page size segment */

	/*

	 * 64k large page size.

	 * We only want to put this in if the CPUs we're emulating

	 * support it, but unfortunately we don't have a vcpu easily

	 * to hand here to test.  Just pick the first vcpu, and if

	 * that doesn't exist yet, report the minimum capability,

	 * i.e., no 64k pages.

	 * 1T segment support goes along with 64k pages.

 Standard 16M large page size segment */

 Require flags and process table base and size to all be zero. */

 We should not get called */

 CONFIG_PPC64 */

 Start out with the default set of hcalls enabled */

	/*

	 * PR KVM can work on POWER9 inside a guest partition

	 * running in HPT mode.  It can't work if we are using

	 * radix translation (because radix provides no way for

	 * a process to have unique translations in quadrant 3).

/*

 * We only support separate modules for book3s 64

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright IBM Corp. 2008

 *

 * Authors: Hollis Blanchard <hollisb@us.ibm.com>

 *          Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>

 Take a lock to avoid concurrent updates */

 sum */

 square sum */

 set min/max */

 save exit time, used next exit when the reenter time is known */

 skip incomplete cycle (e.g. after reset) */

 update statistics for average and standard deviation */

 enter -> timing_last_exit is time spent in guest - log this too */

 Write 'c' to clear the timing statistics. */

		/* Write does not affect our buffers previously generated with

		 * show. seq_file is locked here to prevent races of init with

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2012 Michael Ellerman, IBM Corporation.

 * Copyright 2012 Benjamin Herrenschmidt, IBM Corporation

 -- ICS routines -- */

 -- ICP routines -- */

/*

 * We start the search from our current CPU Id in the core map

 * and go in a circle until we get back to our ID looking for a

 * core that is running in host context and that hasn't already

 * been targeted for another rm_host_ops.

 *

 * In the future, could consider using a fairer algorithm (one

 * that distributes the IPIs better)

 *

 * Returns -1, if no CPU could be found in the host

 * Else, returns a CPU Id which has been reserved for use

 Try to grab this host core if not taken already. */

			/*

			 * Make sure that the store to the rm_action is made

			 * visible before we return to caller (and the

			 * subsequent store to rm_data) to synchronize with

			 * the IPI handler.

 Mark the target VCPU as having an interrupt pending */

 Kick self ? Just set MER and return */

	/*

	 * Check if the core is loaded,

	 * if not, find an available host core to post to wake the VCPU,

	 * if we can't find one, set up state to eventually return too hard.

 Note: Only called on self ! */

 Calculate new output value */

 Attempt atomic update */

	/*

	 * Check for output state update

	 *

	 * Note that this is racy since another processor could be updating

	 * the state already. This is why we never clear the interrupt output

	 * here, we only ever set it. The clear only happens prior to doing

	 * an update and only by the processor itself. Currently we do it

	 * in Accept (H_XIRR) and Up_Cppr (H_XPPR).

	 *

	 * We also do not try to figure out whether the EE state has changed,

	 * we unconditionally set it if the new state calls for it. The reason

	 * for that is that we opportunistically remove the pending interrupt

	 * flag when raising CPPR, so we need to set it back here if an

	 * interrupt is still pending.

 Expose the state change for debug purposes */

 Order this load with the test for need_resend in the caller */

 See if we can deliver */

		/*

		 * If we can, check for a rejection and perform the

		 * delivery

			/*

			 * If we failed to deliver we set need_resend

			 * so a subsequent CPPR state change causes us

			 * to try a new delivery.

	/*

	 * This is used both for initial delivery of an interrupt and

	 * for subsequent rejection.

	 *

	 * Rejection can be racy vs. resends. We have evaluated the

	 * rejection in an atomic ICP transaction which is now complete,

	 * so potentially the ICP can already accept the interrupt again.

	 *

	 * So we need to retry the delivery. Essentially the reject path

	 * boils down to a failed delivery. Always.

	 *

	 * Now the interrupt could also have moved to a different target,

	 * thus we may need to re-do the ICP lookup as well

 Get the ICS state and lock it */

 Unsafe increment, but this does not need to be accurate */

 Get a lock on the ICS */

 Get our server */

 Unsafe increment again*/

 Clear the resend bit of that interrupt */

	/*

	 * If masked, bail out

	 *

	 * Note: PAPR doesn't mention anything about masked pending

	 * when doing a resend, only when doing a delivery.

	 *

	 * However that would have the effect of losing a masked

	 * interrupt that was rejected and isn't consistent with

	 * the whole masked_pending business which is about not

	 * losing interrupts that occur while masked.

	 *

	 * I don't differentiate normal deliveries and resends, this

	 * implementation will differ from PAPR and not lose such

	 * interrupts.

	/*

	 * Try the delivery, this will set the need_resend flag

	 * in the ICP as part of the atomic transaction if the

	 * delivery is not possible.

	 *

	 * Note that if successful, the new delivery might have itself

	 * rejected an interrupt that was "delivered" before we took the

	 * ics spin lock.

	 *

	 * In this case we do the whole sequence all over again for the

	 * new guy. We cannot assume that the rejected interrupt is less

	 * favored than the new one, and thus doesn't need to be delivered,

	 * because by the time we exit icp_rm_try_to_deliver() the target

	 * processor may well have already consumed & completed it, and thus

	 * the rejected interrupt might actually be already acceptable.

		/*

		 * Delivery was successful, did we reject somebody else ?

		/*

		 * We failed to deliver the interrupt we need to set the

		 * resend map bit and mark the ICS state as needing a resend

		/*

		 * Make sure when checking resend, we don't miss the resend

		 * if resend_map bit is seen and cleared.

		/*

		 * If the need_resend flag got cleared in the ICP some time

		 * between icp_rm_try_to_deliver() atomic update and now, then

		 * we know it might have missed the resend_map bit. So we

		 * retry

	/*

	 * This handles several related states in one operation:

	 *

	 * ICP State: Down_CPPR

	 *

	 * Load CPPR with new value and if the XISR is 0

	 * then check for resends:

	 *

	 * ICP State: Resend

	 *

	 * If MFRR is more favored than CPPR, check for IPIs

	 * and notify ICS of a potential resend. This is done

	 * asynchronously (when used in real mode, we will have

	 * to exit here).

	 *

	 * We do not handle the complete Check_IPI as documented

	 * here. In the PAPR, this state will be used for both

	 * Set_MFRR and Down_CPPR. However, we know that we aren't

	 * changing the MFRR state here so we don't need to handle

	 * the case of an MFRR causing a reject of a pending irq,

	 * this will have been handled when the MFRR was set in the

	 * first place.

	 *

	 * Thus we don't have to handle rejects, only resends.

	 *

	 * When implementing real mode for HV KVM, resend will lead to

	 * a H_TOO_HARD return and the whole transaction will be handled

	 * in virtual mode.

 Down_CPPR */

		/*

		 * Cut down Resend / Check_IPI / IPI

		 *

		 * The logic is that we cannot have a pending interrupt

		 * trumped by an IPI at this point (see above), so we

		 * know that either the pending interrupt is already an

		 * IPI (in which case we don't care to override it) or

		 * it's either more favored than us or non existent

 Latch/clear resend bit */

	/*

	 * Now handle resend checks. Those are asynchronous to the ICP

	 * state update in HW (ie bus transactions) so we can handle them

	 * separately here as well.

 First clear the interrupt */

	/*

	 * ICP State: Accept_Interrupt

	 *

	 * Return the pending interrupt (if any) along with the

	 * current CPPR, then clear the XISR & set CPPR to the

	 * pending priority

 Return the result in GPR4 */

	/*

	 * ICP state: Set_MFRR

	 *

	 * If the CPPR is more favored than the new MFRR, then

	 * nothing needs to be done as there can be no XISR to

	 * reject.

	 *

	 * ICP state: Check_IPI

	 *

	 * If the CPPR is less favored, then we might be replacing

	 * an interrupt, and thus need to possibly reject it.

	 *

	 * ICP State: IPI

	 *

	 * Besides rejecting any pending interrupts, we also

	 * update XISR and pending_pri to mark IPI as pending.

	 *

	 * PAPR does not describe this state, but if the MFRR is being

	 * made less favored than its earlier value, there might be

	 * a previously-rejected interrupt needing to be resent.

	 * Ideally, we would want to resend only if

	 *	prio(pending_interrupt) < mfrr &&

	 *	prio(pending_interrupt) < cppr

	 * where pending interrupt is the one that was rejected. But

	 * we don't have that state, so we simply trigger a resend

	 * whenever the MFRR is made less favored.

 Set_MFRR */

 Check_IPI */

 Reject a pending interrupt if not an IPI */

 Handle reject in real mode */

 Handle resends in real mode */

	/*

	 * ICP State: Set_CPPR

	 *

	 * We can safely compare the new value with the current

	 * value outside of the transaction as the CPPR is only

	 * ever changed by the processor on itself

	/*

	 * ICP State: Up_CPPR

	 *

	 * The processor is raising its priority, this can result

	 * in a rejection of a pending interrupt:

	 *

	 * ICP State: Reject_Current

	 *

	 * We can remove EE from the current processor, the update

	 * transaction will set it again if needed

	/*

	 * Check for rejects. They are handled by doing a new delivery

	 * attempt (see comments in icp_rm_deliver_irq).

	/*

	 * ICS EOI handling: For LSI, if P bit is still set, we need to

	 * resend it.

	 *

	 * For MSI, we move Q bit into P (and clear Q). If it is set,

	 * resend it.

 Handle passthrough interrupts */

	/*

	 * ICP State: EOI

	 *

	 * Note: If EOI is incorrectly used by SW to lower the CPPR

	 * value (ie more favored), we do not check for rejection of

	 * a pending interrupt, this is a SW error and PAPR specifies

	 * that we don't have to deal with it.

	 *

	 * The sending of an EOI to the ICS is handled after the

	 * CPPR update

	 *

	 * ICP State: Down_CPPR which we handle

	 * in a separate function as it's shared with H_CPPR.

 IPIs have no EOI */

 EOI it */

/*

 * Increment a per-CPU 32-bit unsigned integer variable.

 * Safe to call in real-mode. Handles vmalloc'ed addresses

 *

 * ToDo: Make this work for any integral type

/*

 * We don't try to update the flags in the irq_desc 'istate' field in

 * here as would happen in the normal IRQ handling path for several reasons:

 *  - state flags represent internal IRQ state and are not expected to be

 *    updated outside the IRQ subsystem

 *  - more importantly, these are useful for edge triggered interrupts,

 *    IRQ probing, etc., but we are only handling MSI/MSIx interrupts here

 *    and these states shouldn't apply to us.

 *

 * However, we do update irq_stats - we somewhat duplicate the code in

 * kstat_incr_irqs_this_cpu() for this since this function is defined

 * in irq/internal.h which we don't want to include here.

 * The only difference is that desc->kstat_irqs is an allocated per CPU

 * variable and could have been vmalloc'ed, so we can't directly

 * call __this_cpu_inc() on it. The kstat structure is a static

 * per CPU variable and it should be accessible by real-mode KVM.

 *

 only MSIs register bypass producers, so it must be MSI here */

 Test P=1, Q=0, this is the only case where we present */

 EOI the interrupt */

  --- Non-real mode XICS-related built-in routines ---  */

/**

 * Host Operations poked by RM KVM

 Order these stores against the real mode KVM */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Malicious or buggy radix guests may have inserted SLB entries

 * (only 0..3 because radix always runs with UPRT=1), so these must

 * be cleared here to avoid side-channels. slbmte is used rather

 * than slbia, as it won't clear cached translations.

	/*

	 * All the isync()s are overkill but trivially follow the ISA

	 * requirements. Some can likely be replaced with justification

	 * comment for why they are not needed.

		/*

		 * Hash host could save and restore host SLB entries to

		 * reduce SLB fault overheads of VM exits, but for now the

		 * existing code clears all entries and restores just the

		 * bolted ones when switching back to host.

		/*

		 * This must run before switching to host (radix host can't

		 * access all SLBs).

	/*

	 * On POWER9 DD2.1 and below, sometimes on a Hypervisor Data Storage

	 * Interrupt (HDSI) the HDSISR is not be updated at all.

	 *

	 * To work around this we put a canary value into the HDSISR before

	 * returning to a guest and then check for this canary when we take a

	 * HDSI. If we find the canary on a HDSI, we know the hardware didn't

	 * update the HDSISR. In this case we return to the guest to retake the

	 * HDSI which should correctly update the HDSISR the second time HDSI

	 * entry.

	 *

	 * Just do this on all p9 processors for now.

	/*

	 * Hash host, hash guest, or radix guest with prefetch bug, all have

	 * to disable the MMU before switching to guest MMU state.

 clear RI */

 TLBIEL uses LPID=LPIDR, so run this after setting guest LPID */

	/*

	 * P9 suppresses the HDEC exception when LPCR[HDICE] = 0,

	 * so set guest LPCR (with HDICE) before writing HDEC.

 XXX: Could get these from r11/12 and paca exsave instead */

 0x2 bit for HSRR is only used by PR and P7/8 HV paths, clear it */

 HSRR interrupts leave MSR[RI] unchanged, SRR interrupts clear it. */

 trap == 0x200 */

	/*

	 * Only set RI after reading machine check regs (DAR, DSISR, SRR0/1)

	 * and hstate scratch (which we need to move into exsave to make

	 * re-entrant vs SRESET/MCE)

	/*

	 * Softpatch interrupt for transactional memory emulation cases

	 * on POWER9 DD2.2.  This is early in the guest exit path - we

	 * haven't saved registers or done a treclaim yet.

		/*

		 * The cases we want to handle here are those where the guest

		 * is in real suspend mode and is trying to transition to

		 * transactional mode.

				/*

				 * Go straight back into the guest with the

				 * new NIP/MSR as set by TM emulation.

				/*

				 * tm_return_to_guest re-loads SRR0/1, DAR,

				 * DSISR after RI is cleared, in case they had

				 * been clobbered by a MCE.

 clear RI */

 Advance host PURR/SPURR by the amount used by guest */

 Preserve PSSCR[FAKE_SUSPEND] until we've called kvmppc_save_tm_hv */

		/*

		 * Since this is radix, do a eieio; tlbsync; ptesync sequence

		 * in case we interrupted the guest between a tlbie and a

		 * ptesync.

	/*

	 * cp_abort is required if the processor supports local copy-paste

	 * to clear the copy buffer that was under control of the guest.

	/*

	 * If we are in real mode, only switch MMU on after the MMU is

	 * switched to host, to avoid the P9_RADIX_PREFETCH_BUG.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright 2012 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

 SRR1 bits for machine check on POWER7 */

 SLB parity error */

 SLB multi-hit */

 SLB parity + multi-hit */

 I-TLB multi-hit */

 DSISR bits for machine check on POWER7 */

 D-ERAT multi-hit */

 D-TLB multi-hit */

 SLB parity error */

 SLB multi-hit */

 SLB parity + multi-hit */

 POWER7 SLB flush and reload */

 First clear out SLB */

 Do they have an SLB shadow buffer registered? */

 Sanity check */

 Load up the SLB from that */

 insert entry number */

/*

 * On POWER7, see if we can handle a machine check that occurred inside

 * the guest in real mode, without switching to the host partition.

 error on load/store */

 flush and reload SLB; flushes D-ERAT too */

 Any other errors we don't understand? */

 FWNMI guests handle their own recovery */

	/*

	 * Now get the event and stash it in the vcpu struct so it can

	 * be handled by the primary thread in virtual mode.  We can't

	 * call machine_check_queue_event() here if we are running on

	 * an offline secondary thread.

 Check if dynamic split is in force and return subcore size accordingly. */

/*

 * kvmppc_realmode_hmi_handler() is called only by primary thread during

 * guest exit path.

 *

 * There are multiple reasons why HMI could occur, one of them is

 * Timebase (TB) error. If this HMI is due to TB error, then TB would

 * have been in stopped state. The opal hmi handler Will fix it and

 * restore the TB value with host timebase value. For HMI caused due

 * to non-TB errors, opal hmi handler will not touch/restore TB register

 * and hence there won't be any change in TB value.

 *

 * Since we are not sure about the cause of this HMI, we can't be sure

 * about the content of TB register whether it holds guest or host timebase

 * value. Hence the idea is to resync the TB on every HMI, so that we

 * know about the exact state of the TB value. Resync TB call will

 * restore TB to host timebase.

 *

 * Things to consider:

 * - On TB error, HMI interrupt is reported on all the threads of the core

 *   that has encountered TB error irrespective of split-core mode.

 * - The very first thread on the core that get chance to fix TB error

 *   would rsync the TB with local chipTOD value.

 * - The resync TB is a core level action i.e. it will sync all the TBs

 *   in that core independent of split-core mode. This means if we trigger

 *   TB sync from a thread from one subcore, it would affect TB values of

 *   sibling subcores of the same core.

 *

 * All threads need to co-ordinate before making opal hmi handler.

 * All threads will use sibling_subcore_state->in_guest[] (shared by all

 * threads in the core) in paca which holds information about whether

 * sibling subcores are in Guest mode or host mode. The in_guest[] array

 * is of size MAX_SUBCORE_PER_CORE=4, indexed using subcore id to set/unset

 * subcore status. Only primary threads from each subcore is responsible

 * to set/unset its designated array element while entering/exiting the

 * guset.

 *

 * After invoking opal hmi handler call, one of the thread (of entire core)

 * will need to resync the TB. Bit 63 from subcore state bitmap flags

 * (sibling_subcore_state->flags) will be used to co-ordinate between

 * primary threads to decide who takes up the responsibility.

 *

 * This is what we do:

 * - Primary thread from each subcore tries to set resync required bit[63]

 *   of paca->sibling_subcore_state->flags.

 * - The first primary thread that is able to set the flag takes the

 *   responsibility of TB resync. (Let us call it as thread leader)

 * - All other threads which are in host will call

 *   wait_for_subcore_guest_exit() and wait for in_guest[0-3] from

 *   paca->sibling_subcore_state to get cleared.

 * - All the primary thread will clear its subcore status from subcore

 *   state in_guest[] array respectively.

 * - Once all primary threads clear in_guest[0-3], all of them will invoke

 *   opal hmi handler.

 * - Now all threads will wait for TB resync to complete by invoking

 *   wait_for_tb_resync() except the thread leader.

 * - Thread leader will do a TB resync by invoking opal_resync_timebase()

 *   call and the it will clear the resync required bit.

 * - All other threads will now come out of resync wait loop and proceed

 *   with individual execution.

 * - On return of this function, primary thread will signal all

 *   secondary threads to proceed.

 * - All secondary threads will eventually call opal hmi handler on

 *   their exit path.

 *

 * Returns 1 if the timebase offset should be applied, 0 if not.

	/*

	 * By now primary thread has already completed guest->host

	 * partition switch but haven't signaled secondaries yet.

	 * All the secondary threads on this subcore is waiting

	 * for primary thread to signal them to go ahead.

	 *

	 * For threads from subcore which isn't in guest, they all will

	 * wait until all other subcores on this core exit the guest.

	 *

	 * Now set the resync required bit. If you are the first to

	 * set this bit then kvmppc_tb_resync_required() function will

	 * return true. For rest all other subcores

	 * kvmppc_tb_resync_required() will return false.

	 *

	 * If resync_req == true, then this thread is responsible to

	 * initiate TB resync after hmi handler has completed.

	 * All other threads on this core will wait until this thread

	 * clears the resync required bit flag.

 Reset the subcore status to indicate it has exited guest */

	/*

	 * Wait for other subcores on this core to exit the guest.

	 * All the primary threads and threads from subcore that are

	 * not in guest will wait here until all subcores are out

	 * of guest context.

	/*

	 * At this point we are sure that primary threads from each

	 * subcore on this core have completed guest->host partition

	 * switch. Now it is safe to call HMI handler.

	/*

	 * Check if this thread is responsible to resync TB.

	 * All other threads will wait until this thread completes the

	 * TB resync.

 Reset TB resync req bit */

	/*

	 * Reset tb_offset_applied so the guest exit code won't try

	 * to subtract the previous timebase offset from the timebase.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright 2010 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

 * Copyright 2011 David Gibson, IBM Corporation <dwg@au1.ibm.com>

 * Copyright 2016 Alexey Kardashevskiy, IBM Corporation <aik@au1.ibm.com>

 Make sure hardware table parameters are compatible */

			/*

			 * Reference the table to avoid races with

			 * add/remove DMA windows.

 stit is being destroyed */

		/*

		 * The table is already known to this KVM, we just increased

		 * its KVM reference counter and can return.

 Check this LIOBN hasn't been previously allocated */

 Allow userspace to poison TCE table */

/*

 * Handles TCE requests for emulated devices.

 * Puts guest TCE values to the table and expects user space to convert them.

 * Cannot fail so kvmppc_tce_validate must be called before it.

 We allow any TCE, not just with read|write permissions */

 it_userspace allocation might be delayed */

 This only handles v2 IOMMU type, v1 is handled via ioctl() */

 udbg_printf("H_PUT_TCE(): liobn=0x%lx ioba=0x%lx, tce=0x%lx\n", */

 	    liobn, ioba, tce); */

	/*

	 * SPAPR spec says that the maximum size of the list is 512 TCEs

	 * so the whole table fits in 4K page

		/*

		 * This looks unsafe, because we validate, then regrab

		 * the TCE from userspace which could have been changed by

		 * another thread.

		 *

		 * But it actually is safe, because the relevant checks will be

		 * re-executed in the following code.  If userspace tries to

		 * change this dodgily it will result in a messier failure mode

		 * but won't threaten the host.

 Check permission bits only to allow userspace poison TCE for debug */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright IBM Corp. 2007

 * Copyright 2011 Freescale Semiconductor, Inc.

 *

 * Authors: Hollis Blanchard <hollisb@us.ibm.com>

 CONFIG_PPC_FPU */

 CONFIG_VSX */

 CONFIG_ALTIVEC */

/*

 * XXX to do:

 * lfiwax, lfiwzx

 * vector loads and stores

 *

 * Instructions that trap when used on cache-inhibited mappings

 * are not emulated here: multiple and string instructions,

 * lq/stq, and the load-reserve/store-conditional instructions.

 this default type might be overwritten by subcategories */

 Hardware enforces alignment of VMX accesses */

 lvx */

 lvewx  */

 lvehx  */

 lvebx  */

 precision convert case: lxsspx, etc */

 lxvw4x, lxvd2x, etc */

			/* if need byte reverse, op.val has been reversed by

			 * analyse_instr().

			/* The FP registers need to be flushed so that

			 * kvmppc_handle_store() can read actual FP vals

			 * from vcpu->arch.

 Hardware enforces alignment of VMX accesses. */

 stvx */

 stvewx  */

 stvehx  */

 stvebx  */

 precise conversion case, like stxsspx */

 stxvw4x, stxvd2x, etc */

			/* Do nothing. The guest is performing dcbi because

			 * hardware DMA is not snooped by the dcache, but

			 * emulated DMA either goes through the dcache as

			 * normal writes, or the host kernel has handled dcache

			 * coherence.

 Advance past emulated instruction. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2010 SUSE Linux Products GmbH. All rights reserved.

 *

 * Authors:

 *     Alexander Graf <agraf@suse.de>

 Add to ePTE list */

 Add to ePTE_long list */

 Add to vPTE list */

 Add to vPTE_long list */

 Add to vPTE_64k list */

 Different for 32 and 64 bit */

 pte already invalidated in between? */

 Find the list of entries in the map */

 Check the list for matching entries and invalidate */

 Find the list of entries in the map */

 Check the list for matching entries and invalidate */

 Doing a complete flush -> start from scratch */

 Flush with mask 0xfffffffff */

 Check the list for matching entries and invalidate */

 Flush with mask 0xffffffff0 */

 Check the list for matching entries and invalidate */

 Flush with mask 0xffffff000 */

 Check the list for matching entries and invalidate */

 init hpte lookup hashes */

 init hpte slab cache */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2009 SUSE Linux Products GmbH. All rights reserved.

 *

 * Authors:

 *     Alexander Graf <agraf@suse.de>

 *     Kevin Wolf <mail@kevin-wolf.de>

/* We keep 512 gvsid->hvsid entries, mapping the guest ones to the array using

 used to check for invalidations in progress */

 Get host physical address for gpa */

 and write the mapping ea -> hpa into the pt */

	/*

	 * Use 64K pages if possible; otherwise, on 64K page kernels,

	 * we need to transfer 4 more bits from guest real to host real addr.

 In case we tried normal mapping already, let's nuke old entries */

 If we couldn't map a primary PTE, try a secondary */

		/*

		 * The mmu_hash_ops code may give us a secondary entry even

		 * though we asked for a primary. Fix up.

	/* We might get collisions that trap in preceding order, so let's

 Make sure we're taking the other map next time */

 Uh-oh ... out of mappings. Let's flush! */

 Are we overwriting? */

 Found a spare entry that was invalidated before */

 No spare invalid entry, so create one */

 Overflowing -> purge */

 Invalidate an entry */

 Set host segment base page size to 64K if possible */

 Invalidate this entry */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright SUSE Linux Products GmbH 2009

 *

 * Authors: Alexander Graf <agraf@suse.de>

 Opcode is officially reserved, reuse it as sc 1 when sc 1 doesn't trap */

 DCBZ is actually 1014, but we patch it to 1010 so we get a trap */

 PAPR VMs only access supervisor SPRs */

 Limit user space to its own small SPR set */

 CR0 = 0 | MSR[TS] | 0 */

 failure recording depends on Failure Summary bit */

	/*

	 * treclaim need quit to non-transactional state.

	/*

	 * need flush FP/VEC/VSX to vcpu save area before

	 * copy.

	/*

	 * as a result of trecheckpoint. set TS to suspended.

 emulate tabort. at guest privilege state */

	/* currently we only emulate tabort. but no emulation of other

	 * tabort variants since there is no kernel usage of them at

	 * present.

 CR0 = 0 | MSR[TS] | 0 */

	/* failure recording depends on Failure Summary bit,

	 * and tabort will be treated as nops in non-transactional

	 * state.

			/*

			 * This is the byte reversed syscall instruction of our

			 * hypercall handler. Early versions of LE Linux didn't

			 * swap the instructions correctly and ended up in

			 * illegal instructions.

			 * Just always fail hypercalls on these broken systems.

			/*

			 * add rules to fit in ISA specification regarding TM

			 * state transistion in TM disable/Suspended state,

			 * and target TM state is TM inactive(00) state. (the

			 * change should be suppressed).

 SC 1 papr hypercalls */

 copy XER[SO] bit to CR0[SO] */

 Gets treated as NOP */

			/* only emulate for privilege guest, since problem state

			 * guest can run with TM enabled and we don't expect to

			 * trap at here for that case.

 generate interrupts based on priorities */

 Privileged Instruction type Program Interrupt */

 TM bad thing interrupt */

 generate interrupt based on priorities */

 Privileged Instruction type Program Intr */

 TM bad thing interrupt */

 Upper BAT */

 Lower BAT */

		/* BAT writes happen so rarely that we're ok to flush

 HID2.PSE controls paired single on gekko */

 lonestar 2.0 */

 lonestar 2.2 */

 gekko 1.0 */

 gekko 2.0 */

 gekko 2.3a */

 gekko 2.3b */

 gekko 2.4 */

 gekko 2.4e (8SE) - retail HW2 */

 broadway */

 Native paired singles */

 HID2.PSE */

 guest HID5 set can change is_dcbz32 */

			/* it is illegal to mtspr() TM regs in

			 * other than non-transactional state, with

			 * the exception of TFHAR in suspend state.

		/*

		 * On exit we would have updated purr

		/*

		 * On exit we would have updated spurr

	/*

	 * Linux's fix_alignment() assumes that DAR is valid, so can we

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright Novell Inc 2010

 *

 * Authors: Alexander Graf <agraf@suse.de>

 #define DEBUG */

 XXX

 Page Fault */

 read from memory */

 put in registers */

 read from memory */

 put in registers */

/*

 * Cuts out inst bits with ordering according to spec.

 * That means the leftmost bit is zero. All given bits are included.

 X form */

 XW form */

 A form */

 RC */

 PS0 */

 PS1 */

 RC */

 PS0 */

 PS1 */

 RC */

 PS0 */

 PS1 */

 Do we need to clear FE0 / FE1 here? Don't think so. */

 X form */

 XXX */

 XXX */

 XXX */

 XXX */

 vcpu->arch.qpr[ax_rd] = VCPU_FPR(vcpu, ax_rb); */

 VCPU_FPR(vcpu, ax_rd) = vcpu->arch.qpr[ax_ra]; */

 vcpu->arch.qpr[ax_rd] = VCPU_FPR(vcpu, ax_rb); */

 VCPU_FPR(vcpu, ax_rd) = vcpu->arch.qpr[ax_ra]; */

 XW form */

 A form */

 Real FPU operations */

 XXX need to implement */

 XXX missing CR */

 XXX missing fm bits */

 XXX missing CR */

 fD = sqrt(fB) */

 fD = 1.0f / fD */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2010,2012 Freescale Semiconductor, Inc. All rights reserved.

 *

 * Author: Varun Sethi, <varun.sethi@freescale.com>

 *

 * Description:

 * This file is derived from arch/powerpc/kvm/e500.c,

 * by Yu Liu <yu.liu@freescale.com>.

 gtlbe must not be mapped by more than one host tlb entry */

 We search the host TLB to invalidate its shadow TLB entry */

 NOTE: tlbsx also updates mas8, so clear it for host tlbwe */

 We use two lpids per VM */

	/*

	 * Since guests have the privilege to enable AltiVec, we need AltiVec

	 * support in the host to save/restore their context.

	 * Don't use CPU_FTR_ALTIVEC to identify cores with AltiVec unit

	 * because it's cleared in the absence of CONFIG_ALTIVEC!

 Invalid PIR value -- this LPID dosn't have valid state on any cpu */

	/*

	 * Use two lpids per VM on cores with two threads like e6500. Use

	 * even numbers to speedup vcpu lpid computation with consecutive lpids

	 * per VM. vm1 will use lpids 2 and 3, vm2 lpids 4 and 5, and so on.

	/*

	 * Use two lpids per VM on dual threaded processors like e6500

	 * to workarround the lack of tlb write conditional instruction.

	 * Expose half the number of available hardware lpids to the lpid

	 * allocator.

 host */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright IBM Corp. 2007

 *

 * Authors: Hollis Blanchard <hollisb@us.ibm.com>

 *          Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>

/*

 * Common checks before entering the guest world.  Call with interrupts

 * disabled.

 *

 * returns:

 *

 * == 1 if we're ready to go into guest state

 * <= 0 if we need to go back to the host with return value

		/*

		 * Reading vcpu->requests must happen after setting vcpu->mode,

		 * so we don't miss a request because the requester sees

		 * OUTSIDE_GUEST_MODE and assumes we'll be checking requests

		 * before next entering the guest (and thus doesn't IPI).

		 * This also orders the write to mode from any reads

		 * to the page tables done while the VCPU is running.

		 * Please see the comment in kvm_flush_remote_tlbs.

 Make sure we process requests preemptable */

			/* interrupts got enabled in between, so we

 return to host */

 32 bit mode */

 Book3S can be little endian, find it out here */

			/*

			 * Older versions of the Linux magic page code had

			 * a bug where they would map their trampoline code

			 * NX. If that's the case, remove !PR NX capability.

		/*

		 * Make sure our 4k magic page is in the same window of a 64k

		 * page within the guest and within the host's page.

 Second return value is in r4 */

 We have to know what CPU to virtualize */

 PAPR only works with book3s_64 */

 HV KVM can only do PAPR mode for now */

		/* Future optimization: only reload non-volatiles if they were

		/* We must reload nonvolatiles because "update" load/store

		/* Future optimization: only reload non-volatiles if they were

 XXX Deliver Program interrupt to guest. */

 Magic page override */

 Magic page override */

	/*

	 * if we have both HV and PR enabled, default is HV

	/*

	 * We call kick_all_cpus_sync() to ensure that all

	 * CPUs have executed any pending IPIs before we

	 * continue and free VCPUs structures below.

 drop the module reference */

 Assume we're using HV mode when the HV module is loaded */

		/*

		 * Hooray - we know which VM type we're running on. Depend on

		 * that rather than the guess above.

 We support this only for PR */

		/*

		 * We need XIVE to be enabled on the platform (implies

		 * a POWER9 processor) and the PowerNV platform, as

		 * nested is not yet supported.

 CONFIG_PPC_BOOK3S_64 */

 P9 can emulate dbells, so allow any mode */

		/*

		 * Recommending a number of CPUs is somewhat arbitrary; we

		 * return the number of present CPUs for -HV (since a host

		 * will have secondary threads "offline"), and for other KVM

		 * implementations just count online CPUs.

 Make sure we're not using the vcpu anymore */

	/*

	 * vrsave (formerly usprg0) isn't used by Linux, but may

	 * be used by the guest.

	 *

	 * On non-booke this is associated with Altivec and

	 * is handled by code in book3s.c.

/*

 * irq_bypass_add_producer and irq_bypass_del_producer are only

 * useful if the architecture supports PCI passthrough.

 * irq_bypass_stop and irq_bypass_start are not needed and so

 * kvm_ops are not defined for them.

 CONFIG_VSX */

 CONFIG_ALTIVEC */

 CONFIG_PPC_FPU */

 conversion between single and double precision */

 Pity C doesn't have a logical XOR operator */

 Same as above, but sign extends */

 Currently, mmio_vsx_copy_nums only allowed to be 4 or less */

 CONFIG_VSX */

 Pity C doesn't have a logical XOR operator */

 Store the value at the lowest bytes in 'data'. */

 Currently, mmio_vsx_copy_nums only allowed to be 4 or less */

 CONFIG_VSX */

 CONFIG_ALTIVEC */

 CONFIG_ALTIVEC */

 CONFIG_ALTIVEC */

 CONFIG_KVM_XICS */

 CONFIG_KVM_XIVE */

 CONFIG_KVM_BOOK3S_HV_POSSIBLE */

	/*

	 * The hypercall to get into KVM from within guest context is as

	 * follows:

	 *

	 *    lis r0, r0, KVM_SC_MAGIC_R0@h

	 *    ori r0, KVM_SC_MAGIC_R0@l

	 *    sc

	 *    nop

/*

 * These functions check whether the underlying hardware is safe

 * against attacks based on observing the effects of speculatively

 * executed instructions, and whether it supplies instructions for

 * use in workarounds.  The information comes from firmware, either

 * via the device tree on powernv platforms or from an hcall on

 * pseries platforms.

 CONFIG_PPC_BOOK3S_64 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Hypervisor Maintenance Interrupt (HMI) handling.

 *

 * Copyright 2015 IBM Corporation

 * Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>

	/*

	 * NULL bitmap pointer indicates that KVM module hasn't

	 * been loaded yet and hence no guests are running.

	 * If no KVM is in use, no need to co-ordinate among threads

	 * as all of them will always be in host and no one is going

	 * to modify TB other than the opal hmi handler.

	 * Hence, just return from here.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2017 Benjamin Herrenschmidt, IBM Corporation

 File to be included by other .c files */

 Dummy interrupt used when taking interrupts out of a queue in H_CPPR */

	/*

	 * Ensure any previous store to CPPR is ordered vs.

	 * the subsequent loads from PIPR or ACK.

 Perform the acknowledge OS to register cycle. */

 Synchronize subsequent queue accesses */

 XXX Check grouping level */

 Anything ? */

 Grab CPPR of the most favored pending interrupt */

 Check consistency */

	/*

	 * Update our image of the HW CPPR. We don't yet modify

	 * xc->cppr, this will be done as we scan for interrupts

	 * in the queues.

 If the XIVE supports the new "store EOI facility, use it */

		/*

		 * For LSIs the HW EOI cycle is used rather than PQ bits,

		 * as they are automatically re-triggred in HW when still

		 * pending.

		/*

		 * Otherwise for EOI, we use the special MMIO that does

		 * a clear of both P and Q and returns the old Q,

		 * except for LSIs where we use the "EOI cycle" special

		 * load.

		 *

		 * This allows us to then do a re-trigger if Q was set

		 * rather than synthetizing an interrupt in software

 Re-trigger if needed */

 Find highest pending priority */

		/*

		 * If pending is 0 this will return 0xff which is what

		 * we want

 Don't scan past the guest cppr */

 Grab queue and pointers */

		/*

		 * Snapshot the queue page. The test further down for EOI

		 * must use the same "copy" that was used by __xive_read_eq

		 * since qpage can be set concurrently and we don't want

		 * to miss an EOI.

		/*

		 * Try to fetch from the queue. Will return 0 for a

		 * non-queueing priority (ie, qpage = 0).

		/*

		 * If this was a signal for an MFFR change done by

		 * H_IPI we skip it. Additionally, if we were fetching

		 * we EOI it now, thus re-enabling reception of a new

		 * such signal.

		 *

		 * We also need to do that if prio is 0 and we had no

		 * page for the queue. In this case, we have non-queued

		 * IPI that needs to be EOId.

		 *

		 * This is safe because if we have another pending MFRR

		 * change that wasn't observed above, the Q bit will have

		 * been set and another occurrence of the IPI will trigger.

 Loop back on same queue with updated idx/toggle */

 If it's the dummy interrupt, continue searching */

 Clear the pending bit if the queue is now empty */

			/*

			 * Check if the queue count needs adjusting due to

			 * interrupts being moved away.

		/*

		 * If the most favoured prio we found pending is less

		 * favored (or equal) than a pending IPI, we return

		 * the IPI instead.

 If fetching, update queue pointers */

 If we are just taking a "peek", do nothing else */

 Update the pending bits */

	/*

	 * If this is an EOI that's it, no CPPR adjustment done here,

	 * all we needed was cleanup the stale pending bits and check

	 * if there's anything left.

	/*

	 * If we found an interrupt, adjust what the guest CPPR should

	 * be as if we had just fetched that interrupt from HW.

	 *

	 * Note: This can only make xc->cppr smaller as the previous

	 * loop will only exit with hirq != 0 if prio is lower than

	 * the current xc->cppr. Thus we don't need to re-check xc->mfrr

	 * for pending IPIs.

	/*

	 * If it was an IPI the HW CPPR might have been lowered too much

	 * as the HW interrupt we use for IPIs is routed to priority 0.

	 *

	 * We re-sync it here.

 First collect pending bits from HW */

 Grab previous CPPR and reverse map it */

 Scan for actual interrupts */

 That should never hit */

	/*

	 * XXX We could check if the interrupt is masked here and

	 * filter it. If we chose to do so, we would need to do:

	 *

	 *    if (masked) {

	 *        lock();

	 *        if (masked) {

	 *            old_Q = true;

	 *            hirq = 0;

	 *        }

	 *        unlock();

	 *    }

 Return interrupt and old CPPR in GPR4 */

 Grab the target VCPU if not the current one */

 Scan all priorities */

 Grab pending interrupt if any */

 Return interrupt and old CPPR in GPR4 */

 For each priority that is now masked */

 For each interrupt in the queue */

 No more ? */

 Skip dummies and IPIs */

 Has it been rerouted ? */

			/*

			 * Allright, it *has* been re-routed, kill it from

			 * the queue.

 Find the HW interrupt */

 If it's not an LSI, set PQ to 11 the EOI will force a resend */

 EOI the source */

 Map CPPR */

 Remember old and update SW state */

	/*

	 * Order the above update of xc->cppr with the subsequent

	 * read of xc->mfrr inside push_pending_to_hw()

		/*

		 * We are masking less, we need to look for pending things

		 * to deliver and set VP pending bits accordingly to trigger

		 * a new interrupt otherwise we might miss MFRR changes for

		 * which we have optimized out sending an IPI signal.

		/*

		 * We are masking more, we need to check the queue for any

		 * interrupt that has been routed to another CPU, take

		 * it out (replace it with the dummy) and retrigger it.

		 *

		 * This is necessary since those interrupts may otherwise

		 * never be processed, at least not until this CPU restores

		 * its CPPR.

		 *

		 * This is in theory racy vs. HW adding new interrupts to

		 * the queue. In practice this works because the interesting

		 * cases are when the guest has done a set_xive() to move the

		 * interrupt away, which flushes the xive, followed by the

		 * target CPU doing a H_CPPR. So any new interrupt coming into

		 * the queue must still be routed to us and isn't a source

		 * of concern.

 Apply new CPPR */

	/*

	 * IPIs are synthetized from MFRR and thus don't need

	 * any special EOI handling. The underlying interrupt

	 * used to signal MFRR changes is EOId when fetched from

	 * the queue.

		/*

		 * This barrier orders the setting of xc->cppr vs.

		 * subsquent test of xc->mfrr done inside

		 * scan_interrupts and push_pending_to_hw

 Find interrupt source */

 Same as above */

	/*

	 * This barrier orders both setting of in_eoi above vs,

	 * subsequent test of guest_priority, and the setting

	 * of xc->cppr vs. subsquent test of xc->mfrr done inside

	 * scan_interrupts and push_pending_to_hw

 Clear old_p, that will cause unmask to perform an EOI */

 Perform EOI on the source */

 If it's an emulated LSI, check level and resend */

	/*

	 * This barrier orders the above guest_priority check

	 * and spin_lock/unlock with clearing in_eoi below.

	 *

	 * It also has to be a full mb() as it must ensure

	 * the MMIOs done in source_eoi() are completed before

	 * state->in_eoi is visible.

 Re-evaluate pending IRQs and update HW */

 Apply new CPPR */

 Find target */

 Locklessly write over MFRR */

	/*

	 * The load of xc->cppr below and the subsequent MMIO store

	 * to the IPI must happen after the above mfrr update is

	 * globally visible so that:

	 *

	 * - Synchronize with another CPU doing an H_EOI or a H_CPPR

	 *   updating xc->cppr then reading xc->mfrr.

	 *

	 * - The target of the IPI sees the xc->mfrr update

 Shoot the IPI if most favored than target cppr */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2017 Benjamin Herrenschmidt, IBM Corporation.

/*

 * Virtual mode variants of the hcalls for use on radix/radix

 * with AIL. They require the VCPU's VP to be "pushed"

 *

 * We still instantiate them here because we use some of the

 * generated utility functions as well in this file.

/*

 * We leave a gap of a couple of interrupts in the queue to

 * account for the IPI and additional safety guard.

 Check enablement at VP level */

/*

 * Push a vcpu's context to the XIVE on guest entry.

 * This assumes we are in virtual mode (MMU on)

	/*

	 * Nothing to do if the platform doesn't have a XIVE

	 * or this vCPU doesn't have its own XIVE context

	 * (e.g. because it's not using an in-kernel interrupt controller).

	/*

	 * We clear the irq_pending flag. There is a small chance of a

	 * race vs. the escalation interrupt happening on another

	 * processor setting it again, but the only consequence is to

	 * cause a spurious wakeup on the next H_CEDE, which is not an

	 * issue.

	/*

	 * In single escalation mode, if the escalation interrupt is

	 * on, we mask it.

		/*

		 * We have a possible subtle race here: The escalation

		 * interrupt might have fired and be on its way to the

		 * host queue while we mask it, and if we unmask it

		 * early enough (re-cede right away), there is a

		 * theorical possibility that it fires again, thus

		 * landing in the target queue more than once which is

		 * a big no-no.

		 *

		 * Fortunately, solving this is rather easy. If the

		 * above load setting PQ to 01 returns a previous

		 * value where P is set, then we know the escalation

		 * interrupt is somewhere on its way to the host. In

		 * that case we simply don't clear the xive_esc_on

		 * flag below. It will be eventually cleared by the

		 * handler for the escalation interrupt.

		 *

		 * Then, when doing a cede, we check that flag again

		 * before re-enabling the escalation interrupt, and if

		 * set, we abort the cede.

 Now P is 0, we can clear the flag */

/*

 * Pull a vcpu's context from the XIVE on guest exit.

 * This assumes we are in virtual mode (MMU on)

	/*

	 * Should not have been pushed if there is no tima

 First load to pull the context, we ignore the value */

 Second load to recover the context state (Words 0 and 1) */

 Fixup some of the state for the next load */

 we are using XIVE with single escalation */

		/*

		 * If we still have a pending escalation, abort the cede,

		 * and we must set PQ to 10 rather than 00 so that we don't

		 * potentially end up with two entries for the escalation

		 * interrupt in the XIVE interrupt queue.  In that case

		 * we also don't want to set xive_esc_on to 1 here in

		 * case we race with xive_esc_irq().

		/*

		 * The escalation interrupts are special as we don't EOI them.

		 * There is no need to use the load-after-store ordering offset

		 * to set PQ to 10 as we won't use StoreEOI.

/*

 * This is a simple trigger for a generic XIVE IRQ. This must

 * only be called for interrupts that support a trigger page

 This should be only for MSIs */

 Those interrupts should always have a trigger page */

	/* Since we have the no-EOI flag, the interrupt is effectively

	 * disabled now. Clearing xive_esc_on means we won't bother

	 * doing so on the next entry.

	 *

	 * This also allows the entry code to know that if a PQ combination

	 * of 10 is observed while xive_esc_on is true, it means the queue

	 * contains an unprocessed escalation interrupt. We don't make use of

	 * that knowledge today but might (see comment in book3s_hv_rmhandler.S)

 This orders xive_esc_on = false vs. subsequent stale_p = true */

 goes with smp_mb() in cleanup_single_escalation */

 Already there ? */

 Hook up the escalation interrupt */

	/* In single escalation mode, we grab the ESB MMIO of the

	 * interrupt and mask it. Also populate the VCPU v/raddr

	 * of the ESB page for use by asm entry/exit code. Finally

	 * set the XIVE_IRQ_FLAG_NO_EOI flag which will prevent the

	 * core code from performing an EOI on the escalation

	 * interrupt, thus leaving it effectively masked after

	 * it fires once.

 Allocate the queue and retrieve infos on current node for now */

	/*

	 * Reconfigure the queue. This will set q->qpage only once the

	 * queue is fully configured. This is a requirement for prio 0

	 * as we will stop doing EOIs for every IPI as soon as we observe

	 * qpage being non-NULL, and instead will only EOI when we receive

	 * corresponding queue 0 entries

 Called with xive->lock held */

 Already provisioned ? */

 Provision each VCPU and enable escalations if needed */

 Order previous stores and mark it as provisioned */

 Locate target server */

 Calculate max number of interrupts in that queue. */

 Locate target server */

 Try pick it */

 Failed, pick another VCPU */

 No available target ! */

	/*

	 * Take the lock, set masked, try again if racing

	 * with H_EOI

 No change ? Bail */

 Get the right irq */

 Set PQ to 10, return old P and old Q and remember them */

	/*

	 * Synchronize hardware to sensure the queues are updated when

	 * masking

	/*

	 * Take the lock try again if racing with H_EOI

 If we aren't changing a thing, move on */

 Get the right irq */

 Old Q set, set PQ to 11 */

	/*

	 * If not old P, then perform an "effective" EOI,

	 * on the source. This will handle the cases where

	 * FW EOI is needed.

 Synchronize ordering and mark unmasked */

/*

 * Target an interrupt to a given server/prio, this will fallback

 * to another server if necessary and perform the HW targetting

 * updates as needed

 *

 * NOTE: Must be called with the state lock held

	/*

	 * This will return a tentative server and actual

	 * priority. The count for that new target will have

	 * already been incremented.

	/*

	 * We failed to find a target ? Not much we can do

	 * at least until we support the GIQ.

	/*

	 * Increment the old queue pending count if there

	 * was one so that the old queue count gets adjusted later

	 * when observed to be empty.

	/*

	 * Update state and HW

 Get the right irq */

/*

 * Targetting rules: In order to avoid losing track of

 * pending interrupts accross mask and unmask, which would

 * allow queue overflows, we implement the following rules:

 *

 *  - Unless it was never enabled (or we run out of capacity)

 *    an interrupt is always targetted at a valid server/queue

 *    pair even when "masked" by the guest. This pair tends to

 *    be the last one used but it can be changed under some

 *    circumstances. That allows us to separate targetting

 *    from masking, we only handle accounting during (re)targetting,

 *    this also allows us to let an interrupt drain into its target

 *    queue after masking, avoiding complex schemes to remove

 *    interrupts out of remote processor queues.

 *

 *  - When masking, we set PQ to 10 and save the previous value

 *    of P and Q.

 *

 *  - When unmasking, if saved Q was set, we set PQ to 11

 *    otherwise we leave PQ to the HW state which will be either

 *    10 if nothing happened or 11 if the interrupt fired while

 *    masked. Effectively we are OR'ing the previous Q into the

 *    HW Q.

 *

 *    Then if saved P is clear, we do an effective EOI (Q->P->Trigger)

 *    which will unmask the interrupt and shoot a new one if Q was

 *    set.

 *

 *    Otherwise (saved P is set) we leave PQ unchanged (so 10 or 11,

 *    effectively meaning an H_EOI from the guest is still expected

 *    for that interrupt).

 *

 *  - If H_EOI occurs while masked, we clear the saved P.

 *

 *  - When changing target, we account on the new target and

 *    increment a separate "pending" counter on the old one.

 *    This pending counter will be used to decrement the old

 *    target's count when its queue has been observed empty.

 First, check provisioning of queues */

	/*

	 * We first handle masking/unmasking since the locking

	 * might need to be retried due to EOIs, we'll handle

	 * targetting changes later. These functions will return

	 * with the SB lock held.

	 *

	 * xive_lock_and_mask() will also set state->guest_priority

	 * but won't otherwise change other fields of the state.

	 *

	 * xive_lock_for_unmask will not actually unmask, this will

	 * be done later by xive_finish_unmask() once the targetting

	 * has been done, so we don't try to unmask an interrupt

	 * that hasn't yet been targetted.

	/*

	 * Then we handle targetting.

	 *

	 * First calculate a new "actual priority"

	/*

	 * Then check if we actually need to change anything,

	 *

	 * The condition for re-targetting the interrupt is that

	 * we have a valid new priority (new_act_prio is not 0xff)

	 * and either the server or the priority changed.

	 *

	 * Note: If act_priority was ff and the new priority is

	 *       also ff, we don't do anything and leave the interrupt

	 *       untargetted. An attempt of doing an int_on on an

	 *       untargetted interrupt will fail. If that is a problem

	 *       we could initialize interrupts with valid default

	/*

	 * Perform the final unmasking of the interrupt source

	 * if necessary

	/*

	 * Finally Update saved_priority to match. Only int_on/off

	 * set this field to a different value.

	/*

	 * Check if interrupt was not targetted

 If saved_priority is 0xff, do nothing */

	/*

	 * Lock and unmask it.

	/*

	 * Lock and mask

	/*

	 * Trigger the IPI. This assumes we never restore a pass-through

	 * interrupt which should be safe enough

 Return the per-cpu state for state saving/migration */

 Grab individual state fields. We don't use pending_pri */

	/*

	 * We can't update the state of a "pushed" VCPU, but that

	 * shouldn't happen because the vcpu->mutex makes running a

	 * vcpu mutually exclusive with doing one_reg get/set on it.

 Update VCPU HW saved state */

	/*

	 * Update MFRR state. If it's not 0xff, we mark the VCPU as

	 * having a pending MFRR change, which will re-evaluate the

	 * target. The VCPU will thus potentially get a spurious

	 * interrupt but that's not a big deal.

	/*

	 * Now saved XIRR is "interesting". It means there's something in

	 * the legacy "1 element" queue... for an IPI we simply ignore it,

	 * as the MFRR restore will handle that. For anything else we need

	 * to force a resend of the source.

	 * However the source may not have been setup yet. If that's the

	 * case, we keep that info and increment a counter in the xive to

	 * tell subsequent xive_set_source() to go look.

	/*

	 * Mark the passed-through interrupt as going to a VCPU,

	 * this will prevent further EOIs and similar operations

	 * from the XIVE code. It will also mask the interrupt

	 * to either PQ=10 or 11 state, the latter if the interrupt

	 * is pending. This will allow us to unmask or retrigger it

	 * after routing it to the guest with a simple EOI.

	 *

	 * The "state" argument is a "token", all it needs is to be

	 * non-NULL to switch to passed-through or NULL for the

	 * other way around. We may not yet have an actual VCPU

	 * target here and we don't really care.

	/*

	 * Mask and read state of IPI. We need to know if its P bit

	 * is set as that means it's potentially already using a

	 * queue entry in the target

 Turn the IPI hard off */

	/*

	 * Reset ESB guest mapping. Needed when ESB pages are exposed

	 * to the guest in XIVE native mode

 Grab info about irq */

	/*

	 * Configure the IRQ to match the existing configuration of

	 * the IPI if it was already targetted. Otherwise this will

	 * mask the interrupt in a lossy way (act_priority is 0xff)

	 * which is fine for a never started interrupt.

	/*

	 * We do an EOI to enable the interrupt (and retrigger if needed)

	 * if the guest has the interrupt unmasked and the P bit was *not*

	 * set in the IPI. If it was set, we know a slot may still be in

	 * use in the target queue thus we have to wait for a guest

	 * originated EOI

 Clear old_p/old_q as they are no longer relevant */

 Restore guest prio (unlocks EOI) */

	/*

	 * Mask and read state of IRQ. We need to know if its P bit

	 * is set as that means it's potentially already using a

	 * queue entry in the target

	/*

	 * If old_p is set, the interrupt is pending, we switch it to

	 * PQ=11. This will force a resend in the host so the interrupt

	 * isn't lost to whatver host driver may pick it up

 Release the passed-through interrupt to the host */

 Forget about the IRQ */

	/*

	 * Reset ESB guest mapping. Needed when ESB pages are exposed

	 * to the guest in XIVE native mode

 Reconfigure the IPI */

	/*

	 * If old_p is set (we have a queue entry potentially

	 * occupied) or the interrupt is masked, we set the IPI

	 * to PQ=10 state. Otherwise we just re-enable it (PQ=00).

 Restore guest prio (unlocks EOI) */

 Clean it up */

 Disable vcpu's escalation interrupt */

	/*

	 * Clear pointers to escalation interrupt ESB.

	 * This is safe because the vcpu->mutex is held, preventing

	 * any other CPU from concurrently executing a KVM_RUN ioctl.

/*

 * In single escalation mode, the escalation interrupt is marked so

 * that EOI doesn't re-enable it, but just sets the stale_p flag to

 * indicate that the P bit has already been dealt with.  However, the

 * assembly code that enters the guest sets PQ to 00 without clearing

 * stale_p (because it has no easy way to address it).  Hence we have

 * to adjust stale_p before shutting down the interrupt.

	/*

	 * This slightly odd sequence gives the right result

	 * (i.e. stale_p set if xive_esc_on is false) even if

	 * we race with xive_esc_irq() and xive_irq_eoi().

 paired with smb_wmb in xive_esc_irq */

 Ensure no interrupt is still routed to that VP */

 Mask the VP IPI */

 Free escalations */

 Disable the VP */

 Clear the cam word so guest entry won't try to push context */

 Free the queues */

 Free the IPI */

 Free the VP */

 Cleanup the vcpu */

	/* We have a block of xive->nr_servers VPs. We just need to check

	 * packed vCPU ids are below that.

 We need to synchronize with queue provisioning */

 Configure VCPU fields for use by assembly push/pull */

 Allocate IPI */

	/*

	 * Enable the VP first as the single escalation mode will

	 * affect escalation interrupts numbering

	/*

	 * Initialize queues. Initially we set them all for no queueing

	 * and we enable escalation for queue 0 only which we'll use for

	 * our mfrr change notifications. If the VCPU is hot-plugged, we

	 * do handle provisioning however based on the existing "map"

	 * of enabled queues.

 Single escalation, no queue 7 */

 Is queue already enabled ? Provision it */

 If not done above, attach priority 0 escalation */

 Route the IPI */

/*

 * Scanning of queues before/after migration save

 Some sanity checking */

	/*

	 * If the interrupt is in a queue it should have P set.

	 * We warn so that gets reported. A backtrace isn't useful

	 * so no need to use a WARN_ON.

 Set flag */

 Mask and save state, this will also sync HW queues */

 Transfer P and Q */

 Unlock */

	/*

	 * Lock / exclude EOI (not technically necessary if the

	 * guest isn't running concurrently. If this becomes a

	 * performance issue we can probably remove the lock.

 Restore mask/prio if it wasn't masked */

 Unlock */

	/*

	 * See comment in xive_get_source() about how this

	 * work. Collect a stable state for all interrupts

 Then scan the queues and update the "in_queue" flag */

 Finally restore interrupt states */

 Clear all the in_queue flags */

 Next get_source() will do a new scan */

/*

 * This returns the source configuration and state to user space.

	/*

	 * So to properly save the state into something that looks like a

	 * XICS migration stream we cannot treat interrupts individually.

	 *

	 * We need, instead, mask them all (& save their previous PQ state)

	 * to get a stable state in the HW, then sync them to ensure that

	 * any interrupt that had already fired hits its queue, and finally

	 * scan all the queues to collect which interrupts are still present

	 * in the queues, so we can set the "pending" flag on them and

	 * they can be resent on restore.

	 *

	 * So we do it all when the "first" interrupt gets saved, all the

	 * state is collected at that point, the rest of xive_get_source()

	 * will merely collect and convert that state to the expected

	 * userspace bit mask.

 Convert saved state into something compatible with xics */

		/*

		 * We mark it pending (which will attempt a re-delivery)

		 * if we are in a queue *or* we were masked and had

		 * Q set which is equivalent to the XICS "masked pending"

		 * state

	/*

	 * If that was the last interrupt saved, reset the

	 * in_queue flags

 Copy the result to userspace */

 block already exists - somebody else got here first */

 Create the ICS */

 Find the source */

 Read user passed data */

	/*

	 * If the source doesn't already have an IPI, allocate

	 * one and get the corresponding data

	/*

	 * We use lock_and_mask() to set us in the right masked

	 * state. We will override that state from the saved state

	 * further down, but this will handle the cases of interrupts

	 * that need FW masking. We set the initial guest_priority to

	 * 0 before calling it to ensure it actually performs the masking.

	/*

	 * Now, we select a target if we have one. If we don't we

	 * leave the interrupt untargetted. It means that an interrupt

	 * can become "untargetted" accross migration if it was masked

	 * by set_xive() but there is little we can do about it.

 First convert prio and mark interrupt as untargetted */

	/*

	 * We need to drop the lock due to the mutex below. Hopefully

	 * nothing is touching that interrupt yet since it hasn't been

	 * advertized to a running guest yet

 If we have a priority target the interrupt */

 First, check provisioning of queues */

 Target interrupt */

		/*

		 * If provisioning or targetting failed, leave it

		 * alone and masked. It will remain disabled until

		 * the guest re-targets it.

	/*

	 * Find out if this was a delayed irq stashed in an ICP,

	 * in which case, treat it as pending

 Cleanup the SW state */

 Restore LSI state */

	/*

	 * Restore P and Q. If the interrupt was pending, we

	 * force Q and !P, which will trigger a resend.

	 *

	 * That means that a guest that had both an interrupt

	 * pending (queued) and Q set will restore with only

	 * one instance of that interrupt instead of 2, but that

	 * is perfectly fine as coalescing interrupts that haven't

	 * been presented yet is always allowed.

	/*

	 * If the interrupt was unmasked, update guest priority and

	 * perform the appropriate state transition and do a

	 * re-trigger if necessary.

 Increment the number of valid sources and mark this one valid */

 Perform locklessly .... (we need to do some RCUisms here...) */

 We don't allow a trigger on a passed-through interrupt */

 Trigger the IPI */

		/* The VP block is allocated once and freed when the device

		 * is released. Better not allow to change its size since its

		 * used by connect_vcpu to validate vCPU ids are valid (eg,

		 * setting it back to a higher value could allow connect_vcpu

		 * to come up with a VP id that goes beyond the VP block, which

		 * is likely to cause a crash in OPAL).

		/* We don't need more servers. Higher vCPU ids get packed

		 * down below KVM_MAX_VCPUS by kvmppc_pack_vcpu_id().

 We honor the existing XICS ioctl */

 We honor the existing XICS ioctl */

 We honor the same limits as XICS, at least for now */

 Pass-through, cleanup too but keep IRQ hw data */

/*

 * Called when device fd is closed.  kvm->lock is held.

	/*

	 * Since this is the device release function, we know that

	 * userspace does not have any open fd referring to the

	 * device.  Therefore there can not be any of the device

	 * attribute set/get functions being executed concurrently,

	 * and similarly, the connect_vcpu and set/clr_mapped

	 * functions also cannot be being executed.

	/*

	 * We should clean up the vCPU interrupt presenters first.

		/*

		 * Take vcpu->mutex to ensure that no one_reg get/set ioctl

		 * (i.e. kvmppc_xive_[gs]et_icp) can be done concurrently.

		 * Holding the vcpu->mutex also means that the vcpu cannot

		 * be executing the KVM_RUN ioctl, and therefore it cannot

		 * be executing the XIVE push or pull code or accessing

		 * the XIVE MMIO regions.

	/*

	 * Now that we have cleared vcpu->arch.xive_vcpu, vcpu->arch.irq_type

	 * and vcpu->arch.xive_esc_[vr]addr on each vcpu, we are safe

	 * against xive code getting called during vcpu execution or

	 * set/get one_reg operations.

 Mask and free interrupts */

	/*

	 * A reference of the kvmppc_xive pointer is now kept under

	 * the xive_devices struct of the machine for reuse. It is

	 * freed when the VM is destroyed for now until we fix all the

	 * execution paths.

/*

 * When the guest chooses the interrupt mode (XICS legacy or XIVE

 * native), the VM will switch of KVM device. The previous device will

 * be "released" before the new one is created.

 *

 * Until we are sure all execution paths are well protected, provide a

 * fail safe (transitional) method for device destruction, in which

 * the XIVE device pointer is recycled and not directly freed.

/*

 * Create a XICS device with XIVE backend.  kvm->lock is held.

 Already there ? */

 We use the default queue size set by the host */

 VP allocation is delayed to the first call to connect_vcpu */

	/* KVM_MAX_VCPUS limits the number of VMs to roughly 64 per sockets

	 * on a POWER9 system.

 The VM should have configured XICS mode before doing XICS hcalls. */

 Register some debug interfaces */

 SPDX-License-Identifier: GPL-2.0

/*

 * Secure pages management: Migration of pages between normal and secure

 * memory of KVM guests.

 *

 * Copyright 2018 Bharata B Rao, IBM Corp. <bharata@linux.ibm.com>

/*

 * A pseries guest can be run as secure guest on Ultravisor-enabled

 * POWER platforms. On such platforms, this driver will be used to manage

 * the movement of guest pages between the normal memory managed by

 * hypervisor (HV) and secure memory managed by Ultravisor (UV).

 *

 * The page-in or page-out requests from UV will come to HV as hcalls and

 * HV will call back into UV via ultracalls to satisfy these page requests.

 *

 * Private ZONE_DEVICE memory equal to the amount of secure memory

 * available in the platform for running secure guests is hotplugged.

 * Whenever a page belonging to the guest becomes secure, a page from this

 * private device memory is used to represent and track that secure page

 * on the HV side. Some pages (like virtio buffers, VPA pages etc) are

 * shared between UV and HV. However such pages aren't represented by

 * device private memory and mappings to shared memory exist in both

 * UV and HV page tables.

/*

 * Notes on locking

 *

 * kvm->arch.uvmem_lock is a per-guest lock that prevents concurrent

 * page-in and page-out requests for the same GPA. Concurrent accesses

 * can either come via UV (guest vCPUs requesting for same page)

 * or when HV and guest simultaneously access the same page.

 * This mutex serializes the migration of page from HV(normal) to

 * UV(secure) and vice versa. So the serialization points are around

 * migrate_vma routines and page-in/out routines.

 *

 * Per-guest mutex comes with a cost though. Mainly it serializes the

 * fault path as page-out can occur when HV faults on accessing secure

 * guest pages. Currently UV issues page-in requests for all the guest

 * PFNs one at a time during early boot (UV_ESM uvcall), so this is

 * not a cause for concern. Also currently the number of page-outs caused

 * by HV touching secure pages is very very low. If an when UV supports

 * overcommitting, then we might see concurrent guest driven page-outs.

 *

 * Locking order

 *

 * 1. kvm->srcu - Protects KVM memslots

 * 2. kvm->mm->mmap_lock - find_vma, migrate_vma_pages and helpers, ksm_madvise

 * 3. kvm->arch.uvmem_lock - protects read/writes to uvmem slots thus acting

 *			     as sync-points for page-in/out

/*

 * Notes on page size

 *

 * Currently UV uses 2MB mappings internally, but will issue H_SVM_PAGE_IN

 * and H_SVM_PAGE_OUT hcalls in PAGE_SIZE(64K) granularity. HV tracks

 * secure GPAs at 64K page size and maintains one device PFN for each

 * 64K secure GPA. UV_PAGE_IN and UV_PAGE_OUT calls by HV are also issued

 * for 64K page at a time.

 *

 * HV faulting on secure pages: When HV touches any secure page, it

 * faults and issues a UV_PAGE_OUT request with 64K page size. Currently

 * UV splits and remaps the 2MB page if necessary and copies out the

 * required 64K page contents.

 *

 * Shared pages: Whenever guest shares a secure page, UV will split and

 * remap the 2MB page if required and issue H_SVM_PAGE_IN with 64K page size.

 *

 * HV invalidating a page: When a regular page belonging to secure

 * guest gets unmapped, HV informs UV with UV_PAGE_INVAL of 64K

 * page size. Using 64K page size is correct here because any non-secure

 * page will essentially be of 64K page size. Splitting by UV during sharing

 * and page-out ensures this.

 *

 * Page fault handling: When HV handles page fault of a page belonging

 * to secure guest, it sends that to UV with a 64K UV_PAGE_IN request.

 * Using 64K size is correct here too as UV would have split the 2MB page

 * into 64k mappings and would have done page-outs earlier.

 *

 * In summary, the current secure pages handling code in HV assumes

 * 64K page size and in fact fails any page-in/page-out requests of

 * non-64K size upfront. If and when UV starts supporting multiple

 * page-sizes, we need to break this assumption.

/*

 * States of a GFN

 * ---------------

 * The GFN can be in one of the following states.

 *

 * (a) Secure - The GFN is secure. The GFN is associated with

 *	a Secure VM, the contents of the GFN is not accessible

 *	to the Hypervisor.  This GFN can be backed by a secure-PFN,

 *	or can be backed by a normal-PFN with contents encrypted.

 *	The former is true when the GFN is paged-in into the

 *	ultravisor. The latter is true when the GFN is paged-out

 *	of the ultravisor.

 *

 * (b) Shared - The GFN is shared. The GFN is associated with a

 *	a secure VM. The contents of the GFN is accessible to

 *	Hypervisor. This GFN is backed by a normal-PFN and its

 *	content is un-encrypted.

 *

 * (c) Normal - The GFN is a normal. The GFN is associated with

 *	a normal VM. The contents of the GFN is accesible to

 *	the Hypervisor. Its content is never encrypted.

 *

 * States of a VM.

 * ---------------

 *

 * Normal VM:  A VM whose contents are always accessible to

 *	the hypervisor.  All its GFNs are normal-GFNs.

 *

 * Secure VM: A VM whose contents are not accessible to the

 *	hypervisor without the VM's consent.  Its GFNs are

 *	either Shared-GFN or Secure-GFNs.

 *

 * Transient VM: A Normal VM that is transitioning to secure VM.

 *	The transition starts on successful return of

 *	H_SVM_INIT_START, and ends on successful return

 *	of H_SVM_INIT_DONE. This transient VM, can have GFNs

 *	in any of the three states; i.e Secure-GFN, Shared-GFN,

 *	and Normal-GFN.	The VM never executes in this state

 *	in supervisor-mode.

 *

 * Memory slot State.

 * -----------------------------

 *	The state of a memory slot mirrors the state of the

 *	VM the memory slot is associated with.

 *

 * VM State transition.

 * --------------------

 *

 *  A VM always starts in Normal Mode.

 *

 *  H_SVM_INIT_START moves the VM into transient state. During this

 *  time the Ultravisor may request some of its GFNs to be shared or

 *  secured. So its GFNs can be in one of the three GFN states.

 *

 *  H_SVM_INIT_DONE moves the VM entirely from transient state to

 *  secure-state. At this point any left-over normal-GFNs are

 *  transitioned to Secure-GFN.

 *

 *  H_SVM_INIT_ABORT moves the transient VM back to normal VM.

 *  All its GFNs are moved to Normal-GFNs.

 *

 *  UV_TERMINATE transitions the secure-VM back to normal-VM. All

 *  the secure-GFN and shared-GFNs are tranistioned to normal-GFN

 *  Note: The contents of the normal-GFN is undefined at this point.

 *

 * GFN state implementation:

 * -------------------------

 *

 * Secure GFN is associated with a secure-PFN; also called uvmem_pfn,

 * when the GFN is paged-in. Its pfn[] has KVMPPC_GFN_UVMEM_PFN flag

 * set, and contains the value of the secure-PFN.

 * It is associated with a normal-PFN; also called mem_pfn, when

 * the GFN is pagedout. Its pfn[] has KVMPPC_GFN_MEM_PFN flag set.

 * The value of the normal-PFN is not tracked.

 *

 * Shared GFN is associated with a normal-PFN. Its pfn[] has

 * KVMPPC_UVMEM_SHARED_PFN flag set. The value of the normal-PFN

 * is not tracked.

 *

 * Normal GFN is associated with normal-PFN. Its pfn[] has

 * no flag set. The value of the normal-PFN is not tracked.

 *

 * Life cycle of a GFN

 * --------------------

 *

 * --------------------------------------------------------------

 * |        |     Share  |  Unshare | SVM       |H_SVM_INIT_DONE|

 * |        |operation   |operation | abort/    |               |

 * |        |            |          | terminate |               |

 * -------------------------------------------------------------

 * |        |            |          |           |               |

 * | Secure |     Shared | Secure   |Normal     |Secure         |

 * |        |            |          |           |               |

 * | Shared |     Shared | Secure   |Normal     |Shared         |

 * |        |            |          |           |               |

 * | Normal |     Shared | Secure   |Normal     |Secure         |

 * --------------------------------------------------------------

 *

 * Life cycle of a VM

 * --------------------

 *

 * --------------------------------------------------------------------

 * |         |  start    |  H_SVM_  |H_SVM_   |H_SVM_     |UV_SVM_    |

 * |         |  VM       |INIT_START|INIT_DONE|INIT_ABORT |TERMINATE  |

 * |         |           |          |         |           |           |

 * --------- ----------------------------------------------------------

 * |         |           |          |         |           |           |

 * | Normal  | Normal    | Transient|Error    |Error      |Normal     |

 * |         |           |          |         |           |           |

 * | Secure  |   Error   | Error    |Error    |Error      |Normal     |

 * |         |           |          |         |           |           |

 * |Transient|   N/A     | Error    |Secure   |Normal     |Normal     |

 * --------------------------------------------------------------------

	/*

	 * If kvmppc_uvmem_bitmap != NULL, then there is an ultravisor

	 * and our data structures have been initialized successfully.

/*

 * All device PFNs are already released by the time we come here.

 mark the GFN as secure-GFN associated with @uvmem pfn device-PFN. */

 mark the GFN as secure-GFN associated with a memory-PFN. */

 mark the GFN as a shared GFN. */

 mark the GFN as a non-existent GFN. */

 return true, if the GFN is a secure-GFN backed by a secure-PFN */

/*

 * starting from *gfn search for the next available GFN that is not yet

 * transitioned to a secure GFN.  return the value of that GFN in *gfn.  If a

 * GFN is found, return true, else return false

 *

 * Must be called with kvm->arch.uvmem_lock  held.

	/*

	 * The code below assumes, one to one correspondence between

	 * kvmppc_uvmem_slot and memslot.

 Only radix guests can be secure guests */

 NAK the transition to secure if not enabled */

 register the memslot */

/*

 * Provision a new page on HV side and copy over the contents

 * from secure memory using UV_PAGE_OUT uvcall.

 * Caller must held kvm->arch.uvmem_lock.

 The requested page is already paged-out, nothing to do */

	/*

	 * This function is used in two cases:

	 * - When HV touches a secure page, for which we do UV_PAGE_OUT

	 * - When a secure page is converted to shared page, we *get*

	 *   the page to essentially unmap the device page. In this

	 *   case we skip page-out.

/*

 * Drop device pages that we maintain for the secure guest

 *

 * We first mark the pages to be skipped from UV_PAGE_OUT when there

 * is HV side fault on these pages. Next we *get* these pages, forcing

 * fault on them, do fault time migration to replace the device PTEs in

 * QEMU page table with normal PTEs from newly allocated pages.

 Fetch the VMA if addr is not in the latest fetched one */

 Remove the shared flag if any */

	/*

	 * Expect to be called only after INIT_START and before INIT_DONE.

	 * If INIT_DONE was completed, use normal VM termination sequence.

/*

 * Get a free device PFN from the pool

 *

 * Called when a normal page is moved to secure memory (UV_PAGE_IN). Device

 * PFN will be used to keep track of the secure page on HV side.

 *

 * Called with kvm->arch.uvmem_lock held

/*

 * Alloc a PFN from private device memory pool. If @pagein is true,

 * copy page from normal memory to secure memory using UV_PAGE_IN uvcall.

 relinquish the cpu if needed */

 migrate any unmoved normal pfn to device pfns*/

			/*

			 * The pages will remain transitioned.

			 * Its the callers responsibility to

			 * terminate the VM, which will undo

			 * all state of the VM. Till then

			 * this VM is in a erroneous state.

			 * Its KVMPPC_SECURE_INIT_DONE will

			 * remain unset.

/*

 * Shares the page with HV, thus making it a normal page.

 *

 * - If the page is already secure, then provision a new page and share

 * - If the page is a normal page, share the existing page

 *

 * In the former case, uses dev_pagemap_ops.migrate_to_ram handler

 * to unmap the device page from QEMU's page tables.

		/*

		 * do not drop the GFN. It is a valid GFN

		 * that is transitioned to a shared GFN.

 it continues to be a valid GFN */

/*

 * H_SVM_PAGE_IN: Move page from normal memory to secure memory.

 *

 * H_PAGE_IN_SHARED flag makes the page shared which means that the same

 * memory in is visible from both UV and HV.

 Fail the page-in request of an already paged-in page */

/*

 * Fault handler callback that gets called when HV touches any page that

 * has been moved to secure memory, we ask UV to give back the page by

 * issuing UV_PAGE_OUT uvcall.

 *

 * This eventually results in dropping of device PFN and the newly

 * provisioned page/PFN gets populated in QEMU page tables.

/*

 * Release the device PFN back to the pool

 *

 * Gets called when secure GFN tranistions from a secure-PFN

 * to a normal PFN during H_SVM_PAGE_OUT.

 * Gets called with kvm->arch.uvmem_lock held.

/*

 * H_SVM_PAGE_OUT: Move page from secure memory to normal memory.

	/*

	 * First try the new ibm,secure-memory nodes which supersede the

	 * secure-memory-ranges property.

	 * If we found some, no need to read the deprecated ones.

		/*

		 * Don't fail the initialization of kvm-hv module if

		 * the platform doesn't export ibm,uv-firmware node.

		 * Let normal guests run on such PEF-disabled platform.

 just one global instance: */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2008-2013 Freescale Semiconductor, Inc. All rights reserved.

 *

 * Author: Yu Liu, yu.liu@freescale.com

 *         Scott Wood, scottwood@freescale.com

 *         Ashish Kalra, ashish.kalra@freescale.com

 *         Varun Sethi, varun.sethi@freescale.com

 *         Alexander Graf, agraf@suse.de

 *

 * Description:

 * This file is based on arch/powerpc/kvm/44x_tlb.c,

 * by Hollis Blanchard <hollisb@us.ibm.com>.

 reserve one entry for magic page */

 Mask off reserved bits. */

		/* Guest is in supervisor mode,

		 * so we need to translate guest

/*

 * writing shadow tlb entry to host TLB

 Must clear mas8 for other host tlbwe's */

/*

 * Acquire a mas0 with victim hint, as if we just took a TLB miss.

 *

 * We don't care about the address we're searching for, other than that it's

 * in the right set and is not present in the TLB.  Using a zero PID and a

 * userspace address means we don't have to set and then restore MAS5, or

 * calculate a proper MAS6 value.

 sesel is for tlb1 only */

 sesel is for tlb1 only */

 XXX should be a hook in the gva2hpa translation */

 Don't bother with unmapped entries */

		/*

		 * TLB1 entry is backed by 4k pages. This should happen

		 * rarely and is not worth optimizing. Invalidate everything.

	/*

	 * If TLB entry is still valid then it's a TLB0 entry, and thus

	 * backed by at most one host tlbe per shadow pid

 Mark the TLB as not backed by the host anymore */

 Use guest supplied MAS2_G and MAS2_E */

 Mark the page accessed */

 FIXME: don't log bogus pfn for TLB1 */

 TID must be supplied by the caller */

 Force IPROT=0 for all guest mappings. */

 silence GCC warning */

 used to check for invalidations in progress */

	/*

	 * Translate guest physical to true physical, acquiring

	 * a page reference if it is normal, non-reserved memory.

	 *

	 * gfn_to_memslot() must succeed because otherwise we wouldn't

	 * have gotten this far.  Eventually we should just pass the slot

	 * pointer through from the first lookup.

			/*

			 * This VMA is a physically contiguous region (e.g.

			 * /dev/mem) that bypasses normal Linux page

			 * management.  Find the overlap between the

			 * vma and the memslot.

			/*

			 * e500 doesn't implement the lowest tsize bit,

			 * or 1K pages.

			/*

			 * Now find the largest tsize (up to what the guest

			 * requested) that will cover gfn, stay within the

			 * range, and for which gfn and pfn are mutually

			 * aligned.

			/*

			 * Take the largest page size that satisfies both host

			 * and guest mapping

			/*

			 * e500 doesn't implement the lowest tsize bit,

			 * or 1K pages.

 Align guest and physical address to page map boundaries */

	/*

	 * We are just looking at the wimg bits, so we don't

	 * care much about the trans splitting bit.

	 * We are holding kvm->mmu_lock so a notifier invalidate

	 * can't run hence pfn won't change.

 Clear i-cache for new pages */

 Drop refcount on page, so that mmu notifiers can clear it */

 XXX only map the one-one case, for now use TLB0 */

/* Caller must ensure that the specified guest TLB entry is safe to insert into

 For both one-one and one-to-many */

 Use TLB0 when we can only map a page with 4k */

 Otherwise map into TLB1 */

 Triggers after clear_tlb_privs or on initial mapping */

 Search TLB for guest pc to get the real address */

	/*

	 * If the TLB entry for guest pc was evicted, return to the guest.

	 * There are high chances to find a valid TLB entry next time.

	/*

	 * Another thread may rewrite the TLB entry in parallel, don't

	 * execute from the address if the execute permission is not set

	/*

	 * The real address will be mapped by a cacheable, memory coherent,

	 * write-back page. Check for mismatches when LRAT is used.

 Get pfn */

 Guard against emulation from devices area */

 Map a page and get guest's instruction */

************ MMU Notifiers *************/

	/*

	 * Flush all shadow tlb entries everywhere. This is slow, but

	 * we are 100% sure that we catch the to be unmapped page

 XXX could be more clever ;) */

 XXX could be more clever ;) */

 The page will get remapped properly on its next fault */

****************************************/

	/*

	 * This should never happen on real e500 hardware, but is

	 * architecturally possible -- e.g. in some weird nested

	 * virtualization case.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2012 Michael Ellerman, IBM Corporation.

 * Copyright 2012 Benjamin Herrenschmidt, IBM Corporation.

/*

 * LOCKING

 * =======

 *

 * Each ICS has a spin lock protecting the information about the IRQ

 * sources and avoiding simultaneous deliveries of the same interrupt.

 *

 * ICP operations are done via a single compare & swap transaction

 * (most ICP state fits in the union kvmppc_icp_state)

/*

 * TODO

 * ====

 *

 * - To speed up resends, keep a bitmap of "resend" set bits in the

 *   ICS

 *

 * - Speed up server# -> ICP lookup (array ? hash table ?)

 *

 * - Make ICS lockless as well, or at least a per-interrupt lock or hashed

 *   locks array to improve scalability

 -- ICS routines -- */

/*

 * Return value ideally indicates how the interrupt was handled, but no

 * callers look at it (given that we don't implement KVM_IRQ_LINE_STATUS),

 * so just return 0.

	/*

	 * Take other values the same as 1, consistent with original code.

	 * maybe WARN here?

 noop for MSI */

 Setting already set LSI ... */

 Test P=1, Q=0, this is the only case where we present */

 Record which CPU this arrived on for passed-through interrupts */

 -- ICP routines, including hcalls -- */

 Calculate new output value */

 Attempt atomic update */

	/*

	 * Check for output state update

	 *

	 * Note that this is racy since another processor could be updating

	 * the state already. This is why we never clear the interrupt output

	 * here, we only ever set it. The clear only happens prior to doing

	 * an update and only by the processor itself. Currently we do it

	 * in Accept (H_XIRR) and Up_Cppr (H_XPPR).

	 *

	 * We also do not try to figure out whether the EE state has changed,

	 * we unconditionally set it if the new state calls for it. The reason

	 * for that is that we opportunistically remove the pending interrupt

	 * flag when raising CPPR, so we need to set it back here if an

	 * interrupt is still pending.

 Order this load with the test for need_resend in the caller */

 See if we can deliver */

		/*

		 * If we can, check for a rejection and perform the

		 * delivery

			/*

			 * If we failed to deliver we set need_resend

			 * so a subsequent CPPR state change causes us

			 * to try a new delivery.

	/*

	 * This is used both for initial delivery of an interrupt and

	 * for subsequent rejection.

	 *

	 * Rejection can be racy vs. resends. We have evaluated the

	 * rejection in an atomic ICP transaction which is now complete,

	 * so potentially the ICP can already accept the interrupt again.

	 *

	 * So we need to retry the delivery. Essentially the reject path

	 * boils down to a failed delivery. Always.

	 *

	 * Now the interrupt could also have moved to a different target,

	 * thus we may need to re-do the ICP lookup as well

 Get the ICS state and lock it */

 Get a lock on the ICS */

 Get our server */

 Clear the resend bit of that interrupt */

	/*

	 * If masked, bail out

	 *

	 * Note: PAPR doesn't mention anything about masked pending

	 * when doing a resend, only when doing a delivery.

	 *

	 * However that would have the effect of losing a masked

	 * interrupt that was rejected and isn't consistent with

	 * the whole masked_pending business which is about not

	 * losing interrupts that occur while masked.

	 *

	 * I don't differentiate normal deliveries and resends, this

	 * implementation will differ from PAPR and not lose such

	 * interrupts.

	/*

	 * Try the delivery, this will set the need_resend flag

	 * in the ICP as part of the atomic transaction if the

	 * delivery is not possible.

	 *

	 * Note that if successful, the new delivery might have itself

	 * rejected an interrupt that was "delivered" before we took the

	 * ics spin lock.

	 *

	 * In this case we do the whole sequence all over again for the

	 * new guy. We cannot assume that the rejected interrupt is less

	 * favored than the new one, and thus doesn't need to be delivered,

	 * because by the time we exit icp_try_to_deliver() the target

	 * processor may well have alrady consumed & completed it, and thus

	 * the rejected interrupt might actually be already acceptable.

		/*

		 * Delivery was successful, did we reject somebody else ?

		/*

		 * We failed to deliver the interrupt we need to set the

		 * resend map bit and mark the ICS state as needing a resend

		/*

		 * Make sure when checking resend, we don't miss the resend

		 * if resend_map bit is seen and cleared.

		/*

		 * If the need_resend flag got cleared in the ICP some time

		 * between icp_try_to_deliver() atomic update and now, then

		 * we know it might have missed the resend_map bit. So we

		 * retry

	/*

	 * This handles several related states in one operation:

	 *

	 * ICP State: Down_CPPR

	 *

	 * Load CPPR with new value and if the XISR is 0

	 * then check for resends:

	 *

	 * ICP State: Resend

	 *

	 * If MFRR is more favored than CPPR, check for IPIs

	 * and notify ICS of a potential resend. This is done

	 * asynchronously (when used in real mode, we will have

	 * to exit here).

	 *

	 * We do not handle the complete Check_IPI as documented

	 * here. In the PAPR, this state will be used for both

	 * Set_MFRR and Down_CPPR. However, we know that we aren't

	 * changing the MFRR state here so we don't need to handle

	 * the case of an MFRR causing a reject of a pending irq,

	 * this will have been handled when the MFRR was set in the

	 * first place.

	 *

	 * Thus we don't have to handle rejects, only resends.

	 *

	 * When implementing real mode for HV KVM, resend will lead to

	 * a H_TOO_HARD return and the whole transaction will be handled

	 * in virtual mode.

 Down_CPPR */

		/*

		 * Cut down Resend / Check_IPI / IPI

		 *

		 * The logic is that we cannot have a pending interrupt

		 * trumped by an IPI at this point (see above), so we

		 * know that either the pending interrupt is already an

		 * IPI (in which case we don't care to override it) or

		 * it's either more favored than us or non existent

 Latch/clear resend bit */

	/*

	 * Now handle resend checks. Those are asynchronous to the ICP

	 * state update in HW (ie bus transactions) so we can handle them

	 * separately here too

 First, remove EE from the processor */

	/*

	 * ICP State: Accept_Interrupt

	 *

	 * Return the pending interrupt (if any) along with the

	 * current CPPR, then clear the XISR & set CPPR to the

	 * pending priority

	/*

	 * ICP state: Set_MFRR

	 *

	 * If the CPPR is more favored than the new MFRR, then

	 * nothing needs to be rejected as there can be no XISR to

	 * reject.  If the MFRR is being made less favored then

	 * there might be a previously-rejected interrupt needing

	 * to be resent.

	 *

	 * ICP state: Check_IPI

	 *

	 * If the CPPR is less favored, then we might be replacing

	 * an interrupt, and thus need to possibly reject it.

	 *

	 * ICP State: IPI

	 *

	 * Besides rejecting any pending interrupts, we also

	 * update XISR and pending_pri to mark IPI as pending.

	 *

	 * PAPR does not describe this state, but if the MFRR is being

	 * made less favored than its earlier value, there might be

	 * a previously-rejected interrupt needing to be resent.

	 * Ideally, we would want to resend only if

	 *	prio(pending_interrupt) < mfrr &&

	 *	prio(pending_interrupt) < cppr

	 * where pending interrupt is the one that was rejected. But

	 * we don't have that state, so we simply trigger a resend

	 * whenever the MFRR is made less favored.

 Set_MFRR */

 Check_IPI */

 Reject a pending interrupt if not an IPI */

 Handle reject */

 Handle resend */

	/*

	 * ICP State: Set_CPPR

	 *

	 * We can safely compare the new value with the current

	 * value outside of the transaction as the CPPR is only

	 * ever changed by the processor on itself

	/*

	 * ICP State: Up_CPPR

	 *

	 * The processor is raising its priority, this can result

	 * in a rejection of a pending interrupt:

	 *

	 * ICP State: Reject_Current

	 *

	 * We can remove EE from the current processor, the update

	 * transaction will set it again if needed

	/*

	 * Check for rejects. They are handled by doing a new delivery

	 * attempt (see comments in icp_deliver_irq).

	/*

	 * ICS EOI handling: For LSI, if P bit is still set, we need to

	 * resend it.

	 *

	 * For MSI, we move Q bit into P (and clear Q). If it is set,

	 * resend it.

	/*

	 * ICP State: EOI

	 *

	 * Note: If EOI is incorrectly used by SW to lower the CPPR

	 * value (ie more favored), we do not check for rejection of

	 * a pending interrupt, this is a SW error and PAPR specifies

	 * that we don't have to deal with it.

	 *

	 * The sending of an EOI to the ICS is handled after the

	 * CPPR update

	 *

	 * ICP State: Down_CPPR which we handle

	 * in a separate function as it's shared with H_CPPR.

 IPIs have no EOI */

 Check if we have an ICP */

 These requests don't have real-mode implementations at present */

 Check for real mode returning too hard */

 -- Initialisation code etc. -- */

 ICS already exists - somebody else got here first */

 Create the ICS */

 Require the new state to be internally consistent */

	/*

	 * Deassert the CPU interrupt request.

	 * icp_try_update will reassert it if necessary.

	/*

	 * Note that if we displace an interrupt from old_state.xisr,

	 * we don't mark it as rejected.  We expect userspace to set

	 * the state of the interrupt sources to be consistent with

	 * the ICP states (either before or afterwards, which doesn't

	 * matter).  We do handle resends due to CPPR becoming less

	 * favoured because that is necessary to end up with a

	 * consistent state in the situation where userspace restores

	 * the ICS states before the ICP states.

 If PENDING, set P in case P is not saved because of old code */

/*

 * Called when device fd is closed. kvm->lock is held.

	/*

	 * Since this is the device release function, we know that

	 * userspace does not have any open fd referring to the

	 * device.  Therefore there can not be any of the device

	 * attribute set/get functions being executed concurrently,

	 * and similarly, the connect_vcpu and set/clr_mapped

	 * functions also cannot be being executed.

	/*

	 * We should clean up the vCPU interrupt presenters first.

		/*

		 * Take vcpu->mutex to ensure that no one_reg get/set ioctl

		 * (i.e. kvmppc_xics_[gs]et_icp) can be done concurrently.

		 * Holding the vcpu->mutex also means that execution is

		 * excluded for the vcpu until the ICP was freed. When the vcpu

		 * can execute again, vcpu->arch.icp and vcpu->arch.irq_type

		 * have been cleared and the vcpu will not be going into the

		 * XICS code anymore.

	/*

	 * A reference of the kvmppc_xics pointer is now kept under

	 * the xics_device pointer of the machine for reuse. It is

	 * freed when the VM is destroyed for now until we fix all the

	 * execution paths.

 Already there ? */

 Enable real mode support */

 CONFIG_KVM_BOOK3S_HV_POSSIBLE */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2011 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

/*

 * Hash page table alignment on newer cpus(CPU_FTR_ARCH_206)

 * should be power of 2.

 256k */

/*

 * By default we reserve 5% of memory for hash pagetable allocation.

/**

 * kvm_cma_reserve() - reserve area for kvm hash pagetable

 *

 * This function reserves memory from early allocator. It should be

 * called by arch specific code once the memblock allocator

 * has been activated and all other subsystems have already allocated/reserved

 * memory.

	/*

	 * We need CMA reservation only when we are in HV mode

/*

 * Real-mode H_CONFER implementation.

 * We check if we are the only vcpu out of this virtual core

 * still running in the guest and not ceded.  If so, we pop up

 * to the virtual-mode implementation; if not, just return to

 * the guest.

 => don't yield */

 => do yield */

/*

 * When running HV mode KVM we need to block certain operations while KVM VMs

 * exist in the system. We use a counter of VMs to track this.

 *

 * One of the operations we need to block is onlining of secondaries, so we

 * protect hv_vm_count with cpus_read_lock/unlock().

/*

 * Send an interrupt or message to another CPU.

 * The caller needs to include any barrier needed to order writes

 * to memory vs. the IPI/message.

 On POWER9 we can use msgsnd for any destination cpu. */

 On POWER8 for IPIs to threads in the same core, use msgsnd. */

 We should never reach this */

 Else poke the target with an IPI */

/*

 * The following functions are called from the assembly code

 * in book3s_hv_rmhandlers.S.

 Order setting of exit map vs. msgsnd/IPI */

	/* Set our bit in the threads-exiting-guest map in the 0xff00

 Are we the first here? */

	/*

	 * Trigger the other threads in this vcore to exit the guest.

	 * If this is a hypervisor decrementer interrupt then they

	 * will be already on their way out of the guest.

	/*

	 * If we are doing dynamic micro-threading, interrupt the other

	 * subcores to pull them out of their guests too.

 Already asked to exit? */

	/*

	 * We access the mapped array here without a lock.  That

	 * is safe because we never reduce the number of entries

	 * in the array and we never change the v_hwirq field of

	 * an entry once it is set.

	 *

	 * We have also carefully ordered the stores in the writer

	 * and the loads here in the reader, so that if we find a matching

	 * hwirq here, the associated GSI and irq_desc fields are valid.

			/*

			 * Order subsequent reads in the caller to serialize

			 * with the writer.

/*

 * If we have an interrupt that's not an IPI, check if we have a

 * passthrough adapter and if so, check if this external interrupt

 * is for the adapter.

 * We will attempt to deliver the IRQ directly to the target VCPU's

 * ICP, the virtual ICP (based on affinity - the xive value in ICS).

 *

 * If the delivery fails or if this is not for a passthrough adapter,

 * return to the host to handle this interrupt. We earlier

 * saved a copy of the XIRR in the PACA, it will be picked up by

 * the host ICP driver.

 We're handling this interrupt, generic code doesn't need to */

/*

 * Determine what sort of external interrupt is pending (if any).

 * Returns:

 *	0 if no interrupt is pending

 *	1 if an interrupt is pending that needs to be handled by the host

 *	2 Passthrough that needs completion in the host

 *	-1 if there was a guest wakeup IPI (which has now been cleared)

 *	-2 if there is PCI passthrough external interrupt that was handled

 see if a host IPI is pending */

 Now read the interrupt from the ICP */

	/*

	 * Save XIRR for later. Since we get control in reverse endian

	 * on LE systems, save it byte reversed and fetch it back in

	 * host endian. Note that xirr is the value read from the

	 * XIRR register, while h_xirr is the host endian version.

	/*

	 * Ensure that the store/load complete to guarantee all side

	 * effects of loading from XIRR has completed

 if nothing pending in the ICP */

	/* We found something in the ICP...

	 *

	 * If it is an IPI, clear the MFRR and EOI it.

 If rc > 0, there is another interrupt pending */

		/*

		 * Need to ensure side effects of above stores

		 * complete before proceeding.

		/*

		 * We need to re-check host IPI now in case it got set in the

		 * meantime. If it's clear, we bounce the interrupt to the

		 * guest

			/* We raced with the host,

			 * we need to resend that IPI, bummer

 Let side effects complete */

 OK, it's an IPI for us */

 CONFIG_KVM_XICS */

	/*

	 * 100 could happen at any time, 200 can happen due to invalid real

	 * address access for example (or any time due to a hardware problem).

 Guest must always run with ME enabled, HV disabled. */

	/*

	 * Check for illegal transactional state bit combination

	 * and if we find it, force the TS field to a safe state.

 If transactional, change to suspend mode on IRQ delivery */

	/*

	 * Perform MSR and PC adjustment for LPCR[AIL]=3 if it is set and

	 * applicable. AIL=2 is not supported.

	 *

	 * AIL does not apply to SRESET, MCE, or HMI (which is never

	 * delivered to the guest), and does not apply if IR=0 or DR=0.

/*

 * Is there a PRIV_DOORBELL pending for the guest (on POWER9)?

 * Can we inject a Decrementer or a External interrupt?

 Insert EXTERNAL bit into LPCR at the MER bit position */

 IS = 2 */

 R=1 PRS=1 RIC=2 */

 increment set number */

 R=1 PRS=1 RIC=0 */

 R=0 PRS=0 RIC=0 */

 increment set number */

	/*

	 * On POWER9, individual threads can come in here, but the

	 * TLB is shared between the 4 threads in a core, hence

	 * invalidating on one thread invalidates for all.

	 * Thus we make all 4 threads use the same bit.

 Clear the bit after the TLB flush */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright 2010 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

 * Copyright 2011 David Gibson, IBM Corporation <dwg@au1.ibm.com>

 * Copyright 2016 Alexey Kardashevskiy, IBM Corporation <aik@au1.ibm.com>

/*

 * Finds a TCE table descriptor by LIOBN.

 *

 * WARNING: This will be called in real or virtual mode on HV KVM and virtual

 *          mode on PR KVM

/*

 * Validates TCE address.

 * At the moment flags and page mask are validated.

 * As the host kernel does not access those addresses (just puts them

 * to the table and user space is supposed to process them), we can skip

 * checking other things (such as TCE is a guest RAM address or the page

 * was actually allocated).

 Allow userspace to poison TCE table */

/* Note on the use of page_address() in real mode,

 *

 * It is safe to use page_address() in real mode on ppc64 because

 * page_address() is always defined as lowmem_page_address()

 * which returns __va(PFN_PHYS(page_to_pfn(page))) which is arithmetic

 * operation and does not access page struct.

 *

 * Theoretically page_address() could be defined different

 * but either WANT_PAGE_VIRTUAL or HASHED_PAGE_VIRTUAL

 * would have to be enabled.

 * WANT_PAGE_VIRTUAL is never enabled on ppc32/ppc64,

 * HASHED_PAGE_VIRTUAL could be enabled for ppc32 only and only

 * if CONFIG_HIGHMEM is defined. As CONFIG_SPARSEMEM_VMEMMAP

 * is not expected to be enabled on ppc32, page_address()

 * is safe for ppc32 as well.

 *

 * WARNING: This will be called in real-mode on HV KVM and virtual

 *          mode on PR KVM

/*

 * Handles TCE requests for emulated devices.

 * Puts guest TCE values to the table and expects user space to convert them.

 * Cannot fail so kvmppc_rm_tce_validate must be called before it.

	/*

	 * kvmppc_rm_ioba_validate() allows pages not be allocated if TCE is

	 * being cleared, otherwise it returns H_TOO_HARD and we skip this.

/*

 * TCEs pages are allocated in kvmppc_rm_tce_put() which won't be able to do so

 * in real mode.

 * Check if kvmppc_rm_tce_put() can succeed in real mode, i.e. a TCEs page is

 * allocated or not required (when clearing a tce entry).

	/*

	 * clearing==true says kvmppc_rm_tce_put won't be allocating pages

	 * for empty tces.

		/*

		 * kvmppc_rm_tce_iommu_do_map() updates the UA cache after

		 * calling this so we still get here a valid UA.

 it_userspace allocation might be delayed */

		/*

		 * real mode xchg can fail if struct page crosses

		 * a page boundary

 it_userspace allocation might be delayed */

		/*

		 * real mode xchg can fail if struct page crosses

		 * a page boundary

 udbg_printf("H_PUT_TCE(): liobn=0x%lx ioba=0x%lx, tce=0x%lx\n", */

 	    liobn, ioba, tce); */

	/*

	 * Called in real mode with MSR_EE = 0. We are safe here.

	 * It is ok to do the lookup with arch.pgdir here, because

	 * we are doing this on secondary cpus and current task there

	 * is not the hypervisor. Also this is safe against THP in the

	 * host, because an IPI to primary thread will wait for the secondary

	 * to exit which will agains result in the below page table walk

	 * to finish.

	/* an rmap lock won't make it safe. because that just ensure hash

	 * page table entries are removed with rmap lock held. After that

	 * mmu notifier returns and we go ahead and removing ptes from Qemu page table.

 Avoid handling anything potentially complicated in realmode */

	/*

	 * used to check for invalidations in progress

	/*

	 * The spec says that the maximum size of the list is 512 TCEs

	 * so the whole table addressed resides in 4K page

		/*

		 * We get here if guest memory was pre-registered which

		 * is normally VFIO case and gpa->hpa translation does not

		 * depend on hpt.

		/*

		 * This is usually a case of a guest with emulated devices only

		 * when TCE list is not in preregistered memory.

		 * We do not require memory to be preregistered in this case

		 * so lock rmap and do __find_linux_pte_or_hugepte().

 Check permission bits only to allow userspace poison TCE for debug */

 This can be called in either virtual mode or real mode */

 KVM_BOOK3S_HV_POSSIBLE */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright IBM Corp. 2008

 * Copyright 2011 Freescale Semiconductor, Inc.

 *

 * Authors: Hollis Blanchard <hollisb@us.ibm.com>

/*

 * NOTE: some of these registers are not emulated on BOOKE_HV (GS-mode).

 * Their backing store is in real registers, and these functions

 * will return the wrong result if called for them in another context

 * (such as debugging).

		/*

		 * If userspace is debugging guest then guest

		 * can not access debug registers.

		/*

		 * If userspace is debugging guest then guest

		 * can not access debug registers.

		/*

		 * If userspace is debugging guest then guest

		 * can not access debug registers.

		/*

		 * If userspace is debugging guest then guest

		 * can not access debug registers.

		/*

		 * If userspace is debugging guest then guest

		 * can not access debug registers.

		/*

		 * If userspace is debugging guest then guest

		 * can not access debug registers.

		/*

		 * If userspace is debugging guest then guest

		 * can not access debug registers.

		/*

		 * If userspace is debugging guest then guest

		 * can not access debug registers.

		/*

		 * If userspace is debugging guest then guest

		 * can not access debug registers.

		/*

		 * If userspace is debugging guest then guest

		 * can not access debug registers.

		/*

		 * WRC is a 2-bit field that is supposed to preserve its

		 * value once written to non-zero.

	/*

	 * Note: SPRG4-7 are user-readable.

	 * These values are loaded into the real SPRGs when resuming the

	 * guest (PR-mode only).

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2017-2019, IBM Corporation.

	/*

	 * The KVM XIVE native device does not use the XIVE_ESB_SET_PQ_10

	 * load operation, so there is no need to enforce load-after-store

	 * ordering.

 Ensure no interrupt is still routed to that VP */

 Free escalations */

 Free the escalation irq */

 Disable the VP */

 Clear the cam word so guest entry won't try to push context */

 Free the queues */

 Free the VP */

 Cleanup the vcpu */

	/*

	 * Enable the VP first as the single escalation mode will

	 * affect escalation interrupts numbering

 Configure VCPU fields for use by assembly push/pull */

 TODO: reset all queues to a clean state ? */

/*

 * Device passthrough support

	/*

	 * Clear the ESB pages of the IRQ number being mapped (or

	 * unmapped) into the guest and let the the VM fault handler

	 * repopulate with the appropriate ESB pages (device or IC)

	/*

	 * Linux/KVM uses a two pages ESB setting, one for trigger and

	 * one for EOI

 Some sanity checking */

	/*

	 * first/even page is for trigger

	 * second/odd page is for EOI and management.

 HW - forbid access */

 HV - forbid access */

 OS */

 USER - TODO */

 We only allow mappings at fixed offset for now */

	/*

	 * Grab the KVM device file address_space to be able to clear

	 * the ESB pages mapping when a device is passed-through into

	 * the guest.

	/*

	 * If the source doesn't already have an IPI, allocate

	 * one and get the corresponding data

 Restore LSI state */

 Mask IRQ to start with */

 Increment the number of valid sources and mark this one valid */

	/*

	 * We only support 64K pages for the moment. This is also

	 * advertised in the DT property "ibm,xive-eq-sizes"

 EQ reset */

	/*

	 * Demangle priority/server tuple from the EQ identifier

 reset queue and disable queueing */

	/*

	 * sPAPR specifies a "Unconditional Notify (n) flag" for the

	 * H_INT_SET_QUEUE_CONFIG hcall which forces notification

	 * without using the coalescing mechanisms provided by the

	 * XIVE END ESBs. This is required on KVM as notification

	 * using the END ESBs is not supported.

	/*

	 * Backup the queue page guest address to the mark EQ page

	 * dirty for migration.

	 /*

	  * Unconditional Notification is forced by default at the

	  * OPAL level because the use of END ESBs is not supported by

	  * Linux.

	/*

	 * Only restore the queue state when needed. When doing the

	 * H_INT_SET_SOURCE_CONFIG hcall, it should not.

	/*

	 * Demangle priority/server tuple from the EQ identifier

 Single escalation, no queue 7 */

		/*

		 * The struct kvmppc_xive_irq_state reflects the state

		 * of the EAS configuration and not the state of the

		 * source. The source is masked setting the PQ bits to

		 * '-Q', which is what is being done before calling

		 * the KVM_DEV_XIVE_EQ_SYNC control.

		 *

		 * If a source EAS is configured, OPAL syncs the XIVE

		 * IC of the source and the XIVE IC of the previous

		 * target if any.

		 *

		 * So it should be fine ignoring MASKED sources as

		 * they have been synced already.

 Mark EQ page dirty for migration */

/*

 * Called when device fd is closed.  kvm->lock is held.

	/*

	 * Clear the KVM device file address_space which is used to

	 * unmap the ESB pages when a device is passed-through.

	/*

	 * Since this is the device release function, we know that

	 * userspace does not have any open fd or mmap referring to

	 * the device.  Therefore there can not be any of the

	 * device attribute set/get, mmap, or page fault functions

	 * being executed concurrently, and similarly, the

	 * connect_vcpu and set/clr_mapped functions also cannot

	 * be being executed.

	/*

	 * We should clean up the vCPU interrupt presenters first.

		/*

		 * Take vcpu->mutex to ensure that no one_reg get/set ioctl

		 * (i.e. kvmppc_xive_native_[gs]et_vp) can be being done.

		 * Holding the vcpu->mutex also means that the vcpu cannot

		 * be executing the KVM_RUN ioctl, and therefore it cannot

		 * be executing the XIVE push or pull code or accessing

		 * the XIVE MMIO regions.

	/*

	 * Now that we have cleared vcpu->arch.xive_vcpu, vcpu->arch.irq_type

	 * and vcpu->arch.xive_esc_[vr]addr on each vcpu, we are safe

	 * against xive code getting called during vcpu execution or

	 * set/get one_reg operations.

	/*

	 * A reference of the kvmppc_xive pointer is now kept under

	 * the xive_devices struct of the machine for reuse. It is

	 * freed when the VM is destroyed for now until we fix all the

	 * execution paths.

/*

 * Create a XIVE device.  kvm->lock is held.

 VP allocation is delayed to the first call to connect_vcpu */

	/* KVM_MAX_VCPUS limits the number of VMs to roughly 64 per sockets

	 * on a POWER9 system.

/*

 * Interrupt Pending Buffer (IPB) offset

 Thread context registers. We only care about IPB and CPPR */

 Get the VP state from OPAL */

	/*

	 * Capture the backup of IPB register in the NVT structure and

	 * merge it in our KVM VP state.

 We can't update the state of a "pushed" VCPU	 */

	/*

	 * Restore the thread context registers. IPB and CPPR should

	 * be the only ones that matter.

	/*

	 * There is no need to restore the XIVE internal state (IPB

	 * stored in the NVT) as the IPB register was merged in KVM VP

	 * state when captured.

 Register some debug interfaces */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011. Freescale Inc. All rights reserved.

 *

 * Authors:

 *    Alexander Graf <agraf@suse.de>

 *    Paul Mackerras <paulus@samba.org>

 *

 * Description:

 *

 * Hypercall handling for running PAPR guests in PR KVM on Book 3S

 * processors.

 bytes per HPT entry */

 Request defs for kvmppc_h_pr_bulk_remove() */

 Exit success */

 Exit fail */

 Exit fail */

 tsl = AVPN */

 Splat the pteg in (userland) hpt */

 CONFIG_SPAPR_TCE_IOMMU */

 CONFIG_SPAPR_TCE_IOMMU */

/*

 * List of hcall numbers to enable by default.

 * For compatibility with old userspace, we enable by default

 * all hcalls that were implemented before the hcall-enabling

 * facility was added.  Note this list should not include H_RTAS.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright IBM Corp. 2007

 * Copyright 2010-2011 Freescale Semiconductor, Inc.

 *

 * Authors: Hollis Blanchard <hollisb@us.ibm.com>

 *          Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>

 *          Scott Wood <scottwood@freescale.com>

 *          Varun Sethi <varun.sethi@freescale.com>

 TODO: use vcpu_printf() */

/*

 * Load up guest vcpu FP state if it's needed.

 * It also set the MSR_FP in thread so that host know

 * we're holding FPU, and then host can help to save

 * guest vcpu FP state if other threads require to use FPU.

 * This simulates an FP unavailable fault.

 *

 * It requires to be called with preemption disabled.

/*

 * Save guest vcpu FP state into thread.

 * It requires to be called with preemption disabled.

	/* We always treat the FP bit as enabled from the host

/*

 * Simulate AltiVec unavailable fault to load guest state

 * from thread to AltiVec unit.

 * It requires to be called with preemption disabled.

/*

 * Save guest vcpu AltiVec state into thread.

 * It requires to be called with preemption disabled.

 Synchronize guest's desire to get debug interrupts into shadow MSR */

 Force enable debug interrupts when user space wants to debug */

		/*

		 * Since there is no shadow MSR, sync MSR_DE into the guest

		 * visible MSR.

/*

 * Helper function for "full" MSR writes.  No need to call this if only

 * EE/CE/ME/DE/RI are changing.

 Deliver the interrupt of the corresponding priority, if possible. */

 Truncate crit indicators in 32 bit mode */

 Critical section when crit == r1 */

 ... and we're in supervisor mode */

	/*

	 * If an interrupt is pending but masked, raise a guest doorbell

	 * so that we are notified when the guest enables the relevant

	 * MSR bit.

/*

 * Return the number of jiffies until the next timeout.  If the timeout is

 * longer than the NEXT_TIMER_MAX_DELTA, then return NEXT_TIMER_MAX_DELTA

 * because the larger value can break the timer APIs.

	/*

	 * The watchdog timeout will hapeen when TB bit corresponding

	 * to watchdog will toggle from 0 to 1.

 Convert timebase ticks to jiffies */

	/*

	 * If TSR_ENW and TSR_WIS are not set then no need to exit to

	 * userspace, so clear the KVM_REQ_WATCHDOG request.

	/*

	 * If the number of jiffies of watchdog timer >= NEXT_TIMER_MAX_DELTA

	 * then do not run the watchdog timer as this can break timer APIs.

 Time out event */

	/*

	 * If this is final watchdog expiry and some action is required

	 * then exit to userspace.

	/*

	 * Stop running the watchdog timer after final expiration to

	 * prevent the host from being flooded with timers if the

	 * guest sets a short period.

	 * Timers will resume when TSR/TCR is updated next time.

 Tell the guest about our interrupt status */

 Check pending exceptions and deliver one, if possible. */

 Exception delivery raised request; start over */

 Indicate we want to get back into the guest */

 interrupts now hard-disabled */

 Save userspace FPU state in stack */

	/*

	 * Since we can't trap on MSR_FP in GS-mode, we consider the guest

	 * as always using the FPU.

 Save userspace AltiVec state in stack */

	/*

	 * Since we can't trap on MSR_VEC in GS-mode, we consider the guest

	 * as always using the AltiVec.

 Switch to guest debug context */

	/* No need for guest_exit. It's done in handle_exit.

 Switch back to user space debug context */

 don't overwrite subtypes, just account kvm_stats */

		/* Future optimization: only reload non-volatiles if

		/* For debugging, encode the failing instruction and

		/*

		 * Debug resources belong to Guest.

		 * Imprecise debug event is not injected

 Inject a program interrupt if trap debug is not allowed */

	/*

	 * Debug resource owned by userspace.

	 * Clear guest dbsr (vcpu->arch.dbsr)

/*

 * For interrupts needed to be handled by host interrupt handlers,

 * corresponding host handler are called from here in similar way

 * (but not exact) as they are called from low level handler

 * (such as from arch/powerpc/kernel/head_fsl_booke.S).

 FIXME */

 Save DBSR before preemption is enabled */

		/* For debugging, encode the failing instruction and

/**

 * kvmppc_handle_exit

 *

 * Return value is in the form (errcode<<2 | RESUME_FLAG_HOST | RESUME_FLAG_NV)

 update before a new last_exit_type is rewritten */

 restart interrupts if they were meant for the host */

	/*

	 * get last instruction before being preempted

	 * TODO: for e6500 check also BOOKE_INTERRUPT_LRAT_ERROR & ESR_DATA

 SW breakpoints arrive as illegal instructions on HV */

		/*

		 * Service IRQs here before vtime_account_guest_exit() so any

		 * ticks that occurred while running the guest are accounted to

		 * the guest. If vtime accounting is enabled, accounting uses

		 * TB rather than ticks, so it can be done without enabling

		 * interrupts here, which has the problem that it accounts

		 * interrupt processing overhead to the host.

 For debugging, send invalid exit reason to user space */

		/*

		 * We are here because there is a pending guest interrupt

		 * which could not be delivered as MSR_CE or MSR_ME was not

		 * set.  Once we break from here we will retry delivery.

		/*

		 * We are here because there is a pending guest interrupt

		 * which could not be delivered as MSR_EE was not set.  Once

		 * we break from here we will retry delivery.

			/*

			 * We are here because of an SW breakpoint instr,

			 * so lets return to host to handle.

			/*

			 * Program traps generated by user-level software must

			 * be handled by the guest kernel.

			 *

			 * In GS mode, hypervisor privileged instructions trap

			 * on BOOKE_INTERRUPT_HV_PRIV, not here, so these are

			 * actual program interrupts, handled by the guest.

		/*

		 * Guest wants SPE, but host kernel doesn't support it.  Send

		 * an "unimplemented operation" program check to the guest.

	/*

	 * These really should never happen without CONFIG_SPE,

	 * as we should never enable the real MSR[SPE] in the guest.

 CONFIG_SPE_POSSIBLE */

/*

 * On cores with Vector category, KVM is loaded only if CONFIG_ALTIVEC,

 * see kvmppc_core_check_processor_compat().

			/*

			 * hcall from guest userspace -- send privileged

			 * instruction program check.

 KVM PV hypercalls */

 Guest syscalls */

 Check the guest TLB. */

 The guest didn't have a mapping for it. */

			/* The guest TLB had a mapping, but the shadow TLB

			 * didn't, and it is RAM. This could be because:

			 * a) the entry is mapping the host kernel, or

			 * b) the guest used a large mapping which we're faking

			 * Either way, we need to satisfy the fault without

			/* Guest has mapped and accessed a page which is not

 Check the guest TLB. */

 The guest didn't have a mapping for it. */

			/* The guest TLB had a mapping, but the shadow TLB

			 * didn't. This could be because:

			 * a) the entry is mapping the host kernel, or

			 * b) the guest used a large mapping which we're faking

			 * Either way, we need to satisfy the fault without

 Guest mapped and leaped at non-RAM! */

	/*

	 * To avoid clobbering exit_reason, only check for signals if we

	 * aren't already exiting to userspace for some other reason.

 interrupts now hard-disabled */

 setup watchdog timer once */

	/*

	 * Clear DBSR.MRR to avoid guest debug interrupt as

	 * this is of host interest

	/*

	 * We may have stopped the watchdog due to

	 * being stuck on final expiration.

 XXX: Add similar MSR protection for BookE-PR */

 Check the guest TLB. */

 Do we have a TLB entry at all? */

 XXX read permissions from the guest TLB */

 Code below handles only HW breakpoints */

	/*

	 * On BookE-HV (e500mc) the guest is always executed with MSR.GS=1

	 * DBCR1 and DBCR2 are set to trigger debug events when MSR.PR is 0

	/*

	 * On BookE-PR (e500v2) the guest is always executed with MSR.PR=1

	 * We set DBCR1 and DBCR2 to only trigger debug events when MSR.PR

	 * is set.

 Setting H/W breakpoint */

 Setting H/W watchpoint */

 Clear pending debug event in DBSR */

 Initial guest state: 16MB mapping 0 -> 0, PC = 0, MSR = 0, R1 = 16MB */

 -8 for the callee-save LR slot */

	/* Eye-catching numbers so we know if the guest takes an interrupt

	/* We install our own exception handlers by hijacking IVPR. IVPR must

 XXX make sure our handlers are smaller than Linux's */

	/* Copy our interrupt handlers to match host IVORs. That way we don't

 !BOOKE_HV */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2008-2013 Freescale Semiconductor, Inc. All rights reserved.

 *

 * Author: Yu Liu, yu.liu@freescale.com

 *         Scott Wood, scottwood@freescale.com

 *         Ashish Kalra, ashish.kalra@freescale.com

 *         Varun Sethi, varun.sethi@freescale.com

 *         Alexander Graf, agraf@suse.de

 *

 * Description:

 * This file is based on arch/powerpc/kvm/44x_tlb.c,

 * by Hollis Blanchard <hollisb@us.ibm.com>.

 Search the guest TLB for a matching entry. */

 since we only have two TLBs, only lower bit is used. */

 This function is supposed to be called for a adding a new valid tlb entry */

 Invalidate all host shadow mappings */

 since we only have two TLBs, only lower bit is used. */

 invalidate all entries */

 Invalidate all host shadow mappings */

 invalidate all entries */

 since we only have two TLBs, only lower bit is used. */

		/*

		 * If a valid tlb1 entry is overwritten then recalculate the

		 * min/max TLB1 map address range otherwise no need to look

		 * in tlb1 array.

 Invalidate shadow mappings for the about-to-be-clobbered TLBE. */

 Premap the faulting page */

 'linear_address' is actually an encoding of AS|PID|EADDR . */

 XXX what does "writeable" and "usermode" even mean? */

****************************************/

 Only allow MMU registers to be set to the config supported by KVM */

 MMU geometry (N_ENTRY/ASSOC) can be set only using SW_TLB */

 Update vcpu's MMU geometry based on SW_TLB input */

 Vcpu's MMU default configuration */

 Initialize RASIZE, PIDSIZE, NTLBS and MAVN fields with host values*/

 Initialize TLBnCFG fields with host values and SW_TLB geometry*/

 Guest mmu emulation currently doesn't handle E.PT */

 SPDX-License-Identifier: GPL-2.0

 XXX */

#define DBG(fmt...) udbg_printf(fmt)

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2017 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

/*

 * This handles the cases where the guest is in real suspend mode

 * and we want to get back to the guest without dooming the transaction.

 * The caller has checked that the guest is in real-suspend mode

 * (MSR[TS] = S and the fake-suspend flag is not set).

	/*

	 * rfid, rfebb, and mtmsrd encode bit 31 = 0 since it's a reserved bit

	 * in these instructions, so masking bit 31 out doesn't change these

	 * instructions. For the tsr. instruction if bit 31 = 0 then it is per

	 * ISA an invalid form, however P9 UM, in section 4.6.10 Book II Invalid

	 * Forms, informs specifically that ignoring bit 31 is an acceptable way

	 * to handle TM-related invalid forms that have bit 31 = 0. Moreover,

	 * for emulation purposes both forms (w/ and wo/ bit 31 set) can

	 * generate a softpatch interrupt. Hence both forms are handled below

	 * for tsr. to make them behave the same way.

 XXX do we need to check for PR=0 here? */

 should only get here for Sx -> T1 transition */

 check for PR=1 and arch 2.06 bit set in PCR */

 check EBB facility is available */

 expect to see a S->T transition requested */

 XXX do we need to check for PR=0 here? */

 check this is a Sx -> T1 transition */

 mtmsrd doesn't change LE */

 ignore bit 31, see comment above */

 we know the MSR has the TS field = S (0b01) here */

 check for PR=1 and arch 2.06 bit set in PCR */

 check for TM disabled in the HFSCR or MSR */

 L=1 => tresume => set TS to T (0b10) */

 Set CR0 to 0b0010 */

/*

 * This is called when we are returning to a guest in TM transactional

 * state.  We roll the guest state back to the checkpointed state.

 go to N state */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2009. SUSE Linux Products GmbH. All rights reserved.

 *

 * Authors:

 *    Alexander Graf <agraf@suse.de>

 *    Kevin Wolf <mail@kevin-wolf.de>

 *

 * Description:

 * This file is derived from arch/powerpc/kvm/44x.c,

 * by Hollis Blanchard <hollisb@us.ibm.com>.

 #define EXIT_DEBUG */

 Truncate crit indicators in 32 bit mode */

 Critical section when crit == r1 */

 ... and we're in supervisor mode */

 might as well deliver this straight away */

 might as well deliver this straight away */

 might as well deliver this straight away */

 might as well deliver this straight away */

 might as well deliver this straight away */

	/*

	 * This case (KVM_INTERRUPT_SET) should never actually arise for

	 * a pseries guest (because pseries guests expect their interrupt

	 * controllers to continue asserting an external interrupt request

	 * until it is acknowledged at the interrupt controller), but is

	 * included to avoid ABI breakage and potentially for other

	 * sorts of guest.

	 *

	 * There is a subtlety here: HV KVM does not test the

	 * external_oneshot flag in the code that synthesizes

	 * external interrupts for the guest just before entering

	 * the guest.  That is OK even if userspace did do a

	 * KVM_INTERRUPT_SET on a pseries guest vcpu, because the

	 * caller (kvm_vcpu_ioctl_interrupt) does a kvm_vcpu_kick()

	 * which ends up doing a smp_send_reschedule(), which will

	 * pull the guest all the way out to the host, meaning that

	 * we will call kvmppc_core_prepare_to_enter() before entering

	 * the guest again, and that will handle the external_oneshot

	 * flag correctly.

/*

 * This function determines if an irqprio should be cleared once issued.

 DEC interrupts get cleared by mtdec */

			/*

			 * External interrupts get cleared by userspace

			 * except when set by the KVM_INTERRUPT ioctl with

			 * KVM_INTERRUPT_SET (not KVM_INTERRUPT_SET_LEVEL).

 Tell the guest about our interrupt status */

 Magic page override */

 CONFIG_VSX */

 CONFIG_KVM_XICS */

 CONFIG_KVM_XIVE */

 CONFIG_VSX */

 CONFIG_KVM_XICS */

 CONFIG_KVM_XIVE */

	/*

	 * Free the XIVE and XICS devices which are not directly freed by the

	 * device 'release' method

 CONFIG_KVM_XICS */

	/*

	 * We always return 0 for book3s. We check

	 * for compatibility while loading the HV

	 * or PR module

 CONFIG_KVM_XICS */

 On 32bit this is our one and only kernel module */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2017 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

 Preserve ROT and TL fields of existing TEXASR */

/*

 * This gets called on a softpatch interrupt on POWER9 DD2.2 processors.

 * We expect to find a TM-related instruction to be emulated.  The

 * instruction image is in vcpu->arch.emul_inst.  If the guest was in

 * TM suspended or transactional state, the checkpointed state has been

 * reclaimed and is in the vcpu struct.  The CPU is in virtual mode in

 * host context.

	/*

	 * The TM softpatch interrupt sets NIP to the instruction following

	 * the faulting instruction, which is not executed. Rewind nip to the

	 * faulting instruction so it looks like a normal synchronous

	 * interrupt, then update nip in the places where the instruction is

	 * emulated.

	/*

	 * rfid, rfebb, and mtmsrd encode bit 31 = 0 since it's a reserved bit

	 * in these instructions, so masking bit 31 out doesn't change these

	 * instructions. For treclaim., tsr., and trechkpt. instructions if bit

	 * 31 = 0 then they are per ISA invalid forms, however P9 UM, in section

	 * 4.6.10 Book II Invalid Forms, informs specifically that ignoring bit

	 * 31 is an acceptable way to handle these invalid forms that have

	 * bit 31 = 0. Moreover, for emulation purposes both forms (w/ and wo/

	 * bit 31 set) can generate a softpatch interrupt. Hence both forms

	 * are handled below for these instructions so they behave the same way.

 XXX do we need to check for PR=0 here? */

 should only get here for Sx -> T1 transition */

 generate an illegal instruction interrupt */

 check EBB facility is available */

 rerun host interrupt handler */

 generate a facility unavailable interrupt */

 expect to see a S->T transition requested */

 XXX do we need to check for PR=0 here? */

 check this is a Sx -> T1 transition */

 mtmsrd doesn't change LE */

 ignore bit 31, see comment above */

 check for PR=1 and arch 2.06 bit set in PCR */

 generate an illegal instruction interrupt */

 check for TM disabled in the HFSCR or MSR */

 rerun host interrupt handler */

 generate a facility unavailable interrupt */

 Set CR0 to indicate previous transactional state */

 L=1 => tresume, L=0 => tsuspend */

 ignore bit 31, see comment above */

 check for TM disabled in the HFSCR or MSR */

 rerun host interrupt handler */

 generate a facility unavailable interrupt */

 If no transaction active, generate TM bad thing */

 If failure was not previously recorded, recompute TEXASR */

 Set CR0 to indicate previous transactional state */

 ignore bit 31, see comment above */

 XXX do we need to check for PR=0 here? */

 check for TM disabled in the HFSCR or MSR */

 rerun host interrupt handler */

 generate a facility unavailable interrupt */

 If transaction active or TEXASR[FS] = 0, bad thing */

 Set CR0 to indicate previous transactional state */

 What should we do here? We didn't recognize the instruction */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright IBM Corporation, 2018

 * Authors Suraj Jitindar Singh <sjitindarsingh@gmail.com>

 *	   Paul Mackerras <paulus@ozlabs.org>

 *

 * Description: KVM functions specific to running nested KVM-HV guests

 * on Book3S processors (specifically POWER9 and later).

bugs.llvm.org/show_bug.cgi?id=49610 */

 No need to reflect the page fault to L1, we've handled it */

	/*

	 * Since the L2 gprs have already been written back into L1 memory when

	 * we complete the mmio, store the L1 memory location of the L2 gpr

	 * being loaded into by the mmio so that the loaded value can be

	 * written there in kvmppc_complete_mmio_load()

	/*

	 * Don't let L1 change LPCR bits for the L2 except these:

	/*

	 * Additional filtering is required depending on hardware

	 * and configuration.

	/*

	 * Don't let L1 enable features for L2 which we don't allow for L1,

	 * but preserve the interrupt cause field.

 Don't let data address watchpoint match in hypervisor state */

 Don't let completed instruction address breakpt match in HV state */

 copy parameters in */

	/*

	 * L1 must have set up a suspended state to enter the L2 in a

	 * transactional state, and only in that case. These have to be

	 * filtered out here to prevent causing a TM Bad Thing in the

	 * host HRFID. We could synthesize a TM Bad Thing back to the L1

	 * here but there doesn't seem like much point.

 translate lpid */

 save l1 values of things */

 convert TB values/offsets to host (L0) values */

 set L1 state to L2 state */

 Guest must always run with ME enabled, HV disabled. */

 save L2 state for return */

 restore L1 state */

 set L1 MSR TS field according to L2 transaction state */

 copy l2_hv_state and regs back to guest */

 find log base 2 of KVMPPC_NR_LPIDS, rounding up */

	/*

	 * N.B. the kvmhv_on_pseries() test is there because it enables

	 * the compiler to remove the call to plpar_hcall_norets()

	 * when CONFIG_PPC_PSERIES=n.

 L0 will do the necessary barriers */

/*

 * Handle the H_SET_PARTITION_TABLE hcall.

 * r4 = guest real address of partition table + log_2(size) - 12

 * (formatted as for the PTCR).

	/*

	 * Limit the partition table to 4096 entries (because that's what

	 * hardware supports), and check the base address.

/*

 * Handle the H_COPY_TOFROM_GUEST hcall.

 * r4 = L1 lpid of nested guest

 * r5 = pid

 * r6 = eaddr to access

 * r7 = to buffer (L1 gpa)

 * r8 = from buffer (L1 gpa)

 * r9 = n bytes to copy

 One must be NULL to determine the direction */

 Load from the nested guest into our buffer */

 Write what was loaded into our buffer back to the L1 guest */

 Load the data to be stored from the L1 guest into our buf */

 Store from our buffer into the nested guest */

/*

 * Reload the partition table entry for a guest.

 * Caller must hold gp->tlb_lock.

/*

 * Free up any resources allocated for a nested guest.

		/*

		 * No vcpu is using this struct and no call to

		 * kvmhv_get_nested can find this struct,

		 * so we don't need to hold kvm->mmu_lock.

/*

 * Free up all nested resources allocated for this guest.

 * This is called with no vcpus of the guest running, when

 * switching the guest to HPT mode or when destroying the

 * guest.

 caller must hold gp->tlb_lock */

 someone else beat us to it */

 Are there any existing entries? */

 No -> use the rmap as a single entry */

 Do any entries match what we're trying to insert? */

 Do we need to create a list or just add the new entry? */

 Not previously a list */

 Not previously a list */

 Set NULL so not freed by caller */

 Find the pte */

	/*

	 * If the pte is present and the pfn is still the same, update the pte.

	 * If the pfn has changed then this is a stale rmap entry, the nested

	 * gpa actually points somewhere else now, and there is nothing to do.

	 * XXX A future optimisation would be to remove the rmap entry here.

/*

 * For a given list of rmap entries, update the rc bits in all ptes in shadow

 * page tables for nested guests which are referenced by the rmap list.

 Find and invalidate the pte */

 Don't spuriously invalidate ptes if the pfn has changed */

 called with kvm->mmu_lock held */

 Invalid ap encoding */

 No such guest -> nothing to do */

 There may be more than one host page backing this single guest pte */

 Invalidate TLB */

		/*

		 * Invalidate PWC

		 * We don't cache this -> nothing to do

 Invalidate TLB, PWC and caching of partition table entries */

	/*

	 * These cases are invalid and are not handled:

	 * r   != 1 -> Only radix supported

	 * prs == 1 -> Not HV privileged

	 * ric == 3 -> No cluster bombs for radix

	 * is  == 1 -> Partition scoped translations not associated with pid

	 * (!is) && (ric == 1 || ric == 2) -> Not supported by ISA

		/*

		 * We know ric == 0

		 * Invalidate TLB for a given target address

 Invalidate matching LPID */

 Invalidate ALL LPIDs */

/*

 * This handles the H_TLB_INVALIDATE hcall.

 * Parameters are (r4) tlbie instruction code, (r5) rS contents,

 * (r6) rB contents.

/*

 * Number of pages above which we invalidate the entire LPID rather than

 * flush individual pages.

/*

 * Performs partition-scoped invalidations for nested guests

 * as part of H_RPT_INVALIDATE hcall.

	/*

	 * If L2 lpid isn't valid, we need to return H_PARAMETER.

	 *

	 * However, nested KVM issues a L2 lpid flush call when creating

	 * partition table entries for L2. This happens even before the

	 * corresponding shadow lpid is created in HV which happens in

	 * H_ENTER_NESTED call. Since we can't differentiate this case from

	 * the invalid case, we ignore such flush requests and return success.

	/*

	 * A flush all request can be handled by a full lpid flush only.

	/*

	 * We don't need to handle a PWC flush like process table here,

	 * because intermediate partition scoped table in nested guest doesn't

	 * really have PWC. Only level we have PWC is in L0 and for nested

	 * invalidate at L0 we always do kvm_flush_lpid() which does

	 * radix__flush_all_lpid(). For range invalidate at any level, we

	 * are not removing the higher level page tables and hence there is

	 * no PWC invalidate needed.

	 *

	 * if (type & H_RPTI_TYPE_PWC) {

	 *	ret = do_tlb_invalidate_nested_all(vcpu, lpid, RIC_FLUSH_PWC);

	 *	if (ret)

	 *		return H_P4;

	 * }

 Used to convert a nested guest real address to a L1 guest real address */

 We didn't find a pte */

 Unsupported mmu config */

 No translation found */

 Couldn't access L1 real address */

 Unknown error */

 We found a pte -> check permissions */

 Can we write? */

 Can we execute? */

 Can we read? */

 Are the rc bits set in the L1 partition scoped pte? */

 Set the rc bit in the pte of our (L0) pgtable for the L1 guest */

 Set the rc bit in the pte of the shadow_pgtable for the nest guest */

 called with gp->tlb_lock held */

 Convert the nested guest real address into a L1 guest real address */

	/*

	 * If the hardware found a translation but we don't now have a usable

	 * translation in the l1 partition-scoped tree, remove the shadow pte

	 * and let the guest retry.

 Failed to set the reference/change bits */

	/*

	 * We took an HISI or HDSI while we were running a nested guest which

	 * means we have no partition scoped translation for that. This means

	 * we need to insert a pte for the mapping into our shadow_pgtable.

 We don't support l1 using a page size smaller than our own */

 1. Get the corresponding host memslot */

 unusual error -> reflect to the guest as a DSI */

 passthrough of emulated MMIO case */

 Give the guest a DSI */

 2. Find the host pte for this L1 guest real address */

 Used to check for invalidations in progress */

 See if can find translation in our partition scoped tables for L1 */

 No suitable pte found -> try to insert a mapping */

 Align gfn to the start of the page */

 3. Compute the pte we need to insert for nest_gpa -> host r_addr */

 The permissions is the combination of the host and l1 guest ptes */

 Only set accessed/dirty (rc) bits if set in host and l1 guest ptes */

 What size pte can we insert? */

 4. Insert the pte into our shadow_pgtable */

 Let the guest try again */

 Let the guest try again */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright 2010-2011 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

 Translate address of a vmalloc'd thing to a linear map address */

 Return 1 if we need to do a global tlbie, 0 if we can use tlbiel */

	/*

	 * If there is only one vcore, and it's currently running,

	 * as indicated by local_paca->kvm_hstate.kvm_vcpu being set,

	 * we can use tlbiel as long as we mark all other physical

	 * cores as potentially having stale TLB entries for this lpid.

	 * Otherwise, don't use tlbiel.

 LPID has been switched to host if in virt mode so can't do local */

 any other core might now have stale TLB entries... */

		/*

		 * On POWER9, threads are independent but the TLB is shared,

		 * so use the bit for the first thread to represent the core.

/*

 * Add this HPTE into the chain for the real page.

 * Must be called with the chain locked; it unlocks the chain.

 Update the dirty bitmap of a memslot */

 Returns a pointer to the revmap entry for the page mapped by a HPTE */

 Remove this HPTE from the chain for a real page */

	/*

	 * The HPTE gets used by compute_tlbie_rb() to set TLBIE bits, so

	 * these functions should work together -- must ensure a guest can not

	 * cause problems with the TLBIE that KVM executes.

 B=0b1x is a reserved value, disallow it. */

 used later to detect if we might have been invalidated */

 Find the memslot (if any) for this address */

 Emulated MMIO - mark this with key=31 */

 Check if the requested page fits entirely in the memslot. */

 Translate to host virtual address */

		/*

		 * We should always find the guest page size

		 * to <= host page size, if host is using hugepage

 make the actual HPTE be read-only */

If we had host pte mapping then  Check WIMG */

		/*

		 * Allow guest to map emulated device memory as

		 * uncacheable, but actually make it cacheable.

 Find and lock the HPTEG slot to use */

			/*

			 * Since try_lock_hpte doesn't retry (not even stdcx.

			 * failures), it could be that there is a free slot

			 * but we transiently failed to lock it.  Try again,

			 * actually locking each slot and checking it.

 Lock the slot and check again */

 Save away the guest's idea of the second HPTE dword */

 Link HPTE into reverse-map chain */

 Check for pending invalidations under the rmap chain lock */

 inval in progress, write a non-present HPTE */

 Only set R/C in real HPTE if already set in *rmap */

 Convert to new format on P9 */

 Write the first HPTE dword, unlocking the HPTE and making it valid */

 Radix flush for a hash guest */

 IS = 2 */

 lpid = 0 */

 partition scoped */

 radix format */

 RIC_FLSUH_TLB */

		/*

		 * Need the extra ptesync to make sure we don't

		 * re-order the tlbie

	/*

	 * We use the POWER9 5-operand versions of tlbie and tlbiel here.

	 * Since we are using RIC=0 PRS=0 R=0, and P7/P8 tlbiel ignores

	 * the RS field, this is backwards-compatible with P7 and P8.

		/*

		 * The reference (R) and change (C) bits in a HPT

		 * entry can be set by hardware at any time up until

		 * the HPTE is invalidated and the TLB invalidation

		 * sequence has completed.  This means that when

		 * removing a HPTE, we need to re-read the HPTE after

		 * the invalidation sequence has completed in order to

		 * obtain reliable values of R and C.

 no more requests */

 parameter error */

 to avoid deadlock, don't spin except for first */

 absolute */

 andcond */

 AVPN */

 insert R and C bits from PTE */

 leave it locked */

 Now that we've collected a batch, do the tlbies */

 Read PTE low words after tlbie to get final R/C values */

 Update guest view of 2nd HPTE dword */

 Update HPTE */

		/*

		 * If the page is valid, don't let it transition from

		 * readonly to writable.  If it should be writable, we'll

		 * take a trap and let the page fault code sort it out.

 If the PTE is changing, invalidate it first */

 Don't lose R/C bit updates done by hardware */

 need to make it temporarily absent so C is stable */

 Find the memslot for this address */

 Translate to host virtual address */

 Try to find the host pte for that virtual address */

 Convert to a physical address */

 Used later to detect if we might have been invalidated */

 Zero the page */

 Used later to detect if we might have been invalidated */

 Copy the page */

 4K page size */

 Don't handle radix mode here, go up to the virtual mode handler */

 Check for invalid flags (H_PAGE_SET_LOANED covers all CMO flags) */

 dest (and src if copy_page flag set) must be page aligned */

 zero and/or copy the page as determined by the flags */

 We can ignore the other flags */

 modify only the second-last byte, which contains the ref bit */

 16M */

 64k */

 16G */

 1M, unsupported */

/* When called from virtmode, this func should be protected by

 * preempt_disable(), otherwise, the holding of HPTE_V_HVLOCK

 * can trigger deadlock issue.

 Get page shift, work out hash and AVPN etc. */

 also includes B */

 Read the PTE racily */

 Check valid/absent, hash, segment size and AVPN */

 Lock the PTE and read it under the lock */

			/*

			 * Check the HPTE again, including base page size

 Return with the HPTE still locked */

/*

 * Called in real mode to check whether an HPTE not found fault

 * is due to accessing a paged-out page or an emulated MMIO page,

 * or if a protection fault is due to accessing a page that the

 * guest wanted read/write access to but which we made read-only.

 * Returns a possibly modified status (DSISR) value if not

 * (i.e. pass the interrupt to the guest),

 * -1 to pass the fault up to host kernel mode code, -2 to do that

 * and also load the instruction word (for MMIO emulation),

 * or 0 if we should make the guest retry the access.

 For protection fault, expect to find a valid HPTE */

 there really was no HPTE */

 for prot fault, HPTE disappeared */

 For not found, if the HPTE is valid by now, retry the instruction */

 Check access permissions to the page */

 DSISR_NOHPTE == SRR1_ISI_NOPT */

 check write permission */

 Check storage key, if applicable */

 Save HPTE info for virtual-mode handler */

 Check the storage key to see if it is possibly emulated MMIO */

 MMIO emulation - load instr word */

 send fault up to host kernel mode */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2012 Michael Ellerman, IBM Corporation.

 CONFIG_KVM_XICS */

 It's not an error to undefine an undefined token */

	/*

	 * r4 contains the guest physical address of the RTAS args

	 * Mask off the top 4 bits since this is a guest real address

	/*

	 * args->rets is a pointer into args->args. Now that we've

	 * copied args we need to fix it up to point into our copy,

	 * not the guest args. We also need to save the original

	 * value so we can restore it on the way out.

		/*

		 * Don't overflow our args array: ensure there is room for

		 * at least rets[0] (even if the call specifies 0 nret).

		 *

		 * Each handler must then check for the correct nargs and nret

		 * values, but they may always return failure in rets[0].

	/*

	 * We only get here if the guest has called RTAS with a bogus

	 * args pointer or nargs/nret values that would overflow the

	 * array. That means we can't get to the args, and so we can't

	 * fail the RTAS call. So fail right out to userspace, which

	 * should kill the guest.

	 *

	 * SLOF should actually pass the hcall return value from the

	 * rtas handler call in r3, so enter_rtas could be modified to

	 * return a failure indication in r3 and we could return such

	 * errors to the guest rather than failing to host userspace.

	 * However old guests that don't test for failure could then

	 * continue silently after errors, so for now we won't do this.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright SUSE Linux Products GmbH 2009

 *

 * Authors: Alexander Graf <agraf@suse.de>

 #define DEBUG_MMU */

 #define DEBUG_MMU_PTE */

 #define DEBUG_MMU_PTE_IP 0xfff14c40 */

	/* Update PTE C and A bits, so the guest's swapper knows we used the

		/*

		 * Use single-byte writes to update the HPTE, to

		 * conform to what real hardware does.

 Magic page override */

 flush this VA on all cpus */

	/* In case we only have one of MSR_IR or MSR_DR set, let's put

	   that in the real-mode context (and hope RM doesn't access

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright 2016 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

/*

 * Supported radix tree geometry.

 * Like p9, we support either 5 or 9 bits at the first (lowest) level,

 * for a page size of 64k or 4k.

 Can't access quadrants 1 or 2 in non-HV mode, call the HV to do it */

 switch the lpid first to avoid running host with unallocated pid */

 switch the pid first to avoid running host with unallocated pid */

 This would cause a data segment intr so don't allow the access */

 Should we be using the nested lpid */

 If accessing quadrant 3 then pid is expected to be 0 */

 Current implementations only support 52-bit space */

 Walk each level of the radix tree */

 Check a valid size */

 Check that low bits of page table base are zero */

 Read the entry from guest memory */

 Check if a leaf entry */

 Get ready to walk the next level */

 Need a leaf at lowest level; 512GB pages not supported */

 We found a valid leaf PTE */

 Offset is now log base 2 of the page size */

 Work out permissions */

/*

 * Used to walk a partition or process table radix tree in guest memory

 * Note: We exploit the fact that a partition table and a process

 * table have the same layout, a partition-scoped page table and a

 * process-scoped page table have the same layout, and the 2nd

 * doubleword of a partition table entry has the same layout as

 * the PTCR register.

 Is the table big enough to contain this entry? */

 Read the table to find the root of the radix tree */

 Root is stored in the first double word */

 Work out effective PID */

 Check privilege (applies only to process scoped translations) */

 Check AMR/IAMR to see if strict mode is in force */

 pmd_populate() will only reference _pa(pte). */

 pud_populate() will only reference _pa(pmd). */

 Called with kvm->mmu_lock held */

 The following only applies to L1 entries */

 1GB or 2MB page */

/*

 * kvmppc_free_p?d are used to free existing page tables, and recursively

 * descend and clear and free children.

 * Callers are responsible for flushing the PWC.

 *

 * When page tables are being unmapped/freed as part of page fault path

 * (full == false), valid ptes are generally not expected; however, there

 * is one situation where they arise, which is when dirty page logging is

 * turned off for a memslot while the VM is running.  The new memslot

 * becomes visible to page faults before the memslot commit function

 * gets to flush the memslot, which can lead to a 2MB page mapping being

 * installed for a guest physical address where there are already 64kB

 * (or 4kB) mappings (of sub-pages of the same 2MB page).

	/*

	 * Clearing the pmd entry then flushing the PWC ensures that the pte

	 * page no longer be cached by the MMU, so can be freed without

	 * flushing the PWC again.

	/*

	 * Clearing the pud entry then flushing the PWC ensures that the pmd

	 * page and any children pte pages will no longer be cached by the MMU,

	 * so can be freed without flushing the PWC again.

/*

 * There are a number of bits which may differ between different faults to

 * the same partition scope entry. RC bits, in the course of cleaning and

 * aging. And the write bit can change, either the access could have been

 * upgraded, or a read fault could happen concurrently with a write fault

 * that sets those bits first.

 Traverse the guest's 2nd-level tree, allocate new levels needed */

 Check if we might have been invalidated; let the guest retry if so */

 Now traverse again under the lock and change the tree */

 Check if we raced and someone else has set the same thing */

 Valid 1GB page here already, add our extra bits */

		/*

		 * If we raced with another CPU which has just put

		 * a 1GB pte in after we saw a pmd page, try again.

 Valid 1GB page here already, remove it */

			/*

			 * There's a page table page here, but we wanted to

			 * install a large page, so remove and free the page

			 * table page.

 Check if we raced and someone else has set the same thing */

 Valid 2MB page here already, add our extra bits */

		/*

		 * If we raced with another CPU which has just put

		 * a 2MB pte in after we saw a pte page, try again.

 Valid 2MB page here already, remove it */

			/*

			 * There's a page table page here, but we wanted to

			 * install a large page, so remove and free the page

			 * table page.

 Check if someone else set the same thing */

 Valid page here already, add our extra bits */

	/*

	 * Need to set an R or C bit in the 2nd-level tables;

	 * since we are just helping out the hardware here,

	 * it is sufficient to do what the hardware does.

 used to check for invalidations in progress */

	/*

	 * Do a fast check first, since __gfn_to_pfn_memslot doesn't

	 * do it with !atomic && !async, which is how we call it.

	 * We always ask for write permission since the common case

	 * is that the page is writable.

 Call KVM generic code to do the slow-path check */

	/*

	 * Read the PTE from the process' radix tree and use that

	 * so we get the shift and attribute bits.

	/*

	 * If the PTE disappeared temporarily due to a THP

	 * collapse, just return and let the guest try again.

 If we're logging dirty pages, always map single pages */

 Get pte level from shift/size */

			/*

			 * If the pte maps more than one page, bring over

			 * bits from the virtual address to get the real

			 * address of the specific single page we want.

 Allocate space in the tree and write the PTE */

 Increment number of large pages if we (successfully) inserted one */

 Check for unusual errors */

 Reflect to the guest as DSI */

 Translate the logical address */

 Get the corresponding memslot */

 No memslot means it's an emulated MMIO region */

			/*

			 * Bad address in guest page table tree, or other

			 * unusual error - reflect it to the guest as DSI.

 give the guest a DSI */

 Failed to set the reference/change bits */

 Try to insert a pte */

 Called with kvm->mmu_lock held */

 Called with kvm->mmu_lock held */

 XXX need to flush tlb here? */

 Also clear bit in ptes in shadow pgtable for nested guests */

 Called with kvm->mmu_lock held */

 Returns the number of PAGE_SIZE pages that are dirty */

	/*

	 * For performance reasons we don't hold kvm->mmu_lock while walking the

	 * partition scoped table.

		/*

		 * Recheck the pte again

			/*

			 * We have KVM_MEM_LOG_DIRTY_PAGES enabled. Hence we can

			 * only find PAGE_SIZE pte entries here. We can continue

			 * to use the pte addr returned by above page table

			 * walk.

 Also clear bit in ptes in shadow pgtable for nested guests */

		/*

		 * Note that if npages > 0 then i must be a multiple of npages,

		 * since huge pages are only used to back the guest at guest

		 * real addresses that are a multiple of their size.

		 * Since we have at most one PTE covering any given guest

		 * real address, if npages > 1 we can skip to i + npages.

	/*

	 * Increase the mmu notifier sequence number to prevent any page

	 * fault that read the memslot earlier from writing a PTE.

 4k page size */

 64k page size */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright IBM Corp. 2007

 * Copyright 2011 Freescale Semiconductor, Inc.

 *

 * Authors: Hollis Blanchard <hollisb@us.ibm.com>

 mtdec lowers the interrupt line when positive. */

 On BOOKE, DEC = 0 is as good as decrementer not enabled */

	/*

	 * The decrementer ticks at the same rate as the timebase, so

	 * that's how we convert the guest DEC value to the number of

	 * host ticks.

	/*

	 * Guest timebase ticks at the same frequency as host timebase.

	 * So use the host timebase calculations for decrementer emulation.

	/* XXX We need to context-switch the timebase for

 PIR can legally be written, but we ignore it */

	/* Note: mftb and TBRL/TBWL are user-accessible, so

	 * the guest can always access the real TB anyways.

	/* Note: SPRG4-7 are user-readable, so we don't get

/* XXX Should probably auto-generate instruction decoding for a particular core

 this default type might be overwritten by subcategories */

 Attempt core-specific emulation below. */

		/*

		 * Instruction with primary opcode 0. Based on PowerISA

		 * these are illegal instructions.

 Advance past emulated instruction. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2008-2011 Freescale Semiconductor, Inc. All rights reserved.

 *

 * Author: Yu Liu, <yu.liu@freescale.com>

 *

 * Description:

 * This file is derived from arch/powerpc/kvm/44x.c,

 * by Hollis Blanchard <hollisb@us.ibm.com>.

/*

 * This table provide mappings from:

 * (guestAS,guestTID,guestPR) --> ID of physical cpu

 * guestAS	[0..1]

 * guestTID	[0..255]

 * guestPR	[0..1]

 * ID		[1..255]

 * Each vcpu keeps one vcpu_id_table.

/*

 * This table provide reversed mappings of vcpu_id_table:

 * ID --> address of vcpu_id_table item.

 * Each physical core has one pcpu_id_table.

/* This variable keeps last used shadow ID on local core.

/*

 * Allocate a free shadow id and setup a valid sid mapping in given entry.

 * A mapping is only valid when vcpu_id_table and pcpu_id_table are match.

 *

 * The caller must have preemption disabled, and keep it that way until

 * it has finished with the returned shadow id (either written into the

 * TLB or arch.shadow_pid, or discarded).

	/*

	 * If sid == NUM_TIDS, we've run out of sids.  We return -1, and

	 * the caller will invalidate everything and start over.

	 *

	 * sid > NUM_TIDS indicates a race, which we disable preemption to

	 * avoid.

/*

 * Check if given entry contain a valid shadow id mapping.

 * An ID mapping is considered valid only if

 * both vcpu and pcpu know this mapping.

 *

 * The caller must have preemption disabled, and keep it that way until

 * it has finished with the returned shadow id (either written into the

 * TLB or arch.shadow_pid, or discarded).

 Invalidate all id mappings on local core -- call with preempt disabled */

/* Map guest pid to shadow.

 * We use PID to keep shadow of current guest non-zero PID,

 * and use PID1 to keep shadow of guest zero PID.

 Invalidate all mappings on vcpu */

 Update shadow pid when mappings are changed */

 Invalidate one ID mapping on vcpu */

 Update shadow pid when mappings are changed */

/*

 * Map guest (vcpu,AS,ID,PR) to physical core shadow id.

 * This function first lookup if a valid mapping exists,

 * if not, then creates a new one.

 *

 * The caller must have preemption disabled, and keep it that way until

 * it has finished with the returned shadow id (either written into the

 * TLB or arch.shadow_pid, or discarded).

 No mapping yet */

 Update shadow pid when mappings are changed */

 gtlbe must not be mapped by more than one host tlbe */

 One guest ID may be mapped to two shadow IDs */

		/*

		 * The shadow PID can have a valid mapping on at most one

		 * host CPU.  In the common case, it will be valid on this

		 * CPU, in which case we do a local invalidation of the

		 * specific address.

		 *

		 * If the shadow PID is not valid on the current host CPU,

		 * we invalidate the entire shadow PID.

		/*

		 * The guest is invalidating a 4K entry which is in a PID

		 * that has a valid shadow mapping on this host CPU.  We

		 * search host TLB to invalidate it's shadow TLB entry,

		 * similar to __tlbil_va except that we need to look in AS1.

 Recalc shadow pid since MSR changes */

 Shadow PID may be expired on local core */

 Insert large initial mapping for guest. */

 4K map for serial output. Used by kernel wrapper. */

 Registers init */

 Process remaining handlers above the generic first 16 */

 copy extra E500 exception handlers */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2008-2011 Freescale Semiconductor, Inc. All rights reserved.

 *

 * Author: Yu Liu, <yu.liu@freescale.com>

 *

 * Description:

 * This file is derived from arch/powerpc/kvm/44x_emulate.c,

 * by Hollis Blanchard <hollisb@us.ibm.com>.

 Always fail to lock the cache */

 Expose one thread per vcpu */

		/*

		 * Guest relies on host power management configurations

		 * Treat the request as a general store

		/*

		 * If we are here, it means that we have already flushed the

		 * branch predictor, so just return to guest.

 extra exceptions */

		/*

		 * Legacy Linux guests access EPTCFG register even if the E.PT

		 * category is disabled in the VM. Give them a chance to live.

 extra exceptions */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright SUSE Linux Products GmbH 2009

 *

 * Authors: Alexander Graf <agraf@suse.de>

 #define DEBUG_MMU */

	/* When running a PAPR guest, SDR1 contains a HVA address instead

 16 - p */

/*

 * Return page size encoded in the second word of a HPTE, or

 * -1 for an invalid encoding for the base page size indicated by

 * the SLB entry.  This doesn't handle mixed pagesize segments yet.

 Magic page override */

 Check all relevant fields of 1st dword */

 If large page bit is set, check pgsize encoding */

	/* Update PTE R and C bits, so the guest's swapper knows we used the

		/*

		 * Set the accessed flag.

		 * We have to write this back with a single byte write

		 * because another vcpu may be accessing this on

		 * non-PAPR platforms such as mac99, and this is

		 * what real hardware does.

 Set the dirty flag */

 Use a single byte write */

 Map the new segment */

	/*

	 * According to Book3 2.01 mtsrin is implemented as:

	 *

	 * The SLB entry specified by (RB)32:35 is loaded from register

	 * RS, as follows.

	 *

	 * SLBE Bit	Source			SLB Field

	 *

	 * 0:31		0x0000_0000		ESID-0:31

	 * 32:35	(RB)32:35		ESID-32:35

	 * 36		0b1			V

	 * 37:61	0x00_0000|| 0b0		VSID-0:24

	 * 62:88	(RS)37:63		VSID-25:51

	 * 89:91	(RS)33:35		Ks Kp N

	 * 92		(RS)36			L ((RS)36 must be 0b0)

	 * 93		0b0			C

 ESID = srnum */

 Set the valid bit */

 Index = ESID */

 VSID = VSID */

 flags = flags */

	/*

	 * The tlbie instruction changed behaviour starting with

	 * POWER6.  POWER6 and later don't have the large page flag

	 * in the instruction but in the RB value, along with bits

	 * indicating page and segment sizes.

 POWER6 or later */

 L bit */

 64k page */

 16M page */

 older processors, e.g. PPC970 */

 flush this VA on all vcpus */

	/*

	 * Mark this as a 64k segment if the host is using

	 * 64k pages, the host MMU supports 64k pages and

	 * the guest segment page size is >= 64k,

	 * but not if this segment contains the magic page.

 Catch magic page case */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright SUSE Linux Products GmbH 2009

 *

 * Authors: Alexander Graf <agraf@suse.de>

/*

 * OpenPIC emulation

 *

 * Copyright (c) 2004 Jocelyn Mayer

 *               2011 Alexander Graf

 *

 * Permission is hereby granted, free of charge, to any person obtaining a copy

 * of this software and associated documentation files (the "Software"), to deal

 * in the Software without restriction, including without limitation the rights

 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell

 * copies of the Software, and to permit persons to whom the Software is

 * furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included in

 * all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN

 * THE SOFTWARE.

 MPIC version ID */

 OpenPIC capability flags */

 OpenPIC address map */

 Generic Vendor ID */

 count inhibit */

 toggles when decrement to zero */

 critical */

 machine check */

 XXX */

 FSL internal interrupt -- level only */

 FSL timer/IPI interrupt, edge, no polarity */

	/* Round up to the nearest 64 IRQs so that the queue length

	 * won't change when moving between 32 and 64 bit hosts.

 IRQ vector/priority register */

 IRQ destination register */

 bitmap of CPU destinations */

 IRQ level, e.g. ILR_INTTGT_INT */

 TRUE if IRQ is pending */

 level-triggered */

 critical interrupts ignore mask on some FSL MPICs */

 IDR[EP/CI] are only for FSL MPIC prior to v4.0 */

 external pin */

 critical interrupt */

 CPU current task priority */

 Count of IRQ sources asserting on non-INT outputs */

 Behavior control */

 Vendor identification register */

 Global registers */

 Feature reporting register */

 Global configuration register  */

 Processor initialization register */

 Spurious vector register */

 Timer frequency reporting register */

 Source registers */

 Local registers per output pin */

 Timer registers */

 Global timer current count register */

 Global timer base count register */

 Shared MSI registers */

 Shared Message Signaled Interrupt Register */

 TODO */

 TODO */

 XXX: optimize */

		/* On Freescale MPIC, critical interrupts ignore priority,

		 * IACK, EOI, etc.  Before MPIC v4.1 they also ignore

		 * masking.

	/* Even if the interrupt doesn't have enough priority,

	 * it is still raised, in case ctpr is lowered later.

 IRQ line stays asserted */

 update pic state because registers for n_IRQ have changed value */

 Interrupt source is disabled */

	/*

	 * We don't have a similar check for already-active because

	 * ctpr may have changed and we need to withdraw the interrupt.

 No target */

 Only one CPU is allowed to receive this IRQ */

 Directed delivery mode */

 Distributed delivery mode */

 level-sensitive irq */

 edge-sensitive irq */

			/* Edge-triggered interrupts shouldn't be used

			 * with non-INT delivery, but just in case,

			 * try to make it do something sane rather than

			 * cause an interrupt storm.  This is close to

			 * what you'd probably see happen in real hardware.

 Initialise controller registers */

 Initialise IRQ sources */

 Initialise IRQ destinations */

 Initialise timers */

 Go out of RESET state */

 TODO: on MPIC v4.0 only, set nomask for non-INT */

	/* NOTE when implementing newer FSL MPIC models: starting with v4.0,

	 * the polarity bit is read-only on internal interrupts.

 ACTIVITY bit is read-only */

	/* For FSL internal interrupts, The sense bit is reserved and zero,

	 * and the interrupt is always level-triggered.  Timers and IPIs

	 * have no sense or polarity bits, and are edge-triggered.

 Block Revision Register1 (BRR1) is Readonly */

 FRR */

 GCR */

 VIR */

 PIR */

		/*

		 * This register is used to reset a CPU core --

		 * let userspace handle it.

 IPI_IVPR */

 SPVE */

 FRR */

 GCR */

 VIR */

 PIR */

 Block Revision Register1 (BRR1) */

 IPI_IVPR */

 SPVE */

 TFRR */

 TCCR */

 TBCR */

 TVPR */

 TDR */

 TFRR */

 TCCR */

 TBCR */

 TIPV */

 TIDE (TIDR) */

 most registers are read-only, thus ignored */

 MSIRs */

 Clear on read */

 MSISR */

 TODO: EISR/EIMR */

 TODO: EISR/EIMR */

 IPIDR */

 we use IDE as mask which CPUs to deliver the IPI to still. */

 CTPR */

 WHOAMI */

 Read-only register */

 IACK */

 Read-only register */

 EOI */

 Notify listeners that the IRQ is over */

 Set up next servicing IRQ */

 Check queued interrupts. */

 No more interrupt pending */

 IRQ enter servicing state */

 edge-sensitive IRQ */

 trigger on CPUs that didn't know about it yet */

 if all CPUs knew about it, set active bit again */

 CTPR */

 WHOAMI */

 IACK */

 EOI */

 Internal interrupts, including message and MSI */

 timers and IPIs */

	/*

	 * Technically only 32-bit accesses are allowed, but be nice to

	 * people dumping registers a byte at a time -- it works in real

	 * hardware (reads only, not writes).

 Create a nop default map, so that dereferencing it still works */

 We only support one MPIC at a time for now */

 This might need to be changed if GCR gets extended */

/*

 * This should only happen immediately before the mpic is destroyed,

 * so we shouldn't need to worry about anything still trying to

 * access the vcpu pointer.

/*

 * Return value:

 *  < 0   Interrupt was ignored (masked or not delivered for other reasons)

 *  = 0   Interrupt was coalesced (previous irq is still pending)

 *  > 0   Number of CPUs interrupt was delivered to

 All code paths we care about don't check for the return value */

	/*

	 * XXX We ignore the target address for now, as we only support

	 *     a single MSI bank.

 All code paths we care about don't check for the return value */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2010 SUSE Linux Products GmbH. All rights reserved.

 *

 * Authors:

 *     Alexander Graf <agraf@suse.de>

 #define DEBUG_MMU */

 #define DEBUG_SR */

 Remove from host HTAB */

 And make sure it's gone from the TLB too */

/* We keep 512 gvsid->hvsid entries, mapping the guest ones to the array using

 Get host physical address for gpa */

 and write the mapping ea -> hpa into the pt */

 not evicting yet */

 Now tell our Shadow PTE code about the new page */

	/* We might get collisions that trap in preceding order, so let's

 Make sure we're taking the other map next time */

 Uh-oh ... out of mappings. Let's flush! */

 Invalidate an entry */

 Remember context id for this combination */

 Remember where the HTAB is */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2011 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

 * Copyright (C) 2009. SUSE Linux Products GmbH. All rights reserved.

 *

 * Authors:

 *    Paul Mackerras <paulus@au1.ibm.com>

 *    Alexander Graf <agraf@suse.de>

 *    Kevin Wolf <mail@kevin-wolf.de>

 *

 * Description: KVM functions specific to running on Book 3S

 * processors in hypervisor mode (specifically POWER7 and later).

 *

 * This file is derived from arch/powerpc/kvm/book3s.c,

 * by Alexander Graf <agraf@suse.de>.

 #define EXIT_DEBUG */

 #define EXIT_DEBUG_SIMPLE */

 #define EXIT_DEBUG_INT */

 Used to indicate that a guest page fault needs to be handled */

 Used to indicate that a guest passthrough interrupt needs to be handled */

 Used as a "null" value for timebase values */

 If set, guests are allowed to create and control nested guests */

/*

 * RWMR values for POWER8.  These control the rate at which PURR

 * and SPURR count and should be set according to the number of

 * online threads in the vcore being run.

 Used to traverse the list of runnable threads for a given vcore */

 If we're a nested hypervisor, fall back to ordinary IPIs for now */

 On POWER9 we can use msgsnd to IPI any cpu */

 On POWER8 for IPIs to threads in the same core, use msgsnd */

 CPU points to the first thread of the core */

/*

 * We use the vcpu_load/put functions to measure stolen time.

 * Stolen time is counted as time when either the vcpu is able to

 * run as part of a virtual core, but the task running the vcore

 * is preempted or sleeping, or when the vcpu needs something done

 * in the kernel by the task running the vcpu, but that task is

 * preempted or sleeping.  Those two things have to be counted

 * separately, since one of the vcpu tasks will take on the job

 * of running the core, and the other vcpu tasks in the vcore will

 * sleep waiting for it to do that, but that sleep shouldn't count

 * as stolen time.

 *

 * Hence we accumulate stolen time when the vcpu can run as part of

 * a vcore using vc->stolen_tb, and the stolen time when the vcpu

 * needs its task to do other things in the kernel (for example,

 * service a page fault) in busy_stolen.  We don't accumulate

 * stolen time for a vcore when it is inactive, or for a vcpu

 * when it is in state RUNNING or NOTREADY.  NOTREADY is a bit of

 * a misnomer; it means that the vcpu task is not executing in

 * the KVM_VCPU_RUN ioctl, i.e. it is in userspace or elsewhere in

 * the kernel.  We don't have any way of dividing up that time

 * between time that the vcpu is genuinely stopped, time that

 * the task is actively working on behalf of the vcpu, and time

 * that the task is preempted, so we don't count any of it as

 * stolen.

 *

 * Updates to busy_stolen are protected by arch.tbacct_lock;

 * updates to vc->stolen_tb are protected by the vcore->stoltb_lock

 * lock.  The stolen times are measured in units of timebase ticks.

 * (Note that the != TB_NIL checks below are purely defensive;

 * they should never fail.)

	/*

	 * We can test vc->runner without taking the vcore lock,

	 * because only this task ever sets vc->runner to this

	 * vcpu, and once it is set to this vcpu, only this task

	 * ever sets it to NULL.

 Dummy value used in computing PCR value below */

 We can (emulate) our own architecture version and anything older */

 Determine lowest PCR bit needed to run guest in given PVR level */

 Check requested PCR bits don't exceed our capabilities */

	/*

	 * Set all PCR bits for which guest_pcr_bit <= bit < host_pcr_bit

	 * Also set all reserved PCR bits

 check address is cacheline aligned */

 Length for a per-processor buffer is passed in at offset 4 in the buffer */

 Registering new area - address must be cache-line aligned */

 convert logical addr to kernel addr and read length */

 Check length */

 register VPA */

		/*

		 * The size of our lppaca is 1kB because of the way we align

		 * it for the guest to avoid crossing a 4kB boundary. We only

		 * use 640 bytes of the structure though, so we should accept

		 * clients that set a size of 640.

 register DTL */

 Check that they have previously registered a VPA */

 register SLB shadow buffer */

 Check that they have previously registered a VPA */

 deregister VPA */

 Check they don't still have a DTL or SLB buf registered */

 deregister DTL */

 deregister SLB shadow buffer */

	/*

	 * We need to pin the page pointed to by vpap->next_gpa,

	 * but we can't call kvmppc_pin_guest_page under the lock

	 * as it does get_user_pages() and down_read().  So we

	 * have to drop the lock, pin the page, then get the lock

	 * again and check that a new area didn't get registered

	 * in the meantime.

 sigh... unpin that one and try again */

		/*

		 * If it's now too short, it must be that userspace

		 * has changed the mappings underlying guest memory,

		 * so unregister the region.

/*

 * Return the accumulated stolen time for the vcore up until `now'.

 * The caller should hold the vcore lock.

 order writing *dt vs. writing vpa->dtl_idx */

 See if there is a doorbell interrupt pending for a vcpu */

	/*

	 * Ensure that the read of vcore->dpdes comes after the read

	 * of vcpu->doorbell_request.  This barrier matches the

	 * smp_wmb() in kvmppc_guest_entry_inject().

 Guests can't breakpoint the hypervisor */

		/*

		 * KVM does not support mflags=2 (AIL=2) and AIL=1 is reserved.

		 * Keep this in synch with kvmppc_filter_guest_lpcr_hv.

 Copy guest memory in place - must reside within a single memslot */

 Get HPA for from address */

 Get HPA for to address */

 Perform copy */

 4K page size */

 Check for invalid flags (H_PAGE_SET_LOANED covers all CMO flags) */

 dest (and src if copy_page flag set) must be page aligned */

 zero and/or copy the page as determined by the flags */

 We can ignore the remaining flags */

	/*

	 * We expect to have been called by the real mode handler

	 * (kvmppc_rm_h_confer()) which would have directly returned

	 * H_SUCCESS if the source vcore wasn't idle (e.g. if it may

	 * have useful work to do and should not confer) so we don't

	 * recheck that here.

	 *

	 * In the case of the P9 single vcpu per vcore case, the real

	 * mode handler is not called but no other threads are in the

	 * source vcore.

/*

 * H_RPT_INVALIDATE hcall handler for nested guests.

 *

 * Handles only nested process-scoped invalidation requests in L0.

	/*

	 * The partition-scoped invalidations aren't handled here in L0.

	/*

	 * Partition-scoped invalidation for nested guests.

 Support only cores as target */

	/*

	 * Process-scoped invalidation for L1 guests.

 Send the error out to userspace via KVM_RUN */

		/*

		 * Even if that call is made by the Ultravisor, the SSR1 value

		 * is the guest context one, with the secure bit clear as it has

		 * not yet been secured. So we can't check it here.

		 * Instead the kvm->arch.secure_guest flag is checked inside

		 * kvmppc_h_svm_init_abort().

/*

 * Handle H_CEDE in the P9 path where we don't call the real-mode hcall

 * handlers in book3s_hv_rmhandlers.S.

 *

 * This has to be done early, not in kvmppc_pseries_do_hcall(), so

 * that the cede logic in kvmppc_run_single_vcpu() works properly.

 See if it's in the real-mode table */

		/*

		 * Fetch failed, so return to guest and

		 * try executing it again.

		/*

		 * If the vcpu is currently running on a physical cpu thread,

		 * interrupt it in order to pull it out of the guest briefly,

		 * which will update its vcore->dpdes value.

/*

 * On POWER9, emulate doorbell-related instructions in order to

 * give the guest the illusion of running on a multi-threaded core.

 * The instructions emulated are msgsndp, msgclrp, mfspr TIR,

 * and mfspr DPDES.

	/*

	 * This can happen if an interrupt occurs in the last stages

	 * of guest entry or the first stages of guest exit (i.e. after

	 * setting paca->kvm_hstate.in_guest to KVM_GUEST_MODE_GUEST_HV

	 * and before setting it to KVM_GUEST_MODE_HOST_HV).

	 * That can happen due to a bug, or due to a machine check

	 * occurring at just the wrong time.

 We're good on these - the host merely wanted to get our attention */

 SR/HMI/PMI are HV interrupts that host has handled. Resume guest.*/

		/*

		 * Print the MCE event to host console. Ratelimit so the guest

		 * can't flood the host log.

		/*

		 * If the guest can do FWNMI, exit to userspace so it can

		 * deliver a FWNMI to the guest.

		 * Otherwise we synthesize a machine check for the guest

		 * so that it knows that the machine check occurred.

 Exit to guest with KVM_EXIT_NMI as exit reason */

 Clear out the old NMI status from run->flags */

 Now set the NMI status */

		/*

		 * Normally program interrupts are delivered directly

		 * to the guest by the hardware, but we can get here

		 * as a result of a hypervisor emulation interrupt

		 * (e40) getting turned into a 700 by BML RTAS.

			/*

			 * Guest userspace executed sc 1. This can only be

			 * reached by the P9 path because the old path

			 * handles this case in realmode hcall handlers.

				/*

				 * A guest could be running PR KVM, so this

				 * may be a PR KVM hcall. It must be reflected

				 * to the guest kernel as a sc interrupt.

				/*

				 * Radix guests can not run PR KVM or nested HV

				 * hash guests which might run PR KVM, so this

				 * is always a privilege fault. Send a program

				 * check to guest kernel.

		/*

		 * hcall - gather args and set exit_reason. This will next be

		 * handled by kvmppc_pseries_do_hcall which may be able to deal

		 * with it and resume guest, or may punt to userspace.

	/*

	 * We get these next two if the guest accesses a page which it thinks

	 * it has mapped but which is not actually present, either because

	 * it is for an emulated I/O device or because the corresonding

	 * host page has been paged out.

	 *

	 * Any other HDSI/HISI interrupts have been handled already for P7/8

	 * guests. For POWER9 hash guests not using rmhandlers, basic hash

	 * fault handling is done here.

 Just retry if it's the canary */

			/*

			 * Radix doesn't require anything, and pre-ISAv3.0 hash

			 * already attempted to handle this in rmhandlers. The

			 * hash fault handling below is v3 only (it uses ASDR

			 * via fault_gpa).

			/*

			 * Radix doesn't require anything, and pre-ISAv3.0 hash

			 * already attempted to handle this in rmhandlers. The

			 * hash fault handling below is v3 only (it uses ASDR

			 * via fault_gpa).

	/*

	 * This occurs if the guest executes an illegal instruction.

	 * If the guest debug is disabled, generate a program interrupt

	 * to the guest. If guest debug is enabled, we need to check

	 * whether the instruction is a software breakpoint instruction.

	 * Accordingly return to Guest or Host.

		/*

		 * This occurs for various TM-related instructions that

		 * we need to emulate on POWER9 DD2.2.  We have already

		 * handled the cases where the guest was in real-suspend

		 * mode and was transitioning to transactional state.

 go to facility unavailable handler */

	/*

	 * This occurs if the guest (kernel or userspace), does something that

	 * is prohibited by HFSCR.

	 * On POWER9, this could be a doorbell instruction that we need

	 * to emulate.

	 * Otherwise, we just generate a program interrupt to the guest.

	/*

	 * This can happen if an interrupt occurs in the last stages

	 * of guest entry or the first stages of guest exit (i.e. after

	 * setting paca->kvm_hstate.in_guest to KVM_GUEST_MODE_GUEST_HV

	 * and before setting it to KVM_GUEST_MODE_HOST_HV).

	 * That can happen due to a bug, or due to a machine check

	 * occurring at just the wrong time.

 We're good on these - the host merely wanted to get our attention */

 SR/HMI/PMI are HV interrupts that host has handled. Resume guest.*/

 Pass the machine check to the L1 guest */

 Print the MCE event to host console. */

	/*

	 * We get these next two if the guest accesses a page which it thinks

	 * it has mapped but which is not actually present, either because

	 * it is for an emulated I/O device or because the corresonding

	 * host page has been paged out.

		/*

		 * This occurs for various TM-related instructions that

		 * we need to emulate on POWER9 DD2.2.  We have already

		 * handled the cases where the guest was in real-suspend

		 * mode and was transitioning to transactional state.

 go to facility unavailable handler */

		/*

		 * Only pass HFU interrupts to the L1 if the facility is

		 * permitted but disabled by the L1's HFSCR, otherwise

		 * the interrupt does not make sense to the L1 so turn

		 * it into a HEAI.

			/*

			 * If the fetch failed, return to guest and

			 * try executing it again.

		/*

		 * The H_RPT_INVALIDATE hcalls issued by nested

		 * guests for process-scoped invalidations when

		 * GTSE=0, are handled here in L0.

 Only accept the same PVR as the host's, since we can't spoof it */

/*

 * Enforce limits on guest LPCR values based on hardware availability,

 * guest configuration, and possibly hypervisor support and security

 * concerns.

 LPCR_TC only applies to HPT guests */

 On POWER8 and above, userspace can modify AIL */

 LPCR[AIL]=1/2 is disallowed */

	/*

	 * On some POWER9s we force AIL off for radix guests to prevent

	 * executing in MSR[HV]=1 mode with the MMU enabled and PIDR set to

	 * guest, which can result in Q0 translations with LPID=0 PID=PIDR to

	 * be cached, which the host TLB management does not expect.

	/*

	 * On POWER9, allow userspace to enable large decrementer for the

	 * guest, whether or not the host has it enabled.

	/*

	 * Userspace can only modify

	 * DPFD (default prefetch depth), ILE (interrupt little-endian),

	 * TC (translation control), AIL (alternate interrupt location),

	 * LD (large decrementer).

	 * These are subject to restrictions from kvmppc_filter_lcpr_hv().

 Broken 32-bit version of LPCR must not clear top bits */

	/*

	 * If ILE (interrupt little-endian) has changed, update the

	 * MSR_LE bit in the intr_msr for each vcpu in this vcore.

		/*

		 * On POWER9, where we are emulating msgsndp etc.,

		 * we return 1 bit for each vcpu, which can come from

		 * either vcore->dpdes or doorbell_request.

		 * On POWER8, doorbell_request is 0.

 Only allow this to be set to zero */

 Don't allow setting breakpoints in hypervisor code */

 disable */

 round up to multiple of 2^24 */

/*

 * On POWER9, threads are independent and can be in different partitions.

 * Therefore we consider each thread to be a subcore.

 * There is a restriction that all threads have to be in the same

 * MMU mode (radix or HPT), unfortunately, but since we only support

 * HPT guests on a HPT host so far, that isn't an impediment yet.

 Create a debugfs directory for the vcpu */

 CONFIG_KVM_BOOK3S_HV_EXIT_TIMING */

 CONFIG_KVM_BOOK3S_HV_EXIT_TIMING */

	/*

	 * The shared struct is never shared on HV,

	 * so we can always use host endianness

 default to host PVR, since we can't spoof it */

	/*

	 * Set the default HFSCR for the guest from the host value.

	 * This value is only used on POWER9.

	 * On POWER9, we want to virtualize the doorbell facility, so we

	 * don't set the HFSCR_MSGP bit, and that causes those instructions

	 * to trap and then we emulate them.

			/*

			 * Take mmu_setup_lock for mutual exclusion

			 * with kvmppc_update_lpcr().

		/*

		 * On POWER8 (or POWER7), the threading mode is "strict",

		 * so we pack smt_mode vcpus per vcore.

		/*

		 * On POWER9, the threading mode is "loose",

		 * so each vcpu gets its own vcore.

 Indicate we want to get back into the guest */

 decrementer has already gone negative */

 Ensure the thread won't go into the kernel if it wakes */

	/*

	 * If the thread is already executing in the kernel (e.g. handling

	 * a stray interrupt), wait for it to get back to nap mode.

	 * The smp_mb() is to ensure that our setting of hwthread_req

	 * is visible before we look at hwthread_state, so if this

	 * races with the code at system_reset_pSeries and the thread

	 * misses our setting of hwthread_req, we are sure to see its

	 * setting of hwthread_state, and vice versa.

	/*

	 * Make sure setting of bit in need_tlb_flush precedes

	 * testing of cpu_in_guest bits.  The matching barrier on

	 * the other side is the first smp_mb() in kvmppc_run_core().

	/*

	 * With radix, the guest can do TLB invalidations itself,

	 * and it could choose to use the local form (tlbiel) if

	 * it is invalidating a translation that has only ever been

	 * used on one vcpu.  However, that doesn't mean it has

	 * only ever been used on one physical cpu, since vcpus

	 * can move around between pcpus.  To cope with this, when

	 * a vcpu moves from one pcpu to another, we need to tell

	 * any vcpus running on the same core as this vcpu previously

	 * ran to flush the TLB.  The TLB is shared between threads,

	 * so we use a single bit in .need_tlb_flush for all 4 threads.

 Order stores to hstate.kvm_vcpu etc. before store to kvm_vcore */

		/*

		 * Check if all threads are finished.

		 * We set the vcore pointer when starting a thread

		 * and the thread clears it when finished, so we look

		 * for any threads that still have a non-NULL vcore ptr.

/*

 * Check that we are on thread 0 and that any other threads in

 * this core are off-line.  Then grab the threads so they can't

 * enter the kernel.

 Are we on a primary subcore? */

 Grab all hw threads so they can't go into the kernel */

 Couldn't grab one; let the others go */

/*

 * A list of virtual cores for each physical CPU.

 * These are vcores that could run but their runner VCPU tasks are

 * (or may be) preempted.

 Start accumulating stolen time */

/*

 * This stores information about the virtual cores currently

 * assigned to a physical core.

/*

 * This mapping means subcores 0 and 1 can use threads 0-3 and 4-7

 * respectively in 2-way micro-threading (split-core) mode on POWER8.

	/*

	 * POWER9 "SMT4" cores are permanently in what is effectively a 4-way

	 * split-core mode, with one thread per subcore.

 On POWER8, can only dynamically split if unsplit to begin with */

 In one_vm_per_core mode, require all vcores to be from the same vm */

/*

 * Work out whether it is possible to piggyback the execution of

 * vcore *pvc onto the execution of the other vcores described in *cip.

		/*

		 * It's safe to unlock the vcore in the loop here, because

		 * for_each_runnable_thread() is safe against removal of

		 * the vcpu, and the vcore state is VCORE_EXITING here,

		 * so any vcpus becoming runnable will have their arch.trap

		 * set to zero and can't actually run in the guest.

 cancel pending dec exception if dec is positive */

 make sure there's a candidate runner awake */

/*

 * Clear core from the list of active host cores as we are about to

 * enter the guest. Only do this if it is the primary thread of the

 * core (not if a subcore) that is entering the guest.

	/*

	 * Memory barrier can be omitted here as we will do a smp_wmb()

	 * later in kvmppc_start_thread and we need ensure that state is

	 * visible to other CPUs only after we enter guest.

/*

 * Advertise this core as an active host core since we exited the guest

 * Only need to do this if it is the primary thread of the core that is

 * exiting.

	/*

	 * Memory barrier can be omitted here because we do a spin_unlock

	 * immediately after this which provides the memory barrier.

/*

 * Run a set of guest threads on a physical core.

 * Called with vc->lock held.

	/*

	 * Remove from the list any threads that have a signal pending

	 * or need a VPA update done

 if the runner is no longer runnable, let the caller pick a new one */

	/*

	 * Initialize *vc.

	/*

	 * Number of threads that we will be controlling: the same as

	 * the number of threads per subcore, except on POWER9,

	 * where it's 1 because the threads are (mostly) independent.

	/*

	 * Make sure we are running on primary threads, and that secondary

	 * threads are offline.  Also check if the number of threads in this

	 * guest are greater than the current system threads per guest.

	/*

	 * See if we could run any other vcores on the physical core

	 * along with this one.

	/*

	 * Hard-disable interrupts, and check resched flag and signals.

	 * If we need to reschedule or deliver a signal, clean up

	 * and return without going into the guest(s).

	 * If the mmu_ready flag has been cleared, don't go into the

	 * guest because that means a HPT resize operation is in progress.

 Unlock all except the primary vcore */

 Put back on to the preempted vcores list */

 Decide on micro-threading (split-core) mode */

 order writes to split_info before kvm_split_mode pointer */

 Initiate micro-threading (split-core) on POWER8 if required */

	/*

	 * On POWER8, set RWMR register.

	 * Since it only affects PURR and SPURR, it doesn't affect

	 * the host, so we don't save/restore the host value.

		/*

		 * Use the 8-thread value if we're doing split-core

		 * or if the vcore's online count looks bogus.

 Start all the threads */

		/*

		 * We need to start the first thread of each subcore

		 * even if it doesn't have a vcpu.

	/*

	 * Ensure that split_info.do_nap is set after setting

	 * the vcore pointer in the PACA of the secondaries.

	/*

	 * When doing micro-threading, poke the inactive threads as well.

	 * This gets them to the nap instruction after kvm_do_nap,

	 * which reduces the time taken to unsplit later.

 ask secondaries to nap when done */

	/*

	 * Interrupts will be enabled once we get into the guest,

	 * so tell lockdep that we're about to enable interrupts.

 prevent other vcpu threads from doing kvmppc_start_thread() now */

 wait for secondary threads to finish writing their state to memory */

 Return to whole-core mode if we split the core earlier */

		/*

		 * Service IRQs here before vtime_account_guest_exit() so any

		 * ticks that occurred while running the guest are accounted to

		 * the guest. If vtime accounting is enabled, accounting uses

		 * TB rather than ticks, so it can be done without enabling

		 * interrupts here, which has the problem that it accounts

		 * interrupt processing overhead to the host.

 Let secondaries go back to the offline loop */

 make sure updates to secondary vcpu structs are visible now */

	/*

	 * DAR, DSISR, and for nested HV, SPRGs must be set with MSR[RI]

	 * clear (or hstate set appropriately to catch those registers

	 * being clobbered if we take a MCE or SRESET), so those are done

	 * later.

/*

 * Privileged (non-hypervisor) host registers to save.

 vcpu guest regs must already be saved */

 Save guest CTRL register, set runlatch to 1 */

/*

 * Guest entry for POWER9 and later CPUs.

 saves it to PACA kvm_hstate */

	/*

	 * When setting DEC, we must always deal with irq_work_raise via NMI vs

	 * setting DEC. The problem occurs right as we switch into guest mode

	 * if a NMI hits and sets pending work and sets DEC, then that will

	 * apply to the guest and not bring us back to the host.

	 *

	 * irq_work_raise could check a flag (or possibly LPCR[HDICE] for

	 * example) and set HDEC to 1? That wouldn't solve the nested hv

	 * case which needs to abort the hcall or zero the time limit.

	 *

	 * XXX: Another day's problem.

		/*

		 * We need to save and restore the guest visible part of the

		 * psscr (i.e. using SPRN_PSSCR_PR) since the hypervisor

		 * doesn't do this for us. Note only required if pseries since

		 * this is done in kvmhv_vcpu_entry_p9() below otherwise.

 call our hypervisor to load up HV regs and go */

 H_CEDE has to be handled now, not later */

 H_CEDE has to be handled now, not later */

 may un-cede */

 XICS hcalls must be handled before xive is pulled */

 Sign extend if not using large decrementer */

 Must save pmu if this guest is capable of running nested guests */

 We may have raced with new irq work */

/*

 * Wait for some other vcpu thread to execute us, and

 * wake us up when we need to handle something in the host.

 CONFIG_KVM_XICS */

/*

 * Check to see if any of the runnable vcpus on the vcore have pending

 * exceptions or are no longer ceded

/*

 * All the vcpus in this vcore are idle, so wait for a decrementer

 * or external interrupt to one of the vcpus.  vc->lock is held.

 Poll for pending exceptions and ceded state */

 If we polled, count this as a successful poll */

 Attribute wait time */

 Attribute failed poll time */

 Attribute successful poll time */

 Adjust poll time */

 We slept and blocked for longer than the max halt time */

 We slept and our poll time is too small */

/*

 * This never fails for a radix guest, as none of the operations it does

 * for a radix guest can fail or have a way to report failure.

	/*

	 * Synchronize with other threads in this virtual core

	/*

	 * This happens the first time this is called for a vcpu.

	 * If the vcore is already running, we may be able to start

	 * this thread straight away and have it join in.

 See if the MMU is ready to go */

 Let something else run */

 Wake up some vcpu to run the core */

 See if the MMU is ready to go */

 Tell lockdep that we're about to enable interrupts */

		/*

		 * Service IRQs here before vtime_account_guest_exit() so any

		 * ticks that occurred while running the guest are accounted to

		 * the guest. If vtime accounting is enabled, accounting uses

		 * TB rather than ticks, so it can be done without enabling

		 * interrupts here, which has the problem that it accounts

		 * interrupt processing overhead to the host.

	/*

	 * cancel pending decrementer exception if DEC is now positive, or if

	 * entering a nested guest in which case the decrementer is now owned

	 * by L2 and the L1 decrementer is provided in hdec_expires

 shut up GCC */

	/*

	 * Don't allow entry with a suspended transaction, because

	 * the guest entry/exit code will lose it.

	 * If the guest has TM enabled, save away their TM-related SPRs

	 * (they will get restored by the TM unavailable interrupt).

 Enable TM so we can read the TM SPRs */

	/*

	 * Force online to 1 for the sake of old userspace which doesn't

	 * set it.

 No need to go into the guest when all we'll do is come back out */

 Order vcpus_running vs. mmu_ready, see kvmppc_alloc_reset_hpt */

 Save userspace EBB and other register values */

				/*

				 * These should have been caught reflected

				 * into the guest by now. Final sanity check:

				 * don't allow userspace to execute hcalls in

				 * the hypervisor.

 Restore userspace EBB and other register values */

	/*

	 * Add 16MB MPSS support (may get filtered out by userspace)

	/*

	 * POWER7, POWER8 and POWER9 all support 32 storage keys for data.

	 * POWER7 doesn't support keys for instruction accesses,

	 * POWER8 and POWER9 do.

 POWER7, 8 and 9 all have 1T segments and 32-entry SLB */

 We only support these sizes for now, and no muti-size segments */

 If running as a nested hypervisor, we don't support HPT guests */

/*

 * Get (and clear) the dirty memory log for a memory slot.

	/*

	 * Use second half of bitmap area because both HPT and radix

	 * accumulate bits in the first half.

	/*

	 * We accumulate dirty bits in the first half of the

	 * memslot's dirty_bitmap area, for when pages are paged

	 * out or modified by the host directly.  Pick up these

	 * bits and add them to the map.

 Harvest dirty bits from VPA and DTL updates */

 Note: we never modify the SLB shadow buffer areas */

	/*

	 * If we are making a new memslot, it might make

	 * some address that was previously cached as emulated

	 * MMIO be no longer emulated MMIO, so invalidate

	 * all the caches of emulated MMIO translations.

	/*

	 * For change == KVM_MR_MOVE or KVM_MR_DELETE, higher levels

	 * have already called kvm_arch_flush_shadow_memslot() to

	 * flush shadow mappings.  For KVM_MR_CREATE we have no

	 * previous mappings.  So the only case to handle is

	 * KVM_MR_FLAGS_ONLY when the KVM_MEM_LOG_DIRTY_PAGES bit

	 * has been changed.

	 * For radix guests, we flush on setting KVM_MEM_LOG_DIRTY_PAGES

	 * to get rid of any THP PTEs in the partition-scoped page tables

	 * so we can track dirtiness at the page level; we flush when

	 * clearing KVM_MEM_LOG_DIRTY_PAGES so that we can go back to

	 * using THP PTEs.

	/*

	 * If UV hasn't yet called H_SVM_INIT_START, don't register memslots.

		/*

		 * @TODO kvmppc_uvmem_memslot_create() can fail and

		 * return error. Fix this.

 TODO: Handle KVM_MR_MOVE */

/*

 * Update LPCR values in kvm->arch and in vcores.

 * Caller must hold kvm->arch.mmu_setup_lock (for mutual exclusion

 * of kvm->arch.lpcr update).

 PS field - page size for VRMA */

 HTABSIZE and HTABORG fields */

 Second dword as set by userspace */

/*

 * Set up HPT (hashed page table) and RMA (real-mode area).

 * Must be called with kvm->arch.mmu_setup_lock held.

 Allocate hashed page table (if not done already) and reset it */

		/* If we get here, it means userspace didn't specify a

		 * size explicitly.  So, try successively smaller

 Look up the memslot for guest physical address 0 */

 We must have some memory at 0 by now */

 Look up the VMA for the start of this memory slot */

 We can handle 4k, 64k or 16M pages in the VRMA */

 Create HPTEs in the hash page table for the VRMA */

 Update VRMASD field in the LPCR */

 the -4 is to account for senc values starting at 0x10 */

 Order updates to kvm->arch.lpcr etc. vs. mmu_ready */

/*

 * Must be called with kvm->arch.mmu_setup_lock held and

 * mmu_ready = 0 and no vcpus running.

 Mutual exclusion with kvm_unmap_gfn_range etc. */

/*

 * Must be called with kvm->arch.mmu_setup_lock held and

 * mmu_ready = 0 and no vcpus running.

 Mutual exclusion with kvm_unmap_gfn_range etc. */

/*

 * Allocate a per-core structure for managing state about which cores are

 * running in the host versus the guest and for exchanging data between

 * real mode KVM and CPU running in the host.

 * This is only done for the first VM.

 * The allocated structure stays even if all VMs have stopped.

 * It is only freed when the kvm-hv module is unloaded.

 * It's OK for this routine to fail, we just don't support host

 * core operations like redirecting H_IPI wakeups.

 Not the first time here ? */

	/*

	 * Make the contents of the kvmppc_host_rm_ops structure visible

	 * to other CPUs before we assign it to the global variable.

	 * Do an atomic assignment (no locks used here), but if someone

	 * beats us to it, just free our copy and return.

 Allocate the guest's logical partition ID */

	/*

	 * Since we don't flush the TLB when tearing down a VM,

	 * and this lpid might have previously been used,

	 * make sure we flush on each core before running the new VM.

	 * On POWER9, the tlbie in mmu_partition_table_set_entry()

	 * does this flush for us.

 Start out with the default set of hcalls enabled */

 Init LPCR for virtual RMA mode */

 On POWER8 turn on online bit to enable PURR/SPURR */

	/*

	 * On POWER9, VPM0 bit is reserved (VPM0=1 behaviour is assumed)

	 * Set HVICE bit to enable hypervisor virtualization interrupts.

	 * Set HEIC to prevent OS interrupts to go to hypervisor (should

	 * be unnecessary but better safe than sorry in case we re-enable

	 * EE in HV mode with this LPCR still set)

		/*

		 * If xive is enabled, we route 0x500 interrupts directly

		 * to the guest.

	/*

	 * If the host uses radix, the guest starts out as radix.

 Initialization for future HPT resizes */

	/*

	 * Work out how many sets the TLB has, for the use of

	 * the TLB invalidation loop in book3s_hv_rmhandlers.S.

		/*

		 * P10 will flush all the congruence class with a single tlbiel

 128 */

 256 */

 512 */

 128 */

	/*

	 * Track that we now have a HV mode VM active. This blocks secondary

	 * CPU threads from coming online.

	/*

	 * Initialize smt_mode depending on processor.

	 * POWER8 and earlier have to use "strict" threading, where

	 * all vCPUs in a vcore have to run on the same (sub)core,

	 * whereas on POWER9 the threads can each run a different

	 * guest.

	/*

	 * Create a debugfs directory for the VM

 Perform global invalidation and return lpid to the pool */

 We don't need to emulate any privileged instructions or dcbz */

 POWER9 in radix mode is capable of being a nested hypervisor. */

 First call, allocate structure to hold IRQ map */

	/*

	 * For now, we only support interrupts for which the EOI operation

	 * is an OPAL call followed by a write to XIRR, since that's

	 * what our real-mode EOI code does, or a XIVE interrupt

	/*

	 * See if we already have an entry for this guest IRQ number.

	 * If it's mapped to a hardware IRQ number, that's an error,

	 * otherwise re-use this entry.

 table is full */

	/*

	 * Order the above two stores before the next to serialize with

	 * the KVM real mode handler.

	/*

	 * The 'host_irq' number is mapped in the PCI-MSI domain but

	 * the underlying calls, which will EOI the interrupt in real

	 * mode, need an HW IRQ number mapped in the XICS IRQ domain.

 invalidate the entry (what do do on error from the above ?) */

	/*

	 * We don't free this structure even when the count goes to

	 * zero. The structure is freed when we destroy the VM.

	/*

	 * When producer of consumer is unregistered, we change back to

	 * default external interrupt handling mode - KVM real mode

	 * will switch back to host.

 If we're a nested hypervisor, we currently only support radix */

/*

 * List of hcall numbers to enable by default.

 * For compatibility with old userspace, we enable by default

 * all hcalls that were implemented before the hcall-enabling

 * facility was added.  Note this list should not include H_RTAS.

 If not on a POWER9, reject it */

 If any unknown flags set, reject it */

 GR (guest radix) bit in process_table field must match */

 Process table size field must be reasonable, i.e. <= 24 */

 We can change a guest to/from radix now, if the host is radix */

 If we're a nested hypervisor, we currently only support radix */

 order mmu_ready vs. vcpus_running */

 kvm == NULL means the caller is testing if the capability exists */

 For now quadrants are the only way to access nested guest memory */

 For now quadrants are the only way to access nested guest memory */

/*

 * Enable a guest to become a secure VM, or test whether

 * that could be enabled.

 * Called when the KVM_CAP_PPC_SECURE_GUEST capability is

 * tested (kvm == NULL) or enabled (kvm != NULL).

/*

 *  IOCTL handler to turn off secure mode of guest

 *

 * - Release all device pages

 * - Issue ucall to terminate the guest on the UV side

 * - Unpin the VPA pages.

 * - Reinit the partition scoped page tables

 order mmu_ready vs. vcpus_running */

	/*

	 * When secure guest is reset, all the guest pages are sent

	 * to UV via UV_PAGE_IN before the non-boot vcpus get a

	 * chance to run and unpin their VPA pages. Unpinning of all

	 * VPA pages is done here explicitly so that VPA pages

	 * can be migrated to the secure side.

	 *

	 * This is required to for the secure SMP guest to reboot

	 * correctly.

 kvm == NULL means the caller is testing if the capability exists */

	/*

	 * POWER9 chips before version 2.02 can't have some threads in

	 * HPT mode and some in radix mode on the same core.

 Ignore if it is already allocated. */

	/*

	 * FIXME!! Do we need to check on all cpus ?

	/*

	 * We need a way of accessing the XICS interrupt controller,

	 * either directly, via paca_ptrs[cpu]->kvm_hstate.xics_phys, or

	 * indirectly, via OPAL.

 presence of intc confirmed - node can be dropped again */

 SPDX-License-Identifier: GPL-2.0-or-later

/* ppc-dis.c -- Disassemble PowerPC instructions

   Copyright (C) 1994-2016 Free Software Foundation, Inc.

   Written by Ian Lance Taylor, Cygnus Support



This file is part of GDB, GAS, and the GNU binutils.



/* This file provides several disassembler functions, all of which use

   the disassembler interface defined in dis-asm.h.  Several functions

   are provided because this file handles disassembly for the PowerPC

   in both big and little endian mode and also for the POWER (RS/6000)

 Extract the operand value from the PowerPC or POWER instruction.  */

 Extract the value from the instruction.  */

	  /* BITM is always some number of zeros followed by some

	  /* top & -top gives the rightmost 1 bit, so this

 Determine whether the optional operand(s) should be printed.  */

/* Find a match for INSN in the opcode table, given machine DIALECT.

 Find the first match in the opcode table for this major opcode.  */

 Check validity of operands.  */

 Print a PowerPC or POWER instruction.  */

 Get the major opcode of the insn.  */

 The operands will be fetched out of the 16-bit instruction.  */

 Now extract and print the operands.  */

	  /* Operands that are marked FAKE are simply ignored.  We

	     already made sure that the extract function considered

	  /* If all of the optional operands have the value zero,

 Print the operand as directed by the flags.  */

      /* We have found and printed an instruction.

 Otherwise, return.  */

 We could not find a match.  */

 SPDX-License-Identifier: GPL-2.0-or-later

/* ppc-opc.c -- PowerPC opcode list

   Copyright (C) 1994-2016 Free Software Foundation, Inc.

   Written by Ian Lance Taylor, Cygnus Support



   This file is part of GDB, GAS, and the GNU binutils.



/* This file holds the PowerPC opcode table.  The opcode table

   includes almost all of the extended instruction mnemonics.  This

   permits the disassembler to use them, and simplifies the assembler

   logic, at the cost of increasing the table size.  The table is

   strictly constant data, so the compiler should be able to put it in

   the .text section.



   This file also holds the operand table.  All knowledge about

   inserting operands into instructions and vice-versa is kept in this

 Local insertion and extraction functions.  */

/* The operands table.



   The fields are bitm, shift, insert, extract, flags.



   We used to put parens around the various additions, like the one

   for BA just below.  However, that caused trouble with feeble

   compilers with a limit on depth of a parenthesized expression, like

   (reportedly) the compiler in Microsoft Developer Studio 5.  So we

   omit the parens, since the macros are never used in a context where

  /* The zero index is used to indicate the end of the list of

 The BA field in an XL form instruction.  */

 The BI field in a B form or XL form instruction.  */

  /* The BA field in an XL form instruction when it must be the same

 The BB field in an XL form instruction.  */

  /* The BB field in an XL form instruction when it must be the same

  /* The VB field in a VX form instruction when it must be the same

  /* The BD field in a B form instruction.  The lower two bits are

  /* The BD field in a B form instruction when absolute addressing is

  /* The BD field in a B form instruction when the - modifier is used.

  /* The BD field in a B form instruction when the - modifier is used

  /* The BD field in a B form instruction when the + modifier is used.

  /* The BD field in a B form instruction when the + modifier is used

 The BF field in an X or XL form instruction.  */

 The CRFD field in an X form instruction.  */

 The CRD field in an XL form instruction.  */

 The BF field in an X or XL form instruction.  */

  /* An optional BF field.  This is used for comparison instructions,

 The BFA field in an X or XL form instruction.  */

  /* The BO field in a B form instruction.  Certain values are

  /* The BO field in a B form instruction when the + or - modifier is

 The RM field in an X form instruction.  */

 The BT field in an X or XL form instruction.  */

 The BI16 field in a BD8 form instruction.  */

 The BI32 field in a BD15 form instruction.  */

 The BO32 field in a BD15 form instruction.  */

 The B8 field in a BD8 form instruction.  */

  /* The B15 field in a BD15 form instruction.  The lowest bit is

  /* The B24 field in a BD24 form instruction.  The lowest bit is

  /* The condition register number portion of the BI field in a B form

     or XL form instruction.  This is used for the extended

     conditional branch mnemonics, which set the lower two bits of the

 The CRB field in an X form instruction.  */

 The MB field in an M form instruction.  */

 The CRD32 field in an XL form instruction.  */

 The CRFS field in an X form instruction.  */

 The CT field in an X form instruction.  */

 The MO field in an mbar instruction.  */

  /* The D field in a D form instruction.  This is a displacement off

     a register, and implies that the next operand is a register in

  /* The D8 field in a D form instruction.  This is a displacement off

     a register, and implies that the next operand is a register in

 The DCMX field in an X form instruction.  */

 The split DCMX field in an X form instruction.  */

  /* The DQ field in a DQ form instruction.  This is like D, but the

  /* The DS field in a DS form instruction.  This is like D, but the

  /* The DUIS or BHRBE fields in a XFX form instruction, 10 bits

 The split D field in a DX form instruction.  */

  /* The split ND field in a DX form instruction.

 The E field in a wrteei instruction.  */

 And the W bit in the pair singles instructions.  */

 And the ST field in a VX form instruction.  */

 The FL1 field in a POWER SC form instruction.  */

 The U field in an X form instruction.  */

 The FL2 field in a POWER SC form instruction.  */

 The FLM field in an XFL form instruction.  */

 The FRA field in an X or A form instruction.  */

 The FRAp field of DFP instructions.  */

 The FRB field in an X or A form instruction.  */

 The FRBp field of DFP instructions.  */

 The FRC field in an A form instruction.  */

  /* The FRS field in an X form instruction or the FRT field in a D, X

  /* The FRSp field of stfdp or the FRTp field of lfdp and DFP

 The FXM field in an XFX instruction.  */

 Power4 version for mfcr.  */

 If the FXM4 operand is ommitted, use the sentinel value -1.  */

 The IMM20 field in an LI instruction.  */

 The L field in a D or X form instruction.  */

 The optional L field in tlbie and tlbiel instructions.  */

 The R field in a HTM X form instruction.  */

 The optional (for 32-bit) L field in cmp[l][i] instructions.  */

 The L field in dcbf instruction.  */

 The LEV field in a POWER SVC form instruction.  */

 The LEV field in an SC form instruction.  */

  /* The LI field in an I form instruction.  The lower two bits are

  /* The LI field in an I form instruction when used as an absolute

 The LS or WC field in an X (sync or wait) form instruction.  */

 The ME field in an M form instruction.  */

  /* The MB and ME fields in an M form instruction expressed a single

     operand which is a bitmask indicating which bits to select.  This

     is a two operand form using PPC_OPERAND_NEXT.  See the

  /* The MB or ME field in an MD or MDS form instruction.  The high

  /* The NB field in an X form instruction.  The value 32 is stored as

  /* The NBI field in an lswi instruction, which has special value

  /* The NSI field in a D form instruction.  This is the same as the

  /* The NSI field in a D form instruction when we accept a wide range

 The RA field in an D, DS, DQ, X, XO, M, or MDS form instruction.  */

 As above, but 0 in the RA field means zero, not r0.  */

  /* The RA field in the DQ form lq or an lswx instruction, which have special

  /* The RA field in a D or X form instruction which is an updating

     load, which means that the RA field may not be zero and may not

  /* The RA field in an lmw instruction, which has special value

  /* The RA field in a D or X form instruction which is an updating

     store or an updating floating point load, which means that the RA

  /* The RA field of the tlbwe, dccci and iccci instructions,

 The RB field in an X, XO, M, or MDS form instruction.  */

  /* The RB field in an X form instruction when it must be the same as

     the RS field in the instruction.  This is used for extended

  /* The RB field in an lswx instruction, which has special value

 The RB field of the dccci and iccci instructions, which are optional.  */

 The RC register field in an maddld, maddhd or maddhdu instruction.  */

  /* The RS field in a D, DS, X, XFX, XS, M, MD or MDS form

     instruction or the RT field in a D, DS, X, XFX or XO form

  /* The RS and RT fields of the DS form stq and DQ form lq instructions,

 The RS field of the tlbwe instruction, which is optional.  */

 The RX field of the SE_RR form instruction.  */

 The ARX field of the SE_RR form instruction.  */

 The RY field of the SE_RR form instruction.  */

 The ARY field of the SE_RR form instruction.  */

 The SCLSCI8 field in a D form instruction.  */

  /* The SCLSCI8N field in a D form instruction.  This is the same as the

 The SD field of the SD4 form instruction.  */

 The SD field of the SD4 form instruction, for halfword.  */

 The SD field of the SD4 form instruction, for word.  */

 The SH field in an X or M form instruction.  */

 The other UIMM field in a EVX form instruction.  */

 The FC field in an atomic X form instruction.  */

 The SI field in a HTM X form instruction.  */

 The SH field in an MD form instruction.  This is split.  */

 The SH field of the tlbwe instruction, which is optional.  */

 The SI field in a D form instruction.  */

  /* The SI field in a D form instruction when we accept a wide range

 The SI8 field in a D form instruction.  */

  /* The SPR field in an XFX form instruction.  This is flipped--the

 The BAT index number in an XFX form m[ft]ibat[lu] instruction.  */

 The SPRG register number in an XFX form m[ft]sprg instruction.  */

 The SR field in an X form instruction.  */

 The 4-bit UIMM field in a VX form instruction.  */

 The STRM field in an X AltiVec form instruction.  */

 The T field in a tlbilx form instruction.  */

 The L field in wclr instructions.  */

 The ESYNC field in an X (sync) form instruction.  */

 The SV field in a POWER SC form instruction.  */

  /* The TBR field in an XFX form instruction.  This is like the SPR

 If the TBR operand is ommitted, use the value 268.  */

 The TO field in a D or X form instruction.  */

 The UI field in a D form instruction.  */

 The IMM field in an SE_IM5 instruction.  */

 The OIMM field in an SE_OIM5 instruction.  */

 The UI7 field in an SE_LI instruction.  */

 The VA field in a VA, VX or VXR form instruction.  */

 The VB field in a VA, VX or VXR form instruction.  */

 The VC field in a VA form instruction.  */

 The VD or VS field in a VA, VX, VXR or X form instruction.  */

 The SIMM field in a VX form instruction, and TE in Z form.  */

 The UIMM field in a VX form instruction.  */

 The 3-bit UIMM field in a VX form instruction.  */

 The 6-bit UIM field in a X form instruction.  */

 The SIX field in a VX form instruction.  */

 The PS field in a VX form instruction.  */

 The SHB field in a VA form instruction.  */

 The other UIMM field in a half word EVX form instruction.  */

 The other UIMM field in a word EVX form instruction.  */

 The other UIMM field in a double EVX form instruction.  */

 The WS or DRM field in an X form instruction.  */

 PowerPC paired singles extensions.  */

 W bit in the pair singles instructions for x type instructions.  */

 The BO16 field in a BD8 form instruction.  */

 IDX bits for quantization in the pair singles instructions.  */

 IDX bits for quantization in the pair singles x-type instructions.  */

 Smaller D field for quantization in the pair singles instructions.  */

 The L field in an mtmsrd or A form instruction or R or W in an X form.  */

 The RMC or CY field in a Z23 form instruction.  */

 The S field in a XL form instruction.  */

 If the SXL operand is ommitted, use the value 1.  */

 SH field starting at bit position 16.  */

 The DCM and DGM fields in a Z form instruction.  */

 The EH field in larx instruction.  */

 The L field in an mtfsf or XFL form instruction.  */

 The A field in a HTM X form instruction.  */

 Xilinx APU related masks and macros */

 Xilinx FSL related masks and macros */

 Xilinx UDI related masks and macros */

 The VLESIMM field in a D form instruction.  */

 The VLENSIMM field in a D form instruction.  */

 The VLEUIMM field in a D form instruction.  */

 The VLEUIMML field in a D form instruction.  */

 The XT and XS fields in an XX1 or XX3 form instruction.  This is split.  */

 The XT and XS fields in an DQ form VSX instruction.  This is split.  */

 The XA field in an XX3 form instruction.  This is split.  */

 The XB field in an XX2 or XX3 form instruction.  This is split.  */

  /* The XB field in an XX3 form instruction when it must be the same as

     the XA field in the instruction.  This is used in extended mnemonics

 The XC field in an XX4 form instruction.  This is split.  */

 The DM or SHW field in an XX3 form instruction.  */

 The DM field in an extended mnemonic XX3 form instruction.  */

 The UIM field in an XX2 form instruction.  */

 The 2-bit UIMM field in a VX form instruction.  */

 The 2-bit L field in a darn instruction.  */

 The 8-bit IMM8 field in a XX1 form instruction.  */

 The functions used to insert and extract complicated operands.  */

 The ARX, ARY, RX and RY operands are alternate encodings of GPRs.  */

/* The BA field in an XL form instruction when it must be the same as

   the BT field in the same instruction.  This operand is marked FAKE.

   The insertion function just copies the BT field into the BA field,

   and the extraction function just checks that the fields are the

/* The BB field in an XL form instruction when it must be the same as

   the BA field in the same instruction.  This operand is marked FAKE.

   The insertion function just copies the BA field into the BB field,

   and the extraction function just checks that the fields are the

/* The BD field in a B form instruction when the - modifier is used.

   This modifier means that the branch is not expected to be taken.

   For chips built to versions of the architecture prior to version 2

   (ie. not Power4 compatible), we set the y bit of the BO field to 1

   if the offset is negative.  When extracting, we require that the y

   bit be 1 and that the offset be positive, since if the y bit is 0

   we just want to print the normal form of the instruction.

   Power4 compatible targets use two bits, "a", and "t", instead of

   the "y" bit.  "at" == 00 => no hint, "at" == 01 => unpredictable,

   "at" == 10 => not taken, "at" == 11 => taken.  The "t" bit is 00001

   in BO field, the "a" bit is 00010 for branch on CR(BI) and 01000

   for branch on CTR.  We only handle the taken/not-taken hint here.

   Note that we don't relax the conditions tested here when

   disassembling with -Many because insns using extract_bdm and

   extract_bdp always occur in pairs.  One or the other will always

/* The BD field in a B form instruction when the + modifier is used.

   This is like BDM, above, except that the branch is expected to be

  /* Certain encodings have bits that are required to be zero.

     These are (z must be zero, y may be anything):

	 0000y

	 0001y

	 001zy

	 0100y

	 0101y

	 011zy

	 1z00y

	 1z01y

	 1z1zz

  /* Certain encodings have bits that are required to be zero.

     These are (z must be zero, a & t may be anything):

	 0000z

	 0001z

	 001at

	 0100z

	 0101z

	 011at

	 1a00t

	 1a01t

	 1z1zz

 Check for legal values of a BO field.  */

  /* When disassembling with -Many, accept either encoding on the

/* The BO field in a B form instruction.  Warn about attempts to set

/* The BO field in a B form instruction when the + or - modifier is

   used.  This is like the BO field, but it must be even.  When

/* The DCMX field in a X form instruction when the field is split

/* The D field in a DX form instruction when the field is split

 FXM mask in mfcr and mtcrf instructions.  */

  /* If we're handling the mfocrf and mtocrf insns ensure that exactly

  /* If only one bit of the FXM field is set, we can use the new form

     of the instruction, which is faster.  Unlike the Power4 branch hint

     encoding, this is not backward compatible.  Do not generate the

     new form unless -mpower4 has been given, or -many and the two

 Any other value on mfcr is an error.  */

      /* A value of -1 means we used the one operand form of

 Is this a Power4 insn?  */

 Exactly one bit of MASK should be set.  */

 Check that non-power4 form of mfcr has a zero MASK.  */

/* The 2-bit L field in a SYNC or WC field in a WAIT instruction.

   For SYNC, some L values are reserved:

     * Value 3 is reserved on newer server cpus.

 For SYNC, some L values are illegal.  */

/* The 4-bit E field in a sync instruction that accepts 2 operands.

   If ESYNC is non-zero, then the L field must be either 0 or 1 and

/* The MB and ME fields in an M form instruction expressed as a single

   operand which is itself a bitmask.  The extraction function always

   marks it as invalid, since we never want to recognize an

 mb: location of last 0->1 transition */

 me: location of last 1->0 transition */

 count: # transitions */

 (mb > me + 1) */

/* The MB or ME field in an MD or MDS form instruction.  The high bit

/* The NB field in an X form instruction.  The value 32 is stored as

/* The NB field in an lswi instruction, which has special value

/* The NSI field in a D form instruction.  This is the same as the SI

   field, only negated.  The extraction function always marks it as

   invalid, since we never want to recognize an instruction which uses

/* The RA field in a D or X form instruction which is an updating

   load, which means that the RA field may not be zero and may not

/* The RA field in an lmw instruction, which has special value

/* The RA field in the DQ form lq or an lswx instruction, which have special

/* The RA field in a D or X form instruction which is an updating

   store or an updating floating point load, which means that the RA

/* The RB field in an X form instruction when it must be the same as

   the RS field in the instruction.  This is used for extended

   mnemonics like mr.  This operand is marked FAKE.  The insertion

   function just copies the BT field into the BA field, and the

/* The RB field in an lswx instruction, which has special value

 The SCI8 field is made up of SCL and {U,N}I8 fields.  */

 The SH field in an MD form instruction.  This is split.  */

 SH6 operand in the rldixor instructions.  */

 SH6 operand in the rldixor instructions.  */

/* The SPR field in an XFX form instruction.  This is flipped--the

 Some dialects have 8 SPRG registers instead of the standard 4.  */

  /* If this is mfsprg4..7 then use spr 260..263 which can be read in

  /* mfsprg can use 260..263 and 272..279.  mtsprg only uses spr 272..279

/* The TBR field in an XFX instruction.  This is just like SPR, but it

 The XT and XS fields in an XX1 or XX3 form instruction.  This is split.  */

 The XT and XS fields in an DQ form VSX instruction.  This is split.  */

 The XA field in an XX3 form instruction.  This is split.  */

 The XB field in an XX3 form instruction.  This is split.  */

/* The XB field in an XX3 form instruction when it must be the same as

   the XA field in the instruction.  This is used for extended

   mnemonics like xvmovdp.  This operand is marked FAKE.  The insertion

   function just copies the XA field into the XB field, and the

 The XC field in an XX4 form instruction.  This is split.  */

 The VLESIMM field in an I16A form instruction.  This is split.  */

 Don't use for disassembly.  */

 The VLEUIMM field in an I16A form instruction.  This is split.  */

 The VLEUIMML field in an I16L form instruction.  This is split.  */

 Macros used to form opcodes.  */

 The main opcode.  */

/* The main opcode combined with a trap code in the TO field of a D

   form instruction.  Used for extended mnemonics for the trap

/* The main opcode combined with a comparison size bit in the L field

   of a D form or X form instruction.  Used for extended mnemonics for

/* The main opcode combined with an update code in D form instruction.

/* The main opcode combined with an update code and the RT fields specified in

   D form instruction.  Used for VLE volatile context save/restore

 An A form instruction.  */

 An A_MASK with the FRB field fixed.  */

 An A_MASK with the FRC field fixed.  */

 An A_MASK with the FRA and FRC fields fixed.  */

 An AFRAFRC_MASK, but with L bit clear.  */

 A B form instruction.  */

 A BD8 form instruction.  This is a 16-bit instruction.  */

 Another BD8 form instruction.  This is a 16-bit instruction.  */

 A BD8 form instruction for simplified mnemonics.  */

 A mask that excludes BO32 and BI32.  */

 A mask that includes BO32 and excludes BI32.  */

 A mask that include BO32 AND BI32.  */

 A BD15 form instruction.  */

 A BD15 form instruction for extended conditional branch mnemonics.  */

 A BD15 form instruction for extended conditional branch mnemonics with BI.  */

 A BD24 form instruction.  */

 A B form instruction setting the BO field.  */

/* A BBO_MASK with the y bit of the BO field removed.  This permits

   matching a conditional branch regardless of the setting of the y

/* A B form instruction setting the BO field and the condition bits of

 A BBOCB_MASK with the y bit of the BO field removed.  */

 A BBOYCB_MASK in which the BI field is fixed.  */

 A VLE C form instruction.  */

 An Context form instruction.  */

 A User Context form instruction.  */

 The main opcode mask with the RA field clear.  */

 A DQ form VSX instruction.  */

 A DS form instruction.  */

 An DX form instruction.  */

 An EVSEL form instruction.  */

 An IA16 form instruction.  */

 An I16A form instruction.  */

 An I16L form instruction.  */

 An IM7 form instruction.  */

 An M form instruction.  */

 An LI20 form instruction.  */

 An M form instruction with the ME field specified.  */

 An M_MASK with the MB and ME fields fixed.  */

 An M_MASK with the SH and ME fields fixed.  */

 An MD form instruction.  */

 An MD_MASK with the MB field fixed.  */

 An MD_MASK with the SH field fixed.  */

 An MDS form instruction.  */

 An MDS_MASK with the MB field fixed.  */

 An SC form instruction.  */

 An SCI8 form instruction.  */

 An SCI8 form instruction.  */

 An SD4 form instruction.  This is a 16-bit instruction.  */

 An SE_IM5 form instruction.  This is a 16-bit instruction.  */

 An SE_R form instruction.  This is a 16-bit instruction.  */

 An SE_RR form instruction.  This is a 16-bit instruction.  */

 A VX form instruction.  */

 The mask for an VX form instruction.  */

 A VX_MASK with the VA field fixed.  */

 A VX_MASK with the VB field fixed.  */

 A VX_MASK with the VA and VB fields fixed.  */

 A VX_MASK with the VD and VA fields fixed.  */

 A VX_MASK with a UIMM4 field.  */

 A VX_MASK with a UIMM3 field.  */

 A VX_MASK with a UIMM2 field.  */

 A VX_MASK with a PS field.  */

 A VX_MASK with the VA field fixed with a PS field.  */

 A VA form instruction.  */

 The mask for an VA form instruction.  */

 A VXA_MASK with a SHB field.  */

 A VXR form instruction.  */

 The mask for a VXR form instruction.  */

 A VX form instruction with a VA tertiary opcode.  */

 An X form instruction.  */

 A X form instruction for Quad-Precision FP Instructions.  */

 An EX form instruction.  */

 The mask for an EX form instruction.  */

 An XX2 form instruction.  */

 A XX2 form instruction with the VA bits specified.  */

 An XX3 form instruction.  */

 An XX3 form instruction with the RC bit specified.  */

 An XX4 form instruction.  */

 A Z form instruction.  */

 An X form instruction with the RC bit specified.  */

 A X form instruction for Quad-Precision FP Instructions with RC bit.  */

 An X form instruction with the RA bits specified as two ops.  */

 A Z form instruction with the RC bit specified.  */

 The mask for an X form instruction.  */

 The mask for an X form instruction with the BF bits specified.  */

 An X form wait instruction with everything filled in except the WC field.  */

 The mask for an XX1 form instruction.  */

 An XX1_MASK with the RB field fixed.  */

 The mask for an XX2 form instruction.  */

 The mask for an XX2 form instruction with the UIM bits specified.  */

 The mask for an XX2 form instruction with the 4 UIM bits specified.  */

 The mask for an XX2 form instruction with the BF bits specified.  */

 The mask for an XX2 form instruction with the BF and DCMX bits specified.  */

 The mask for an XX2 form instruction with a split DCMX bits specified.  */

 The mask for an XX3 form instruction.  */

 The mask for an XX3 form instruction with the BF bits specified.  */

 The mask for an XX3 form instruction with the DM or SHW bits specified.  */

 The mask for an XX4 form instruction.  */

 An X form wait instruction with everything filled in except the WC field.  */

 The mask for an XMMF form instruction.  */

 The mask for a Z form instruction.  */

 An X_MASK with the RA/VA field fixed.  */

 An XRA_MASK with the A_L/W field clear.  */

 An X_MASK with the RB field fixed.  */

 An X_MASK with the RT field fixed.  */

 An XRT_MASK mask with the L bits clear.  */

 An X_MASK with the RA and RB fields fixed.  */

 An XBF_MASK with the RA and RB fields fixed.  */

 An XRARB_MASK, but with the L bit clear.  */

 An XRARB_MASK, but with the L bits in a darn instruction clear.  */

 An X_MASK with the RT and RA fields fixed.  */

 An X_MASK with the RT and RB fields fixed.  */

 An XRTRA_MASK, but with L bit clear.  */

 An X_MASK with the RT, RA and RB fields fixed.  */

 An XRTRARB_MASK, but with L bit clear.  */

 An XRTRARB_MASK, but with A bit clear.  */

 An XRTRARB_MASK, but with BF bits clear.  */

 An X form instruction with the L bit specified.  */

 An X form instruction with the L bits specified.  */

 An X form instruction with the L bit and RC bit specified.  */

 An X form instruction with RT fields specified */

 An X form instruction with RT and RA fields specified */

 The mask for an X form comparison instruction.  */

/* The mask for an X form comparison instruction with the L field

 An X form trap instruction with the TO field specified.  */

 An X form tlb instruction with the SH field specified.  */

 An X form sync instruction.  */

 An X form sync instruction with everything filled in except the LS field.  */

 An X form sync instruction with everything filled in except the L and E fields.  */

 An X_MASK, but with the EH bit clear.  */

 An X form AltiVec dss instruction.  */

 An XFL form instruction.  */

 An X form isel instruction.  */

 An XL form instruction with the LK field set to 0.  */

 An XL form instruction which uses the LK field.  */

 The mask for an XL form instruction.  */

 An XL_MASK with the RT, RA and RB fields fixed, but S bit clear.  */

 An XL form instruction which explicitly sets the BO field.  */

/* An XL form instruction which explicitly sets the y bit of the BO

/* An XL form instruction which sets the BO field and the condition

 An XL_MASK or XLYLK_MASK or XLOCB_MASK with the BB field fixed.  */

 A mask for branch instructions using the BH field.  */

 An XL_MASK with the BO and BB fields fixed.  */

 An XL_MASK with the BO, BI and BB fields fixed.  */

 An X form mbar instruction with MO field.  */

 An XO form instruction.  */

 An XO_MASK with the RB field fixed.  */

 An XOPS form instruction for paired singles.  */

 An XS form instruction.  */

 A mask for the FXM version of an XFX form instruction.  */

 An XFX form instruction with the FXM field filled in.  */

 An XFX form instruction with the SPR field filled in.  */

/* An XFX form instruction with the SPR field filled in except for the

/* An XFX form instruction with the SPR field filled in except for the

 An X form instruction with everything filled in except the E field.  */

 An X form user context instruction.  */

 An XW form instruction.  */

 The mask for a G form instruction. rc not supported at present.  */

 An APU form instruction.  */

 The mask for an APU form instruction.  */

 The BO encodings used in extended conditional branch mnemonics.  */

 The BO16 encodings used in extended VLE conditional branch mnemonics.  */

 The BO32 encodings used in extended VLE conditional branch mnemonics.  */

/* The BI condition bit encodings used in extended conditional branch

 The TO encodings used in extended trap mnemonics.  */

/* Smaller names for the flags so each entry in the opcodes table will

/* The list of embedded processors that use the embedded operand ordering

/* The opcode table.



   The format of the opcode table is:



   NAME		OPCODE		MASK	     FLAGS	ANTI		{OPERANDS}



   NAME is the name of the instruction.

   OPCODE is the instruction opcode.

   MASK is the opcode mask; this is used to tell the disassembler

     which bits in the actual opcode must match OPCODE.

   FLAGS are flags indicating which processors support the instruction.

   ANTI indicates which processors don't support the instruction.

   OPERANDS is the list of operands.



   The disassembler reads the table in order and prints the first

   instruction which matches, so this table is sorted to put more

   specific instructions before more general instructions.



   This table must be sorted by major opcode.  Please try to keep it

   vaguely sorted within major opcode too, except of course where

/* The "yield", "mdoio" and "mdoom" instructions are extended mnemonics for

/* The VLE opcode table.



 The macro table.  This is only used by the assembler.  */

/* The expressions of the form (-x ! 31) & (x | 31) have the value 0

   when x=0; 32-x when x is between 1 and 31; are negative if x is

   negative; and are 32 or more otherwise.  This is what you want

   when, for instance, you are emulating a right shift by a

   rotate-left-and-mask, because the underlying instructions support

   shifts of size 0 but not shifts of size 32.  By comparison, when

   extracting x bits from some word you want to use just 32-x, because

   the underlying instructions don't support extracting 0 bits but do

 SPDX-License-Identifier: GPL-2.0-or-later

/* SPU opcode list



   Copyright 2006 Free Software Foundation, Inc.



   This file is part of GDB, GAS, and the GNU binutils.



 This file holds the Spu opcode table */

/*

   Example contents of spu-insn.h

      id_tag	mode	mode	type	opcode	mnemonic	asmtype	    dependency		FPU	L/S?	branch?	instruction   

                QUAD	WORD                                               (0,RC,RB,RA,RT)    latency  			              		

   APUOP(M_LQD,	1,	0,	RI9,	0x1f8,	"lqd",		ASM_RI9IDX,	00012,		FXU,	1,	0)	Load Quadword d-form 

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 1996-2005 Paul Mackerras.

 Lines Per Page */

 nothing */

 No udbg hooks, fallback to printk() - dangerous */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Routines providing a simple monitor for use on the PowerMac.

 *

 * Copyright (C) 1996-2005 Paul Mackerras.

 * Copyright (C) 2001 PPC64 Team, IBM Corp

 * Copyrignt (C) 2006 Michael Ellerman, IBM Corp

 CONFIG_SMP */

 Breakpoint stuff */

 Bits in bpt.enabled */

 trap */

 Prototypes */

 CONFIG_SECURITY */

/**

 * write_ciabr() - write the CIABR SPR

 * @ciabr:	The value to write.

 *

 * This function writes a value to the CIARB register either directly

 * through mtspr instruction if the kernel is in HV privilege mode or

 * call a hypervisor function to achieve the same in case the kernel

 * is in supervisor privilege mode.

/**

 * set_ciabr() - set the CIABR

 * @addr:	The value to set.

 *

 * This function sets the correct privilege value into the the HW

 * breakpoint address before writing it up in the CIABR register.

/*

 * Disable surveillance (the service processor watchdog function)

 * while we are in xmon.

 * XXX we should re-enable it when we leave. :)

 Since this can't be a module, args should end up below 4GB. */

	/*

	 * At this point we have got all the cpus we can into

	 * xmon, so there is hopefully no other cpu calling RTAS

	 * at the moment, even though we don't take rtas.lock.

	 * If we did try to take rtas.lock there would be a

	 * real possibility of deadlock.

 CONFIG_PPC_PSERIES */

		/*

		 * Wait a full second for the lock, we might be on a slow

		 * console, but check every 100us.

 hostile takeover */

 We wait for 2s, which is a metric "little while" */

 CONFIG_SMP */

		/*

		 * We catch SPR read/write faults here because the 0x700, 0xf60

		 * etc. handlers don't call debugger_fault_handler().

 we are the first cpu to come in */

 interrupt other cpu(s) */

			/*

			 * A system reset (trap == 0x100) can be triggered on

			 * all CPUs, so when we come in via 0x100 try waiting

			 * for the other CPUs to come in before we send the

			 * debugger break (IPI). This is similar to

			 * crash_kexec_secondary().

 for breakpoint or single step, print curr insn */

 missed it */

 exiting xmon */

 have switched to some other cpu */

 UP is simple... */

 for breakpoint or single step, print current insn */

 Are we at the trap at bp->instr[1] for some bp? */

 Are we at a breakpoint? */

 doesn't return */

 Force enable xmon if not already enabled */

 Enable xmon hooks if needed */

		/*

		 * Check the address is not a suffix by looking for a prefix in

		 * front of it.

		/*

		 * We might still be a suffix - if the prefix has already been

		 * replaced by a breakpoint we won't catch it with the above

		 * test.

 Based on uptime_proc_show(). */

 Command interpreting routine */

 CONFIG_SMP */

 print regs */

/*

 * Step a single instruction.

 * Some instructions we emulate, others we execute with MSR_SE set.

 check we are in 64-bit kernel mode, translation enabled */

 take control back */

 simulate command entry */

 print regs */

 print cpus waiting or in xmon */

 try to switch to cpu specified */

 CONFIG_SMP */

/*

 * Check if this is a suitable place to put a breakpoint.

 bd - hardware data breakpoint */

 bi - hardware instr breakpoint */

 clear all breakpoints */

 assume a breakpoint number */

 bp nums are 1 based */

 assume a breakpoint address */

 print all breakpoints */

 Very cheap human name for vector lookup. */

		/*

		 * For the first stack frame, try to work out if

		 * LR and/or the saved LR value in the bottommost

		 * stack frame are valid.

		/* Look for "regshere" marker to see if this is

 not in kernel */

 address of trap instruction */

 CONFIG_BUG */

 CONFIG_SMP */

 wait a little while to see if we get a machine check */

 Actually some of these pre-date 2.06, but whatevs */

 Only if TM has been enabled in the kernel */

 Looks like read was a nop, confirm */

 dump ALL SPRs */

/*

 * Stuff for reading and writing memory safely

 wait a little while to see if we get a machine check */

 wait a little while to see if we get a machine check */

 wait a little while to see if we get a machine check */

 is there really any use for this? */

=0x100, =0x1000, etc\n"

 Put c back, it wasn't 'a' */

 Put c back, it wasn't 'a' */

 CONFIG_PPC_POWERNV */

 dump virtual to physical translation */

 wait a little while to see if we get a machine check */

 wait a little while to see if we get a machine check */

/*

 * Memory operations - move, set, print differences

 destination address */

 source address */

 byte value to set memory to */

 # bytes to affect */

 max # differences to print */

	/*

	 * Cloned from kdb_task_state_char(), which is not entirely

	 * appropriate for calling from xmon. This could be moved

	 * to a common, generic, routine used by both.

 CONFIG_PPC_BOOK3S_64 */

 Input scanning routines */

 parse register name */

 skip leading "0x" if any */

 Print an address in numeric and symbolic form (if possible) */

 wait a little while to see if we get a machine check */

 CONFIG_44x */

 Gather some infos about the MMU */

 CONFIG_PPC_BOOK3E */

		/*

		 * Get the token here to avoid trying to get a lock

		 * during the crash, causing a deadlock.

 ensure xmon is enabled */

 CONFIG_MAGIC_SYSRQ */

 clear/unpatch all breakpoints */

 Disable all breakpoints */

 Clear any data or iabr breakpoints */

 make sure all breakpoints removed when disabling */

 CONFIG_DEBUG_FS */

 just "xmon" is equivalent to "xmon=early" */

 Enough for current hardware */

 ! CONFIG_SPU_BASE */

 SPDX-License-Identifier: GPL-2.0-or-later

/* Disassemble SPU instructions



   Copyright 2006 Free Software Foundation, Inc.



   This file is part of GDB, GAS, and the GNU binutils.



/* This file provides a disassembler function which uses

  /* If two instructions have the same opcode then we prefer the first

 abort (); */

 Determine the instruction from the 10 least significant bits. */

  /* Init the table.  This assumes that element 0/opcode 0 (currently

 Print a Spu instruction.  */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  PowerPC version

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 *

 *  Dave Engebretsen <engebret@us.ibm.com>

 *      Rework for PPC64 port.

 used by kvm_hv module */

/*

 * Create a kmem_cache() for pagetables.  This is not used for PTE

 * pages - they're linked to struct page, come from the normal free

 * pages pool and have a different entry size (see real_pte_t) to

 * everything else.  Caches created by this function are used for all

 * the higher level pagetables, and for hugepage pagetables.

	/* When batching pgtable pointers for RCU freeing, we store

	 * the index size in the low bits.  Table alignment must be

	 * big enough to fit it.

	 *

	 * Likewise, hugeapge pagetable pointers contain a (different)

	 * shift value in the low bits.  All tables must be aligned so

	/* It would be nice if this was a BUILD_BUG_ON(), but at the

	 * moment, gcc doesn't seem to recognize is_power_of_2 as a

 Already have a cache of this size */

 used by kvm_hv module */

	/*

	 * In all current configs, when the PUD index exists it's the

	 * same size as either the pgd or pmd index except with THP enabled

	 * on book3s 64

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * address space "slices" (meta-segments) support

 *

 * Copyright (C) 2007 Benjamin Herrenschmidt, IBM Corporation.

 *

 * Based on hugetlb implementation

 *

 * Copyright (C) 2003 David Gibson, IBM Corporation.

	/* Hack, so that each addresses is controlled by exactly one

	 * of the high or low area bitmaps, the first high area starts

 Write the new slice psize bits */

	/* We need to use a spinlock here to protect against

	 * concurrent 64k -> 4k demotion ...

 Update the slice_mask */

 Update the sizes array */

 Update the slice_mask */

 Update the sizes array */

/*

 * Compute which slice addr is part of;

 * set *boundary_addr to the start or end boundary of that slice

 * (depending on 'end' parameter);

 * return boolean indicating if the slice is marked as available in the

 * 'available' slice_mark.

	/*

	 * Check till the allow max value for this mmap request

		/*

		 * At this point [info.low_limit; addr) covers

		 * available slices only and ends at a slice boundary.

		 * Check if we need to reduce the range, or if we can

		 * extend it to cover the next available slice.

	/*

	 * If we are trying to allocate above DEFAULT_MAP_WINDOW

	 * Add the different to the mmap_base.

	 * Only for that request for which high_limit is above

	 * DEFAULT_MAP_WINDOW we should apply this.

		/*

		 * At this point [addr; info.high_limit) covers

		 * available slices only and starts at a slice boundary.

		 * Check if we need to reduce the range, or if we can

		 * extend it to cover the previous available slice.

	/*

	 * A failed mmap() very likely causes application failure,

	 * so fall back to the bottom-up function here. This scenario

	 * can happen with large stack limits and large mmap()

	 * allocations.

		/*

		 * Increasing the slb_addr_limit does not require

		 * slice mask cache to be recalculated because it should

		 * be already initialised beyond the old address limit.

 Sanity checks */

 If hint, make sure it matches our alignment restrictions */

 Ignore hint if it's too large or overlaps a VMA */

	/* First make up a "good" mask of slices that have the right size

	 * already

	/*

	 * Here "good" means slices that are already the right page size,

	 * "compat" means slices that have a compatible page size (i.e.

	 * 4k in a 64k pagesize kernel), and "free" means slices without

	 * any VMAs.

	 *

	 * If MAP_FIXED:

	 *	check if fits in good | compat => OK

	 *	check if fits in good | compat | free => convert free

	 *	else bad

	 * If have hint:

	 *	check if hint fits in good => OK

	 *	check if hint fits in good | free => convert free

	 * Otherwise:

	 *	search in good, found => OK

	 *	search in good | free, found => convert free

	 *	search in good | compat | free, found => convert free.

	/*

	 * If we support combo pages, we can allow 64k pages in 4k slices

	 * The mask copies could be avoided in most cases here if we had

	 * a pointer to good mask for the next code to use.

 First check hint if it's valid or if we have MAP_FIXED */

		/* Check if we fit in the good mask. If we do, we just return,

		 * nothing else to do

		/* Now let's see if we can find something in the existing

		 * slices for that size

			/* Found within the good mask, we don't have to setup,

			 * we thus return directly

	/*

	 * We don't fit in the good mask, check what other slices are

	 * empty and thus can be converted

 If we have MAP_FIXED and failed the above steps, then error out */

	/* If we had a hint that didn't work out, see if we can fit

	 * anywhere in the good area.

	/* Now let's see if we can find something in the existing slices

	 * for that size plus free slices

 retry the search with 4k-page slices included */

	/*

	 * Try to allocate the context before we do slice convert

	 * so that we handle the context allocation failure gracefully.

	/*

	 * In the case of exec, use the default limit. In the

	 * case of fork it is just inherited from the mm being

	 * duplicated.

	/*

	 * Set all slice psizes to the default.

	/*

	 * Slice mask cache starts zeroed, fill the default size cache.

/*

 * is_hugepage_only_range() is used by generic code to verify whether

 * a normal mmap mapping (non hugetlbfs) is valid on a given area.

 *

 * until the generic code provides a more generic hook and/or starts

 * calling arch get_unmapped_area for MAP_FIXED (which our implementation

 * here knows how to deal with), we hijack it to keep standard mappings

 * away from us.

 *

 * because of that generic code limitation, MAP_FIXED mapping cannot

 * "convert" back a slice with no VMAs to the standard page size, only

 * get_unmapped_area() can. It would be possible to fix it here but I

 * prefer working on fixing the generic code instead.

 *

 * WARNING: This will not work if hugetlbfs isn't enabled since the

 * generic code will redefine that function as 0 in that. This is ok

 * for now as we only use slices with hugetlbfs enabled. This should

 * be fixed as the generic code gets fixed.

 We need to account for 4k slices too */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Handling Page Tables through page fragments

 *

 drop all the pending references */

 We allow PTE_FRAG_NR fragments from a PTE page */

		/*

		 * If we have taken up all the fragments mark PTE page NULL

	/*

	 * if we support only one fragment just return the

	 * allocated page.

	/*

	 * If we find pgtable_page set, we return

	 * the allocated page with single fragement

	 * count.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  PowerPC version

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *  PPC44x/36-bit changes by Matt Porter (mporter@mvista.com)

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 The amount of lowmem must be within 0xF0000000 - KERNELBASE. */

 Used in __va()/__pa() */

/*

 * this tells the system to map all of ram with the segregs

 * (i.e. page tables) instead of the bats.

 * -- Cort

 max amount of low RAM to map in */

/*

 * Check for command-line options that affect what MMU_init will do.

 Check for nobats option (used in mapin_ram). */

/*

 * MMU_init sets up the basic memory mappings for the kernel,

 * including both RAM and possibly some I/O regions,

 * and sets up the page tables and the MMU hardware ready to go.

 parse args from command line */

	/*

	 * Reserve gigantic pages for hugetlb.  This MUST occur before

	 * lowmem_end_addr is initialized below.

	/* Freescale Book-E parts expect lowmem to be mapped by fixed TLB

	 * entries, so we need to adjust lowmem to match the amount we can map

 CONFIG_FSL_BOOKE */

 CONFIG_HIGHMEM */

 Initialize the MMU hardware */

 Map in all of RAM starting at KERNELBASE */

 Initialize early top-down ioremap allocator */

 From now on, btext is no longer BAT mapped if it was at all */

 Shortly after that, the entire linear mapping will be available */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Common implementation of switch_mm_irqs_off

 *

 *  Copyright IBM Corp. 2017

 32-bit keeps track of the current PGDIR in the thread struct */

 64-bit Book3E keeps track of current PGD in the PACA */

 Mark this context has been used on the new CPU */

		/*

		 * This full barrier orders the store to the cpumask above vs

		 * a subsequent load which allows this CPU/MMU to begin loading

		 * translations for 'next' from page table PTEs into the TLB.

		 *

		 * When using the radix MMU, that operation is the load of the

		 * MMU context id, which is then moved to SPRN_PID.

		 *

		 * For the hash MMU it is either the first load from slb_cache

		 * in switch_slb() to preload the SLBs, or the load of

		 * get_user_context which loads the context for the VSID hash

		 * to insert a new SLB, in the SLB fault handler.

		 *

		 * On the other side, the barrier is in mm/tlb-radix.c for

		 * radix which orders earlier stores to clear the PTEs before

		 * the load of mm_cpumask to check which CPU TLBs should be

		 * flushed. For hash, pte_xchg to clear the PTE includes the

		 * barrier.

		 *

		 * This full barrier is also needed by membarrier when

		 * switching between processes after store to rq->curr, before

		 * user-space memory accesses.

 Some subarchs need to track the PGD elsewhere */

 Nothing else to do if we aren't actually switching */

	/*

	 * We must stop all altivec streams before changing the HW

	 * context

	/*

	 * The actual HW switching method differs between the various

	 * sub architectures. Out of line for now

 SPDX-License-Identifier: GPL-2.0-or-later

 We don't support the 4K PFN hack with ioremap */

	/*

	 * Choose an address to map it to. Once the vmalloc system is running,

	 * we use it. Before that, we map using addresses going up from

	 * ioremap_bot.  vmalloc will use the addresses from IOREMAP_BASE

	 * through ioremap_bot.

/*

 * Unmap an IO region and remove it from vmalloc'd list.

 * Access to IO memory should be serialized by driver.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  This file contains pgtable related functions for 64-bit machines.

 *

 *  Derived from arch/ppc64/mm/init.c

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@samba.org)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 *

 *  Dave Engebretsen <engebret@us.ibm.com>

 *      Rework for PPC64 port.

/*

 * partition table and process table for ISA 3.0

/*

 * page table size

 4 level page table */

/*

 * For hugepage we have pfn in the pmd, we use PTE_RPN_SHIFT bits for flags

 * For PTE page, we have a PTE_FRAG_SIZE (4K) aligned virtual address.

 mark_initmem_nx() should have already run by now

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  flexible mmap layout support

 *

 * Copyright 2003-2004 Red Hat Inc., Durham, North Carolina.

 * All Rights Reserved.

 *

 * Started by Ingo Molnar <mingo@elte.hu>

/*

 * Top of mmap area (just below the process stack).

 *

 * Leave at least a ~128 MB hole.

 8MB for 32bit, 1GB for 64bit */

 Values close to RLIM_INFINITY can overflow. */

/*

 * Same function as generic code used only for radix, because we don't need to overload

 * the generic one. But we will have to duplicate, because hash select

 * HAVE_ARCH_UNMAPPED_AREA

	/*

	 * A failed mmap() very likely causes application failure,

	 * so fall back to the bottom-up function here. This scenario

	 * can happen with large stack limits and large mmap()

	 * allocations.

 dummy */

/*

 * This function, called very early during the creation of a new

 * process VM image, sets up which VM layout function to use:

	/*

	 * Fall back to the standard layout if the personality

	 * bit is set, or if the expected stack growth is unlimited:

 SPDX-License-Identifier: GPL-2.0

/*

 * MMU-generic set_memory implementation for powerpc

 *

 * Copyright 2019-2021, IBM Corporation.

/*

 * Updates the attributes of a page in three steps:

 *

 * 1. take the page_table_lock

 * 2. install the new entry with the updated attributes

 * 3. flush the TLB

 *

 * This sequence is safe against concurrent updates, and also allows updating the

 * attributes of a page currently being executed or accessed.

 modify the PTE bits as desired, then apply */

 See ptesync comment in radix__set_pte_at() */

	/*

	 * On hash, the linear mapping is not in the Linux page table so

	 * apply_to_existing_page_range() will have no effect. If in the future

	 * the set_memory_* functions are used on the linear map this will need

	 * to be updated.

/*

 * Set the attributes of a page:

 *

 * This function is used by PPC32 at the end of init to set final kernel memory

 * protection. It includes changing the maping of the page it is executing from

 * and data pages it is using.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * pSeries NUMA support

 *

 * Copyright (C) 2002 Anton Blanchard <anton@au.ibm.com>, IBM

/*

 * Allocate node_to_cpumask_map based on number of available nodes

 * Requires node_possible_map to be valid.

 *

 * Note: cpumask_of_node() is not valid until after this is done.

 setup nr_node_ids if not done yet */

 allocate the map */

 cpumask_of_node() will now work */

	/*

	 * Modify node id, iff we started creating NUMA nodes

	 * We want to continue from where we left of the last time

	/*

	 * In case there are no more arguments to parse, the

	 * node_id should be the same as the last fake node id

	 * (we've handled this above).

		/*

		 * Skip commas and spaces

 CONFIG_HOTPLUG_CPU || CONFIG_PPC_SPLPAR */

	/*

	 * primary_domain_index is 1 based array index.

 POWER4 LPAR uses 0xffff as invalid node */

/*

 * Returns nid in the range [0..nr_node_ids], or -1 if no useful NUMA

 * info is found.

 Skip the first element in the associativity array */

 We should not get called with FORM0 */

 must hold reference to node during call */

 Double the distance for each NUMA level */

/* Returns the nid associated with the given device tree node,

 * or -1 if not found.

 Walk the device tree upwards, looking for an associativity id */

			/*

			 * broken hierarchy, return with broken distance table

 Skip the first element in the associativity array */

/*

 * Used to update distance information w.r.t newly added node.

 FORM2 affinity  */

	/*

	 * With FORM2 we expect NUMA distance of all possible NUMA

	 * nodes to be provided during boot.

/*

 * ibm,numa-lookup-index-table= {N, domainid1, domainid2, ..... domainidN}

 * ibm,numa-distance-table = { N, 1, 2, 4, 5, 1, 6, .... N elements}

 first element of the array is the size and is encode-int */

 Skip the size which is encoded int */

 +1 skip the max_numa_index in the property */

 don't use it

	/*

	 * Check for which form of affinity.

	/*

	 * This property is a set of 32-bit integers, each representing

	 * an index into the ibm,associativity nodes.

	 *

	 * With form 0 affinity the first integer is for an SMP configuration

	 * (should be all 0's) and the second is for a normal NUMA

	 * configuration. We have only one level of NUMA.

	 *

	 * With form 1 affinity the first integer is the most significant

	 * NUMA boundary and the following are progressively less significant

	 * boundaries. There can be more than one level of NUMA.

		/*

		 * Both FORM1 and FORM2 affinity find the primary domain details

		 * at the same offset.

	/*

	 * Warn and cap if the hardware supports more than

	 * MAX_DISTANCE_REF_POINTS domains.

/*

 * Retrieve and validate the list of associativity arrays for drconf

 * memory from the ibm,associativity-lookup-arrays property of the

 * device tree..

 *

 * The layout of the ibm,associativity-lookup-arrays property is a number N

 * indicating the number of associativity arrays, followed by a number M

 * indicating the size of each associativity array, followed by a list

 * of N associativity arrays.

	/* Now that we know the number of arrays and size of each array,

	 * revalidate the size of the property read in.

			/*

			 * lookup array associativity entries have

			 * no length of the array as the first element.

/*

 * This is like of_node_to_nid_single() for memory represented in the

 * ibm,dynamic-reconfiguration-memory node.

	/*

	 * On a shared lpar, device tree will not have node associativity.

	 * At this time lppaca, or its __old_status field may not be

	 * updated. Hence kernel cannot detect if its on a shared lpar. So

	 * request an explicit associativity irrespective of whether the

	 * lpar is shared or dedicated. Use the device tree property as a

	 * fallback. cpu_to_phys_id is only valid between

	 * smp_setup_cpu_maps() and smp_setup_pacas().

 CONFIG_PPC_SPLPAR */

/*

 * Figure out to which domain a cpu belongs and stick it there.

 * Return the id of the domain used.

	/*

	 * If a valid cpu-to-node mapping is already available, use it

	 * directly instead of querying the firmware, since it represents

	 * the most recent mapping notified to us by the platform (eg: VPHN).

	 * Since cpu_to_node binding remains the same for all threads in the

	 * core. If a valid cpu-to-node mapping is already available, for

	 * the first thread in the core, use it.

	/*

	 * Update for the first thread of the core. All threads of a core

	 * have to be part of the same node. This not only avoids querying

	 * for every other thread in the core, but always avoids a case

	 * where virtual node associativity change causes subsequent threads

	 * of a core to be associated with different nid. However if first

	 * thread is already online, expect it to have a valid mapping.

 Verify that all the threads in the core belong to the same node */

 Must run before sched domains notifier. */

/*

 * Check and possibly modify a memory region to enforce the memory limit.

 *

 * Returns the size the region should have to enforce the memory limit.

 * This will either be the original value of size, a truncated value,

 * or zero. If the returned value of size is 0 the region should be

 * discarded as it lies wholly above the memory limit.

	/*

	 * We use memblock_end_of_DRAM() in here instead of memory_limit because

	 * we've already adjusted it for the limit and it takes care of

	 * having memory holes below the limit.  Also, in the case of

	 * iommu_is_off, memory_limit is not set but is implicitly enforced.

/*

 * Reads the counter for a given entry in

 * linux,drconf-usable-memory property

	/*

	 * For each lmb in ibm,dynamic-memory a corresponding

	 * entry in linux,drconf-usable-memory property contains

	 * a counter followed by that many (base, size) duple.

	 * read the counter from linux,drconf-usable-memory

/*

 * Extract NUMA information from the ibm,dynamic-reconfiguration-memory

 * node.  This assumes n_mem_{addr,size}_cells have been set.

	/*

	 * Skip this block if the reserved bit is set in flags (0x80)

	 * or if the block is not assigned to this partition (0x8)

 there are no (base, size) duple */

		/*

		 * if we fail to parse primary_domain_index from device tree

		 * mark the numa disabled, boot with numa disabled.

	/*

	 * If it is FORM2 initialize the distance table here.

	/*

	 * Even though we connect cpus to numa domains later in SMP

	 * init, we need to know the node ids now. This is because

	 * each node to be onlined must have NODE_DATA etc backing it.

			/*

			 * Don't fall back to default_nid yet -- we will plug

			 * cpus into nodes once the memory scan has discovered

			 * the topology.

 ranges in cell */

 these are order-sensitive, and modify the buffer pointer */

		/*

		 * Assumption: either all memory nodes or none will

		 * have associativity properties.  If none, then

		 * everything goes to default_nid.

	/*

	 * Now do the same thing for each MEMBLOCK listed in the

	 * ibm,dynamic-memory property in the

	 * ibm,dynamic-reconfiguration-memory node.

		/*

		 * If we used a CPU iterator here we would miss printing

		 * the holes in the cpumap.

 Initialize NODE_DATA for a node on the local memory */

 report and initialize */

	/*

	 * ibm,current-associativity-domains is a fairly recent property. If

	 * it doesn't exist, then fallback on ibm,max-associativity-domains.

	 * Current denotes what the platform can support compared to max

	 * which denotes what the Hypervisor can support.

	 *

	 * If the LPAR is migratable, new nodes might be activated after a LPM,

	 * so we should consider the max number in that case.

	/*

	 * Linux/mm assumes node 0 to be online at boot. However this is not

	 * true on PowerPC, where node 0 is similar to any other node, it

	 * could be cpuless, memoryless node. So force node 0 to be offline

	 * for now. This will prevent cpuless, memoryless node 0 showing up

	 * unnecessarily as online. If a node has cpus or memory that need

	 * to be online, then node will anyway be marked online.

	/*

	 * Modify the set of possible NUMA nodes to reflect information

	 * available about the set of online nodes, and the set of nodes

	 * that we expect to make use of for this platform's affinity

	 * calculations.

		/*

		 * Powerpc with CONFIG_NUMA always used to have a node 0,

		 * even if it was memoryless or cpuless. For all cpus that

		 * are possible but not present, cpu_to_node() would point

		 * to node 0. To remove a cpuless, memoryless dummy node,

		 * powerpc need to make sure all possible but not present

		 * cpu_to_node are set to a proper node.

	/*

	 * We need the numa_cpu_lookup_table to be accurate for all CPUs,

	 * even before we online them, so that we can use cpu_to_{node,mem}

	 * early in boot, cf. smp_prepare_cpus().

	 * _nocalls() + manual invocation is used because cpuhp is not yet

	 * initialized for the boot CPU.

/*

 * Find the node associated with a hot added memory section for

 * memory represented in the device tree by the property

 * ibm,dynamic-reconfiguration-memory/ibm,dynamic-memory.

		/* skip this block if it is reserved or not assigned to

/*

 * Find the node associated with a hot added memory section for memory

 * represented in the device tree as a node (i.e. memory@XXXX) for

 * each memblock.

 ranges in cell */

/*

 * Find the node associated with a hot added memory section.  Section

 * corresponds to a SPARSEMEM section, not an MEMBLOCK.  It is assumed that

 * sections are fully contained within a single MEMBLOCK.

/*

 * memory_hotplug_max - return max address of memory that may be added

 *

 * This is currently only used on systems that support drconfig memory

 * hotplug.

 CONFIG_MEMORY_HOTPLUG */

 Virtual Processor Home Node (VPHN) support */

/*

 * Retrieve the new associativity information for a virtual processor's

 * home node.

 Use associativity from first thread for all siblings */

		/*

		 * Need to ensure that NODE_DATA is initialized for a node from

		 * available memory (see memblock_alloc_try_nid). If unable to

		 * init the node, then default to nearest node that has memory

		 * installed. Skip onlining a node if the subsystems are not

		 * yet initialized.

		/*

		 * Default to using the nearest node that has memory installed.

		 * Otherwise, it would be necessary to patch the kernel MM code

		 * to deal with more memoryless-node error conditions.

 CONFIG_PPC_SPLPAR */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  PowerPC version

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Derived from "arch/i386/mm/fault.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 *

 *  Modified by Cort Dougan and Paul Mackerras.

 *

 *  Modified for PPC64 by Dave Engebretsen (engebret@ibm.com)

/*

 * do_page_fault error handling helpers

	/*

	 * If we are in kernel mode, bail out with a SEGV, this will

	 * be caught by the assembly which will restore the non-volatile

	 * registers before calling bad_page_fault()

	/*

	 * Something tried to access memory that isn't in our memory map..

	 * Fix it, but check if it's kernel or user first..

	/*

	 * We don't try to fetch the pkey from page table because reading

	 * page table without locking doesn't guarantee stable pte value.

	 * Hence the pkey value that we return to userspace can be different

	 * from the pkey that actually caused access error.

	 *

	 * It does *not* guarantee that the VMA we find here

	 * was the one that we faulted on.

	 *

	 * 1. T1   : mprotect_key(foo, PAGE_SIZE, pkey=4);

	 * 2. T1   : set AMR to deny access to pkey=4, touches, page

	 * 3. T1   : faults...

	 * 4.    T2: mprotect_key(foo, PAGE_SIZE, pkey=5);

	 * 5. T1   : enters fault handler, takes mmap_lock, etc...

	 * 6. T1   : reaches here, sees vma_pkey(vma)=5, when we really

	 *	     faulted on a pte with its pkey=4.

	/*

	 * If we are in kernel mode, bail out with a SEGV, this will

	 * be caught by the assembly which will restore the non-volatile

	 * registers before calling bad_page_fault()

 shutup gcc */

	/*

	 * Kernel page fault interrupted by SIGKILL. We have no reason to

	 * continue processing.

 Out of memory */

		/*

		 * We ran out of memory, or some other thing happened to us that

		 * made us unable to handle the page fault gracefully.

 Is this a bad kernel fault ? */

 Kernel exec fault is always bad

 Kernel fault on kernel address is bad

 Read/write fault blocked by KUAP is bad, it can never succeed.

 Fault on user outside of certain regions (eg. copy_tofrom_user()) is bad

 Read/write fault in a valid region (the exception table search passed

 above), but blocked by KUAP is bad, it can never succeed.

 What's left? Kernel fault on user and allowed by KUAP in the faulting context.

	/*

	 * Make sure to check the VMA so that we do not perform

	 * faults just to hit a pkey fault as soon as we fill in a

	 * page. Only called for current mm, hence foreign == 0

	/*

	 * Allow execution from readable areas if the MMU does not

	 * provide separate controls over reading and executing.

	 *

	 * Note: That code used to not be enabled for 4xx/BookE.

	 * It is now as I/D cache coherency for these is done at

	 * set_pte_at() time and I see no reason why the test

	 * below wouldn't be valid on those processors. This -may-

	 * break programs compiled with a really old ABI though.

	/*

	 * We should ideally do the vma pkey access check here. But in the

	 * fault path, handle_mm_fault() also does the same check. To avoid

	 * these multiple checks, we skip it here and handle access error due

	 * to pkeys later.

 CONFIG_PPC_SMLPAR */

	/*

	 * Userspace trying to access kernel address, we get PROTFAULT for that.

	/*

	 * For hash translation mode, we should never get a

	 * PROTFAULT. Any update to pte to reduce access will result in us

	 * removing the hash page table entry, thus resulting in a DSISR_NOHPTE

	 * fault instead of DSISR_PROTFAULT.

	 *

	 * A pte update to relax the access will not result in a hash page table

	 * entry invalidate and hence can result in DSISR_PROTFAULT.

	 * ptep_set_access_flags() doesn't do a hpte flush. This is why we have

	 * the special !is_write in the below conditional.

	 *

	 * For platforms that doesn't supports coherent icache and do support

	 * per page noexec bit, we do setup things such that we do the

	 * sync between D/I cache via fault. But that is handled via low level

	 * hash fault code (hash_page_do_lazy_icache()) and we should not reach

	 * here in such case.

	 *

	 * For wrong access that can result in PROTFAULT, the above vma->vm_flags

	 * check should handle those and hence we should fall to the bad_area

	 * handling correctly.

	 *

	 * For embedded with per page exec support that doesn't support coherent

	 * icache we do get PROTFAULT and we handle that D/I cache sync in

	 * set_pte_at while taking the noexec/prot fault. Hence this is WARN_ON

	 * is conditional for server MMU.

	 *

	 * For radix, we can get prot fault for autonuma case, because radix

	 * page table will have them marked noaccess for user.

/*

 * Define the correct "is_write" bit in error_code based

 * on the processor family

/*

 * For 600- and 800-family processors, the error_code parameter is DSISR

 * for a data fault, SRR1 for an instruction fault.

 * For 400-family processors the error_code parameter is ESR for a data fault,

 * 0 for an instruction fault.

 * For 64-bit processors, the error_code parameter is DSISR for a data access

 * fault, SRR1 & 0x08000000 for an instruction access fault.

 *

 * The return value is 0 if the fault was handled, or the signal

 * number if this is a kernel fault that can't be handled here.

 Additional sanity check(s) */

	/*

	 * The kernel should never take an execute fault nor should it

	 * take a page fault to a kernel address or a page fault to a user

	 * address outside of dedicated places

	/*

	 * If we're in an interrupt, have no user context or are running

	 * in a region with pagefaults disabled then we must not take the fault

	/*

	 * We want to do this outside mmap_lock, because reading code around nip

	 * can result in fault, which will cause a deadlock when called with

	 * mmap_lock held

	/* When running in the kernel we expect faults to occur only to

	 * addresses in user space.  All other faults represent errors in the

	 * kernel and should generate an OOPS.  Unfortunately, in the case of an

	 * erroneous fault occurring in a code path which already holds mmap_lock

	 * we will deadlock attempting to validate the fault against the

	 * address space.  Luckily the kernel only validly references user

	 * space from well defined areas of code, which are listed in the

	 * exceptions table.

	 *

	 * As the vast majority of faults will be valid we will only perform

	 * the source reference check when there is a possibility of a deadlock.

	 * Attempt to lock the address space, if we cannot we then validate the

	 * source.  If this is invalid we can skip the address space check,

	 * thus avoiding the deadlock.

		/*

		 * The above down_read_trylock() might have succeeded in

		 * which case we'll have missed the might_sleep() from

		 * down_read():

	/*

	 * If for any reason at all we couldn't handle the fault,

	 * make sure we exit gracefully rather than endlessly redo

	 * the fault.

	/*

	 * Handle the retry right now, the mmap_lock has been released in that

	 * case.

	/*

	 * Major/minor page fault accounting.

 Same as do_page_fault but interrupt entry has already run in do_hash_fault */

/*

 * bad_page_fault is called when we have a bad access from the kernel.

 * It is called from the DSI and ISI handlers in head.S and from some

 * of the procedures in traps.c.

 kernel has accessed a bad area */

 Are we prepared to handle this fault?  */

 SPDX-License-Identifier: GPL-2.0-or-later

	/*

	 * Choose an address to map it to.

	 * Once the vmalloc system is running, we use it.

	 * Before then, we use space going down from IOREMAP_TOP

	 * (ioremap_bot records where we're up to).

	/*

	 * If the address lies within the first 16 MB, assume it's in ISA

	 * memory space

	/*

	 * Don't allow anybody to remap normal RAM that we're using.

	 * mem_init() sets high_memory so only do the check after that.

	/*

	 * Is it already mapped?  Perhaps overlapped by a previous

	 * mapping.

	/*

	 * Should check if it is a candidate for a BAT mapping

	/*

	 * If mapped by BATs then there is nothing to do.

	 * Calling vfree() generates a benign warning.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  PowerPC version

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 *

 *  Dave Engebretsen <engebret@us.ibm.com>

 *      Rework for PPC64 port.

/*

 * Given an address within the vmemmap, determine the page that

 * represents the start of the subsection it is within.  Note that we have to

 * do this by hand as the proffered address may not be correctly aligned.

 * Subtraction of non-aligned pointers produces undefined results.

 Return the pfn of the start of the section. */

/*

 * Since memory is added in sub-section chunks, before creating a new vmemmap

 * mapping, the kernel should check whether there is an existing memmap mapping

 * covering the new subsection added. This is needed because kernel can map

 * vmemmap area using 16MB pages which will cover a memory range of 16G. Such

 * a range covers multiple subsections (2M)

 *

 * If any subsection in the 16G range mapped by vmemmap is valid we consider the

 * vmemmap populated (There is a page table entry already present). We can't do

 * a page table lookup here because with the hash translation we don't keep

 * vmemmap details in linux page table.

		/*

		 * pfn valid check here is intended to really check

		 * whether we have any subsection already initialized

		 * in this range.

/*

 * vmemmap virtual address space management does not have a traditonal page

 * table to track which virtual struct pages are backed by physical mapping.

 * The virtual to physical mappings are tracked in a simple linked list

 * format. 'vmemmap_list' maintains the entire vmemmap physical mapping at

 * all times where as the 'next' list maintains the available

 * vmemmap_backing structures which have been deleted from the

 * 'vmemmap_global' list during system runtime (memory hotplug remove

 * operation). The freed 'vmemmap_backing' structures are reused later when

 * new requests come in without allocating fresh memory. This pointer also

 * tracks the allocated 'vmemmap_backing' structures as we allocate one

 * full page memory at a time when we dont have any.

/*

 * The same pointer 'next' tracks individual chunks inside the allocated

 * full page during the boot time and again tracks the freeed nodes during

 * runtime. It is racy but it does not happen as they are separated by the

 * boot process. Will create problem if some how we have memory hotplug

 * operation during boot !!

 get from freed entries first */

 allocate a page when required and hand out chunks */

 Align to the page size of the linear mapping. */

		/*

		 * This vmemmap range is backing different subsections. If any

		 * of that subsection is marked valid, that means we already

		 * have initialized a page table covering this range and hence

		 * the vmemmap range is populated.

		/*

		 * Allocate from the altmap first if we have one. This may

		 * fail due to alignment issues when using 16MB hugepages, so

		 * fall back to system memory if the altmap allocation fail.

			/*

			 * If we don't populate vmemap list, we don't have

			 * the ability to free the allocated vmemmap

			 * pages in section_deactivate. Hence free them

			 * here.

 look for it with prev pointer recorded */

 remove it from vmemmap_list */

 remove head */

 next point to this freed entry */

		/*

		 * We have already marked the subsection we are trying to remove

		 * invalid. So if we want to remove the vmemmap range, we

		 * need to make sure there is no subsection marked valid

		 * in this range.

 allocated from bootmem */

				/*

				 * this shouldn't happen, but if it is

				 * the case, leave the memory there

 CONFIG_SPARSEMEM_VMEMMAP */

/*

 * If we're running under a hypervisor, we need to check the contents of

 * /chosen/ibm,architecture-vec-5 to see if the hypervisor is willing to do

 * radix.  If not, we clear the radix feature bit so we fall back to hash.

 Check for supported configuration */

 Hypervisor only supports radix - check enabled && GTSE */

 Do radix anyway - the hypervisor said we had to */

 Hypervisor only supports hash - disable radix */

 Disable radix mode based on kernel command line. */

	/*

	 * Check /chosen/ibm,architecture-vec-5 if running as a guest.

	 * When running bare-metal, we can use radix if we like

	 * even though the ibm,architecture-vec-5 property created by

	 * skiboot doesn't have the necessary bits set.

		/*

		 * We have finalized the translation we are going to use by now.

		 * Radix mode is not limited by RMA / VRMA addressing.

		 * Hence don't limit memblock allocations.

 CONFIG_PPC_BOOK3S_64 */

/*

 * PPC Huge TLB Page Support for Kernel.

 *

 * Copyright (C) 2003 David Gibson, IBM Corporation.

 * Copyright (C) 2011 Becky Bruce, Freescale Semiconductor

 *

 * Based on the IA-32 version:

 * Copyright (C) 2002, Rohit Seth <rohit.seth@intel.com>

	/*

	 * Only called for hugetlbfs pages, hence can ignore THP and the

	 * irq disabled walk.

	/*

	 * Make sure other cpus find the hugepd set only after a

	 * properly initialized page table is visible to them.

	 * For more details look for comment in __pte_alloc().

	/*

	 * We have multiple higher-level entries that point to the same

	 * actual pte location.  Fill in each as we go and backtrack on error.

	 * We need all of these so the DTLB pgtable walk code can find the

	 * right higher-level entry without knowing if it's a hugepage or not.

 If we bailed from the for loop early, an error occurred, clean up */

/*

 * At this point we do the placement change only for BOOK3S 64. This would

 * possibly work on other subarchs.

 16GB huge page */

		/*

		 * We need to use hugepd table

 16MB hugepage */

/*

 * Tracks gpages after the device tree is scanned and before the

 * huge_boot_pages list is ready on pseries.

/*

 * Build list of addresses of gigantic pages.  This function is used in early

 * boot before the buddy allocator is setup.

 Return true when the entry to be freed maps more than the area being freed */

 Note: On fsl the hpdp may be the first of several */

			/*

			 * if it is not hugepd pointer, we should already find

			 * it cleared.

		/*

		 * Increment next by the size of the huge mapping since

		 * there may be more than one entry at this level for a

		 * single hugepage, but all of them point to

		 * the same kmem cache that holds the hugepte.

			/*

			 * Increment next by the size of the huge mapping since

			 * there may be more than one entry at this level for a

			 * single hugepage, but all of them point to

			 * the same kmem cache that holds the hugepte.

/*

 * This function frees user-level page tables of a process.

	/*

	 * Because there are a number of different possible pagetable

	 * layouts for hugepage ranges, we limit knowledge of how

	 * things should be laid out to the allocation path

	 * (huge_pte_alloc(), above).  Everything else works out the

	 * structure as it goes from information in the hugepd

	 * pointers.  That means that we can't here use the

	 * optimization used in the normal page free_pgd_range(), of

	 * checking whether we're actually covering a large enough

	 * range to have to do anything at the top level of the walk

	 * instead of at the bottom.

	 *

	 * To make sense of this, you should probably go read the big

	 * block comment at the top of the normal free_pgd_range(),

	 * too.

			/*

			 * Increment next by the size of the huge mapping since

			 * there may be more than one entry at the pgd level

			 * for a single hugepage, but all of them point to the

			 * same kmem cache that holds the hugepte.

	/*

	 * hugepage directory entries are protected by mm->page_table_lock

	 * Use this instead of huge_pte_lockptr

 With radix we don't use slice, so derive it from vma*/

	/* Check that it is a page size supported by the hardware and

		/*

		 * if we have pdshift and shift value same, we don't

		 * use pgt cache for hugepd.

		/*

		 * For pseries we do use ibm,expected#pages for reserving 16G pages.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Dynamic reconfiguration memory support

 *

 * Copyright 2017 IBM Corporation

	/*

	 * Return the value of the lmb flags field minus the reserved

	 * bit used internally for hotplug processing.

 First pass, determine how many LMB sets are needed. */

 Second pass, populate the LMB set data */

 Start of first LMB set */

 end of one set, start of another */

 close out last LMB set */

	/*

	 * Set in_drmem_update to prevent the notifier callback to process the

	 * DT property back since the change is coming from the LMB tree.

 skip reserved field */

 Get the address & size cells */

/*

 * Update the LMB associativity index.

/*

 * Update the LMB associativity index.

 *

 * This needs to be called when the hypervisor is updating the

 * dynamic-reconfiguration-memory node property.

	/*

	 * Don't update the LMBs if triggered by the update done in

	 * drmem_update_dt(), the LMB values have been used to the update the DT

	 * property in that case.

/*

 * Returns the property linux,drconf-usable-memory if

 * it exists (the property exists only in kexec/kdump kernels,

 * added by kexec-tools)

 Get the address & size cells */

 first pass, calculate the number of LMBs */

 second pass, read in the LMB information */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * This file contains common routines for dealing with free of page tables

 * Along with common page table handling code

 *

 *  Derived from arch/powerpc/mm/tlb_64.c:

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 *

 *  Dave Engebretsen <engebret@us.ibm.com>

 *      Rework for PPC64 port.

/* We only try to do i/d cache coherency on stuff that looks like

 * reasonably "normal" PTEs. We currently require a PTE to be present

 * and we avoid _PAGE_SPECIAL and cache inhibited pte. We also only do that

 * on userspace PTEs

/* Server-style MMU handles coherency when hashing if HW exec permission

 * is supposed per page (currently 64-bit only). If not, then, we always

 * flush the cache for valid PTEs in set_pte. Embedded CPU without HW exec

 * support falls into the same category.

 CONFIG_PPC_BOOK3S */

 CONFIG_PPC_BOOK3S */

/* Embedded type MMU with HW exec support. This is a bit more complicated

 * as we don't have two bits to spare for _PAGE_EXEC and _PAGE_HWEXEC so

 * instead we "filter out" the exec permission for non clean pages.

 No exec permission in the first place, move on */

 If you set _PAGE_EXEC on weird pages you're on your own */

 If the page clean, we move on */

 If it's an exec fault, we flush the cache and make it clean */

 Else, we filter out _PAGE_EXEC */

	/* So here, we only care about exec faults, as we use them

	 * to recover lost _PAGE_EXEC and perform I$/D$ coherency

	 * if necessary. Also if _PAGE_EXEC is already set, same deal,

	 * we just bail out

	/* So this is an exec fault, _PAGE_EXEC is not set. If it was

	 * an error we would have bailed out earlier in do_page_fault()

	 * but let's make sure of it

 CONFIG_DEBUG_VM */

 If you set _PAGE_EXEC on weird pages you're on your own */

 If the page is already clean, we move on */

 Clean the page and set PG_dcache_clean */

/*

 * set_pte stores a linux PTE into the linux page table.

	/*

	 * Make sure hardware valid bit is not set. We don't do

	 * tlb flush for this update.

	/* Note: mm->context.id might not yet have been assigned as

	 * this context might not have been activated yet when this

	 * is called.

 Perform the setting of the PTE */

/*

 * This is called when relaxing access to a PTE. It's also called in the page

 * fault path when we don't hit any of the major fault cases, ie, a minor

 * update of _PAGE_ACCESSED, _PAGE_DIRTY, etc... The generic code will have

 * handled those two for us, we additionally deal with missing execute

 * permission here on some processors

	/*

	 * The "return 1" forces a call of update_mmu_cache, which will write a

	 * TLB entry.  Without this, platforms that don't do a write of the TLB

	 * entry in the TLB miss handler asm will fault ad infinitum.

		/*

		 * Not used on non book3s64 platforms.

		 * 8xx compares it with mmu_virtual_psize to

		 * know if it is a huge page or not.

	/*

	 * Make sure hardware valid bit is not set. We don't do

	 * tlb flush for this update.

 CONFIG_HUGETLB_PAGE */

	/*

	 * khugepaged to collapse normal pages to hugepage, first set

	 * pmd to none to force page fault/gup to take mmap_lock. After

	 * pmd is set to none, we do a pte_clear which does this assertion

	 * so if we find pmd none, return.

 CONFIG_DEBUG_VM */

/*

 * We have 4 cases for pgds and pmds:

 * (1) invalid (all zeroes)

 * (2) pointer to next table, as normal; bottom 6 bits == 0

 * (3) leaf pte for huge page _PAGE_PTE set

 * (4) hugepd pointer, _PAGE_PTE = 0 and bits [2..6] indicate size of table

 *

 * So long as we atomically load page table pointers we are safe against teardown,

 * we can follow the address down to the the page and take a ref on it.

 * This function need to be called with interrupts disabled. We use this variant

 * when we have MSR[EE] = 0 but the paca->irq_soft_mask = IRQS_ENABLED

	/*

	 * Always operate on the local stack value. This make sure the

	 * value don't get updated by a parallel THP split/collapse,

	 * page fault or a page unmap. The return pte_t * is still not

	 * stable. So should be checked there for above conditions.

	 * Top level is an exception because it is folded into p4d.

	/*

	 * Even if we end up with an unmap, the pgtable will not

	 * be freed, because we do an rcu free and here we are

	 * irq disabled

	/*

	 * A hugepage collapse is captured by this condition, see

	 * pmdp_collapse_flush.

	/*

	 * A hugepage split is captured by this condition, see

	 * pmdp_invalidate.

	 *

	 * Huge page modification can be caught here too.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  PowerPC version derived from arch/arm/mm/consistent.c

 *    Copyright (C) 2001 Dan Malek (dmalek@jlc.net)

 *

 *  Copyright (C) 2000 Russell King

/*

 * make an area consistent.

		/*

		 * invalidate only when cache-line aligned otherwise there is

		 * the potential for discarding uncommitted data from the cache

 writeback only */

 writeback and invalidate */

/*

 * __dma_sync_page() implementation for systems using highmem.

 * In this case, each page of a buffer must be kmapped/kunmapped

 * in order to have a virtual address for __dma_sync(). This must

 * not sleep so kmap_atomic()/kunmap_atomic() are used.

 *

 * Note: yes, it is possible and correct to have a buffer extend

 * beyond the first page.

 Sync this buffer segment */

 Calculate next buffer segment size */

 Add the segment size to our running total */

 CONFIG_HIGHMEM */

/*

 * __dma_sync_page makes memory consistent. identical to __dma_sync, but

 * takes a struct page instead of a virtual address

 SPDX-License-Identifier: GPL-2.0-or-later

/**

 * flush_coherent_icache() - if a CPU has a coherent icache, flush it

 * Return true if the cache was flushed, false otherwise

	/*

	 * For a snooping icache, we still need a dummy icbi to purge all the

	 * prefetched instructions from the ifetch buffers. We also need a sync

	 * before the icbi to order the the actual stores to memory that might

	 * have modified instructions with the icbi.

 sync */

 sync */

/**

 * invalidate_icache_range() - Flush the icache by issuing icbi across an address range

 * @start: the start address

 * @stop: the stop address (exclusive)

 sync */

/**

 * flush_icache_range: Write any modified data cache blocks out to memory

 * and invalidate the corresponding blocks in the instruction cache

 *

 * Generic code will call this after writing memory, before executing from it.

 *

 * @start: the start address

 * @stop: the stop address (exclusive)

		/*

		 * Flash invalidate on 44x because we are passed kmapped

		 * addresses and this doesn't work for userspace pages due to

		 * the virtually tagged icache.

 sync */

/**

 * flush_dcache_icache_phys() - Flush a page by it's physical address

 * @physaddr: the physical address of the page

	/*

	 * This must remain as ASM to prevent potential memory accesses

	 * while the data MMU is disabled

/**

 * __flush_dcache_icache(): Flush a particular page from the data cache to RAM.

 * Note: this is necessary because the instruction cache does *not*

 * snoop from the data cache.

 *

 * @p: the address of the page to flush

	/*

	 * We don't flush the icache on 44x. Those have a virtual icache and we

	 * don't have access to the virtual address here (it's not the page

	 * vaddr but where it's mapped in user space). The flushing of the

	 * icache on these is handled elsewhere, when a change in the address

	 * space occurs, before returning to user space.

	/*

	 * We shouldn't have to do this, but some versions of glibc

	 * require it (ld.so assumes zero filled pages are icache clean)

	 * - Anton

	/*

	 * We should be able to use the following optimisation, however

	 * there are two problems.

	 * Firstly a bug in some versions of binutils meant PLT sections

	 * were not marked executable.

	 * Secondly the first word in the GOT section is blrl, used

	 * to establish the GOT address. Until recently the GOT was

	 * not marked executable.

	 * - Anton

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  PowerPC version

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *  PPC44x/36-bit changes by Matt Porter (mporter@mvista.com)

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 Remove htab bolted mappings for this section of memory */

	/* Ensure all vmalloc mappings are flushed in case they also

	 * hit that section of memory

	/* Place all memblock_regions in the same node and merge contiguous

	 * memblock_regions

 mark pages that don't exist as nosave */

 CONFIG_NUMA */

/*

 * Zones usage:

 *

 * We setup ZONE_DMA to be 31-bits on all platforms and ZONE_NORMAL to be

 * everything else. GFP_DMA32 page allocations automatically fall back to

 * ZONE_DMA.

 *

 * By using 31-bit unconditionally, we can exploit zone_dma_bits to inform the

 * generic DMA mapping code.  32-bit only devices (if not handled by an IOMMU

 * anyway) will take a first dip into ZONE_NORMAL and get otherwise served by

 * ZONE_DMA.

/*

 * paging_init() sets up the page tables - in fact we've already done this.

 XXX gross */

 XXX gross */

 CONFIG_HIGHMEM */

	/*

	 * Allow 30-bit DMA for very limited Broadcom wifi chips on many

	 * powerbooks.

	/*

	 * book3s is limited to 16 page sizes due to encoding this in

	 * a 4-bit field for slices.

	/*

	 * Some platforms (e.g. 85xx) limit DMA-able memory way below

	 * 4G. We force memblock to bottom-up mode to ensure that the

	 * memory allocated in swiotlb_init() is DMA-able.

	 * As it's the last memblock allocation, no need to reset it

	 * back to to-down.

 CONFIG_HIGHMEM */

	/*

	 * If smp is enabled, next_tlbcam_idx is initialized in the cpu up

	 * functions.... do it here for the non-smp case.

 CONFIG_HIGHMEM */

 CONFIG_PPC32 */

/*

 * System memory should not be in /proc/iomem but various tools expect it

 * (eg kdump).

			/*

			 * In memblock, end points to the first byte after

			 * the range while in resourses, end points to the

			 * last byte in the range.

/*

 * devmem_is_allowed(): check to see if /dev/mem access to a certain address

 * is valid. The argument is a physical page number.

 *

 * Access has to be given to non-kernel-ram areas as well, these contain the

 * PCI mmio resources as well as potential bios/acpi data regions.

 CONFIG_STRICT_DEVMEM */

/*

 * This is defined in kernel/resource.c but only powerpc needs to export it, for

 * the EHEA driver. Drop this when drivers/net/ethernet/ibm/ehea is removed.

 SPDX-License-Identifier: GPL-2.0-or-later

 writeable implies dirty for kernel addresses */

 we don't want to let _PAGE_USER and _PAGE_EXEC leak out */

 Should clean up */

/*

 * Override the generic version in mm/memremap.c.

 *

 * With hash translation, the direct-map range is mapped with just one

 * page size selected by htab_init_page_sizes(). Consult

 * mmu_psize_defs[] to determine the minimum page size alignment.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * CoProcessor (SPU/AFU) mm fault handler

 *

 * (C) Copyright IBM Deutschland Entwicklung GmbH 2007

 *

 * Author: Arnd Bergmann <arndb@de.ibm.com>

 * Author: Jeremy Kerr <jk@ozlabs.org>

/*

 * This ought to be kept in sync with the powerpc specific do_page_fault

 * function. Currently, there are a few corner cases that we haven't had

 * to handle fortunately.

		/*

		 * PROT_NONE is covered by the VMA check above.

		 * and hash should get a NOHPTE fault instead of

		 * a PROTFAULT in case fixup is needed for things

		 * like autonuma.

 Bad address */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * This file contains the routines setting up the linux page tables.

 *  -- paulus

 *

 *  Derived from arch/ppc/mm/init.c:

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 Use upper 10 bits of VA to index the first level map */

 Use middle 10 bits of VA to index the second-level map */

		/* The PTE should never be already set nor present in the

		 * hash table

/*

 * Map in a chunk of physical memory starting at start.

	/*

	 * mark .rodata as read only. Use __init_begin rather than __end_rodata

	 * to cover NOTES and EXCEPTION_TABLE.

 mark_initmem_nx() should have already run by now

 CONFIG_DEBUG_PAGEALLOC */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * This file contains the routines for TLB flushing.

 * On machines where the MMU uses a hash table to store virtual to

 * physical translations, these routines flush entries from the

 * hash table also.

 *  -- paulus

 *

 *  Derived from arch/ppc/mm/init.c:

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

/*

 * TLB flushing:

 *

 *  - flush_tlb_mm(mm) flushes the specified mm context TLB's

 *  - flush_tlb_page(vma, vmaddr) flushes one page

 *  - flush_tlb_range(vma, start, end) flushes a range of pages

 *  - flush_tlb_kernel_range(start, end) flushes kernel pages

 *

 * since the hardware hash table functions as an extension of the

 * tlb as far as the linux tables are concerned, flush it too.

 *    -- Cort

/*

 * For each address in the range, find the pte for the address

 * and check _PAGE_HASHPTE bit; if it is set, find and destroy

 * the corresponding HPTE.

/*

 * Flush all the (user) entries for the address space described by mm.

	/*

	 * It is safe to go down the mm's list of vmas when called

	 * from dup_mmap, holding mmap_lock.  It would also be safe from

	 * unmap_region or exit_mmap, but not from vmtruncate on SMP -

	 * but it seems dup_mmap is the only SMP case which gets here.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * This file contains the routines for handling the MMU on those

 * PowerPC implementations where the MMU substantially follows the

 * architecture specification.  This includes the 6xx, 7xx, 7xxx,

 * and 8260 implementations but excludes the 8xx and 4xx.

 *  -- paulus

 *

 *  Derived from arch/ppc/mm/init.c:

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

/*

 * Room for two PTE pointers, usually the kernel and current user pointers

 * to their respective root page table.

/*

 * On 32-bit PowerPC 6xx/7xx/7xxx CPUs, we use a set of 16 VSIDs

 * (virtual segment identifiers) for each context.  Although the

 * hardware supports 24-bit VSIDs, and thus >1 million contexts,

 * we only use 32,768 of them.  That is ample, since there can be

 * at most around 30,000 tasks in the system anyway, and it means

 * that we can use a bitmap to indicate which contexts are in use.

 * Using a bitmap means that we entirely avoid all of the problems

 * that we used to have when the context number overflowed,

 * particularly on SMP systems.

 *  -- paulus.

/*

 * Set up the context for a new address space.

/*

 * Free a context ID. Make sure to call this with preempt disabled!

/*

 * We're finished using the context for an address space.

/*

 * Initialize the context management stuff.

 Reserve context 0 for kernel use */

 sync */

 SPDX-License-Identifier: GPL-2.0-or-later

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * This file contains the routines for handling the MMU on those

 * PowerPC implementations where the MMU substantially follows the

 * architecture specification.  This includes the 6xx, 7xx, 7xxx,

 * and 8260 implementations but excludes the 8xx and 4xx.

 *  -- paulus

 *

 *  Derived from arch/ppc/mm/init.c:

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 8 pairs of IBAT, DBAT */

 stores address ranges mapped by BATs */

/*

 * Return PA for this VA if it is mapped by a BAT, or 0

/*

 * Return VA for a given PA or 0 if not mapped

/*

 * This function calculates the size of the larger block usable to map the

 * beginning of an area based on the start address and size of that area:

 * - max block size is 256 on 6xx.

 * - base address must be aligned to the block size. So the maximum block size

 *   is identified by the lowest bit set to 1 in the base address (for instance

 *   if base is 0x16000000, max size is 0x02000000).

 * - block size has to be a power of two. This is calculated by finding the

 *   highest bit set to 1.

/*

 * Set up one of the IBAT (block address translation) register pairs.

 * The parameters are not checked; in particular size must be a power

 * of 2 between 128k and 256M.

 Vs=1, Vp=0 */

 Vp = 1 */

 Do not set NX on VM space for modules */

/*

 * Set up one of the I/D BAT (block address translation) register pairs.

 * The parameters are not checked; in particular size must be a power

 * of 2 between 128k and 256M.

 * On 603+, only set IBAT when _PAGE_EXEC is set

 Do DBAT first */

 Vs=1, Vp=0 */

 Vp = 1 */

 G bit must be zero in IBATs */

/*

 * Preload a translation in the hash table

/*

 * This is called at the end of handling a user page fault, when the

 * fault has been handled by updating a PTE in the linux page tables.

 * We use it to preload an HPTE into the hash table corresponding to

 * the updated linux PTE.

 *

 * This must always be called with the pte lock held.

	/*

	 * We don't need to worry about _PAGE_PRESENT here because we are

	 * called with either mm->page_table_lock held or ptl lock held

 We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */

 We have to test for regs NULL since init will get here first thing at boot */

 We also avoid filling the hash if not coming from a fault */

/*

 * Initialize the hash table and patch the instructions in hashtable.S.

 64 bytes per HPTEG */

 min 64kB hash table */

	/*

	 * Allow 1 HPTE (1/8 HPTEG) for each page of memory.

	 * This is less than the recommended amount, but then

	 * Linux ain't AIX.

 round up if not power of 2 */

	/*

	 * Find some memory for the hash table.

 WARNING: Make sure nothing can trigger a KASAN check past this point */

	/*

	 * Patch up the instructions in hashtable.S:create_hpte

	/*

	 * Patch up the instructions in hashtable.S:flush_hash_page

	/* We don't currently support the first MEMBLOCK not mapping 0

	 * physical on those processors

 SPDX-License-Identifier: GPL-2.0-or-later

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Modifications by Kumar Gala (galak@kernel.crashing.org) to support

 * E500 Book E processors.

 *

 * Copyright 2004,2010 Freescale Semiconductor, Inc.

 *

 * This file contains the routines for initializing the MMU

 * on the 4xx series of chips.

 *  -- paulus

 *

 *  Derived from arch/ppc/mm/init.c:

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

/*

 * Return PA for this VA if it is mapped by a CAM, or 0

/*

 * Return VA for a given PA or 0 if not mapped

/*

 * Set up a variable-size TLB entry (tlbcam). The parameters are not checked;

 * in particular size must be a power of 4 between 4k and the max supported by

 * an implementation; max may further be limited by what can be represented in

 * an unsigned long (for example, 32-bit implementations cannot support a 4GB

 * size).

 Below is unlikely -- only for large user pages or similar */

 Convert (4^max) kB to (2^max) bytes */

 Convert (2^max) kB to (2^max) bytes */

 Calculate CAM values */

/*

 * MMU_init_hw does the chip-specific initialization of the MMU hardware.

 adjust lowmem size to __max_low_memory */

 Everything is done in mmu_mark_initmem_nx() */

 64M mapped initially according to head_fsl_booke.S */

	/*

	 * Relocatable kernel support based on processing of dynamic

	 * relocation entries. Before we get the real memstart_addr,

	 * We will compute the virt_phys_offset like this:

	 * virt_phys_offset = stext.run - kernstart_addr

	 *

	 * stext.run = (KERNELBASE & ~0x3ffffff) +

	 *				(kernstart_addr & 0x3ffffff)

	 * When we relocate, we have :

	 *

	 *	(kernstart_addr & 0x3ffffff) = (stext.run & 0x3ffffff)

	 *

	 * hence:

	 *  virt_phys_offset = (KERNELBASE & ~0x3ffffff) -

	 *                              (kernstart_addr & ~0x3ffffff)

	 *

	/*

	 * We now get the memstart_addr, then we should check if this

	 * address is the same as what the PAGE_OFFSET map to now. If

	 * not we have to change the map of PAGE_OFFSET to memstart_addr

	 * and do a second relocation.

 map a 64M area for the second relocation */

 We should never reach here */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * This file contains the routines for TLB flushing.

 * On machines where the MMU does not use a hash table to store virtual to

 * physical translations (ie, SW loaded TLBs or Book3E compilant processors,

 * this does -not- include 603 however which shares the implementation with

 * hash based processors)

 *

 *  -- BenH

 *

 * Copyright 2008,2009 Ben Herrenschmidt <benh@kernel.crashing.org>

 *                     IBM Corp.

 *

 *  Derived from arch/ppc/mm/init.c:

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

/*

 * This struct lists the sw-supported page sizes.  The hardawre MMU may support

 * other sizes not listed here.   The .ind field is only used on MMUs that have

 * indirect page table entries.

 CONFIG_FSL_BOOKE */

 This isn't used on !Book3E for now */

 CONFIG_PPC_BOOK3E_MMU */

/* The variables below are currently only used on 64-bit Book3E

 * though this will probably be made common with other nohash

 * implementations at some point

 Page size used for the linear mapping */

 Page size used for PTE pages */

 Page size used for the virtual mem map */

 HW tablewalk?  Value is PPC_HTW_* */

 Top of linear mapping */

/*

 * Number of bytes to add to SPRN_SPRG_TLB_EXFRAME on crit/mcheck/debug

 * exceptions.  This is used for bolted and e6500 TLB miss handlers which

 * do not modify this SPRG in the TLB miss code; for other TLB miss handlers,

 * this is set to zero.

 CONFIG_PPC64 */

 next_tlbcam_idx is used to round-robin tlbcam entry assignment */

/*

 * Base TLB flushing operations:

 *

 *  - flush_tlb_mm(mm) flushes the specified mm context TLB's

 *  - flush_tlb_page(vma, vmaddr) flushes one page

 *  - flush_tlb_range(vma, start, end) flushes a range of pages

 *  - flush_tlb_kernel_range(start, end) flushes kernel pages

 *

 *  - local_* variants of page and mm only apply to the current

 *    processor

/*

 * These are the base non-SMP variants of page and mm flushing

/*

 * And here are the SMP non-local implementations

/* Note on invalidations and PID:

 *

 * We snapshot the PID with preempt disabled. At this point, it can still

 * change either because:

 * - our context is being stolen (PID -> NO_CONTEXT) on another CPU

 * - we are invaliating some target that isn't currently running here

 *   and is concurrently acquiring a new PID on another CPU

 * - some other CPU is re-acquiring a lost PID for this mm

 * etc...

 *

 * However, this shouldn't be a problem as we only guarantee

 * invalidation of TLB entries present prior to this call, so we

 * don't care about the PID changing, and invalidating a stale PID

 * is generally harmless.

 Ignores smp_processor_id() even if set. */

	/*

	 * This function as well as __local_flush_tlb_page() must only be called

	 * for user contexts.

 If broadcast tlbivax is supported, use it */

 Ignores smp_processor_id() even if set in cpu_mask */

 CONFIG_SMP */

 CONFIG_SMP */

 CONFIG_PPC_47x */

/*

 * Flush kernel TLB entries in the given range

/*

 * Currently, for range flushing, we just do a full mm flush. This should

 * be optimized based on a threshold on the size of the range, since

 * some implementation can stack multiple tlbivax before a tlbsync but

 * for now, we keep it that way

/*

 * Below are functions specific to the 64-bit variant of Book3E though that

 * may change in the future

/*

 * Handling of virtual linear page tables or indirect TLB entries

 * flushing when PTE pages are freed

		/* This isn't the most optimal, ideally we would factor out the

		 * while preempt & CPU mask mucking around, or even the IPI but

		 * it will do for now

 adjust to be in terms of 4^shift Kb */

		/*

		 * We expect 4K subpage size and unrestricted indirect size.

		 * The lack of a restriction on indirect size is a Freescale

		 * extension, indicated by PSn = 0 but SPSn != 0.

 Look for supported direct sizes */

 Indirect page sizes supported ? */

	/* Now, we only deal with one IND page size for each

	 * direct size. Hopefully all implementations today are

	 * unambiguous, but we might want to be careful in the

	 * future.

 Cleanup array and print summary */

	/*

	 * If we want to use HW tablewalk, enable it by patching the TLB miss

	 * handlers to branch to the one dedicated to it.

/*

 * Early initialization of the MMU TLB code

 Set MAS4 based on page table setting */

 use a quarter of the TLBCAM for bolted linear map */

		/*

		 * Only do the mapping once per core, or else the

		 * transient mapping would cause problems.

	/* A sync won't hurt us after mucking around with

	 * the MMU configuration

	/* XXX This will have to be decided at runtime, but right

	 * now our boot and TLB miss code hard wires it. Ideally

	 * we should find out a suitable page size and patch the

	 * TLB miss code (either that or use the PACA to store

	 * the value we want)

	/* XXX This should be decided at runtime based on supported

	 * page sizes in the TLB, but for now let's assume 16M is

	 * always there and a good fit (which it probably is)

	 *

	 * Freescale booke only supports 4K pages in TLB0, so use that.

	/* XXX This code only checks for TLB 0 capabilities and doesn't

	 *     check what page size combos are supported by the HW. It

	 *     also doesn't handle the case where a separate array holds

	 *     the IND entries from the array loaded by the PT.

 Look for supported page sizes */

 Look for HW tablewalk support */

	/* Set the global containing the top of the linear mapping

	 * for use by the TLB miss code

		/*

		 * Limit memory so we dont have linear faults.

		 * Unlike memblock_set_current_limit, which limits

		 * memory available during early boot, this permanently

		 * reduces the memory available to Linux.  We need to

		 * do this because highmem is not supported on 64-bit.

 boot cpu only */

	/* On non-FSL Embedded 64-bit, we adjust the RMA size to match

	 * the bolted TLB entry. We know for now that only 1G

	 * entries are supported though that may eventually

	 * change.

	 *

	 * on FSL Embedded 64-bit, usually all RAM is bolted, but with

	 * unusual memory sizes it's possible for some RAM to not be mapped

	 * (such RAM is not used at all by Linux, since we don't support

	 * highmem on 64-bit).  We limit ppc64_rma_size to what would be

	 * mappable if this memblock is the only one.  Additional memblocks

	 * can only increase, not decrease, the amount that ends up getting

	 * mapped.  We still limit max to 1G even if we'll eventually map

	 * more.  This is due to what the early init code is set up to do.

	 *

	 * We crop it to the size of the first MEMBLOCK to

	 * avoid going over total available memory just in case...

 use a quarter of the TLBCAM for bolted linear map */

 Finally limit subsequent allocations */

 ! CONFIG_PPC64 */

 CONFIG_PPC64 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Modifications by Matt Porter (mporter@mvista.com) to support

 * PPC44x Book E processors.

 *

 * This file contains the routines for initializing the MMU

 * on the 4xx series of chips.

 *  -- paulus

 *

 *  Derived from arch/ppc/mm/init.c:

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

/* Used by the 44x TLB replacement exception handler.

 * Just needed it declared someplace.

 = 0 */

	/* The TLB miss handlers hard codes the watermark in a cmpli

	 * instruction to improve performances rather than loading it

	 * from the global variable. Thus, we patch the instructions

	 * in the 2 TLB miss handlers when updating the value

/*

 * "Pins" a 256MB TLB entry in AS0 for kernel lowmem for 44x type MMU

/*

 * "Pins" a 256MB TLB entry in AS0 for kernel lowmem for 47x type MMU

 Base rA is HW way select, way 0, bolted bit set */

 Look for a bolted entry slot */

 Insert bolted slot number */

 This is not useful on 47x but won't hurt either */

	/* Pin in enough TLBs to cover any lowmem not covered by the

 DEBUG */

	/* We don't currently support the first MEMBLOCK not mapping 0

	 * physical on those processors

 44x has a 256M TLB entry pinned at boot */

	/* Pin in enough TLBs to cover any lowmem not covered by the

	 * initial 256M mapping established in head_44x.S

	 *

	 * WARNING: This is called with only the first 256M of the

	 * linear mapping in the TLB and we can't take faults yet

	 * so beware of what this code uses. It runs off a temporary

	 * stack. current (r2) isn't initialized, smp_processor_id()

	 * will not work, current thread info isn't accessible, ...

 CONFIG_SMP */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * This file contains the routines for handling the MMU on those

 * PowerPC implementations where the MMU is not using the hash

 * table, such as 8xx, 4xx, BookE's etc...

 *

 * Copyright 2008 Ben Herrenschmidt <benh@kernel.crashing.org>

 *                IBM Corp.

 *

 *  Derived from previous arch/powerpc/mm/mmu_context.c

 *  and arch/powerpc/include/asm/mmu_context.h

 *

 * TODO:

 *

 *   - The global context lock will not scale very well

 *   - The maps should be dynamically allocated to allow for processors

 *     that support more PID bits at runtime

 *   - Implement flush_tlb_mm() by making the context stale and picking

 *     a new one

 *   - More aggressively clear stale map bits and maybe find some way to

 *     also clear mm->cpu_vm_mask bits when processes are migrated

/*

 * Room for two PTE table pointers, usually the kernel and current user

 * pointer to their respective root page table (pgdir).

/*

 * The MPC8xx has only 16 contexts. We rotate through them on each task switch.

 * A better way would be to keep track of tasks that own contexts, and implement

 * an LRU usage. That way very active tasks don't always have to pay the TLB

 * reload overhead. The kernel pages are mapped shared, so the kernel can run on

 * behalf of any task that makes a kernel entry. Shared does not mean they are

 * not protected, just that the ASID comparison is not performed. -- Dan

 *

 * The IBM4xx has 256 contexts, so we can just rotate through these as a way of

 * "switching" contexts. If the TID of the TLB is zero, the PID/TID comparison

 * is disabled, so we can use a TID of zero to represent all kernel pages as

 * shared among all contexts. -- Dan

 *

 * The IBM 47x core supports 16-bit PIDs, thus 65535 contexts. We should

 * normally never have to steal though the facility is present if needed.

 * -- BenH

/* Steal a context from a task that has one at the moment.

 *

 * This is used when we are running out of available PID numbers

 * on the processors.

 *

 * This isn't an LRU system, it just frees up each context in

 * turn (sort-of pseudo-random replacement :).  This would be the

 * place to implement an LRU scheme if anyone was motivated to do it.

 *  -- paulus

 *

 * For context stealing, we use a slightly different approach for

 * SMP and UP. Basically, the UP one is simpler and doesn't use

 * the stale map as we can just flush the local CPU

 *  -- benh

 Attempt to free next_context first and then loop until we manage */

 Pick up the victim mm */

		/* We have a candidate victim, check if it's active, on SMP

		 * we cannot steal active contexts

 Mark this mm has having no context anymore */

		/* Mark it stale on all CPUs that used this mm. For threaded

		 * implementations, we set it on all threads on each core

		 * represented in the mask. A future implementation will use

		 * a core map instead but this will do for now.

	/* This will happen if you have more CPUs than available contexts,

	 * all we can do here is wait a bit and try again

 This will cause the caller to try again */

 Pick up the victim mm */

 Mark this mm as having no context anymore */

 Flush the TLB for all contexts (not to be used on SMP) */

/* Note that this will also be called on SMP if all other CPUs are

 * offlined, which means that it may be called for cpu != 0. For

 * this to work, we somewhat assume that CPUs that are onlined

 * come up with a fully clean TLB (or are cleaned when offlined)

 Pick up the victim mm */

 Flush the TLB for that context */

 Mark this mm has having no context anymore */

 XXX This clear should ultimately be part of local_flush_tlb_mm */

		/*

		 * Register M_TWB will contain base address of level 1 table minus the

		 * lower part of the kernel PGDIR base address, so that all accesses to

		 * level 1 table are done relative to lower part of kernel PGDIR base

		 * address.

 Update context */

 sync */

 sync */

 No lockless fast path .. yet */

 Mark us active and the previous one not anymore */

 If we already have a valid assigned context, skip all that */

 We really don't have a context, let's try to acquire one */

 No more free contexts, let's try to steal one */

 We know there's at least one free context, try to find it */

	/* If that context got marked stale on this CPU, then flush the

	 * local TLB for it and unmark it before we use it

 XXX This clear should ultimately be part of local_flush_tlb_mm */

 Flick the MMU and release lock */

/*

 * Set up the context for a new address space.

	/*

	 * We have MMU_NO_CONTEXT set to be ~0. Hence check

	 * explicitly against context.id == 0. This ensures that we properly

	 * initialize context slice details for newly allocated mm's (which will

	 * have id == 0) and don't alter context slice inherited via fork (which

	 * will have id != 0).

/*

 * We're finished using the context for an address space.

	/* We don't touch CPU 0 map, it's allocated at aboot and kept

	 * around forever

 We also clear the cpu_vm_mask bits of CPUs going away */

/*

 * Initialize the context management stuff.

	/* Mark init_mm as being active on all possible CPUs since

	 * we'll get called with prev == init_mm the first time

	 * we schedule on a given CPU

	/*

	 * Allocate the maps used by context management

	/*

	 * Some processors have too few contexts to reserve one for

	 * init_mm, and require using context 0 for a normal task.

	 * Other processors reserve the use of context zero for the kernel.

	 * This code assumes FIRST_CONTEXT < 32.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * This file contains the routines for initializing the MMU

 * on the 8xx series of chips.

 *  -- christophe

 *

 *  Derived from arch/powerpc/mm/40x_mmu.c:

/*

 * Return PA for this VA if it is in an area mapped with LTLBs or fixmap.

 * Otherwise, returns 0

/*

 * Return VA for a given PA mapped with LTLBs or fixmap

 * Return 0 if not mapped

 The PTE should never be already present */

/*

 * MMU_init_hw does the chip-specific initialization of the MMU hardware.

	/* We don't currently support the first MEMBLOCK not mapping 0

	 * physical on those processors

 8xx can only access 32MB at the moment */

 SPDX-License-Identifier: GPL-2.0

/*

 * PPC Huge TLB Page Support for Book3E MMU

 *

 * Copyright (C) 2009 David Gibson, IBM Corporation.

 * Copyright (C) 2011 Becky Bruce, Freescale Semiconductor

 *

	/*

	 * Besides being unnecessary in the absence of SMT, this

	 * check prevents trying to do lbarx/stbcx. on e5500 which

	 * doesn't implement either feature.

 Just round-robin the entries and wrap when we hit the end */

	/*

	 * We can't be interrupted while we're setting up the MAS

	 * regusters or after we've confirmed that no tlb exists.

 We have to use the CAM(TLB1) on FSL parts for hugepages */

/*

 * This is called at the end of handling a user page fault, when the

 * fault has been handled by updating a PTE in the linux page tables.

 *

 * This must always be called with the pte lock held.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2005, Paul Mackerras, IBM Corporation.

 * Copyright 2009, Benjamin Herrenschmidt, IBM Corporation.

 * Copyright 2015-2016, Aneesh Kumar K.V, IBM Corporation.

/*

 * On Book3E CPUs, the vmemmap is currently mapped in the top half of

 * the vmalloc space using normal page tables, though the size of

 * pages encoded in the PTEs can be different

 Create a PTE encoding without page size */

 PTEs only contain page size encodings up to 32M */

 Encode the size in the PTE */

	/* For each PTE for that area, map things. Note that we don't

	 * increment phys because all PTEs are of the large size and

	 * thus must have the low bits clear

 CONFIG_SPARSEMEM_VMEMMAP */

/*

 * map_kernel_page currently only called by __ioremap

 * map_kernel_page adds an entry to the ioremap page table

 * and adds an entry to the HPT, possibly bolting it

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * This file contains the routines for initializing the MMU

 * on the 4xx series of chips.

 *  -- paulus

 *

 *  Derived from arch/ppc/mm/init.c:

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

/*

 * MMU_init_hw does the chip-specific initialization of the MMU hardware.

	/*

	 * The Zone Protection Register (ZPR) defines how protection will

	 * be applied to every page which is a member of a given zone. At

	 * present, we utilize only two of the 4xx's zones.

	 * The zone index bits (of ZSEL) in the PTE are used for software

	 * indicators, except the LSB.  For user access, zone 1 is used,

	 * for kernel access, zone 0 is used.  We set all but zone 1

	 * to zero, allowing only kernel access as indicated in the PTE.

	 * For zone 1, we set a 01 binary (a value of 10 will not work)

	 * to allow user access as indicated in the PTE.  This also allows

	 * kernel access as indicated in the PTE.

	/*

	 * Set up the real-mode cache parameters for the exception vector

	 * handlers (which are run in real-mode).

 All caching is write-back */

        /*

	 * Cache instruction and data space where the exception

	 * vectors and the kernel live in real-mode.

 2GByte of data space at 0x0. */

 2GByte of instr. space at 0x0. */

	/* If the size of RAM is not an exact power of two, we may not

	 * have covered RAM in its entirety with 16 and 4 MiB

	 * pages. Consequently, restrict the top end of RAM currently

	 * allocable so that calls to the MEMBLOCK to allocate PTEs for "tail"

	 * coverage with normal-sized pages (or other reasons) do not

	 * attempt to allocate outside the allowed range.

	/* We don't currently support the first MEMBLOCK not mapping 0

	 * physical on those processors

 40x can only access 16MB at the moment (see head_40x.S) */

 SPDX-License-Identifier: GPL-2.0-only



 Copyright (C) 2019 Jason Yan <yanaijie@huawei.com>

 Simplified build-specific string for starting entropy. */

 Rotate by odd number of bits and XOR. */

/* Attempt to create a simple starting entropy. This can make it defferent for

 * every build but it is still not enough. Stronger entropy should

 * be added to make it change for every boot.

 check for overlap with /memreserve/ entries */

 check for overlap with static reservations in /reserved-memory */

	/*

	 * Retrieve the #address-cells and #size-cells properties

	 * from the 'node', or use the default if not provided.

	/*

	 * Retrieve (and wipe) the seed from the FDT

 If the linear size is smaller than 64M, do not randmize */

 check for a reserved-memory node and record its cell sizes */

	/*

	 * Decide which 64M we want to start

	 * Only use the low 8 bits of the random seed

 Decide offset inside 64M */

/*

 * To see if we need to relocate the kernel to a random offset

 * void *dt_ptr - address of the device tree

 * phys_addr_t size - size of the first memory block

 Create kernel map to relocate in */

 Copy the kernel to it's new location and run */

 If randomized, clear the original kernel */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2016, Rashmica Gupta, IBM Corp.

 *

 * This traverses the kernel virtual memory and dumps the pages that are in

 * the hash pagetable, along with their flags to

 * /sys/kernel/debug/kernel_hash_pagetable.

 *

 * If radix is enabled then there is no hash page table and so no debugfs file

 * is generated.

 flag not defined so don't check it */

 Some 'flags' are actually values */

 calculate hash */

 to check in the secondary hash table, we invert the hash */

 HPTE matches */

 calculate hash */

 to check in the secondary hash table, we invert the hash */

 see if we can find an entry in the hpte with this hash */

 HPTE matches */

	/*.

	 * The LP field has 8 bits. Depending on the actual page size, some of

	 * these bits are concatenated with the APRN to get the RPN. The rest

	 * of the bits in the LP field is the LP value and is an encoding for

	 * the base page size and the actual page size.

	 *

	 *  -	find the mmu entry for our base page size

	 *  -	go through all page encodings and use the associated mask to

	 *	find an encoding that matches our encoding in the LP field.

 Look in primary table */

 Look in secondary table */

 No entry found */

	/*

	 * We found an entry in the hash page table:

	 *  - check that this has the same base page

	 *  - find the actual page size

	 *  - find the RPN

 4K actual page size */

 In this case there are no LP bits */

	/*

	 * We didn't find a matching encoding, so the PTE we found isn't for

	 * this address.

 check for secret 4K mappings */

 check for hashpte */

 found a hpte that is not in the linux page tables */

 pmd exists */

 pud exists */

 p4d exists */

	/*

	 * Traverse the linux pagetable structure and dump pages that are in

	 * the hash pagetable.

 pgd exists */

	/*

	 * Traverse the linear mapping section of virtual memory and dump pages

	 * that are in the hash pagetable.

	/*

	 * Traverse the vmemmaped memory and dump pages that are in the hash

	 * pagetable.

	/*

	 * Traverse the 0xc, 0xd and 0xf areas of the kernel virtual memory and

	 * dump pages that are in the hash pagetable.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2016, Rashmica Gupta, IBM Corp.

 *

 * This traverses the kernel pagetables and dumps the

 * information about the used sections of memory to

 * /sys/kernel/debug/kernel_pagetables.

 *

 * Derived from the arm64 implementation:

 * Copyright (c) 2014, The Linux Foundation, Laura Abbott.

 * (C) Copyright 2008 Intel Corporation, Arjan van de Ven.

/*

 * To visualise what is happening,

 *

 *  - PTRS_PER_P** = how many entries there are in the corresponding P**

 *  - P**_SHIFT = how many bits of the address we use to index into the

 * corresponding P**

 *  - P**_SIZE is how much memory we can access through the table - not the

 * size of the table itself.

 * P**={PGD, PUD, PMD, PTE}

 *

 *

 * Each entry of the PGD points to a PUD. Each entry of a PUD points to a

 * PMD. Each entry of a PMD points to a PTE. And every PTE entry points to

 * a page.

 *

 * In the case where there are only 3 levels, the PUD is folded into the

 * PGD: every PUD has only one entry which points to the PMD.

 *

 * The page dumper groups page table entries of the same type into a single

 * description. It uses pg_state to track the range information while

 * iterating over the PTE entries. When the continuity is broken it then

 * dumps out a description of the range - ie PTEs that are virtually contiguous

 * with the same PTE flags are chunked together. This is to make it clear how

 * different areas of the kernel virtual memory are used.

 *

 Work out what appropriate unit to use */

 flag not defined so don't check it */

 Some 'flags' are actually values */

 At first no level is set */

	/*

	 * Dump the section of virtual memory when:

	 *   - the PTE flags from one entry to the next differs.

	 *   - we change levels in the tree.

	 *   - the address is in a different section of memory and is thus

	 *   used for a different purpose, regardless of the flags.

 Check the PTE flags */

 Dump all the flags */

		/*

		 * Address indicates we have passed the end of the

		 * current section of virtual memory

 What is the ifdef about? */

 !CONFIG_PPC64 */

 CONFIG_PPC64 */

 Traverse kernel page tables */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright 2018, Christophe Leroy CS S.I.

 * <christophe.leroy@c-s.fr>

 *

 * This dumps the content of Segment Registers

 SPDX-License-Identifier: GPL-2.0

/*

 * From split of dump_linuxpagetables.c

 * Copyright 2016, Rashmica Gupta, IBM Corp.

 *

 pgd */

 p4d */

 pud */

 pmd */

 pte */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright 2018, Christophe Leroy CS S.I.

 * <christophe.leroy@c-s.fr>

 *

 * This dumps the content of BATS

 SPDX-License-Identifier: GPL-2.0

/*

 * From split of dump_linuxpagetables.c

 * Copyright 2016, Rashmica Gupta, IBM Corp.

 *

 CONFIG_PPC_64K_PAGES */

 CONFIG_PPC_64K_PAGES */

 pgd */

 p4d */

 pud */

 pmd */

 pte */

 SPDX-License-Identifier: GPL-2.0

/*

 * From split of dump_linuxpagetables.c

 * Copyright 2016, Rashmica Gupta, IBM Corp.

 *

 pgd */

 p4d */

 pud */

 pmd */

 pte */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 At this point kasan is fully initialized. Enable error messages */

 SPDX-License-Identifier: GPL-2.0

/*

 * PPC64 Huge TLB Page Support for hash based MMUs (POWER4 and later)

 *

 * Copyright (C) 2003 David Gibson, IBM Corporation.

 *

 * Based on the IA-32 version:

 * Copyright (C) 2002, Rohit Seth <rohit.seth@intel.com>

 Search the Linux page table for a match with va */

	/*

	 * At this point, we have a pte (old_pte) which can be used to build

	 * or update an HPTE. There are 2 cases:

	 *

	 * 1. There is a valid (present) pte with no associated HPTE (this is

	 *	the most common case)

	 * 2. There is a valid (present) pte with an associated HPTE. The

	 *	current values of the pp bits in the HPTE prevent access

	 *	because we are doing software DIRTY bit management and the

	 *	page is currently not DIRTY.

 If PTE busy, retry the access */

 If PTE permissions don't match, take page fault */

		/*

		 * Try to lock the PTE, add ACCESSED and DIRTY if it was

		 * a write access

 Make sure this is a hugetlb entry */

		/*

		 * No CPU has hugepages but lacks no execute, so we

		 * don't need to worry about that case

 Check if pte already has an hpte (case 2) */

 There MIGHT be an HPTE for this pte */

 clear HPTE slot informations in new PTE */

		/*

		 * Hypervisor failure. Restore old pte and return -1

		 * similar to __hash_page_*

	/*

	 * No need to use ldarx/stdcx here

	/*

	 * Clear the _PAGE_PRESENT so that no hardware parallel update is

	 * possible. Also keep the pte_present true so that we don't take

	 * wrong fault.

	/* Set default large page size. Currently, we pick 16M or 1M

	 * depending on what is available

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * PowerPC64 port by Mike Corrigan and Dave Engebretsen

 *   {mikejc|engebret}@us.ibm.com

 *

 *    Copyright (c) 2000 Mike Corrigan <mikejc@us.ibm.com>

 *

 * SMP scalability work:

 *    Copyright (C) 2001 Anton Blanchard <anton@au.ibm.com>, IBM

 *

 *    Module name: htab.c

 *

 *    Description:

 *      PowerPC Hashed Page Table functions

/*

 * Note:  pte   --> Linux PTE

 *        HPTE  --> PowerPC Hashed Page Table Entry

 *

 * Execution context:

 *   htab_initialize is called with the MMU off (of course), but

 *   the kernel has been copied down to zero so it can directly

 *   reference global data.  At this point it is very difficult

 *   to print debug info.

 *

 CONFIG_DEBUG_PAGEALLOC */

/*

 * These are definitions of page sizes arrays to be used when none

 * is provided by the firmware.

/*

 * Fallback (4k pages only)

/*

 * POWER4, GPUL, POWER5

 *

 * Support for 16Mb large pages

/*

 * 'R' and 'C' update notes:

 *  - Under pHyp or KVM, the updatepp path will not set C, thus it *will*

 *     create writeable HPTEs without C set, because the hcall H_PROTECT

 *     that we use in that case will not update C

 *  - The above is however not a problem, because we also don't do that

 *     fancy "no flush" variant of eviction and we use H_REMOVE which will

 *     do the right thing and thus we don't have the race I described earlier

 *

 *    - Under bare metal,  we do have the race, so we need R and C set

 *    - We make sure R is always set and never lost

 *    - C is _PAGE_DIRTY, and *should* always be set for a writeable mapping

 _PAGE_EXEC -> NOEXEC */

	/*

	 * PPP bits:

	 * Linux uses slb key 0 for kernel and 1 for user.

	 * kernel RW areas are mapped with PPP=0b000

	 * User area is mapped with PPP=0b010 for read/write

	 * or PPP=0b011 for read-only (including writeable but clean pages).

		/*

		 * Kernel read only mapped with ppp bits 0b110

	/*

	 * We can't allow hardware to update hpte bits. Hence always

	 * set 'R' bit and set 'C' if it is a write fault

	/*

	 * Add in WIG bits

		/*

		 * Add memory coherence if cache inhibited is not set

 Carefully map only the possible range */

		/*

		 * If we hit a bad address return error.

 Make kernel text executable */

		/*

		 * If relocatable, check if it overlaps interrupt vectors that

		 * are copied down to real 0. For relocatable kernel

		 * (e.g. kdump case) we copy interrupt vectors down to real

		 * address 0. Mark that region as executable. This is

		 * because on p8 system with relocation on exception feature

		 * enabled, exceptions are raised with MMU (IR=DR=1) ON. Hence

		 * in order to execute the interrupt handlers in virtual

		 * mode the vector region need to be marked as executable.

			/*

			 * Try to to keep bolted entries in primary.

			 * Remove non bolted entries and try insert again

 CONFIG_DEBUG_PAGEALLOC */

 Unmap the full range specificied */

		/*

		 * For large number of mappings introduce a cond_resched()

		 * to prevent softlockup warnings.

 We are scanning "cpu" nodes only */

 We are scanning "cpu" nodes only */

 skip the pte encoding also */

		/*

		 * We don't know for sure what's up with tlbiel, so

		 * for now we only set it for 4K and 64K pages

/*

 * Scan for 16G memory blocks that have been set aside for huge pages

 * and reserve those blocks for 16G huge pages.

 We are scanning "memory" nodes only */

	/*

	 * This property is the log base 2 of the number of virtual pages that

	 * will represent this memory block.

 CONFIG_HUGETLB_PAGE */

	/*

	 * The HEA ethernet adapter requires awareness of the

	 * GX bus. Without that awareness we can easily assume

	 * we will never see an HEA ethernet device.

 #ifdef CONFIG_PPC_64K_PAGES */

 se the invalid penc to -1 */

 Default to 4K pages only */

	/*

	 * Try to find the available page sizes in the device-tree

		/*

		 * Nothing in the device-tree, but the CPU supports 16M pages,

		 * so let's fallback on a known size list for 16M capable CPUs.

 Reserve 16G huge page memory sections for huge pages */

 CONFIG_HUGETLB_PAGE */

/*

 * Fill in the hpte_page_sizes[] array.

 * We go through the mmu_psize_defs[] array looking for all the

 * supported base/actual page size combinations.  Each combination

 * has a unique pagesize encoding (penc) value in the low bits of

 * the LP field of the HPTE.  For actual page sizes less than 1MB,

 * some of the upper LP bits are used for RPN bits, meaning that

 * we need to fill in several entries in hpte_page_sizes[].

 *

 * In diagrammatic form, with r = RPN bits and z = page size bits:

 *        PTE LP     actual page size

 *    rrrr rrrz		>=8KB

 *    rrrr rrzz		>=16KB

 *    rrrr rzzz		>=32KB

 *    rrrr zzzz		>=64KB

 *    ...

 *

 * The zzzz bits are implementation-specific but are chosen so that

 * no encoding for a larger page size uses the same value in its

 * low-order N bits as the encoding for the 2^(12+N) byte page size

 * (if it exists).

 not a supported page size */

 should never happen */

			/*

			 * For page sizes less than 1MB, this loop

			 * replicates the entry for all possible values

			 * of the rrrr bits.

		/*

		 * Pick a size for the linear mapping. Currently, we only

		 * support 16M, 1M and 4K which is the default

	/*

	 * Pick a size for the ordinary pages. Default is 4K, we support

	 * 64K for user mappings and vmalloc if supported by the processor.

	 * We only use 64k for ioremap if the processor

	 * (and firmware) support cache-inhibited large pages.

	 * If not, we use 4k and set mmu_ci_restrictions so that

	 * hash_page knows to switch processes that use cache-inhibited

	 * mappings to 4k pages.

			/*

			 * When running on pSeries using 64k pages for ioremap

			 * would stop us accessing the HEA ethernet. So if we

			 * have the chance of ever seeing one, stay at 4k.

 CONFIG_PPC_64K_PAGES */

	/*

	 * We try to use 16M pages for vmemmap if that is supported

	 * and we have at least 1G of RAM at boot

 CONFIG_SPARSEMEM_VMEMMAP */

 We are scanning "cpu" nodes only */

 pft_size[0] is the NUMA CEC cookie */

 round mem_size up to next power of 2 */

 aim for 2 pages / pteg */

	/*

	 * 2^11 PTEGS of 128 bytes each, ie. 2^18 bytes is the minimum htab

	 * size permitted by the architecture.

	/*

	 * If hash size isn't already provided by the platform, we try to

	 * retrieve it from the device-tree. If it's not there neither, we

	 * calculate it now based on the total RAM size

	/*

	 * To avoid lots of HPT resizes if memory size is fluctuating

	 * across a boundary, we deliberately have some hysterisis

	 * here: we immediately increase the HPT size if the target

	 * shift exceeds the current shift, but we won't attempt to

	 * reduce unless the target shift is at least 2 below the

	 * current shift

 CONFIG_MEMORY_HOTPLUG */

	/*

	 * PS field (VRMA page size) is not used for LPID 0, hence set to 0.

	 * For now, UPRT is 0 and we have no segment table.

	/*

	 * Calculate the required size of the htab.  We want the number of

	 * PTEGs to equal one half the number of real pages.

 Using a hypervisor which owns the htab */

		/*

		 * If firmware assisted dump is active firmware preserves

		 * the contents of htab along with entire partition memory.

		 * Clear the htab if firmware assisted dump is active so

		 * that we dont end up using old mappings.

		/*

		 * Cell may require the hash table down low when using the

		 * Axon IOMMU in order to fit the dynamic region over it, see

		 * comments in cell/iommu.c

 CONFIG_PPC_CELL */

 htab absolute addr + encoded htabsize */

 Initialize the HPT with no entries */

 Set SDR1 */

 CONFIG_DEBUG_PAGEALLOC */

 create bolted the linear mapping in the hash table */

	/*

	 * If we have a memory_limit and we've allocated TCEs then we need to

	 * explicitly map the TCE area at the top of RAM. We also cope with the

	 * case that the TCEs start below memory_limit.

	 * tce_alloc_start/end are 16MB aligned so the mapping should work

	 * for either 4K or 16MB pages.

 Initialize segment sizes */

 Initialize page sizes */

	/*

	 * We have code in __hash_page_4K() and elsewhere, which assumes it can

	 * do the following:

	 *   new_pte |= (slot << H_PAGE_F_GIX_SHIFT) & (H_PAGE_F_SECOND | H_PAGE_F_GIX);

	 *

	 * Where the slot number is between 0-15, and values of 8-15 indicate

	 * the secondary bucket. For that code to work H_PAGE_F_SECOND and

	 * H_PAGE_F_GIX must occupy four contiguous bits in the PTE, and

	 * H_PAGE_F_SECOND must be placed above H_PAGE_F_GIX. Assert that here

	 * with a BUILD_BUG_ON().

 CONFIG_PPC_64K_PAGES */

	/*

	 * initialize page table size

	/*

	 * 4k use hugepd format, so for hash set then to

	 * zero

 Select appropriate backend */

	/*

	 * Initialize the MMU Hash table and create the linear mapping

	 * of memory. Has to be done before SLB initialization as this is

	 * currently where the page size encoding is obtained.

 Initialize SLB management */

 Initialize hash table for that CPU */

 Initialize SLB */

 CONFIG_SMP */

/*

 * Called by asm hashtable.S for doing lazy icache flush

 page is dirty */

/*

 * Demote a segment to using 4k pages.

 * For now this makes the whole process use 4k pages.

 CONFIG_PPC_64K_PAGES */

/*

 * This looks up a 2-bit protection code for a 4k subpage of a 64k page.

 * Userspace sets the subpage permissions using the subpage_prot system call.

 *

 * Result is 0: full permissions, _PAGE_RW: read-only,

 * _PAGE_RWX: no access.

 addresses below 4GB use spt->low_prot */

 extract 2-bit bitfield for this 4k subpage */

	/*

	 * 0 -> full premission

	 * 1 -> Read only

	 * 2 -> no access.

	 * We return the flag that need to be cleared.

 CONFIG_PPC_SUBPAGE_PROT */

/*

 * Result code is:

 *  0 - handled

 *  1 - normal page fault

 * -1 - critical hash insertion error

 * -2 - access not permitted by subpage protection mechanism

 Get region & vsid */

		/*

		 * Not a valid range

		 * Send the problem up to do_page_fault()

 Bad address. */

 Get pgdir */

 Check CPU locality */

	/*

	 * If we use 4K pages and our psize is not 4K, then we might

	 * be hitting a special driver mapping, and need to align the

	 * address before we fetch the PTE.

	 *

	 * It could also be a hugepage mapping, in which case this is

	 * not necessary, but it's not harmful, either.

 CONFIG_PPC_64K_PAGES */

 Get PTE and page size from page tables */

	/*

	 * Add _PAGE_PRESENT to the required access perm. If there are parallel

	 * updates to the pte that can possibly clear _PAGE_PTE, catch that too.

	 *

	 * We can safely use the return pte address in rest of the function

	 * because we do set H_PAGE_BUSY which prevents further updates to pte

	 * from generic code.

	/*

	 * Pre-check access permissions (will be re-checked atomically

	 * in __hash_page_XX but this pre-check is a fast path

			/*

			 * if we have hugeshift, and is not transhuge with

			 * hugetlb disabled, something is really wrong.

 Do actual hashing */

 If H_PAGE_4K_PFN is set, make sure this is a 4k segment */

	/*

	 * If this PTE is non-cacheable and we have restrictions on

	 * using non cacheable large pages, then we switch to 4k

			/*

			 * some driver did a non-cacheable mapping

			 * in vmalloc space, so switch vmalloc

			 * to 4k pages

 CONFIG_PPC_64K_PAGES */

 CONFIG_PPC_64K_PAGES */

	/*

	 * Dump some info in case of hash insertion failure, they should

	 * never happen so it is really useful to know if/when they do

	/*

	 * We set _PAGE_PRIVILEGED only when

	 * kernel mode access kernel space.

	 *

	 * _PAGE_PRIVILEGED is NOT set

	 * 1) when kernel mode access user space

	 * 2) user space access kernel space.

 failed to instert a hash PTE due to an hypervisor error

/*

 * The _RAW interrupt entry checks for the in_nmi() case before

 * running the full handler.

	/*

	 * If we are in an "NMI" (e.g., an interrupt when soft-disabled), then

	 * don't call hash_page, just fail the fault. This is required to

	 * prevent re-entrancy problems in the hash code, namely perf

	 * interrupts hitting while something holds H_PAGE_BUSY, and taking a

	 * hash fault. See the comment in hash_preload().

	 *

	 * We come here as a result of a DSI at a point where we don't want

	 * to call hash_page, such as when we are accessing memory (possibly

	 * user memory) inside a PMU interrupt that occurred while interrupts

	 * were soft-disabled.  We want to invoke the exception handler for

	 * the access, or panic if there isn't a handler.

 We only prefault standard pages for now */

	/*

	 * Don't prefault if subpage protection is enabled for the EA.

 Get Linux PTE if available */

 Get VSID */

	/* If either H_PAGE_4K_PFN or cache inhibited is set (and we are on

	 * a 64K kernel), then we don't preload, hash_page() will take

	 * care of it once we actually try to access the page.

	 * That way we don't have to duplicate all of the logic for segment

	 * page size demotion here

	 * Called with  PTL held, hence can be sure the value won't change in

	 * between.

 CONFIG_PPC_64K_PAGES */

	/*

	 * __hash_page_* must run with interrupts off, as it sets the

	 * H_PAGE_BUSY bit. It's possible for perf interrupts to hit at any

	 * time and may take a hash fault reading the user stack, see

	 * read_user_stack_slow() in the powerpc/perf code.

	 *

	 * If that takes a hash fault on the same page as we lock here, it

	 * will bail out when seeing H_PAGE_BUSY set, and retry the access

	 * leading to an infinite loop.

	 *

	 * Disabling interrupts here does not prevent perf interrupts, but it

	 * will prevent them taking hash faults (see the NMI test in

	 * do_hash_page), then read_user_stack's copy_from_user_nofault will

	 * fail and perf will fall back to read_user_stack_slow(), which

	 * walks the Linux page tables.

	 *

	 * Interrupts must also be off for the duration of the

	 * mm_is_thread_local test and update, to prevent preempt running the

	 * mm on another CPU (XXX: this may be racy vs kthread_use_mm).

 Is that local to this CPU ? */

 Hash it in */

 CONFIG_PPC_64K_PAGES */

	/* Dump some info in case of hash insertion failure, they should

	 * never happen so it is really useful to know if/when they do

/*

 * This is called at the end of handling a user page fault, when the

 * fault has been handled by updating a PTE in the linux page tables.

 * We use it to preload an HPTE into the hash table corresponding to

 * the updated linux PTE.

 *

 * This must always be called with the pte lock held.

	/*

	 * We don't need to worry about _PAGE_PRESENT here because we are

	 * called with either mm->page_table_lock held or ptl lock held

 We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */

	/*

	 * We try to figure out if we are coming from an instruction

	 * access fault and pass that down to __hash_page so we avoid

	 * double-faulting on execution of fresh text. We have to test

	 * for regs NULL since init will get here first thing at boot.

	 *

	 * We also avoid filling the hash if not coming from a fault.

	/*

	 * Transactions are not aborted by tlbiel, only tlbie. Without, syncing a

	 * page back to a block device w/PIO could pick up transactional data

	 * (bad!) so we force an abort here. Before the sync the page will be

	 * made read-only, which will flush_hash_page. BIG ISSUE here: if the

	 * kernel uses a page from userspace without unmapping it first, it may

	 * see the speculated version.

/*

 * Return the global hash slot, corresponding to the given PTE, which contains

 * the HPTE.

		/*

		 * We use same base page size and actual psize, because we don't

		 * use these functions for hugepage

	/*

	 * IF we try to do a HUGE PTE update after a withdraw is done.

	 * we will find the below NULL. This happens when we do

	 * split_huge_pmd

	/*

	 * No bluk hpte removal support, invalidate each entry

		/*

		 * 8 bits per each hpte entries

		 * 000| [ secondary group (one bit) | hidx (3 bits) | valid bit]

 get the vpn */

 CONFIG_TRANSPARENT_HUGEPAGE */

 Insert into the hash table, primary slot */

 Primary is full, try the secondary */

 Don't create HPTE entries for bad address */

 CONFIG_DEBUG_PAGEALLOC */

	/*

	 * We don't currently support the first MEMBLOCK not mapping 0

	 * physical on those processors

	/*

	 * On virtualized systems the first entry is our RMA region aka VRMA,

	 * non-virtualized 64-bit hash MMU systems don't have a limitation

	 * on real mode access.

	 *

	 * For guests on platforms before POWER9, we clamp the it limit to 1G

	 * to avoid some funky things such as RTAS bugs etc...

	 *

	 * On POWER9 we limit to 1TB in case the host erroneously told us that

	 * the RMA was >1TB. Effective address bits 0:23 are treated as zero

	 * (meaning the access is aliased to zero i.e. addr = addr % 1TB)

	 * for virtual real mode addressing and so it doesn't make sense to

	 * have an area larger than 1TB as it can't be addressed.

 Finally limit subsequent allocations */

 CONFIG_DEBUG_FS */

/*

 * Copyright IBM Corporation, 2013

 * Author Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of version 2.1 of the GNU Lesser General Public License

 * as published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it would be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

 *

/*

 * PPC64 THP Support for hash based MMUs

	/*

	 * atomically mark the linux large page PMD busy and dirty

 If PMD busy, retry the access */

 If PMD permissions don't match, take page fault */

		/*

		 * Try to lock the PTE, add ACCESSED and DIRTY if it was

		 * a write access

	/*

	 * Make sure this is thp or devmap entry

		/*

		 * No CPU has hugepages but lacks no execute, so we

		 * don't need to worry about that case

	/*

	 * Find the slot index details for this ea, using base page size.

		/*

		 * invalidate the old hpte entry if we have that mapped via 64K

		 * base page size. This is because demote_segment won't flush

		 * hash page table entries.

			/*

			 * With THP, we also clear the slot information with

			 * respect to all the 64K hash pte mapping the 16MB

			 * page. They are all invalid now. This make sure we

			 * don't find the slot valid when we fault with 4k

			 * base page size.

			 *

 update the hpte bits */

		/*

		 * We failed to update, try to insert a new entry.

			/*

			 * large pte is marked busy, so we can be sure

			 * nobody is looking at hpte_slot_array. hence we can

			 * safely update this here.

 insert new entry */

 Insert into the hash table, primary slot */

		/*

		 * Primary is full, try the secondary

		/*

		 * Hypervisor failure. Restore old pmd and return -1

		 * similar to __hash_page_*

		/*

		 * large pte is marked busy, so we can be sure

		 * nobody is looking at hpte_slot_array. hence we can

		 * safely update this here.

	/*

	 * Mark the pte with H_PAGE_COMBO, if we are trying to hash it with

	 * base page size 4k.

	/*

	 * The hpte valid is stored in the pgtable whose address is in the

	 * second half of the PMD. Order this against clearing of the busy bit in

	 * huge pmd.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  MMU context allocation for 64-bit kernels.

 *

 *  Copyright (C) 2004 Anton Blanchard, IBM Corp. <anton@samba.org>

	/*

	 * id 0 (aka. ctx->id) is special, we always allocate a new one, even if

	 * there wasn't one allocated previously (which happens in the exec

	 * case where ctx is newly allocated).

	 *

	 * We have to be a bit careful here. We must keep the existing ids in

	 * the array, so that we can test if they're non-zero to decide if we

	 * need to allocate a new one. However in case of error we must free the

	 * ids we've allocated but *not* any of the existing ones (or risk a

	 * UAF). That's why we decrement i at the start of the error handling

	 * loop, to skip the id that we just tested but couldn't reallocate.

 The caller expects us to return id */

	/*

	 * The old code would re-promote on fork, we don't do that when using

	 * slices as it could cause problem promoting slices that have been

	 * forced down to 4K.

	 *

	 * For book3s we have MMU_NO_CONTEXT set to be ~0. Hence check

	 * explicitly against context.id == 0. This ensures that we properly

	 * initialize context slice details for newly allocated mm's (which will

	 * have id == 0) and don't alter context slice inherited via fork (which

	 * will have id != 0).

	 *

	 * We should not be calling init_new_context() on init_mm. Hence a

	 * check against 0 is OK.

 This is fork. Copy hash_context details from current->mm */

 inherit subpage prot details if we have one. */

	/*

	 * set the process table entry,

	/*

	 * Order the above store with subsequent update of the PID

	 * register (at which point HW can start loading/caching

	 * the entry) and the corresponding load by the MMU from

	 * the L2 cache.

 drop all the pending references */

 We allow PTE_FRAG_NR fragments from a PTE page */

	/*

	 * For tasks which were successfully initialized we end up calling

	 * arch_exit_mmap() which clears the process table entry. And

	 * arch_exit_mmap() is called before the required fullmm TLB flush

	 * which does a RIC=2 flush. Hence for an initialized task, we do clear

	 * any cached process table entries.

	 *

	 * The condition below handles the error case during task init. We have

	 * set the process table entry early and if we fail a task

	 * initialization, we need to ensure the process table entry is zeroed.

	 * We need not worry about process table entry caches because the task

	 * never ran with the PID value.

		/*

		 * Radix doesn't have a valid bit in the process table

		 * entries. However we know that at least P9 implementation

		 * will avoid caching an entry with an invalid RTS field,

		 * and 0 is invalid. So this will do.

		 *

		 * This runs before the "fullmm" tlb flush in exit_mmap,

		 * which does a RIC=2 tlbie to clear the process table

		 * entry. See the "fullmm" comments in tlb-radix.c.

		 *

		 * No barrier required here after the store because

		 * this process will do the invalidate, which starts with

		 * ptesync.

/**

 * cleanup_cpu_mmu_context - Clean up MMU details for this CPU (newly offlined)

 *

 * This clears the CPU from mm_cpumask for all processes, and then flushes the

 * local TLB to ensure TLB coherency in case the CPU is onlined again.

 *

 * KVM guest translations are not necessarily flushed here. If KVM started

 * using mm_cpumask or the Linux APIs which do, this would have to be resolved.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Page table handling routines for radix page table.

 *

 * Copyright 2015-2016, Aneesh Kumar K.V, IBM Corporation.

/*

 * When allocating pud or pmd pointers, we allocate a complete page

 * of PAGE_SIZE rather than PUD_TABLE_SIZE or PMD_TABLE_SIZE. This

 * is to ensure that the page obtained from the memblock allocator

 * can be completely used as page table page and can be freed

 * correctly when the page table entries are removed.

/*

 * nid, region_start, and region_end are hints to try to place the page

 * table memory in the same node or region.

	/*

	 * Make sure task size is correct as per the max adddr

	/*

	 * Should make page table allocation functions be able to take a

	 * node, so we can place kernel page tables on the right nodes after

	 * boot.

 aligns up

 CONFIG_STRICT_KERNEL_RWX */

 We don't support slb for radix */

	/*

	 * Create the linear mapping

		/*

		 * The memblock allocator  is up at this point, so the

		 * page tables will be allocated within the range. No

		 * need or a node (which we don't have yet).

 Find out how many PID bits are supported */

		/*

		 * Older versions of KVM on these machines perfer if the

		 * guest only uses the low 19 PID bits.

	/*

	 * Allocate Partition table and process table for the

	 * host.

	/*

	 * Fill in the process table.

	/*

	 * The init_mm context is given the first available (non-zero) PID,

	 * which is the "guard PID" and contains no page table. PIDR should

	 * never be set to zero because that duplicates the kernel address

	 * space at the 0x0... offset (quadrant 0)!

	 *

	 * An arbitrary PID that may later be allocated by the PID allocator

	 * for userspace processes must not be used either, because that

	 * would cause stale user mappings for that PID on CPUs outside of

	 * the TLB invalidation scheme (because it won't be in mm_cpumask).

	 *

	 * So permanently carve out one PID for the purpose of a guard PID.

 We are scanning "cpu" nodes only */

 Find MMU PID size */

 Grab page size encodings */

 top 3 bit is AP encoding */

 needed ? */

		/*

		 * Nothing in the device tree

	/*

	 * OPAL firmware feature is set by now. Hence we are ok

	 * to test OPAL feature.

 CONFIG_MEMORY_HOTPLUG */

 CONFIG_MEMORY_HOTPLUG */

	/*

	 * Try to find the available page sizes in the device-tree

		/*

		 * No page size details found in device tree.

		 * Let's assume we have page 4k and 64k support

	/*

	 * Max mapping size used when mapping pages. We don't use

	 * ppc_md.memory_block_size() here because this get called

	 * early and we don't have machine probe called yet. Also

	 * the pseries implementation only check for ibm,lmb-size.

	 * All hypervisor supporting radix do expose that device

	 * tree node.

	/*

	* In HV mode, we init AMOR (Authority Mask Override Register) so that

	* the hypervisor and guest can setup IAMR (Instruction Authority Mask

	* Register), enable key 0 and set it to 1.

	*

	* AMOR = 0b1100 .... 0000 (Mask for key 0 is 11)

 PAGE_SIZE mappings */

 vmemmap mapping */

		/*

		 * map vmemmap using 2M if available

	/*

	 * initialize page table size

 Switch to the guard PID before turning on MMU */

	/*

	 * update partition table control register and UPRT

 Make sure userspace can't change the AMR */

 Called during kexec sequence with MMU off */

			/*

			 * The vmemmap_free() and remove_section_mapping()

			 * codepaths call us with aligned addresses.

 CONFIG_MEMORY_HOTPLUG */

 Create a PTE encoding */

	/*

	 * khugepaged calls this for normal pmd

	/*

	 * pmdp collapse_flush need to ensure that there are no parallel gup

	 * walk after this call. This is needed so that we can have stable

	 * page ref count when collapsing a page. We don't allow a collapse page

	 * if we have gup taken on the page. We can ensure that by sending IPI

	 * because gup walk happens with IRQ disabled.

/*

 * For us pgtable_t is pte_t *. Inorder to save the deposisted

 * page table, we consider the allocated page table as a list

 * head. On withdraw we need to make sure we zero out the used

 * list_head memory area.

 FIFO */

 FIFO */

 CONFIG_TRANSPARENT_HUGEPAGE */

	/*

	 * To avoid NMMU hang while relaxing access, we need mark

	 * the pte invalid in between.

		/*

		 * new value of pte

		/*

		 * Book3S does not require a TLB flush when relaxing access

		 * restrictions when the address space is not attached to a

		 * NMMU, because the core MMU will reload the pte after taking

		 * an access fault, which is defined by the architecture.

 See ptesync comment in radix__set_pte_at */

	/*

	 * To avoid NMMU hang while relaxing access we need to flush the tlb before

	 * we set the new value. We need to do this only for radix, because hash

	 * translation does flush when updating the linux pte.

 radix unused */, ptep, new_pud);

 radix unused */, ptep, new_pmd);

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Flush PWC even if we get PUD_SIZE hugetlb invalidate to keep this simpler.

/*

 * A vairant of hugetlb_get_unmapped_area doing topdown search

 * FIXME!! should we do as x86 does or non hugetlb area does ?

 * ie, use topdown or not based on mmap_is_legacy check ?

	/*

	 * We are always doing an topdown search here. Slice code

	 * does that too.

	/*

	 * To avoid NMMU hang while relaxing access we need to flush the tlb before

	 * we set the new value.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2005, Paul Mackerras, IBM Corporation.

 * Copyright 2009, Benjamin Herrenschmidt, IBM Corporation.

 * Copyright 2015-2016, Aneesh Kumar K.V, IBM Corporation.

/*

 * vmemmap is the starting address of the virtual address space where

 * struct pages are allocated for all possible PFNs present on the system

 * including holes and bad memory (hence sparse). These virtual struct

 * pages are stored in sequence in this virtual address space irrespective

 * of the fact whether the corresponding PFN is valid or not. This achieves

 * constant relationship between address of struct page and its PFN.

 *

 * During boot or memory hotplug operation when a new memory section is

 * added, physical memory allocation (including hash table bolting) will

 * be performed for the set of struct pages which are part of the memory

 * section. This saves memory by not allocating struct pages for PFNs

 * which are not valid.

 *

 *		----------------------------------------------

 *		| PHYSICAL ALLOCATION OF VIRTUAL STRUCT PAGES|

 *		----------------------------------------------

 *

 *	   f000000000000000                  c000000000000000

 * vmemmap +--------------+                  +--------------+

 *  +      |  page struct | +--------------> |  page struct |

 *  |      +--------------+                  +--------------+

 *  |      |  page struct | +--------------> |  page struct |

 *  |      +--------------+ |                +--------------+

 *  |      |  page struct | +       +------> |  page struct |

 *  |      +--------------+         |        +--------------+

 *  |      |  page struct |         |   +--> |  page struct |

 *  |      +--------------+         |   |    +--------------+

 *  |      |  page struct |         |   |

 *  |      +--------------+         |   |

 *  |      |  page struct |         |   |

 *  |      +--------------+         |   |

 *  |      |  page struct |         |   |

 *  |      +--------------+         |   |

 *  |      |  page struct |         |   |

 *  |      +--------------+         |   |

 *  |      |  page struct | +-------+   |

 *  |      +--------------+             |

 *  |      |  page struct | +-----------+

 *  |      +--------------+

 *  |      |  page struct | No mapping

 *  |      +--------------+

 *  |      |  page struct | No mapping

 *  v      +--------------+

 *

 *		-----------------------------------------

 *		| RELATION BETWEEN STRUCT PAGES AND PFNS|

 *		-----------------------------------------

 *

 * vmemmap +--------------+                 +---------------+

 *  +      |  page struct | +-------------> |      PFN      |

 *  |      +--------------+                 +---------------+

 *  |      |  page struct | +-------------> |      PFN      |

 *  |      +--------------+                 +---------------+

 *  |      |  page struct | +-------------> |      PFN      |

 *  |      +--------------+                 +---------------+

 *  |      |  page struct | +-------------> |      PFN      |

 *  |      +--------------+                 +---------------+

 *  |      |              |

 *  |      +--------------+

 *  |      |              |

 *  |      +--------------+

 *  |      |              |

 *  |      +--------------+                 +---------------+

 *  |      |  page struct | +-------------> |      PFN      |

 *  |      +--------------+                 +---------------+

 *  |      |              |

 *  |      +--------------+

 *  |      |              |

 *  |      +--------------+                 +---------------+

 *  |      |  page struct | +-------------> |      PFN      |

 *  |      +--------------+                 +---------------+

 *  |      |  page struct | +-------------> |      PFN      |

 *  v      +--------------+                 +---------------+

/*

 * On hash-based CPUs, the vmemmap is bolted in the hash table.

 *

 CONFIG_SPARSEMEM_VMEMMAP */

/*

 * map_kernel_page currently only called by __ioremap

 * map_kernel_page adds an entry to the ioremap page table

 * and adds an entry to the HPT, possibly bolting it

		/*

		 * If the mm subsystem is not fully up, we cannot create a

		 * linux page table entry for this mapping.  Simply bolt an

		 * entry in the hardware page table.

		 *

	/*

	 * Wait for all pending hash_page to finish. This is needed

	 * in case of subpage collapse. When we collapse normal pages

	 * to hugepage, we first clear the pmd, then invalidate all

	 * the PTE entries. The assumption here is that any low level

	 * page fault will see a none pmd and take the slow path that

	 * will wait on mmap_lock. But we could very well be in a

	 * hash_page with local ptep pointer value. Such a hash page

	 * can result in adding new HPTE entries for normal subpages.

	 * That means we could be modifying the page content as we

	 * copy them to a huge page. So wait for parallel hash_page

	 * to finish before invalidating HPTE entries. We can do this

	 * by sending an IPI to all the cpus and executing a dummy

	 * function there.

	/*

	 * Now invalidate the hpte entries in the range

	 * covered by pmd. This make sure we take a

	 * fault and will find the pmd as none, which will

	 * result in a major fault which takes mmap_lock and

	 * hence wait for collapse to complete. Without this

	 * the __collapse_huge_page_copy can result in copying

	 * the old content.

/*

 * We want to put the pgtable in pmd and use pgtable for tracking

 * the base page size hptes

	/*

	 * we store the pgtable in the second half of PMD

	/*

	 * expose the deposited pgtable to other cpus.

	 * before we set the hugepage PTE at pmd level

	 * hash fault code looks at the deposted pgtable

	 * to store hash index values.

	/*

	 * Once we withdraw, mark the entry NULL.

	/*

	 * We store HPTE information in the deposited PTE fragment.

	 * zero out the content on withdraw.

/*

 * A linux hugepage PMD was changed and the corresponding hash table entries

 * neesd to be flushed.

 get the base page size,vsid and segment size */

	/*

	 * We have pmd == none and we are holding page_table_lock.

	 * So we can safely go and clear the pgtable hash

	 * index info.

	/*

	 * Let's zero out old valid and hash index details

	 * hash fault look at them.

	/*

	 * We support THP only if PMD_SIZE is 16MB.

	/*

	 * We need to make sure that we support 16MB hugepage in a segement

	 * with base page size 64K or 4K. We only enable THP with a PAGE_SIZE

	 * of 64K.

	/*

	 * If we have 64K HPTE, we will be using that by default

	/*

	 * Ok we only have 4K HPTE

 CONFIG_TRANSPARENT_HUGEPAGE */

 We'd rather this was on the stack but it has to be in the RMO

 And therefore we need a lock to protect it from concurrent use

 Not sure if we can do much with the return value */

 Switch to real mode and leave interrupts off

 Tell the master we are in real mode

 Spin until the counter goes to zero

 Switch back to virtual mode

 outputs

 inputs

 clobbers

 Wait for all but one CPU (this one) to call-in

 Signal the other CPUs that we're done

 aligns up

 Ensure state is consistent before we call the other CPUs

/*

 * Copyright IBM Corporation, 2015

 * Author Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of version 2 of the GNU Lesser General Public License

 * as published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it would be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

 *

	/*

	 * atomically mark the linux large page PTE busy and dirty

 If PTE busy, retry the access */

 If PTE permissions don't match, take page fault */

		/*

		 * Try to lock the PTE, add ACCESSED and DIRTY if it was

		 * a write access. Since this is 4K insert of 64K page size

		 * also add H_PAGE_COMBO

	/*

	 * PP bits. _PAGE_USER is already PP bit 0x2, so we only

	 * need to add in 0x1 if it's a read-only user page

		/*

		 * There MIGHT be an HPTE for this pte

 Insert into the hash table, primary slot */

		/*

		 * Primary is full, try the secondary

				/*

				 * FIXME!! Should be try the group from which we removed ?

		/*

		 * Hypervisor failure. Restore old pte and return -1

		 * similar to __hash_page_*

 SPDX-License-Identifier: GPL-2.0+

/*

 * PowerPC Memory Protection Keys management

 *

 * Copyright 2017, Ram Pai, IBM Corporation.

 Max number of pkeys supported */

/*

 *  Keys marked in the reservation list cannot be allocated by  userspace

 Bits set for the initially allocated keys */

/*

 * Even if we allocate keys with sys_pkey_alloc(), we need to make sure

 * other thread still find the access denied using the same keys.

/*

 * Key used to implement PROT_EXEC mmap. Denies READ/WRITE

 * We pick key 2 because 0 is special key and 1 is reserved as per ISA.

 We are scanning "cpu" nodes only */

	/*

	 * Pkey is not supported with Radix translation.

		/*

		 * Let's assume 32 pkeys on P8/P9 bare metal, if its not defined by device

		 * tree. We make this exception since some version of skiboot forgot to

		 * expose this property on power8/9.

	/*

	 * Adjust the upper limit, based on the number of bits supported by

	 * arch-neutral code.

	/*

	 * We define PKEY_DISABLE_EXECUTE in addition to the arch-neutral

	 * generic defines for PKEY_DISABLE_ACCESS and PKEY_DISABLE_WRITE.

	 * Ensure that the bits a distinct.

	/*

	 * pkey_to_vmflag_bits() assumes that the pkey bits are contiguous

	 * in the vmaflag. Make sure that is really the case.

	/*

	 * Only P7 and above supports SPRN_AMR update with MSR[PR] = 1

 scan the device tree for pkey feature */

 Allow all keys to be modified by default */

	/*

	 * The device tree cannot be relied to indicate support for

	 * execute_disable support. Instead we use a PVR check.

	/*

	 * The OS can manage only 8 pkeys due to its inability to represent them

	 * in the Linux 4K PTE. Mark all other keys reserved.

		/*

		 * Insufficient number of keys to support

		 * execute only key. Mark it unavailable.

		/*

		 * Mark the execute_only_pkey as not available for

		 * user allocation via pkey_alloc.

		/*

		 * Deny READ/WRITE for execute_only_key.

		 * Allow execute in IAMR.

		/*

		 * Clear the uamor bits for this key.

		/*

		 * Insufficient number of keys to support

		 * KUAP/KUEP feature.

  handle key which is used by kernel for KAUP */

		/*

		 * Mark access for kup_key in default amr so that

		 * we continue to operate with that AMR in

		 * copy_to/from_user().

	/*

	 * Allow access for only key 0. And prevent any other modification.

	/*

	 * key 0 is special in that we want to consider it an allocated

	 * key which is preallocated. We don't allow changing AMR bits

	 * w.r.t key 0. But one can pkey_free(key0)

	/*

	 * key 1 is recommended not to be used. PowerISA(3.0) page 1015,

	 * programming note.

	/*

	 * Prevent the usage of OS reserved keys. Update UAMOR

	 * for those keys. Also mark the rest of the bits in the

	 * 32 bit mask as reserved.

	/*

	 * Prevent the allocation of reserved keys too.

	/*

	 * Setup uamor on boot cpu

	/*

	 * On hash if PKEY feature is not enabled, disable KUAP too.

	/*

	 * Radix always uses key0 of the IAMR to determine if an access is

	 * allowed. We set bit 0 (IBM bit 1) of key0, to prevent instruction

	 * fetch.

	/*

	 * On hash if PKEY feature is not enabled, disable KUAP too.

	/*

	 * Set the default kernel AMR values on all cpus.

/*

 * Set the access rights in AMR IAMR and UAMOR registers for @pkey to that

 * specified in @init_val.

	/*

	 * Check whether the key is disabled by UAMOR.

	/*

	 * Both the bits in UAMOR corresponding to the key should be set

 Set the bits we need in AMR: */

 Do this check first since the vm_flags should be hot */

/*

 * This should only be called for *plain* mprotect calls.

	/*

	 * If the currently associated pkey is execute-only, but the requested

	 * protection is not execute-only, move it back to the default pkey.

	/*

	 * The requested protection is execute-only. Hence let's use an

	 * execute-only pkey.

 Nothing to override. */

/*

 * We only want to enforce protection keys on the current thread because we

 * effectively have no access to AMR/IAMR for other threads or any way to tell

 * which AMR/IAMR in a threaded process we could use.

 *

 * So do not enforce things if the VMA is not from the current mm, or if we are

 * in a kernel thread.

	/*

	 * Do not enforce our key-permissions on a foreign vma.

 Duplicate the oldmm pkey state in mm: */

 CONFIG_PPC_MEM_KEYS */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * TLB flush routines for radix kernels.

 *

 * Copyright 2015-2016, Aneesh Kumar K.V, IBM Corporation.

/*

 * tlbiel instruction for radix, set invalidation

 * i.e., r=1 and is=01 or is=10 or is=11

	/*

	 * Flush the first set of the TLB, and the entire Page Walk Cache

	 * and partition table entries. Then flush the remaining sets of the

	 * TLB.

 MSR[HV] should flush partition scope translations first. */

 Flush process scoped entries. */

 IS = 1 */

 process scoped */

 radix format */

 IS = 1 */

 process scoped */

 radix format */

 IS = 1 */

 process scoped */

 radix format */

 IS = 2 */

 partition scoped */

 radix format */

 IS = 2 */

 process scoped */

 radix format */

 process scoped */

 radix format */

 process scoped */

 radix format */

 process scoped */

 radix format */

 partition scoped */

 radix format */

	/*

	 * We can use any address for the invalidation, pick one which is

	 * probably unused as an optimisation.

	/*

	 * We can use any address for the invalidation, pick one which is

	 * probably unused as an optimisation.

	/*

	 * We can use any address for the invalidation, pick one which is

	 * probably unused as an optimisation.

/*

 * We use 128 set in radix mode and 256 set in hpt mode.

 For PWC, only one flush is needed */

		/*

		 * Flush the first set of the TLB, and if

		 * we're doing a RIC_FLUSH_ALL, also flush

		 * the entire Page Walk Cache.

 For the remaining sets, just flush the TLB */

	/*

	 * Workaround the fact that the "ric" argument to __tlbie_pid

	 * must be a compile-time contraint to match the "i" constraint

	 * in the asm statement.

	/*

	 * Workaround the fact that the "ric" argument to __tlbie_pid

	 * must be a compile-time contraint to match the "i" constraint

	 * in the asm statement.

	/*

	 * Always want the CPU translations to be invalidated with tlbiel in

	 * these paths, so while coprocessors must use tlbie, we can not

	 * optimise away the tlbiel component.

	/*

	 * Workaround the fact that the "ric" argument to __tlbie_pid

	 * must be a compile-time contraint to match the "i" constraint

	 * in the asm statement.

	/*

	 * Workaround the fact that the "ric" argument to __tlbie_pid

	 * must be a compile-time contraint to match the "i" constraint

	 * in the asm statement.

/*

 * Base TLB flushing operations:

 *

 *  - flush_tlb_mm(mm) flushes the specified mm context TLB's

 *  - flush_tlb_page(vma, vmaddr) flushes one page

 *  - flush_tlb_range(vma, start, end) flushes a range of pages

 *  - flush_tlb_kernel_range(start, end) flushes kernel pages

 *

 *  - local_* variants of page and mm only apply to the current

 *    processor

 CONFIG_SMP */

 need the return fix for nohash.c */

	/*

	 * P9 nest MMU has issues with the page walk cache

	 * caching PTEs and not flushing them properly when

	 * RIC = 0 for a PID/LPID invalidate

/*

 * If always_flush is true, then flush even if this CPU can't be removed

 * from mm_cpumask.

	/*

	 * A kthread could have done a mmget_not_zero() after the flushing CPU

	 * checked mm_cpumask, and be in the process of kthread_use_mm when

	 * interrupted here. In that case, current->mm will be set to mm,

	 * because kthread_use_mm() setting ->mm and switching to the mm is

	 * done with interrupts off.

 Is a kernel thread and is using mm as the lazy tlb */

	/*

	 * This IPI may be initiated from any source including those not

	 * running the mm, so there may be a racing IPI that comes after

	 * this one which finds the cpumask already clear. Check and avoid

	 * underflowing the active_cpus count in that case. The race should

	 * not otherwise be a problem, but the TLB must be flushed because

	 * that's what the caller expects.

	/*

	 * Would be nice if this was async so it could be run in

	 * parallel with our local flush, but generic code does not

	 * give a good API for it. Could extend the generic code or

	 * make a special powerpc IPI for flushing TLBs.

	 * For now it's not too performance critical.

 CONFIG_SMP */

 CONFIG_SMP */

/*

 * Interval between flushes at which we send out IPIs to check whether the

 * mm_cpumask can be trimmed for the case where it's not a single-threaded

 * process flushing its own mm. The intent is to reduce the cost of later

 * flushes. Don't want this to be so low that it adds noticable cost to TLB

 * flushing, or so high that it doesn't help reduce global TLBIEs.

			/*

			 * Asynchronous flush sources may trim down to nothing

			 * if the process is not running, so occasionally try

			 * to trim.

 Coprocessors require TLBIE to invalidate nMMU. */

	/*

	 * In the fullmm case there's no point doing the exit_flush_lazy_tlbs

	 * because the mm is being taken down anyway, and a TLBIE tends to

	 * be faster than an IPI+TLBIEL.

	/*

	 * If we are running the only thread of a single-threaded process,

	 * then we should almost always be able to trim off the rest of the

	 * CPU mask (except in the case of use_mm() races), so always try

	 * trimming the mask.

		/*

		 * use_mm() race could prevent IPIs from being able to clear

		 * the cpumask here, however those users are established

		 * after our first check (and so after the PTEs are removed),

		 * and the TLB still gets flushed by the IPI, so this CPU

		 * will only require a local flush.

	/*

	 * Occasionally try to trim down the cpumask. It's possible this can

	 * bring the mask to zero, which results in no flush.

	/*

	 * Order loads of mm_cpumask (in flush_type_needed) vs previous

	 * stores to clear ptes before the invalidate. See barrier in

	 * switch_mm_irqs_off

 see radix__flush_tlb_mm */

 see radix__flush_tlb_mm */

 CONFIG_SMP */

		/*

		 * Coherent accelerators don't refcount kernel memory mappings,

		 * so have to always issue a tlbie for them. This is quite a

		 * slow path anyway.

/*

 * If kernel TLBIs ever become local rather than global, then

 * drivers/misc/ocxl/link.c:ocxl_link_add_pe will need some work, as it

 * assumes kernel TLBIs are global.

/*

 * Number of pages above which we invalidate the entire PID rather than

 * flush individual pages, for local and global flushes respectively.

 *

 * tlbie goes out to the interconnect and individual ops are more costly.

 * It also does not iterate over sets like the local tlbiel variant when

 * invalidating a full PID, so it has a far lower threshold to change from

 * individual page flushes to full-pid flushes.

 see radix__flush_tlb_mm */

	/*

	 * full pid flush already does the PWC flush. if it is not full pid

	 * flush check the range is more than PMD and force a pwc flush

	 * mremap() depends on this behaviour.

		/*

		 * We are now flushing a range larger than PMD size force a RIC_FLUSH_ALL

 For PWC, only one flush is needed */

/*

 * Flush partition scoped LPID address translation for all CPUs.

/*

 * Flush partition scoped PWC from LPID for all CPUs.

/*

 * Flush partition scoped translations from LPID (=LPIDR)

/*

 * Flush process scoped translations from LPID (=LPIDR)

	/*

	 * if page size is not something we understand, do a full mm flush

	 *

	 * A "fullmm" flush must always do a flush_all_mm (RIC=2) flush

	 * that flushes the process table entry cache upon process teardown.

	 * See the comment for radix in arch_exit_mmap().

 see radix__flush_tlb_mm */

 4k page size, just blow the world */

 Otherwise first do the PWC, then iterate the pages. */

 see radix__flush_tlb_mm */

 CONFIG_TRANSPARENT_HUGEPAGE */

 IS = 3 */

 partition scoped */

 radix format */

 any LPID value to flush guest mappings */

	/*

	 * now flush guest entries by passing PRS = 1 and LPID != 0

	/*

	 * now flush host entires by passing PRS = 0 and LPID == 0

/*

 * Performs process-scoped invalidations for a given LPID

 * as part of H_RPT_INVALIDATE hcall.

	/*

	 * A H_RPTI_TYPE_ALL request implies RIC=3, hence

	 * do a single IS=1 based flush.

 Full PID flush */

 Do range invalidation for all the valid page sizes */

		/*

		 * If the number of pages spanning the range is above

		 * the ceiling, convert the request into a full PID flush.

		 * And since PID flush takes out all the page sizes, there

		 * is no need to consider remaining page sizes.

 CONFIG_KVM_BOOK3S_HV_POSSIBLE */

/*

 * Copyright IBM Corporation, 2015

 * Author Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of version 2 of the GNU Lesser General Public License

 * as published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it would be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

 *

/*

 * Return true, if the entry has a slot value which

 * the software considers as invalid.

/*

 * index from 0 - 15

	/*

	 * atomically mark the linux large page PTE busy and dirty

 If PTE busy, retry the access */

 If PTE permissions don't match, take page fault */

		/*

		 * Try to lock the PTE, add ACCESSED and DIRTY if it was

		 * a write access. Since this is 4K insert of 64K page size

		 * also add H_PAGE_COMBO

	/*

	 * Handle the subpage protection bits

		/*

		 * No CPU has hugepages but lacks no execute, so we

		 * don't need to worry about that case

	/*

	 *None of the sub 4k page is hashed

	/*

	 * Check if the pte was already inserted into the hash table

	 * as a 64k HW page, and invalidate the 64k HPTE if so.

		/*

		 * clear the old slot details from the old and new pte.

		 * On hash insert failure we use old pte value and we don't

		 * want slot information there if we have a insert failure.

	/*

	 * Check for sub page valid and update

		/*

		 * If we failed because typically the HPTE wasn't really here

		 * we try an insertion.

	/*

	 * Initialize all hidx entries to invalid value, the first time

	 * the PTE is about to allocate a 4K HPTE.

	/*

	 * handle H_PAGE_4K_PFN case

		/*

		 * All the sub 4k page have the same

		 * physical address.

 Insert into the hash table, primary slot */

	/*

	 * Primary is full, try the secondary

			/*

			 * We got a valid slot from a hardware point of view.

			 * but we cannot use it, because we use this special

			 * value; as defined by hpte_soft_invalid(), to track

			 * invalid slots. We cannot use it. So invalidate it.

			/*

			 * For soft invalid slot, let's ensure that we release a

			 * slot from the primary, with the hope that we will

			 * acquire that slot next time we try. This will ensure

			 * that we do not get the same soft-invalid slot.

			/*

			 * FIXME!! Should be try the group from which we removed ?

	/*

	 * Hypervisor failure. Restore old pte and return -1

	 * similar to __hash_page_*

	/*

	 * atomically mark the linux large page PTE busy and dirty

 If PTE busy, retry the access */

 If PTE permissions don't match, take page fault */

		/*

		 * Check if PTE has the cache-inhibit bit set

		 * If so, bail out and refault as a 4k page

		/*

		 * Try to lock the PTE, add ACCESSED and DIRTY if it was

		 * a write access.

		/*

		 * There MIGHT be an HPTE for this pte

 Insert into the hash table, primary slot */

		/*

		 * Primary is full, try the secondary

				/*

				 * FIXME!! Should be try the group from which we removed ?

		/*

		 * Hypervisor failure. Restore old pte and return -1

		 * similar to __hash_page_*

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2015-2016, Aneesh Kumar K.V, IBM Corporation.

/*

 * This is called when relaxing access to a hugepage. It's also called in the page

 * fault path when we don't hit any of the major fault cases, ie, a minor

 * update of _PAGE_ACCESSED, _PAGE_DIRTY, etc... The generic code will have

 * handled those two for us, we additionally deal with missing execute

 * permission here on some processors

		/*

		 * We can use MMU_PAGE_2M here, because only radix

		 * path look at the psize.

/*

 * set a new huge pmd. We should not be called for updating

 * an existing pmd entry. That should go via pmd_hugepage_update.

	/*

	 * Make sure hardware valid bit is not set. We don't do

	 * tlb flush for this update.

 We've taken the IPI, so try to trim the mask while here */

/*

 * Serialize against find_current_mm_pte which does lock-less

 * lookup in page tables with local interrupts disabled. For huge pages

 * it casts pmd_t to pte_t. Since format of pte_t is different from

 * pmd_t we want to prevent transit from pmd pointing to page table

 * to pmd pointing to huge page (and back) while interrupts are disabled.

 * We clear pmd to possibly replace it with page table pointer in

 * different code paths. So make sure we wait for the parallel

 * find_current_mm_pte to finish.

/*

 * We use this to invalidate a pmdp entry before switching from a

 * hugepte to regular pmd entry.

	/*

	 * if it not a fullmm flush, then we can possibly end up converting

	 * this PMD pte entry to a regular level 0 PTE by a parallel page fault.

	 * Make sure we flush the tlb in this case.

/*

 * At some point we should be able to get rid of

 * pmd_mkhuge() and mk_huge_pmd() when we update all the

 * other archs to mark the pmd huge in pfn_pmd()

 CONFIG_TRANSPARENT_HUGEPAGE */

 For use by kexec, called with MMU off */

 CONFIG_MEMORY_HOTPLUG */

 Initialize the Partition Table with no entries */

	/*

	 * update partition table control register,

	 * 64 K size.

 do we need fixup here ?*/

	/*

	 * When ultravisor is enabled, the partition table is stored in secure

	 * memory and can only be accessed doing an ultravisor call. However, we

	 * maintain a copy of the partition table in normal memory to allow Nest

	 * MMU translations to occur (for normal VMs).

	 *

	 * Therefore, here we always update partition_tb, regardless of whether

	 * we are running under an ultravisor or not.

	/*

	 * If ultravisor is enabled, we do an ultravisor call to register the

	 * partition table entry (PATE), which also do a global flush of TLBs

	 * and partition table caches for the lpid. Otherwise, just do the

	 * flush. The type of flush (hash or radix) depends on what the previous

	 * use of the partition ID was, not the new use.

		/*

		 * Boot does not need to flush, because MMU is off and each

		 * CPU does a tlbiel_all() before switching them on, which

		 * flushes everything.

		/*

		 * If we have taken up all the fragments mark PTE page NULL

	/*

	 * if we support only one fragment just return the

	 * allocated page.

	/*

	 * If we find pgtable_page set, we return

	 * the allocated page with single fragement

	 * count.

 16M hugepd directory at pud level */

 16G hugepd directory at the pgd level */

 We don't free pgd table via RCU callback */

	/*

	 * Hash maps the memory with one size mmu_linear_psize.

	 * So don't bother to print these on hash

 CONFIG_PROC_FS */

	/*

	 * Clear the _PAGE_PRESENT so that no hardware parallel update is

	 * possible. Also keep the pte_present true so that we don't take

	 * wrong fault.

/*

 * For hash translation mode, we use the deposited table to store hash slot

 * information and they are stored at PTRS_PER_PMD offset from related pmd

 * location. Hence a pmd move requires deposit and withdraw.

 *

 * For radix translation with split pmd ptl, we store the deposited table in the

 * pmd page. Hence if we have different pmd page we need to withdraw during pmd

 * move.

 *

 * With hash we use deposited table always irrespective of anon or not.

 * With radix we use deposited table only for anonymous mapping.

/*

 * Does the CPU support tlbie?

/*

 * Should tlbie be used for management of CPU TLBs, for kernel and process

 * address spaces? tlbie may still be used for nMMU accelerators, and for KVM

 * guest address spaces.

	/*

	 * There is no locking vs tlb flushing when changing this value.

	 * The tlb flushers will see one value or another, and use either

	 * tlbie or tlbiel with IPIs. In both cases the TLBs will be

	 * invalidated as expected.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * PowerPC64 SLB support.

 *

 * Copyright (C) 2004 David Gibson <dwg@au.ibm.com>, IBM

 * Based on earlier code written by:

 * Dave Engebretsen and Mike Corrigan {engebret|mikejc}@us.ibm.com

 *    Copyright (c) 2001 Dave Engebretsen

 * Copyright (C) 2002 Anton Blanchard <anton@au.ibm.com>, IBM

	/*

	 * slbfee. requires bit 24 (PPC bit 39) be clear in RB. Hardware

	 * ignores all other bits from 0-27, so just clear them all.

	/*

	 * Clear the ESID first so the entry is not valid while we are

	 * updating it.  No write barriers are needed here, provided

	 * we only update the current CPU's SLB shadow buffer.

	/*

	 * Updating the shadow buffer before writing the SLB ensures

	 * we don't get a stale entry here if we get preempted by PHYP

	 * between these two statements.

/*

 * Insert bolted entries into SLB (which may not be empty, so don't clear

 * slb_cache_ptr).

 No isync needed because realmode. */

/*

 * Insert the bolted entries into an empty SLB.

/*

 * This flushes all SLB entries including 0, so it must be realmode.

	/*

	 * SLBIA IH=1 on ISA v2.05 and newer processors may preserve lookaside

	 * information created with Class=0 entries, which we use for kernel

	 * SLB entries (the SLB entries themselves are still invalidated).

	 *

	 * Older processors will ignore this optimisation. Over-invalidation

	 * is fine because we never rely on lookaside information existing.

/*

 * This flushes non-bolted entries, it can be run in virtual mode. Must

 * be called with interrupts disabled.

	/*

	 * We can't take a PMU exception in the following code, so hard

	 * disable interrupts.

 Save slb_cache_ptr value. */

 RR is not so useful as it's often not used for allocation */

 Dump slb cache entires as well. */

	/*

	 * vmalloc is not bolted, so just have to flush non-bolted.

 EAs are stored >> 28 so 256MB segments don't need clearing */

	/*

	 * preload cache can only be used to determine whether a SLB

	 * entry exists if it does not start to overflow.

	/*

	 * We have no good place to clear the slb preload cache on exec,

	 * flush_thread is about the earliest arch hook but that happens

	 * after we switch to the mm and have aleady preloaded the SLBEs.

	 *

	 * For the most part that's probably okay to use entries from the

	 * previous exec, they will age out if unused. It may turn out to

	 * be an advantage to clear the cache before switching to it,

	 * however.

	/*

	 * preload some userspace segments into the SLB.

	 * Almost all 32 and 64bit PowerPC executables are linked at

	 * 0x10000000 so it makes sense to preload this segment.

 Libraries and mmaps. */

 see switch_slb */

 see above */

 Userspace entry address. */

 Top of stack, grows down. */

 Bottom of heap, grows up. */

 see switch_slb */

 user slbs have C=1 */

 Flush all user entries from the segment table of the current processor. */

	/*

	 * We need interrupts hard-disabled here, not just soft-disabled,

	 * so that a PMU interrupt can't occur, which might try to access

	 * user memory (to get a stack trace) and possible cause an SLB miss

	 * which would update the slb_cache/slb_cache_ptr fields in the PACA.

		/*

		 * SLBIA IH=3 invalidates all Class=1 SLBEs and their

		 * associated lookaside structures, which matches what

		 * switch_slb wants. So ARCH_300 does not use the slb

		 * cache.

			/*

			 * Could assert_slb_presence(true) here, but

			 * hypervisor or machine check could have come

			 * in and removed the entry at this point.

 Workaround POWER5 < DD2.1 issue */

 Flush but retain kernel lookaside information */

	/*

	 * We gradually age out SLBs after a number of context switches to

	 * reduce reload overhead of unused entries (like we do with FP/VEC

	 * reload). Each time we wrap 256 switches, take an entry out of the

	 * SLB preload cache.

	/*

	 * Synchronize slbmte preloads with possible subsequent user memory

	 * address accesses by the kernel (user mode won't happen until

	 * rfid, which is safe).

 Prepare our SLB miss handler based on our page size */

 Invalidate the entire SLB (even entry 0) & all the ERATS */

	/*

	 * For the boot cpu, we're running on the stack in init_thread_union,

	 * which is in the first segment of the linear mapping, and also

	 * get_paca()->kstack hasn't been initialized yet.

	 * For secondary cpus, we need to bolt the kernel stack entry now.

 ISAv3.0B and later does not use slb_cache */

	/*

	 * Now update slb cache entries

		/*

		 * We have space in slb cache for optimized switch_slb().

		 * Top 36 bits from esid_data as per ISA

		/*

		 * Our cache is full and the current cache content strictly

		 * doesn't indicate the active SLB conents. Bump the ptr

		 * so that switch_slb() will ignore the cache.

	/*

	 * The allocation bitmaps can become out of synch with the SLB

	 * when the _switch code does slbie when bolting a new stack

	 * segment and it must not be anywhere else in the SLB. This leaves

	 * a kernel allocated entry that is unused in the SLB. With very

	 * large systems or small segment sizes, the bitmaps could slowly

	 * fill with these entries. They will eventually be cleared out

	 * by the round robin allocator in that case, so it's probably not

	 * worth accounting for.

	/*

	 * SLBs beyond 32 entries are allocated with stab_rr only

	 * POWER7/8/9 have 32 SLB entries, this could be expanded if a

	 * future CPU has more.

 round-robin replacement of slb starting at SLB_NUM_BOLTED. */

	/*

	 * There must not be a kernel SLB fault in alloc_slb_index or before

	 * slbmte here or the allocation bitmaps could get out of whack with

	 * the SLB.

	 *

	 * User SLB faults or preloads take this path which might get inlined

	 * into the caller, so add compiler barriers here to ensure unsafe

	 * memory accesses do not come between.

	/*

	 * No need for an isync before or after this slbmte. The exception

	 * we enter with and the rfid we exit with are context synchronizing.

	 * User preloads should add isync afterwards in case the kernel

	 * accesses user memory before it returns to userspace with rfid.

		/*

		 * stress_slb() does not use slb cache, repurpose as a

		 * cache of inserted (non-bolted) kernel SLB entries. All

		 * non-bolted kernel entries are flushed on any user fault,

		 * or if there are already 3 non-boled kernel entries.

 We only support upto H_MAX_PHYSMEM_BITS */

	/*

	 * consider this as bad access if we take a SLB miss

	 * on an address above addr limit.

 IRQs are not reconciled here, so can't check irqs_disabled */

	/*

	 * SLB kernel faults must be very careful not to touch anything that is

	 * not bolted. E.g., PACA and global variables are okay, mm->context

	 * stuff is not. SLB user faults may access all of memory (and induce

	 * one recursive SLB kernel fault), so the kernel fault must not

	 * trample on the user fault state at those points.

	/*

	 * This is a raw interrupt handler, for performance, so that

	 * fast_interrupt_return can be used. The handler must not touch local

	 * irq state, or schedule. We could test for usermode and upgrade to a

	 * normal process context (synchronous) interrupt for those, which

	 * would make them first-class kernel code and able to be traced and

	 * instrumented, although performance would suffer a bit, it would

	 * probably be a good tradeoff.

 Catch recursive kernel SLB faults. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  IOMMU helpers in MMU context.

 *

 *  Copyright (C) 2015 IBM Corp. <aik@ozlabs.ru>

 userspace address */

 number of entries in hpas/hpages[] */

	/*

	 * in mm_iommu_get we temporarily use this to store

	 * struct page address.

	 *

	 * We need to convert ua to hpa in real mode. Make it

	 * simpler by storing physical address.

 vmalloc'ed */

 Device memory base address */

	/*

	 * For a starting point for a maximum page size calculation

	 * we use @ua and @entries natural alignment to allow IOMMU pages

	 * smaller than huge pages but still bigger than PAGE_SIZE.

 Overlap? */

		/*

		 * Allow to use larger than 64k IOMMU pages. Only do that

		 * if we are backed by hugetlb. Skip device memory as it is not

		 * backed with page structs.

			/*

			 * We don't need struct page reference any more, switch

			 * to physical address.

 free the references taken */

 There are still users, exit */

 Are there still mappings? */

 @mapped became 0 so now mappings are disabled, release the region */

			/*

			 * Since the IOMMU page size might be bigger than

			 * PAGE_SIZE, the amount of preregistered memory

			 * starting from @hpa might be smaller than 1<<pageshift

			 * and the caller needs to distinguish this situation.

 Last mm_iommu_put() has been called, no more mappings allowed() */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2007-2008 Paul Mackerras, IBM Corp.

/*

 * Free all pages allocated for subpage protection maps and pointers.

 * Also makes sure that the subpage_prot_table structure is

 * reinitialized for the next user.

/*

 * Clear the subpage protection map for an address range, allowing

 * all accesses that are allowed by the pte permissions.

 now flush any existing HPTEs for the range */

	/*

	 * We don't try too hard, we just mark all the vma in that range

	 * VM_NOHUGEPAGE and split them.

	/*

	 * If the range is in unmapped range, just return

/*

 * Copy in a subpage protection map for an address range.

 * The map has 2 bits per 4k subpage, so 32 bits per 64k page.

 * Each 2-bit field is 0 to allow any access, 1 to prevent writes,

 * 2 or 3 to prevent all accesses.

 * Note that the normal page protections also apply; the subpage

 * protection mechanism is an additional constraint, so putting 0

 * in a 2-bit field won't allow writes to a page that is otherwise

 * write-protected.

 Check parameters */

 Clear out the protection map for the address range */

		/*

		 * Allocate subpage prot table if not already done.

		 * Do this with mmap_lock held

 now flush any existing HPTEs for the range */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * native hashtable management.

 *

 * SMP scalability work:

 *    Copyright (C) 2001 Anton Blanchard <anton@au.ibm.com>, IBM

/*

 * tlbiel instruction for hash, set invalidation

 * i.e., r=1 and is=01 or is=10 or is=11

 hash format */

	/*

	 * Flush the partition table cache if this is HV mode.

	/*

	 * Now invalidate the process table cache. UPRT=0 HPT modes (what

	 * current hardware implements) do not use the process table, but

	 * add the flushes anyway.

	 *

	 * From ISA v3.0B p. 1078:

	 *     The following forms are invalid.

	 *      * PRS=1, R=0, and RIC!=2 (The only process-scoped

	 *        HPT caching is of the Process Table.)

	/*

	 * Then flush the sets of the TLB proper. Hash mode uses

	 * partition scoped TLB translations, which may be flushed

	 * in !HV mode.

	/*

	 * We need 14 to 65 bits of va for a tlibe of 4K page

	 * With vpn we ignore the lower VPN_SHIFT bits already.

	 * And top two bits are already ignored because we can

	 * only accomodate 76 bits in a 64 bit vpn with a VPN_SHIFT

	 * of 12.

	/*

	 * clear top 16 bits of 64bit va, non SLS segment

	 * Older versions of the architecture (2.02 and earler) require the

	 * masking of the top 16 bits.

 clear out bits after (52) [0....52.....63] */

 We need 14 to 14 + i bits of va */

		/*

		 * AVAL bits:

		 * We don't need all the bits, but rest of the bits

		 * must be ignored by the processor.

		 * vpn cover upto 65 bits of va. (0...65) and we need

		 * 58..64 bits of va.

 AVAL */

 L */

 Radix flush for a hash guest */

 IS = 2 */

 lpid = 0 */

 partition scoped */

 radix format */

 RIC_FLSUH_TLB */

		/*

		 * Need the extra ptesync to make sure we don't

		 * re-order the tlbie

 Need the extra ptesync to ensure we don't reorder tlbie*/

 VPN_SHIFT can be atmost 12 */

	/*

	 * clear top 16 bits of 64 bit va, non SLS segment

	 * Older versions of the architecture (2.02 and earler) require the

	 * masking of the top 16 bits.

 clear out bits after(52) [0....52.....63] */

 We need 14 to 14 + i bits of va */

		/*

		 * AVAL bits:

		 * We don't need all the bits, but rest of the bits

		 * must be ignored by the processor.

		 * vpn cover upto 65 bits of va. (0...65) and we need

		 * 58..64 bits of va.

 L */

 retry with lock held */

 Guarantee the second dword is visible before the valid bit */

	/*

	 * Now set the first dword including the valid bit

	 * NOTE: this also unlocks the hpte

 pick a random entry to start at */

 retry with lock held */

 Invalidate the hpte. NOTE: this also unlocks it */

	/*

	 * We need to invalidate the TLB always because hpte_remove doesn't do

	 * a tlb invalidate. If a hash bucket gets full, we "evict" a more/less

	 * random entry from it. When we do that we don't invalidate the TLB

	 * (hpte_remove) because we assume the old translation is still

	 * technically "valid".

 recheck with locks held */

 Update the HPTE */

	/*

	 * Ensure it is out of the tlb too if it is not a nohpte fault

 HPTE matches */

	/*

	 * We try to keep bolted entries always in primary hash

	 * But in some case we can find them in secondary too.

 Try in secondary */

/*

 * Update the page protection bits. Intended to be used to create

 * guard pages for kernel data structures on pages which are bolted

 * in the HPT. Assumes pages being operated on will not be stolen.

 *

 * No need to lock here because we should be the only user.

 Update the HPTE */

	/*

	 * Ensure it is out of the tlb too. Bolted entries base and

	 * actual page size will be same.

/*

 * Remove a bolted kernel entry. Memory hotplug uses this.

 *

 * No need to lock here because we should be the only user.

 Invalidate the hpte */

 Invalidate the TLB */

 recheck with locks held */

 Invalidate the hpte. NOTE: this also unlocks it */

	/*

	 * We need to invalidate the TLB always because hpte_remove doesn't do

	 * a tlb invalidate. If a hash bucket gets full, we "evict" a more/less

	 * random entry from it. When we do that we don't invalidate the TLB

	 * (hpte_remove) because we assume the old translation is still

	 * technically "valid".

 get the vpn */

 Even if we miss, we need to invalidate the TLB */

 recheck with locks held */

				/*

				 * Invalidate the hpte. NOTE: this also unlocks it

		/*

		 * We need to do tlb invalidate for all the address, tlbie

		 * instruction compares entry_VA in tlb with the VA specified

		 * here

 Look at the 8 bit LP value */

 This works for all page sizes, and for 256M and 1T segments */

 We only have 28 - 23 bits of seg_off in avpn */

 We can find more bits from the pteg value */

 We only have 40 - 23 bits of seg_off in avpn */

/*

 * clear all mappings on kexec.  All cpus are in real mode (or they will

 * be when they isi), and we are the only one left.  We rely on our kernel

 * mapping being 0xC0's and the hardware ignoring those two real bits.

 *

 * This must be called with interrupts disabled.

 *

 * Taking the native_tlbie_lock is unsafe here due to the possibility of

 * lockdep being on. On pre POWER5 hardware, not taking the lock could

 * cause deadlock. POWER5 and newer not taking the lock is fine. This only

 * gets called during boot before secondary CPUs have come up and during

 * crashdump and all bets are off anyway.

 *

 * TODO: add batching support when enabled.  remember, no dynamic memory here,

 * although there is the control page available...

		/*

		 * we could lock the pte here, but we are the only cpu

		 * running,  right?  and for crash dump, we probably

		 * don't want to wait for a maybe bad cpu.

		/*

		 * Call __tlbie() here rather than tlbie() since we can't take the

		 * native_tlbie_lock.

/*

 * Batched hash table flush, we batch the tlbie's to avoid taking/releasing

 * the lock all the time

 lock and try again */

		/*

		 * Just do one more with the last used values.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * This file contains the routines for flushing entries from the

 * TLB and MMU hash table.

 *

 *  Derived from arch/ppc64/mm/init.c:

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)

 *  and Cort Dougan (PReP) (cort@cs.nmt.edu)

 *    Copyright (C) 1996 Paul Mackerras

 *

 *  Derived from "arch/i386/mm/init.c"

 *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 *

 *  Dave Engebretsen <engebret@us.ibm.com>

 *      Rework for PPC64 port.

/*

 * A linux PTE was changed and the corresponding hash table entry

 * neesd to be flushed. This function will either perform the flush

 * immediately or will batch it up if the current CPU has an active

 * batch on it.

	/*

	 * Get page size (maybe move back to caller).

	 *

	 * NOTE: when using special 64K mappings in 4K environment like

	 * for SPEs, we obtain the page size from the slice, which thus

	 * must still exist (and thus the VMA not reused) at the time

	 * of this call

 Mask the address for the correct page size */

 shutup gcc */

		/*

		 * Mask the address for the standard page size.  If we

		 * have a 64k page kernel, but the hardware does not

		 * support 64k pages, this might be different from the

		 * hardware page size encoded in the slice table.

 Build full vaddr */

	/*

	 * Check if we have an active batch on this CPU. If not, just

	 * flush now and return.

	/*

	 * This can happen when we are in the middle of a TLB batch and

	 * we encounter memory pressure (eg copy_page_range when it tries

	 * to allocate a new pte). If we have to reclaim memory and end

	 * up scanning and resetting referenced bits then our batch context

	 * will change mid stream.

	 *

	 * We also need to ensure only one page size is present in a given

	 * batch

/*

 * This function is called when terminating an mmu batch or when a batch

 * is full. It will perform the flush of all the entries currently stored

 * in a batch.

 *

 * Must be called from within some kind of spinlock/non-preempt region...

	/*

	 * If there's a TLB batch pending, then we must flush it because the

	 * pages are going to be freed and we really don't want to have a CPU

	 * access a freed page because it has a stale TLB

/**

 * __flush_hash_table_range - Flush all HPTEs for a given address range

 *                            from the hash table (and the TLB). But keeps

 *                            the linux PTEs intact.

 *

 * @start	: starting address

 * @end         : ending address (not included in the flush)

 *

 * This function is mostly to be used by some IO hotplug code in order

 * to remove all hash entries from a given address range used to map IO

 * space on a removed PCI-PCI bidge without tearing down the full mapping

 * since 64K pages may overlap with other bridges when using 64K pages

 * with 4K HW pages on IO space.

 *

 * Because of that usage pattern, it is implemented for small size rather

 * than speed.

	/*

	 * Note: Normally, we should only ever use a batch within a

	 * PTE locked section. This violates the rule, but will work

	 * since we don't actually modify the PTEs, we just flush the

	 * hash while leaving the PTEs intact (including their reference

	 * to being hashed). This is not the most performance oriented

	 * way to do things but is fine for our needs here.

	/*

	 * Note: Normally, we should only ever use a batch within a

	 * PTE locked section. This violates the rule, but will work

	 * since we don't actually modify the PTEs, we just flush the

	 * hash while leaving the PTEs intact (including their reference

	 * to being hashed). This is not the most performance oriented

	 * way to do things but is fine for our needs here.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * PowerPC 64-bit swsusp implementation

 *

 * Copyright 2006 Johannes Berg <johannes@sipsolutions.net>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * PCI address cache; allows the lookup of PCI devices based on I/O address

 *

 * Copyright IBM Corporation 2004

 * Copyright Linas Vepstas <linas@austin.ibm.com> 2004

/**

 * DOC: Overview

 *

 * The pci address cache subsystem.  This subsystem places

 * PCI device address resources into a red-black tree, sorted

 * according to the address range, so that given only an i/o

 * address, the corresponding PCI device can be **quickly**

 * found. It is safe to perform an address lookup in an interrupt

 * context; this ability is an important feature.

 *

 * Currently, the only customer of this code is the EEH subsystem;

 * thus, this code has been somewhat tailored to suit EEH better.

 * In particular, the cache does *not* hold the addresses of devices

 * for which EEH is not enabled.

 *

 * (Implementation Note: The RB tree seems to be better/faster

 * than any hash algo I could think of for this problem, even

 * with the penalty of slow pointer chases for d-cache misses).

/**

 * eeh_addr_cache_get_dev - Get device, given only address

 * @addr: mmio (PIO) phys address or i/o port number

 *

 * Given an mmio phys address, or a port number, find a pci device

 * that implements this address.  I/O port numbers are assumed to be offset

 * from zero (that is, they do *not* have pci_io_addr added in).

 * It is safe to call this function within an interrupt.

/*

 * Handy-dandy debug print routine, does nothing more

 * than print out the contents of our addr cache.

 Insert address range into the rb tree. */

 Walk tree, find a place to insert into tree */

 Skip any devices for which EEH is not enabled. */

	/*

	 * Walk resources on this device, poke the first 7 (6 normal BAR and 1

	 * ROM BAR) into the tree.

 We are interested only bus addresses, not dma or other stuff */

/**

 * eeh_addr_cache_insert_dev - Add a device to the address cache

 * @dev: PCI device whose I/O addresses we are interested in.

 *

 * In order to support the fast lookup of devices based on addresses,

 * we maintain a cache of devices that can be quickly searched.

 * This routine adds a device to that cache.

/**

 * eeh_addr_cache_rmv_dev - remove pci device from addr cache

 * @dev: device to remove

 *

 * Remove a device from the addr-cache tree.

 * This is potentially expensive, since it will walk

 * the tree multiple times (once per resource).

 * But so what; device removal doesn't need to be that fast.

/**

 * eeh_addr_cache_init - Initialize a cache of I/O addresses

 *

 * Initialize a cache of pci i/o addresses.  This cache will be used to

 * find the pci device that corresponds to a given address.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Derived from "arch/i386/kernel/process.c"

 *    Copyright (C) 1995  Linus Torvalds

 *

 *  Updated and modified by Cort Dougan (cort@cs.nmt.edu) and

 *  Paul Mackerras (paulus@cs.anu.edu.au)

 *

 *  PowerPC version

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 Transactional Memory debug */

/*

 * Are we running in "Suspend disabled" mode? If so we have to block any

 * sigreturn that would get us into suspended state, and we also warn in some

 * other paths that we should never reach with suspend disabled.

	/*

	 * If we are saving the current thread's registers, and the

	 * thread is in a transactional state, set the TIF_RESTORE_TM

	 * bit so that we know to restore the registers before

	 * returning to userspace.

 CONFIG_PPC_TRANSACTIONAL_MEM */

 notrace because it's called by restore_math */

 notrace because it's called by restore_math */

/*

 * Make sure the floating-point register state in the

 * the thread_struct is up to date for task tsk.

		/*

		 * We need to disable preemption here because if we didn't,

		 * another process could get scheduled after the regs->msr

		 * test but before we have finished saving the FP registers

		 * to the thread_struct.  That process could take over the

		 * FPU, and then when we get scheduled again we would store

		 * bogus values for the remaining FP registers.

			/*

			 * This should only ever be called for current or

			 * for a stopped child process.  Since we save away

			 * the FP register state on context switch,

			 * there is something wrong if a stopped child appears

			 * to still have its FP state in the CPU registers.

		/*

		 * If a thread has already been reclaimed then the

		 * checkpointed registers are on the CPU but have definitely

		 * been saved by the reclaim code. Don't need to and *cannot*

		 * giveup as this would save  to the 'live' structure not the

		 * checkpointed structure.

 CONFIG_PPC_FPU */

		/*

		 * If a thread has already been reclaimed then the

		 * checkpointed registers are on the CPU but have definitely

		 * been saved by the reclaim code. Don't need to and *cannot*

		 * giveup as this would save  to the 'live' structure not the

		 * checkpointed structure.

/*

 * Make sure the VMX/Altivec register state in the

 * the thread_struct is up to date for task tsk.

 CONFIG_ALTIVEC */

	/*

	 * We should never be ssetting MSR_VSX without also setting

	 * MSR_FP and MSR_VEC

 __giveup_fpu will clear MSR_VSX */

		/*

		 * If a thread has already been reclaimed then the

		 * checkpointed registers are on the CPU but have definitely

		 * been saved by the reclaim code. Don't need to and *cannot*

		 * giveup as this would save  to the 'live' structure not the

		 * checkpointed structure.

 CONFIG_VSX */

 CONFIG_SPE */

 CONFIG_PPC_FPU */

 CONFIG_ALTIVEC */

 CONFIG_VSX */

/*

 * The exception exit path calls restore_math() with interrupts hard disabled

 * but the soft irq state not "reconciled". ftrace code that calls

 * local_irq_save/restore causes warnings.

 *

 * Rather than complicate the exit path, just don't trace restore_math. This

 * could be done by having ftrace entry code check for this un-reconciled

 * condition where MSR[EE]=0 and PACA_IRQ_HARD_DIS is not set, and

 * temporarily fix it up for the duration of the ftrace call.

	/*

	 * new_msr tracks the facilities that are to be restored. Only reload

	 * if the bit is not set in the user MSR (if it is set, the registers

	 * are live for the user thread).

 This also covers VSX, because VSX implies FP

 CONFIG_PPC_BOOK3S_64 */

 Deliver the signal to userspace */

 breakpoint or watchpoint id */

 !CONFIG_PPC_ADV_DEBUG_REGS */

	/*

	 * If underneath hw supports only one watchpoint, we know it

	 * caused exception. 8xx also falls into this category.

 Otherwise findout which DAWR caused exception and disable it. */

	/*

	 * We reach here only when watchpoint exception is generated by ptrace

	 * event (or hw is buggy!). Now if CONFIG_HAVE_HW_BREAKPOINT is set,

	 * watchpoint is already handled by hw_breakpoint_handler() so we don't

	 * have to do anything. But when CONFIG_HAVE_HW_BREAKPOINT is not set,

	 * we need to manually handle the watchpoint here.

 Deliver the signal to userspace */

 CONFIG_PPC_ADV_DEBUG_REGS */

/*

 * Set the debug registers back to their default "safe" values.

	/*

	 * Force User/Supervisor bits to b11 (user-only MSR[PR]=1)

	/*

	 * Force Data Address Compare User/Supervisor bits to be User-only

	 * (0b11 MSR[PR]=1) and set all other bits in DBCR2 register to be 0.

	/*

	 * We could have inherited MSR_DE from userspace, since

	 * it doesn't get cleared on exception entry.  Make sure

	 * MSR_DE is clear before we enable any debug events.

/*

 * Unless neither the old or new thread are making use of the

 * debug registers, set the debug registers from the values

 * stored in the new thread.

 !CONFIG_PPC_ADV_DEBUG_REGS */

 no need to check hw_len. it's calculated from address and len */

 !CONFIG_HAVE_HW_BREAKPOINT */

 CONFIG_PPC_ADV_DEBUG_REGS */

 Power8 or later

 Power7 or earlier

 Shouldn't happen due to higher level checks

 Check if we have DAWR or DABR hardware */

 POWER8 DAWR or POWER9 forced DAWR */

 POWER9 with DAWR disabled */

 DABR: Everything but POWER8 and POWER9 */

	/*

	 * Use the current MSR TM suspended bit to track if we have

	 * checkpointed state outstanding.

	 * On signal delivery, we'd normally reclaim the checkpointed

	 * state to obtain stack pointer (see:get_tm_stackpointer()).

	 * This will then directly return to userspace without going

	 * through __switch_to(). However, if the stack frame is bad,

	 * we need to exit this thread which calls __switch_to() which

	 * will again attempt to reclaim the already saved tm state.

	 * Hence we need to check that we've not already reclaimed

	 * this state.

	 * We do this using the current MSR, rather tracking it in

	 * some specific thread_struct bit, as it has the additional

	 * benefit of checking for a potential TM bad thing exception.

	/*

	 * If we are in a transaction and FP is off then we can't have

	 * used FP inside that transaction. Hence the checkpointed

	 * state is the same as the live state. We need to copy the

	 * live state to the checkpointed state so that when the

	 * transaction is restored, the checkpointed state is correct

	 * and the aborted transaction sees the correct state. We use

	 * ckpt_regs.msr here as that's what tm_reclaim will use to

	 * determine if it's going to write the checkpointed state or

	 * not. So either this will write the checkpointed registers,

	 * or reclaim will. Similarly for VMX.

	/* We have to work out if we're switching from/to a task that's in the

	 * middle of a transaction.

	 *

	 * In switching we need to maintain a 2nd register state as

	 * oldtask->thread.ckpt_regs.  We tm_reclaim(oldproc); this saves the

	 * checkpointed (tbegin) state in ckpt_regs, ckfp_state and

	 * ckvr_state

	 *

	 * We also context switch (save) TFHAR/TEXASR/TFIAR in here.

	/* Always save the regs here, even if a transaction's not active.

	 * This context-switches a thread's TM info SPRs.  We do it here to

	 * be consistent with the restore path (in recheckpoint) which

	 * cannot happen later in _switch().

	/* We really can't be interrupted here as the TEXASR registers can't

	 * change and later in the trecheckpoint code, we have a userspace R1.

	 * So let's hard disable over this region.

	/* The TM SPRs are restored here, so that TEXASR.FS can be set

	 * before the trecheckpoint and no explosion occurs.

	/* Recheckpoint the registers of the thread we're about to switch to.

	 *

	 * If the task was using FP, we non-lazily reload both the original and

	 * the speculative FP register states.  This is because the kernel

	 * doesn't see if/when a TM rollback occurs, so if we take an FP

	 * unavailable later, we are unable to determine which set of FP regs

	 * need to be restored.

 Recheckpoint to restore original checkpointed register state. */

	/*

	 * The checkpointed state has been restored but the live state has

	 * not, ensure all the math functionality is turned off to trigger

	 * restore_math() to reload.

/*

 * This is called if we are on the way out to userspace and the

 * TIF_RESTORE_TM flag is set.  It checks if we need to reload

 * FP and/or vector state and does so if necessary.

 * If userspace is inside a transaction (whether active or

 * suspended) and FP/VMX/VSX instructions have ever been enabled

 * inside that transaction, then we have to keep them enabled

 * and keep the FP/VMX/VSX state loaded while ever the transaction

 * continues.  The reason is that if we didn't, and subsequently

 * got a FP/VMX/VSX unavailable interrupt inside a transaction,

 * we don't know whether it's the same transaction, and thus we

 * don't know which of the checkpointed state and the transactional

 * state to use.

	/*

	 * This is the only moment we should clear TIF_RESTORE_TM as

	 * it is here that ckpt_regs.msr and pt_regs.msr become the same

	 * again, anything else could lead to an incorrect ckpt_msr being

	 * saved and therefore incorrect signal contexts.

 Ensure that restore_math() will restore */

 !CONFIG_PPC_TRANSACTIONAL_MEM */

 CONFIG_PPC_TRANSACTIONAL_MEM */

		/*

		 * Note that the TAR is not available for use in the kernel.

		 * (To provide this, the TAR should be backed up/restored on

		 * exception entry/exit instead, and be in pt_regs.  FIXME,

		 * this should be in pt_regs anyway (for debug).)

	/*

	 * On POWER9 the copy-paste buffer can only paste into

	 * foreign real addresses, so unprivileged processes can not

	 * see the data or use it in any way unless they have

	 * foreign real mappings. If the new process has the foreign

	 * real address mappings, we must issue a cp_abort to clear

	 * any state and prevent snooping, corruption or a covert

	 * channel. ISA v3.1 supports paste into local memory.

 CONFIG_PPC_BOOK3S_64 */

/*

 * For PPC_BOOK3S_64, we use the hw-breakpoint interfaces that would

 * schedule DABR

 CONFIG_HAVE_HW_BREAKPOINT */

	/*

	 * We need to save SPRs before treclaim/trecheckpoint as these will

	 * change a number of them.

 Save FPU, Altivec, VSX and SPE state */

		/*

		 * We can't take a PMU exception inside _switch() since there

		 * is a window where the kernel stack SLB and the kernel stack

		 * are out of sync. Hard disable here.

	/*

	 * Call restore_sprs() and set_return_regs_changed() before calling

	 * _switch(). If we move it after _switch() then we miss out on calling

	 * it for new tasks. The reason for this is we manually create a stack

	 * frame for new tasks that directly returns through ret_from_fork() or

	 * ret_from_kernel_thread(). See copy_thread() for details.

 _switch changes stack (and regs) */

	/*

	 * Nothing after _switch will be run for newly created tasks,

	 * because they switch directly to ret_from_fork/ret_from_kernel_thread

	 * etc. Code added here should have a comment explaining why that is

	 * okay.

	/*

	 * This applies to a process that was context switched while inside

	 * arch_enter_lazy_mmu_mode(), to re-activate the batch that was

	 * deactivated above, before _switch(). This will never be the case

	 * for new tasks.

	/*

	 * Math facilities are masked out of the child MSR in copy_thread.

	 * A new task does not need to restore_math because it will

	 * demand fault them.

 CONFIG_PPC_BOOK3S_64 */

	/*

	 * If we were executing with the MMU off for instructions, adjust pc

	 * rather than printing XXXXXXXX.

 enough for 8 times 9 + 2 chars */

/*

 * This only prints something if at least one of the TM bit is set.

 * Inside the TM[], the output means:

 *   E: Enabled		(bit 32)

 *   S: Suspended	(bit 33)

 *   T: Transactional	(bit 34)

	/*

	 * Lookup NIP late so we have the best change of getting the

	 * above info out without failing

 CONFIG_HAVE_HW_BREAKPOINT */

 CONFIG_HAVE_HW_BREAKPOINT */

	/*

	 * If we exec out of a kernel thread then thread.regs will not be

	 * set.  Do it now.

/**

 * Assign a TIDR (thread ID) for task @t and set it in the thread

 * structure. For now, we only support setting TIDR for 'current' task.

 *

 * Since the TID value is a truncated form of it PID, it is possible

 * (but unlikely) for 2 threads to have the same TID. In the unlikely event

 * that 2 threads share the same TID and are waiting, one of the following

 * cases will happen:

 *

 * 1. The correct thread is running, the wrong thread is not

 * In this situation, the correct thread is woken and proceeds to pass it's

 * condition check.

 *

 * 2. Neither threads are running

 * In this situation, neither thread will be woken. When scheduled, the waiting

 * threads will execute either a wait, which will return immediately, followed

 * by a condition check, which will pass for the correct thread and fail

 * for the wrong thread, or they will execute the condition check immediately.

 *

 * 3. The wrong thread is running, the correct thread is not

 * The wrong thread will be woken, but will fail it's condition check and

 * re-execute wait. The correct thread, when scheduled, will execute either

 * it's condition check (which will pass), or wait, which returns immediately

 * when called the first time after the thread is scheduled, followed by it's

 * condition check (which will pass).

 *

 * 4. Both threads are running

 * Both threads will be woken. The wrong thread will fail it's condition check

 * and execute another wait, while the correct thread will pass it's condition

 * check.

 *

 * @t: the task to set the thread ID for

 CONFIG_PPC64 */

/*

 * this gets called so that we can store coprocessor state into memory and

 * copy the current task into the new thread.

	/*

	 * Flush TM state out so we can copy it.  __switch_to_tm() does this

	 * flush but it removes the checkpointed state from the current CPU and

	 * transitions the CPU out of TM mode.  Hence we need to call

	 * tm_recheckpoint_new_task() (on the same task) to restore the

	 * checkpointed state back and the TM mode.

	 *

	 * Can't pass dst because it isn't ready. Doesn't matter, passing

	 * dst is only important for __switch_to()

/*

 * Copy a thread..

/*

 * Copy architecture-specific thread state

 Copy registers */

 kernel thread */

 function */

 no user register state */

 user thread */

 64s sets this in ret_from_fork */

 Result from fork() */

	/*

	 * The way this works is that at some point in the future

	 * some task will call _switch to switch to the new task.

	 * That will pop off the stack frame created below and start

	 * the new task running at ret_from_fork.  The new task will

	 * do some house keeping and then return from the fork or clone

	 * system call, using the stack frame created above.

	/*

	 * Run with the current AMR value of the kernel

/*

 * Set up a thread for executing a new program

 saved by ELF_PLAT_INIT */

	/*

	 * Clear any transactional state, we're exec()ing. The cause is

	 * not important as there will never be a recheckpoint so it's not

	 * user visible.

 Look ma, no function descriptors! */

			/*

			 * Ulrich says:

			 *   The latest iteration of the ABI requires that when

			 *   calling a function (at its global entry point),

			 *   the caller must ensure r12 holds the entry point

			 *   address (so that the function can quickly

			 *   establish addressability).

 Make sure that's restored on entry to userspace. */

			/* start is a relocated pointer to the function

			 * descriptor for the elf _start routine.  The first

			 * entry in the function descriptor is the entry

			 * address of _start and the second entry is the TOC

			 * value we need to use.

			/* Check whether the e_entry function descriptor entries

			 * need to be relocated before we can use them.

 Java mode disabled */

 CONFIG_ALTIVEC */

 CONFIG_SPE */

 CONFIG_PPC_TRANSACTIONAL_MEM */

	/* This is a bit hairy.  If we are an SPE enabled  processor

	 * (have embedded fp) we store the IEEE exception enable flags in

	 * fpexc_mode.  fpexc_mode is also used for setting FP exception

			/*

			 * When the sticky exception bits are set

			 * directly by userspace, it must call prctl

			 * with PR_GET_FPEXC (with PR_FP_EXC_SW_ENABLE

			 * in the existing prctl settings) or

			 * PR_SET_FPEXC (with PR_FP_EXC_SW_ENABLE in

			 * the bits being set).  <fenv.h> functions

			 * saving and restoring the whole

			 * floating-point environment need to do so

			 * anyway to restore the prctl settings from

			 * the saved environment.

	/* on a CONFIG_SPE this does not hurt us.  The bits that

	 * __pack_fe01 use do not overlap with bits used for

	 * PR_FP_EXC_SW_ENABLE.  Additionally, the MSR[FE0,FE1] bits

	 * on CONFIG_SPE implementations are reserved so writing to

			/*

			 * When the sticky exception bits are set

			 * directly by userspace, it must call prctl

			 * with PR_GET_FPEXC (with PR_FP_EXC_SW_ENABLE

			 * in the existing prctl settings) or

			 * PR_SET_FPEXC (with PR_FP_EXC_SW_ENABLE in

			 * the bits being set).  <fenv.h> functions

			 * saving and restoring the whole

			 * floating-point environment need to do so

			 * anyway to restore the prctl settings from

			 * the saved environment.

		/*

		 * See if this is an exception frame.

		 * We look for the "regshere" marker in the current frame.

 Called with hard IRQs off */

		/*

		 * Least significant bit (RUN) is the only writable bit of

		 * the CTRL register, so we can avoid mfspr. 2.06 is not the

		 * earliest ISA where this is the case, but it's convenient.

		/*

		 * Some architectures (e.g., Cell) have writable fields other

		 * than RUN, so do the read-modify-write.

 Called with hard IRQs off */

 CONFIG_PPC64 */

 8MB for 32bit, 1GB for 64bit */

	/*

	 * If we are using 1TB segments and we are allowed to randomise

	 * the heap, we can put it above 1TB so it is backed by a 1TB

	 * segment. Otherwise the heap will be in the bottom 1TB

	 * which always uses 256MB segments and this may result in a

	 * performance penalty. We don't need to worry about radix. For

	 * radix, mmu_highuser_ssize remains unchanged from 256MB.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Dynamic Ftrace based Kprobes Optimization

 *

 * Copyright (C) Hitachi Ltd., 2012

 * Copyright 2016 Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>

 *		  IBM Corporation

 Ftrace callback handler for kprobes */

		/*

		 * On powerpc, NIP is *before* this instruction for the

		 * pre handler

			/*

			 * Emulate singlestep (and also recover regs->nip)

			 * as if there is a nop

		/*

		 * If pre_handler returns !0, it changes regs->nip. We have to

		 * skip emulating post_handler.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Helper routines to scan the device tree for PCI devices and busses

 *

 * Migrated out of PowerPC architecture pci_64.c file by Grant Likely

 * <grant.likely@secretlab.ca> so that these routines are available for

 * 32 bit also.

 *

 * Copyright (C) 2003 Anton Blanchard <anton@au.ibm.com>, IBM

 *   Rework, based on alpha PCI code.

 * Copyright (c) 2009 Secret Lab Technologies Ltd.

/**

 * get_int_prop - Decode a u32 from a device tree property

/**

 * pci_parse_of_flags - Parse the flags cell of a device tree PCI address

 * @addr0: value of 1st cell of a device tree PCI address.

 * @bridge: Set this flag if the address is from a bridge 'ranges' property

 *

 * PCI Bus Binding to IEEE Std 1275-1994

 *

 * Bit#            33222222 22221111 11111100 00000000

 *                 10987654 32109876 54321098 76543210

 * phys.hi cell:   npt000ss bbbbbbbb dddddfff rrrrrrrr

 * phys.mid cell:  hhhhhhhh hhhhhhhh hhhhhhhh hhhhhhhh

 * phys.lo cell:   llllllll llllllll llllllll llllllll

 *

 * where:

 * n        is 0 if the address is relocatable, 1 otherwise

 * p        is 1 if the addressable region is "prefetchable", 0 otherwise

 * t        is 1 if the address is aliased (for non-relocatable I/O),

 *          below 1 MB (for Memory),or below 64 KB (for relocatable I/O).

 * ss       is the space code, denoting the address space:

 *              00 denotes Configuration Space

 *              01 denotes I/O Space

 *              10 denotes 32-bit-address Memory Space

 *              11 denotes 64-bit-address Memory Space

 * bbbbbbbb is the 8-bit Bus Number

 * ddddd    is the 5-bit Device Number

 * fff      is the 3-bit Function Number

 * rrrrrrrr is the 8-bit Register Number

		/* Note: We don't know whether the ROM has been left enabled

		 * by the firmware or not. We mark it as disabled (ie, we do

		 * not set the IORESOURCE_ROM_ENABLE flag) for now rather than

		 * do a config space read, it will be force-enabled if needed

/**

 * of_pci_parse_addrs - Parse PCI addresses assigned in the device tree node

 * @node: device tree node for the PCI device

 * @dev: pci_dev structure for the device

 *

 * This function parses the 'assigned-addresses' property of a PCI devices'

 * device tree node and writes them into the associated pci_dev structure.

/**

 * of_create_pci_dev - Given a device tree node on a pci bus, create a pci_dev

 * @node: device tree node pointer

 * @bus: bus the device is sitting on

 * @devfn: PCI function number, extracted from device tree by caller.

 maybe a lie? */

 pcie fundamental reset required */

 unknown power state */

 Early fixups, before probing the BARs */

 a PCI-PCI bridge */

 Maybe do a default OF mapping here */

/**

 * of_scan_pci_bridge - Set up a PCI bridge and scan for child nodes

 * @dev: pci_dev structure for the bridge

 *

 * of_scan_bus() calls this routine for each PCI bridge that it finds, and

 * this routine in turn call of_scan_bus() recusively to scan for more child

 * devices.

 parse bus-range property */

 parse ranges property */

 PCI #address-cells == 3 and #size-cells == 2 always */

 Check if the PCI device is already there */

 Device removed permanently ? */

 create a new pci_dev for this device */

/**

 * __of_scan_bus - given a PCI bus node, setup bus and scan for child devices

 * @node: device tree node for the PCI bus

 * @bus: pci_bus structure for the PCI bus

 * @rescan_existing: Flag indicating bus has already been set up

 Scan direct children */

	/* Apply all fixups necessary. We don't fixup the bus "self"

	 * for an existing bridge that is being rescanned

 Now scan child busses */

/**

 * of_scan_bus - given a PCI bus node, setup bus and scan for child devices

 * @node: device tree node for the PCI bus

 * @bus: pci_bus structure for the PCI bus

/**

 * of_rescan_bus - given a PCI bus node, scan for child devices

 * @node: device tree node for the PCI bus

 * @bus: pci_bus structure for the PCI bus

 *

 * Same as of_scan_bus, but for a pci_bus structure that has already been

 * setup.

 SPDX-License-Identifier: GPL-2.0

/*

 * temp.c	Thermal management for cpu's with Thermal Assist Units

 *

 * Written by Troy Benjegerdes <hozer@drgw.net>

 *

 * TODO:

 * dynamic power management to limit peak CPU temp (using ICTC)

 * calibration???

 *

 * Silly, crazy ideas: use cpu load (from scheduler) and ICTC to extend battery

 * life in portables, and add a 'performance/watt' metric somewhere in /proc

/* TODO: put these in a /proc interface, with some sanity checks, and maybe

/* configurable values for step size and how much to expand the window when

 step size when temp goes out of range */

 expand the window by this much */

 configurable values for shrinking the window */

 period between shrinking the window */

 minimum window size, degrees C */

 setup THRM1, threshold, valid bit, interrupt when below threshold */

 setup THRM2, threshold, valid bit, interrupt when above threshold */

	/* if both thresholds are crossed, the step_sizes cancel out

/*

 * TAU interrupts - called when we have a thermal assist unit interrupt

 * with interrupts disabled

 CONFIG_TAU_INT */

 Stop thermal sensor comparisons and interrupts */

 do an exponential shrink of half the amount currently over size */

 size must have been min_window + 1 */

 debug */

	/* Restart thermal sensor comparisons and interrupts.

	 * The "PowerPC 740 and PowerPC 750 Microprocessor Datasheet"

	 * recommends that "the maximum value be set in THRM3 under all

	 * conditions."

 schedule ourselves to be run again */

/*

 * setup the TAU

 *

 * Set things up to use THRM1 as a temperature lower bound, and THRM2 as an upper bound.

 * Start off at zero

	/* set these to a reasonable value and let the timer shrink the

	/* We assume in SMP that if one CPU has TAU support, they

	 * all have it --BenH

/*

 * return current temp

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2001 Dave Engebretsen, IBM Corporation

 * Copyright (C) 2003 Anton Blanchard <anton@au.ibm.com>, IBM

 *

 * RTAS specific routines for PCI.

 *

 * Based on code from pci.c, chrp_pci.c and pSeries_pci.c

 RTAS tokens */

 Validity of pdn is checked in here */

 Validity of pdn is checked in here. */

 Python's register file is 1 MB in size. */

	/*

	 * Firmware doesn't always clear this bit which is critical

	 * for good performance - Anton

		/*

		 * We must read it back for changes to

		 * take effect

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Contains common pci routines for ALL ppc platform

 * (based on pci_32.c and pci_64.c)

 *

 * Port for PPC64 David Engebretsen, IBM Corp.

 * Contains common pci routines for ppc64 platform, pSeries and iSeries brands.

 *

 * Copyright (C) 2003 Anton Blanchard <anton@au.ibm.com>, IBM

 *   Rework, based on alpha PCI code.

 *

 * Common pmac/prep/chrp pci routines. -- Cort

 hose_spinlock protects accesses to the the phb_bitmap. */

 For dynamic PHB numbering on get_phb_number(): max number of PHBs. */

/*

 * For dynamic PHB numbering: used/free PHBs tracking bitmap.

 * Accesses to this bitmap should be protected by hose_spinlock.

 ISA Memory physical address */

/*

 * This function should run under locking protection, specifically

 * hose_spinlock.

	/*

	 * Try fixed PHB numbering first, by checking archs and reading

	 * the respective device-tree properties. Firstly, try powernv by

	 * reading "ibm,opal-phbid", only present in OPAL environment.

 We need to be sure to not use the same PHB number twice. */

	/*

	 * If not pseries nor powernv, or if fixed PHB numbering tried to add

	 * the same PHB number twice, then fallback to dynamic PHB numbering.

 Clear bit of phb_bitmap to allow reuse of this PHB number. */

/*

 * This function is used to call pcibios_free_controller()

 * in a deferred manner: a callback from the PCI subsystem.

 *

 * _*DO NOT*_ call pcibios_free_controller() explicitly if

 * this is used (or it may access an invalid *phb pointer).

 *

 * The callback occurs when all references to the root bus

 * are dropped (e.g., child buses/devices and their users).

 *

 * It's called as .release_fn() of 'struct pci_host_bridge'

 * which is associated with the 'struct pci_controller.bus'

 * (root bus) - it expects .release_data to hold a pointer

 * to 'struct pci_controller'.

 *

 * In order to use it, register .release_fn()/release_data

 * like this:

 *

 * pci_set_host_bridge_release(bridge,

 *                             pcibios_free_controller_deferred

 *                             (void *) phb);

 *

 * e.g. in the pcibios_root_bridge_prepare() callback from

 * pci_create_root_bus().

/*

 * The function is used to return the minimal alignment

 * for memory or I/O windows of the associated P2P bridge.

 * By default, 4KiB alignment for I/O windows and 1MiB for

 * memory windows.

	/*

	 * PCI core will figure out the default

	 * alignment: 4KiB for I/O and 1MiB for

	 * memory window.

 CONFIG_PCI_IOV */

/*

 * Return the domain number for this bus.

/* This routine is meant to be used early during boot, when the

 * PCI bus numbers have not yet been assigned, and you need to

 * issue PCI config cycles to an OF device.

 * It could also be used to "fix" RTAS config cycles if you want

 * to set pci_assign_all_buses to 1 and still use RTAS for PCI

 * config cycles.

/*

 * Reads the interrupt pin to determine if interrupt is use by card.

 * If the interrupt is used, then gets the interrupt line from the

 * openfirmware and sets it in the pci_dev and pci_config line.

 Preallocate vi as rewind is complex if this fails after mapping */

 Try to get a mapping from the device-tree */

		/* If that fails, lets fallback to what is in the config

		 * space and map that through the default controller. We

		 * also set the type to level low since that's what PCI

		 * interrupts are. If your platform does differently, then

		 * either provide a proper interrupt tree or don't use this

		 * function.

/*

 * Platform support for /proc/bus/pci/X/Y mmap()s.

 *  -- paulus.

 Convert to an offset within this PCI controller */

/*

 * This one is used by /dev/mem and fbdev who have no clue about the

 * PCI device, it tries to find the PCI device first and calls the

 * above routine

 Active and same type? */

 In the range of this resource? */

 This provides legacy IO read access on a bus */

	/* Check if port can be supported by that bus. We only check

	 * the ranges of the PHB though, not the bus itself as the rules

	 * for forwarding legacy cycles down bridges are not our problem

	 * here. So if the host bridge supports it, we do it.

 This provides legacy IO write access on a bus */

	/* Check if port can be supported by that bus. We only check

	 * the ranges of the PHB though, not the bus itself as the rules

	 * for forwarding legacy cycles down bridges are not our problem

	 * here. So if the host bridge supports it, we do it.

	/* WARNING: The generic code is idiotic. It gets passed a pointer

	 * to what can be a 1, 2 or 4 byte quantity and always reads that

	 * as a u32, which means that we have to correct the location of

	 * the data read within those 32 bits for size 1 and 2

 This provides legacy IO or memory mmap access on a bus */

		/* Hack alert !

		 *

		 * Because X is lame and can fail starting if it gets an error trying

		 * to mmap legacy_mem (instead of just moving on without legacy memory

		 * access) we fake it here by giving it anonymous memory, effectively

		 * behaving just like /dev/zero

	/* We pass a CPU physical address to userland for MMIO instead of a

	 * BAR value because X is lame and expects to be able to use that

	 * to pass to /dev/mem!

	 *

	 * That means we may have 64-bit values where some apps only expect

	 * 32 (like X itself since it thinks only Sparc has 64-bit MMIO).

/**

 * pci_process_bridge_OF_ranges - Parse PCI bridge resources from device tree

 * @hose: newly allocated pci_controller to be setup

 * @dev: device node of the host bridge

 * @primary: set if primary bus (32 bits only, soon to be deprecated)

 *

 * This function will parse the "ranges" property of a PCI host bridge device

 * node and setup the resource mapping of a pci controller based on its

 * content.

 *

 * Life would be boring if it wasn't for a few issues that we have to deal

 * with here:

 *

 *   - We can only cope with one IO space range and up to 3 Memory space

 *     ranges. However, some machines (thanks Apple !) tend to split their

 *     space into lots of small contiguous ranges. So we have to coalesce.

 *

 *   - Some busses have IO space not starting at 0, which causes trouble with

 *     the way we do our IO resource renumbering. The code somewhat deals with

 *     it for 64 bits but I would expect problems on 32 bits.

 *

 *   - Some 32 bits platforms such as 4xx can have physical space larger than

 *     32 bits so we need to use 64 bits values for the parsing

 Check for ranges property */

 Parse it */

		/* If we failed translation or got a zero-sized region

		 * (some FW try to feed us with non sensical zero sized regions

		 * such as power3 which look like some kind of attempt at exposing

		 * the VGA memory hole)

 Act based on address space type */

 We support only one IO range */

 On 32 bits, limit I/O space to 16MB */

 32 bits needs to map IOs here */

 Expect trouble if pci_addr is not 0 */

 CONFIG_PPC32 */

			/* pci_io_size and io_base_phys always represent IO

			 * space starting at 0 so we factor in pci_addr

 Build resource */

 We support only 3 memory ranges */

 Handles ISA memory hole space here */

 Build resource */

 Decide whether to display the domain number in /proc */

/* This header fixup will do the resource fixup for all devices as they are

 * probed, but not for bridge ranges

		/* If we're going to re-assign everything, we mark all resources

		 * as unset (and 0-base them). In addition, we mark BARs starting

		 * at 0 as unset as well, except if PCI_PROBE_ONLY is also set

		 * since in that case, we don't want to re-assign anything

 Only print message if not re-assigning */

 Call machine specific resource fixup */

/* This function tries to figure out if a bridge resource has been initialized

 * by the firmware or not. It doesn't have to be absolutely bullet proof, but

 * things go more smoothly when it gets it right. It should covers cases such

 * as Apple "closed" bridge resources and bare-metal pSeries unassigned bridges

 We don't do anything if PCI_PROBE_ONLY is set */

 Job is a bit different between memory and IO */

 If the BAR is non-0 then it's probably been initialized */

		/* The BAR is 0, let's check if memory decoding is enabled on

		 * the bridge. If not, we consider it unassigned

		/* Memory decoding is enabled and the BAR is 0. If any of the bridge

		 * resources covers that starting address (0 then it's good enough for

		 * us for memory space)

		/* Well, it starts at 0 and we know it will collide so we may as

		 * well consider it as unassigned. That covers the Apple case.

 If the BAR is non-0, then we consider it assigned */

		/* Here, we are a bit different than memory as typically IO space

		 * starting at low addresses -is- valid. What we do instead if that

		 * we consider as unassigned anything that doesn't have IO enabled

		 * in the PCI command register, and that's it.

		/* It's starting at 0 and IO is disabled in the bridge, consider

		 * it unassigned

 Fixup resources of a PCI<->PCI bridge */

		/* If we're going to reassign everything, we can

		 * shrink the P2P resource to have size as being

		 * of 0 in order to save space.

		/* Try to detect uninitialized P2P bridge resources,

		 * and clear them out so they get re-assigned later

 Fix up the bus resources for P2P bridges */

	/* Platform specific bus fixups. This is currently only used

	 * by fsl_pci and I'm hoping to get rid of it at some point

 Setup bus DMA mappings */

	/* Fixup NUMA node as it may not be setup yet by the generic

	 * code and is needed by the DMA init

 Hook up default DMA ops */

 Additional platform DMA/iommu setup */

 Read default IRQs and fixup if necessary */

 CONFIG_PCI_IOV */

 No special bus mastering setup handling */

	/* When called from the generic PCI probe, read PCI<->PCI bridge

	 * bases. This is -not- called when generating the PCI tree from

	 * the OF device-tree.

 Now fixup the bus bus */

/*

 * We need to avoid collisions with `mirrored' VGA ports

 * and other strange ISA hardware, so we always want the

 * addresses to be allocated in the 0x000-0x0ff region

 * modulo 0x400.

 *

 * Why? Because some silly external IO cards only decode

 * the low 10 bits of the IO address. The 0x00-0xff region

 * is reserved for motherboard devices that decode all 16

 * bits, so it's ok to allocate at, say, 0x2800-0x28ff,

 * but we want to try to avoid allocating at 0x2900-0x2bff

 * which might have be mirrored at 0x0100-0x03ff..

/*

 * Reparent resource children of pr that conflict with res

 * under res, and make res replace those children.

 not completely contained */

 didn't find any conflicting entries? */

/*

 *  Handle resources of PCI devices.  If the world were perfect, we could

 *  just allocate all the resource regions and do nothing more.  It isn't.

 *  On the other hand, we cannot just re-allocate all devices, as it would

 *  require us to know lots of host bridge internals.  So we attempt to

 *  keep as much of the original configuration as possible, but tweak it

 *  when it's found to be wrong.

 *

 *  Known BIOS problems we have to work around:

 *	- I/O or memory regions not configured

 *	- regions configured, but not enabled in the command register

 *	- bogus I/O addresses above 64K used

 *	- expansion ROMs left enabled (this may sound harmless, but given

 *	  the fact the PCI specs explicitly allow address decoders to be

 *	  shared between expansion ROMs and other resource regions, it's

 *	  at least dangerous)

 *

 *  Our solution:

 *	(1) Allocate resources for all buses behind PCI-to-PCI bridges.

 *	    This gives us fixed barriers on where we can allocate.

 *	(2) Allocate resources for all enabled devices.  If there is

 *	    a collision, just mark the resource as unallocated. Also

 *	    disable expansion ROMs during this step.

 *	(3) Try to allocate resources for disabled devices.  If the

 *	    resources were assigned correctly, everything goes well,

 *	    if they weren't, they won't disturb allocation of other

 *	    resources.

 *	(4) Assign new addresses to resources which were either

 *	    not configured at all or misconfigured.  If explicitly

 *	    requested by the user, configure expansion ROM address

 *	    as well.

 If the resource was left unset at this point, we clear it */

				/* this happens when the generic PCI

				 * code (wrongly) decides that this

				 * bridge is transparent  -- paulus

			/*

			 * Must be a conflict with an existing entry.

			 * Move that entry (or entries) under the

			 * bridge resource and try again.

		/* The resource might be figured out when doing

		 * reassignment based on the resources required

		 * by the downstream PCI devices. Here we set

		 * the size of the resource to be 0 in order to

		 * save more space.

 We'll assign a new address later */

 Already allocated */

 Not assigned at all */

			/* We only allocate ROMs on pass 1 just in case they

			 * have been screwed up by firmware

			/* Turn the ROM off, leave the resource region,

			 * but keep it unregistered.

 Check for IO */

 Check for memory */

 Allocate and assign resources */

	/* Before we start assigning unassigned resource, we try to reserve

	 * the low IO area and the VGA memory area if they intersect the

	 * bus available resources to avoid allocating things on top of them

	/* Now, if the platform didn't decide to blindly trust the firmware,

	 * we proceed to assigning things that were left unassigned

/* This is used by the PCI hotplug driver to allocate resource

 * of newly plugged busses. We can try to consolidate with the

 * rest of the code later, for now, keep it as-is as our main

 * resource allocation function doesn't deal with sub-trees yet.

/* pcibios_finish_adding_to_bus

 *

 * This is to be called by the hotplug code after devices have been

 * added to a bus, this include calling it for a PHB that is just

 * being added

 Allocate bus and devices resources */

 Add new devices to global lists.  Register in proc, sysfs. */

 Hookup PHB IO resource */

 Hookup PHB Memory resources */

/*

 * Null PCI config access functions, for the case when we can't

 * find a hose.

/*

 * These functions are used early on before PCI scanning is done

 * and all of the pci_dev and pci_bus structures have been created.

/**

 * pci_scan_phb - Given a pci_controller, setup and scan the PCI bus

 * @hose: Pointer to the PCI host controller instance structure

 Get some IO space for the new PHB */

 Wire up PHB bus resources */

 Create an empty bus for the toplevel */

 Get probe mode and perform scan */

	/* Platform gets a chance to do some global fixups before

	 * we proceed to resource allocation

 Configure PCI Express settings */

 When configured as agent, programing interface = 1 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*  Kernel module help for PPC64.

    Copyright (C) 2001, 2003 Rusty Russell IBM Corporation.



/* FIXME: We don't do .init separately.  To do this, we'd need to have

   a separate r2 value in the init and core section, and stub between

   them, too.



   Using a magic allocator which places modules within 32MB solves

   this, and makes other things simpler.  Anton?

 An address is simply the address of the function. */

 PowerPC64 specific values for the Elf64_Sym st_other field.  */

	/* sym->st_other indicates offset to local entry point

	 * (otherwise it will assume r12 is the address of the start

 An address is address of the OPD entry, which contains address of fn. */

 stub */

/* Like PPC32, we need little trampolines to do > 24-bit jumps (into

   the kernel itself).  But on PPC64, these need to be used for every

	/* 28 byte jump instruction sequence (7 instructions). We only

	 * need 6 instructions on ABIv2 but we always allocate 7 so

 Used by ftrace to identify stubs */

 Data for the above code */

/*

 * PPC64 uses 24 bit jumps, but we need to jump into other modules or

 * the kernel which may be further.  So we jump to a stub.

 *

 * For ELFv1 we need to use this to set up the new r2 value (aka TOC

 * pointer).  For ELFv2 it's the callee's responsibility to set up the

 * new r2, but for both we need to save the old r2.

 *

 * We could simply patch the new r2 value and function pointer into

 * the stub, but it's significantly shorter to put these values at the

 * end of the stub code, and patch the stub address (32-bits relative

 * to the TOC ptr, r2) into the stub.

 Save current r2 value in magic place on the stack. */

 Set up new r2 from function descriptor */

/* Count how many different 24-bit relocations (different symbol,

 FIXME: Only count external ones --RR */

 Only count 24-bit relocs, others don't need stubs */

	/* Compare the entire r_info (as opposed to ELF64_R_SYM(r_info) only) to

	 * make the comparison cheaper/faster. It won't affect the sorting or

	 * the counting algorithms' performance

 Get size of potential trampolines required. */

 One extra reloc so it's always 0-funcaddr terminated */

 Every relocated section... */

			/* Sort the relocation information based on a symbol and

			 * addend key. This is a stable O(n*log n) complexity

			 * alogrithm but it will reduce the complexity of

			 * count_relocs() to linear complexity O(n)

 make the trampoline to the ftrace_caller */

 an additional one for ftrace_regs_caller */

 Still needed for ELFv2, for .TOC. */

/*

 * Undefined symbols which refer to .funcname, hack to funcname. Make .TOC.

 * seem to be defined (value set later).

 Find .toc and .stubs sections, symtab and strtab */

 We don't handle .init for the moment: rename to _init */

	/* If we don't have a .toc, just use .stubs.  We need to set r2

	   to some reasonable value in case the module calls out to

	   other functions via a stub, or if a function pointer escapes

 Override the stubs size */

/*

 * For mprofile-kernel we use a special stub for ftrace_caller() because we

 * can't rely on r2 containing this module's TOC when we enter the stub.

 *

 * That can happen if the function calling us didn't need to use the toc. In

 * that case it won't have setup r2, and the r2 value will be either the

 * kernel's toc, or possibly another modules toc.

 *

 * To deal with that this stub uses the kernel toc, which is always accessible

 * via the paca (in r13). The target (ftrace_caller()) is responsible for

 * saving and restoring the toc before returning.

 Stub uses address relative to kernel toc (from the paca) */

 Eventhough we don't use funcdata in the stub, it's needed elsewhere. */

/*

 * r2 is the TOC pointer: it actually points 0x8000 into the TOC (this gives the

 * value maximum span in an instruction which uses a signed offset). Round down

 * to a 256 byte boundary for the odd case where we are setting up r2 without a

 * .toc section.

 Patch stub to reference function and correct r2 value. */

 Stub uses address relative to r2. */

/* Create stub to jump to function described in this OPD/ptr: we need the

 Find this stub, or if that fails, the next avail. entry */

/* We expect a noop next: if it is, replace it with instruction to

	/*

	 * Make sure the branch isn't a sibling call.  Sibling calls aren't

	 * "link" branches and they don't return, so they don't need the r2

	 * restore afterwards.

 ld r2,R2_STACK_OFFSET(r1) */

 First time we're called, we can fix up .TOC. */

		/* It's theoretically possible that a module doesn't want a

 This is where to make the change */

 This is the symbol it is referring to */

 `Everything is relative'. */

 Simply set it */

 Simply set it */

 Subtract TOC pointer */

 Subtract TOC pointer */

 Subtract TOC pointer */

 Subtract TOC pointer */

 Subtract TOC pointer */

 FIXME: Handle weak symbols here --RR */

 External: go via stub */

 Convert value to relative */

 Only replace bits 2 through 26 */

 64 bits relative (used by features fixups) */

 32 bits relative (used by relative exception tables) */

 Convert value to relative */

			/*

			 * Marker reloc indicates we don't have to save r2.

			 * That would only save us one instruction, so ignore

			 * it.

			/*

			 * Optimize ELFv2 large code model entry point if

			 * the TOC is within 2GB range of current location.

			/*

			 * Check for the large code model prolog sequence:

		         *	ld r2, ...(r12)

			 *	add r2, r2, r12

			/*

			 * If found, replace it with:

			 *	addis r2, r12, (.TOC.-func)@ha

			 *	addi  r2,  r2, (.TOC.-func)@l

 Subtract location pointer */

 Subtract location pointer */

 SPDX-License-Identifier: GPL-2.0

/*

 * Stack trace utility functions etc.

 *

 * Copyright 2008 Christoph Hellwig, IBM Corp.

 * Copyright 2018 SUSE Linux GmbH

 * Copyright 2018 Nick Piggin, Michael Ellerman, IBM Corp.

/*

 * This function returns an error if it detects any unreliable features of the

 * stack.  Otherwise it guarantees that the stack trace is reliable.

 *

 * If the task is not 'current', the caller *must* ensure the task is inactive.

		/*

		 * For user tasks, this is the SP value loaded on

		 * kernel entry, see "PACAKSAVE(r13)" in _switch() and

		 * system_call_common()/EXCEPTION_PROLOG_COMMON().

		 *

		 * Likewise for non-swapper kernel threads,

		 * this also happens to be the top of the stack

		 * as setup by copy_thread().

		 *

		 * Note that stack backlinks are not properly setup by

		 * copy_thread() and thus, a forked task() will have

		 * an unreliable stack trace until it's been

		 * _switch()'ed to for the first time.

		/*

		 * idle tasks have a custom stack layout,

		 * c.f. cpu_idle_thread_init().

 sanity check: ABI requires SP to be aligned 16 bytes. */

 Stack grows downwards; unwinder may only go up. */

 invalid backlink, too far up. */

		/*

		 * We can only trust the bottom frame's backlink, the

		 * rest of the frame may be uninitialized, continue to

		 * the next.

 Mark stacktraces with exception frames as unreliable. */

 Examine the saved LR: it must point into kernel code. */

		/*

		 * FIXME: IMHO these tests do not belong in

		 * arch-dependent code, they are generic.

		/*

		 * Mark stacktraces with kretprobed functions on them

		 * as unreliable.

 Now wait up to 5s for the other CPU to do its backtrace

 Other CPU cleared itself from the mask

 defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_NMI_IPI) */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Sysfs entries for PCI Error Recovery for PAPR-compliant platform.

 * Copyright IBM Corporation 2007

 * Copyright Linas Vepstas <linas@austin.ibm.com> 2007

 *

 * Send comments and feedback to Linas Vepstas <linas@austin.ibm.com>

/**

 * EEH_SHOW_ATTR -- Create sysfs entry for eeh statistic

 * @_name: name of file in sysfs directory

 * @_memb: name of member in struct eeh_dev to access

 * @_format: printf format for display

 *

 * All of the attributes look very similar, so just

 * auto-gen a cut-n-paste routine to display them.

 Nothing to do if it's not frozen */

 CONFIG_PCI_IOV && CONFIG PPC_PSERIES*/

	/*

	 * The parent directory might have been removed. We needn't

	 * continue for that case.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 IBM Corporation

 * Author: Nayna Jain

 *

 * This file initializes secvar operations for PowerPC Secureboot

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Copyright (C) 1995-1996  Gary Thomas (gdt@linuxppc.org)

 *  Copyright 2007-2010 Freescale Semiconductor, Inc.

 *

 *  Modified by Cort Dougan (cort@cs.nmt.edu)

 *  and Paul Mackerras (paulus@samba.org)

/*

 * This file handles the architecture-dependent parts of hardware exceptions

 print_modules */

 Transactional Memory trap debug */

/*

 * Trap & Exception support

/*

 * If oops/die is expected to crash the machine, return true here.

 *

 * This should not be expected to be 100% accurate, there may be

 * notifiers registered or other unexpected conditions that may bring

 * down the kernel. Or if the current process in the kernel is holding

 * locks or has other critical state, the kernel may become effectively

 * unusable anyway.

	/*

	 * These are mostly taken from kernel/panic.c, but tries to do

	 * relatively minimal work. Don't use delay functions (TB may

	 * be broken), don't crash dump (need to set a firmware log),

	 * don't run notifiers. We do want to get some information to

	 * Linux console.

 racy, but better than risking deadlock. */

 nested oops. should stop eventually */;

 Nest count reaches zero, release the lock. */

	/*

	 * system_reset_excption handles debugger, crash dump, panic, for 0x100

	/*

	 * While our oops output is serialised by a spinlock, output

	 * from panic() called below can race and corrupt it. If we

	 * know we are going to panic, delay for 1 second so we have a

	 * chance to get clean backtraces from all CPUs that are oopsing.

	/*

	 * system_reset_excption handles debugger, crash dump, panic, for 0x100

	/*

	 * Must not enable interrupts even for user-mode exception, because

	 * this can be called from machine check, which may be a NMI or IRQ

	 * which don't like interrupts being enabled. Could check for

	 * in_hardirq || in_nmi perhaps, but there doesn't seem to be a good

	 * reason why _exception() should enable irqs for an exception handler,

	 * the handlers themselves do that directly.

/*

 * The interrupt architecture has a quirk in that the HV interrupts excluding

 * the NMIs (0x100 and 0x200) do not clear MSR[RI] at entry. The first thing

 * that an interrupt handler must do is save off a GPR into a scratch register,

 * and all interrupts on POWERNV (HV=1) use the HSPRG1 register as scratch.

 * Therefore an NMI can clobber an HV interrupt's live HSPRG1 without noticing

 * that it is non-reentrant, which leads to random data corruption.

 *

 * The solution is for NMI interrupts in HV mode to check if they originated

 * from these critical HV interrupt regions. If so, then mark them not

 * recoverable.

 *

 * An alternative would be for HV NMIs to use SPRG for scratch to avoid the

 * HSPRG1 clobber, however this would cause guest SPRG to be clobbered. Linux

 * guests should always have MSR[RI]=0 when its scratch SPRG is in use, so

 * that would work. However any other guest OS that may have the SPRG live

 * and MSR[RI]=1 could encounter silent corruption.

 *

 * Builds that do not support KVM could take this second option to increase

 * the recoverability of NMIs.

	/*

	 * Now test if the interrupt has hit a range that may be using

	 * HSPRG1 without having RI=0 (i.e., an HSRR interrupt). The

	 * problem ranges all run un-relocated. Test real and virt modes

	 * at the same time by dropping the high bit of the nip (virt mode

	 * entry points still have the +0x4000 offset).

 Trampoline code runs un-relocated so subtract kbase. */

	/*

	 * System reset can interrupt code where HSRRs are live and MSR[RI]=1.

	 * The system reset interrupt itself may clobber HSRRs (e.g., to call

	 * OPAL), so save them here and restore them before returning.

	 *

	 * Machine checks don't need to save HSRRs, as the real mode handler

	 * is careful to avoid them, and the regular handler is not delivered

	 * as an NMI.

 See if any machine dependent calls */

	/*

	 * A system reset is a request to dump, so we always send

	 * it through the crashdump code (if fadump or kdump are

	 * registered).

	/*

	 * We aren't the primary crash CPU. We need to send it

	 * to a holding pattern to avoid it ending up in the panic

	 * code.

	/*

	 * No debugger or crash dump registered, print logs then

	 * panic.

 Wait a little while for others to print */

 Must die if the interrupt is not recoverable */

 For the reason explained in die_mce, nmi_exit before die */

 What should we do here? We could issue a shutdown or hard reset. */

/*

 * I/O accesses can cause machine checks on powermacs.

 * Check if the NIP corresponds to the address of a sync

 * instruction for which there is an entry in the exception

 * table.

 *  -- paulus.

		/*

		 * Check that it's a sync instruction, or somewhere

		 * in the twi; isync; nop sequence that inb/inw/inl uses.

		 * As the address is in the exception table

		 * we should be able to read the instr there.

		 * For the debug message, we look at the preceding

		 * load or store.

 CONFIG_PPC32 */

/* On 4xx, the reason for the machine check or program exception

 single-step stuff */

/* On non-4xx, the reason for the machine check or program

		/*

		 * This is recoverable by invalidating the i-cache.

		/*

		 * This will generally be accompanied by an instruction

		 * fetch error report -- only treat MCSR_IF as fatal

		 * if it wasn't due to an L1 parity error.

		/*

		 * In write shadow mode we auto-recover from the error, but it

		 * may still get logged and cause a machine check.  We should

		 * only treat the non-write shadow case as non-recoverable.

		/* On e6500 core, L1 DCWS (Data cache write shadow mode) bit

		 * is not implemented but L1 data cache always runs in write

		 * shadow mode. Hence on data cache parity errors HW will

		 * automatically invalidate the L1 Data Cache.

 7450 MSS error and TEA */

 everything else */

	/*

	 * The machine check wants to kill the interrupted context, but

	 * do_exit() checks for in_interrupt() and panics in that case, so

	 * exit the irq/nmi before calling die.

/*

 * BOOK3S_64 does not usually call this handler as a non-maskable interrupt

 * (it uses its own early real-mode handler to handle the MCE proper

 * and then raises irq_work to call this handler when interrupts are

 * enabled). The only time when this is not true is if the early handler

 * is unrecoverable, then it does call this directly to try to get a

 * message out.

	/* See if any machine dependent calls. In theory, we would want

	 * to call the CPU first, and call the ppc_md. one if the CPU

	 * one returns a positive number. However there is existing code

	 * that assumes the board gets a first chance, so let's keep it

	 * that way for now and fix things later. --BenH.

 Must die if the interrupt is not recoverable */

 async? */

	/*

	 * lxvb16x	opcode: 0x7c0006d8

	 * lxvd2x	opcode: 0x7c000698

	 * lxvh8x	opcode: 0x7c000658

	 * lxvw4x	opcode: 0x7c000618

 Grab vector registers into the task struct */

 Grab msr before we flush the bits */

	/*

	 * Is userspace running with a different endian (this is rare but

	 * not impossible)

 Decode the instruction */

 Grab the vector address */

 Check it */

 Read the vector */

 unaligned case */

 Grab instruction "selector" */

	/*

	 * Check to make sure the facility is actually enabled. This

	 * could happen if we get a false positive hit.

	 *

	 * lxvd2x/lxvw4x always check MSR VSX sel = 0,2

	 * lxvh8x/lxvb16x check MSR VSX or VEC depending on VSR used sel = 1,3

 lxvh8x & lxvb16x + VSR >= 32 */

 Do logging here before we modify sel based on endian */

 lxvw4x */

 lxvh8x */

 lxvd2x */

 lxvb16x */

	/*

	 * An LE kernel stores the vector in the task struct as an LE

	 * byte array (effectively swapping both the components and

	 * the content of the components). Those instructions expect

	 * the components to remain in ascending address order, so we

	 * swap them back.

	 *

	 * If we are running a BE user space, the expectation is that

	 * of a simple memcpy, so forcing the emulation to look like

	 * a lxvb16x should do the trick.

 lxvw4x */

 lxvh8x */

 lxvd2x */

 lxvb16x */

 __LITTLE_ENDIAN__ */

 On a big endian kernel, a BE userspace only needs a memcpy */

 Otherwise, we need to swap the content of the components */

 lxvw4x */

 lxvh8x */

 lxvd2x */

 lxvb16x */

 !__LITTLE_ENDIAN__ */

 Go to next instruction */

 CONFIG_VSX */

 Real mode flagged P9 special emu is needed */

		/*

		 * We don't want to take page faults while doing the

		 * emulation, we just replay the instruction if necessary.

 CONFIG_VSX */

/*

 * After we have successfully emulated an instruction, we have to

 * check if the instruction was being single-stepped, and if so,

 * pretend we got a single-step exception.  This was pointed out

 * by Kumar Gala.  -- paulus

 Invalid operation */

 Overflow */

 Underflow */

 Divide by zero */

 Inexact result */

/*

 * Illegal instruction emulation support.  Originally written to

 * provide the PVR to user applications using the mfspr rd, PVR.

 * Return non-zero if we can't emulate, or -EFAULT if the associated

 * memory access caused an access fault.  Return zero on success.

 *

 * There are a couple of ways to do this, either "decode" the instruction

 * or directly match lots of bits.  In this case, matching lots of

 * bits is faster and easier.

 *

 Early out if we are an invalid form of lswx */

 if process is 32-bit, clear upper 32 bits of EA */

				/* first time updating this reg,

 move EA to next address */

 manage our position within the register */

        /* If we're emulating a load/store in an active transaction, we cannot

         * emulate it as the kernel operates in transaction suspended context.

         * We need to abort the transaction.  This creates a persistent TM

         * abort so tell the user what caused it with a new code.

 Emulate the mfspr rD, PVR. */

 Emulating the dcba insn is just a no-op.  */

 Emulate the mcrxr insn.  */

 Emulate load/store string insn. */

 Emulate the popcntb (Population Count Bytes) instruction. */

 Emulate isel (Integer Select) instruction */

 Emulate sync instruction variants */

 Emulate the mfspr rD, DSCR. */

 Emulate the mtspr DSCR, rD. */

	/* We can now get here via a FP Unavailable exception if the core

 IEEE FP exception */

		/* Debugger is first in line to stop recursive faults in

 trap exception */

		/*

		 * Fixup bugaddr for BUG_ON() in real mode

 not user-mode */

		/* This is a TM "Bad Thing Exception" program check.

		 * This occurs when:

		 * -  An rfid/hrfid/mtmsrd attempts to cause an illegal

		 *    transition in TM states.

		 * -  A trechkpt is attempted when transactional.

		 * -  A treclaim is attempted when non transactional.

		 * -  A tend is illegally attempted.

		 * -  writing a TM SPR when transactional.

		 *

		 * If usermode caused this, it's done something illegal and

		 * gets a SIGILL slap on the wrist.  We call it an illegal

		 * operand to distinguish from the instruction just being bad

		 * (e.g. executing a 'tend' on a CPU without TM!); it's an

		 * illegal /placement/ of a valid instruction.

	/*

	 * If we took the program check in the kernel skip down to sending a

	 * SIGILL. The subsequent cases all relate to emulating instructions

	 * which we should only do for userspace. We also do not want to enable

	 * interrupts for kernel faults because that might lead to further

	 * faults, and loose the context of the original exception.

	/* (reason & REASON_ILLEGAL) would be the obvious thing here,

	 * but there seems to be a hardware bug on the 405GP (RevD)

	 * that means ESR is sometimes set incorrectly - either to

	 * ESR_DST (!?) or 0.  In the process of chasing this with the

	 * hardware people - not sure if it can happen on any illegal

	 * instruction or only on FP instructions, whether there is a

	 * pattern to occurrences etc. -dgibson 31/Mar/2003

 Try to emulate it if we should. */

/*

 * This occurs when running in hypervisor mode on POWER6 or later

 * and an illegal instruction is encountered.

 we don't implement logging of alignment exceptions */

 skip over emulated instruction */

 Operand address was bad */

		/* A user program has executed an altivec instruction,

		/* A user program has executed an vsx instruction,

 We should not have taken this interrupt in kernel */

		/*

		 * User is accessing the DSCR register using the problem

		 * state only SPR number (0x03) either through a mfspr or

		 * a mtspr instruction. If it is a write attempt through

		 * a mtspr, then we set the inherit bit. This also allows

		 * the user to write or read the register directly in the

		 * future by setting via the FSCR DSCR bit. But in case it

		 * is a read DSCR attempt through a mfspr instruction, we

		 * just emulate the instruction instead. This code path will

		 * always emulate all the mfspr instructions till the user

		 * has attempted at least one mtspr instruction. This way it

		 * preserves the same behaviour when the user is accessing

		 * the DSCR through privilege level only SPR number (0x11)

		 * which is emulated through illegal instruction exception.

		 * We always leave HFSCR DSCR set.

 Write into DSCR (mtspr 0x03, RS) */

 Read from DSCR (mfspr RT, 0x03) */

		/*

		 * If we're here then the hardware is TM aware because it

		 * generated an exception with FSRM_TM set.

		 *

		 * If cpu_has_feature(CPU_FTR_TM) is false, then either firmware

		 * told us not to do TM, or the kernel is not built with TM

		 * support.

		 *

		 * If both of those things are true, then userspace can spam the

		 * console by triggering the printk() below just by continually

		 * doing tbegin (or any TM instruction). So in that case just

		 * send the process a SIGILL immediately.

 Note:  This does not handle any kind of FP laziness. */

        /* We can only have got here if the task started using FP after

         * beginning the transaction.  So, the transactional regs are just a

         * copy of the checkpointed ones.  But, we still need to recheckpoint

         * as we're enabling FP for the process; it will return, abort the

         * transaction, and probably retry but now with FP enabled.  So the

         * checkpointed FP registers need to be loaded.

	/*

	 * Reclaim initially saved out bogus (lazy) FPRs to ckfp_state, and

	 * then it was overwrite by the thr->fp_state by tm_reclaim_thread().

	 *

	 * At this point, ck{fp,vr}_state contains the exact values we want to

	 * recheckpoint.

 Enable FP for the task: */

	/*

	 * Recheckpoint all the checkpointed ckpt, ck{fp, vr}_state registers.

	/* See the comments in fp_unavailable_tm().  This function operates

	 * the same way.

	/* See the comments in fp_unavailable_tm().  This works similarly,

	 * though we're loading both FP and VEC registers in here.

	 *

	 * If FP isn't in use, load FP regs.  If VEC isn't in use, load VEC

	 * regs.  Either way, set MSR_VSX.

 This reclaims FP and/or VR regs if they're already enabled */

 CONFIG_PPC_TRANSACTIONAL_MEM */

	/*

	 * On 64-bit, if perf interrupts hit in a local_irq_disable

	 * (soft-masked) region, we consider them as NMIs. This is required to

	 * prevent hash faults on user addresses when reading callchains (and

	 * looks better from an irq tracing perspective).

	/*

	 * Determine the cause of the debug event, clear the

	 * event flags and send a trap to the handler. Torez

	/*

	 * At the point this routine was called, the MSR(DE) was turned off.

	 * Check all other debug flags and see if that bit needs to be turned

	 * back on or not.

 Make sure the IDM flag is off */

	/* Hack alert: On BookE, Branch Taken stops on the branch itself, while

	 * on server, it stops on the target of the branch. In order to simulate

	 * the server behaviour, we thus restart right away with a single step

	 * instead of stopping here when hitting a BT

 Disable BT */

 Clear the BT event */

 Do the single step trick only when coming from userspace */

 Instruction complete */

 Disable instruction completion */

 Clear the instruction completion event */

 Make sure the IDM bit is off */

 CONFIG_PPC_ADV_DEBUG_REGS */

 skip emulated instruction */

 got an error reading the instruction */

 didn't recognize the instruction */

 XXX quick hack for now: set the non-Java bit in the VSCR */

 CONFIG_ALTIVEC */

	/* We treat cache locking instructions from the user

	 * as priv ops, in the future we could try to do

	 * something smarter

 CONFIG_FSL_BOOKE */

 skip emulated instruction */

 got an error reading the instruction */

 didn't recognize the instruction */

 skip emulated instruction */

 got an error reading the instruction */

 didn't recognize the instruction */

/*

 * We enter here if we get an unrecoverable exception, that is, one

 * that happened at a point where the RI (recoverable interrupt) bit

 * in the MSR is 0.  This indicates that SRR0/1 are live, and that

 * we therefore lost state by taking this exception.

 die() should not return */

/*

 * Default handler for a Watchdog exception,

 * spins until a reboot occurs

 Generic WatchdogHandler, implement your own */

/*

 * We enter here if we discover during exception entry that we are

 * running in supervisor mode with a userspace value in the stack pointer.

 CONFIG_PPC_EMULATED_STATS */

 SPDX-License-Identifier: GPL-2.0+

/*

 * DAWR infrastructure

 *

 * Copyright 2019, Michael Neuling, IBM Corporation.

	/*

	 * DAWR length is stored in field MDR bits 48:53.  Matches range in

	 * doublewords (64 bits) baised by -1 eg. 0b000000=1DW and

	 * 0b111111=64DW.

	 * brk->hw_len is in bytes.

	 * This aligns up to double word size, shifts and does the bias.

 Send error to user if they hypervisor won't allow us to write DAWR */

 If we are clearing, make sure all CPUs have the DAWR cleared */

 Don't setup sysfs file for user control on P8 */

 Turn DAWR off by default, but allow admin to turn it on */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Procedures for creating, accessing and interpreting the device tree.

 *

 * Paul Mackerras	August 1996.

 * Copyright (C) 1996-2005 Paul Mackerras.

 * 

 *  Adapted for 64bit PowerPC by Dave Engebretsen and Peter Bergner.

 *    {engebret|bergner}@us.ibm.com 

/*

 * overlaps_initrd - check for overlap with page aligned extension of

 * initrd.

/**

 * move_device_tree - move tree to an unused area, if needed.

 *

 * The device tree may be allocated beyond our memory limit, or inside the

 * crash kernel region for kdump, or within the page aligned range of initrd.

 * If so, move it out of the way.

/*

 * ibm,pa-features is a per-cpu property that contains a string of

 * attribute descriptors, each of which has a 2 byte header plus up

 * to 254 bytes worth of processor attribute bits.  First header

 * byte specifies the number of bytes following the header.

 * Second header byte is an "attribute-specifier" type, of which

 * zero is the only currently-defined value.

 * Implementation:  Pass in the byte and bit offset for the feature

 * that we are interested in.  The function will return -1 if the

 * pa-features property is missing, or a 1/0 to indicate if the feature

 * is supported/not supported.  Note that the bit numbers are

 * big-endian to match the definition in PAPR.

 CPU_FTR_xxx bit */

 MMU_FTR_xxx bit */

 PPC_FEATURE_xxx bit */

 PPC_FEATURE2_xxx bit */

 byte number in ibm,pa-features */

 bit number (big-endian) */

 if 1, pa bit set => clear feature */

	/*

	 * If the kernel doesn't support TM (ie CONFIG_PPC_TRANSACTIONAL_MEM=n),

	 * we don't want to turn on TM here, so we use the *_COMP versions

	 * which are 0 if the kernel doesn't support TM.

 find descriptor with type == 0 */

 descriptor 0 not found */

 loop over bits we know about */

 CONFIG_ALTIVEC */

 Yes, this _really_ is ibm,vmx == 2 to enable VSX */

 CONFIG_VSX */

 CONFIG_PPC64 */

	/*

	 * Since 440GR(x)/440EP(x) processors have the same pvr,

	 * we check the node path and set bit 28 in the cur_cpu_spec

	 * pvr for EP(x) processor version. This bit is always 0 in

	 * the "real" pvr. Then we call identify_cpu again with

	 * the new logical pvr to enable FPU support.

 We are scanning "cpu" nodes only */

 Get physical cpuid */

	/*

	 * Now see if any of these threads match our boot cpu.

	 * NOTE: This must match the parsing done in smp_setup_cpu_maps.

 logical cpu id is always 0 on UP kernels */

 Not the boot CPU */

	/*

	 * PAPR defines "logical" PVR values for cpus that

	 * meet various levels of the architecture:

	 * 0x0f000001	Architecture version 2.04

	 * 0x0f000002	Architecture version 2.05

	 * If the cpu-version property in the cpu node contains

	 * such a value, we call identify_cpu again with the

	 * logical PVR value in order to use the cpu feature

	 * bits appropriate for the architecture level.

	 *

	 * A POWER6 partition in "POWER6 architected" mode

	 * uses the 0x0f000002 PVR value; in POWER5+ mode

	 * it uses 0x0f000001.

	 *

	 * If we're using device tree CPU feature discovery then we don't

	 * support the cpu-version property, and it's the responsibility of the

	 * firmware/hypervisor to provide the correct feature set for the

	 * architecture level via the ibm,powerpc-cpu-features binding.

 All these set by kernel, so no need to convert endian */

 Use common scan routine to determine if this is the chosen node */

 check if iommu is forced on or off */

 mem=x on the command line is the preferred mechanism */

 break now */

/*

 * Compare the range against max mem limit and update

 * size if it cross the limit.

/*

 * Interpret the ibm dynamic reconfiguration memory LMBs.

 * This contains a list of memory blocks along with NUMA affinity

 * information.

	/*

	 * Skip this block if the reserved bit is set in flags

	 * or if the block is not assigned to this partition.

		/*

		 * For each memblock in ibm,dynamic-memory, a

		 * corresponding entry in linux,drconf-usable-memory

		 * property contains a counter 'p' followed by 'p'

		 * (base, size) duple. Now read the counter from

		 * linux,drconf-usable-memory property

 there are no (base, size) duple */

 CONFIG_PPC_PSERIES */

/*

 * For a relocatable kernel, we need to get the memstart_addr first,

 * then use it to calculate the virtual kernel start address. This has

 * to happen at a very early stage (before machine_init). In this case,

 * we just want to get the memstart_address and would not like to mess the

 * memblock at this stage. So introduce a variable to skip the memblock_add()

 * for this reason.

	/* Keep track of the beginning of memory -and- the size of

	 * the very first block in the device-tree as it represents

	 * the RMA on ppc64 server

 Add the chunk to the MEMBLOCK list */

	/* Each reserved range is an (address,size) pair, 2 cells each,

 Look for the new "reserved-regions" property in the DT */

 Then reserve the initrd, if any */

 CONFIG_BLK_DEV_INITRD */

	/* 

	 * Handle the case where we might be booting from an old kexec

	 * image that setup the mem_rsvmap as pairs of 32-bit values

 CONFIG_PPC_TRANSACTIONAL_MEM */

	/*

	 * Ensure the init_task (pid 0, aka swapper) uses the value of FSCR we

	 * have configured via the device tree features or via __init_FSCR().

	 * That value will then be propagated to pid 1 (init) and all future

	 * processes.

 Too early to BUG_ON(), do it by hand */

 Some machines might need RTAS info for debugging, grab it now. */

 Some machines might need OPAL info for debugging, grab it now. */

 Scan tree for ultravisor feature */

 scan tree to see if dump is active during last boot */

	/* Retrieve various informations from the /chosen node of the

	 * device-tree, including the platform type, initrd location and

	 * size, TCE reserve, and more ...

 Scan memory nodes and rebuild MEMBLOCKs */

 make sure we've parsed cmdline for mem= before this */

 Reserve MEMBLOCK regions used by kernel, initrd, dt, etc... */

 If relocatable, reserve first 32k for interrupt vectors etc. */

	/*

	 * If we fail to reserve memory for firmware-assisted dump then

	 * fallback to kexec based kdump.

 Ensure that total memory size is page-aligned. */

	/* We may need to relocate the flat tree, do it now.

	/* Retrieve CPU related informations from the flat tree

	 * (altivec support, boot CPU ID, ...)

	/* We'll later wait for secondaries to check in; there are

	 * NCPUS-1 non-boot CPUs  :-)

 Scan and build the list of machine check recoverable ranges */

 Now try to figure out if we are running on LPAR and so on */

	/*

	 * Initialize pkey features and default AMR/IAMR values

 Identify PS3 firmware */

/*

 * This function run before early_init_devtree, so we have to init

 * initial_boot_params.

 Setup flat device-tree pointer */

	/*

	 * Scan the memory nodes and set add_mem_to_memblock to 0 to avoid

	 * mess the memblock.

/*******

 *

 * New implementation of the OF "find" APIs, return a refcounted

 * object, call of_node_put() when done.  The device tree and list

 * are protected by a rw_lock.

 *

 * Note that property management will need some locking as well,

 * this isn't dealt with yet.

 *

/**

 * of_get_ibm_chip_id - Returns the IBM "chip-id" of a device

 * @np: device node of the device

 *

 * This looks for a property "ibm,chip-id" in the node or any

 * of its parents and returns its content, or -1 if it cannot

 * be found.

		/*

		 * Skiboot may produce memory nodes that contain more than one

		 * cell in chip-id, we only read the first one here.

/**

 * cpu_to_chip_id - Return the cpus chip-id

 * @cpu: The logical cpu number.

 *

 * Return the value of the ibm,chip-id property corresponding to the given

 * logical cpu number. If the chip-id can not be found, returns -1.

	/*

	 * Early firmware scanning must use this rather than

	 * get_hard_smp_processor_id because we don't have pacas allocated

	 * until memory topology is discovered.

 SPDX-License-Identifier: GPL-2.0

 UART_IIR % 4 == 2 */

 get clock freq. if present */

 get default speed if present */

 get register shift if present */

 If we have a location index, then try to use it */

	/* if our index is still out of range, that mean that

	 * array is full, we could scan for a free slot but that

	 * make little sense to bother, just skip the port

 Check if there is a port who already claimed our slot */

 if we still have some room, move it, else override */

 Now fill the entry */

	/* We only support ports that have a clock frequency properly

	 * encoded in the device-tree.

 if reg-offset don't try to use it */

 if rtas uses this device, don't try to use it as well */

 Get the address */

	/* Add port, irq will be dealt with later. We passed a translated

	 * IO port value. It will be fixed up later along with the irq

 Get the ISA port number */

 Verify it's an IO port, we don't support anything else */

	/* Now look for an "ibm,aix-loc" property that gives us ordering

	 * if any...

 If we have a location index, then use it */

	/* Translate ISA address. If it fails, we still register the port

	 * with no translated address so that it can be picked up as an IO

	 * port later by the serial driver

	 *

	 * Note: Don't even try on P8 lpc, we know it's not directly mapped

 Add port, irq will be dealt with later */

	/* We only support ports that have a clock frequency properly

	 * encoded in the device-tree (that is have an fcode). Anything

	 * else can't be used that early and will be normally probed by

	 * the generic 8250_pci driver later on. The reason is that 8250

	 * compatible UARTs on PCI need all sort of quirks (port offsets

	 * etc...) that this code doesn't know about

 Get the PCI address. Assume BAR 0 */

 We only support BAR 0 for now */

	/* Set the IO base to the same as the translated address for MMIO,

	 * or to the domain local IO base for PIO (it will be fixed up later)

	/* Try to guess an index... If we have subdevices of the pci dev,

	 * we get to their "reg" property

	/* Local index means it's the Nth port in the PCI chip. Unfortunately

	 * the offset to add here is device specific. We know about those

	 * EXAR ports and we default to the most common case. If your UART

	 * doesn't work for these settings, you'll have to add your own special

	 * cases here

	/* Add port, irq will be dealt with later. We passed a translated

	 * IO port value. It will be fixed up later along with the irq

 Check if a translated MMIO address has been found */

 Check if it's PIO and we support untranslated PIO */

 Try to query the current speed */

 Set it up */

/*

 * This is called very early, as part of setup_system() or eventually

 * setup_arch(), basically before anything else in this file. This function

 * will try to build a list of all the available 8250-compatible serial ports

 * in the machine using the Open Firmware device-tree. It currently only deals

 * with ISA and PCI busses but could be extended. It allows a very early boot

 * console to be initialized, that list is also used later to provide 8250 with

 * the machine non-PCI ports and to properly pick the default console port

 Now find out if one of these is out firmware console */

 Iterate over all the 16550 ports, looking for known parents */

 Next, fill our array with ISA ports */

 Next, try to locate PCI ports */

		/* Check for known pciclass, and also check whether we have

		 * a device with child nodes for ports or not

/*

 * This is called as an arch initcall, hopefully before the PCI bus is

 * probed and/or the 8250 driver loaded since we need to register our

 * platform devices before 8250 PCI ones are detected as some of them

 * must properly "override" the platform ones.

 *

 * This function fixes up the interrupt value for platform ports as it

 * couldn't be done earlier before interrupt maps have been parsed. It

 * also "corrects" the IO address for PIO ports for the same reason,

 * since earlier, the PHBs virtual IO space wasn't assigned yet. It then

 * registers all those platform ports for use by the 8250 driver when it

 * finally loads.

	/*

	 * Before we register the platform serial devices, we need

	 * to fixup their interrupts and their IO ports.

/*

 * This is called very early, as part of console_init() (typically just after

 * time_init()). This function is respondible for trying to find a good

 * default console on serial ports. It tries to match the open firmware

 * default output with one of the available serial console drivers that have

 * been probed earlier by find_legacy_serial_ports()

 The user has requested a console so this is already set up. */

 We are getting a weird phandle from OF ... */

 ... So use the full path instead */

 Look for it in probed array */

 CONFIG_SERIAL_8250_CONSOLE */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Machine check exception handling.

 *

 * Copyright 2013 IBM Corporation

 * Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>

/*

 * Decode and save high level MCE information into per cpu buffer which

 * is an array of machine_check_event structure.

	/*

	 * Return if we don't have enough space to log mce event.

	 * mce_nest_count may go beyond MAX_MC_EVT but that's ok,

	 * the check below will stop buffer overrun.

 Populate generic machine check info */

 Mark it recovered if we have handled it and MSR(RI=1). */

	/*

	 * Populate the mce error_type and type-specific error_type.

/*

 * get_mce_event:

 *	mce	Pointer to machine_check_event structure to be filled.

 *	release Flag to indicate whether to free the event slot or not.

 *		0 <= do not release the mce event. Caller will invoke

 *		     release_mce_event() once event has been consumed.

 *		1 <= release the slot.

 *

 *	return	1 = success

 *		0 = failure

 *

 * get_mce_event() will be called by platform specific machine check

 * handle routine and in KVM.

 * When we call get_mce_event(), we are still in interrupt context and

 * preemption will not be scheduled until ret_from_expect() routine

 * is called.

 Sanity check */

 Check if we have MCE info to process. */

 Copy the event structure and release the original */

 Decrement the count to free the slot. */

/*

 * Queue up the MCE event which then can be handled later.

 If queue is full, just return for now. */

 Queue work to process this event later. */

/*

 * Queue up the MCE event which then can be handled later.

 If queue is full, just return for now. */

	/*

	 * Queue irq work to process this event later. Before

	 * queuing the work enable translation for non radix LPAR,

	 * as irq_work_queue may try to access memory outside RMO

	 * region.

/*

 * process pending MCE event from the mce event queue. This function will be

 * called during syscall exit.

		/*

		 * This should probably queued elsewhere, but

		 * oh! well

		 *

		 * Don't report this machine check because the caller has a

		 * asked us to ignore the event, it has a fixup handler which

		 * will do the appropriate error handling and reporting.

/*

 * process pending MCE event from the mce event queue. This function will be

 * called during syscall exit.

	/*

	 * For now just print it to console.

	 * TODO: log this error event to FSP or nvram.

 Print things out */

 Load/Store address */

 Display faulty slb contents for SLB errors. */

/*

 * This function is called in real mode. Strictly no printk's please.

 *

 * regs->nip and regs->msr contains srr0 and ssr1.

	/*

	 * See if platform is capable of handling machine check.

 Possible meanings for HMER_DEBUG_TRIG bit being set on POWER9 */

 need to emulate vector CI load instr */

 need to escape from TM suspend mode */

 First look in the device tree */

 If we found the property, don't look at PVR */

 Check for POWER9 Nimbus (scale-out) */

 DD2.2 and later */

 DD2.0 and DD2.1 - used for vector CI load emulation */

/*

 * Handle HMIs that occur as a result of a debug trigger.

 * Return values:

 * -1 means this is not a HMI cause that we know about

 *  0 means no further handling is required

 *  1 means further handling is required

 HMER_DEBUG_TRIG bit is used for various workarounds on P9 */

 HMER is a write-AND register */

		/*

		 * Now to avoid problems with soft-disable we

		 * only do the emulation if we are coming from

		 * host user space

	/*

	 * See if any other HMI causes remain to be handled

/*

 * Return values:

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * I/O string operations

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *    Copyright (C) 2006 IBM Corporation

 *

 * Largely rewritten by Cort Dougan (cort@cs.nmt.edu)

 * and Paul Mackerras.

 *

 * Adapted for iSeries by Mike Corrigan (mikejc@us.ibm.com)

 * PPC64 updates by Dave Engebretsen (engebret@us.ibm.com)

 *

 * Rewritten in C by Stephen Rothwell.

 See definition in io.h */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Signal handling for 32bit PPC and 32bit tasks on 64bit PPC

 *

 *  PowerPC version

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 * Copyright (C) 2001 IBM

 * Copyright (C) 1997,1998 Jakub Jelinek (jj@sunsite.mff.cuni.cz)

 * Copyright (C) 1997 David S. Miller (davem@caip.rutgers.edu)

 *

 *  Derived from "arch/i386/kernel/signal.c"

 *    Copyright (C) 1991, 1992 Linus Torvalds

 *    1997-11-28  Modified for POSIX.1b signals by Richard Henderson

/*

 * Userspace code may pass a ucontext which doesn't include VSX added

 * at the end.  We need to check for this case.

/*

 * Returning 0 means we return to userspace via

 * ret_from_except and thus restore all user

 * registers from *regs.  This is what we need

 * to do when a signal has been delivered.

/*

 * Functions for flipping sigsets (thanks to brain dead generic

 * implementation that makes things simple for little endian only)

 Force usr to alway see softe as 1 (interrupts enabled) */

 CONFIG_PPC64 */

 copy up to but not including MSR */

 copy from orig_r3 (the word after the MSR) up to the end */

/*

 * When we have signals to deliver, we set up on the

 * user stack, going down from the original stack pointer:

 *	an ABI gap of 56 words

 *	an mcontext struct

 *	a sigcontext struct

 *	a gap of __SIGNAL_FRAMESIZE bytes

 *

 * Each of these things must be a multiple of 16 bytes in size. The following

 * structure represent all of this except the __SIGNAL_FRAMESIZE gap

 *

 the sigcontext */

 all the register values */

	/*

	 * Programs using the rs6000/xcoff abi can save up to 19 gp

	 * regs and 18 fp regs below sp before decrementing it.

/*

 *  When we have rt signals to deliver, we set up on the

 *  user stack, going down from the original stack pointer:

 *	one rt_sigframe struct (siginfo + ucontext + ABI gap)

 *	a gap of __SIGNAL_FRAMESIZE+16 bytes

 *  (the +16 is to get the siginfo and ucontext in the same

 *  positions as in older kernels).

 *

 *  Each of these things must be a multiple of 16 bytes in size.

 *

	/*

	 * Programs using the rs6000/xcoff abi can save up to 19 gp

	 * regs and 18 fp regs below sp before decrementing it.

/*

 * Save the current user registers on the user stack.

 * We only save the altivec/spe registers if the process has used

 * altivec/spe instructions at some point.

 Make sure floating point registers are stored in regs */

 save general registers */

 save altivec registers */

		/* set MSR_VEC in the saved MSR value to indicate that

 else assert((regs->msr & MSR_VEC) == 0) */

	/* We always copy to/from vrsave, it's 0 if we don't have or don't

	 * use altivec. Since VSCR only contains 32 bits saved in the least

	 * significant bits of a vector, we "cheat" and stuff VRSAVE in the

	 * most significant bits of that same vector. --BenH

	 * Note that the current VRSAVE value is in the SPR at this point.

 CONFIG_ALTIVEC */

	/*

	 * Clear the MSR VSX bit to indicate there is no valid state attached

	 * to this context, except in the specific case below where we set it.

	/*

	 * Copy VSR 0-31 upper half from thread_struct to local

	 * buffer, then write that to userspace.  Also set MSR_VSX in

	 * the saved MSR value to indicate that frame->mc_vregs

	 * contains valid data

 CONFIG_VSX */

 save spe registers */

		/* set MSR_SPE in the saved MSR value to indicate that

 else assert((regs->msr & MSR_SPE) == 0) */

 We always copy to/from spefscr */

 CONFIG_SPE */

	/* We need to write 0 the MSR top 32 bits in the tm frame so that we

	 * can check it on the restore to see if TM is active

/*

 * Save the current user registers on the user stack.

 * We only save the altivec/spe registers if the process has used

 * altivec/spe instructions at some point.

 * We also save the transactional registers to a second ucontext in the

 * frame.

 *

 * See __unsafe_save_user_regs() and signal_64.c:setup_tm_sigcontexts().

 Save both sets of general registers */

	/* Stash the top half of the 64bit MSR into the 32bit MSR word

	 * of the transactional mcontext.  This way we have a backward-compatible

	 * MSR in the 'normal' (checkpointed) mcontext and additionally one can

	 * also look at what type of transaction (T or S) was active at the

	 * time of the signal.

 save altivec registers */

		/* set MSR_VEC in the saved MSR value to indicate that

		 * frame->mc_vregs contains valid data

	/* We always copy to/from vrsave, it's 0 if we don't have or don't

	 * use altivec. Since VSCR only contains 32 bits saved in the least

	 * significant bits of a vector, we "cheat" and stuff VRSAVE in the

	 * most significant bits of that same vector. --BenH

	/*

	 * Copy VSR 0-31 upper half from thread_struct to local

	 * buffer, then write that to userspace.  Also set MSR_VSX in

	 * the saved MSR value to indicate that frame->mc_vregs

	 * contains valid data

/*

 * Restore the current user register values from the user stack,

 * (except for MSR).

	/*

	 * restore general registers but not including MSR or SOFTE. Also

	 * take care of keeping r2 (TLS) intact if not a signal

 if doing signal return, restore the previous little-endian mode */

	/*

	 * Force the process to reload the altivec registers from

	 * current->thread when it next does altivec instructions

 restore altivec registers from the stack */

 Always get VRSAVE back */

 CONFIG_ALTIVEC */

	/*

	 * Force the process to reload the VSX registers from

	 * current->thread when it next does VSX instruction.

		/*

		 * Restore altivec registers from the stack to a local

		 * buffer, then write this out to the thread_struct

 CONFIG_VSX */

	/*

	 * force the process to reload the FP registers from

	 * current->thread when it next does FP instructions

	/* force the process to reload the spe registers from

 restore spe registers from the stack */

 Always get SPEFSCR back */

 CONFIG_SPE */

/*

 * Restore the current user register values from the user stack, except for

 * MSR, and recheckpoint the original checkpointed register state for processes

 * in transactions.

	/*

	 * restore general registers but not including MSR or SOFTE. Also

	 * take care of keeping r2 (TLS) intact if not a signal.

	 * See comment in signal_64.c:restore_tm_sigcontexts();

	 * TFHAR is restored from the checkpointed NIP; TEXASR and TFIAR

	 * were set by the signal delivery.

 Restore the previous little-endian mode */

 restore altivec registers from the stack */

 Always get VRSAVE back */

		/*

		 * Restore altivec registers from the stack to a local

		 * buffer, then write this out to the thread_struct

 restore altivec registers from the stack */

 Always get VRSAVE back */

		/*

		 * Restore altivec registers from the stack to a local

		 * buffer, then write this out to the thread_struct

 Get the top half of the MSR from the user context */

 If TM bits are set to the reserved value, it's an invalid context */

	/*

	 * Disabling preemption, since it is unsafe to be preempted

	 * with MSR[TS] set without recheckpointing.

	/*

	 * CAUTION:

	 * After regs->MSR[TS] being updated, make sure that get_user(),

	 * put_user() or similar functions are *not* called. These

	 * functions can generate page faults which will cause the process

	 * to be de-scheduled with MSR[TS] set but without calling

	 * tm_recheckpoint(). This can cause a bug.

	 *

	 * Pull in the MSR TM bits from the user context

	/* Now, recheckpoint.  This loads up all of the checkpointed (older)

	 * registers, including FP and V[S]Rs.  After recheckpointing, the

	 * transactional versions should be loaded.

 Make sure the transaction is marked as failed */

 This loads the checkpointed FP/VEC state, if used */

 This loads the speculative FP/VEC state, if used */

 CONFIG_PPC64 */

/*

 * Set up a signal frame for a "real-time" signal handler

 * (one which gets siginfo).

 Save the thread's msr before get_tm_stackpointer() changes it */

 Set up Signal Frame */

 Put the siginfo & fill in most of the ucontext */

 Save user registers on the stack */

 turn off all fp exceptions */

 create a stack frame for the caller of the handler */

 Fill registers for signal handler */

 enter the signal handler in native-endian mode */

/*

 * OK, we're invoking a handler

 Save the thread's msr before get_tm_stackpointer() changes it */

 Set up Signal Frame */

 turn off all fp exceptions */

 create a stack frame for the caller of the handler */

 enter the signal handler in native-endian mode */

 no need to check access_ok(mcp), since mcp < 4GB */

		/*

		 * Get pointer to the real mcontext.  No need for

		 * access_ok since we are dealing with compat

		 * pointers.

	/*

	 * Check that the context is not smaller than the original

	 * size (with VMX but without VSX)

	/*

	 * If the new context state sets the MSR VSX bits but

	 * it doesn't provide VSX state.

 Does the context have enough room to store VSX data? */

	/* Context size is for future use. Right now, we only make sure

	 * we are passed something we understand

		/*

		 * old_ctx might not be 16-byte aligned, in which

		 * case old_ctx->uc_mcontext won't be either.

		 * Because we have the old_ctx->uc_pad2 field

		 * before old_ctx->uc_mcontext, we need to round down

		 * from &old_ctx->uc_mcontext to a 16-byte boundary.

	/*

	 * If we get a fault copying the context into the kernel's

	 * image of the user's registers, we can't just return -EFAULT

	 * because the user's registers will be corrupted.  For instance

	 * the NIP value may have been updated but not some of the

	 * other registers.  Given that we have done the access_ok

	 * and successfully read the first and last bytes of the region

	 * above, this should only happen in an out-of-memory situation

	 * or if another thread unmaps the region containing the context.

	 * We kill the task with a SIGSEGV in this situation.

 Always make any pending restarted system calls return -EINTR */

	/*

	 * If there is a transactional state then throw it away.

	 * The purpose of a sigreturn is to destroy all traces of the

	 * signal frame, this includes any transactional state created

	 * within in. We only check for suspended as we can never be

	 * active in the kernel, we are active, there is nothing better to

	 * do than go ahead and Bad Thing later.

	 * The cause is not important as there will never be a

	 * recheckpoint so it's not user visible.

		/* The top 32 bits of the MSR are stashed in the transactional

 Trying to start TM on non TM system */

			/* We only recheckpoint on return if we're

			 * transaction.

		/*

		 * Unset regs->msr because ucontext MSR TS is not

		 * set, and recheckpoint was not called. This avoid

		 * hitting a TM Bad thing at RFID

 Fall through, for non-TM restore */

	/*

	 * It's not clear whether or why it is desirable to save the

	 * sigaltstack setting on signal delivery and restore it on

	 * signal return.  But other architectures do this and we have

	 * always done it up until now so it is probably better not to

	 * change it.  -- paulus

	/* We wait until here to actually install the values in the

	   registers so if we fail in the above loop, it will not

	   affect the contents of these registers.  After this point,

	   failure is a problem, anyway, and it's very unlikely unless

	/*

	 * If we get a fault copying the context into the kernel's

	 * image of the user's registers, we can't just return -EFAULT

	 * because the user's registers will be corrupted.  For instance

	 * the NIP value may have been updated but not some of the

	 * other registers.  Given that we have done the access_ok

	 * and successfully read the first and last bytes of the region

	 * above, this should only happen in an out-of-memory situation

	 * or if another thread unmaps the region containing the context.

	 * We kill the task with a SIGSEGV in this situation.

	/*

	 * It's not clear whether or why it is desirable to save the

	 * sigaltstack setting on signal delivery and restore it on

	 * signal return.  But other architectures do this and we have

	 * always done it up until now so it is probably better not to

	 * change it.  -- paulus

/*

 * Do a signal return; undo the signal stack.

 Always make any pending restarted system calls return -EINTR */

	/*

	 * Note that PPC32 puts the upper 32 bits of the sigmask in the

	 * unused part of the signal stackframe

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Code for Kernel probes Jump optimization.

 *

 * Copyright 2017, Anju T, IBM Corp.

/*

 * Check if we can optimize this probe. Returns NIP post-emulation if this can

 * be optimized and 0 otherwise.

	/*

	 * kprobe placed for kretprobe during boot time

	 * has a 'nop' instruction, which can be emulated.

	 * So further checks can be skipped.

	/*

	 * We only support optimizing kernel addresses, but not

	 * module addresses.

	 *

	 * FIXME: Optimize kprobes placed in module addresses.

	/*

	 * Kprobe placed in conditional branch instructions are

	 * not optimized, as we can't predict the nip prior with

	 * dummy pt_regs and can not ensure that the return branch

	 * from detour buffer falls in the range of address (i.e 32MB).

	 * A branch back from trampoline is set up in the detour buffer

	 * to the nip returned by the analyse_instr() here.

	 *

	 * Ensure that the instruction is not a conditional branch,

	 * and that can be emulated.

 This is possible if op is under delayed unoptimizing */

/*

 * Generate instructions to load provided immediate 64-bit value

 * to register 'reg' and patch these instructions at 'addr'.

 Allocate instruction slot for detour buffer */

	/*

	 * OPTPROBE uses 'b' instruction to branch to optinsn.insn.

	 *

	 * The target address has to be relatively nearby, to permit use

	 * of branch instruction in powerpc, because the address is specified

	 * in an immediate field in the instruction opcode itself, ie 24 bits

	 * in the opcode specify the address. Therefore the address should

	 * be within 32MB on either side of the current instruction.

 Check if the return address is also within 32MB range */

 Setup template */

 We can optimize this via patch_instruction_window later */

	/*

	 * Fixup the template with instructions to:

	 * 1. load the address of the actual probepoint

	/*

	 * 2. branch to optimized_callback() and emulate_step()

	/*

	 * 3. load instruction to be emulated into relevant register, and

	/*

	 * 4. branch back from trampoline

/*

 * On powerpc, Optprobes always replaces one instruction (4 bytes

 * aligned and 4 bytes long). It is impossible to encounter another

 * kprobe in this address range. So always return 0.

		/*

		 * Backup instructions which will be replaced

		 * by jump address

 SPDX-License-Identifier: GPL-2.0

/*

 * Procedures for drawing on the screen early on in the boot process.

 *

 * Benjamin Herrenschmidt <benh@kernel.crashing.org>

/* Calc BAT values for mapping the display and store them

 * in disp_BAT.  Those values are then used from head.S to map

 * the display during identify_machine() and MMU_Init()

 *

 * The display is mapped to virtual address 0xD0000000, rather

 * than 1:1, because some some CHRP machines put the frame buffer

 * in the region starting at 0xC0000000 (PAGE_OFFSET).

 * This mapping is temporary and will disappear as soon as the

 * setup done by MMU_Init() is applied.

 *

 * For now, we align the BAT and then map 8Mb on 601 and 16Mb

 * on other PPCs. This may cause trouble if the framebuffer

 * is really badly aligned, but I didn't encounter this case

 * yet.

/* This function can be used to enable the early boot text when doing

 * OF booting or within bootx init. It must be followed by a btext_unmap()

 * call before the logical address becomes unusable

/* Here's a small text engine to use during early boot

 * or for debugging purposes

 *

 * todo:

 *

 *  - build some kind of vgacon with it to enable early printk

 *  - move to a separate file

 *  - add a few video driver hooks to keep in sync with display

 *    changes.

 By default, we are no longer mapped */

	/* FIXME: Add support for PCI reg properties. Right now, only

	 * reliable on macs

 Calc the base address of a given point (x,y) */

 Adjust the display to a new resolution */

 check it's the same frame buffer (within 256MB) */

 ndef NO_SCROLL */

	/* wrap around from bottom to top of screen so we don't

	/* If btext is enabled, we might have a BAT setup for early display,

	 * thus we do enable some very basic udbg output

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Processor cache information made available to userspace via sysfs;

 * intended to be compatible with x86 intel_cacheinfo implementation.

 *

 * Copyright 2008 IBM Corporation

 * Author: Nathan Lynch

/* per-cpu object for tracking:

 * - a "cache" kobject for the top-level directory

 * - a list of "index" objects representing the cpu's local cache hierarchy

	struct kobject *kobj; /* bare (not embedded) kobject for cache

 list of index objects */

/* "index" object: each cpu's cache directory has an index

 * subdirectory corresponding to a cache object associated with the

 * cpu.  This object's lifetime is managed via the embedded kobject.

 next index in parent directory */

/* Template for determining which OF properties to query for a given

	/* Allow for both [di]-cache-line-size and

	 * [di]-cache-block-size properties.  According to the PowerPC

	 * Processor binding, -line-size should be provided if it

	 * differs from the cache block size (that which is operated

	 * on by cache instructions), so we look for -line-size first.

 These are used to index the cache_type_info array. */

 cache-size, cache-block-size, etc. */

 d-cache-size, d-cache-block-size, etc */

		/* Embedded systems that use cache-size, cache-block-size,

		/* PowerPC Processor binding says the [di]-cache-*

		 * must be equal on unified caches, so just use

/* Cache object: each instance of this corresponds to a distinct cache

 * in the system.  There are separate objects for Harvard caches: one

 * each for instruction and data, and each refers to the same OF node.

 * The refcount of the OF node is elevated for the lifetime of the

 * cache object.  A cache object is released when its shared_cpu_map

 * is cleared (see cache_cpu_clear).

 *

 * A cache object is on two lists: an unsorted global list

 * (cache_list) of cache objects; and a singly-linked list

 * representing the local cache hierarchy, which is ordered by level

 * (e.g. L1d -> L1i -> L2 -> L3).

 OF node for this cache, may be cpu */

 online CPUs using this cache */

 split cache disambiguation */

 level not explicit in device tree */

 id of the group of threads that share this cache */

 global list of cache objects */

 next cache of >= level */

/* traversal/modification of this list occurs only at cpu hotplug time;

 * access is serialized by cpu hotplug locking

 not cache_line_size() because that's a macro in include/linux/cache.h */

	/* If the cache is fully associative, there is no need to

	 * check the other properties.

 helper for dealing with split caches */

 return the first cache on a local list matching node and thread-group id */

/*

 * Unified caches can have two different sets of tags.  Most embedded

 * use cache-size, etc. for the unified cache size, but open firmware systems

 * use d-cache-size, etc.   Check on initialization for which type we have, and

 * return the appropriate structure type.  Assume it's embedded if it isn't

 * open firmware.  If it's yet a 3rd type, then there will be missing entries

 * in /sys/devices/system/cpu/cpu0/cache/index2/, and this code will need

 * to be extended further.

 already linked */

	/*

	 * The cache->next_local list sorts by level ascending:

	 * L1d -> L1i -> L2 -> L3 ...

/*

 * If sub-groups of threads in a core containing @cpu_id share the

 * L@level-cache (information obtained via "ibm,thread-groups"

 * device-tree property), then we identify the group by the first

 * thread-sibling in the group. We define this to be the group-id.

 *

 * In the absence of any thread-group information for L@level-cache,

 * this function returns -1.

/* Attributes which should always be created -- the kobject/sysfs core

 * does this automatically via kobj_type->default_attrs.  This is the

 * minimum data required to uniquely identify a cache.

/* Attributes which should be created if the cache device node has the

 * right properties -- see cacheinfo_create_index_opt_attrs

	/* We don't want to create an attribute that can't provide a

	 * meaningful value.  Check the return value of each optional

	 * attribute's ->show method before registering the

	 * attribute.

 functions needed to remove cache entry for cpu offline or suspend/resume */

 Remove cache dir from sysfs */

		/* Release the cache object if all the cpus using it

	/* Prevent userspace from seeing inconsistent state - remove

 careful, sysfs population may have failed */

	/* clear the CPU's bit in its cache chain, possibly freeing

 (CONFIG_PPC_PSERIES && CONFIG_SUSPEND) || CONFIG_HOTPLUG_CPU */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Common time routines among all ppc machines.

 *

 * Written by Cort Dougan (cort@cs.nmt.edu) to merge

 * Paul Mackerras' version and mine for PReP and Pmac.

 * MPC8xx/MBX changes by Dan Malek (dmalek@jlc.net).

 * Converted for 64-bit by Mike Corrigan (mikejc@us.ibm.com)

 *

 * First round of bugfixes by Gabriel Paubert (paubert@iram.es)

 * to make clock more stable (2.4.0-test5). The only thing

 * that this code assumes is that the timebases have been synchronized

 * by firmware on SMP and are never stopped (never do sleep

 * on SMP then, nap and doze are OK).

 * 

 * Speeded up do_gettimeofday by getting rid of references to

 * xtime (which required locks for consistency). (mikejc@us.ibm.com)

 *

 * TODO (not necessarily in this file):

 * - improve precision and reproducibility of timebase frequency

 * measurement at boot time.

 * - for astronomical applications: add a new function to get

 * non ambiguous timestamps even around leap seconds. This needs

 * a new timestamp format and a good name.

 *

 * 1997-09-10  Updated NTP code according to technical memorandum Jan '96

 *             "A Kernel Model for Precision Timekeeping" by Dave Mills

 powerpc clocksource/clockevent code */

 compute ((xsec << 12) * max) >> 32 */

 sane default */

 for cputime_t conversions */

/*

 * Factor for converting from cputime_t (timebase ticks) to

 * microseconds. This is stored as 0.64 fixed-point binary fraction.

/*

 * Read the SPURR on systems that have it, otherwise the PURR,

 * or if that doesn't exist return the timebase value passed in.

/*

 * Scan the dispatch trace log and count up the stolen time.

 * Should be called with interrupts disabled.

 buffer has overflowed */

/*

 * Accumulate stolen time by scanning the dispatch trace log.

 * Called on entry from user mode.

 CONFIG_PPC_SPLPAR */

 CONFIG_PPC_SPLPAR */

/*

 * Account time for a transition between system, hard irq

 * or soft irq state.

	/*

	 * Because we don't read the SPURR on every kernel entry/exit,

	 * deltascaled includes both user and system SPURR ticks.

	 * Apportion these ticks to system SPURR ticks and user

	 * SPURR ticks in the same ratio as the system time (delta)

	 * and user time (udelta) values obtained from the timebase

	 * over the same interval.  The system ticks get accounted here;

	 * the user ticks get saved up in paca->user_time_scaled to be

	 * used by account_process_tick.

/*

 * Account the whole cputime accumulated in the paca

 * Must be called with interrupts disabled.

 * Assumes that vtime_account_kernel/idle() has been called

 * recently (i.e. since the last entry from usermode) so that

 * get_paca()->user_time_scaled is up to date.

 ! CONFIG_VIRT_CPU_ACCOUNTING_NATIVE */

		/*

		 * TB is in error state and isn't ticking anymore.

		 * HMI handler was unable to recover from TB error.

		 * Return immediately, so that kernel won't get stuck here.

/*

 * 64-bit uses a byte in the PACA, 32-bit uses a per-cpu variable...

 32-bit */

 32 vs 64 bit */

	/*

	 * 64-bit code that uses irq soft-mask can just cause an immediate

	 * interrupt here that gets soft masked, if this is called under

	 * local_irq_disable(). It might be possible to prevent that happening

	 * by noticing interrupts are disabled and setting decrementer pending

	 * to be replayed when irqs are enabled. The problem there is that

	 * tracing can call irq_work_raise, including in code that does low

	 * level manipulations of irq soft-mask state (e.g., trace_hardirqs_on)

	 * which could get tangled up if we're messing with the same state

	 * here.

 CONFIG_IRQ_WORK */

 CONFIG_IRQ_WORK */

/*

 * timer_interrupt - gets called when the decrementer overflows,

 * with interrupts disabled.

	/*

	 * Some implementations of hotplug will get timer interrupts while

	 * offline, just ignore these.

	/* Ensure a positive value is written to the decrementer, or else

	 * some CPUs will continue to take decrementer exceptions. When the

	 * PPC_WATCHDOG (decrementer based) is configured, keep this at most

	 * 31 bits, which is about 4 seconds on most systems, which gives

	 * the watchdog a chance of catching timer interrupt hard lockups.

	/* Conditionally hard-enable interrupts now that the DEC has been

	 * bumped to its maximum value

 We may have raced with new irq work */

 Overrides the weak version in kernel/power/main.c */

	/* Disable the decrementer, so that it doesn't interfere

	 * with suspending.

 Overrides the weak version in kernel/power/main.c */

/*

 * Scheduler clock - returns current time in nanosec units.

 *

 * Note: mulhdu(a, b) (multiply high double unsigned) returns

 * the high 64 bits of a * b, i.e. (a * b) >> 64, where a and b

 * are 64-bit unsigned numbers.

/*

 * Running clock - attempts to give a view of time passing for a virtualised

 * kernels.

 * Uses the VTB register if available otherwise a next best guess.

	/*

	 * Don't read the VTB as a host since KVM does not switch in host

	 * timebase into the VTB when it takes a guest off the CPU, reading the

	 * VTB would result in reading 'last switched out' guest VTB.

	 *

	 * Host kernels are often compiled with CONFIG_PPC_PSERIES checked, it

	 * would be unsafe to rely only on the #ifdef above.

	/*

	 * This is a next best approximation without a VTB.

	 * On a host which is running bare metal there should never be any stolen

	 * time and on a host which doesn't do any virtualisation TB *should* equal

	 * VTB so it makes no difference anyway.

 The cpu node should have timebase and clock frequency properties */

 Clear any pending timer interrupts */

	/*

	 * The watchdog may have already been enabled by u-boot. So leave

	 * TRC[WP] (Watchdog Period) alone.

 Clear all bits except for TCR[WP] */

 Enable decrementer */

 hardcoded default */

 hardcoded default */

 XXX this is a litle fragile but will work okay in the short term */

 get_boot_time() isn't guaranteed to be safe to call late */

 Sanitize it in case real time clock is set below EPOCH */

 clocksource code */

 We may have raced with new irq work */

 Set values for KVM, see kvm_emulate_dec() */

	/*

	 * If we're running as the hypervisor we need to enable the LD manually

	 * otherwise firmware should have done it for us.

 Prior to ISAv3 the decrementer is always 32 bit */

 calculate the signed maximum given this many bits */

 Enable and test the large decrementer for this cpu */

	/* Start the decrementer on CPUs that have manual control

	 * such as BookE

	/* FIME: Should make unrelatred change to move snapshot_timebase

 This function is only called on the boot processor */

 Normal PowerPC with timebase register */

	/*

	 * Compute scale factor for sched_clock.

	 * The calibrate_decr() function has set tb_ticks_per_sec,

	 * which is the timebase frequency.

	 * We compute 1e9 * 2^64 / tb_ticks_per_sec and interpret

	 * the 128-bit result as a 64.64 fixed-point number.

	 * We then shift that number right until it is less than 1.0,

	 * giving us the scale factor and shift count to use in

	 * sched_clock().

 Save the current timebase to pretty up CONFIG_PRINTK_TIME */

 If platform provided a timezone (pmac), we correct the time */

 initialise and enable the large decrementer (if we have one) */

	/* Start the decrementer on CPUs that have manual control

	 * such as BookE

 Register the clocksource */

/*

 * Divide a 128-bit dividend by a 32-bit divisor, leaving a 128 bit

 * result.

 We don't need to calibrate delay, we use the CPU timebase for that */

	/* Some generic code (such as spinlock debug) use loops_per_jiffy

	 * as the number of __delay(1) in a jiffy, so make it so

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Idle daemon for PowerPC.  Idle daemon will handle any action

 * that needs to be taken when the system becomes idle.

 *

 * Originally written by Cort Dougan (cort@cs.nmt.edu).

 * Subsequent 32-bit hacking by Tom Rini, Armin Kuster,

 * Paul Mackerras and others.

 *

 * iSeries supported added by Mike Corrigan <mikejc@us.ibm.com>

 *

 * Additional shared processor, SMT, and firmware support

 *    Copyright (c) 2003 Dave Engebretsen <engebret@us.ibm.com>

 *

 * 32-bit and 64-bit versions merged by Paul Mackerras <paulus@samba.org>

		/*

		 * Some power_save functions return with

		 * interrupts enabled, some don't.

		/*

		 * Go into low thread priority and possibly

		 * low power mode.

	/*

	 * power4_idle_nap returns with interrupts enabled (soft and hard).

	 * to our caller with interrupts enabled (soft and hard). Our caller

	 * can cope with either interrupts disabled or enabled upon return.

/*

 * Register the sysctl to set/clear powersave_nap.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Implementation of various system calls for Linux/PowerPC

 *

 *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)

 *

 * Derived from "arch/i386/kernel/sys_i386.c"

 * Adapted from the i386 version by Gary Thomas

 * Modified by Cort Dougan (cort@cs.nmt.edu)

 * and Paul Mackerras (paulus@cs.anu.edu.au).

 *

 * This file contains various random system calls that

 * have a non-standard calling sequence on the Linux/PPC

 * platform.

/*

 * Due to some executables calling the wrong select we sometimes

 * get wrong args.  This determines how the args are being passed

 * (a single ptr to them all args passed) then calls

 * sys_select() with the appropriate args. -- Cort

	/*

	 * Set TIF_RESTOREALL so that r3 isn't clobbered on return to

	 * userspace. That also has the effect of restoring the non-volatile

	 * GPRs, so we saved them on the way in here.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2010 SUSE Linux Products GmbH. All rights reserved.

 * Copyright 2010-2011 Freescale Semiconductor, Inc.

 *

 * Authors:

 *     Alexander Graf <agraf@suse.de>

	/* On relocatable kernels interrupts handlers and our code

 Find out where we are and put everything there */

 Make sure we only write valid b instructions */

 Modify the chunk to fit the invocation */

 Patch the invocation */

 Find out where we are and put everything there */

 Make sure we only write valid b instructions */

 Modify the chunk to fit the invocation */

 Make clobbered registers work too */

 Patch the invocation */

 Find out where we are and put everything there */

 Make sure we only write valid b instructions */

 Modify the chunk to fit the invocation */

 Make clobbered registers work too */

 Patch the invocation */

 Find out where we are and put everything there */

 Make sure we only write valid b instructions */

 Patch the invocation */

 Find out where we are and put everything there */

 Make sure we only write valid b instructions */

 Modify the chunk to fit the invocation */

 Patch the invocation */

 Loads */

 CONFIG_PPC_BOOK3E_MMU */

 Stores */

 CONFIG_PPC_BOOK3E_MMU */

 Nops */

 Rewrites */

 Tell the host to map the magic page to -4096 on all CPUs */

 Quick self-test to see if the mapping works */

 Now loop through all code and find instructions */

	/*

	 * Being interrupted in the middle of patching would

	 * be bad for SPRG4-7, which KVM can't keep in sync

	 * with emulated accesses because reads don't trap.

 Avoid patching the template code */

 Enable napping */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *    Copyright (C) 2006 Benjamin Herrenschmidt, IBM Corp.

 *			 <benh@kernel.crashing.org>

 *    and		 Arnd Bergmann, IBM Corp.

/* The probing of PCI controllers from of_platform is currently

 * 64 bits only, mostly due to gratuitous differences between

 * the 32 and 64 bits PCI code on PowerPC and the 32 bits one

 * lacking some bits needed here.

 Check if we can do that ... */

 Alloc and setup PHB data structure */

 Setup parent in sysfs */

 Setup the PHB using arch provided callback */

 Process "ranges" property */

 Init pci_dn data structures */

 Create EEH PE for the PHB */

 Scan the bus */

	/* Claim resources. This might need some rework as well depending

	 * whether we are doing probe-only or not, like assigning unassigned

	 * resources etc...

 Add probed PCI devices to the device model */

 CONFIG_PPC_OF_PLATFORM_PCI */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * HW_breakpoint: a unified kernel/user-space hardware breakpoint facility,

 * using the CPU's debug registers. Derived from

 * "arch/x86/kernel/hw_breakpoint.c"

 *

 * Copyright 2010 IBM Corporation

 * Author: K.Prasad <prasad@linux.vnet.ibm.com>

/*

 * Stores the breakpoints currently in use on each breakpoint address

 * register for every cpu

/*

 * Returns total number of data or instruction breakpoints available.

 no instruction breakpoints available */

/*

 * Install a perf counter breakpoint.

 *

 * We seek a free debug address register and use it for this

 * breakpoint.

 *

 * Atomic: we hold the counter->ctx->lock and we only handle variables

 * and registers local to this cpu.

	/*

	 * Do not install DABR values if the instruction must be single-stepped.

	 * If so, DABR will be populated in single_step_dabr_instruction().

/*

 * Uninstall the breakpoint contained in the given counter.

 *

 * First we search the debug address register it uses and then we disable

 * it.

 *

 * Atomic: we hold the counter->ctx->lock and we only handle variables

 * and registers local to this cpu.

/*

 * If any task has breakpoint from alternate infrastructure,

 * return true. Otherwise return false.

/*

 * If same task has breakpoint from alternate infrastructure,

 * return true. Otherwise return false.

/*

 * We don't use any locks to serialize accesses to cpu_bps or task_bps

 * because are already inside nr_bp_mutex.

 ptrace breakpoint */

 perf breakpoint */

/*

 * Perform cleanup of arch-specific counters during unregistration

 * of the perf-event

	/*

	 * If the breakpoint is unregistered between a hw_breakpoint_handler()

	 * and the single_step_dabr_instruction(), then cleanup the breakpoint

	 * restoration variables to prevent dangling pointers.

	 * FIXME, this should not be using bp->ctx at all! Sayeth peterz.

/*

 * Check for virtual address in kernel space.

/*

 * Watchpoint match range is always doubleword(8 bytes) aligned on

 * powerpc. If the given range is crossing doubleword boundary, we

 * need to increase the length such that next doubleword also get

 * covered. Ex,

 *

 *          address   len = 6 bytes

 *                |=========.

 *   |------------v--|------v--------|

 *   | | | | | | | | | | | | | | | | |

 *   |---------------|---------------|

 *    <---8 bytes--->

 *

 * In this case, we should configure hw as:

 *   start_addr = address & ~(HW_BREAKPOINT_SIZE - 1)

 *   len = 16 bytes

 *

 * @start_addr is inclusive but @end_addr is exclusive.

 DAWR region can't cross 512 bytes boundary on p10 predecessors */

 8xx can setup a range without limitation */

/*

 * Validate the arch-specific HW Breakpoint register settings

 must set alteast read or write */

/*

 * Restores the breakpoint on the debug registers.

 * Invoke this function if it is known that the execution context is

 * about to change to cause loss of MSR_SE settings.

/*

 * We've failed in reliably handling the hw-breakpoint. Unregister

 * it and throw a warning message to let the user know about it.

 Do not emulate user-space instructions, instead single-step them */

	/*

	 * Handle spurious exception only when any bp_per_reg is set.

	 * Otherwise this might be created by xmon and not actually a

	 * spurious exception.

		/*

		 * Ending address of DAWR range is less than starting

		 * address of op.

		/*

		 * Those addresses need to be in the same or in two

		 * consecutive 512B blocks;

		/*

		 * 'op address + 64B' generates an address that has a

		 * carry into bit 52 (crosses 2K boundary).

 Disable breakpoints during exception handling */

	/*

	 * The counter may be concurrently released but that can only

	 * occur from a call_rcu() path. We can then safely fetch

	 * the breakpoint, use its callback, touch its counter

	 * while we are in an rcu_read_lock() path.

 Workaround for Power10 DD1 */

	/*

	 * Return early after invoking user-callback function without restoring

	 * DABR if the breakpoint is from ptrace which always operates in

	 * one-shot mode. The ptrace-ed process will receive the SIGTRAP signal

	 * generated in do_dabr().

	/*

	 * As a policy, the callback is invoked in a 'trigger-after-execute'

	 * fashion

/*

 * Handle single-step exceptions following a DABR hit.

	/*

	 * Check if we are single-stepping as a result of a

	 * previous HW Breakpoint exception

		/*

		 * We shall invoke the user-defined callback function in the

		 * single stepping handler to confirm to 'trigger-after-execute'

		 * semantics

	/*

	 * If the process was being single-stepped by ptrace, let the

	 * other single-step actions occur (e.g. generate SIGTRAP).

/*

 * Handle debug exception notifications.

/*

 * Release the user breakpoints used by ptrace

 TODO */

	/*

	 * Disable the breakpoint request here since ptrace has defined a

	 * one-shot behaviour for breakpoint exceptions in PPC64.

	 * The SIGTRAP signal is generated automatically for us in do_dabr().

	 * We don't have to do anything about that here

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2019 IBM Corporation <nayna@linux.ibm.com>

 *

 * This code exposes secure variables to user via sysfs

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Contains routines needed to support swiotlb for ppc.

 *

 * Copyright (C) 2009-2010 Freescale Semiconductor, Inc.

 * Author: Becky Bruce

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2001 Mike Corrigan & Dave Engebretsen, IBM Corporation

 * 

 * Rewrite, cleanup, new allocation schemes, virtual merging: 

 * Copyright (C) 2004 Olof Johansson, IBM Corporation

 *               and  Ben. Herrenschmidt, IBM Corporation

 *

 * Dynamic DMA mapping support, bus-independent parts.

/*

 * We precalculate the hash to avoid doing it on every allocation.

 *

 * The hash is important to spread CPUs across all the pools. For example,

 * on a POWER7 with 4 way SMT we want interrupts on the primary threads and

 * with 4 pools all primary threads would map to the same pool.

/*

 * Must execute after PCI and VIO subsystem have initialised but before

 * devices are probed.

 This allocator was derived from x86_64's bit string search */

 Sanity check */

	/*

	 * We don't need to disable preemption here because any CPU can

	 * safely use any IOMMU pool.

	/* The case below can happen if we have a small segment appended

	 * to a large, or when the previous alloc was at the very end of

	 * the available space. If so, go back to the initial start.

		/* If we're constrained on address range, first try

		 * at the masked hint to avoid O(n) search complexity,

		 * but on second pass, start at 0 in pool 0.

 First try the pool from the start */

 Now try scanning all the other pools */

 Last resort: try largepool */

 Give up */

 Bump the hint to a new block for small allocs. */

 Don't bump to new block to avoid fragmentation */

 Overflow will be taken care of at the next allocation */

 Update handle for SG allocations */

 Offset into real TCE table */

 Set the return dma address */

 Put the TCEs in the HW table */

	/* tbl->it_ops->set() only returns non-zero for transient errors.

	 * Clean up the table bitmap in this case and return

	 * DMA_MAPPING_ERROR. For all other errors the functionality is

	 * not altered.

 Flush/invalidate TLB caches if necessary */

 Make sure updates are seen by hardware */

 The large pool is the last pool at the top of the table */

	/* Make sure TLB cache is flushed if the HW needs it. We do

	 * not do an mb() here on purpose, it is not needed on any of

	 * the current platforms.

 Init first segment length for backout at failure */

 Sanity check */

 Allocate iommu entries for that segment */

 Handle failure */

 Convert entry to a dma_addr_t */

 Insert into HW table */

 If we are in an open segment, try merging */

			/* We cannot merge if:

			 * - allocated dma_addr isn't contiguous to previous allocation

 Can't merge: create a new segment */

 This is a new segment, fill entries */

 Calculate next page pointer for contiguous check */

 Flush/invalidate TLB caches if necessary */

	/* For the sake of ppc_iommu_unmap_sg, we clear out the length in the

	 * next entry of the sglist if we didn't fill the list completely

 Make sure updates are seen by hardware */

	/* Flush/invalidate TLBs if necessary. As for iommu_free(), we

	 * do not do an mb() here, the affected platforms do not need it

	 * when freeing.

	/*

	 * In case of firmware assisted dump system goes through clean

	 * reboot process at the time of system crash. Hence it's safe to

	 * clear the TCE entries if firmware assisted dump is active.

 Clear the table in case firmware left allocations in it */

 Reserve the existing mappings left by the first kernel. */

			/*

			 * Freed TCE entry contains 0x7fffffffffffffff on JS20

	/*

	 * Reserve page 0 so it will not be used for any mappings.

	 * This avoids buggy drivers that consider page 0 to be invalid

	 * to crash the machine or even lose data.

 Check if res_start..res_end is a valid range in the table */

/*

 * Build a iommu_table structure.  This contains a bit map which

 * is used to manage allocation of the tce space.

 number of bytes needed for the bitmap */

 We only split the IOMMU table if we have 1GB or more of space */

 We reserve the top 1/4 of the table for large allocations */

 ignore reserved bit0 */

 verify that table contains no entries */

 free bitmap */

 free table */

/* Creates TCEs for a user provided buffer.  The user buffer must be

 * contiguous real kernel storage (not vmalloc).  The address passed here

 * comprises a page address and offset into that page. The dma_addr_t

 * returned will point to the same byte within the page as was passed in.

/* Allocates a contiguous real buffer and creates mappings over it.

 * Returns the virtual address of the buffer and sets dma_handle

 * to the dma address (mapping) of the first page.

 	/*

	 * Client asked for way too much space.  This is checked later

	 * anyway.  It is easier to debug here for the drivers than in

	 * the tce tables.

 Alloc enough pages (and possibly more) */

 Set up tces to cover the allocated range */

/*

 * SPAPR TCE API

 Flush/invalidate TLB caches if necessary */

 Make sure updates are seen by hardware */

	/*

	 * VFIO does not control TCE entries allocation and the guest

	 * can write new TCEs on top of existing ones so iommu_tce_build()

	 * must be able to release old pages. This functionality

	 * requires exchange() callback defined so if it is not

	 * implemented, we disallow taking ownership over the table.

	/*

	 * The sysfs entries should be populated before

	 * binding IOMMU group. If sysfs entries isn't

	 * ready, we simply bail.

	/*

	 * Some devices might not have IOMMU table and group

	 * and we needn't detach them from the associated

	 * IOMMU groups

 CONFIG_IOMMU_API */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2001 Mike Corrigan & Dave Engebretsen IBM Corporation

 CONFIG_PPC64 */

/*

 * Create the ppc64 and ppc64/rtas directories early. This allows us to

 * assume that they have been previously created in drivers.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Common boot and setup code for both 32-bit and 64-bit.

 * Extracted from arch/powerpc/kernel/setup_64.c.

 *

 * Copyright (C) 2001 PPC64 Team, IBM Corp

/* The main machine-dep calls structure

/*

 * These are used in binfmt_elf.c to put aux entries on the stack

 * for each elf executable being started.

/*

 * This still seems to be needed... -- paulus

 Variables required to store legacy IO irq routing */

 XXX should go elsewhere eventually */

 This keeps a track of which one is the crashing cpu. */

 also used by kexec */

	/*

	 * if fadump is active, cleanup the fadump registration before we

	 * shutdown.

 Used by the G5 thermal driver */

 Display the amount of memory */

 more straightforward, but potentially misleading */

 show the actual temp sensor range */

 CONFIG_TAU */

	/*

	 * Platforms that have variable clock rates, should implement

	 * the method ppc_md.get_proc_freq() that reports the clock

	 * rate of a given cpu. The rest can use ppc_proc_freq to

	 * report the clock rate that is same across all cpus.

	/* If we are a Freescale core do a simple check so

 7441/7450/7451, Voyager */

 7445/7455, Apollo 6 */

 7447/7457, Apollo 7 */

 7447A, Apollo 7 PM */

 7448, Apollo 8 */

 7410, Nitro */

 e500/book-e */

 740P/750P ?? */

 POWER9 bits 12-15 give chip type */

 POWER10 bit 12 gives SMT8/4 */

 If this is the last cpu, print the summary */

 just in case, cpu 0 is not the first */

	/* If we were passed an initrd, set the ROOT_DEV properly if the values

	 * look sensible. If not, clear initrd reference.

 CONFIG_BLK_DEV_INITRD */

	/* This implementation only supports power of 2 number of threads

	 * for simplicity and performance

/**

 * setup_cpu_maps - initialize the following cpu maps:

 *                  cpu_possible_mask

 *                  cpu_present_mask

 *

 * Having the possible map set up early allows us to restrict allocations

 * of things like irqstacks to nr_cpu_ids rather than NR_CPUS.

 *

 * We do not initialize the online map here; cpus set their own bits in

 * cpu_online_mask as they come up.

 *

 * This function is valid only for Open Firmware systems.  finish_device_tree

 * must be called before using this.

 *

 * While we're here, we may as well set the "physical" cpu ids in the paca.

 *

 * NOTE: This must match the parsing done in early_init_dt_scan_cpus.

 XXX: what is this? uninitialized?? */

 assume logical == phys */

 If no SMT supported, nthreads is forced to 1 */

	/*

	 * On pSeries LPAR, we need to know how many cpus

	 * could possibly be added to this partition.

 Double maxcpus for processors which have SMT capability */

 CONFIG_PPC64 */

        /* Initialize CPU <=> thread mapping/

	 *

	 * WARNING: We assume that the number of threads is the same for

	 * every CPU in the system. If that is not the case, then some code

	 * here will have to be reworked

 Now that possible cpus are set, set nr_cpu_ids for later use */

 CONFIG_SMP */

 CONFIG_PCSPKR_PLATFORM */

	/*

	 * Iterate all ppc_md structures until we find the proper

	 * one for the current machine type

	/*

	 * Check ppc_md is empty, if not we have a bug, ie, we setup an

	 * entry before probe_machine() which will be overwritten

 What can we do if we didn't find ? */

 Match a class of boards, not a specific device configuration. */

		/* Pegasos has no device_type on its 8042 node, look for the

 FDC1 */

 ipmi is supposed to fail here */

	/*

	 * panic does a local_irq_disable, but we really

	 * want interrupts to be hard disabled.

	/*

	 * If firmware-assisted dump has been registered then trigger

	 * firmware-assisted dump and let firmware handle everything else.

 May not return */

 may not return; must be done last */

/*

 * Dump out kernel offset information on panic.

 PPC64 always does a hard irq disable in its panic handler */

/*

 * For platforms that have configurable cache-coherency.  This function

 * checks that the cache coherency setting of the kernel matches the setting

 * left by the firmware, as indicated in the device tree.  Since a mismatch

 * will eventually result in DMA failures, we print * and error and call

 * BUG() in that case.

 CONFIG_CHECK_CACHE_COHERENCY */

/*

 * Called into from start_kernel this initializes memblock, which is used

 * to manage page allocation until mem_init is called.

 Set a half-reasonable default so udelay does something sensible */

 Unflatten the device-tree passed by prom_init or kexec */

	/*

	 * Initialize cache line/block info from device-tree (on ppc64) or

	 * just cputable (on ppc32).

 Initialize RTAS if available. */

 Check if we have an initrd provided via the device-tree. */

 Probe the machine type, establish ppc_md. */

 Setup panic notifier if requested by the platform. */

	/*

	 * Configure ppc_md.power_save (ppc32 only, 64-bit machines do

	 * it from their respective probe() function.

 Discover standard serial ports. */

 Register early console with the printk subsystem. */

 Setup the various CPU maps based on the device-tree. */

 Initialize xmon. */

 Check the SMT related command line arguments (ppc64). */

 Parse memory topology */

	/*

	 * Release secondary cpus out of their spinloops at 0x60 now that

	 * we can map physical -> logical CPU ids.

	 *

	 * Freescale Book3e parts spin in a loop provided by firmware,

	 * so smp_release_cpus() does nothing for them.

 On BookE, setup per-core TLB data structures. */

 Print various info about the machine that has been gathered so far. */

 Reserve large chunks of memory for use by CMA for KVM. */

  Reserve large chunks of memory for us by CMA for hugetlb */

 Initialize the MMU context management stuff. */

 Interrupt code needs to be 64K-aligned. */

 SPDX-License-Identifier: GPL-2.0

/*

 * ppc64 "iomap" interface implementation.

 *

 * (C) Copyright 2004 Linus Torvalds

 CONFIG_PCI */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * pci_dn.c

 *

 * Copyright (C) 2001 Todd Inglett, IBM Corporation

 *

 * PCI manipulation via device_nodes.

/*

 * The function is used to find the firmware data of one

 * specific PCI device, which is attached to the indicated

 * PCI bus. For VFs, their firmware data is linked to that

 * one of PF's bridge. For other devices, their firmware

 * data is linked to that of their bridge.

	/*

	 * We probably have virtual bus which doesn't

	 * have associated bridge.

	/*

	 * Except virtual bus, all PCI buses should

	 * have device nodes.

 Fast path: fetch from PCI device */

 Fast path: fetch from device node */

 Slow path: fetch from firmware data hierarchy */

 Search device directly */

 Check device node */

	/*

	 * VFs don't have device nodes. We hook their

	 * firmware data to PF's bridge.

 Allocate EEH device */

 Associate EEH device with OF node */

 CONFIG_EEH */

 Except PHB, we always have the parent */

 Only support IOV for now */

 Check if VFs have been populated */

 Create the EEH device for the VF */

 FIXME: these should probably be populated by the EEH probe */

 CONFIG_EEH */

 Only support IOV PF for now */

 Check if VFs have been populated */

	/*

	 * We might introduce flag to pci_dn in future

	 * so that we can release VF's firmware data in

	 * a batch mode.

			/*

			 * Release EEH state for this VF. The PCI core

			 * has already torn down the pci_dev for this VF, but

			 * we're responsible to removing the eeh_dev since it

			 * has the same lifetime as the pci_dn that spawned it.

				/*

				 * We allocate pci_dn's for the totalvfs count,

				 * but only only the vfs that were activated

				 * have a configured PE.

 CONFIG_EEH */

 CONFIG_PCI_IOV */

 First register entry is addr (00BBSS00)  */

 vendor/device IDs and class code */

 Extended config space */

 Create EEH device */

 Attach to parent node */

 Drop the parent pci_dn's ref to our backing dt node */

	/*

	 * At this point we *might* still have a pci_dev that was

	 * instantiated from this pci_dn. So defer free()ing it until

	 * the pci_dev's release function is called.

 NB: pdev has a ref to dn */

/*

 * Traverse a device tree stopping each PCI device in the tree.

 * This is done depth first.  As each node is processed, a "pre"

 * function is called and the children are processed recursively.

 *

 * The "pre" func returns a value.  If non-zero is returned from

 * the "pre" func, the traversal stops and this value is returned.

 * This return value is useful when using traverse as a method of

 * finding a device.

 *

 * NOTE: we do not run the func for devices that do not appear to

 * be PCI except for the start node which we assume (this is good

 * because the start node is often a phb which may be missing PCI

 * properties).

 * We use the class-code as an indicator. If we run into

 * one of these nodes we also assume its siblings are non-pci for

 * performance.

 We started with a phb, iterate all childs */

 If we are a PCI bridge, go down */

 Depth first...do children */

 ok, try next sibling instead. */

 Walk up to next valid sibling. */

/** 

 * pci_devs_phb_init_dynamic - setup pci devices under this PHB

 * phb: pci-to-host bridge (top-level bridge connecting to cpu)

 *

 * This routine is called both during boot, (before the memory

 * subsystem is set up, before kmalloc is valid) and during the 

 * dynamic lpar operation of adding a PHB to a running system.

 PHB nodes themselves must not match */

 Update dn->phb ptrs for new phb and children devices */

 Setup the fast path */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2001 Anton Blanchard <anton@au.ibm.com>, IBM

 *

 * Communication to userspace based on kernel/printk.c

 RTAS service tokens */

 Stop logging to nvram after first fatal error */

static int logging_enabled; /* Until we initialize everything,

                             * make sure we don't try logging

/*

 * Since we use 32 bit RTAS, the physical address of this must be below

 * 4G or else bad things happen. Allocate this in the kernel data and

 * make it big enough.

/* To see this info, grep RTAS /var/log/messages and each entry

 * will be collected together with obvious begin/end.

 * There will be a unique identifier on the begin and end lines.

 * This will persist across reboots.

 *

 * format of error logs returned from RTAS:

 * bytes	(size)	: contents

 * --------------------------------------------------------

 * 0-7		(8)	: rtas_error_log

 * 8-47		(40)	: extended info

 * 48-51	(4)	: vendor id

 * 52-1023 (vendor specific) : location code and debug data

		/*

		 * Print perline bytes on each line, each line will start

		 * with RTAS and a changing number, so syslogd will

		 * print lines that are otherwise the same.  Separate every

		 * 4 bytes with a space.

 rtas fixed header */

 extended header */

/*

 * First write to nvram, if fatal error, that is the only

 * place we log the info.  The error will be picked up

 * on the next reboot by rtasd.  If not fatal, run the

 * method for the type of error.  Currently, only RTAS

 * errors have methods implemented, but in the future

 * there might be a need to store data in nvram before a

 * call to panic().

 *

 * XXX We write to nvram periodically, to indicate error has

 * been written and sync'd, but there is a possibility

 * that if we don't shutdown correctly, a duplicate error

 * record will be created on next reboot.

 get length and increase count */

 @@@ DEBUG @@@ */

 Write error to NVRAM */

 CONFIG_PPC64 */

	/*

	 * rtas errors can occur during boot, and we do want to capture

	 * those somewhere, even if nvram isn't ready (why not?), and even

	 * if rtasd isn't ready. Put them into the boot log, at least.

 Check to see if we need to or have stopped logging */

 @@@ DEBUG @@@ */

 call type specific method for error */

 First copy over sequence number */

 Second copy over error log data */

 @@@ DEBUG @@@ */

 @@@ DEBUG @@@ */

/* This will check if all events are logged, if they are then, we

 * know that we can safely clear the events in NVRAM.

 * Next we'll sit and wait for something else to log.

 if it's 0, then we know we got the last one (the one in NVRAM) */

 CONFIG_PPC64 */

/*

 * Delay should be at least one second since some machines have problems if

 * we call event-scan too quickly.

 raw_ OK because just using CPU as starting point. */

 See if we have any error stored in NVRAM */

 We can use rtas_log_buf now */

 CONFIG_PPC64 */

 CONFIG_PPC64 */

 Retrieve errors from nvram if any */

 Cancel the rtas event scan work */

 No RTAS */

 Broken firmware: take a rate of zero to mean don't scan */

 Make room for the sequence number */

 We only do surveillance on pseries */

 SPDX-License-Identifier: GPL-2.0+



 Security related flags and so on.



 Copyright 2018, Michael Ellerman, IBM Corporation.

	/*

	 * It would make sense to check SEC_FTR_SPEC_BAR_ORI31 below as well.

	 * But there's a good reason not to. The two flags we check below are

	 * both are enabled by default in the kernel, so if the hcall is not

	 * functional they will be enabled.

	 * On a system where the host firmware has been updated (so the ori

	 * functions as a barrier), but on which the hypervisor (KVM/Qemu) has

	 * not been updated, we would like to enable the barrier. Dropping the

	 * check for SEC_FTR_SPEC_BAR_ORI31 achieves that. The only downside is

	 * we potentially enable the barrier on systems where the host firmware

	 * is not updated, but that's harmless as it's a no-op.

 CONFIG_DEBUG_FS */

 CONFIG_PPC_FSL_BOOK3E || CONFIG_PPC_BOOK3S_64 */

 CONFIG_PPC_FSL_BOOK3E */

/*

 * Store-forwarding barrier support.

 This is the generic flag used by other architectures */

 Until firmware tells us, we have the barrier with auto */

 This is the generic flag used by other architectures */

 Default to fallback in case fw-features are not available */

		/*

		 * We don't have an explicit signal from firmware that we're

		 * vulnerable or not, we only have certain CPU revisions that

		 * are known to be vulnerable.

		 *

		 * We assume that if we're on another CPU, where the barrier is

		 * NONE, then we are not vulnerable.

		/*

		 * If we do have a barrier type then we are vulnerable. The

		 * barrier is not a global or per-process mitigation, so the

		 * only value we can report here is PR_SPEC_ENABLE, which

		 * appears as "vulnerable" in /proc.

 Only do anything if we're changing state */

 CONFIG_DEBUG_FS */

 This controls the branch from guest_exit_cont to kvm_flush_link_stack

 Could use HW flush, but that could also flush count cache

 Patch out the bcctr first, then nop the rest

 This controls the branch from _switch to flush_branch_caches

 Nothing to be done

 Patch in the bcctr last

 li r9,0x7fff

 mtctr r9

 If we just need to flush the link stack, early return

 If we have flush instruction, early return

	/*

	 * There's no firmware feature flag/hypervisor bit to tell us we need to

	 * flush the link stack on context switch. So we set it here if we see

	 * either of the Spectre v2 mitigations that aim to protect userspace.

/*

 * The RFI flush is not KPTI, but because users will see doco that says to use

 * nopti we hijack that option here to also disable the RFI flush.

	/*

	 * We don't need to do the flush explicitly, just enter+exit kernel is

	 * sufficient, the RFI exit handlers will do the right thing.

 Only allocate the fallback flush area once (at boot time). */

	/*

	 * If there is no d-cache-size property in the device tree, l1d_size

	 * could be zero. That leads to the loop in the asm wrapping around to

	 * 2^64-1, and then walking off the end of the fallback area and

	 * eventually causing a page fault which is fatal. Just default to

	 * something vaguely sane.

	/*

	 * Align to L1d size, and size it at 2x L1d size, to catch possible

	 * hardware prefetch runoff. We don't have a recipe for load patterns to

	 * reliably avoid the prefetcher.

 Only do anything if we're changing state */

 Only do anything if we're changing state */

 Only do anything if we're changing state */

 CONFIG_DEBUG_FS */

 CONFIG_PPC_BOOK3S_64 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Author: Kumar Gala <galak@kernel.crashing.org>

 *

 * Copyright 2009 Freescale Semiconductor Inc.

 already performed the barrier */

 CONFIG_SMP */

 CONFIG_SMP */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2006-2007, Michael Ellerman, IBM Corporation.

 PowerPC doesn't support multiple MSI yet */

	/*

	 * We can be called even when arch_setup_msi_irqs() returns -ENOSYS,

	 * so check the pointer again.

 SPDX-License-Identifier: GPL-2.0-or-later

 for show_regs */

/*

 * local irqs must be disabled. Returns false if the caller must re-enable

 * them, check for new work, and try again.

 *

 * This should be called with local irqs disabled, but if they were previously

 * enabled when the interrupt handler returns (indicating a process-context /

 * synchronous interrupt) then irqs_enabled should be true.

 *

 * restartable is true then EE/RI can be left on because interrupts are handled

 * with a restart sequence.

 This must be done with RI=1 because tracing may touch vmaps */

 This pattern matches prep_irq_for_idle */

 Has to run notrace because it is entered not completely "reconciled" */

 finish reconciling */

		/*

		 * When entering from userspace we mostly have the AMR/IAMR

		 * different from kernel default values. Hence don't compare.

	/*

	 * This is not required for the syscall exit path, but makes the

	 * stack frame look nicer. If this was initialised in the first stack

	 * frame, or if the unwinder was taught the first stack frame always

	 * returns to user with IRQS_ENABLED, this store could be avoided!

	/*

	 * If system call is called with TM active, set _TIF_RESTOREALL to

	 * prevent RFSCV being used to return to userspace, because POWER9

	 * TM implementation has problems with this instruction returning to

	 * transactional state. Final register values are not relevant because

	 * the transaction will be aborted upon return anyway. Or in the case

	 * of unsupported_scv SIGILL fault, the return state does not much

	 * matter because it's an edge case.

	/*

	 * If the system call was made with a transaction active, doom it and

	 * return without performing the system call. Unless it was an

	 * unsupported scv vector, in which case it's treated like an illegal

	 * instruction.

 Enable TM in the kernel, and disable EE (for scv) */

 tabort, this dooms the transaction, nothing else */

		/*

		 * Userspace will never see the return value. Execution will

		 * resume after the tbegin. of the aborted transaction with the

		 * checkpointed register state. A context switch could occur

		 * or signal delivered to the process before resuming the

		 * doomed transaction context, but that should all be handled

		 * as expected.

 CONFIG_PPC_TRANSACTIONAL_MEM

 Unsupported scv vector */

		/*

		 * We use the return value of do_syscall_trace_enter() as the

		 * syscall number. If the syscall was rejected for any reason

		 * do_syscall_trace_enter() returns an invalid syscall number

		 * and the test against NR_syscalls will fail and the return

		 * value to be used is in regs->gpr[3].

 Unsupported scv vector */

 May be faster to do array_index_nospec? */

	/*

	 * Check to see if the dbcr0 register is set up to debug.

	 * Use the internal debug mode bit to do this.

 EE in HV mode sets HSRRs like 0xea0

	/*

	 * A NMI / soft-NMI interrupt may have come in after we found

	 * srr_valid and before the SRRs are loaded. The interrupt then

	 * comes in and clobbers SRRs and clears srr_valid. Then we load

	 * the SRRs here and test them above and find they don't match.

	 *

	 * Test validity again after that, to catch such false positives.

	 *

	 * This test in general will have some window for false negatives

	 * and may not catch and fix all such cases if an NMI comes in

	 * later and clobbers SRRs without clearing srr_valid, but hopefully

	 * such things will get caught most of the time, statistically

	 * enough to be able to get a warning out.

 fixup */

			/*

			 * SIGPENDING must restore signal handler function

			 * argument GPRs, and some non-volatiles (e.g., r1).

			 * Restore all for now. This could be made lighter.

			/*

			 * If userspace MSR has all available FP bits set,

			 * then they are live and no need to restore. If not,

			 * it means the regs were given up and restore_math

			 * may decide to restore them (to avoid taking an FP

			 * fault).

 Restore user access locks last */

/*

 * This should be called after a syscall returns, with r3 the return value

 * from the syscall. If this function returns non-zero, the system call

 * exit assembly should additionally load all GPR registers and CTR and XER

 * from the interrupt frame.

 *

 * The function graph tracer can not trace the return side of this function,

 * because RI=0 and soft mask state is "unreconciled", so it is marked notrace.

 Check whether the syscall is issued inside a restartable sequence */

 Set SO bit in CR */

	/*

	 * This is called when detecting a soft-pending interrupt as well as

	 * an alternate-return interrupt. So we can't just have the alternate

	 * return path clear SRR1[MSR] and set PACA_IRQ_HARD_DIS (unless

	 * the soft-pending case were to fix things up as well). RI might be

	 * disabled, in which case it gets re-enabled by __hard_irq_disable().

	/*

	 * We don't need to restore AMR on the way back to userspace for KUAP.

	 * AMR can only have been unlocked if we interrupted the kernel.

	/*

	 * CT_WARN_ON comes here via program_check_exception,

	 * so avoid recursion.

 Returning to a kernel context with local irqs enabled. */

 Return to preemptible kernel context */

		/*

		 * Stack store exit can't be restarted because the interrupt

		 * stack frame might have been clobbered.

			/*

			 * Replay pending soft-masked interrupts now. Don't

			 * just local_irq_enabe(); local_irq_disable(); because

			 * if we are returning from an asynchronous interrupt

			 * here, another one might hit after irqs are enabled,

			 * and it would exit via this same path allowing

			 * another to fire, and so on unbounded.

 Took an interrupt, may have more exit work to do. */

		/*

		 * An interrupt may clear MSR[EE] and set this concurrently,

		 * but it will be marked pending and the exit will be retried.

		 * This leaves a racy window where MSR[EE]=0 and HARD_DIS is

		 * clear, until interrupt_exit_kernel_restart() calls

		 * hard_irq_disable(), which will set HARD_DIS again.

		/*

		 * Returning to a kernel context with local irqs disabled.

		 * Here, if EE was enabled in the interrupted context, enable

		 * it on return as well. A problem exists here where a soft

		 * masked interrupt may have cleared MSR[EE] and set HARD_DIS

		 * here, and it will still exist on return to the caller. This

		 * will be resolved by the masked interrupt firing again.

 CONFIG_PPC64 */

	/*

	 * 64s does not want to mfspr(SPRN_AMR) here, because this comes after

	 * mtmsr, which would cause Read-After-Write stalls. Hence, take the

	 * AMR value from the check above.

/*

 * No real need to return a value here because the stack store case does not

 * get restarted.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2010 Michael Ellerman, IBM Corp.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 IBM Corporation

 * Author: Nayna Jain

 SPDX-License-Identifier: GPL-2.0-or-later

/*  Kernel module help for PPC.

    Copyright (C) 2001 Rusty Russell.



/* Count how many different relocations (different symbol, different

 Only count 24-bit relocs, others don't need stubs */

 add one for ftrace_caller */

	/* Compare the entire r_info (as opposed to ELF32_R_SYM(r_info) only) to

	 * make the comparison cheaper/faster. It won't affect the sorting or

	 * the counting algorithms' performance

/* Get the potential trampolines size required of the init and

	/* Everything marked ALLOC (this includes the exported

		/* If it's called *.init*, and we're not init, we're

 We don't want to look at debug sections. */

			/* Sort the relocation information based on a symbol and

			 * addend key. This is a stable O(n*log n) complexity

			 * alogrithm but it will reduce the complexity of

			 * count_relocs() to linear complexity O(n)

 Find .plt and .init.plt sections */

 Override their sizes */

 Set up a trampoline in the PLT to bounce us to the distant function */

 Init, or core PLT? */

 Find this entry, or if that fails, the next avail. entry */

 This is where to make the change */

		/* This is the symbol it is referring to.  Note that all

 `Everything is relative'. */

 Simply set it */

 Low half of the symbol */

 Higher half of the symbol */

			/* Sign-adjusted lower 16 bits: PPC ELF ABI says:

			   (((x >> 16) + ((x & 0x8000) ? 1 : 0))) & 0xFFFF.

			   This is the same, only sane.

 Only replace bits 2 through 26 */

 32-bit relative jump. */

 SPDX-License-Identifier: GPL-2.0

 5 sec */

 This is boot time so we spin. */

/* NOTE: get_rtc_time will get an error if executed in interrupt context

 * and if a delay is needed to read the clock.  In this case we just

 * silently return without updating rtc_tm.

 delay not allowed */

 probably decrementer */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Procedures for interfacing to the RTAS on CHRP machines.

 *

 * Peter Bergner, IBM	March 2001.

 * Copyright (C) 2001 IBM.

 This is here deliberately so it's only used in this file */

 rtas uses SRRs, invalidate */

/*

 * If non-NULL, this gets called when the kernel terminates.

 * This is done like this so rtas_flash can be a module.

/* RTAS use home made raw locking instead of spin_lock_irqsave

 * because those can be called from within really nasty contexts

 * such as having the timebase stopped which would lockup with

 * normal locks and spinlock debugging enabled

/*

 * call_rtas_display_status and call_rtas_display_status_delay

 * are designed only for very early low-level debugging, which

 * is why the token is hard-coded to 10.

 did last write end with unprinted newline? */

/* If you think you're dying before early_init_dt_scan_rtas() does its

 * work, you can hard code the token values for your firmware here and

 * hardcode rtas.base/entry etc.

 Add CRs before LFs */

 if there is more than one character to be displayed, wait a bit */

 CONFIG_UDBG_RTAS_CONSOLE */

 did last write end with unprinted newline? */

 use hex display if available */

	/*

	 * Last write ended with newline, but we didn't print it since

	 * it would just clear the bottom line of output. Print it now

	 * instead.

	 *

	 * If no newline is pending and form feed is supported, clear the

	 * display with a form feed; otherwise, print a CR to start output

	 * at the beginning of the line.

			/* If newline is the last character, save it

			 * until next call to avoid bumping up the

			 * display output.

 RTAS wants CR-LF, not just LF */

				/* CR might be used to re-draw a line, so we'll

				 * leave it alone and not add LF.

 if we overwrite the screen length */

 needed by rtas_flash module */

/*

 * Return the firmware-specified size of the error log buffer

 *  for all rtas calls that require an error buffer argument.

 *  This includes 'check-exception' and 'rtas-last-error'.

/** Return a copy of the detailed error text associated with the

 *  most recent failed call to rtas.  Because the error text

 *  might go stale if there are any other intervening rtas calls,

 *  this routine must be called atomically with whatever produced

 *  the error (i.e. with rtas.lock still held from the previous call).

 Log the error in the unlikely case that there was one. */

 CONFIG_RTAS_ERROR_LOGGING */

 We use the global rtas args buffer */

	/* A -1 return code indicates that the last command couldn't

/* For RTAS_BUSY (-2), delay for 1 millisecond.  For an extended busy status

 * code of 990n, perform the hinted delay of 10^n (last digit) milliseconds.

 For an RTAS busy status code, perform the hinted delay. */

 Hardware Error */

 Bad indicator/domain/etc */

 Isolation error */

 Outstanding TCE/PTE */

 No usable slot */

/*

 * Ignoring RTAS extended delay

/**

 * rtas_ibm_suspend_me() - Call ibm,suspend-me to suspend the LPAR.

 *

 * @fw_status: RTAS call status will be placed here if not NULL.

 *

 * rtas_ibm_suspend_me() should be called only on a CPU which has

 * received H_CONTINUE from the H_JOIN hcall. All other active CPUs

 * should be waiting to return from H_JOIN.

 *

 * rtas_ibm_suspend_me() may suspend execution of the OS

 * indefinitely. Callers should take appropriate measures upon return, such as

 * resetting watchdog facilities.

 *

 * Callers may choose to retry this call if @fw_status is

 * %RTAS_THREADS_ACTIVE.

 *

 * Return:

 * 0          - The partition has resumed from suspend, possibly after

 *              migration to a different host.

 * -ECANCELED - The operation was aborted.

 * -EAGAIN    - There were other CPUs not in H_JOIN at the time of the call.

 * -EBUSY     - Some other condition prevented the suspend from succeeding.

 * -EIO       - Hardware/platform error.

 allow power on only with power button press */

 allow power on only with power button press */

 Must be in the RMO region, so we place it here */

	/*

	 * Firmware with the ibm,extended-os-term property is guaranteed

	 * to always return from an ibm,os-term call. Earlier versions without

	 * this property may terminate the partition which we want to avoid

	 * since it interferes with panic_timeout.

/**

 * rtas_activate_firmware() - Activate a new version of firmware.

 *

 * Activate a new version of partition firmware. The OS must call this

 * after resuming from a partition hibernation or migration in order

 * to maintain the ability to perform live firmware updates. It's not

 * catastrophic for this method to be absent or to fail; just log the

 * condition in that case.

 *

 * Context: This function may sleep.

/**

 * rtas_call_reentrant() - Used for reentrant rtas calls

 * @token:	Token for desired reentrant RTAS call

 * @nargs:	Number of Input Parameters

 * @nret:	Number of Output Parameters

 * @outputs:	Array of outputs

 * @...:	Inputs for desired RTAS call

 *

 * According to LoPAR documentation, only "ibm,int-on", "ibm,int-off",

 * "ibm,get-xive" and "ibm,set-xive" are currently reentrant.

 * Reentrant calls need their own rtas_args buffer, so not using rtas.args, but

 * PACA one instead.

 *

 * Return:	-1 on error,

 *		First output value of RTAS call if (nret > 0),

 *		0 otherwise,

 We use the per-cpu (PACA) rtas args buffer */

 CONFIG_PPC_PSERIES */

/**

 * Find a specific pseries error log in an RTAS extended event log.

 * @log: RTAS error/event log

 * @section_id: two character section identifier

 *

 * Returns a pointer to the specified errorlog or NULL if not found.

 Check that we understand the format */

/*

 * The sys_rtas syscall, as originally designed, allows root to pass

 * arbitrary physical addresses to RTAS calls. A number of RTAS calls

 * can be abused to write to arbitrary memory and do other things that

 * are potentially harmful to system integrity, and thus should only

 * be used inside the kernel and not exposed to userspace.

 *

 * All known legitimate users of the sys_rtas syscall will only ever

 * pass addresses that fall within the RMO buffer, and use a known

 * subset of RTAS calls.

 *

 * Accordingly, we filter RTAS requests to check that the call is

 * permitted, and that provided pointers fall within the RMO buffer.

 * The rtas_filters list contains an entry for each permitted call,

 * with the indexes of the parameters which are expected to contain

 * addresses and sizes of buffers allocated inside the RMO buffer.

 Indexes into the args buffer, -1 if not used */

 Special cased */

			/*

			 * Special case for ibm,configure-connector where the

			 * address can be 0

 CONFIG_PPC_RTAS_FILTER */

 We assume to be passed big endian arguments */

 Copy in args. */

 Need to handle ibm,suspend_me call specially */

		/*

		 * rtas_ibm_suspend_me assumes the streamid handle is in cpu

		 * endian, or at least the hcall within it requires it.

	/* A -1 return code indicates that the last command couldn't

 Copy out args. */

/*

 * Call early during boot, before mem init, to retrieve the RTAS

 * information from the device-tree and allocate the RMO buffer for userland

 * accesses.

	/* Get RTAS dev node and fill up our "rtas" structure with infos

	 * about it.

	/* If RTAS was found, allocate the RMO buffer for it and look for

	 * the stop-self token if any

 break now */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ePAPR para-virtualization support.

 *

 * Copyright (C) 2012 Freescale Semiconductor, Inc.

 SPDX-License-Identifier: GPL-2.0

/*

 *   Copyright (C) 2000 Tilmann Bitterberg

 *   (tilmann@bitterberg.de)

 *

 *   RTAS (Runtime Abstraction Services) stuff

 *   Intention is to provide a clean user interface

 *   to use the RTAS.

 *

 *   TODO:

 *   Split off a header file and maybe move it to a different

 *   location. Write Documentation on what the /proc/rtas/ entries

 *   actually do.

 for ppc_md */

 Token for Sensors */

 IBM specific sensors */

 9000 */

 9001 */

 9002 */

 9003 */

 9004 */

 Status return values */

 Location Codes */

 reserved / not used		'H' */

 reserved / not used		'J' */

 for _u_nit is rack mounted */

 Tokens for indicators */

 0 - 1000 (HZ)*/

 0 - 100 (%) */

 9000 */

 9001 */

 9002 */

 9003 - 9004: Vendor specific */

 9006 - 9999: Vendor specific */

 other */

 I only know of 17 sensors */    

 Globals */

 Save the time the user set */

 ****************************************************************** */

 Declarations */

 ****************************************************************** */

 POWER-ON-TIME                                                      */

 ****************************************************************** */

 save the time */

 nano */);

 ****************************************************************** */

 ****************************************************************** */

 PROGRESS                                                           */

 ****************************************************************** */

 save the string */

 Lets see if the user passed hexdigits */

 clear the line */

 rtas_progress("                   ", 0xffff);*/

 ****************************************************************** */

 ****************************************************************** */

 CLOCK                                                              */

 ****************************************************************** */

 ****************************************************************** */

 ****************************************************************** */

 SENSOR STUFF                                                       */

 ****************************************************************** */

 A sensor may have multiple instances */

 ****************************************************************** */

 int + int */

 ****************************************************************** */

/*

 * Builds a string of what rtas returned

 ****************************************************************** */

/*

 * Builds a string out of what the sensor said

 Defined return vales */

 What kind of sensor do we have here? */

 ****************************************************************** */

 ****************************************************************** */

/* 

 * Format: 

 * ${LETTER}${NUMBER}[[-/]${LETTER}${NUMBER} [ ... ] ]

 * the '.' may be an abbreviation

 ****************************************************************** */

 does not have a location */

 ****************************************************************** */

 INDICATORS - Tone Frequency                                        */

 ****************************************************************** */

 save it for later */

 ****************************************************************** */

 ****************************************************************** */

 INDICATORS - Tone Volume                                           */

 ****************************************************************** */

 save it for later */

 ****************************************************************** */

/**

 * ppc_rtas_rmo_buf_show() - Describe RTAS-addressable region for user space.

 *

 * Base + size description of a range of RTAS-addressable memory set

 * aside for user space to use as work area(s) for certain RTAS

 * functions. User space accesses this region via /dev/mem. Apart from

 * security policies, the kernel does not arbitrate or serialize

 * access to this region, and user space must ensure that concurrent

 * users do not interfere with each other.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Common pmac/prep/chrp pci routines. -- Cort

/* By default, we don't re-assign bus numbers. We do this only on

 * some pmacs

/* This will remain NULL for now, until isa-bridge.c is made common

 * to both 32-bit and 64-bit.

	/* Hide the PCI64 BARs from the kernel as their content doesn't

	 * fit well in the resource management

/*

 * Functions below are used on OpenFirmware machines.

	/* We fill the bus map with invalid values, that helps

	 * debugging.

 For each hose, we begin searching bridges */

/*

 * Returns the PCI device matching a given OF node

 Check if it might have a chance to be a PCI device */

	/* Ok, here we need some tweak. If we have already renumbered

	 * all busses, we can't rely on the OF bus number any more.

	 * the pci_to_OF_bus_map is not enough as several PCI busses

	 * may match the same OF bus number.

/* We create the "pci-OF-bus-map" property now so it appears in the

 * /proc device tree

 Fixup IO space offset */

 Scan all of the recorded PCI controllers.  */

	/* OpenFirmware based machines need a map of OF bus

	 * numbers vs. kernel bus numbers since we may have to

	 * remap them.

 Call common code to handle resource allocation */

 Call machine dependent fixup */

 Call machine dependent post-init code */

/* Provide information on locations of various I/O regions in physical

 * memory.  Do this on a per-card basis so that we choose the right

 * root bridge.

 * Note that the returned IO or memory base is a physical address

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 IBM Corporation

 * Author: Nayna Jain

/*

 * The "secure_rules" are enabled only on "secureboot" enabled systems.

 * These rules verify the file signatures against known good values.

 * The "appraise_type=imasig|modsig" option allows the known good signature

 * to be stored as an xattr or as an appended signature.

 *

 * To avoid duplicate signature verification as much as possible, the IMA

 * policy rule for module appraisal is added only if CONFIG_MODULE_SIG

 * is not enabled.

/*

 * The "trusted_rules" are enabled only on "trustedboot" enabled systems.

 * These rules add the kexec kernel image and kernel modules file hashes to

 * the IMA measurement list.

/*

 * The "secure_and_trusted_rules" contains rules for both the secure boot and

 * trusted boot. The "template=ima-modsig" option includes the appended

 * signature, when available, in the IMA measurement list.

/*

 * Returns the relevant IMA arch-specific policies based on the system secure

 * boot state.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * 

 * Common boot and setup code.

 *

 * Copyright (C) 2001 PPC64 Team, IBM Corp

		/*

		 * If we boot via kdump on a non-primary thread,

		 * make sure we point at the thread that actually

		 * set up this TLB.

		/*

		 * If we have threads, we need either tlbsrx.

		 * or e6500 tablewalk mode, or else TLB handlers

		 * will be racy and could produce duplicate entries.

		 * Should we panic instead?

 Look for ibm,smt-enabled OF option */

 Default to enabling all threads */

 Allow the command line to overrule the OF option */

 Look for smt-enabled= cmdline option */

 CONFIG_SMP */

* Fix up paca fields required for the boot cpu */

 The boot cpu is started */

 Allow percpu accesses to work until we setup percpu data */

 Mark interrupts disabled in PACA */

	/*

	 * Setup the trampolines from the lowmem exception vectors

	 * to the kdump kernel when not using a relocatable kernel.

 Under a PAPR hypervisor, we need hypercalls */

 Enable AIL if possible */

		/*

		 * Tell the hypervisor that we want our exceptions to

		 * be taken in little endian mode.

		 *

		 * We don't call this for big endian as our calling convention

		 * makes us always enter in BE, and the call may fail under

		 * some circumstances with kdump.

 Set endian mode using OPAL */

 AIL on native is done in cpu_ready_for_interrupts() */

	/*

	 * Enable AIL if supported, and we are in hypervisor mode. This

	 * is called once for every processor.

	 *

	 * If we are not in hypervisor mode the job is done once for

	 * the whole partition in configure_exceptions().

 P10 DD1 does not have HAIL */

	/*

	 * Set HFSCR:TM based on CPU features:

	 * In the special case of TM no suspend (P9N DD2.1), Linux is

	 * told TM is off via the dt-ftrs but told to (partially) use

	 * it via OPAL_REINIT_CPUS_TM_SUSPEND_DISABLED. So HFSCR[TM]

	 * will be off from dt-ftrs but we need to turn it on for the

	 * no suspend case.

 Set IR and DR in PACA MSR */

/*

 * Early initialization entry point. This is called by head.S

 * with MMU translation disabled. We rely on the "feature" of

 * the CPU that ignores the top 2 bits of the address in real

 * mode so we can access kernel globals normally provided we

 * only toy with things in the RMO region. From here, we do

 * some early parsing of the device-tree to setup out MEMBLOCK

 * data structures, and allocate & initialize the hash table

 * and segment tables so we can start running with translation

 * enabled.

 *

 * It is this function which will call the probe() callback of

 * the various platform types and copy the matching one to the

 * global ppc_md structure. Your platform can eventually do

 * some very early initializations from the probe() routine, but

 * this is not recommended, be very careful as, for example, the

 * device-tree is not accessible via normal means at this point.

 -------- printk is _NOT_ safe to use here ! ------- */

	/*

	 * Assume we're on cpu 0 for now.

	 *

	 * We need to load a PACA very early for a few reasons.

	 *

	 * The stack protector canary is stored in the paca, so as soon as we

	 * call any stack protected code we need r13 pointing somewhere valid.

	 *

	 * If we are using kcov it will call in_task() in its instrumentation,

	 * which relies on the current task from the PACA.

	 *

	 * dt_cpu_ftrs_init() calls into generic OF/fdt code, as well as

	 * printk(), which can trigger both stack protector and kcov.

	 *

	 * percpu variables and spin locks also use the paca.

	 *

	 * So set up a temporary paca. It will be replaced below once we know

	 * what CPU we are on.

 -------- printk is now safe to use ------- */

 Try new device tree based feature discovery ... */

 Otherwise use the old style CPU table */

 Enable early debugging if any specified (see udbg.h) */

	/*

	 * Do early initialization using the flattened device

	 * tree, such as retrieving the physical memory map or

	 * calculating/retrieving the hash table size.

 Now we know the logical id of our boot cpu, setup the paca. */

 Poison paca_ptrs[0] again if it's not the boot cpu */

	/*

	 * Configure exception handlers. This include setting up trampolines

	 * if needed, setting exception endian mode, etc...

	/*

	 * Configure Kernel Userspace Protection. This needs to happen before

	 * feature fixups for platforms that implement this using features.

 Apply all the dynamic patching */

 Initialize the hash table or TLB handling */

	/*

	 * After firmware and early platform setup code has set things up,

	 * we note the SPR values for configurable control/performance

	 * registers, and use those as initial defaults.

	/*

	 * At this point, we can let interrupts switch to virtual mode

	 * (the MMU has been setup), so adjust the MSR in the PACA to

	 * have IR and DR set and enable AIL if it exists

	/*

	 * We enable ftrace here, but since we only support DYNAMIC_FTRACE, it

	 * will only actually get enabled on the boot cpu much later once

	 * ftrace itself has been initialized.

	/*

	 * This needs to be done *last* (after the above udbg_printf() even)

	 *

	 * Right after we return from this function, we turn on the MMU

	 * which means the real-mode access trick that btext does will

	 * no longer work, it needs to switch to using a real MMU

	 * mapping. This call will ensure that it does

 CONFIG_PPC_EARLY_DEBUG_BOOTX */

 Mark interrupts disabled in PACA */

 Initialize the hash table or TLB handling */

 Perform any KUP setup that is per-cpu */

	/*

	 * At this point, we can let interrupts switch to virtual mode

	 * (the MMU has been setup), so adjust the MSR in the PACA to

	 * have IR and DR set.

 CONFIG_SMP */

		/*

		 * See comments in head_64.S -- not all platforms insert

		 * secondaries at __secondary_hold and wait at the spin

		 * loop.

	/*

	 * When book3e boots from kexec, the ePAPR spin table does

	 * not get used.

	/* All secondary cpus are spinning on a common spinloop, release them

	 * all now so they can start to spin on their individual paca

	 * spinloops. For non SMP kernels, the secondary cpus never get out

	 * of the common spinloop.

 And wait a bit for them to catch up */

 CONFIG_SMP || CONFIG_KEXEC_CORE */

/*

 * Initialize some remaining members of the ppc64_caches and systemcfg

 * structures

 * (at least until we get rid of them completely). This is mostly some

 * cache informations about the CPU that will be used by cache flush

 * routines and/or provided to userland

	/*

	 * OF is weird .. it represents fully associative caches

	 * as "1 way" which doesn't make much sense and doesn't

	 * leave room for direct mapped. We'll assume that 0

	 * in OF means direct mapped for that reason.

	/*

	 * All shipping POWER8 machines have a firmware bug that

	 * puts incorrect information in the device-tree. This will

	 * be (hopefully) fixed for future chips but for now hard

	 * code the values if we are running on one of these

 size    lsize   blk  sets */

	/*

	 * We're assuming *all* of the CPUs have the same

	 * d-cache and i-cache sizes... -Peter

		/*

		 * Try to find the L2 and L3 if any. Assume they are

		 * unified and use the D-side properties.

 For use by binfmt_elf */

/*

 * This returns the limit below which memory accesses to the linear

 * mapping are guarnateed not to cause an architectural exception (e.g.,

 * TLB or SLB miss fault).

 *

 * This is used to allocate PACAs and various interrupt stacks that

 * that are accessed early in interrupt handlers that must not cause

 * re-entrant interrupts.

 Freescale BookE bolts the entire linear mapping */

 XXX: BookE ppc64_rma_limit setup seems to disagree? */

 Other BookE, we assume the first GB is bolted */

 BookS radix, does not take faults on linear mapping */

 BookS hash, the first segment is bolted */

	/*

	 * Interrupt stacks must be in the first segment since we

	 * cannot afford to take SLB misses on them. They are not

	 * accessed in realmode.

/*

 * Stack space used when we detect a bad kernel stack pointer, and

 * early in SMP boots before relocation is enabled. Exclusive emergency

 * stack for machine checks.

	/*

	 * Emergency stacks must be under 256MB, we cannot afford to take

	 * SLB misses on them. The ABI also requires them to be 128-byte

	 * aligned.

	 *

	 * Since we use these as temporary stacks during secondary CPU

	 * bringup, machine check, system reset, and HMI, we need to get

	 * at them in real mode. This means they must also be within the RMO

	 * region.

	 *

	 * The IRQ stacks allocated elsewhere in this file are zeroed and

	 * initialized in kernel/irq.c. These are initialized here in order

	 * to have emergency stacks available as early as possible.

	/*

	 * Machine check on pseries calls rtas, but can't use the static

	 * rtas_args due to a machine check hitting while the lock is held.

	 * rtas args have to be under 4GB, so the machine check stack is

	 * limited to 4GB so args can be put on stack.

 emergency stack for NMI exception handling. */

 emergency stack for machine check exception handling. */

/**

 * pcpu_alloc_bootmem - NUMA friendly alloc_bootmem wrapper for percpu

 * @cpu: cpu to allocate for

 * @size: size allocation in bytes

 * @align: alignment

 *

 * Allocate @size bytes aligned at @align for cpu @cpu.  This wrapper

 * does the right thing for NUMA regardless of the current

 * configuration.

 *

 * RETURNS:

 * Pointer to the allocated area on success, NULL on failure.

	/*

	 * Linear mapping is one of 4K, 1M and 16M.  For 4K, no need

	 * to group units.  For larger mappings, use 1M atom which

	 * should be large enough to contain a number of units.

/*

 * The perf based hardlockup detector breaks PMU event based branches, so

 * disable it by default. Book3S has a soft-nmi hardlockup detector based

 * on the decrementer interrupt, so it does not suffer from this problem.

 *

 * It is likely to get false positives in KVM guests, so disable it there

 * by default too. PowerVM will not stop or arbitrarily oversubscribe

 * CPUs, but give a minimum regular allotment even with SPLPAR, so enable

 * the detector for non-KVM guests, assume PowerVM.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Copyright (C) 2001 Ben. Herrenschmidt (benh@kernel.crashing.org)

 *

 *  Modifications for ppc64:

 *      Copyright (C) 2003 Dave Engebretsen <engebret@us.ibm.com>

 for PTRRELOC on ARCH=ppc */

 The platform string corresponding to the real PVR */

/* NOTE:

 * Unlike ppc32, ppc64 will only call this once for the boot CPU, it's

 * the responsibility of the appropriate CPU save/restore functions to

 * eventually copy these settings over. Those save/restore aren't yet

 * part of the cputable though. That has to be fixed for both ppc32

 * and ppc64

 CONFIG_PPC32 */

 CONFIG_PPC64 */

 CONFIG_E500 */

/* This table only contains "desktop" CPUs, it need to be filled with embedded

 * ones as well...

 PPC970 */

 PPC970FX */

 PPC970MP DD1.0 - no DEEPNAP, use regular 970 init */

 PPC970MP */

 PPC970GX */

 Power5 GR */

 Power5++ */

 Power5 GS */

 POWER6 in P5+ mode; 2.04-compliant processor */

 Power6 */

 2.05-compliant processor, i.e. Power6 "architected" mode */

 2.06-compliant processor, i.e. Power7 "architected" mode */

 2.07-compliant processor, i.e. Power8 "architected" mode */

 3.00-compliant processor, i.e. Power9 "architected" mode */

 3.1-compliant processor, i.e. Power10 "architected" mode */

 Power7 */

 Power7+ */

 Power8E */

 Power8NVL */

 Power8 */

 Power9 DD2.0 */

 Power9 DD 2.1 */

 Power9 DD2.2 or later */

 Power10 */

 Cell Broadband Engine */

 PA Semi PA6T */

 default match */

 CONFIG_PPC_BOOK3S_64 */

 604 */

 604e */

 604r */

 604ev */

 740/750 (0x4202, don't support TAU ?) */

 750CX (80100 and 8010x?) */

 750CX (82201 and 82202) */

 750CXe (82214) */

 750CXe "Gekko" (83214) */

 750CL (and "Broadway") */

 745/755 */

 750FX rev 1.x */

 750FX rev 2.0 must disable HID0[DPM] */

 750FX (All revs except 2.0) */

 750GX */

 740/750 (L2CR bit need fixup for 740) */

 7400 rev 1.1 ? (no TAU) */

 7400 */

 7410 */

 7450 2.0 - no doze/nap */

 7450 2.1 */

 7450 2.3 and newer */

 7455 rev 1.x */

 7455 rev 2.0 */

 7455 others */

 7447/7457 Rev 1.0 */

 7447/7457 Rev 1.1 */

 7447/7457 Rev 1.2 and later */

 7447A */

 7448 */

 CONFIG_PPC_BOOK3S_604 */

 603 */

 603e */

 603ev */

 82xx (8240, 8245, 8260 are all 603e cores) */

 All G2_LE (603e core, plus some) have the same pvr */

 e300c1 (a 603e core, plus some) on 83xx */

 e300c2 (an e300c1 core, plus some, minus FPU) on 83xx */

 e300c3 (e300c1, plus one IU, half cache size) on 83xx */

 e300c4 (e300c1, plus one IU) */

 CONFIG_PPC_BOOK3S_603 */

 default match, we assume split I/D cache & TB (non-601)... */

 CONFIG_PPC_BOOK3S_604 */

 CONFIG_PPC_BOOK3S_32 */

 8xx */

		/* CPU_FTR_MAYBE_CAN_DOZE is possible,

 CONFIG_PPC_8xx */

 STB 04xxx */

 NP405L */

 NP4GS3 */

 NP405H */

 405GPr */

 STBx25xx */

 405LP */

 405EP */

 405EX Rev. A/B with Security */

 405EX Rev. C without Security */

 405EX Rev. C with Security */

 405EX Rev. D without Security */

 405EX Rev. D with Security */

 405EXr Rev. A/B without Security */

 405EXr Rev. C without Security */

 405EXr Rev. C with Security */

 405EXr Rev. D without Security */

 405EXr Rev. D with Security */

 405EZ */

 APM8018X */

 default match */

 CONFIG_40x */

 Use logical PVR for 440EP (logical pvr = pvr | 0x8) */

 Matches both physical and logical PVR for 440EP (logical pvr = pvr | 0x8) */

 Use logical PVR for 440EP (logical pvr = pvr | 0x8) */

 440GRX */

 Use logical PVR for 440EPx (logical pvr = pvr | 0x8) */

 440GP Rev. B */

 440GP Rev. C */

 440GX Rev. A */

 440GX Rev. B */

 440GX Rev. C */

 440GX Rev. F */

 440SP Rev. A */

 440SPe Rev. A */

 440SPe Rev. B */

 460EX */

 460EX Rev B */

 460GT */

 460GT Rev B */

 460SX */

 464 in APM821xx */

 default match */

 CONFIG_PPC_47x */

 476 DD2 core */

 476fpe */

 476 iss */

 476 others */

 default match */

 CONFIG_PPC_47x */

 CONFIG_44x */

 CONFIG_PPC32 */

 e500 */

 e500v2 */

 e500mc */

 CONFIG_PPC_E500MC */

 CONFIG_PPC32 */

 e5500 */

 e6500 */

 CONFIG_PPC_E500MC */

 default match */

 CONFIG_PPC32 */

 CONFIG_E500 */

	/*

	 * use memcpy() instead of *t = *s so that GCC replaces it

	 * by __memcpy() when KASAN is active

	/*

	 * Copy everything, then do fixups. Use memcpy() instead of *t = *s

	 * so that GCC replaces it by __memcpy() when KASAN is active

	/*

	 * If we are overriding a previous value derived from the real

	 * PVR with a new value obtained using a logical PVR value,

	 * don't modify the performance monitor fields.

		/*

		 * If we have passed through this logic once before and

		 * have pulled the default case because the real PVR was

		 * not found inside cpu_specs[], then we are possibly

		 * running in compatibility mode. In that case, let the

		 * oprofiler know which set of compatibility counters to

		 * pull from by making sure the oprofile_cpu_type string

		 * is set to that of compatibility mode. If the

		 * oprofile_cpu_type already has a value, then we are

		 * possibly overriding a real PVR with a logical one,

		 * and, in that case, keep the current value for

		 * oprofile_cpu_type. Futhermore, let's ensure that the

		 * fix for the PMAO bug is enabled on compatibility mode.

	/*

	 * Set the base platform string once; assumes

	 * we're called with real pvr first.

	/* ppc64 and booke expect identify_cpu to also call setup_cpu for

	 * that processor. I will consolidate that at a later time, for now,

	 * just use #ifdef. We also don't need to PTRRELOC the function

	 * pointer on ppc64 and booke as we are running at 0 in real mode

	 * on ppc64 and reloc_offset is always 0 on booke.

 CONFIG_PPC64 || CONFIG_BOOKE */

/*

 * Used by cpufeatures to get the name for CPUs with a PVR table.

 * If they don't hae a PVR table, cpufeatures gets the name from

 * cpu device-tree node.

