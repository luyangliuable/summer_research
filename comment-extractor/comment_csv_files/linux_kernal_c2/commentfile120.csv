 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2016-present, Facebook, Inc.
  All rights reserved.
 307s to avoid pathologically clashing with transaction commit 
 jiffies 
  Zstd Workspace Management
  Zstd workspaces have different memory requirements depending on the level.
  The zstd workspaces are managed by having individual lists for each level
  and a global lru.  Forward progress is maintained by protecting a max level
  workspace.
  Getting a workspace is done by using the bitmap to identify the levels that
  have available workspaces and scans up.  This lets us recycle higher level
  workspaces because of the monotonic memory guarantee.  A workspace's
  last_used is only updated if it is being used by the corresponding memory
  level.  Putting a workspace involves adding it back to the appropriate places
  and adding it back to the lru if necessary.
  A timer is used to reclaim workspaces if they have not been used for
  ZSTD_BTRFS_RECLAIM_JIFFIES.  This helps keep only active workspaces around.
  The upper bound is provided by the workqueue limit which is 2 (percpu limit).
  zstd_reclaim_timer_fn - reclaim timer
  @t: timer
  This scans the lru_list and attempts to reclaim any workspace that hasn't
  been used for ZSTD_BTRFS_RECLAIM_JIFFIES.
 workspace is in use 
  zstd_calc_ws_mem_sizes - calculate monotonic memory bounds
  It is possible based on the level configurations that a higher level
  workspace uses less memory than a lower level workspace.  In order to reuse
  workspaces, this must be made a monotonic relationship.  This precomputes
  the required memory for each level and enforces the monotonicity between
  level and memory required.
  zstd_find_workspace - find workspace
  @level: compression level
  This iterates over the set bits in the active_map beginning at the requested
  compression level.  This lets us utilize already allocated workspaces before
  allocating a new one.  If the workspace is of a larger size, it is used, but
  the place in the lru_list and last_used times are not updated.  This is to
  offer the opportunity to reclaim the workspace in favor of allocating an
  appropriately sized one in the future.
 keep its place if it's a lower level using this 
  zstd_get_workspace - zstd's get_workspace
  @level: compression level
  If @level is 0, then any compression level can be used.  Therefore, we begin
  scanning from 1.  We first scan through possible workspaces and then after
  attempt to allocate a new workspace.  If we fail to allocate one due to
  memory pressure, go to sleep waiting for the max level workspace to free up.
 level == 0 means we can use any workspace 
  zstd_put_workspace - zstd put_workspace
  @ws: list_head for the workspace
  When putting back a workspace, we only need to update the LRU if we are of
  the requested compression level.  Here is where we continue to protect the
  max level workspace or update last_used accordingly.  If the reclaim timer
  isn't set, it is also set here.  Only the max level workspace tries and wakes
  up waiting workspaces.
 A node is only taken off the lru if we are the corresponding level 
 Hide a max level workspace from reclaim 
 The current page to read 
 The current page to write to 
 Initialize the stream 
 map in the first page of input data 
 Allocate and map in the output buffer 
 Check to see if we are making it bigger 
 We've reached the end of our output range 
 Check if we need more output space 
 We've reached the end of the input 
 Check if we need more input 
 Cleanup 
 Check if we've hit the end of a frame 
 Check if the frame is over and we still need more input 
 ZSTD uses own workspace manager 
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Red Hat.  All rights reserved.
 lookup the xattr by name 
 if size is 0, that means we want the size of the attr 
 now get the data out of our dir_item 
	
	  The way things are packed into the leaf is like this
	  |struct btrfs_dir_item|name|data|
	  where name is the xattr name, so security.foo, and data is the
	  content of the xattr.  data_ptr points to the location in memory
	  where the data starts in the in memory leaf
	
	  For a replace we can't just do the insert blindly.
	  Do a lookup first (read-only btrfs_search_slot), and return if xattr
	  doesn't exist. If it exists, fall down below to the insertreplace
	  path - we can't race with a concurrent xattr delete, because the VFS
	  locks the inode's i_mutex before calling setxattr or removexattr.
		
		  We have an existing item in a leaf, split_leaf couldn't
		  expand it. That item might have or not a dir_item that
		  matches our target xattr, so lets check.
 logic error 
		
		  We're doing a replace, and it must be atomic, that is, at
		  any point in time we have either the old or the new xattr
		  value in the tree. We don't want readers (getxattr and
		  listxattrs) to miss a value, this is specially important
		  for ACLs.
 No other xattrs packed in the same leaf item. 
 There are other xattrs packed in the same item. 
		
		  Insert, and we had space for the xattr, so path->slots[0] is
		  where our xattr dir_item is and btrfs_insert_xattr_item()
		  filled it.
  @value: "" makes the attribute to empty, NULL removes it
		
		  1 unit for insertingupdatingdeleting the xattr
		  1 unit for the inode item update
		
		  This can happen when smack is enabled and a directory is being
		  created. It happens through d_instantiate_new(), which calls
		  smack_d_instantiate(), which in turn calls __vfs_setxattr() to
		  set the transmute xattr (XATTR_NAME_SMACKTRANSMUTE) on the
		  inode. We have already reserved space for the xattr and inode
		  update at btrfs_mkdir(), so just use the transaction handle.
		  We don't join or start a transaction, as that will reset the
		  block_rsv of the handle and trigger a warning for the start
		  case.
	
	  ok we want all objects associated with this id.
	  NOTE: we set key.offset = 0; because we want to start with the
	  first xattr that we find and walk forward
 search for our xattrs 
 this is where we start walking through the path 
			
			  if we've reached the last slot in this leaf we need
			  to go to the next leaf and reset everything
 check to make sure this item is what we want 
			
			  We are just looking for how big our buffer needs to
			  be.
	
	  We're holding a transaction handle, so use a NOFS memory allocation
	  context to avoid deadlock if reclaim happens.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
  btrfs_inode_lock - lock inode i_rwsem based on arguments passed
  ilock_flags can have the following bit set:
  BTRFS_ILOCK_SHARED - acquire a shared lock on the inode
  BTRFS_ILOCK_TRY - try to acquire the lock, if fails on first attempt
 		     return -EAGAIN
  BTRFS_ILOCK_MMAP - acquire a write lock on the i_mmap_lock
  btrfs_inode_unlock - unock inode i_rwsem
  ilock_flags should contain the same bits set as passed to btrfs_inode_lock()
  to decide whether the lock acquired is shared or exclusive.
  Cleanup all submitted ordered extents in specified range to handle errors
  from the btrfs_run_delalloc_range() callback.
  NOTE: caller must ensure that when an error happens, it can not call
  extent_clear_unlock_delalloc() to clear both the bits EXTENT_DO_ACCOUNTING
  and EXTENT_DELALLOC simultaneously, because that causes the reserved metadata
  to be released, which we want to happen only when finishing the ordered
  extent (btrfs_finish_ordered_io()).
		
		  For locked page, we will call end_extent_writepage() on it
		  in run_delalloc_range() for the error handling.  That
		  end_extent_writepage() function will call
		  btrfs_mark_ordered_io_finished() to clear page Ordered and
		  run the ordered extent accounting.
		 
		  Here we can't just clear the Ordered bit, or
		  btrfs_mark_ordered_io_finished() would skip the accounting
		  for the page range, and the ordered extent will never finish.
		
		  Here we just clear all Ordered bits for every page in the
		  range, then __endio_write_update_ordered() will handle
		  the ordered extent accounting for the range.
 The locked page covers the full range, nothing needs to be done 
	
	  In case this page belongs to the delalloc range being instantiated
	  then skip it, since the first page of a range is going to be
	  properly cleaned up by the caller of run_delalloc_range
  this does all the hard work for inserting an inline extent into
  the btree.  The caller should have done a btrfs_drop_extents so that
  no overlapping inline items exist in the btree
	
	  We align size to sectorsize for inline extents just for simplicity
	  sake.
	
	  we're an inline extent, so nobody can
	  extend the file past i_size without locking
	  a page we already have locked.
	 
	  We must do any isize and inode updates
	  before we unlock the pages.  Otherwise we
	  could end up racing with unlink.
  conditionally insert an inline extent into the file.  This
  does the checks required to make sure the data is small enough
  to fit as an inline extent.
	
	  Don't forget to free the reserved space, as for inlined extent
	  it won't count as data extent, free them directly here.
	  And at reserve time, it's always aligned to page size, so
	  just free one page here.
 -ENOMEM 
  Check if the inode has flags compatible with compression
  Check if the inode needs to be submitted to compression, based on mount
  options, defragmentation, properties or heuristics.
	
	  Special check for subpage.
	 
	  We lock the full page then run each delalloc range in the page, thus
	  for the following case, we will hit some subpage specific corner case:
	 
	  0		32K		64K
	  |	||	||
	 		\- A		\- B
	 
	  In above case, both range A and range B will try to unlock the full
	  page [0, 64K), causing the one finished later will have page
	  unlocked already, triggering various page lock requirement BUG_ON()s.
	 
	  So here we add an artificial limit that subpage compression can only
	  if the range is fully page aligned.
	 
	  In theory we only need to ensure the first page is fully covered, but
	  the tailing partial page will be locked until the full compression
	  finishes, delaying the write of other range.
	 
	  TODO: Make btrfs_run_delalloc_range() to lock all delalloc range
	  first to prevent any submitted async extent to unlock the full page.
	  By this, we can ensure for subpage case that only the last async_cow
	  will unlock the full page.
 force compress 
 defrag ioctl 
 bad compression ratios 
 If this is a small write inside eof, kick off a defrag 
  we create compressed extents in two phases.  The first
  phase compresses a range of pages that have already been
  locked (both pages and state bits are locked).
  This is done inside an ordered work queue, and the compression
  is spread across many cpus.  The actual IO submission is step
  two, and the ordered work queue takes care of making sure that
  happens in the same order things were put onto the queue by
  writepages and friends.
  If this code finds it can't get good compression, it puts an
  entry onto the work queue to write the uncompressed bytes.  This
  makes sure that both compressed inodes and uncompressed inodes
  are written in the same order that the flusher thread sent them
  down.
	
	  We need to save i_size before now because it could change in between
	  us evaluating the size and assigning it.  This is because we lock and
	  unlock the page in truncate and fallocate, and then modify the i_size
	  later on.
	 
	  The barriers are to emulate READ_ONCE, remove that once i_size_read
	  does that for us.
	
	  we don't want to send crud past the end of i_size through
	  compression, that's just a waste of CPU time.  So, if the
	  end of the file is before the start of our current
	  requested range of bytes, we bail out to the uncompressed
	  cleanup code that can deal with all of this.
	 
	  It isn't really the fastest way to fix things, but this is a
	  very uncommon corner.
	
	  Skip compression for a small file range(<=blocksize) that
	  isn't an inline extent, since it doesn't save disk space at all.
	
	  For subpage case, we require full page alignment for the sector
	  aligned range.
	  Thus we must also check against @actual_end, not just @end.
	
	  we do compression for mount -o compress and when the
	  inode has not been flagged as nocompress.  This flag can
	  change at any time if we discover bad compression ratios.
 just bail out to the uncompressed code 
		
		  we need to call clear_page_dirty_for_io on each
		  page in the range.  Otherwise applications with the file
		  mmap'd can wander in and change the page contents while
		  we are compressing them.
		 
		  If the compression fails for any reason, we set the pages
		  dirty again later on.
		 
		  Note that the remaining part is redirtied, the start pointer
		  has moved, the end is the original one.
 Compression level is applied here and only here 
			 zero the tail end of the last page, we might be
			  sending it down to disk
	
	  Check cow_file_range() for why we don't even try to create inline
	  extent for subpage case.
 lets try to make an inline extent 
			 we didn't compress the entire range, try
			  to make an uncompressed inline extent.
 try making a compressed inline extent 
			
			  inline extent creation worked or returned error,
			  we don't need to create any more async work items.
			  Unlock and free up our temp pages.
			 
			  We use DO_ACCOUNTING here because we need the
			  delalloc_release_metadata to be done _after_ we drop
			  our outstanding extent for clearing delalloc for this
			  range.
			
			  Ensure we only free the compressed pages if we have
			  them allocated, as we can still reach here with
			  inode_need_compress() == false.
		
		  we aren't doing an inline extent round the compressed size
		  up to a block size boundary so the allocator does sane
		  things
		
		  one last check to make sure the compression is really a
		  win, compare the page count read with the blocks on disk,
		  compression must free at least one sector size
			
			  The async work queues will take care of doing actual
			  allocation on disk for these compressed pages, and
			  will submit them to the elevator.
		
		  the compression code ran but failed to make things smaller,
		  free any pages it allocated and our page pointer array
 flag the file so we don't compress in the future 
	
	  No compression, but we still need to write the pages in the file
	  we've been given so far.  redirty the locked page if it corresponds
	  to our extent and set things up for the async work queue to run
	  cow_file_range to do the normal delalloc dance.
 unlocked later on in the async handlers 
	
	  Call cow_file_range() to run the delalloc range directly, since we
	  won't go to NOCOW or async path again.
	 
	  Also we call cow_file_range() with @unlock_page == 0, so that we
	  can directly submit them without interruption.
 Inline extent inserted, page gets unlocked and everything is done 
 All pages will be unlocked, including @locked_page 
	
	  If async_chunk->locked_page is in the async_extent range, we need to
	  handle it.
 We have fall back to uncompressed write 
		
		  Here we used to try again by going back to non-compressed
		  path for ENOSPC.  But we can't reserve space even for
		  compressed size, how could it work for uncompressed size
		  which requires larger size?  So here we directly go error
		  path.
 Here we're doing allocation and writeback of the compressed pages 
 len 
 orig_start 
 block_start 
 block_len 
 orig_block_len 
 ram_bytes 
 file_offset 
 disk_bytenr 
 num_bytes 
 disk_num_bytes 
 Clear dirty, set writeback and unlock the pages. 
 file_offset 
 num_bytes 
 disk_bytenr 
 compressed_len 
 compressed_pages 
  Phase two of compressed writeback.  This is the ordered portion of the code,
  which only gets called in the order the work was queued.  We walk all the
  async extents created by compress_file_range and send them down to the disk.
		
		  if block start isn't an actual block number then find the
		  first block in this inode and use that as a hint.  If that
		  block is also bogus then just don't worry about it.
  when extent_io.c finds a delayed allocation range in the file,
  the call backs end up in this code.  The basic idea is to
  allocate extents on disk for the range, and create ordered data structs
  in ram to track those extents.
  locked_page is the page that writepage had locked already.  We use
  it to make sure we don't do extra locks or unlocks.
  page_started is set to one if we unlock locked_page and do everything
  required to start IO on it.  It may be clean and already done with
  IO when we return.
	
	  Due to the page size limit, for subpage we can only trigger the
	  writeback for the dirty sectors of page, that means data writeback
	  is doing more writeback than what we want.
	 
	  This is especially unexpected for some call sites like fallocate,
	  where we only increase i_size after everything is done.
	  This means we can trigger inline extent even if we didn't want to.
	  So here we skip inline extent creation completely.
 lets try to make an inline extent 
			
			  We use DO_ACCOUNTING here because we need the
			  delalloc_release_metadata to be run _after_ we drop
			  our outstanding extent for clearing delalloc for this
			  range.
			
			  locked_page is locked by the caller of
			  writepage_delalloc(), not locked by
			  __process_pages_contig().
			 
			  We can't let __process_pages_contig() to unlock it,
			  as it doesn't have any subpage::writers recorded.
			 
			  Here we manually unlock the page, since the caller
			  can't use page_started to determine if it's an
			  inline extent or a compressed extent.
	
	  Relocation relies on the relocated extents to have exactly the same
	  size as the original extents. Normally writeback for relocation data
	  extents follows a NOCOW path because relocation preallocates the
	  extents. However, due to an operation such as scrub turning a block
	  group to RO mode, it may fallback to COW mode, so we must make sure
	  an extent allocated during COW has exactly the requested size and can
	  not be split into smaller extents, otherwise relocation breaks and
	  fails during the stage where it updates the bytenr of file extent
	  items.
 len 
 orig_start 
 block_start 
 block_len 
 orig_block_len 
 ram_bytes 
 compress_type 
 type );
			
			  Only drop cache here, and process as normal.
			 
			  We must not allow extent_clear_unlock_delalloc()
			  at out_unlock label to free meta of this ordered
			  extent, as its meta should be freed by
			  btrfs_finish_ordered_io().
			 
			  So we must continue until @start is increased to
			  skip current ordered extent.
		
		  We're not doing compressed IO, don't unlock the first page
		  (which the caller expects to stay locked), don't clear any
		  dirty bits and don't set any writeback bits
		 
		  Do set the Ordered (Private2) bit so we know this page was
		  properly setup for writepage.
		
		  btrfs_reloc_clone_csums() error, since start is increased
		  extent_clear_unlock_delalloc() at out_unlock label won't
		  free metadata of current ordered extent, we're OK to exit.
	
	  If we reserved an extent for our delalloc range (or a subrange) and
	  failed to create the respective ordered extent, then it means that
	  when we reserved the extent we decremented the extent's size from
	  the data space_info's bytes_may_use counter and incremented the
	  space_info's bytes_reserved counter by the same amount. We must make
	  sure extent_clear_unlock_delalloc() does not try to decrement again
	  the data space_info's bytes_may_use counter, therefore we do not pass
	  it the flag EXTENT_CLEAR_DATA_RESV.
  work queue call back to started compression on a file and pages
  work queue call back to submit previously compressed pages
	
	  ->inode could be NULL if async_chunk_start has failed to compress,
	  in which case we don't have anything to submit, yet we need to
	  always adjust ->async_delalloc_pages as its paired with the init
	  happening in cow_file_range_async
 atomic_sub_return implies a barrier 
		
		  igrab is called higher up in the call chain, take only the
		  lightweight reference for the callback lifetime
		
		  The locked_page comes all the way from writepage and its
		  the original page we were actually given.  As we spread
		  this large delalloc region across multiple async_chunk
		  structs, only the first struct needs a pointer to locked_page
		 
		  This way we don't need racey decisions about who is supposed
		  to unlock it.
			
			  Depending on the compressibility, the pages might or
			  might not go through async.  We want all of them to
			  be accounted against wbc once.  Let's do it here
			  before the paths diverge.  wbc accounting is used
			  only for foreign writeback detection and doesn't
			  need full accuracy.  Just account the whole thing
			  against the first page.
	
	  If EXTENT_NORESERVE is set it means that when the buffered write was
	  made we had not enough available data space and therefore we did not
	  reserve data space for it, since we though we could do NOCOW for the
	  respective file range (either there is prealloc extent or the inode
	  has the NOCOW bit set).
	 
	  However when we need to fallback to COW mode (because for example the
	  block group for the corresponding extent was turned to RO mode by a
	  scrub or relocation) we need to do the following:
	 
	  1) We increment the bytes_may_use counter of the data space info.
	     If COW succeeds, it allocates a new data extent and after doing
	     that it decrements the space info's bytes_may_use counter and
	     increments its bytes_reserved counter by the same amount (we do
	     this at btrfs_add_reserved_bytes()). So we need to increment the
	     bytes_may_use counter to compensate (when space is reserved at
	     buffered write time, the bytes_may_use counter is incremented);
	 
	  2) We clear the EXTENT_NORESERVE bit from the range. We do this so
	     that if the COW path fails for any reason, it decrements (through
	     extent_clear_unlock_delalloc()) the bytes_may_use counter of the
	     data space info, which we incremented in the step above.
	 
	  If we need to fallback to cow and the inode corresponds to a free
	  space cache inode or an inode of the data relocation tree, we must
	  also increment bytes_may_use of the data space_info for the same
	  reason. Space caches and relocated data extents always get a prealloc
	  extent for them, however scrub or balance may have set the block
	  group that contains that extent to RO mode and therefore force COW
	  when starting writeback.
  when nowcow writeback call back.  This checks for snapshots or COW copies
  of the extents that exist in the file, and COWs the file as required.
  If no cow copies or snapshots exist, we write directly to the existing
  blocks on disk
		
		  If there is no extent for our range when doing the initial
		  search, then go back to the previous slot as it will be the
		  one containing the search offset
 Go to next leaf if we have exhausted the current one 
 Didn't find anything for our INO 
		
		  Keep searching until we find an EXTENT_ITEM or there are no
		  more extents for this inode
 Found key is not EXTENT_DATA_KEY or starts after req range 
		
		  If the found extent starts after requested offset, then
		  adjust extent_end to be right before this extent begins
		
		  Found extent which begins before our range and potentially
		  intersect it
			
			  If the extent we got ends before our current offset,
			  skip to the next extent.
 Skip holes 
 Skip compressedencryptedencoded extents 
			
			  If extent is created before the last volume's snapshot
			  this implies the extent is shared, hence we can't do
			  nocow. This is the same check as in
			  btrfs_cross_ref_exist but without calling
			  btrfs_search_slot.
			
			  The following checks can be expensive, as they need to
			  take other locks and do btree or rbtree searches, so
			  release the path to avoid blocking other tasks for too
			  long.
				
				  ret could be -EIO if the above fails to read
				  metadata.
			
			  If there are pending snapshots for this root, we
			  fall into common COW way
			
			  force cow if csum exists in the range.
			  this ensure that csum for a given extent are
			  either valid or do not exist.
				
				  ret could be -EIO if the above fails to read
				  metadata.
 If the extent's block group is RO, we must COW 
 Skip extents outside of our requested range 
 If this triggers then we have a memory corruption 
		
		  If nocow is false then record the beginning of the range
		  that needs to be COWed
		
		  COW range from cow_start to found_key.offset - 1. As the key
		  will contain the beginning of the first extent that can be
		  NOCOW, following one which needs to be COW'ed
 block_start 
 block_len 
 orig_block_len 
			
			  Error handled later, as we must prevent
			  extent_clear_unlock_delalloc() in error handler
			  from freeing metadata of created ordered extent.
		
		  btrfs_reloc_clone_csums() error, now we're OK to call error
		  handler, as metadata for created ordered extent will only
		  be freed by btrfs_finish_ordered_io().
  Function to process delayed allocation (create CoW) for ranges which are
  being touched for the first time.
	
	  The range must cover part of the @locked_page, or the returned
	  @page_started can confuse the caller.
		
		  Normally on a zoned device we're only doing COW writes, but
		  in case of relocation on a zoned filesystem we have taken
		  precaution, that we're only writing sequentially. It's safe
		  to use run_delalloc_nocow() here, like for  regular
		  preallocated inodes.
 not delalloc, ignore it 
		
		  See the explanation in btrfs_merge_delalloc_extent, the same
		  applies here, just in reverse.
  Handle merged delayed allocation extents so we can keep track of new extents
  that are just merged onto old extents, such as when we are doing sequential
  writes, so we can properly account for the metadata space we'll need.
 not delalloc, ignore it 
 we're not bigger than the max, unreserve the space and go 
	
	  We have to add up either side to figure out how many extents were
	  accounted for before we merged into one big extent.  If the number of
	  extents we accounted for is <= the amount we need for the new range
	  then we can return, otherwise drop.  Think of it like this
	 
	  [ 4k][MAX_SIZE]
	 
	  So we've grown the extent by a MAX_SIZE extent, this would mean we
	  need 2 outstanding extents, on one side we have 1 and the other side
	  we have 1 so they are == and we can return.  But in this case
	 
	  [MAX_SIZE+4k][MAX_SIZE+4k]
	 
	  Each range on their own accounts for 2 extents, but merged together
	  they are only 3 extents worth of accounting, so we need to drop in
	  this case.
  Properly track delayed allocation bytes in the inode and to maintain the
  list of inodes that have pending delalloc work to be done.
	
	  set_bit and clear bit hooks normally require _irqsaverestore
	  but in this case, we are only testing for the DELALLOC
	  bit, which is only set or cleared with irqs on
 For sanity tests 
  Once a range is no longer delalloc this function ensures that proper
  accounting happens.
	
	  set_bit and clear bit hooks normally require _irqsaverestore
	  but in this case, we are only testing for the DELALLOC
	  bit, which is only set or cleared with irqs on
		
		  We don't reserve metadata space for space cache inodes so we
		  don't need to call delalloc_release_metadata if there is an
		  error.
 For sanity tests. 
  in order to insert checksums into the metadata in large chunks,
  we wait until bio submission time.   All the pages in the bio are
  checksummed and sums are attached onto the ordered extent record.
  At IO completion time the cums attached on the ordered extent record
  are inserted into the btree
  Split an extent_map at [start, start + len]
  This function is intended to be used only for extract_ordered_extent().
 Sanity check 
 First, replace the em with a new extent_map starting from  em->start 
	
	  Now we only have an extent_map at:
	      [em->start, em->start + pre] if pre != 0
	      [em->start, em->start + em->len - post] if pre == 0
 Insert the middle extent_map 
 Once for us 
 Once for the tree 
 No need to split 
 We cannot split once end_bio'd ordered extent 
 We cannot split a compressed ordered extent 
 bio must be in one ordered extent 
 Checksum list should be empty 
  extent_io.c submission hook. This does the right thing for csum calculation
  on write, or reading the csums from the tree before a read.
  Rules about asyncsync submit,
  a) read:				sync submit
  b) write without checksum:		sync submit
  c) write with checksum:
     c-1) if bio is issued by fsync:	sync submit
          (sync_writers != 0)
     c-2) if root is reloc root:	sync submit
          (only in case of buffered IO)
     c-3) otherwise:			async submit
			
			  Lookup bio sums does extra checks around whether we
			  need to csum or not, which is why we ignore skip_sum
			  here.
 csum items have already been cloned 
 we're doing a write, do the async checksumming 
  given a list of ordered sums record them in the inode.  This happens
  at IO completion time based on sums calculated at bio submission time.
		
		  There can't be any extents following eof in this case so just
		  set the delalloc new bit for the range directly.
 see btrfs_writepage_start_hook for details on why this is required 
	
	  This is similar to page_mkwrite, we need to reserve the space before
	  we take the page lock.
	
	  Before we queued this fixup, we took a reference on the page.
	  page->mapping may go NULL, but it shouldn't be moved to a different
	  address space.
		
		  Unfortunately this is a little tricky, either
		 
		  1) We got here and our page had already been dealt with and
		     we reserved our space, thus ret == 0, so we need to just
		     drop our space reservation and bail.  This can happen the
		     first time we come into the fixup worker, or could happen
		     while waiting for the ordered extent.
		  2) Our page was already dealt with, but we happened to get an
		     ENOSPC above from the btrfs_delalloc_reserve_space.  In
		     this case we obviously don't have anything to release, but
		     because the page was already dealt with we don't want to
		     mark the page with an error, so make sure we're resetting
		     ret to 0.  This is why we have this check _before_ the ret
		     check, because we do not want to have a surprise ENOSPC
		     when the page was already properly dealt with.
	
	  We can't mess with the page state unless it is locked, so now that
	  it is locked bail if we failed to make our space reservation.
 already ordered? We're done 
	
	  Everything went as planned, we're now the owner of a dirty page with
	  delayed allocation bits set and space reserved for our COW
	  destination.
	 
	  The page was dirty when we started, nothing should have cleaned it.
		
		  We hit ENOSPC or other errors.  Update the mapping and page
		  to reflect the errors and clean the page.
	
	  As a precaution, do a delayed iput in case it would be the last iput
	  that could need flushing space. Recursing back to fixup worker would
	  deadlock.
  There are a few paths in the higher layers of the kernel that directly
  set the page dirty bit without asking the filesystem if it is a
  good idea.  This causes problems because we want to make sure COW
  properly happens and the data=ordered rules are followed.
  In our case any range that doesn't have the ORDERED bit set
  hasn't been properly setup for IO.  We kick off an async process
  to fix it up.  The async helper will wait for ordered extents, set
  the delalloc bit and make it safe to write the page.
 This page has ordered extent covering it already 
	
	  PageChecked is set below when we create a fixup worker for this page,
	  don't try to create another one if we're already PageChecked()
	 
	  The extent_io writepage code will redirty the page if we send back
	  EAGAIN.
	
	  We are already holding a reference to this inode from
	  write_cache_pages.  We need to hold it because the space reservation
	  takes place outside of the page lock, and we can't trust
	  page->mapping outside of the page lock.
	
	  we may be replacing one extent in the tree with another.
	  The new extent is pinned in the extent map, and we don't want
	  to drop it from the cache until it is completely in the btree.
	 
	  So, tell btrfs_drop_extents to leave this extent in the cache.
	  the caller is expected to unpin it and allow it to be merged
	  with the others.
	
	  If we dropped an inline extent here, we know the range where it is
	  was not marked with the EXTENT_DELALLOC_NEW bit, so we update the
	  number of bytes only for that range containing the inline extent.
	  The remaining of the range will be processed when clearning the
	  EXTENT_DELALLOC_BIT bit through the ordered extent completion.
 Encryption and other encoding is reserved and all 0 
	
	  For delalloc, when completing an ordered extent we update the inode's
	  bytes when clearing the range in the inode's io tree, so pass false
	  as the argument 'update_inode_bytes' to insert_reserved_file_extent(),
	  except if the ordered extent was truncated.
  As ordered data IO finishes, this gets called so we can finish
  an ordered extent if the range of bytes in the file it covers are
  fully written.
 A valid bdev implies a write on a sequential zone 
 Truncated the entire extent, don't bother adding 
 Logic error 
 -ENOMEM or corruption 
	
	  If this is a new delalloc range, clear its new delalloc flag to
	  update the inode's number of bytes. This needs to be done first
	  before updating the inode item.
 -ENOMEM or corruption 
		
		  If we failed to finish this ordered extent for any reason we
		  need to make sure BTRFS_ORDERED_IOERR is set on the ordered
		  extent, and mark the inode with the error if it wasn't
		  already set.  Any error during writeback would have already
		  set the mapping error, so we need to set it if we're the ones
		  marking this ordered extent as failed.
 Drop the cache for the part of the extent we didn't write. 
		
		  If the ordered extent had an IOERR or something else went
		  wrong we need to return the space for this ordered extent
		  back to the allocator.  We only free the extent in the
		  truncated case if we didn't write out the extent at all.
		 
		  If we made it past insert_reserved_file_extent before we
		  errored out then we don't need to do this as the accounting
		  has already been done.
			
			  Discard the range before returning it back to the
			  free space pool
	
	  This needs to be done to make sure anybody waiting knows we are done
	  updating everything for this ordered extent.
 once for us 
 once for the tree 
  check_data_csum - verify checksum of one sector of uncompressed data
  @inode:	inode
  @io_bio:	btrfs_io_bio which contains the csum
  @bio_offset:	offset to the beginning of the bio (in bytes)
  @page:	page where is the data to be verified
  @pgoff:	offset inside the page
  @start:	logical offset in the file
  The length of such check is always one sector size.
  When reads are done, we need to check csums to verify the data is correct.
  if there's a match, we allow the bio to finish.  If not, the code in
  extent_io.c will try to find good copies for us.
  @bio_offset:	offset to the beginning of the bio (in bytes)
  @start:	file offset of the range start
  @end:	file offset of the range end (inclusive)
  Return a bitmap where bit set means a csum mismatch, and bit not set means
  csum match.
	
	  This only happens for NODATASUM or compressed read.
	  Normally this should be covered by above check for compressed read
	  or the next check for NODATASUM.  Just do a quicker exit here.
 Skip the range without csum for data reloc inode 
  btrfs_add_delayed_iput - perform a delayed iput on @inode
  @inode: The inode we want to perform iput on
  This function uses the generic vfs_inode::i_count to track whether we should
  just decrement it (in case it's > 1) or if this is the last iput then link
  the inode to the delayed iput machinery. Delayed iputs are processed at
  transaction commit timesuperblock commitcleaner kthread.
  Wait for flushing all delayed iputs
  @fs_info:  the filesystem
  This will wait on any delayed iputs that are currently running with KILLABLE
  set.  Once they are all done running we will return, unless we are killed in
  which case we return EINTR. This helps in user operations like fallocate etc
  that might get blocked on the iputs.
  Return EINTR if we were killed, 0 if nothing's pending
  This creates an orphan entry for the given inode in case something goes wrong
  in the middle of an unlink.
  We have done the delete so we can go ahead and remove the orphan item for
  this particular inode.
  this cleans up any orphans that may be left on the list from the last use
  of this root.
		
		  if ret == 0 means we found what we were searching for, which
		  is weird, but possible, so only screw with path if we didn't
		  find the key and see if we have stuff that matches
 pull out the item 
 make sure the item matches what we want 
 release the path since we're done with it 
		
		  this is where we are basically btrfs_lookup, without the
		  crossing root thing.  we store the inode number in the
		  offset of the orphan item.
			
			  This is an orphan in the tree root. Currently these
			  could come from 2 sources:
			   a) a root (snapshotsubvolume) deletion in progress
			   b) a free space cache inode
			  We need to distinguish those two, as the orphan item
			  for a root must not get deleted before the deletion
			  of the snapshotsubvolume's tree completes.
			 
			  btrfs_find_orphan_roots() ran before us, which has
			  found all deleted roots and loaded them into
			  fs_info->fs_roots_radix. So here we can find if an
			  orphan item corresponds to a deleted root by looking
			  up the root from that radix tree.
 prevent this orphan from being found again 
		
		  If we have an inode with links, there are a couple of
		  possibilities:
		 
		  1. We were halfway through creating fsverity metadata for the
		  file. In that case, the orphan item represents incomplete
		  fsverity metadata which must be cleaned up with
		  btrfs_drop_verity_items and deleting the orphan item.
		  2. Old kernels (before v3.12) used to create an
		  orphan item for truncate indicating that there were possibly
		  extent items past i_size that needed to be deleted. In v3.12,
		  truncate was changed to update i_size in sync with the extent
		  items, but the (useless) orphan item was still created. Since
		  v4.18, we don't create the orphan item for truncate at all.
		 
		  So, this item could mean that we need to do a truncate, but
		  only if this filesystem was last used on a pre-v3.12 kernel
		  and was not cleanly unmounted. The odds of that are quite
		  slim, and it's a pain to do the truncate now, so just delete
		  the orphan item.
		 
		  It's also possible that this orphan item was supposed to be
		  deleted but wasn't. The inode number may have been reused,
		  but either way, we can delete the orphan item.
 this will do delete_inode and everything for us 
 release the path since we're done with it 
  very simple check to peek ahead in the leaf looking for xattrs.  If we
  don't find any xattrs, we know there can't be any acls.
  slot is the slot the inode is in, objectid is the objectid of the inode
 we found a different objectid, there must not be acls 
 we found an xattr, assume we've got an acl 
		
		  we found a key greater than an xattr key, there can't
		  be any acls later on
		
		  it goes inode, inode backrefs, xattrs, extents,
		  so if there are a ton of hard links to an inode there can
		  be a lot of backrefs.  Don't waste time searching too hard,
		  this is just an optimization
	 we hit the end of the leaf before we found an xattr or
	  something larger than an xattr.  We have to assume the inode
	  has acls
  read an inode from the btree into the in-memory inode
	
	  If we were modified in the current generation and evicted from memory
	  and then re-read we need to do a full sync since we don't have any
	  idea about which extents were modified before we were evicted from
	  cache.
	 
	  This is required for both inode re-read from disk and delayed inode
	  in delayed_nodes_tree.
	
	  We don't persist the id of the transaction where an unlink operation
	  against the inode was last made. So here we assume the inode might
	  have been evicted, and therefore the exact value of last_unlink_trans
	  lost, and set it to last_trans to avoid metadata inconsistencies
	  between the inode and its parent if the inode is fsync'ed and the log
	  replayed. For example, in the scenario:
	 
	  touch mydirfoo
	  ln mydirfoo mydirbar
	  sync
	  unlink mydirbar
	  echo 2 > procsysvmdrop_caches   # evicts inode
	  xfs_io -c fsync mydirfoo
	  <power failure>
	  mount fs, triggers fsync log replay
	 
	  We must make sure that when we fsync our inode foo we also log its
	  parent inode, otherwise after log replay the parent still has the
	  dentry with the "bar" name but our inode foo has a link count of 1
	  and doesn't have an inode ref with the name "bar" anymore.
	 
	  Setting last_unlink_trans to last_trans is a pessimistic approach,
	  but it guarantees correctness at the expense of occasional full
	  transaction commits on fsync if our inode is a directory, or if our
	  inode is not a directory, logging its parent unnecessarily.
	
	  Same logic as for last_unlink_trans. We don't persist the generation
	  of the last transaction where this inode was used for a reflink
	  operation, so after eviction and reloading the inode we must be
	  pessimistic and assume the last transaction that modified the inode.
	
	  try to precache a NULL acl entry for files that don't have
	  any xattrs or acls
  given a leaf and an inode, copy the inode fields into the leaf
  copy everything in the in-memory inode into the btree.
  copy everything in the in-memory inode into the btree.
	
	  If the inode is a free space inode, we can deadlock during commit
	  if we put it into the delayed code.
	 
	  The data relocation inode should also be directly updated
	  without delay
  unlink helper that gets used here in inode.c and in the tree logging
  recovery code.  It remove a link in a directory with a given name, and
  also drops the back refs in the inode to the directory
	
	  If we don't have dir index, we have to get it by looking up
	  the inode ref, since we get the inode ref, remove it directly,
	  it is unnecessary to do delayed deletion.
	 
	  But if we have dir index, needn't search inode ref to get it.
	  Since the inode ref is close to the inode item, it is better
	  that we delay to delete it, and just do this deletion when
	  we update the inode item.
	
	  If we have a pending delayed iput we could end up with the final iput
	  being run in btrfs-cleaner context.  If we have enough of these built
	  up we can end up burning a lot of time in btrfs-cleaner without any
	  way to throttle the unlinks.  Since we're currently holding a ref on
	  the inode we can run the delayed iput here without any issues as the
	  final iput won't be done until after we drop the ref we're currently
	  holding.
  helper to start transaction for unlink and rmdir.
  unlink and rmdir are special in btrfs, they do not always free space, so
  if we cannot make our reservations the normal way try and see if there is
  plenty of slack room in the global reserve to migrate, otherwise we cannot
  allow the unlink to occur.
	
	  1 for the possible orphan item
	  1 for the dir item
	  1 for the dir index
	  1 for the inode ref
	  1 for the inode
	
	  This is a placeholder inode for a subvolume we didn't have a
	  reference to at the time of the snapshot creation.  In the meantime
	  we could have renamed the real subvol link into our snapshot, so
	  depending on btrfs_del_root_ref to return -ENOENT here is incorrect.
	  Instead simply lookup the dir_index_item for this entry so we can
	  remove it.  Otherwise we know we have a ref to the root and we can
	  call btrfs_del_root_ref, and it _shouldn't_ fail.
  Helper to check if the subvolume references other subvolumes or if it's
  default.
 Make sure this root isn't set as the default subvol 
 Delete all dentries for inodes belonging to the root 
			
			  btrfs_drop_inode will have it removed from the inode
			  cache when its usage count hits zero.
	
	  Don't allow to delete a subvolume with send in progress. This is
	  inside the inode lock so the error handling that has to drop the bit
	  again is not run concurrently.
	
	  One for dir inode,
	  two for dir entries,
	  two for root refbackref.
 now the directory is empty 
		
		  Propagate the last_unlink_trans value of the deleted dir to
		  its parent directory. This is to prevent an unrecoverable
		  log tree in the case we do something like this:
		  1) create dir foo
		  2) create snapshot under dir foo
		  3) delete the snapshot
		  4) rmdir foo
		  5) mkdir foo
		  6) fsync foo or some file inside foo
  Return this if we need to call truncate_block for the last bit of the
  truncate.
  Remove inode items from a given root.
  @trans:		A transaction handle.
  @root:		The root from which to remove items.
  @inode:		The inode whose items we want to remove.
  @new_size:		The new i_size for the inode. This is only applicable when
 			@min_type is BTRFS_EXTENT_DATA_KEY, must be 0 otherwise.
  @min_type:		The minimum key type to remove. All keys with a type
 			greater than this value are removed and all keys with
 			this type are removed only if their offset is >= @new_size.
  @extents_found:	Output parameter that will contain the number of file
 			extent items that were removed or adjusted to the new
 			inode i_size. The caller is responsible for initializing
 			the counter. Also, it can be NULL if the caller does not
 			need this counter.
  Remove all keys associated with the inode from the given root that have a key
  with a type greater than or equals to @min_type. When @min_type has a value of
  BTRFS_EXTENT_DATA_KEY, only remove file extent items that have an offset value
  greater than or equals to @new_size. If a file extent item that starts before
  @new_size and ends after it is found, its length is adjusted.
  Returns: 0 on success, < 0 on error and NEED_TRUNCATE_BLOCK when @min_type is
  BTRFS_EXTENT_DATA_KEY and the caller must truncate the last block.
	
	  For non-free space inodes and non-shareable roots, we want to back
	  off from time to time.  This means all inodes in subvolume roots,
	  reloc roots, and data reloc roots.
		
		  We want to drop from the next block forward in case this
		  new size is not block aligned since we will be keeping the
		  last block of the extent just the way it is.
	
	  This function is also used to drop the items in the log tree before
	  we relog the inode, so if root != BTRFS_I(inode)->root, it means
	  it is used to drop the logged items. So we shouldn't kill the delayed
	  items.
	
	  with a 16K leaf size and 128MB extents, you can actually queue
	  up a huge file in a single leaf.  Most of the time that
	  bytes_deleted is > 0, it will be huge by the time we get here
		 there are no items in the tree for us to truncate, we're
		  done
 FIXME, shrink the extent if the ref count is only 1 
 FIXME blocksize != 4096 
			
			  we can't truncate inline items that have had
			  special encodings
				
				  We have to bail so the last_size is set to
				  just before this extent.
				
				  Inline extents are special, we just treat
				  them as a full sector worth in the file
				  extent tree just for simplicity sake.
		
		  We use btrfs_truncate_inode_items() to clean up log trees for
		  multiple fsyncs, and in this case we don't want to clear the
		  file extent range because it's just the log.
 no pending yet, add ourselves 
 hop on the pending chunk 
			
			  We can generate a lot of delayed refs, so we need to
			  throttle every once and a while and make sure we're
			  adding enough space to keep up with the work we are
			  generating.  Since we hold a transaction here we
			  can't flush, and we don't want to FLUSH_LIMIT because
			  we could have generated too many delayed refs to
			  actually allocate, so just bail if we're short and
			  let the normal reservation dance happen higher up.
  btrfs_truncate_block - read, zero a chunk and write a block
  @inode - inode that we're zeroing
  @from - the offset to start zeroing
  @len - the length to zero, 0 to zero the entire range respective to the
 	offset
  @front - zero up to the offset instead of from the offset on
  This will find the block for the "from" offset and cow the block and zero the
  part we want to zero.  This is used with truncate and hole punching.
 For nocow case, no need to reserve data space 
	
	  If NO_HOLES is enabled, we don't need to do anything.
	  Later, up in the call chain, either btrfs_set_inode_last_sub_trans()
	  or btrfs_update_inode() will be called, which guarantee that the next
	  fsync will know this inode was changed and needs to be logged.
	
	  1 - for the one we're dropping
	  1 - for the one we're adding
	  1 - for updating the inode.
  This function puts in dummy file extents for the area we're creating a hole
  for.  So if we are truncating this file to a larger size we need to insert
  these file extents so that btrfs_get_extent will return a EXTENT_MAP_HOLE for
  the range between oldsize and size
	
	  If our size started in the middle of a block we need to zero out the
	  rest of the block before we expand the i_size, otherwise we could
	  expose stale data.
	
	  The regular truncate() case without ATTR_CTIME and ATTR_MTIME is a
	  special case where we need to update the times despite not having
	  these flags set.  For all other operations the VFS set these flags
	  explicitly if it wants a timestamp update.
		
		  Don't do an expanding truncate while snapshotting is ongoing.
		  This is to ensure the snapshot captures a fully consistent
		  state of this file - if the snapshot captures this expanding
		  truncation, it must capture all writes that happened before
		  this truncation.
		
		  We're truncating a file that used to have good data down to
		  zero. Make sure any new writes to the file get on disk
		  on close.
			
			  Truncate failed, so fix up the in-memory size. We
			  adjusted disk_i_size down as we removed extents, so
			  wait for disk_i_size to be stable and then update the
			  in-memory size to match.
  While truncating the inode pages during eviction, we get the VFS calling
  btrfs_invalidatepage() against each page of the inode. This is slow because
  the calls to btrfs_invalidatepage() result in a huge amount of calls to
  lock_extent_bits() and clear_extent_bit(), which keep merging and splitting
  extent_state structures over and over, wasting lots of time.
  Therefore if the inode is being evicted, let btrfs_invalidatepage() skip all
  those expensive operations on a per page basis and do only the ordered io
  finishing, while we release here the extent_map and extent_state structures,
  without the excessive merging and splitting.
	
	  Keep looping until we have no more ranges in the io tree.
	  We can have ongoing bios started by readahead that have
	  their endio callback (extent_io.c:end_bio_extent_readpage)
	  still in progress (unlocked the pages in the bio but did not yet
	  unlocked the ranges in the io tree). Therefore this means some
	  ranges can still be locked and eviction started because before
	  submitting those bios, which are executed by a separate task (work
	  queue kthread), inode references (inode->i_count) were not taken
	  (which would be dropped in the end io callback of each bio).
	  Therefore here we effectively end up waiting for those bios and
	  anyone else holding locked ranges without having bumped the inode's
	  reference count - if we don't do it, when they access the inode's
	  io_tree to unlock a range it may be too late, leading to an
	  use-after-free issue.
		
		  If still has DELALLOC flag, the extent didn't reach disk,
		  and its reserved space won't be freed by delayed_ref.
		  So we need to free its reserved space here.
		  (Refer to comment in btrfs_invalidatepage, case 2)
		 
		  Note, end is the bytenr of last byte, so we need + 1 here.
	
	  Eviction should be taking place at some place safe because of our
	  delayed iputs.  However the normal flushing code will run delayed
	  iputs, so we cannot use FLUSH_ALL otherwise we'll deadlock.
	 
	  We reserve the delayed_refs_extra here again because we can't use
	  btrfs_start_transaction(root, 0) for the same deadlocky reason as
	  above.  We reserve our extra bit here because we generate a ton of
	  delayed refs activity by truncating.
	 
	  If we cannot make our reservation we'll attempt to steal from the
	  global reserve, because we really want to be able to free up space.
		
		  Try to steal from the global reserve if there is space for
		  it.
	
	  Errors here aren't a big deal, it just means we leave orphan items in
	  the tree. They will be cleaned up on the next mount. If the inode
	  number gets reused, cleanup deletes the orphan item without doing
	  anything, and unlink reuses the existing orphan item.
	 
	  If it turns out that we are dropping too many of these, we might want
	  to add a mechanism for retrying these after a commit.
	
	  If we didn't successfully delete, the orphan item will still be in
	  the tree and we'll retry on the next mount. Again, we might also want
	  to retry these periodically in the future.
  Return the key found in the dir entry in the location pointer, fill @type
  with BTRFS_FT_, and return 0.
  If no dir entries were found, returns -ENOENT.
  If found a corrupted location in dir entry, returns -EUCLEAN.
  when we hit a tree root in a directory, the btrfs part of the inode
  needs to be changed to reflect the root directory of the tree root.  This
  is kind of like crossing a mount point.
  Get an inode object given its inode number and corresponding root.
  Path can be preallocated to prevent recursing back to iget through
  allocator. NULL is also valid but may require an additional allocation
  later.
			
			  ret > 0 can come from btrfs_search_slot called by
			  btrfs_read_locked_inode, this means the inode item
			  was not found.
	
	  We only need lookup, the rest is read-only and there's no inode
	  associated with the dentry
	
	  Compile-time asserts that generic FT_ types still match
	  BTRFS_FT_ types
 Do extra check against inode mode with di_type 
  All this infrastructure exists because dir_emit can fault, and we are holding
  the tree lock when doing readdir.  For now just allocate a buffer and copy
  our information into that, and then dir_emit from the buffer.  This is
  similar to what NFS does, only we don't keep the buffer around in pagecache
  because I'm afraid I'll mess that up.  Long term we need to make filldir do
  copy_to_user_inatomic so we don't have to worry about page faulting under the
  tree lock.
	
	  Stop new entries from being returned after we return the last
	  entry.
	 
	  New directory entries are assigned a strictly increasing
	  offset.  This means that new entries created during readdir
	  are guaranteed to be seen in the future by that readdir.
	  This has broken buggy programs which operate on names as
	  they're returned by readdir.  Until we re-use freed offsets
	  we have this hack to stop new entries from being returned
	  under the assumption that they'll never reach this huge
	  offset.
	 
	  This is being careful not to overflow 32bit loff_t unless the
	  last entry requires it because doing so has broken 32bit apps
	  in the past.
  This is somewhat expensive, updating the tree every time the
  inode changes.  But, it is most likely to find the inode in cache.
  FIXME, needs more benchmarking...there are no reasons other than performance
  to keep or drop this code.
 whoops, lets try again with the full transaction 
  This is a copy of file_update_time.  We need this so we can return error on
  ENOSPC for updating the inode in the case of file write and mmap writes.
  find the highest existing sequence number in a directory
  and then set the in-memory index_cnt variable to reflect
  free sequence numbers
 FIXME: we should be able to handle this 
	
	  MAGIC NUMBER EXPLANATION:
	  since we search a directory based on f_pos we have to start at 2
	  since '.' and '..' have f_pos of 0 and 1 respectively, so everybody
	  else has to start at 2
  helper to find a free sequence number in a given directory.  This current
  code is very simple, later versions will do smarter things in the btree
  Inherit flags from the parent inode.
  Currently only the compression flags and the cow flags are inherited.
	
	  O_TMPFILE, set link count to 0, so that after this point,
	  we fill in an inode item with the correct link count.
	
	  we have to initialize this early, so we can reclaim the inode
	  number if we fail afterwards in this function.
	
	  index_cnt is ignored for everything but a dir,
	  btrfs_set_inode_index_count has an explanation for the magic
	  number
	
	  We could have gotten an inode number from somebody who was fsynced
	  and then removed in this same transaction, so let's just set full
	  sync since it will be a full sync anyway and this will blow away the
	  old info in the log.
		
		  Start new inodes with an inode_ref. This is slightly more
		  efficient for small numbers of hard links since they will
		  be packed into one item. Extended refs will kick in if we
		  add more hard links than can fit in the ref item.
  utility function to add 'inode' into 'parent_inode' with
  a give name and a given sequence number.
  if 'add_backref' is true, also insert a backref from the
  inode to the parent directory.
 Nothing to clean up yet 
	
	  If we are replaying a log tree, we do not want to update the mtime
	  and ctime of the parent directory with the current time, since the
	  log replay procedure is responsible for setting them to their correct
	  values (the ones it had when the fsync was done).
 Return the original error code 
	
	  2 for inode item and ref
	  2 for dir items
	  1 for xattr if selinux is on
	
	 If the active LSM wants to access the inode during
	 d_instantiate it needs these. Smack checks to see
	 if the filesystem supports xattrs by looking at the
	 ops vector.
	
	  2 for inode item and ref
	  2 for dir items
	  1 for xattr if selinux is on
	
	 If the active LSM wants to access the inode during
	 d_instantiate it needs these. Smack checks to see
	 if the filesystem supports xattrs by looking at the
	 ops vector.
 do not allow sys_link's with other subvols of the same device 
	
	  2 items for inode and inode ref
	  2 items for dir items
	  1 item for parent inode
	  1 item for orphan item deletion if O_TMPFILE
 There are several dir indexes for this inode, clear the cache. 
			
			  If new hard link count is 1, it's a file created
			  with open(2) O_TMPFILE flag.
	
	  2 items for inode and ref
	  2 items for dir items
	  1 for xattr if selinux is on
 these must be set before we unlock the inode 
	
	  decompression code contains a memset to fill in any space between the end
	  of the uncompressed data and the end of max_size in case the decompressed
	  data ends up shorter than ram_bytes.  That doesn't cover the hole between
	  the end of an inline extent and the beginning of the next block, so we
	  cover that region here.
  btrfs_get_extent - Lookup the first extent overlapping a range in a file.
  @inode:	file to search in
  @page:	page to read extent data into if the extent is inline
  @pg_offset:	offset into @page to copy to
  @start:	file offset
  @len:	length of range starting at @start
  This returns the first &struct extent_map which overlaps with the given
  range, reading it from the B-tree and caching it if necessary. Note that
  there may be more extents which overlap the given range after the returned
  extent_map.
  If @page is not NULL and the extent is inline, this also reads the extent
  data directly into the page and marks the extent up to date in the io_tree.
  Return: ERR_PTR on error, non-NULL extent_map on success.
 Chances are we'll be called again, so go ahead and do readahead 
	
	  The same explanation in load_free_space_cache applies here as well,
	  we only read when we're loading the free space cache, and at that
	  point the commit_root has everything we need.
		
		  If we backup past the first extent we want to move forward
		  and see if there is an extent in front of us, otherwise we'll
		  say there is a hole for our whole search range which can
		  cause problems.
 Only regular file could have regularprealloc extent 
 New extent overlaps with existing one 
	
	  If our em maps to:
	  - a hole or
	  - a pre-alloc extent,
	  there might actually be delalloc bytes behind it.
 check to see if we've wrapped (len == -1 or similar) 
 ok, we didn't find anything, lets look for delalloc 
	
	  We didn't find anything useful, return the original results from
	  get_extent()
	
	  Adjust the delalloc_start to make sure it doesn't go backwards from
	  the start they passed in
		
		  When btrfs_get_extent can't find anything it returns one
		  huge hole
		 
		  Make sure what it found really fits our range, and adjust to
		  make sure it is based on the start from the caller
			
			  Our hole starts before our delalloc, so we have to
			  return just the parts of the hole that go until the
			  delalloc starts
			
			  Don't adjust block start at all, it is fixed at
			  EXTENT_MAP_HOLE
			
			  Hole is out of passed range or it starts after
			  delalloc range
 compress_type 
  Check if we can do nocow write into the range [@offset, @offset + @len)
  @offset:	File offset
  @len:	The length to write, will be updated to the nocow writeable
 		range
  @orig_start:	(optional) Return the original file offset of the file extent
  @orig_len:	(optional) Return the original on-disk length of the file extent
  @ram_bytes:	(optional) Return the ram_bytes of the file extent
  @strict:	if true, omit optimizations that might force us into unnecessary
 		cow. e.g., don't trust generation number.
  Return:
  >0	and update @len if we can do nocow write
   0	if we can't do nocow write
  <0	if error happened
  NOTE: This only checks the file extents, caller is responsible to wait for
 	 any ordered extents.
 can't find the item, must cow 
 not our file or wrong item type, must cow 
 Wrong offset, must cow 
 not a regular extent, must cow 
	
	  Do the same check as in btrfs_cross_ref_exist but without the
	  unnecessary search.
	
	  look for other files referencing this extent, if we
	  find any we must cow
	
	  adjust disk_bytenr and num_bytes to cover just the bytes
	  in this extent we are about to write.  If there
	  are any csums in that range we have to cow in order
	  to keep the csums correct
	
	  all of the above have passed, it is safe to overwrite this extent
	  without cow
		
		  We're concerned with the entire range that we're going to be
		  doing DIO to, so we need to make sure there's no ordered
		  extents in this range.
		
		  We need to make sure there are no buffered pages in this
		  range either, we could have raced between the invalidate in
		  generic_file_direct_write and locking the extent.  The
		  invalidate needs to happen so that reads after a write do not
		  get stale data.
			
			  If we are doing a DIO read and the ordered extent we
			  found is for a buffered write, we can not wait for it
			  to complete and retry, because if we do so we can
			  deadlock with concurrent buffered writes on page
			  locks. This happens only if our DIO read covers more
			  than one extent map, if at this point has already
			  created an ordered extent for a previous extent map
			  and locked its range in the inode's io tree, and a
			  concurrent write against that previous extent map's
			  range and this range started (we unlock the ranges
			  in the io tree only when the bios complete and
			  buffered writes always lock pages before attempting
			  to lock range in the io tree).
			
			  We could trigger writeback for this range (and wait
			  for it to complete) and then invalidate the pages for
			  this range (through invalidate_inode_pages2_range()),
			  but that can lead us to a deadlock with a concurrent
			  call to readahead (a buffered read or a defrag call
			  triggered a readahead) on a page lock due to an
			  ordered dio extent we created before but did not have
			  yet a corresponding bio submitted (whence it can not
			  complete), which makes readahead wait for that
			  ordered extent to complete while holding a lock on
			  that page.
 The callers of this must take lock_extent() 
		
		  The caller has taken lock_extent(), who could race with us
		  to add em?
 em got 2 refs now, callers needs to do free_extent_map once. 
	
	  We don't allocate a new extent in the following cases
	 
	  1) The inode is marked as NODATACOW. In this case we'll just use the
	  existing extent.
	  2) The extent is marked as PREALLOC. We're good to go here and can
	  just use the extent.
	 
			
			  For inode marked NODATACOW or extent marked PREALLOC,
			  use the existing or preallocated extent, so does not
			  need to adjust btrfs_space_info's bytes_may_use.
 this will cow the extent 
	
	  Need to update the i_size under the extent lock so buffered
	  readers will get the updated i_size when we unlock.
	
	  The generic stuff only does filemap_write_and_wait_range, which
	  isn't enough if we've written compressed pages to this area, so we
	  need to flush the dirty pages again to make absolutely sure that any
	  outstanding dirty pages are on disk.
	
	  If this errors out it's because we couldn't invalidate pagecache for
	  this range and we need to fallback to buffered.
	
	  Ok for INLINE and COMPRESSED extents we need to fallback on buffered
	  io.  INLINE is special, and we could probably kludge it in here, but
	  it's still buffered so for safety lets just fall back to the generic
	  buffered path.
	 
	  For COMPRESSED we _have_ to read the entire extent in so we can
	  decompress it, so there will be buffering required no matter what we
	  do, so go ahead and fallback to buffered.
	 
	  We return -ENOTBLK because that's what makes DIO go ahead and go back
	  to buffered IO.  Don't blame me, this is the price we pay for using
	  the generic code.
 Recalc len in case the new em is smaller than requested 
		
		  We need to unlock only the end area that we aren't using.
		  The rest is going to be unlocked by the endio routine.
	
	  Translate extent map information to iomap.
	  We trim the extents (and move the addr) even though iomap code does
	  that, since we have locked only the parts we are performing IO in.
 If reading from a hole, unlock and return 
	
	  This implies a barrier so that stores to dio_bio->bi_status before
	  this and loads of dio_bio->bi_status after this are fully ordered.
 Check btrfs_submit_bio_hook() for rules about async submit. 
		
		  If we aren't doing async submit, calculate the csum of the
		  bio now.
  If this succeeds, the btrfs_dio_private is responsible for cleaning up locked
  or ordered extents whether or not we submit any bios.
		
		  Load the csums up front to reduce csum tree searches and
		  contention when submitting bios.
		 
		  If we have csums disabled this will do nothing.
		
		  This will never fail as it's passing GPF_NOFS and
		  the allocation is backed by btrfs_bioset.
		
		  Increase the count before we submit the bio so we know
		  the end IO handler won't happen before we increase the
		  count. Otherwise, the dip might get freed before we're
		  done setting it up.
		 
		  We transfer the initial reference to the last bio, so we
		  don't need to increment the reference count for the last one.
			
			  If we are submitting more than one bio, submit them
			  all asynchronously. The exception is RAID 5 or 6, as
			  asynchronous checksums make it difficult to collect
			  full stripe writes.
	
	  If we are under memory pressure we will call this directly from the
	  VM, we need to make sure we have the inode referenced for the ordered
	  extent.  If not just return like we didn't do anything.
  For releasepage() and invalidatepage() we have a race window where
  end_page_writeback() is called but the subpage spinlock is not yet released.
  If we continue to releaseinvalidate the page, we could cause use-after-free
  for subpage spinlock.  So this function is to spin and wait for subpage
  spinlock.
	
	  This may look insane as we just acquire the spinlock and release it,
	  without doing anything.  But we just want to make sure no one is
	  still holding the subpage spinlock.
	  And since the page is not dirty nor writeback, and we have page
	  locked, the only possible way to hold a spinlock is from the endio
	  function to clear page writeback.
	 
	  Here we just acquire the spinlock so that all existing callers
	  should exit and we're safe to releaseinvalidate the page.
	
	  We have page locked so no new ordered extent can be created on this
	  page, nor bio can be submitted for this page.
	 
	  But already submitted bio can still be finished on this page.
	  Furthermore, endio function won't skip page which has Ordered
	  (Private2) already cleared, so it's possible for endio and
	  invalidatepage to do the same ordered extent accounting twice
	  on one page.
	 
	  So here we wait for any submitted bios to finish, so that we won't
	  do double ordered extent accounting on the same page.
	
	  For subpage case, we have call sites like
	  btrfs_punch_hole_lock_range() which passes range not aligned to
	  sectorsize.
	  If the range doesn't cover the full page, we don't need to and
	  shouldn't clear page extent mapped, as page->private can still
	  record subpage dirty bits for other part of the range.
	 
	  For cases that can invalidate the full even the range doesn't
	  cover the full page, like invalidating the last page, we're
	  still safe to wait for ordered extent to finish.
			
			  No ordered extent covering this range, we are safe
			  to delete all extent states in the range.
			
			  There is a range between [cur, oe->file_offset) not
			  covered by any ordered extent.
			  We are safe to delete all extent states, and handle
			  the ordered extent in the next iteration.
			
			  If Ordered (Private2) is cleared, it means endio has
			  already been executed for the range.
			  We can't delete the extent states as
			  btrfs_finish_ordered_io() may still use some of them.
		
		  IO on this page will never be started, so we need to account
		  for any ordered extents now. Don't clear EXTENT_DELALLOC_NEW
		  here, must leave that up for the ordered extent completion.
		 
		  This will also unlock the range for incoming
		  btrfs_finish_ordered_io().
			
			  The ordered extent has finished, now we're again
			  safe to delete all extent states of the range.
			
			  btrfs_finish_ordered_io() will get executed by endio
			  of other pages, thus we can't delete extent states
			  anymore
		
		  Qgroup reserved space handler
		  Sector(s) here will be either:
		 
		  1) Already written to disk or bio already finished
		     Then its QGROUP_RESERVED bit in io_tree is already cleared.
		     Qgroup will be handled by its qgroup_record then.
		     btrfs_qgroup_free_data() call will do nothing here.
		 
		  2) Not written to disk yet
		     Then btrfs_qgroup_free_data() call will clear the
		     QGROUP_RESERVED bit of its io_tree, and free the qgroup
		     reserved data space.
		     Since the IO will never happen for this page.
	
	  We have iterated through all ordered extents of the page, the page
	  should not have Ordered (Private2) anymore, or the above iteration
	  did something wrong.
  btrfs_page_mkwrite() is not allowed to change the file size as it gets
  called from a page fault handler when a page is first dirtied. Hence we must
  be careful to check for EOF conditions here. We set the page up correctly
  for a written page which means we get ENOSPC checking when writing into
  holes and correct delalloc and unwritten extent mapping on filesystems that
  support these features.
  We are not allowed to take the i_mutex here so we have to play games to
  protect against truncate races as the page could now be beyond EOF.  Because
  truncate_setsize() writes the inode size before removing pages, once we have
  the page lock we can determine safely if the page is beyond EOF. If it is not
  beyond EOF, then the page is guaranteed safe against truncation until we
  unlock the page.
	
	  Reserving delalloc space after obtaining the page lock can lead to
	  deadlock. For example, if a dirty page is locked by this function
	  and the call to btrfs_delalloc_reserve_space() ends up triggering
	  dirty page write out, then the btrfs_writepage() function could
	  end up waiting indefinitely to get a lock on the page currently
	  being processed by btrfs_page_mkwrite() function.
 make the VM retry the fault 
 page got truncated out from underneath us 
	
	  we can't set the delalloc bits if there are pending ordered
	  extents.  Drop our locks and wait for them to finish
	
	  page_mkwrite gets called when the page is firstly dirtied after it's
	  faulted in, but write(2) could also dirty a page and set delalloc
	  bits, thus in this case for space account reason, we still need to
	  clear any delalloc bits within this page range since we have to
	  reserve data&meta space before lock_page() (see above comments).
 page is wholly or partially inside EOF 
	
	  Yes ladies and gentlemen, this is indeed ugly.  We have a couple of
	  things going on here:
	 
	  1) We need to reserve space to update our inode.
	 
	  2) We need to have something to cache all the space that is going to
	  be free'd up by the truncate operation, but also have some slack
	  space reserved in case it uses space during the truncate (thank you
	  very much snapshotting).
	 
	  And we need these to be separate.  The fact is we can use a lot of
	  space doing the truncate, and we have no earthly idea how much space
	  we will use, so we need the truncate reservation to be separate so it
	  doesn't end up using space reserved for updating the inode.  We also
	  need to be able to stop the transaction and start a new one, which
	  means we need to be able to update the inode several times, and we
	  have no idea of knowing how many times that will be, so we can't just
	  reserve 1 item for the entirety of the operation, so that has to be
	  done separately as well.
	 
	  So that leaves us with
	 
	  1) rsv - for the truncate reservation, which we will steal from the
	  transaction reservation.
	  2) fs_info->trans_block_rsv - this will have 1 items worth left for
	  updating the inode.
	
	  1 for the truncate slack space
	  1 for updating the inode.
 Migrate the slack space for the truncate to our reserve 
 shouldn't happen 
	
	  We can't call btrfs_truncate_block inside a trans handle as we could
	  deadlock with freeze, if we got NEED_TRUNCATE_BLOCK then we know
	  we've truncated everything except the last little bit, and can do
	  btrfs_truncate_block and then update the disk_i_size.
	
	  So if we truncate and then write and fsync we normally would just
	  write the extents that changed, which is a problem if we need to
	  first truncate that entire inode.  So set this flag so we write out
	  all of the extents in the inode to the sync log so we're completely
	  safe.
	 
	  If no extents were dropped or trimmed we don't need to force the next
	  fsync to truncate all the inode's items from the log and re-log them
	  all. This means the truncate operation did not change the file size,
	  or changed it to a smaller size but there was only an implicit hole
	  between the old i_size and the new i_size, and there were no prealloc
	  extents beyond i_size to drop.
  create a new subvolume directoryinode (helper for the ioctl).
	
	  This can happen where we create an inode, but somebody else also
	  created the same inode and we need to destroy the one we already
	  created.
 the snapsubvol tree is on deleting 
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
	
	  For non-subvolumes allow exchange only within one subvolume, in the
	  same inode namespace. Two subvolumes (represented as directory) can
	  be exchanged as they're a logical link and have a fixed inode number.
 close the race window with snapshot createdestroy ioctl 
	
	  We want to reserve the absolute worst case amount of items.  So if
	  both inodes are subvols and we need to unlink them then that would
	  require 4 item modifications, but if they are both normal inodes it
	  would require 5 item modifications, so we'll assume their normal
	  inodes.  So 5  2 is 10, plus 2 for the new links, so 12 total items
	  should cover the worst case number of items we'll modify.
	
	  We need to find a free sequence number both in the source and
	  in the destination directory for the exchange.
 Reference for the source. 
 force full log commit if subvolume involved. 
 And now for the dest. 
 force full log commit if subvolume involved. 
 Update inode version and ctimemtime. 
	
	  Now pin the logs of the roots. We do it to ensure that no other task
	  can sync the logs while we are in progress with the rename, because
	  that could result in an inconsistency in case any of the inodes that
	  are part of this rename operation were logged before.
	 
	  We pin the logs even if at this precise moment none of the inodes was
	  logged before. This is because right after we checked for that, some
	  other task fsyncing some other inode not involved with this rename
	  operation could log that one of our inodes exists.
	 
	  We don't need to pin the logs before the above calls to
	  btrfs_insert_inode_ref(), since those don't ever need to change a log.
 src is a subvolume 
 src is an inode 
 dest is a subvolume 
 dest is an inode 
	
	  If we have pinned a log and an error happened, we unpin tasks
	  trying to sync the log and force them to fallback to a transaction
	  commit if the log currently contains any of the inodes involved in
	  this rename operation (to ensure we do not persist a log with an
	  inconsistent state for any of these inodes or leading to any
	  inconsistencies when replayed). If the transaction was aborted, the
	  abortion reason is propagated to userspace when attempting to commit
	  the transaction. If the log does not contain any of these inodes, we
	  allow the tasks to sync it.
 we only allow rename subvolume link between subvolumes 
 check for collisions, even if the  name isn't there 
			 we shouldn't get
 maybe -EOVERFLOW 
	
	  we're using rename to replace one file with another.  Start IO on it
	  now so  we don't add too much work to the end of the transaction
 close the racy window with snapshot createdestroy ioctl 
	
	  We want to reserve the absolute worst case amount of items.  So if
	  both inodes are subvols and we need to unlink them then that would
	  require 4 item modifications, but if they are both normal inodes it
	  would require 5 item modifications, so we'll assume they are normal
	  inodes.  So 5  2 is 10, plus 1 for the new link, so 11 total items
	  should cover the worst case number of items we'll modify.
	  If our rename has the whiteout flag, we need more 5 units for the
	  new inode (1 inode item, 1 inode ref, 2 dir items and 1 xattr item
	  when selinux is enabled).
 force full log commit if subvolume involved. 
		
		  Now pin the log. We do it to ensure that no other task can
		  sync the log while we are in progress with the rename, as
		  that could result in an inconsistency in case any of the
		  inodes that are part of this rename operation were logged
		  before.
		 
		  We pin the log even if at this precise moment none of the
		  inodes was logged before. This is because right after we
		  checked for that, some other task fsyncing some other inode
		  not involved with this rename operation could log that one of
		  our inodes exists.
		 
		  We don't need to pin the logs before the above call to
		  btrfs_insert_inode_ref(), since that does not need to change
		  a log.
	
	  If we have pinned the log and an error happened, we unpin tasks
	  trying to sync the log and force them to fallback to a transaction
	  commit if the log currently contains any of the inodes involved in
	  this rename operation (to ensure we do not persist a log with an
	  inconsistent state for any of these inodes or leading to any
	  inconsistencies when replayed). If the transaction was aborted, the
	  abortion reason is propagated to userspace when attempting to commit
	  the transaction. If the log does not contain any of these inodes, we
	  allow the tasks to sync it.
  some fairly slow code that needs optimization. This walks the list
  of all the inodes with pending delalloc and forces them to disk.
		
		  Reset nr_to_write here so we know that we're doing a full
		  flush.
	
	  2 items for inode item and ref
	  2 items for dir items
	  1 item for updating parent inode item
	  1 item for the inline extent item
	  1 item for xattr if selinux is on
	
	 If the active LSM wants to access the inode during
	 d_instantiate it needs these. Smack checks to see
	 if the filesystem supports xattrs by looking at the
	 ops vector.
	
	  Last step, add directory indexes for our symlink inode. This is the
	  last step to avoid extra cleanup of these indexes if an error happens
	  elsewhere above.
 Encryption and other encoding is reserved and all 0 
	
	  We have released qgroup data range at the beginning of the function,
	  and normally qgroup_released bytes will be freed when committing
	  transaction.
	  But if we error out early, we have to free what we have released
	  or we leak qgroup data reservation.
		
		  If we are severely fragmented we could end up with really
		  small allocations, so if the allocator is returning small
		  chunks lets make its job easier by only searching for those
		  sized chunks.
		
		  We've reserved this space, and thus converted it from
		  ->bytes_may_use to ->bytes_reserved.  Any error that happens
		  from here on out we will only need to clear our reservation
		  for the remaining unreserved area, so advance our
		  clear_offset by our extent size.
		
		  Now that we inserted the prealloc extent we can finally
		  decrement the number of reservations in the block group.
		  If we did it before, we could race with relocation and have
		  relocation miss the reserved extent, making it fail later.
	
	  5 units required for adding orphan entry
	
	  We set number of links to 0 in btrfs_new_inode(), and here we set
	  it to 1 because d_tmpfile() will issue a warning if the count is 0,
	  through:
	 
	     d_tmpfile() -> inode_dec_link_count() -> drop_nlink()
 Pages should be in the extent_io_tree 
  Add an entry indicating a block group or device which is pinned by a
  swapfile. Returns 0 on success, 1 if there is already an entry for it, or a
  negative errno on failure.
 Free all of the entries pinned by this swapfile. 
	
	  If the swap file was just created, make sure delalloc is done. If the
	  file changes again after this, the user is doing something stupid and
	  we don't really care.
	
	  The inode is locked, so these flags won't change after we check them.
	
	  Balance or device removereplaceresize can move stuff around from
	  under us. The exclop protection makes sure they aren't runningwon't
	  run concurrently while we are mapping the swap extents, and
	  fs_info->swapfile_pins prevents them from running while the swap
	  file is active and moving the extents. Note that this also prevents
	  a concurrent device add which isn't actually necessary, but it's not
	  really worth the trouble to allow it.
	
	  Prevent snapshot creation while we are activating the swap file.
	  We do not want to race with snapshot creation. If snapshot creation
	  already started before we bumped nr_swapfiles from 0 to 1 and
	  completes before the first write into the swap file after it is
	  activated, than that write would fallback to COW.
	
	  Snapshots can create extents which require COW even if NODATACOW is
	  set. We use this counter to prevent snapshots. We must increment it
	  before walking the extents because we don't want a concurrent
	  snapshot to run after we've already checked the extents.
			
			  It's unlikely we'll ever actually find ourselves
			  here, as a file small enough to fit inline won't be
			  big enough to store more than the swap header, but in
			  case something changes in the future, let's catch it
			  here rather than later.
  Update the number of bytes used in the VFS' inode. When we replace extents in
  a range (clone, dedupe, fallocate's zero range), we must update the number of
  bytes used by the inode in an atomic manner, so that concurrent stat(2) calls
  always get a correct value.
  btrfs doesn't support the bmap operation because swapfiles
  use bmap to make a mapping of extents in the file.  They assume
  these extents won't change over the life of the file and they
  use the bmap result to do IO directly to the drive.
  the btrfs bmap call would return logical addresses that aren't
  suitable for IO and they also will change frequently as COW
  operations happen.  So, swapfile + btrfs == corruption.
  For now we're avoiding this by dropping bmap.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2011 STRATO.  All rights reserved.
  This is the implementation for the generic read ahead framework.
  To trigger a readahead, btrfs_reada_add must be called. It will start
  a read ahead for the given range [start, end) on tree root. The returned
  handle can either be used to wait on the readahead to finish
  (btrfs_reada_wait), or to send it to the background (btrfs_reada_detach).
  The read ahead works as follows:
  On btrfs_reada_add, the root of the tree is inserted into a radix_tree.
  reada_start_machine will then search for extents to prefetch and trigger
  some reads. When a read finishes for a node, all contained nodeleaf
  pointers that lie in the given range will also be enqueued. The reads will
  be triggered in sequential order, thus giving a big win over a naive
  enumeration. It will also make use of multi-device layouts. Each disk
  will have its on read pointer and all disks will by utilized in parallel.
  Also will no two disks read both sides of a mirror simultaneously, as this
  would waste seeking capacity. Instead both disks will read different parts
  of the filesystem.
  Any number of readaheads can be started in parallel. The read order will be
  determined globally, i.e. 2 parallel readaheads will normally finish faster
  than the 2 started one after another.
	struct btrfs_device	devs[BTRFS_MAX_MIRRORS];  full list, incl
 recurses 
 in case of err, eb might be NULL 
	
	  just take the full list from the extent. afterwards we
	  don't need the lock anymore
	
	  this is the error case, the extent buffer has not been
	  read correctly. We won't access anything from it and
	  just cleanup our data structures. Effectively this will
	  cut the branch below this node from read ahead.
	
	  FIXME: currently we just set nritems to 0 if this is a leaf,
	  effectively ignoring the content. In a next step we could
	  trigger more readahead depending from the content, e.g.
	  fetch the checksums for the extents in the leaf.
			
			  if the generation doesn't match, just ignore this
			  extctl. This will probably cut off a branch from
			  prefetch. Alternatively one could start a new (sub-)
			  prefetch for this branch, starting again from root.
			  FIXME: move the generation check out of this loop
	
	  free extctl records
 one ref for each entry 
 find extent 
 our ref 
 our device always sits at index 0 
 bounds have already been checked 
	
	  map block
 cannot read ahead on missing device. 
 not a single zone found, error and out 
 Insert extent in reada tree + all per-device trees, all or nothing 
			
			  in case of DUP, just add the first zone. As both
			  are on the same device, there's nothing to gain
			  from adding both.
			  Also, it wouldn't work, as the tree is per device
			  and adding would fail with EEXIST
			
			  as this device is selected for reading only as
			  a last resort, skip it for read ahead.
 ignore whether the entry was inserted 
			
			  no fs_info->reada_lock needed, as this can't be
			  the last ref
			 no fs_info->reada_lock needed, as this can't be
 takes one ref 
 leave the ref on the extent 
  called with fs_info->reada_lock held
  called with fs_info->reada_lock held
 pick the zone with the most elements 
	
	  FIXME currently we issue the reads one extent at a time. If we have
	  a contiguous block of extents, we could also coagulate them or use
	  plugging to speed things up
	
	  find mirror num
 Try to start up to 10k READA requests for a group of devices 
	
	  If everything is already in the cache, this is effectively single
	  threaded. To a) not hold the caller for too long and b) to utilize
	  more cores, we broke the loop above after 10000 iterations and now
	  enqueue to workers to finish it. This will distribute the load to
	  the cores.
 FIXME we cannot handle this properly right now 
  interface
 one ref for having elements 
  Before removing a device (device replace or device remove ioctls), call this
  function to wait for all existing readahead requests on the device and to
  make sure no one queues more readahead requests for the device.
  Must be called without holding neither the device list mutex nor the device
  replace semaphore, otherwise it will deadlock.
 Serialize with readahead extent creation at reada_find_extent(). 
	
	  There might be readahead requests added to the radix trees which
	  were not yet added to the readahead work queue. We need to start
	  them and wait for their completion, otherwise we can end up with
	  use-after-free problems when dropping the last reference on the
	  readahead extents and their zones, as they need to access the
	  device structure.
  If when removing a device (device replace or device remove ioctls) an error
  happens after calling btrfs_reada_remove_dev(), call this to undo what that
  function did. This is safe to call even if btrfs_reada_remove_dev() was not
  called before.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
 0 == as many as possible 
  Convert block group flags (BTRFS_BLOCK_GROUP_) to btrfs_raid_types, which
  can be used as index to access btrfs_raid_array[].
 BTRFS_BLOCK_GROUP_SINGLE 
  Fill @buf with textual description of @bg_flags, no more than @size_buf
  bytes including terminating null byte.
 remove last | 
	
	  The text is trimmed, it's up to the caller to provide sufficiently
	  large buffer
  Device locking
  ==============
  There are several mutexes that protect manipulation of devices and low-level
  structures like chunks but not block groups, extents or files
  uuid_mutex (global lock)
  ------------------------
  protects the fs_uuids list that tracks all per-fs fs_devices, resulting from
  the SCAN_DEV ioctl registration or from mount either implicitly (the first
  device) or requested by the device= mount option
  the mutex can be very coarse and can cover long-running operations
  protects: updates to fs_devices counters like missing devices, rw devices,
  seeding, structure cloning, openingclosing devices at mountumount time
  global::fs_devs - add, remove, updates to the global list
  does not protect: manipulation of the fs_devices::devices list in general
  but in mount context it could be used to exclude list modifications by eg.
  scan ioctl
  btrfs_device::name - renames (write side), read is RCU
  fs_devices::device_list_mutex (per-fs, with RCU)
  ------------------------------------------------
  protects updates to fs_devices::devices, ie. adding and deleting
  simple list traversal with read-only actions can be done with RCU protection
  may be used to exclude some operations from running concurrently without any
  modifications to the list (see write_all_supers)
  Is not required at mount and close times, because our device list is
  protected by the uuid_mutex at that point.
  balance_mutex
  -------------
  protects balance structures (status, state) and context accessed from
  several places (internally, ioctl)
  chunk_mutex
  -----------
  protects chunks, adding or removing during allocation, trim or when a new
  device is addedremoved. Additionally it also protects post_commit_list of
  individual devices, since they can be added to the transaction's
  post_commit_list only with chunk_mutex held.
  cleaner_mutex
  -------------
  a big lock that is held by the cleaner thread and prevents running subvolume
  cleaning together with relocation or delayed iputs
  Lock nesting
  ============
  uuid_mutex
    device_list_mutex
      chunk_mutex
    balance_mutex
  Exclusive operations
  ====================
  Maintains the exclusivity of the following operations that apply to the
  whole filesystem and cannot run in parallel.
  - Balance ()
  - Device add
  - Device remove
  - Device replace ()
  - Resize
  The device operations (as above) can be in one of the following states:
  - Running state
  - Paused state
  - Completed state
  Only device operations marked with () can go into the Paused state for the
  following reasons:
  - ioctl (only Balance can be Paused through ioctl)
  - filesystem remounted as read-only
  - filesystem unmounted and mounted as read-only
  - system power-cycle and filesystem mounted as read-only
  - filesystem or device errors leading to forced read-only
  The status of exclusive operation is set and cleared atomically.
  During the course of Paused state, fs_info::exclusive_operation remains set.
  A device operation in Paused or Running state can be canceled or resumed
  either by ioctl (Balance only) or when remounted as read-write.
  The exclusive status is cleared when the device operation is canceled or
  completed.
  alloc_fs_devices - allocate struct btrfs_fs_devices
  @fsid:		if not NULL, copy the UUID to fs_devices::fsid
  @metadata_fsid:	if not NULL, copy the UUID to fs_devices::metadata_fsid
  Return a pointer to a new struct btrfs_fs_devices on success, or ERR_PTR().
  The returned struct is not linked onto any lists and can be destroyed with
  kfree() right away.
 Handle non-split brain cases 
	
	  Handle scanned device having completed its fsid change but
	  belonging to a fs_devices that was created by first scanning
	  a device which didn't have its fsidmetadata_uuid changed
	  at all and the CHANGING_FSID_V2 flag set.
	
	  Handle scanned device having completed its fsid change but
	  belonging to a fs_devices that was created by a device that
	  has an outdated pair of fsidmetadata_uuid and
	  CHANGING_FSID_V2 flag set.
   Search and remove all stale (devices which are not mounted) devices.
   When both inputs are NULL, it will search and release all stale devices.
   path:	Optional. When provided will it release all unmounted devices
 		matching this path only.
   skip_dev:	Optional. Will skip this device when searching for the stale
 		devices.
   Return:	0 for success or if @path is NULL.
  		-EBUSY if @path is a mounted device.
  		-ENOENT if @path does not match any device in the list.
 for an already deleted device return 0 
 delete the stale device 
  This is only used on mount, and we are protected from competing things
  messing with our fs_devices by the uuid_mutex, thus we do not need the
  fs_devices->device_list_mutex here.
  Handle scanned device having its CHANGING_FSID_V2 flag set and the fs_devices
  being created with a disk that has already completed its fsid change. Such
  disk can belong to an fs which has its FSID changed or to one which doesn't.
  Handle both cases here.
	
	  Handles the case where scanned device is part of an fs that had
	  multiple successful changes of FSID but currently device didn't
	  observe it. Meaning our fsid will be different than theirs. We need
	  to handle two subcases :
	   1 - The fs still continues to have different METADATAFSID uuids.
	   2 - The fs is switched back to its original FSID (METADATAFSID
	   are equal).
 Changed UUIDs 
 Unchanged UUIDs 
	
	  Handle the case where the scanned device is part of an fs whose last
	  metadata UUID change reverted it to the original FSID. At the same
	  time  fs_devices was first created by another constitutent device
	  which didn't fully observe the operation. This results in an
	  btrfs_fs_devices created with metadatafsid different AND
	  btrfs_fs_devices::fsid_change set AND the metadata_uuid of the
	  fs_devices equal to the FSID of the disk.
  Add new device to list of registered devices
  Returns:
  device pointer which was just added or updated when successful
  error pointer when failed
		
		  If this disk has been pulled into an fs devices created by
		  a device which had the CHANGING_FSID_V2 flag then replace the
		  metadata_uuidfsid values of the fs_devices.
 we can safely leave the fs_devices entry around 
		
		  When FS is already mounted.
		  1. If you are here and if the device->name is NULL that
		     means this device was missing at time of FS mount.
		  2. If you are here and if the device->name is different
		     from 'path' that means either
		       a. The same device disappeared and reappeared with
		          different name. or
		       b. The missing-disk-which-was-replaced, has
		          reappeared now.
		 
		  We must allow 1 and 2a above. But 2b would be a spurious
		  and unintentional.
		 
		  Further in case of 1 and 2a above, the disk at 'path'
		  would have missed some transaction when it was away and
		  in case of 2a the stale bdev has to be updated as well.
		  2b must not be allowed at all time.
		
		  For now, we do allow update to btrfs_fs_device through the
		  btrfs dev scan cli after FS has been mounted.  We're still
		  tracking a problem where systems fail mount by subvolume id
		  when we reject replacement on a mounted FS.
			
			  That is if the FS is _not_ mounted and if you
			  are here, that means there is more than one
			  disk with same uuid and devid.We keep the one
			  with larger generation number or the last-in if
			  generation are equal.
		
		  We are going to replace the device path for a given devid,
		  make sure it's the same device if the device is mounted
				
				  device->fs_info may not be reliable here, so
				  pass in a NULL instead. This avoids a
				  possible use-after-free when the fs_info and
				  fs_info->sb are already torn down.
	
	  Unmount does not free the btrfs_device struct but would zero
	  generation along with most of the other members. So just update
	  it back. We need it to pick the disk with largest generation
	  (as above).
		
		  This is ok to do without rcu read locked because we hold the
		  uuid mutex so nothing we touch in here is going to disappear.
 This is the initialized path, it is safe to release the devices. 
		
		  We have already validated the presence of BTRFS_DEV_REPLACE_DEVID,
		  in btrfs_init_dev_replace() so just continue.
  After we have read the system tree and know devids belonging to this
  filesystem, remove the device which does not belong there.
	
	  Reset the flush error record. We might have a transient flush error
	  in this mount, and if so we aborted the current transaction and set
	  the fs to an error state, guaranteeing no super blocks can be further
	  committed. However that error might be transient and if we unmount the
	  filesystem and mount it again, we should allow the mount to succeed
	  (btrfs_check_rw_degradable() should not fail) - if after mounting the
	  filesystem again we still get flush errors, then we will again abort
	  any transaction and set the error state, guaranteeing no commits of
	  unsafe super blocks.
 Verify the device is back in a pristine state  
	
	  The device_list_mutex cannot be taken here in case opening the
	  underlying device takes further locks like open_mutex.
	 
	  We also don't need the lock here as this is called during mount and
	  exclusion is provided by uuid_mutex
 make sure our super fits in the device 
 make sure our super fits in the page 
 make sure our super doesn't straddle pages on disk 
 pull in the page with our super 
 align our pointer to the offset of the super block 
  Look for a btrfs signature on a device. This may be called out of the mount path
  and we are not allowed to call set_blocksize during the scan. The superblock
  is read via pagecache
	
	  we would like to check all the supers, but that would make
	  a btrfs mount succeed after a mkfs from a different FS.
	  So, we need to add a special mount option to scan for
	  later supers, using BTRFS_SUPER_MIRROR_MAX instead
  Try to find a chunk that intersects [start, start + len] range and when one
  such is found, record the end of it in start
		
		  We don't want to overwrite the superblock on the drive nor
		  any area used by the boot loader (grub for example), so we
		  make sure to start at an offset of at least 1MB.
		
		  We don't care about the starting region like regular
		  allocator, because we anyway usereserve the first two zones
		  for superblock logging.
 Range is ensured to be empty 
 Given hole range was invalid (outside of device) 
  dev_extent_hole_check - check if specified hole is suitable for allocation
  @device:	the device which we have the hole
  @hole_start: starting position of the hole
  @hole_size:	the size of the hole
  @num_bytes:	the size of the free space that we need
  This function may modify @hole_start and @hole_size to reflect the suitable
  position for allocation. Returns 1 if hole position is updated, 0 otherwise.
		
		  Check before we set max_hole_start, otherwise we could end up
		  sending back this offset anyway.
 No extra check 
				
				  The changed hole can contain pending extent.
				  Loop again to check that.
  find_free_dev_extent_start - find free space in the specified device
  @device:	  the device which we search the free space in
  @num_bytes:	  the size of the free space that we need
  @search_start: the position from which to begin the search
  @start:	  store the start of the free space.
  @len:	  the size of the free space. that we find, or the size
 		  of the max free space if we don't find suitable free space
  this uses a pretty simple search, the expectation is that it is
  called very infrequently and that a given device has a small number
  of extents
  @start is used to store the start of the free space if we find. But if we
  don't find suitable free space, it will be used to store the start position
  of the max free space.
  @len is used to store the size of the free space that we find.
  But if we don't find suitable free space, it is used to store the size of
  the max free space.
  NOTE: This function will search commit root of device tree, and does extra
  check to ensure dev extents are not double allocated.
  This makes the function safe to allocate dev extents but may not report
  correct usable device space, as device extent freed in current transaction
  is not reported as available.
			
			  If this free space is greater than which we need,
			  it must be the max free space that we have found
			  until now, so max_hole_start must point to the start
			  of this free space and the length of this free space
			  is stored in max_hole_size. Thus, we return
			  max_hole_start and max_hole_size and go back to the
			  caller.
	
	  At this point, search_start should be the end of
	  allocated dev extents, and when shrinking the device,
	  search_end may be smaller than search_start.
 See above. 
 FIXME use last free of some kind 
 Corruption 
  the device information is stored in the chunk root
  the btrfs_device struct should be fully filled in
  Function to update ctimemtime for a given device path.
  Mainly used for ctimemtime based probe like libblkid.
  We don't care about errors here, this is just to be kind to userspace.
  Verify that @num_devices satisfies the RAID profile constraints in the whole
  filesystem. It's up to the caller to adjust that number regarding eg. device
  replace.
  Helper function to check if the given device is part of s_bdev  latest_dev
  and replace it with the provided or the next active device, in the context
  where this function called, there should be always be another device (or
  this_dev) which is active.
  Return btrfs_fs_devices::num_devices excluding the device that's being
  currently replaced.
 write_on_page() unlocks the page 
 Notify udev that device has changed 
 Update ctimemtime for device path for libblkid 
	
	  The device list in fs_devices is accessed without locks (neither
	  uuid_mutex nor device_list_mutex) as it won't change on a mounted
	  filesystem and another device rm cannot run.
	
	  TODO: the superblock still includes this device in its num_devices
	  counter although write_all_supers() is not locked out. This
	  could give a filesystem state which requires a degraded mount.
	
	  the device list mutex makes sure that we don't change
	  the device list while someone else is writing out all
	  the device supers. Whoever is writing all supers, should
	  lock the device list mutex before getting the number of
	  devices in the super block (super_copy). Conversely,
	  whoever updates the number of devices in the super block
	  (super_copy) should hold the device list mutex.
	
	  In normal cases the cur_devices == fs_devices. But in case
	  of deleting a seed device, the cur_devices should point to
	  its own fs_devices listed under the fs_devices->seed_list.
 Update total_devices of the parent fs_devices if it's seed 
 remove sysfs entry 
	
	  At this point, the device is zero sized and detached from the
	  devices list.  All that's left is to zero out the old supers and
	  free the device.
	 
	  We cannot call btrfs_close_bdev() here because we're holding the sb
	  write lock, and blkdev_put() will pull in the ->open_mutex on the
	  block device and it's dependencies.  Instead just flush the device
	  and let the caller do the final blkdev_put.
	
	  This can happen if cur_devices is the private seed devices list.  We
	  cannot call close_fs_devices() here because it expects the uuid_mutex
	  to be held, but in fact we don't need that for the private
	  seed_devices, we can simply decrement cur_devices->opened and then
	  remove it from our list and free the fs_devices.
	
	  in case of fs with no seed, srcdev->fs_devices will point
	  to fs_devices of fs_info. However when the dev being replaced is
	  a seed dev it will point to the seed's local fs_devices. In short
	  srcdev will have its correct fs_devices in both the cases.
 if this is no devs we rather delete the fs_devices 
		
		  On a mounted FS, num_devices can't be zero unless it's a
		  seed. In case of a seed device being replaced, the replace
		  target added to the sprout FS, so there will be no more
		  device left under the seed FS.
  Populate args from device at path
  @fs_info:	the filesystem
  @args:	the args to populate
  @path:	the path to the device
  This will read the super block of the device at @path and populate @args with
  the devid, fsid, and uuid.  This is meant to be used for ioctls that need to
  lookup a device to operate on, but need to do it before we take any locks.
  This properly handles the special case of "missing" that a user may pass in,
  and does some basic sanity checks.  The caller must make sure that @path is
  properly NUL terminated before calling in, and must call
  btrfs_put_dev_args_from_path() in order to free up the temporary fsid and
  uuid buffers.
  Return: 0 for success, -errno for failure
  Only use this jointly with btrfs_get_dev_args_from_path() because we will
  allocate our ->uuid and ->fsid pointers, everybody else uses local variables
  that don't need to be freed.
  does all the dirty work required for changing file system's UUID.
	
	  Private copy of the seed devices, anchored at
	  fs_info->fs_devices->seed_list
	
	  It's necessary to retain a copy of the original seed fs_devices in
	  fs_uuids so that filesystems which have been seeded can successfully
	  reference the seed device from open_seed_devices. This also supports
	  multiple fs seed.
  Store the expected generation for seed devices in device items.
 Logic error 
 we can safely leave the fs_devices entry around 
	
	  we've got more storage, clear any full flags on the space
	  infos
 Add sysfs device entry 
		
		  fs_devices now represents the newly sprouted filesystem and
		  its fsid has been changed by btrfs_prepare_sprout
 transaction commit 
	
	  Now that we have written a new super block to this device, check all
	  other fs_devices list if device_path alienates any other scanned
	  device.
	  We can ignore the return value as it typically returns -EINVAL and
	  only succeeds if the device was an alien.
 Update ctimemtime for blkid or udev 
 Logic error or corruption 
  btrfs_get_chunk_map() - Find the mapping containing the given logical extent.
  @logical: Logical block offset in bytes.
  @length: Length of extent in bytes.
  Return: Chunk mapping or ERR_PTR.
 callers are responsible for dropping em's ref. 
	
	  Removing chunk items and updating the device items in the chunks btree
	  requires holding the chunk_mutex.
	  See the comment at btrfs_chunk_alloc() for the details.
		
		  This is a logic error, but we don't want to just rely on the
		  user having built with ASSERT enabled, so if ASSERT doesn't
		  do anything we still error out.
	
	  First delete the device extent items from the devices btree.
	  We take the device_list_mutex to avoid racing with the finishing phase
	  of a device replace operation. See the comment below before acquiring
	  fs_info->chunk_mutex. Note that here we do not acquire the chunk_mutex
	  because that can result in a deadlock when deleting the device extent
	  items from the devices btree - COWing an extent buffer from the btree
	  may result in allocating a new metadata chunk, which would attempt to
	  lock again fs_info->chunk_mutex.
	
	  We acquire fs_info->chunk_mutex for 2 reasons:
	 
	  1) Just like with the first phase of the chunk allocation, we must
	     reserve system space, do all chunk btree updates and deletions, and
	     update the system chunk array in the superblock while holding this
	     mutex. This is for similar reasons as explained on the comment at
	     the top of btrfs_chunk_alloc();
	 
	  2) Prevent races with the final phase of a device replace operation
	     that replaces the device object associated with the map's stripes,
	     because the device object's id can change at any time during that
	     final phase of the device replace operation
	     (dev-replace.c:btrfs_dev_replace_finishing()), so we could grab the
	     replaced device and then see it with an ID of
	     BTRFS_DEV_REPLACE_DEVID, which would cause a failure when updating
	     the device item, which does not exists on the chunk btree.
	     The finishing phase of device replace acquires both the
	     device_list_mutex and the chunk_mutex, in that order, so we are
	     safe by just acquiring the chunk_mutex.
	
	  Normally we should not get -ENOSPC since we reserved space before
	  through the call to check_system_chunk().
	 
	  Despite our system space_info having enough free space, we may not
	  be able to allocate extents from its block groups, because all have
	  an incompatible profile, which will force us to allocate a new system
	  block group with the right profile, or right after we called
	  check_system_space() above, a scrub turned the only system block group
	  with enough free space into RO mode.
	  This is explained with more detail at do_chunk_alloc().
	 
	  So if we get -ENOSPC, allocate a new system chunk and retry once.
	
	  We are done with chunk btree updates and deletions, so release the
	  system space we previously reserved (with check_system_chunk()).
 once for us 
	
	  Prevent races with automatic removal of unused block groups.
	  After we relocate and before we remove the chunk with offset
	  chunk_offset, automatic removal of the block group can kick in,
	  resulting in a failure when calling btrfs_remove_chunk() below.
	 
	  Make sure to acquire this mutex before doing a tree search (dev
	  or chunk trees) to find chunks. Otherwise the cleaner kthread might
	  call btrfs_remove_chunk() (through btrfs_delete_unused_bgs()) after
	  we release the path used to search the chunkdev tree and before
	  the current task acquires this mutex and calls us.
 step one, relocate all the extents inside this chunk 
	
	  On a zoned file system, discard the whole block group, this will
	  trigger a REQ_OP_ZONE_RESET operation on the device zone. If
	  resetting the zone fails, don't treat it as a fatal problem from the
	  filesystem's point of view.
	
	  step two, delete the device extents and the
	  chunk tree entries
 Corruption 
  return 1 : allocate a data chunk successfully,
  return <0: errors during allocating a data chunk,
  return 0 : no need to allocate a data chunk.
  This is a heuristic used to reduce the number of chunks balanced on
  resume after balance was interrupted.
	
	  Turn on soft mode for chunk types that were being converted.
	
	  Turn on usage filter if is not already used.  The idea is
	  that chunks that we have already balanced should be
	  reasonably full.  Don't do it for chunks that are being
	  converted - that will keep us from relocating unconverted
	  (albeit full) chunks.
  Clear the balance status in fs_info and delete the balance item from disk.
  Balance filters.  Return 1 if chunk should be filtered out
  (should not be balanced).
 [pstart, pend) 
 [vstart, vend) 
 at least part of the chunk is inside this vrange 
 type filter 
 profiles filter 
 usage filter 
 devid filter 
 drange filter, makes sense only with devid filter 
 vrange filter 
 stripes filter 
 soft profile changing mode 
	
	  limited by count, must be the last filter
		
		  Same logic as the 'limit' filter; the minimum cannot be
		  determined here because we do not have the global information
		  about the count of all chunks that satisfy the filters.
 The single value limit and minmax limits use the same bytes in the 
 zero out stat counters 
		
		  The single value limit and minmax limits use the same bytes
		  in the
		
		  this shouldn't happen, it means the last relocate
		  failed
 FIXME break ? 
		
		  Apply limit_min filter, no need to check if the LIMITS
		  filter is used, limit_min is 0 by default
			
			  We may be relocating the only data chunk we have,
			  which could potentially end up with losing data's
			  raid profile, so lets allocate an empty one in
			  advance.
  alloc_profile_is_valid - see if a given profile is valid and reduced
  @flags: profile to validate
  @extended: if true @flags is treated as an extended profile
 1) check that all other bits are zeroed 
 2) see if profile is reduced 
 "0" is valid for usual profiles 
 cancel requested || normal exit path 
  Validate target profile against allowed profiles and return true if it's OK.
  Otherwise print the error message and return false.
 Profile is valid and does not have bits outside of the allowed set 
  Fill @buf with textual description of balance filter flags @bargs, up to
  @size_buf including the terminating null. The output may be trimmed if it
  does not fit into the provided buffer.
 remove last , 
 remove last " " 
  Should be called with balance mutexe held
	
	  In case of mixed groups both data and meta should be picked,
	  and identical options should be given for both of them.
	
	  rw_devices will not change at the moment, device adddeletereplace
	  are exclusive
	
	  SINGLE profile on-disk has no profile bit, but in-memory we have a
	  special bit for it, to make it easier to distinguish.  Thus we need
	  to set it manually, or balance would refuse the profile.
	
	  Allow to reduce metadata or system integrity only if force set for
	  profiles with redundancy (copies, parity)
 if we're not converting, the target field is uninitialized 
	
	  Balance can be canceled by:
	 
	  - Regular cancel request
	    Then ret == -ECANCELED and balance_cancel_req > 0
	 
	  - Fatal signal to "btrfs" process
	    Either the signal caught by wait_reserve_ticket() and callers
	    got -EINTR, or caught by btrfs_should_cancel_balance() and
	    got -ECANCELED.
	    Either way, in this case balance_cancel_req = 0, and
	    ret == -EINTR or ret == -ECANCELED.
	 
	  So here we only check the return value to catch canceled balance.
	
	  A ro->rw remount sequence should continue with the paused balance
	  regardless of who pauses it, system or the user as of now, so set
	  the resume flag.
 ret = -ENOENT; 
	
	  This should never happen, as the paused balance state is recovered
	  during mount without any chance of other exclusive ops to collide.
	 
	  This gives the exclusive op status to balance and keeps in paused
	  state until user intervention (cancel or umount). If the ownership
	  cannot be assigned, show a message but do not fail. The balance
	  is in a paused state and must have fs_info::balance_ctl properly
	  set up.
 we are good with balance_ctl ripped off from under us 
	
	  A paused balance with the item stored on disk can be resumed at
	  mount time if the mount is read-write. Otherwise it's still paused
	  and we must not allow cancelling as it deletes the item.
	
	  if we are running just wait and return, balance item is
	  deleted in btrfs_balance in this case
		
		  Lock released to allow other waiters to continue, we'll
		  reexamine the status again.
			
			  1 - subvol uuid item
			  1 - received_subvol uuid item
	
	  1 - root node
	  1 - root item
 fs_info->update_uuid_tree_gen remains 0 in all error case 
  shrinking a device means finding all of the device extents past
  the new size, and then following the back refs to the chunks.
  The chunk relocation code actually frees the device extent
	
	  Once the device's size has been set to the new size, ensure all
	  in-memory chunks are synced to disk so that the loop below sees them
	  and relocates them accordingly.
		
		  We may be relocating the only data chunk we have,
		  which could potentially end up with losing data's
		  raid profile, so lets allocate an empty one in
		  advance.
 Shrinking succeeded, else we would be at "done". 
 Clear all state bits beyond the shrunk device size 
 Now btrfs_update_device() will change the on-disk size. 
  sort the devices in descending order by max_avail, total_avail
  Structure used internally for btrfs_create_chunk() function.
  Wraps needed parameters.
 Total number of stripes to allocate 
 sub_stripes info for map 
 Stripes per device 
 Maximum number of devices to use 
 Minimum number of devices to use 
 ndevs has to be a multiple of this 
 Number of copies 
 Number of stripes worth of bytes to store parity information 
 For larger filesystems, use larger metadata chunks 
 We don't want a chunk larger than 10% of writable space 
 We don't want a chunk larger than 10% of writable space 
	
	  in the first pass through the devices list, we gather information
	  about the available holes on each device.
 If there is no space on this device, skip it. 
	
	  now sort the devices by hole size  available space
 Number of stripes that count for block group size 
	
	  The primary goal is to maximize the number of stripes, so use as
	  many devices as possible, even if the stripes are not maximum sized.
	 
	  The DUP profile stores more than one stripe per device, the
	  max_avail is the total size so we have to adjust.
 This will have to be fixed for RAID1 and RAID10 over more drives 
	
	  Use the number of data stripes to figure out how big this chunk is
	  really going to be in terms of logical address space, and compare
	  that answer with the max chunk size. If it's higher, we try to
	  reduce stripe_size.
		
		  Reduce stripe_size, round it up to a 16MB boundary again and
		  then use it, unless it ends up being even bigger than the
		  previous value we had already.
 Align to BTRFS_STRIPE_LEN 
 Number of stripes that count for block group size 
	
	  It should hold because:
	     dev_extent_min == dev_extent_want == zone_size  dev_stripes
 stripe_size is fixed in zoned filesysmte. Reduce ndevs instead. 
	
	  Round down to number of usable stripes, devs_increment can be any
	  number so we can't use round_down() that requires power of 2, while
	  rounddown is safe.
 One for our allocation 
 One for the tree reference 
  This function, btrfs_chunk_alloc_add_chunk_item(), typically belongs to the
  phase 1 of chunk allocation. It belongs to phase 2 only when allocating system
  chunks.
  See the comment at btrfs_chunk_alloc() for details about the chunk allocation
  phases.
	
	  We take the chunk_mutex for 2 reasons:
	 
	  1) Updates and insertions in the chunk btree must be done while holding
	     the chunk_mutex, as well as updating the system chunk array in the
	     superblock. See the comment on top of btrfs_chunk_alloc() for the
	     details;
	 
	  2) To prevent races with the final phase of a device replace operation
	     that replaces the device object associated with the map's stripes,
	     because the device object's id can change at any time during that
	     final phase of the device replace operation
	     (dev-replace.c:btrfs_dev_replace_finishing()), so we could grab the
	     replaced device and then see it with an ID of BTRFS_DEV_REPLACE_DEVID,
	     which would cause a failure when updating the device item, which does
	     not exists, or persisting a stripe of the chunk item with such ID.
	     Here we can't use the device_list_mutex because our caller already
	     has locked the chunk_mutex, and the final phase of device replace
	     acquires both mutexes - first the device_list_mutex and then the
	     chunk_mutex. Using any of those two mutexes protects us from a
	     concurrent device replace.
	
	  When adding a new device for sprouting, the seed device is read-only
	  so we must first allocate a metadata and a system chunk. But before
	  adding the block group items to the extent, device and chunk btrees,
	  we must first:
	 
	  1) Create both chunks without doing any changes to the btrees, as
	     otherwise we would get -ENOSPC since the block groups from the
	     seed device are read-only;
	 
	  2) Add the device item for the new sprout device - finishing the setup
	     of a new block group requires updating the device item in the chunk
	     btree, so it must exist when we attempt to do it. The previous step
	     ensures this does not fail with -ENOSPC.
	 
	  After that we can add the block group items to their btrees:
	  update existing device item in the chunk btree, add a new block group
	  item to the extent btree, add a new chunk item to the chunk btree and
	  finally add the new device extent items to the devices btree.
	
	  If the number of missing devices is larger than max errors, we can
	  not write the data into that chunk successfully.
 once for us 
 once for the tree 
		
		  We could return errors for these cases, but that could get
		  ugly and we'd probably do the same thing which is just not do
		  anything else and exit, so return 1 so the callers don't try
		  to use other copies.
		
		  There could be two corrupted data stripes, we need
		  to loop retry in order to rebuild the correct data.
		 
		  Fail a stripe at a time on every retry except the
		  stripe under reconstruction.
 Shouldn't happen, just warn and use pid instead of failing 
	
	  try to avoid the drive that is the source drive for a
	  dev-replace procedure, only choose it if no other non-missing
	  mirror is available
	 we couldn't find one that doesn't fail.  Just return something
	  and the io error handling code will clean up eventually
 Bubble-sort the stripe set to put the paritysyndrome stripes last 
 Swap if parity is on a smaller index 
 The size of btrfs_io_context 
 Plus the variable array for the stripes 
 Plus the variable array for the tgt dev 
		
		  Plus the raid_map, which includes both the tgt dev
		  and the stripes.
 can REQ_OP_DISCARD be sent with other REQ like REQ_OP_WRITE? 
  Please note that, discard won't be sent to target device of device
  replace.
 Discard always returns a bioc. 
 we don't discard raid56 yet 
	
	  stripe_nr counts the total number of stripes we have to stride
	  to get to this block
 stripe_offset is the offset of this block in its stripe 
	
	  after this, stripe_nr is the number of stripes on this
	  device we have to walk to find the data, and stripe_index is
	  the number of our device in the stripe array
			
			  Special for the first stripe and
			  the last stripe:
			 
			  |-------|...|-------|
			      |----------|
			     off     end_off
  In dev-replace case, for repair case (that's the only case where the mirror
  is selected explicitly when calling btrfs_map_block), blocks left of the
  left cursor can also be read from the target drive.
  For REQ_GET_READ_MIRRORS, the target drive is added as the last one to the
  array of stripes.
  For READ, it also needs to be supported using the same mirror number.
  If the requested block is not left of the left cursor, EIO is returned. This
  can happen because btrfs_num_copies() returns one more in the dev-replace
  case.
		
		  BTRFS_MAP_GET_READ_MIRRORS does not contain this mirror,
		  that means that the requested area is not left of the left
		  cursor
	
	  process the rest of the function using the mirror_num of the source
	  drive. Therefore look it up first.  At the end, patch the device
	  pointer to the one of the target drive.
		
		  In case of DUP, in order to keep it simple, only add the
		  mirror with the lowest physical address
 Non zoned filesystem does not use "to_copy" flag 
		
		  A block group which have "to_copy" set will eventually
		  copied by dev-replace process. We can avoid cloning IO here.
		
		  duplicate the write operations while the dev replace
		  procedure is running. Since the copying of the old disk to
		  the new disk takes place at run time while the filesystem is
		  mounted writable, the regular write operations to the old
		  disk have to be duplicated to go to the new disk as well.
		 
		  Note that device->missing is handled by the caller, and that
		  the write to the old disk is already set up in the stripes
		  array.
 write to new disk, too 
		
		  During the dev-replace procedure, the target drive can also
		  be used to read data in case it is needed to repair a corrupt
		  block elsewhere. This is possible if the requested area is
		  left of the left cursor. In this area, the target drive is a
		  full copy of the source drive.
				
				  In case of DUP, in order to keep it simple,
				  only add the mirror with the lowest physical
				  address
  Calculate the geometry of a particular (address, len) tuple. This
  information is used to calculate how big a particular bio can get before it
  straddles a stripe.
  @fs_info: the filesystem
  @em:      mapping containing the logical extent
  @op:      type of operation - write or read
  @logical: address that we want to figure out the geometry of
  @io_geom: pointer used to return values
  Returns < 0 in case a chunk for the given logical address cannot be found,
  usually shouldn't happen unless @logical is corrupted, 0 otherwise.
 Offset of this logical address in the chunk 
 Len of a stripe in a chunk 
 Stripe where this block falls in 
 Offset of stripe in the chunk 
 stripe_offset is the offset of this block in its stripe 
		
		  In case of raid56, we need to know the stripe aligned start
			
			  Allow a write of a full stripe, but make sure we
			  don't allow straddling of stripes
			
			  For writes to RAID[56], allow a full stripeset across
			  all disks. For other RAID types and for RAID[56]
			  reads, just allow a single stripe (on a single disk).
	
	  Hold the semaphore for read during the whole operation, write is
	  requested at commit time but must wait.
 push stripe_nr back to the start of the full stripe 
 RAID[56] write or recovery. Return all stripes 
			
			  Mirror #0 or #1 means the original data block.
			  Mirror #2 is RAID5 parity block.
			  Mirror #3 is RAID6 Q block.
 We distribute the parity blocks across stripes 
		
		  after this, stripe_nr is the number of stripes on this
		  device we have to walk to find the data, and stripe_index is
		  the number of our device in the stripe array
 Build raid_map 
 Work out the disk rotation on this stripe-set 
 Fill in the logical address of each stripe 
	
	  this is the case that REQ_READ && dev_replace_is_ongoing &&
	  mirror_num == num_stripes + 1 && dev_replace target drive is
	  available as a mirror
 Unlock and let waiting writers proceed 
 For Scrubreplace 
		 only send an error to the higher layers if it is
		  beyond the tolerance of the btrfs bio
			
			  this bio is actually up to date, we didn't
			  go over the max number of errors
	
	  For zone append writing, bi_sector must point the beginning of the
	  zone
 Should be the original bio. 
		 In this case, map_length has been set to the length of
  Find a device specified by @devid or @uuid in the list of @fs_devices, or
  return NULL.
  If devid and uuid are both specified, the match must be exact, otherwise
  only devid is used.
	
	  We call this under the chunk_mutex, so we want to use NOFS for this
	  allocation, however we don't want to change btrfs_alloc_device() to
	  always do NOFS because we use it in a lot of other GFP_KERNEL safe
	  places.
  btrfs_alloc_device - allocate struct btrfs_device
  @fs_info:	used only for generating a new devid, can be NULL if
 		devid is provided (i.e. @devid != NULL).
  @devid:	a pointer to devid for this device.  If NULL a new devid
 		is generated.
  @uuid:	a pointer to UUID for this device.  If NULL a new UUID
 		is generated.
  Return: a pointer to a new &struct btrfs_device on success; ERR_PTR()
  on error.  Returned struct is not linked onto any lists and must be
  destroyed with btrfs_free_device.
	
	  Preallocate a bio that's always going to be used for flushing device
	  barriers and matches the device lifespan
  Due to page cache limit, metadata beyond BTRFS_32BIT_MAX_FILE_SIZE
  can't be accessed on 32bit systems.
  This function do mount time check to reject the fs if it already has
  metadata chunk beyond that limit.
  This is to give early warning for any metadata chunk reaching
  BTRFS_32BIT_EARLY_WARN_THRESHOLD.
  Although we can still access the metadata, it's not going to be possible
  once the limit is reached.
	
	  Only need to verify chunk item if we're reading from sys chunk array,
	  as chunk item in tree block is already verified by tree-checker.
 already mapped? 
 This will match only for multi-device seed fs 
	
	  Upon first call for a seed fs fsid, just create a private copy of the
	  respective fs_devices and anchor it at fs_info->fs_devices->seed_list
			
			  this happens when a device that was properly setup
			  in the device info lists suddenly goes bad.
			  device->bdev is NULL, and so we have to set
			  device->missing to one here
 Move the device to its own fs_devices 
	
	  This will create extent buffer of nodesize, superblock size is
	  fixed to BTRFS_SUPER_INFO_SIZE. If nodesize > sb size, this will
	  overallocate but we can keep it as-is, only the first page is used.
	
	  The sb extent buffer is artificial and just used to read the system array.
	  set_extent_buffer_uptodate() call does not properly mark all it's
	  pages up-to-date when the page is larger: extent does not cover the
	  whole page and consequently check_page_uptodate does not find all
	  the page's extents up-to-date (the hole beyond sb),
	  write_extent_buffer then triggers a WARN_ON.
	 
	  Regular short extents go through mark_extent_buffer_dirtywriteback cycle,
	  but sb spans only this function. Add an explicit SetPageUptodate call
	  to silence the warning eg. on PowerPC 64.
		
		  At least one btrfs_chunk with one stripe must be present,
		  exact stripe count check comes afterwards
  Check if all chunks in the fs are OK for read-write degraded mount
  If the @failing_dev is specified, it's accounted as missing.
  Return true if all chunks meet the minimal RW mount requirements.
  Return false if any chunk doesn't meet the minimal RW mount requirements.
 No chunk at all? Return false anyway 
	
	  uuid_mutex is needed only if we are mounting a sprout FS
	  otherwise we don't need it.
	
	  It is possible for mount and umount to race in such a way that
	  we execute this code path, but open_fs_devices failed to clear
	  total_rw_bytes. We certainly want it cleared before reading the
	  device items, so clear it here.
	
	  Lockdep complains about possible circular locking dependency between
	  a disk's open_mutex (struct gendisk.open_mutex), the rw semaphores
	  used for freeze procection of a fs (struct super_block.s_writers),
	  which we take when starting a transaction, and extent buffers of the
	  chunk tree if we call read_one_dev() while holding a lock on an
	  extent buffer of the chunk tree. Since we are mounting the filesystem
	  and at this point there can't be any concurrent task modifying the
	  chunk tree, to keep it simple, just skip locking on the chunk tree.
	
	  Read all device items, and then all the chunk items. All
	  device items are found before any chunk item (their object id
	  is smaller than the lowest possible object id for a chunk
	  item - BTRFS_FIRST_CHUNK_TREE_OBJECTID).
			
			  We are only called at mount time, so no need to take
			  fs_info->chunk_mutex. Plus, to avoid lockdep warnings,
			  we always lock first fs_info->chunk_mutex before
			  acquiring any locks on the chunk tree. This is a
			  requirement for chunk allocation, see the comment on
			  top of btrfs_chunk_alloc() for details.
	
	  After loading chunk tree, we've got all device information,
	  do another round of validation checks.
 need to delete old one and insert a new one 
 need to insert a new item 
  called from commit_transaction. Writes all changed device stats to disk.
		
		  There is a LOAD-LOAD control dependency between the value of
		  dev_stats_ccnt and updating the on-disk values which requires
		  reading the in-memory counters. Such control dependencies
		  require explicit read memory barriers.
		 
		  This memory barriers pairs with smp_mb__before_atomic in
		  btrfs_dev_stat_incbtrfs_dev_stat_set and with the full
		  barrier implied by atomic_xchg in
		  btrfs_dev_stats_read_and_reset
 all values == 0, suppress message 
  Update the size and bytes used for each device where it changed.  This is
  delayed since we would otherwise get errors while writing out the
  superblocks.
  Must be invoked during transaction commit.
	
	  We don't need the device_list_mutex here.  This list is owned by the
	  transaction and the transaction must complete before the device is
	  released.
  Multiplicity factor for simple profiles: DUP, RAID1-like and RAID10.
 Make sure no dev extent is beyond device boundary 
  Ensure that all dev extents are mapped to correct chunk, otherwise
  later chunk allocationfree would cause unexpected behavior.
  NOTE: This will iterate through the whole device tree, which should be of
  the same size level as the chunk tree.  This slightly increases mount time.
	
	  We don't have a dev_root because we mounted with ignorebadroots and
	  failed to load the root, so we want to skip the verification in this
	  case for sure.
	 
	  However if the dev root is fine, but the tree itself is corrupted
	  we'd still fail to mount.  This verification is only to make sure
	  writes can happen safely, so instead just bypass this check
	  completely in the case of IGNOREBADROOTS.
 No dev extents at all? Not good 
 Check if this dev extent overlaps with the previous one 
 Ensure all chunks have corresponding dev extents 
  Check whether the given block group or device is pinned by any inode being
  used as a swapfile.
 Ensure block group still exists 
 Do not attempt to repair in degraded state 
 SPDX-License-Identifier: GPL-2.0
  Return target flags in extended format or 0 if restripe for this chunk_type
  is not in progress
  Should be called with balance_lock held
  @flags: available profiles in extended format (see ctree.h)
  Return reduced profile in chunk format.  If profile changing is in progress
  (either running or paused) picks the target profile (if it's already
  available), otherwise falls back to plain reducing.
	
	  See if restripe for this chunk_type is in progress, if so try to
	  reduce to the target profile
 First, mask out the RAID levels which aren't possible 
		
		  A block_group shouldn't be on the discard_list anymore.
		  Remove the block_group from the discard_list to prevent us
		  from causing a panic due to NULL pointer dereference.
		
		  If not empty, someone is still holding mutex of
		  full_stripe_lock, which can only be released by caller.
		  And it will definitely cause use-after-free when caller
		  tries to release full stripe lock.
		 
		  No better way to resolve, but only to warn.
  This adds the block group to the fs_info rb tree for the block group cache
  This will return the block group at or after bytenr if contains is 0, else
  it will return the block group that contains the bytenr
  Return the block group that starts at or after bytenr
  Return the block group that contains the given bytenr
 If our block group was removed, we need a full search. 
 No put on block group, done by btrfs_dec_nocow_writers 
	
	  Once for our lookup and once for the lookup done by a previous call
	  to btrfs_inc_nocow_writers()
	
	  Our block group is read only but before we set it to read only,
	  some task might have had allocated an extent from it already, but it
	  has not yet created a respective ordered extent (and added it to a
	  root's list of ordered extents).
	  Therefore wait for any task currently allocating extents, since the
	  block group's reservations counter is incremented while a read lock
	  on the groups' semaphore is held and decremented after releasing
	  the read access on that semaphore and creating the ordered extent.
  When we wait for progress in the block group caching, its because our
  allocation attempt failed at least once.  So, we must sleep and let some
  progress happen before we try again.
  This function will sleep at least once waiting for new free space to show
  up, and then it will check the block group free space numbers for our min
  num_bytes.  Another option is to have it go ahead and look in the rbtree for
  a free extent of a given size, but this is a good start.
  Callers of this must check if cache->cached == BTRFS_CACHE_ERROR before using
  any of the information in this block group.
  This is only called by btrfs_cache_block_group, since we could have freed
  extents we need to check the pinned_extents for any extents that can't be
  used yet since their free space will be released as soon as the transaction
  commits.
 -ENOMEM or logic error 
 -ENOMEM or logic error 
	
	  If we're fragmenting we don't want to make anybody think we can
	  allocate from this block group until we've had a chance to fragment
	  the free space.
	
	  We don't want to deadlock with somebody trying to allocate a new
	  extent for the extent root while also trying to search the extent
	  root to add free space.  So we skip locking and search the commit
	  root, since its read-only
		
		  We failed to load the space cache, set ourselves to
		  CACHE_STARTED and carry on.
	
	  If we are in the transaction that populated the free space tree we
	  can't actually cache from the free space tree as our commit root and
	  real root are the same, so we could change the contents of the blocks
	  while caching.  Instead do the slow caching in this case, and after
	  the transaction has committed we will be safe.
 Allocator for zoned filesystems does not use the cache at all 
  Clear incompat bits for the following feature(s):
  - RAID56 - in case there's neither RAID5 nor RAID6 profile block group
             in the whole filesystem
  - RAID1C34 - same as above for RAID1C3 and RAID1C4 block groups
	
	  Free the reserved super bytes from this block group before
	  remove it.
 make sure this block group isn't part of an allocation cluster 
	
	  make sure this block group isn't part of a metadata
	  allocation cluster
	
	  get the inode first so any iput calls done for the io_list
	  aren't the final iput (no unlinks allowed now)
	
	  Make sure our free space cache IO is done before removing the
	  free space inode
 Once for the block groups rbtree 
	
	  we must use list_del_init so people can check to see if they
	  are still on the list after taking the semaphore
 Once for the caching bgs list and once for us. 
	
	  Remove the free space for the block group from the free space tree
	  and the block group's item from the extent tree before marking the
	  block group as removed. This is to prevent races with tasks that
	  freeze and unfreeze a block group, this task and another task
	  allocating a new block group - the unfreeze task ends up removing
	  the block group's extent map before the task calling this function
	  deletes the block group item from the extent tree, allowing for
	  another task to attempt to create another block group with the same
	  item key (and failing with -EEXIST and a transaction abort).
	
	  At this point trimming or scrub can't start on this block group,
	  because we removed the block group from the rbtree
	  fs_info->block_group_cache_tree so no one can't find it anymore and
	  even if someone already got this block group before we removed it
	  from the rbtree, they have already incremented block_group->frozen -
	  if they didn't, for the trimming case they won't find any free space
	  entries because we already removed them all when we called
	  btrfs_remove_free_space_cache().
	 
	  And we must not remove the extent map from the fs_info->mapping_tree
	  to prevent the same logical address range and physical device space
	  ranges from being reused for a new block group. This is needed to
	  avoid races with trimming and scrub.
	 
	  An fs trim operation (btrfs_trim_fs()  btrfs_ioctl_fitrim()) is
	  completely transactionless, so while it is trimming a range the
	  currently running transaction might finish and a new one start,
	  allowing for new block groups to be created that can reuse the same
	  physical device locations unless we take this special care.
	 
	  There may also be an implicit trim operation if the file system
	  is mounted with -odiscard. The same protections must remain
	  in place until the extents have been discarded completely when
	  the transaction commit has completed.
 once for the tree 
 Once for the lookup reference 
	
	  We need to reserve 3 + N units from the metadata space info in order
	  to remove a block group (done at btrfs_remove_chunk() and at
	  btrfs_remove_block_group()), which are used for:
	 
	  1 unit for adding the free space inode's orphan (located in the tree
	  of tree roots).
	  1 unit for deleting the block group item (located in the extent
	  tree).
	  1 unit for deleting the free space item (located in tree of tree
	  roots).
	  N units for deleting N device extent items corresponding to each
	  stripe (located in the device tree).
	 
	  In order to remove a block group we also need to reserve units in the
	  system space info in order to update the chunk tree (update one or
	  more device items and remove one chunk item), but this is done at
	  btrfs_remove_chunk() through a call to check_system_chunk().
  Mark block group @cache read-only, so later write won't happen to block
  group @cache.
  If @force is not set, this function will only mark the block group readonly
  if we have enough free space (1M) in other metadatasystem block groups.
  If @force is not set, this function will mark the block group readonly
  without checking free space.
  NOTE: This function doesn't care if other block groups can contain all the
  data in this block group. That check should be done by relocation routine,
  not this function.
	
	  Data never overcommits, even in mixed mode, so do just the straight
	  check of left over space in how much we have allocated.
		
		  Here we make sure if we mark this bg RO, we still have enough
		  free space as buffer.
		
		  We overcommit metadata, so we need to do the
		  btrfs_can_overcommit check here, and we need to pass in
		  BTRFS_RESERVE_NO_FLUSH to give ourselves the most amount of
		  leeway to allow us to mark this block group as read only.
 Migrate zone_unusable bytes to readonly 
	
	  Hold the unused_bg_unpin_mutex lock to avoid racing with
	  btrfs_finish_extent_commit(). If we are at transaction N, another
	  task might be running finish_extent_commit() for the previous
	  transaction N - 1, and have seen a range belonging to the block
	  group in pinned_extents before we were able to clear the whole block
	  group range from pinned_extents. This means that task can lookup for
	  the block group after we unpinned it from pinned_extents and removed
	  it, leading to a BUG_ON() at unpin_extent_range().
  Process the unused_bgs list and remove any that don't have any allocated
  space inside of them.
	
	  Long running balances can keep us blocked here for eternity, so
	  simply skip deletion if we're unable to get the mutex.
 Don't want to race with allocators so take the groups_sem 
		
		  Async discard moves the final block group discard to be prior
		  to the unused_bgs code path.  Therefore, if it's not fully
		  trimmed, punt it back to the async discard lists.
 Requeue if we failed because of async discard 
			
			  We want to bail if we made new allocations or have
			  outstanding allocations in this block group.  We do
			  the ro check in case balance is currently acting on
			  this block group.
 We don't want to force the issue, only flip if it's ok. 
		
		  Want to do this before we do anything else so we can recover
		  properly if we fail to join the transaction.
		
		  We could have pending pinned extents for this block group,
		  just delete them, we don't care about them anymore.
		
		  At this point, the block_group is read only and should fail
		  new allocations.  However, btrfs_finish_extent_commit() can
		  cause this block_group to be placed back on the discard
		  lists because now the block_group isn't fully discarded.
		  Bail here and try again later after discarding everything.
 Reset pinned so btrfs_put_block_group doesn't complain 
		
		  The normal path here is an unused block group is passed here,
		  then trimming is handled in the transaction commit path.
		  Async discard interposes before this to do the trimming
		  before coming down the unused block group path as trimming
		  will no longer be done later in the transaction commit path.
		
		  DISCARD can flip during remount. On zoned filesystems, we
		  need to reset sequential-required zones.
 Implicit trim during transaction commit. 
		
		  Btrfs_remove_chunk will abort the transaction if things go
		  horribly wrong.
		
		  If we're not mounted with -odiscard, we can just forget
		  about this block group. Otherwise we'll need to wait
		  until transaction commit to do the actual discard.
			
			  A concurrent scrub might have added us to the list
			  fs_info->unused_bgs, so use a list_move operation
			  to add the block group to the deleted_bgs list.
  We want block groups with a low number of used bytes to be in the beginning
  of the list, so they will get reclaimed first.
	
	  Long running balances can keep us blocked here for eternity, so
	  simply skip reclaim if we're unable to get the mutex.
	
	  Sort happens under lock because we can't simply splice it and sort.
	  The block groups might still be in use and reachable via bg_list,
	  and their presence in the reclaim_bgs list must be preserved.
 Don't race with allocators so take the groups_sem 
			
			  We want to bail if we made new allocations or have
			  outstanding allocations in this block group.  We do
			  the ro check in case balance is currently acting on
			  this block group.
 Get out fast, in case we're unmounting the filesystem 
		
		  Cache the zone_unusable value before turning the block group
		  to read only. As soon as the blog group is read only it's
		  zone_unusable value gets moved to the block group's read-only
		  bytes and isn't available for calculations anymore.
  Map a physical disk address to a list of logical addresses
  @fs_info:       the filesystem
  @chunk_start:   logical address of block group
  @bdev:	   physical device to resolve, can be NULL to indicate any device
  @physical:	   physical address to map to logical addresses
  @logical:	   return array of logical addresses which map to @physical
  @naddrs:	   length of @logical
  @stripe_len:    size of IO stripe for the given block group
  Maps a particular @physical disk address to a list of @logical addresses.
  Used primarily to exclude those portions of a block group that contain super
  block copies.
 For RAID56 adjust to a full IO stripe length 
		
		  The remaining case would be for RAID56, multiply by
		  nr_data_stripes().  Alternatively, just use rmap_len below
		  instead of map->stripe_len
 Ensure we don't add duplicate addresses 
 Shouldn't have super stripes in sequential zones 
  Iterate all chunks and verify that each of them has the corresponding block
  group
		
		  lookup_extent_mapping will return the first extent map
		  intersecting the range, so setting @len to 1 is enough to
		  get the first chunk.
		
		  When we mount with old space cache, we need to
		  set BTRFS_DC_CLEAR and set dirty flag.
		 
		  a) Setting 'BTRFS_DC_CLEAR' makes sure that we
		     truncate the old free space cache inode and
		     setup a new one.
		  b) Setting 'dirty flag' makes sure that we flush
		     the new space cache info onto disk.
	
	  We need to exclude the super stripes now so that the space info has
	  super bytes accounted for, otherwise we'll think we have more space
	  than we actually do.
 We may have excluded something, so call this just in case. 
	
	  For zoned filesystem, space after the allocation offset is the only
	  free space for a block group. So, we don't need any caching work.
	  btrfs_calc_zone_unusable() will set the amount of free space and
	  zone_unusable space.
	 
	  For regular filesystem, check for two cases, either we are full, and
	  therefore don't need to bother with the caching work since we won't
	  find any space, or we are empty, and we can just add all the space
	  in and be done with it.  This saves us _a_lot_ of time, particularly
	  in the full case.
 Should not have any excluded extents. Just in case, though. 
 Fill dummy cache as FULL 
		
		  We may have some valid block group cache added already, in
		  that case we skip to the next one.
		
		  Avoid allocating from un-mirrored block group if there are
		  mirrored block groups.
	
	  We've hit some error while reading the extent tree, and have
	  rescue=ibadroots mount option.
	  Try to fill the tree using dummy block groups so that the user can
	  continue to mount and grab their data.
  This function, insert_block_group_item(), belongs to the phase 2 of chunk
  allocation.
  See the comment at btrfs_chunk_alloc() for details about the chunk allocation
  phases.
  This function belongs to phase 2.
  See the comment at btrfs_chunk_alloc() for details about the chunk allocation
  phases.
	
	  Take the device list mutex to prevent races with the final phase of
	  a device replace operation that replaces the device object associated
	  with the map's stripes, because the device object's id can change
	  at any time during that final phase of the device replace operation
	  (dev-replace.c:btrfs_dev_replace_finishing()), so we could grab the
	  replaced device and then see it with an ID of BTRFS_DEV_REPLACE_DEVID,
	  resulting in persisting a device extent item with such ID.
  This function, btrfs_create_pending_block_groups(), belongs to the phase 2 of
  chunk allocation.
  See the comment at btrfs_chunk_alloc() for details about the chunk allocation
  phases.
		
		  If we restriped during balance, we may have added a new raid
		  type, so now add the sysfs entries when it is safe to do so.
		  We don't have to worry about locking here as it's handled in
		  btrfs_sysfs_add_block_group_type.
 Already aborted the transaction if it failed. 
	
	  New block group is likely to be used soon. Try to activate it now.
	  Failure is OK for now.
 We may have excluded something, so call this just in case 
	
	  Ensure the corresponding space_info object is created and
	  assigned to our block group. We want our bg to be added to the rbtree
	  with its ->space_info set.
	
	  Now that our block group has its ->space_info set and is inserted in
	  the rbtree, update the space info's counters.
  Mark one block group RO, can be called several times for the same block
  group.
  @cache:		the destination block group
  @do_chunk_alloc:	whether need to do chunk pre-allocation, this is to
  			ensure we still have some free space after marking this
  			block group RO.
		
		  We're not allowed to set block groups readonly after the dirty
		  block group cache has started writing.  If it already started,
		  back off and let this transaction commit.
		
		  If we are changing raid levels, try to allocate a
		  corresponding block group with the new raid level.
			
			  ENOSPC is allowed here, we may have enough space
			  already allocated at the new raid level to carry on
 Migrate zone_unusable bytes back 
	
	  If this block group is smaller than 100 megs don't bother caching the
	  block group.
	
	  We want to set the generation to 0, that way if anything goes wrong
	  from here on out we know not to trust this cache when we load up next
	  time.
		
		  So theoretically we could recover from this, simply set the
		  super cache generation to 0 so we know to invalidate the
		  cache, but then we'd have to keep track of the block groups
		  that fail this way so we know we _have_ to reset this cache
		  before the next commit or risk reading stale cache.  So to
		  limit our exposure to horrible edge cases lets just abort the
		  transaction, this only happens in really bad situations
		  anyway.
 We've already setup this transaction, go ahead and exit 
		
		  don't bother trying to write stuff out _if_
		  a) we're not cached,
		  b) we're with nospace_cache mount option,
		  c) we're with v2 space_cache (FREE_SPACE_TREE).
	
	  We hit an ENOSPC when setting up the cache in this transaction, just
	  skip doing the setup, we've already cleared the cache so we're safe.
	
	  Try to preallocate enough space based on how big the block group is.
	  Keep in mind this has to include any pinned space which could end up
	  taking up quite a bit since it's not folded into the other space
	  cache.
	
	  Our cache requires contiguous chunks so that we don't modify a bunch
	  of metadata or split extents when writing the cache out, which means
	  we can enospc if we are heavily fragmented in addition to just normal
	  out of space conditions.  So if we hit this just skip setting up any
	  other block groups for this transaction, maybe we'll unpin enough
	  space the next time around.
 Could add new block groups, use _safe just in case 
  Transaction commit does final block group cache writeback during a critical
  section where nothing is allowed to change the FS.  This is required in
  order for the cache to actually match the block group, but can introduce a
  lot of latency into the commit.
  So, btrfs_start_dirty_block_groups is here to kick off block group cache IO.
  There's a chance we'll have to redo some of it if the block group changes
  again during the commit, but it greatly reduces the commit latency by
  getting rid of the easy block groups while we're still allowing others to
  join the commit.
 Make sure all the block groups on our dirty list actually exist 
	
	  cache_write_mutex is here only to save us from balance or automatic
	  removal of empty block groups deleting this block group while we are
	  writing out the cache
		
		  This can happen if something re-dirties a block group that
		  is already under IO.  Just wait for it to finish and then do
		  it all again
		
		  btrfs_wait_cache_io uses the cache->dirty_list to decide if
		  it should update the cache_state.  Don't delete until after
		  we wait.
		 
		  Since we're not running in the commit critical section
		  we need the dirty_bgs_lock to protect from update_block_group
				
				  The cache_write_mutex is protecting the
				  io_list, also refer to the definition of
				  btrfs_transaction::io_bgs for more details
				
				  If we failed to write the cache, the
				  generation will be bad and life goes on
			
			  Our block group might still be attached to the list
			  of new block groups in the transaction handle of some
			  other task (struct btrfs_trans_handle->new_bgs). This
			  means its block group item isn't yet in the extent
			  tree. If this happens ignore the error, as we will
			  try again later in the critical section of the
			  transaction commit.
 If it's not on the io list, we need to put the block group 
		
		  Avoid blocking other tasks for too long. It might even save
		  us from writing caches for block groups that are going to be
		  removed.
	
	  Go through delayed refs for all the stuff we've just kicked off
	  and then loop back (just once)
		
		  dirty_bgs_lock protects us from concurrent block group
		  deletes too (not just cache_write_mutex).
	
	  Even though we are in the critical section of the transaction commit,
	  we can still have concurrent tasks adding elements to this
	  transaction's list of dirty block groups. These tasks correspond to
	  endio free space workers started when writeback finishes for a
	  space cache, which run inode.c:btrfs_finish_ordered_io(), and can
	  allocate new block groups as a result of COWing nodes of the root
	  tree when updating the free space inode. The writeback for the space
	  caches is triggered by an earlier call to
	  btrfs_start_dirty_block_groups() and iterations of the following
	  loop.
	  Also we want to do the cache_save_setup first and then run the
	  delayed refs to make sure we have the best chance at doing this all
	  in one shot.
		
		  This can happen if cache_save_setup re-dirties a block group
		  that is already under IO.  Just wait for it to finish and
		  then do it all again
		
		  Don't remove from the dirty list until after we've waited on
		  any pending IO
				
				  If we failed to write the cache, the
				  generation will be bad and life goes on
			
			  One of the free space endio workers might have
			  created a new block group while updating a free space
			  cache's inode (at inode.c:btrfs_finish_ordered_io())
			  and hasn't released its transaction handle yet, in
			  which case the new block group is still attached to
			  its transaction handle and its creation has not
			  finished yet (no block group item in the extent tree
			  yet, etc). If this is the case, wait for all free
			  space endio workers to finish and retry. This is a
			  very rare case so no need for a more efficient and
			  complex approach.
 If its not on the io list, we need to put the block group 
	
	  Refer to the definition of io_bgs member for details why it's safe
	  to use it without any locking
 Block accounting for super block 
		
		  If this block group has free space cache written out, we
		  need to make sure to load it if we are removing space.  This
		  is because we need the unpinning stage to actually add the
		  space back to the block group, otherwise we will leak space.
		
		  No longer have used bytes in this block group, queue it for
		  deletion. We do this after adding the block group to the
		  dirty list to avoid races between cleaner kthread and space
		  cache writeout.
 Modified block groups are accounted for in the delayed_refs_rsv. 
  btrfs_add_reserved_bytes - update the block_group and space info counters
  @cache:	The cache we are manipulating
  @ram_bytes:  The number of bytes of file content, and will be same to
               @num_bytes except for the compress path.
  @num_bytes:	The number of bytes in question
  @delalloc:   The blocks are allocated for the delalloc write
  This is called by the allocator when it reserves space. If this is a
  reservation and the block group has become read only we cannot make the
  reservation and return -EAGAIN, otherwise this function always succeeds.
		
		  Compression can use less space than we reserved, so wake
		  tickets if that happens
  btrfs_free_reserved_bytes - update the block_group and space info counters
  @cache:      The cache we are manipulating
  @num_bytes:  The number of bytes in question
  @delalloc:   The blocks are allocated for the delalloc write
  This is called by somebody who is freeing space that was never actually used
  on disk.  For example if you reserve some space for a new leaf in transaction
  A and before transaction A commits you free that leaf, you call this with
  reserve set to 0 in order to clear the reservation.
	
	  in limited mode, we want to have some free space up to
	  about 1% of the FS size.
	
	  Check if we have enough space in the system space info because we
	  will need to update device items in the chunk btree and insert a new
	  chunk item in the chunk btree as well. This will allocate a new
	  system block group if needed.
	
	  Normally we are not expected to fail with -ENOSPC here, since we have
	  previously reserved space in the system space_info and allocated one
	  new system chunk if necessary. However there are three exceptions:
	 
	  1) We may have enough free space in the system space_info but all the
	     existing system block groups have a profile which can not be used
	     for extent allocation.
	 
	     This happens when mounting in degraded mode. For example we have a
	     RAID1 filesystem with 2 devices, lose one device and mount the fs
	     using the other device in degraded mode. If we then allocate a chunk,
	     we may have enough free space in the existing system space_info, but
	     none of the block groups can be used for extent allocation since they
	     have a RAID1 profile, and because we are in degraded mode with a
	     single device, we are forced to allocate a new system chunk with a
	     SINGLE profile. Making check_system_chunk() iterate over all system
	     block groups and check if they have a usable profile and enough space
	     can be slow on very large filesystems, so we tolerate the -ENOSPC and
	     try again after forcing allocation of a new system chunk. Like this
	     we avoid paying the cost of that search in normal circumstances, when
	     we were not mounted in degraded mode;
	 
	  2) We had enough free space info the system space_info, and one suitable
	     block group to allocate from when we called check_system_chunk()
	     above. However right after we called it, the only system block group
	     with enough free space got turned into RO mode by a running scrub,
	     and in this case we have to allocate a new one and retry. We only
	     need do this allocate and retry once, since we have a transaction
	     handle and scrub uses the commit root to search for block groups;
	 
	  3) We had one system block group with enough free space when we called
	     check_system_chunk(), but after that, right before we tried to
	     allocate the last extent buffer we needed, a discard operation came
	     in and it temporarily removed the last free space entry from the
	     block group (discard removes a free space entry, discards it, and
	     then adds back the entry to the block group cache).
  Chunk allocation is done in 2 phases:
  1) Phase 1 - through btrfs_chunk_alloc() we allocate device extents for
     the chunk, the chunk mapping, create its block group and add the items
     that belong in the chunk btree to it - more specifically, we need to
     update device items in the chunk btree and add a new chunk item to it.
  2) Phase 2 - through btrfs_create_pending_block_groups(), we add the block
     group item to the extent btree and the device extent items to the devices
     btree.
  This is done to prevent deadlocks. For example when COWing a node from the
  extent btree we are holding a write lock on the node's parent and if we
  trigger chunk allocation and attempted to insert the new block group item
  in the extent btree right way, we could deadlock because the path for the
  insertion can include that parent node. At first glance it seems impossible
  to trigger chunk allocation after starting a transaction since tasks should
  reserve enough transaction units (metadata space), however while that is true
  most of the time, chunk allocation may still be triggered for several reasons:
  1) When reserving metadata, we check if there is enough free space in the
     metadata space_info and therefore don't trigger allocation of a new chunk.
     However later when the task actually tries to COW an extent buffer from
     the extent btree or from the device btree for example, it is forced to
     allocate a new block group (chunk) because the only one that had enough
     free space was just turned to RO mode by a running scrub for example (or
     device replace, block group reclaim thread, etc), so we can not use it
     for allocating an extent and end up being forced to allocate a new one;
  2) Because we only check that the metadata space_info has enough free bytes,
     we end up not allocating a new metadata chunk in that case. However if
     the filesystem was mounted in degraded mode, none of the existing block
     groups might be suitable for extent allocation due to their incompatible
     profile (for e.g. mounting a 2 devices filesystem, where all block groups
     use a RAID1 profile, in degraded mode using a single device). In this case
     when the task attempts to COW some extent buffer of the extent btree for
     example, it will trigger allocation of a new metadata block group with a
     suitable profile (SINGLE profile in the example of the degraded mount of
     the RAID1 filesystem);
  3) The task has reserved enough transaction units  metadata space, but when
     it attempts to COW an extent buffer from the extent or device btree for
     example, it does not find any free extent in any metadata block group,
     therefore forced to try to allocate a new metadata block group.
     This is because some other task allocated all available extents in the
     meanwhile - this typically happens with tasks that don't reserve space
     properly, either intentionally or as a bug. One example where this is
     done intentionally is fsync, as it does not reserve any transaction units
     and ends up allocating a variable number of metadata extents for log
     tree extent buffers;
  4) The task has reserved enough transaction units  metadata space, but right
     before it tries to allocate the last extent buffer it needs, a discard
     operation comes in and, temporarily, removes the last free space entry from
     the only metadata block group that had free space (discard starts by
     removing a free space entry from a block group, then does the discard
     operation and, once it's done, it adds back the free space entry to the
     block group).
  We also need this 2 phases setup when adding a device to a filesystem with
  a seed device - we must create new metadata and system chunks without adding
  any of the block group items to the chunk, extent and device btrees. If we
  did not do it this way, we would get ENOSPC when attempting to update those
  btrees, since all the chunks from the seed device are read-only.
  Phase 1 does the updates and insertions to the chunk btree because if we had
  it done in phase 2 and have a thundering herd of tasks allocating chunks in
  parallel, we risk having too many system chunks allocated by many tasks if
  many tasks reach phase 1 without the previous ones completing phase 2. In the
  extreme case this leads to exhaustion of the system chunk array in the
  superblock. This is easier to trigger if using a btree nodeleaf size of 64K
  and with RAID filesystems (so we have more device items in the chunk btree).
  This has happened before and commit eafa4fd0ad0607 ("btrfs: fix exhaustion of
  the system chunk array due to concurrent allocations") provides more details.
  Allocation of system chunks does not happen through this function. A task that
  needs to update the chunk btree (the only btree that uses system chunks), must
  preallocate chunk space by calling either check_system_chunk() or
  btrfs_reserve_chunk_metadata() - the former is used when allocating a data or
  metadata chunk or when removing a chunk, while the later is used before doing
  a modification to the chunk btree - use cases for the later are adding,
  removing and resizing a device as well as relocation of a system chunk.
  See the comment below for more details.
  The reservation of system space, done through check_system_chunk(), as well
  as all the updates and insertions into the chunk btree must be done while
  holding fs_info->chunk_mutex. This is important to guarantee that while COWing
  an extent buffer from the chunks btree we never trigger allocation of a new
  system chunk, which would result in a deadlock (trying to lock twice an
  extent buffer of the chunk btree, first time before triggering the chunk
  allocation and the second time during chunk allocation while attempting to
  update the chunks btree). The system chunk array is also updated while holding
  that mutex. The same logic applies to removing chunks - we must reserve system
  space, update the chunk btree and the system chunk array in the superblock
  while holding fs_info->chunk_mutex.
  This function, btrfs_chunk_alloc(), belongs to phase 1.
  If @force is CHUNK_ALLOC_FORCE:
     - return 1 if it successfully allocates a chunk,
     - return errors including -ENOSPC otherwise.
  If @force is NOT CHUNK_ALLOC_FORCE:
     - return 0 if it doesn't need to allocate a new chunk,
     - return 1 if it successfully allocates a chunk,
     - return errors including -ENOSPC otherwise.
 Don't re-enter if we're already allocating a chunk 
	
	  Allocation of system chunks can not happen through this path, as we
	  could end up in a deadlock if we are allocating a data or metadata
	  chunk and there is another task modifying the chunk btree.
	 
	  This is because while we are holding the chunk mutex, we will attempt
	  to add the new chunk item to the chunk btree or update an existing
	  device item in the chunk btree, while the other task that is modifying
	  the chunk btree is attempting to COW an extent buffer while holding a
	  lock on it and on its parent - if the COW operation triggers a system
	  chunk allocation, then we can deadlock because we are holding the
	  chunk mutex and we may need to access that extent buffer or its parent
	  in order to add the chunk item or update a device item.
	 
	  Tasks that want to modify the chunk tree should reserve system space
	  before updating the chunk btree, by calling either
	  btrfs_reserve_chunk_metadata() or check_system_chunk().
	  It's possible that after a task reserves the space, it still ends up
	  here - this happens in the cases described above at do_chunk_alloc().
	  The task will have to either retry or fail.
 No more free physical space 
			
			  Someone is already allocating, so we need to block
			  until this someone is finished and then loop to
			  recheck if we should continue with our allocation
			  attempt.
 Proceed with allocation 
	
	  If we have mixed datametadata chunks we want to make sure we keep
	  allocating mixed chunks instead of individual chunks.
	
	  if we're doing a data chunk, go ahead and make sure that
	  we keep a reasonable number of metadata chunks allocated in the
	  FS as well.
	
	  Needed because we can end up allocating a system chunk and for an
	  atomic and race free space reservation in the chunk block reserve.
		
		  Ignore failure to create system chunk. We might end up not
		  needing it, as we might not need to COW all nodesleafs from
		  the paths we visit in the chunk tree (they were already COWed
		  or created in the current transaction for example).
			
			  If we fail to add the chunk item here, we end up
			  trying again at phase 2 of chunk allocation, at
			  btrfs_create_pending_block_groups(). So ignore
			  any error here. An ENOSPC here could happen, due to
			  the cases described at do_chunk_alloc() - the system
			  block group we just created was just turned into RO
			  mode by a scrub for example, or a running discard
			  temporarily removed its free space entries, etc.
  Reserve space in the system space for allocating or removing a chunk.
  The caller must be holding fs_info->chunk_mutex.
 num_devs device items to update and 1 chunk item to add or remove. 
  Reserve space in the system space, if needed, for doing a modification to the
  chunk btree.
  @trans:		A transaction handle.
  @is_item_insertion:	Indicate if the modification is for inserting a new item
 			in the chunk btree or if it's for the deletion or update
 			of an existing item.
  This is used in a context where we need to update the chunk btree outside
  block group allocation and removal, to avoid a deadlock with a concurrent
  task that is allocating a metadata or data block group and therefore needs to
  update the chunk btree while holding the chunk mutex. After the update to the
  chunk btree is done, btrfs_trans_release_chunk_metadata() should be called.
  Must be called only after stopping all workers, since we could have block
  group caching kthreads running, and therefore they could race with us if we
  freed the block groups before stopping them.
		
		  We haven't cached this block group, which means we could
		  possibly have excluded extents on this block group.
		
		  Do not hide this behind enospc_debug, this is actually
		  important and indicates a real bug if this happens.
 logic error, can't happen 
 once for us and once for the tree 
		
		  We may have left one free space entry and other possible
		  tasks trimming this block group have left 1 entry each one.
		  Free them if any.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2012 Alexander Block.  All rights reserved.
  Maximum number of references an extent can have in order for us to attempt to
  issue clone operations instead of write operations. This currently exists to
  avoid hitting limitations of the backreference walking code (taking a lot of
  time and using too much memory for extents with large number of references).
  A fs_path is a helper to dynamically build path names with unknown size.
  It reallocates the internal buffer on demand.
  It allows fast adding of path elements on the right side (normal path) and
  fast adding to the left side (reversed path). A reversed path can also be
  unreversed if needed.
		
		  Average path length does not exceed 200 bytes, we'll have
		  better packing in the slab and higher chance to satisfy
		  a allocation later during send.
 reused for each extent 
 'flags' member of btrfs_ioctl_send_args is u64 
 Protocol version compatibility requested 
 current state of the compare_tree call 
	
	  infos of the currently processed inode. In case of deleted inodes,
	  these are the values from the deleted inode.
	
	  We process inodes by their increasing order, so if before an
	  incremental send we reverse the parentchild relationship of
	  directories such that a directory with a lower inode number was
	  the parent of a directory with a higher inode number, and the one
	  becoming the new parent got renamed too, we can't renamemove the
	  directory with lower inode number when we finish processing it - we
	  must process the directory with higher inode number first, then
	  renamemove it and then renamemove the directory with lower inode
	  number. Example follows.
	 
	  Tree state when the first send was performed:
	 
	  .
	  |-- a                   (ino 257)
	      |-- b               (ino 258)
	          |
	          |
	          |-- c           (ino 259)
	          |   |-- d       (ino 260)
	          |
	          |-- c2          (ino 261)
	 
	  Tree state when the second (incremental) send is performed:
	 
	  .
	  |-- a                   (ino 257)
	      |-- b               (ino 258)
	          |-- c2          (ino 261)
	              |-- d2      (ino 260)
	                  |-- cc  (ino 259)
	 
	  The sequence of steps that lead to the second state was:
	 
	  mv abcd abc2d2
	  mv abc abc2d2cc
	 
	  "c" has lower inode number, but we can't move it (2nd mv operation)
	  before we move "d", which has higher inode number.
	 
	  So we just memorize which moverename operations must be performed
	  later when their respective parent is processed and movedrenamed.
 Indexed by parent directory inode number. 
	
	  Reverse index, indexed by the inode number of a directory that
	  is waiting for the moverename of its immediate parent before its
	  own moverename can be performed.
	
	  A directory that is going to be rm'ed might have a child directory
	  which is in the pending directory moves index above. In this case,
	  the directory can only be removed after the moverename of its child
	  is performed. Example:
	 
	  Parent snapshot:
	 
	  .                        (ino 256)
	  |-- a                   (ino 257)
	      |-- b               (ino 258)
	          |-- c           (ino 259)
	          |   |-- x       (ino 260)
	          |
	          |-- y           (ino 261)
	 
	  Send snapshot:
	 
	  .                        (ino 256)
	  |-- a                   (ino 257)
	      |-- b               (ino 258)
	          |-- YY          (ino 261)
	               |-- x      (ino 260)
	 
	  Sequence of steps that lead to the send snapshot:
	  rm -f abcfoo.txt
	  mv aby abYY
	  mv abcx abYY
	  rmdir abc
	 
	  When the child is processed, its moverename is delayed until its
	  parent is processed (as explained above), but all other operations
	  like update utimes, chown, chgrp, etc, are performed and the paths
	  that it uses for those operations must use the orphanized name of
	  its parent (the directory we're going to rm later), so we need to
	  memorize that name.
	 
	  Indexed by the inode number of the directory to be deleted.
	
	  There might be some directory that could not be removed because it
	  was waiting for this directory inode to be moved first. Therefore
	  after this directory is moved, we can try to rmdir the ino rmdir_ino.
	
	  radix_tree has only 32bit entries but we need to handle 64bit inums.
	  We use the lower 32bit of the 64bit inum to store it in the tree. If
	  more then one inum would fall into the same entry, we use radix_list
	  to store the additional entries. radix_list is also used to store
	  entries where two entries have the same inum but different
	  generations.
	
	  First time the inline_buf does not suffice
	
	  The real size of the buffer is bigger, this will let the fast path
	  happen most of the time
 TODO handle that correctly 
		if (ret == -ERESTARTSYS) {
			continue;
  For each commanditem we want to send to userspace, we call this function.
  Sends a move instruction to user space
  Sends a link instruction to user space
  Sends an unlink instruction to user space
  Sends a rmdir instruction to user space
  Helper function to retrieve some fields from an inode item.
  Helper function to iterate the entries in ONE btrfs_inode_ref or
  btrfs_inode_extref.
  The iterate callback may return a non zero value to stop iteration. This can
  be a negative value for error codes or 1 to simply stop it.
  path must point to the INODE_REF or INODE_EXTREF when called.
 overflow , try again with larger buffer 
  Helper function to iterate the entries in ONE btrfs_dir_item.
  The iterate callback may return a non zero value to stop iteration. This can
  be a negative value for error codes or 1 to simply stop it.
  path must point to the dir item when called.
	
	  Start with a small buffer (1 page). If later we end up needing more
	  space, which can happen for xattrs on a fs with a leaf size greater
	  then the page size, attempt to increase the buffer. Typically xattr
	  values are small.
			
			  Path too long
 we want the first only 
  Retrieve the first path of an inode. If an inode has more then one
  refhardlink, this is ignored.
 number of total found references 
	
	  used for clones found in send_root. clones found behind cur_objectid
	  and cur_offset are not considered as allowed clones.
 may be truncated in case it's the last extent in a file 
 Just to check for bugs in backref resolving 
  Called for every backref that is found for the current extent.
  Results are collected in sctx->clone_roots->inooffsetfound_refs
 First check if the root is in the list of accepted clone sources 
	
	  Make sure we don't consider clones from send_root that are
	  behind the current inodeoffset.
		
		  If the source inode was not yet processed we can't issue a
		  clone operation, as the source extent does not exist yet at
		  the destination of the stream.
		
		  We clone from the inode currently being sent as long as the
		  source extent is already processed, otherwise we could try
		  to clone from an extent that does not exist yet at the
		  destination of the stream.
		
		  same extent found more then once in the same file.
  Given an inode, offset and extent item, it finds a good clone for a clone
  instruction. Returns -ENOENT when none could be found. The function makes
  sure that the returned clone is usable at the point where sending is at the
  moment. This means, that no clones are accepted which lie behind the current
  inode+offset.
  path must point to the extent item when called.
 We only use this path under the commit sem 
		
		  There may be extents that lie behind the file's size.
		  I at least had this in combination with snapshotting while
		  writing large files.
	
	  Backreference walking (iterate_extent_inodes() below) is currently
	  too expensive when an extent has a large number of references, both
	  in time spent and used memory. So for now just fallback to write
	  operations instead of clone operations when an extent has more than
	  a certain amount of references.
	
	  Setup the clone roots.
	
	  The last extent of a file may be too large due to page alignment.
	  We need to adjust extent_len in this case so that the checks in
	  __iterate_backrefs work.
	
	  Now collect all backrefs.
 found a bug in backref code? 
 prefer clones from send_root over others 
		
		  An empty symlink inode. Can happen in rare error paths when
		  creating a symlink (transaction committed before the inode
		  eviction handler removed the symlink inode items and a crash
		  happened in between or the subvol was snapshoted in between).
		  Print an informative message to dmesgsyslog so that the user
		  can delete the symlink.
  Helper function to generate a file name that is unique in the root of
  send_root and parent_root. This is used to generate names for orphan inodes.
 not unique, try again 
 unique 
 not unique, try again 
 unique 
  Helper function to lookup a dir item in a dir.
  Looks up the first btrfs_inode_ref of a given ino. It returns the parent dir,
  generation of the parent dir and the name of the dir entry.
  Used by process_recorded_refs to determine if a new ref would overwrite an
  already existing ref. In case it detects an overwrite, it returns the
  inodegen in who_inowho_gen.
  When an overwrite is detected, process_recorded_refs does proper orphanizing
  to make sure later references to the overwritten inode are possible.
  Orphanizing is however only required for the first ref of an inode.
  process_recorded_refs does an additional is_first_ref check to see if
  orphanizing is really required.
	
	  If we have a parent root we need to verify that the parent dir was
	  not deleted and then re-created, if it was then we have no overwrite
	  and we can just unlink this entry.
	
	  Check if the overwritten ref was already processed. If yes, the ref
	  was already unlinkedmoved, so we can safely assume that we will not
	  overwrite anything at this point in time.
  Checks if the ref was overwritten by an already processed inode. This is
  used by __get_cur_name_and_parent to find out if the ref was orphanized and
  thus the orphan name needs be used.
  process_recorded_refs also uses it to avoid unlinking of refs that were
  overwritten.
 check if the ref was overwritten by another ref 
 was never and will never be overwritten 
	
	  We know that it is or will be overwritten. Check this now.
	  The current inode being processed might have been the one that caused
	  inode 'ino' to be orphanized, therefore check if ow_inode matches
	  the current inode being processed.
  Same as did_overwrite_ref, but also checks if it is the first ref of an inode
  that got overwritten. This is used by process_recorded_refs to determine
  if it has to use the path as returned by get_cur_path or the orphan name.
  Insert a name cache entry. On 32bit kernels the radix tree index is 32bit,
  so we need to do some special handling in case we have clashes. This function
  takes care of this with the help of name_cache_entry::radix_list.
  In case of error, nce is kfreed.
	
	  We may not get to the final release of nce_head if the lookup fails
  Remove some entries from the beginning of name_cache_list.
  Used by get_cur_path for each ref up to the root.
  Returns 0 if it succeeded.
  Returns 1 if the inode is not existent or got overwritten. In that case, the
  name is an orphan name. This instructs get_cur_path to stop iterating. If 1
  is returned, parent_inoparent_gen are not guaranteed to be valid.
  Returns <0 in case of error.
	
	  First check if we already did a call to this function with the same
	  inogen. If yes, check if the cache entry is still up-to-date. If yes
	  return the cached result.
			
			  Removes the entry from the list and adds it back to
			  the end.  This marks the entry as recently used so
			  that name_cache_clean_unused does not remove it.
	
	  If the inode is not existent yet, add the orphan name and return 1.
	  This should only happen for the parent dir that we determine in
	  __record_new_ref
	
	  Depending on whether the inode was already processed or not, use
	  send_root or parent_root for ref lookup.
	
	  Check if the ref was overwritten by an inode's ref that was processed
	  earlier. If yes, treat as orphan and return 1.
	
	  Store the result of the lookup in the name cache.
  Magic happens here. This function returns the first ref to an inode as it
  would look like while receiving the stream at this point in time.
  We walk the path up to the root. For every inode in between, we check if it
  was already processedsent. If yes, we continue with the parent as found
  in send_root. If not, we continue with the parent as found in parent_root.
  If we encounter an inode that was deleted at this point in time, we use the
  inodes "orphan" name instead of the real name and stop. Same with new inodes
  that were not created yet and overwritten inodesrefs.
  When do we have orphan inodes:
  1. When an inode is freshly created and thus no valid refs are available yet
  2. When a directory lost all it's refs (deleted) but still has dir items
     inside which were not processed yet (pending for movedelete). If anyone
     tried to get the path to the dir items, it would get a path inside that
     orphan directory.
  3. When an inode is moved around or gets new links, it may overwrite the ref
     of an unprocessed inode. If in that case the first ref would be
     overwritten, the overwritten inode gets "orphanized". Later when we
     process this overwritten inode, it is restored at a new place by moving
     the orphan inode.
  sctx->send_progress tells this function at which point in time receiving
  would be.
  Sends a BTRFS_SEND_C_SUBVOL commanditem to userspace
 TODO Add otime support when the otime patches get into upstream 
  Sends a BTRFS_SEND_C_MKXXX or SYMLINK command to user space. We don't have
  a valid path yet because we did not process the refs yet. So, the inode
  is created as orphan.
  We need some special handling for inodes that get processed before the parent
  directory got created. See process_recorded_refs for details.
  This function does the check if we already created the dir out of order.
  Only creates the inode if it is:
  1. Not a directory
  2. Or a directory which was not created already due to out of order
     directories. See did_create_dir and process_recorded_refs for details.
  We need to process new refs before deleted refs, but compare_tree gives us
  everything mixed. So we first record all refs and later process them.
  This function is a helper to record one ref.
  Renamesmoves a filedir to its orphan name. Used when the first
  ref of an unprocessed inode gets overwritten and for all non empty
  directories.
  Returns 1 if a directory can be removed at this point in time.
  We check this by iterating all dir items and checking if the inode behind
  the dir item was already processed.
	
	  Don't try to rmdir the toproot subvolume dir.
 already deleted 
	
	  After renamemove, need to update the utimes of both new parent(s)
	  and old parent(s).
		
		  The parent inode might have been deleted in the send snapshot
  We might need to delay a directory rename even when no ancestor directory
  (in the send root) with a higher inode number than ours (sctx->cur_ino) was
  renamed. This happens when we rename a directory to the old name (the name
  in the parent root) of some other unrelated directory that got its rename
  delayed due to some ancestor with higher number that got renamed.
  Example:
  Parent snapshot:
  .                                       (ino 256)
  |---- a                                (ino 257)
  |     |---- file                        (ino 260)
  |
  |---- b                                (ino 258)
  |---- c                                (ino 259)
  Send snapshot:
  .                                       (ino 256)
  |---- a                                (ino 258)
  |---- x                                (ino 259)
        |---- y                          (ino 257)
              |----- file                 (ino 260)
  Here we can not rename 258 from 'b' to 'a' without the rename of inode 257
  from 'a' to 'xy' happening first, which in turn depends on the rename of
  inode 259 from 'c' to 'x'. So the order of rename commands the send stream
  must issue is:
  1 - rename 259 from 'c' to 'x'
  2 - rename 257 from 'a' to 'xy'
  3 - rename 258 from 'b' to 'a'
  Returns 1 if the rename of sctx->cur_ino needs to be delayed, 0 if it can
  be done right away and < 0 on error.
	
	  di_key.objectid has the number of the inode that has a dentry in the
	  parent directory with the same name that sctx->cur_ino is being
	  renamed to. We need to check if that inode is in the send root as
	  well and if it is currently marked as an inode with a pending rename,
	  if it is, we need to delay the rename of sctx->cur_ino as well, so
	  that it happens after that other inode is renamed.
 Different inode, no need to delay the rename of sctx->cur_ino 
  Check if inode ino2, or any of its ancestors, is inode ino1.
  Return 1 if true, 0 if false and < 0 on error.
  Check if ino ino1 is an ancestor of inode ino2 in the given root for any
  possible path (in case ino2 is not a directory and has multiple hard links).
  Return 1 if true, 0 if false and < 0 on error.
	
	  Our current directory inode may not yet be renamedmoved because some
	  ancestor (immediate or not) has to be renamedmoved first. So find if
	  such ancestor exists and make sure our own renamemove happens after
	  that ancestor is processed to avoid path build infinite loops (done
	  at get_cur_path()).
			
			  If the current inode is an ancestor of ino in the
			  parent root, we need to delay the rename of the
			  current inode, otherwise don't delayed the rename
			  because we can end up with a circular dependency
			  of renames, resulting in some directories never
			  getting the respective rename operations issued in
			  the send stream or getting into infinite path build
			  loops.
	
	  Our reference's name member points to its full_path member string, so
	  we use here a new path.
  When processing the new references for an inode we may orphanize an existing
  directory inode because its old name conflicts with one of the new references
  of the current inode. Later, when processing another new reference of our
  inode, we might need to orphanize another inode, but the path we have in the
  reference reflects the pre-orphanization name of the directory we previously
  orphanized. For example:
  parent snapshot looks like:
  .                                     (ino 256)
  |----- f1                             (ino 257)
  |----- f2                             (ino 258)
  |----- d1                            (ino 259)
         |----- d2                     (ino 260)
  send snapshot looks like:
  .                                     (ino 256)
  |----- d1                             (ino 258)
  |----- f2                            (ino 259)
         |----- f2_link                (ino 260)
         |       |----- f1              (ino 257)
         |
         |----- d2                      (ino 258)
  When processing inode 257 we compute the name for inode 259 as "d1", and we
  cache it in the name cache. Later when we start processing inode 258, when
  collecting all its new references we set a full path of "d1d2" for its new
  reference with name "d2". When we start processing the new references we
  start by processing the new reference with name "d1", and this results in
  orphanizing inode 259, since its old reference causes a conflict. Then we
  move on the next new reference, with name "d2", and we find out we must
  orphanize inode 260, as its old reference conflicts with ours - but for the
  orphanization we use a source path corresponding to the path we stored in the
  new reference, which is "d1d2" and not "o259-6-0d2" - this makes the
  receiver fail since the path component "d1" no longer exists, it was renamed
  to "o259-6-0" when processing the previous new reference. So in this case we
  must recompute the path in the new reference and use it for the new
  orphanization operation.
 Update the reference's base name pointer. 
  This does all the movelinkunlinkrmdir magic.
	
	  This should never happen as the root dir always has the same ref
	  which is always '..'
	
	  First, check if the first ref of the current inode was overwritten
	  before. If yes, we know that the current inode was already orphanized
	  and thus use the orphan name. If not, we can use get_cur_path to
	  get the path of the first ref as it would like while receiving at
	  this point in time.
	  New inodes are always orphan at the beginning, so force to use the
	  orphan name in this case.
	  The first ref is stored in valid_path and will be updated if it
	  gets moved around.
	
	  Before doing any rename and link operations, do a first pass on the
	  new references to orphanize any unprocessed inodes that may have a
	  reference that conflicts with one of the new references of the current
	  inode. This needs to happen first because a new reference may conflict
	  with the old reference of a parent directory, so we must make sure
	  that the path used for link and rename commands don't use an
	  orphanized name when an ancestor was not yet orphanized.
	 
	  Example:
	 
	  Parent snapshot:
	 
	  .                                                      (ino 256)
	  |----- testdir                                        (ino 259)
	  |          |----- a                                    (ino 257)
	  |
	  |----- b                                               (ino 258)
	 
	  Send snapshot:
	 
	  .                                                      (ino 256)
	  |----- testdir_2                                      (ino 259)
	  |          |----- a                                    (ino 260)
	  |
	  |----- testdir                                         (ino 257)
	  |----- b                                               (ino 257)
	  |----- b2                                              (ino 258)
	 
	  Processing the new reference for inode 257 with name "b" may happen
	  before processing the new reference with name "testdir". If so, we
	  must make sure that by the time we send a link command to create the
	  hard link "b", inode 259 was already orphanized, since the generated
	  path in "valid_path" already contains the orphanized name for 259.
	  We are processing inode 257, so only later when processing 259 we do
	  the rename operation to change its temporary (orphanized) name to
	  "testdir_2".
		
		  Check if this new ref would overwrite the first ref of another
		  unprocessed inode. If yes, orphanize the overwritten inode.
		  If we find an overwritten ref that is not the first ref,
		  simply unlink it.
				
				  If ow_inode has its rename operation delayed
				  make sure that its orphanized name is used in
				  the source path when performing its rename
				  operation.
				
				  Make sure we clear our orphanized inode's
				  name from the name cache. This is because the
				  inode ow_inode might be an ancestor of some
				  other inode that will be orphanized as well
				  later and has an inode number greater than
				  sctx->send_progress. We need to prevent
				  future name lookups from using the old name
				  and get instead the orphan name.
				
				  ow_inode might currently be an ancestor of
				  cur_ino, therefore compute valid_path (the
				  current path of cur_ino) again because it
				  might contain the pre-orphanization name of
				  ow_inode, which is no longer valid.
				
				  If we previously orphanized a directory that
				  collided with a new reference that we already
				  processed, recompute the current path because
				  that directory may be part of the path.
		
		  We may have refs where the parent directory does not exist
		  yet. This happens if the parent directories inum is higher
		  than the current inum. To handle this case, we create the
		  parent directory out of order. But we need to check if this
		  did already happen before due to other refs in the same dir.
			
			  First check if any of the current inodes refs did
			  already create the dir.
			
			  If that did not happen, check if a previous inode
			  did already create the dir.
		
		  linkmove the ref to the new place. If we have an orphan
		  inode, move it and update valid_path. If not, link or move
		  it depending on the inode mode.
				
				  Dirs can't be linked, so move it. For moved
				  dirs, we always have one new and one deleted
				  ref. The deleted ref is ignored later.
				
				  We might have previously orphanized an inode
				  which is an ancestor of our current inode,
				  so our reference's full path, which was
				  computed before any such orphanizations, must
				  be updated.
		
		  Check if we can already rmdir the directory. If not,
		  orphanize it. For every dir item inside that gets deleted
		  later, we do this check again and rmdir it then if possible.
		  See the use of check_dirs for more details.
		
		  We have a moved dir. Add the old parent to check_dirs
		
		  We have a non dir inode. Go through all deleted refs and
		  unlink them if they were not already overwritten by other
		  inodes.
				
				  If we orphanized any ancestor before, we need
				  to recompute the full path for deleted names,
				  since any such path was computed before we
				  processed any references and orphanized any
				  ancestor inode.
		
		  If the inode is still orphan, unlink the orphan. This may
		  happen when a previous inode did overwrite the first ref
		  of this inode and no new refs were added for the current
		  inode. Unlinking does not mean that the inode is deleted in
		  all cases. There may still be links to this inode in other
		  places.
	
	  We did collect all parent dirs where cur_inode was once located. We
	  now go through all these dirs and check if they are pending for
	  deletion and if it's finally possible to perform the rmdir now.
	  We also update the inode stats of the parent dirs here.
		
		  In case we had refs into dirs that were not processed yet,
		  we don't need to do the utime and rmdir logic for these dirs.
		  The dir will be processed later.
 TODO delayed utimes 
		
		  To avoid doing extra lookups we'll only do this if everything
		  else matches.
  Record and process all refs at once. Needed when an inode changes the
  generation number, which means that it was deleted and recreated.
	
	  We don't actually care about pending_move as we are simply
	  re-creating this inode and will be rename'ing it into place once we
	  rename the parent directory.
 Capabilities are emitted by finish_inode_if_needed 
	
	  This hack is needed because empty acls are stored as zero byte
	  data in xattrs. Problem with that is, that receiving these zero byte
	  acls will fail later. To fix this, we send a dummy acl list that
	  only contains the version number and no entries.
 initial readahead 
  Read some bytes from the current inodefile and send a write command to
  user space.
  Send a clone command to user space.
	
	  If the parent we're using has a received_uuid set then use that as
	  our clone source as that is what we will look for when doing a
	  receive.
	 
	  This covers the case that we create a snapshot off of a received
	  subvolume and then use that as the parent and try to receive on a
	  different host.
  Send an update extent command to user space.
	
	  A hole that starts at EOF or beyond it. Since we do not yet support
	  fallocate (for extent preallocation and hole punching), sending a
	  write of zeroes starting at EOF or beyond would later require issuing
	  a truncate operation which would undo the write and achieve nothing.
	
	  Don't go beyond the inode's i_size due to prealloc extents that start
	  after the i_size.
  Search for a capability xattr related to sctx->cur_ino. If the capability is
  found, call send_set_xattr function to emit it.
  Return 0 if there isn't a capability, or when the capability was emitted
  successfully, or < 0 if an error occurred.
 There is no xattr for this inode 
	
	  Prevent cloning from a zero offset with a length matching the sector
	  size because in some scenarios this will make the receiver fail.
	 
	  For example, if in the source filesystem the extent at offset 0
	  has a length of sectorsize and it was written using direct IO, then
	  it can never be an inline extent (even if compression is enabled).
	  Then this extent can be cloned in the original filesystem to a non
	  zero file offset, but it may not be possible to clone in the
	  destination filesystem because it can be inlined due to compression
	  on the destination filesystem (as the receiver's write operations are
	  always done using buffered IO). The same happens when the original
	  filesystem does not have compression enabled but the destination
	  filesystem has.
	
	  There are inodes that have extents that lie behind its i_size. Don't
	  accept clones from these extents.
	
	  We can't send a clone operation for the entire range if we find
	  extent items in the respective range in the source file that
	  refer to different extents or if we find holes.
	  So check for that and do a mix of clone and regular writecopy
	  operations if needed.
	 
	  Example:
	 
	  mkfs.btrfs -f devsda
	  mount devsda mnt
	  xfs_io -f -c "pwrite -S 0xaa 0K 100K" mntfoo
	  cp --reflink=always mntfoo mntbar
	  xfs_io -c "pwrite -S 0xbb 50K 50K" mntfoo
	  btrfs subvolume snapshot -r mnt mntsnap
	 
	  If when we send the snapshot and we are processing file bar (which
	  has a higher inode number than foo) we blindly send a clone operation
	  for the [0, 100K[ range from foo to bar, the receiver ends up getting
	  a file bar that matches the content of file foo - iow, doesn't match
	  the content from bar in the original filesystem.
		
		  We might have an implicit trailing hole (NO_HOLES feature
		  enabled). We deal with it after leaving this loop.
 Implicit hole, NO_HOLES feature enabled. 
			
			  We can't clone the last block, when its size is not
			  sector size aligned, into the middle of a file. If we
			  do so, the receiver will get a failure (-EINVAL) when
			  trying to clone or will silently corrupt the data in
			  the destination file if it's on a kernel without the
			  fix introduced by commit ac765f83f1397646
			  ("Btrfs: fix data corruption due to cloning of eof
			  block).
			 
			  So issue a clone of the aligned down range plus a
			  regular write for the eof block, if we hit that case.
			 
			  Also, we use the maximum possible sector size, 64K,
			  because we don't know what's the sector size of the
			  filesystem that receives the stream, so we have to
			  assume the largest possible sector size.
		
		  If we are cloning from the file we are currently processing,
		  and using the send root as the clone root, we must stop once
		  the current clone offset reaches the current eof of the file
		  at the receiver, otherwise we would issue an invalid clone
		  operation (source range going beyond eof) and cause the
		  receiver to fail. So if we reach the current eof, bail out
		  and fallback to a regular write.
	
	  Following comments will refer to these graphics. L is the left
	  extents which we are checking at the moment. 1-8 are the right
	  extents that we iterate.
	 
	        |-----L-----|
	  |-1-|-2a-|-3-|-4-|-5-|-6-|
	 
	        |-----L-----|
	  |--1--|-2b-|...(same as above)
	 
	  Alternative situation. Happens on files where extents got split.
	        |-----L-----|
	  |-----------7-----------|-6-|
	 
	  Alternative situation. Happens on files which got larger.
	        |-----L-----|
	  |-8-|
	  Nothing follows after 8.
	
	  Handle special case where the right side has no extents at all.
 If we're a hole then just pretend nothing changed 
	
	  We're now on 2a, 2b or 7.
		
		  Are we at extent 8? If yes, we know the extent is changed.
		  This may only happen on the first iteration.
 If we're a hole just pretend nothing changed 
		
		  We just wanted to see if when we have an inline extent, what
		  follows it is a regular extent (wanted to check the above
		  condition for inline extents too). This should normally not
		  happen but it's possible for example when we have an inline
		  compressed extent representing data with a size matching
		  the page size (currently the same as sector size).
 Fix the right offset for 2a and 7. 
 Fix the left offset for all behind 2a and 2b 
		
		  Check if we have the same extent.
		
		  Go to the next extent.
	
	  We're now behind the left extent (treat as unchanged) or at the end
	  of the right side (treat as changed).
		
		  We might have skipped entire leafs that contained only
		  file extent items for our current inode. These leafs have
		  a generation number smaller (older) than the one in the
		  current leaf and the leaf our last extent came from, and
		  are located between these 2 leafs.
			
			  The send spec does not have a prealloc command yet,
			  so just leave a hole for prealloc'ed extents until
			  we have enough commands queued up to justify rev'ing
			  the send spec.
 Have a hole, just skip it. 
	
	  We have processed the refs and thus need to advance send_progress.
	  Now, calls to get_cur_xxx will take the updated refs of the current
	  inode into account.
	 
	  On the other hand, if our current inode is a directory and couldn't
	  be movedrenamed because its parent was renamedmoved too and it has
	  a higher inode number, we can only moverename our current inode
	  after we movedrenamed its parent. Therefore in this case operate on
	  the old path (pre moverename) of our current inode, and the
	  moverename will be performed later.
	
	  If other directory inodes depended on our current directory
	  inode's moverename, now do their moverename operations.
		
		  Need to send that every time, no matter if it actually
		  changed between the two trees as we have done changes to
		  the inode before. If our inode is a directory and it's
		  waiting to be movedrenamed, we will send its utimes when
		  it's movedrenamed, therefore we don't need to do it here.
  Issue unlink operations for all paths of the current inode found in the
  parent snapshot.
	
	  Set send_progress to current inode. This will tell all get_cur_xxx
	  functions that the current inode's refs are not updated yet. Later,
	  when process_recorded_refs is finished, it is set to cur_ino + 1.
		
		  The cur_ino = root dir case is special here. We can't treat
		  the inode as deleted+reused because it would generate a
		  stream that tries to deletemkdir the root dir.
	
	  Normally we do not find inodes with a link count of zero (orphans)
	  because the most common case is to create a snapshot and use it
	  for a send operation. However other less common use cases involve
	  using a subvolume and send it after turning it to RO mode just
	  after deleting all hard links of a file while holding an open
	  file descriptor against it or turning a RO snapshot into RW mode,
	  keep an open file descriptor against a file, delete it and then
	  turn the snapshot back to RO mode before using it for a send
	  operation. So if we find such cases, ignore the inode and all its
	  items completely if it's a new inode, or if it's a changed inode
	  make sure all its previous paths (from the parent snapshot) are all
	  unlinked and all other the inode items are ignored.
		
		  We need to do some special handling in case the inode was
		  reported as changed with a changed generation number. This
		  means that the original inode was deleted and new inode
		  reused the same inum. So we have to treat the old inode as
		  deleted and the new one as new.
			
			  First, process the inode as if it was deleted.
			
			  Now process the inode as if it was new.
			
			  Advance send_progress now as we did not get into
			  process_recorded_refs_if_needed in the new_gen case.
			
			  Now process all extents and xattrs of the inode as if
			  they were all new.
  We have to process new refs before deleted refs, but compare_trees gives us
  the new and deleted refs mixed. To fix this, we record the newdeleted refs
  first and later process them in process_recorded_refs.
  For the cur_inode_new_gen case, we skip recording completely because
  changed_inode did already initiate processing of refs. The reason for this is
  that in this case, compare_tree actually compares the refs of 2 different
  inodes. To fix this, process_all_refs is used in changed_inode to handle all
  refs of the right tree as deleted and all refs of the left tree as new.
  Process newdeletedchanged xattrs. We skip processing in the
  cur_inode_new_gen case because changed_inode did already initiate processing
  of xattrs. The reason is the same as in changed_ref
  Process newdeletedchanged extents. We skip processing in the
  cur_inode_new_gen case because changed_inode did already initiate processing
  of extents. The reason is the same as in changed_ref
	
	  We have found an extent item that changed without the inode item
	  having changed. This can happen either after relocation (where the
	  disk_bytenr of an extent item is replaced at
	  relocation.c:replace_file_extents()) or after deduplication into a
	  file in both the parent and send snapshots (where an extent item can
	  get modified or replaced with a new one). Note that deduplication
	  updates the inode item, but it only changes the iversion (sequence
	  field in the inode item) of the inode, so if a file is deduplicated
	  the same amount of times in both the parent and send snapshots, its
	  iversion becomes the same in both snapshots, whence the inode item is
	  the same on both snapshots.
 Easy case, just check this one dirid 
  Updates compare related fields in sctx and simply forwards to the actual
  changed_xxx functions.
 Ignore non-FS objects 
	
	  Trigger readahead for the next leaves we will process, so that it is
	  very likely that when we need them they are already in memory and we
	  will not block on disk IO. For nodes we only do readahead for one,
	  since the time window between processing nodes is typically larger.
 move upnext 
  Returns 1 if it had to move up and next. 0 is returned if it moved only next
  or down.
  This function compares two trees and calls the provided callback for
  every changednewdeleted item it finds.
  If shared tree blocks are encountered, whole subtrees are skipped, making
  the compare pretty fast on snapshotted subvolumes.
  This currently works on commit roots only. As commit roots are read only,
  we don't do any locking. The commit roots are protected with transactions.
  Transactions are ended and rejoined when a commit is tried in between.
  This function checks for modifications done to the trees while comparing.
  If it detects a change, it aborts immediately.
	
	  Strategy: Go to the first items of both trees. Then do
	 
	  If both trees are at level 0
	    Compare keys of current items
	      If left < right treat left item as new, advance left tree
	        and repeat
	      If left > right treat right item as deleted, advance right tree
	        and repeat
	      If left == right do deep compare of items, treat as changed if
	        needed, advance both trees and repeat
	  If both trees are at the same level but not at level 0
	    Compare keys of current nodesleafs
	      If left < right advance left tree and repeat
	      If left > right advance right tree and repeat
	      If left == right compare blockptrs of the next nodesleafs
	        If they match advance both trees but stay at the same level
	          and repeat
	        If they don't match advance both trees while allowing to go
	          deeper and repeat
	  If tree levels are different
	    Advance the tree that needs it and repeat
	 
	  Advancing a tree means:
	    If we are at level 0, try to go to the next slot. If that's not
	    possible, go one level up and repeat. Stop when we found a level
	    where we could go to the next slot. We may at this point be on a
	    node or a leaf.
	 
	    If we are not at level 0 and not on shared tree blocks, go one
	    level deeper.
	 
	    If we are not at level 0 and on shared tree blocks, go one slot to
	    the right if possible or go up and right.
	
	  Our right root is the parent root, while the left root is the "send"
	  root. We know that all new nodesleaves in the left root must have
	  a generation greater than the right root's generation, so we trigger
	  readahead for those nodes and leaves of the left root, as we know we
	  will need to read them at some point.
					
					  As we're on a shared block, don't
					  allow to go deeper.
  If orphan cleanup did remove any orphans from a root, it means the tree
  was modified and therefore the commit root is not the same as the current
  root anymore. This is a problem, because send uses the commit root and
  therefore can see inode items that don't exist in the current root anymore,
  and for example make calls to btrfs_iget, which will do tree lookups based
  on the current root and not on the commit root. Those lookups will fail,
  returning a -ESTALE error, and making send fail with that error. So make
  sure a send does not see any orphans we have just removed, and that it will
  see the same inodes regardless of whether a transaction commit happened
  before it started (meaning that the commit root will be the same as the
  current root) or not.
 Use any root, all fs roots will get their commit roots updated. 
  Make sure any existing dellaloc is flushed for any root used by a send
  operation so that we do not miss any data and we do not race with writeback
  finishing and changing a tree while send is using the tree. This could
  happen if a subvolume is in RW mode, has delalloc, is turned to RO mode and
  a send operation then uses the subvolume.
  After flushing delalloc ensure_commit_roots_uptodate() must be called.
	
	  Not much left to do, we don't know why it's unbalanced and
	  can't blindly reset it to 0.
	
	  The subvolume must remain read-only during send, protect against
	  making it RW. This also protects against deletion.
	
	  Userspace tools do the checks and warn the user if it's
	  not RO.
	
	  Check that we don't overflow at later allocations, we request
	  clone_sources_count + 1 items, and compare to unsigned long inside
	  access_ok.
 Zero means "use the highest version" 
	
	  Unlikely but possible, if the subvolume is marked for deletion but
	  is slow to remove the directory entry, send can still be started
	
	  Clones from send_root are allowed, but only if the clone source
	  is behind the current send position. This is checked while searching
	  for possible clone sources.
 We do a bsearch later 
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2014 Facebook.  All rights reserved.
  Used to keep track the roots and number of refs each root has for a given
  bytenr.  This just tracks the number of direct references, no shared
  references.
  These are meant to represent what should exist in the extent tree, these can
  be used to verify the extent tree is consistent as these should all match
  what the extent tree says.
  Whenever we addremove a reference we record the action.  The action maps
  back to the delayed ref action.  We hold the ref we are changing in the
  action so we can account for the history properly, and we record the root we
  were called with since it could be different from ref_root.  We also store
  stack traces because that's how I roll.
  One of these for every block we reference, it holds the roots and references
  to it as well as all of the ref actions that have occurred to it.  We never
  free it until we unmount the file system in order to make sure re-allocations
  are happening properly.
 Walk down to the leaf from the given level 
 Walk up to the next node that needs to be processed 
  Dumps all the information from the block entry to printk, it's going to be
  awesome.
  btrfs_ref_tree_mod: called when we modify a ref for a bytenr
  This will add an action item to the given bytenr and do sanity checks to make
  sure we haven't messed something up.  If we are making a new allocation and
  this block entry has history we will delete all previous actions as long as
  our sanity checks pass as they are no longer needed.
	
	  Save the extra info from the delayed ref in the ref action to make it
	  easier to figure out what is happening.  The real ref's we add to the
	  ref tree need to reflect what we save on disk so it matches any
	  on-disk refs we pre-loaded.
	
	  This is an allocation, preallocate the block_entry in case we haven't
	  used it before.
		
		  For subvol_create we'll just pass in whatever the parent root
		  is and the new root objectid, so let's not treat the passed
		  in root as if it really has a ref for this bytenr.
			
			  This is the root that is modifying us, so it's the
			  one we want to lookup below when we modify the
			  re->num_refs.
			
			  This shouldn't happen because we will add our re
			  above when we lookup the be with !parent, but just in
			  case catch this case so we don't panic because I
			  didn't think of some other corner case.
 Free up the ref cache 
 We want to get as close to start as possible 
	
	  Could have an empty block group, maybe have something to check for
	  this case to verify we were actually empty?
 Walk down all roots and build the ref tree, meant to be called at mount 
		
		  We have to keep track of the bytenrnum_bytes we last hit
		  because we could have run out of space for an inline ref, and
		  would have had to added a ref key item which may appear on a
		  different leaf from the original extent item.
 SPDX-License-Identifier: GPL-2.0
  Subpage (sectorsize < PAGE_SIZE) support overview:
  Limitations:
  - Only support 64K page size for now
    This is to make metadata handling easier, as 64K page would ensure
    all nodesize would fit inside one page, thus we don't need to handle
    cases where a tree block crosses several pages.
  - Only metadata read-write for now
    The data read-write part is in development.
  - Metadata can't cross 64K page boundary
    btrfs-progs and kernel have done that for a while, thus only ancient
    filesystems could have such problem.  For such case, do a graceful
    rejection.
  Special behavior:
  - Metadata
    Metadata read is fully supported.
    Meaning when reading one tree block will only trigger the read for the
    needed range, other unrelated range in the same page will not be touched.
    Metadata write support is partial.
    The writeback is still for the full page, but we will only submit
    the dirty extent buffers in the page.
    This means, if we have a metadata page like this:
    Page offset
    0         16K         32K         48K        64K
    ||           ||
         \- Tree block A        \- Tree block B
    Even if we just want to writeback tree block A, we will also writeback
    tree block B if it's also dirty.
    This may cause extra metadata writeback which results more COW.
  Implementation:
  - Common
    Both metadata and data will use a new structure, btrfs_subpage, to
    record the status of each sector inside a page.  This provides the extra
    granularity needed.
  - Metadata
    Since we have multiple tree blocks inside one page, we can't rely on page
    locking anymore, or we will have greatly reduced concurrency or even
    deadlocks (hold one tree lock while trying to lock another tree lock in
    the same page).
    Thus for metadata locking, subpage support relies on io_tree locking only.
    This means a slightly higher tree locking latency.
	
	  We have cases like a dummy extent buffer page, which is not mappped
	  and doesn't need to be locked.
 Either not subpage, or the page already has private attached 
 Either not subpage, or already detached 
  Increase the eb_refs of current subpage.
  This is important for eb allocation, to prevent race with last eb freeing
  of the same page.
  With the eb_refs increased before the eb inserted into radix tree,
  detach_extent_buffer_page() won't detach the page private while we're still
  allocating the extent buffer.
 Basic checks 
	
	  The range check only works for mapped page, we can still have
	  unmapped page like dummy extent buffer pages.
	
	  For data we need to unlock the page if the last read has finished.
	 
	  And please don't replace @last with atomic_sub_and_test() call
	  inside if () condition.
	  As we want the atomic_sub_and_test() to be always executed.
	
	  For certain call sites like btrfs_drop_pages(), we may have pages
	  beyond the target range. In that case, just set @len to 0, subpage
	  helpers can handle @len == 0 without any problem.
	
	  We have call sites passing @lock_page into
	  extent_clear_unlock_delalloc() for compression path.
	 
	  This @locked_page is locked by plain lock_page(), thus its
	  subpage::writers is 0.  Handle them in a special way.
  Lock a page for delalloc page writeback.
  Return -EAGAIN if the page is not properly initialized.
  Return 0 with the page locked, and writer counter updated.
  Even with 0 returned, the page still need extra check to make sure
  it's really the correct page, as the caller is using
  find_get_pages_contig(), which can race with page invalidating.
  Extra clear_and_test function for subpage dirty bitmap.
  Return true if we're the last bits in the dirty_bitmap and clear the
  dirty_bitmap.
  Return false otherwise.
  NOTE: Callers should manually clear page dirty for true case, as we have
  extra handling for tree blocks.
  Unlike setclear which is dependent on each page status, for test all bits
  are tested in the same way.
  Note that, in selftests (extent-io-tests), we can have empty fs_info passed
  in.  We only test sectorsize == PAGE_SIZE cases so far, thus we can fall
  back to regular sectorsize branch.
  Make sure not only the page dirty bit is cleared, but also subpage dirty bit
  is cleared.
  Handle different locked pages with different page sizes:
  - Page locked by plain lock_page()
    It should not have any subpage::writers count.
    Can be unlocked by unlock_page().
    This is the most common locked page for __extent_writepage() called
    inside extent_write_cache_pages() or extent_write_full_page().
    Rarer cases include the @locked_page from extent_write_locked_range().
  - Page locked by lock_delalloc_pages()
    There is only one caller, all pages except @locked_page for
    extent_write_locked_range().
    In this case, we have to call subpage helper to handle the case.
 For regular page size case, we just unlock the page 
	
	  For subpage case, there are two types of locked page.  With or
	  without writers number.
	 
	  Since we own the page lock, no one else could touch subpage::writers
	  and we are safe to do several atomic operations without spinlock.
 No writers, locked by plain lock_page() 
 Have writers, use proper subpage helper to end it 
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2011 STRATO.  All rights reserved.
 Just an arbitrary number so we can be sure this happened 
	
	  from the shared data ref, we only have the leaf but we need
	  the key. thus, we must look into all items and see that we
	  find one (some) with a reference to our extent item.
 don't skip BTRFS_FILE_EXTENT_PREALLOC, we can handle that 
 BTRFS_SHARED_[DATA|BLOCK]_REF_KEY 
 BTRFS_[TREE_BLOCK|EXTENT_DATA]_REF_KEY 
  Checks for a shared extent during backref search.
  The share_count tracks prelim_refs (direct and indirect) having a
  ref->count >0:
   - incremented when a ref->count transitions to >0
   - decremented when a ref->count transitions to <1
  Return 0 when both refs are for the same block (and can be merged).
  A -1 return indicates ref1 is a 'lower' block than ref2, while 1
  indicates a 'higher' block.
  Add @newref to the @root rbtree, merging identical refs.
  Callers should assume that newref has been freed after calling.
 Identical refs, merge them and free @newref 
			
			  A delayed ref can have newref->count < 0.
			  The ref->count is updated to follow any
			  BTRFS_[ADD|DROP]_DELAYED_REF actions.
  Release the entire tree.  We don't care about internal consistency so
  just free everything and then reset the tree root.
  the rules for all callers of this function are:
  - obtaining the parent is the goal
  - if you add a key, you must know that it is a correct key
  - if you cannot add the parent or a correct key, then we will look into the
    block later to set a correct key
  delayed refs
  ============
         backref type | shared | indirect | shared | indirect
  information         |   tree |     tree |   data |     data
  --------------------+--------+----------+--------+----------
       parent logical |    y   |     -    |    -   |     -
       key to resolve |    -   |     y    |    y   |     y
   tree block logical |    -   |     -    |    -   |     -
   root for resolving |    y   |     y    |    y   |     y
  - column 1:       we've the parent -> done
  - column 2, 3, 4: we use the key to find the parent
  on disk refs (inline or keyed)
  ==============================
         backref type | shared | indirect | shared | indirect
  information         |   tree |     tree |   data |     data
  --------------------+--------+----------+--------+----------
       parent logical |    y   |     -    |    y   |     -
       key to resolve |    -   |     -    |    -   |     y
   tree block logical |    y   |     y    |    y   |     y
   root for resolving |    -   |     y    |    y   |     y
  - column 1, 3: we've the parent -> done
  - column 2:    we take the first key from the block to find the parent
                 (see add_missing_keys)
  - column 4:    we use the key to find the parent
  additional information that's available but not required to find the parent
  block might help in merging entries to gain some speed.
 direct refs use root == 0, key == NULL 
 indirect refs use parent == 0 
	
	  1. We normally enter this function with the path already pointing to
	     the first item to check. But sometimes, we may enter it with
	     slot == nritems.
	  2. We are searching for normal backref but bytenr of this leaf
	     matches shared data backref
	  3. The leaf owner is not equal to the root we are searching
	 
	  For these cases, go to the next leaf before we continue.
		
		  We are searching for normal backref but bytenr of this leaf
		  matches shared data backref, OR
		  the leaf owner is not equal to the root we are searching for
  resolve an indirect backref in the form (root_id, key, level)
  to a logical address
	
	  If we're search_commit_root we could possibly be holding locks on
	  other tree nodes.  This happens when qgroups does backref walks when
	  adding new delayed refs.  To deal with this we need to look in cache
	  for the root, and if we don't find it then we need to search the
	  tree_root's commit root, thus the btrfs_get_fs_root_commit_root usage
	  here.
	
	  We can often find data backrefs with an offset that is too large
	  (>= LLONG_MAX, maximum allowed file offset) due to underflows when
	  subtracting a file's offset with the data offset of its
	  corresponding extent data item. This can happen for example in the
	  clone ioctl.
	 
	  So if we detect such case we set the search key's offset to zero to
	  make sure we will find the matching file extent item at
	  add_all_parents(), otherwise we will miss it because the offset
	  taken form the backref is much larger then the offset of the file
	  extent item. This can make us scan a very large number of file
	  extent items, but at least it will not make us miss any.
	 
	  This is an ugly workaround for a behaviour that should have never
	  existed, but it does and a fix for the clone ioctl would touch a lot
	  of places, cause backwards incompatibility and would not fix the
	  problem for extents cloned with older kernels.
  We maintain three separate rbtrees: one for direct refs, one for
  indirect refs which have a key, and one for indirect refs which do not
  have a key. Each tree does merge on insertion.
  Once all of the references are located, we iterate over the tree of
  indirect refs with missing keys. An appropriate key is located and
  the ref is moved onto the tree for indirect refs. After all missing
  keys are thus located, we iterate over the indirect ref tree, resolve
  each reference, and then insert the resolved reference onto the
  direct tree (merging there too).
  New backrefs (i.e., for parent nodes) are added to the appropriate
  rbtree as they are encountered. The new backrefs are subsequently
  resolved as above.
	
	  We could trade memory usage for performance here by iterating
	  the tree, allocating new refs for each insertion, and then
	  freeing the entire indirect tree when we're done.  In some test
	  cases, the tree can grow quite large (~200k objects).
		
		  we can only tolerate ENOENT,otherwise,we should catch error
		  and return directly.
 we put the first parent into the ref at hand 
 Add a prelim_ref(s) for any other parent(s). 
		
		  Now it's a direct ref, put it in the direct tree. We must
		  do this last because the ref could be mergedfreed here.
  read tree blocks and add keys where required.
 should not be a direct ref 
  add all currently queued delayed refs from this head whose seq nr is
  smaller or equal that seq to the list
 NORMAL INDIRECT METADATA backref 
 SHARED DIRECT METADATA backref 
 NORMAL INDIRECT DATA backref 
			
			  Found a inum that doesn't match our known inum, we
			  know it's shared.
 SHARED DIRECT FULL backref 
		
		  We must ignore BACKREF_FOUND_SHARED until all delayed
		  refs have been checked.
  add all inline backrefs for bytenr to the list
  Returns 0 on success, <0 on error, or BACKREF_FOUND_SHARED.
	
	  enumerate all inline refs
  add all non-inline backrefs for bytenr to the list
  Returns 0 on success, <0 on error, or BACKREF_FOUND_SHARED.
 SHARED DIRECT METADATA backref 
 SHARED DIRECT FULL backref 
 NORMAL INDIRECT METADATA backref 
 NORMAL INDIRECT DATA backref 
  this adds all existing backrefs (inline backrefs, backrefs and delayed
  refs) for the given bytenr to the refs list, merges duplicates and resolves
  indirect refs to their parent bytenr.
  When roots are found, they're added to the roots list
  If time_seq is set to BTRFS_SEQ_LAST, it will not search delayed_refs, and
  behave much like trans == NULL case, the difference only lies in it will not
  commit root.
  The special case is for qgroup to search roots in commit_transaction().
  @sc - if !NULL, then immediately return BACKREF_FOUND_SHARED when a
  shared extent is detected.
  Otherwise this returns 0 for success and <0 for an error.
  If ignore_offset is set to false, only extent refs whose offsets match
  extent_item_pos are returned.  If true, every extent ref is returned
  and extent_item_pos is ignored.
  FIXME some caching might speed things up
	
	  grab both a lock on the path and a lock on the delayed ref head.
	  We need both to get a consistent picture of how the refs look
	  at a specified point in time
		
		  look if there are updates for this ref queued and lock the
		  head
				
				  Mutex was contended, block until it's
				  released and try again
	
	  This walks the tree of merged and resolved refs. Tree blocks are
	  read in as needed. Unique entries are added to the ulist, and
	  the list of found roots is updated.
	 
	  We release the entire tree in one go before returning.
		
		  ref->count < 0 can happen here if there are delayed
		  refs with a node->action of BTRFS_DROP_DELAYED_REF.
		  prelim_ref_insert() relies on this when merging
		  identical refs to keep the overall count correct.
		  prelim_ref_insert() will merge only those refs
		  which compare identically.  Any refs having
		  e.g. different offsets would not be merged,
		  and would retain their original ref->count < 0.
 no parent == root of tree 
				
				  we've recorded that parent, so we must extend
				  its inode list here
  Finds all leafs with a reference to the specified combination of bytenr and
  offset. key_list_head will point to a list of corresponding keys (caller must
  free each list element). The leafs will be stored in the leafs ulist, which
  must be freed with ulist_free.
  returns 0 on success, <0 on error
  walk all backrefs for a given extent to find all roots that reference this
  extent. Walking a backref means finding all extents that reference this
  extent and in turn walk the backrefs of those, too. Naturally this is a
  recursive process, but here it is implemented in an iterative fashion: We
  find all referencing extents for the extent in question and put them on a
  list. In turn, we find all referencing extents for those, further appending
  to the list. The way we iterate the list allows adding more elements after
  the current while iterating. The process stops when we reach the end of the
  list. Found roots are added to the roots list.
  returns 0 on success, < 0 on error.
  Check if an extent is shared or not
  @root:   root inode belongs to
  @inum:   inode number of the inode whose extent we are checking
  @bytenr: logical bytenr of the extent we are checking
  @roots:  list of roots this extent is shared among
  @tmp:    temporary list used for iteration
  btrfs_check_shared uses the backref walking code but will short
  circuit as soon as it finds a root or inode that doesn't match the
  one passed in. This provides a significant performance benefit for
  callers (such as fiemap) which want to know whether the extent is
  shared but do not need a ref count.
  This attempts to attach to the running transaction in order to account for
  delayed refs, but continues on even when no running transaction exists.
  Return: 0 if extent is not shared, 1 if it is shared, < 0 on error.
 this is the only condition under which we return 1 
			
			  If the item at offset is not found,
			  btrfs_search_slot will point us to the slot
			  where it should be inserted. In our case
			  that will be the slot directly before the
			  next INODE_REF_KEY_V2 item. In the case
			  that we're pointing to the last slot in a
			  leaf, we must move one leaf over.
		
		  Check that we're still looking at an extended ref key for
		  this particular objectid. If we have different
		  objectid or type then there are no more to be found
		  in the tree and we can exit.
  this iterates to turn a name (from irefextref) into a full filesystem path.
  Elements of the path are separated by '' and the path is guaranteed to be
  0-terminated. the path is only given within the current file system.
  Therefore, it never starts with a ''. the caller is responsible to provide
  "size" bytes in "dest". the dest buffer will be filled backwards. finally,
  the start point of the resulting string is returned. this pointer is within
  dest, normally.
  in case the path buffer would overflow, the pointer is decremented further
  as if output was written to the buffer, though no more output is actually
  generated. that way, the caller can determine how much space would be
  required for the path to fit into the buffer. in that case, the returned
  value will be smaller than dest. callers must check this!
 regular exit ahead 
 make sure we can use eb after releasing the path 
  this makes the path point to (logical EXTENT_ITEM )
  returns BTRFS_EXTENT_FLAG_DATA for data, BTRFS_EXTENT_FLAG_TREE_BLOCK for
  tree blocks and <0 on error.
  helper function to iterate extent inline refs. ptr must point to a 0 value
  for the first call and may be modified. it is used to track state.
  if more refs exist, 0 is returned and the next call to
  get_extent_inline_ref must pass the modified ptr parameter to get the
  next ref. after the last ref was processed, 1 is returned.
  returns <0 on error
 first call 
 a skinny metadata extent 
 last 
  reads the tree block backref for an extent. tree level and root are returned
  through out_level and out_root. ptr must point to a 0 value for the first
  call and may be modified (see get_extent_inline_ref comment).
  returns 0 if data was provided, 1 if there was no more data to provide or
  <0 on error.
 we can treat both ref types equally here 
  calls iterate() for every inode that references the extent identified by
  the given parameters.
  when the iterator function returns a non-zero value, iteration stops.
 path must be released before calling iterate()! 
  returns 0 if the path could be dumped (probably truncated)
  returns <0 in case of an error
  this dumps all file system paths to the inode into the ipath struct, provided
  is has been created large enough. each path is zero-terminated and accessed
  from ipath->fspath->val[i].
  when it returns, there are ipath->fspath->elem_cnt number of paths available
  in ipath->fspath->val[]. when the allocated space wasn't sufficient, the
  number of missed paths is recorded in ipath->fspath->elem_missed, otherwise,
  it's zero. ipath->fspath->bytes_missing holds the number of bytes that would
  have been needed to return all paths.
  allocates space to return multiple file system paths for an inode.
  total_bytes to allocate are passed, note that space usable for actual path
  information will be total_bytes - sizeof(struct inode_fs_paths).
  the returned pointer must be freed with free_ipath() in the end.
 Current backref iterator only supports iteration in commit root 
	
	  Only support iteration on tree backref yet.
	 
	  This is an extra precaution for non skinny-metadata, where
	  EXTENT_ITEM is also used for tree blocks, that we can only use
	  extent flags to determine if it's a tree block.
 If there is no inline backref, go search for keyed backref 
 No inline nor keyed ref 
  Go to the next backref item of current bytenr, can be either inlined or
  keyed.
  Caller needs to check whether it's inline ref or not by iter->cur_key.
  Return 0 if we get next backref without problem.
  Return >0 if there is no extra backref for this bytenr.
  Return <0 if there is something wrong happened.
 We're still inside the inline refs 
 First tree block info 
 Use inline ref type to determine the size 
 All inline items iterated, fall through 
 We're at keyed items, there is no inline item, go to the next one 
  Drop the backref node from cache, also cleaning up all its
  upper edges and any uncached nodes in the path.
  This cleanup happens bottom up, thus the node should either
  be the lowest node in the cache or a detached node.
		
		  Add the node to leaf node list if no other child block
		  cached.
  Release all nodesedges from current cache
  Handle direct tree backref
  Direct tree backref means, the backref item shows its parent bytenr
  directly. This is for SHARED_BLOCK_REF backref (keyed or inlined).
  @ref_key:	The converted backref key.
 		For keyed backref, it's the item key.
 		For inlined backref, objectid is the bytenr,
 		type is btrfs_inline_ref_type, offset is
 		btrfs_inline_ref_offset.
 Only reloc root uses backref pointing to itself 
 Only reloc backref cache cares about a specific root 
			
			  For generic purpose backref cache, reloc root node
			  is useless.
 Parent node not yet cached 
		
		   Backrefs for the upper level block isn't cached, add the
		   block to pending list
 Parent node already cached 
  Handle indirect tree backref
  Indirect tree backref means, we only know which tree the node belongs to.
  We still need to do a tree search to find out the parents. This is for
  TREE_BLOCK_REF backref (keyed or inlined).
  @ref_key:	The same as @ref_key in  handle_direct_tree_backref()
  @tree_key:	The first key of this tree block.
  @path:	A clean (released) path, to avoid allocating path every time
 		the function get called.
 Tree root 
		
		  For reloc backref cache, we may ignore reloc root.  But for
		  general purpose backref cache, we can't rely on
		  btrfs_should_ignore_reloc_root() as it may conflict with
		  current running relocation and lead to missing root.
		 
		  For general purpose backref cache, reloc root detection is
		  completely relying on direct backref (key->offset is parent
		  bytenr), thus only do such check for reloc cache.
 Search the tree to find parent blocks referring to the block 
 Add all nodes and edges in the path 
 Same as previous should_ignore_reloc_root() call 
			
			  If we know the block isn't shared we can avoid
			  checking its backrefs.
			
			  Add the block to pending list if we need to check its
			  backrefs, we only do this once while walking up a
			  tree as we will catch anything else later on.
  Add backref node @cur into @cache.
  NOTE: Even if the function returned 0, @cur is not yet cached as its upper
 	 links aren't yet bi-directional. Needs to finish such links.
 	 Use btrfs_backref_finish_upper_links() to finish such linkage.
  @path:	Released path for indirect tree backref lookup
  @iter:	Released backref iter for extent tree search
  @node_key:	The first key of the tree block
	
	  We skip the first btrfs_tree_block_info, as we don't use the key
	  stored in it, but fetch it from the tree block
 No extra backref? This means the tree block is corrupted 
		
		  The backref was added previously when processing backref of
		  type BTRFS_TREE_BLOCK_REF_KEY
		
		  Add the upper level block to pending list if we need check
		  its backrefs
 Update key for inline backref 
		
		  Parent node found and matches current inline ref, no need to
		  rebuild this node for this inline ref
 SHARED_BLOCK_REF means key.offset is the parent bytenr 
		
		  key.type == BTRFS_TREE_BLOCK_REF_KEY, inline ref offset
		  means the root objectid. We need to search the tree to get
		  its parent bytenr.
  Finish the upwards linkage created by btrfs_backref_add_tree_node()
 Insert this node to cache if it's not COW-only 
	
	  Use breadth first search to iterate all related edges.
	 
	  The starting points are all the edges of this node
 Parent is detached, no need to keep any edges 
 Lower node is orphan, queue for cleanup 
		
		  All new nodes added in current build_backref_tree() haven't
		  been linked to the cache rb tree.
		  So if we have upper->rb_node populated, this means a cache
		  hit. We only need to link the edge, as @upper and all its
		  parents have already been linked.
 Sanity check, we shouldn't have any unchecked nodes 
 Sanity check, COW-only node has non-COW-only parent 
 Only cache non-COW-only (subvolume trees) tree blocks 
		
		  Also queue all the parent edges of this uncached node
		  to finish the upper linkage
		
		  Lower is no longer linked to any upper backref nodes and
		  isn't in the cache, we can free it ourselves.
 Add this guy's upper edges to the list to process 
 SPDX-License-Identifier: GPL-2.0
  HOW DO BLOCK RESERVES WORK
    Think of block_rsv's as buckets for logically grouped metadata
    reservations.  Each block_rsv has a ->size and a ->reserved.  ->size is
    how large we want our block rsv to be, ->reserved is how much space is
    currently reserved for this block reserve.
    ->failfast exists for the truncate case, and is described below.
  NORMAL OPERATION
    -> Reserve
      Entrance: btrfs_block_rsv_add, btrfs_block_rsv_refill
      We call into btrfs_reserve_metadata_bytes() with our bytes, which is
      accounted for in space_info->bytes_may_use, and then add the bytes to
      ->reserved, and ->size in the case of btrfs_block_rsv_add.
      ->size is an over-estimation of how much we may use for a particular
      operation.
    -> Use
      Entrance: btrfs_use_block_rsv
      When we do a btrfs_alloc_tree_block() we call into btrfs_use_block_rsv()
      to determine the appropriate block_rsv to use, and then verify that
      ->reserved has enough space for our tree block allocation.  Once
      successful we subtract fs_info->nodesize from ->reserved.
    -> Finish
      Entrance: btrfs_block_rsv_release
      We are finished with our operation, subtract our individual reservation
      from ->size, and then subtract ->size from ->reserved and free up the
      excess if there is any.
      There is some logic here to refill the delayed refs rsv or the global rsv
      as needed, otherwise the excess is subtracted from
      space_info->bytes_may_use.
  TYPES OF BLOCK RESERVES
  BLOCK_RSV_TRANS, BLOCK_RSV_DELOPS, BLOCK_RSV_CHUNK
    These behave normally, as described above, just within the confines of the
    lifetime of their particular operation (transaction for the whole trans
    handle lifetime, for example).
  BLOCK_RSV_GLOBAL
    It is impossible to properly account for all the space that may be required
    to make our extent tree updates.  This block reserve acts as an overflow
    buffer in case our delayed refs reserve does not reserve enough space to
    update the extent tree.
    We can steal from this in some cases as well, notably on evict() or
    truncate() in order to help users recover from ENOSPC conditions.
  BLOCK_RSV_DELALLOC
    The individual item sizes are determined by the per-inode size
    calculations, which are described with the delalloc code.  This is pretty
    straightforward, it's just the calculation of ->size encodes a lot of
    different items, and thus it gets used when updating inodes, inserting file
    extents, and inserting checksums.
  BLOCK_RSV_DELREFS
    We keep a running tally of how many delayed refs we have on the system.
    We assume each one of these delayed refs are going to use a full
    reservation.  We use the transaction items and pre-reserve space for every
    operation, and use this reservation to refill any gap between ->size and
    ->reserved that may exist.
    From there it's straightforward, removing a delayed ref means we remove its
    count from ->size and free up reservations as necessary.  Since this is
    the most dynamic block reserve in the system, we will try to refill this
    block reserve first with any excess returned by any other block reserve.
  BLOCK_RSV_EMPTY
    This is the fallback block reserve to make us try to reserve space if we
    don't have a specific bucket for this allocation.  It is mostly used for
    updating the device tree and such, since that is a separate pool we're
    content to just reserve space from the space_info on demand.
  BLOCK_RSV_TEMP
    This is used by things like truncate and iput.  We will temporarily
    allocate a block reserve, set it to some size, and then truncate bytes
    until we have no space left.  With ->failfast set we'll simply return
    ENOSPC from btrfs_use_block_rsv() to signal that we need to unwind and try
    to make a new reservation.  This is because these operations are
    unbounded, so we want to do as much work as we can, and then back off and
    re-reserve.
	
	  If we are the delayed_rsv then push to the global rsv, otherwise dump
	  into the delayed rsv if it is not full.
	
	  The global block rsv is based on the size of the extent tree, the
	  checksum tree and the root tree.  If the fs is empty we want to set
	  it to a minimal amount for safety.
	
	  We at a minimum are going to modify the csum root, the tree root, and
	  the extent root.
	
	  But we also want to reserve enough space so we can do the fallback
	  global reserve for an unlink, which is an additional 5 items (see the
	  comment in __unlink_start_trans for what we're modifying.)
	 
	  But we also need space for the delayed ref updates from the unlink,
	  so its 10, 5 for the actual operation, and 5 for the delayed ref
	  updates.
	
	  Our various recovery options can leave us with NULL roots, so check
	  here and just bail before we go dereferencing NULLs everywhere.
	
	  The global reserve still exists to save us from ourselves, so don't
	  warn_on if we are short on our delayed refs reserve.
DEFAULT_RATELIMIT_BURST 1);
	
	  If we couldn't reserve metadata bytes try and use some from
	  the global reserve if its space type is the same as the global
	  reservation.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
  insert a name into a directory, doing overflow properly if there is a hash
  collision.  data_size indicates how big the item inserted should be.  On
  success a struct btrfs_dir_item pointer is returned, otherwise it is
  an ERR_PTR.
  The name is not copied into the dir item, you have to do that yourself.
  xattrs work a lot like directories, this inserts an xattr item
  into the tree
  insert a directory item in the tree, doing all the magic for
  both indexes. 'dir' indicates which objectid to insert it into,
  'location' is the key to stuff into the directory item, 'type' is the
  type of the inode we're pointing to, and 'index' is the sequence number
  to use for the second index (if one is created).
  Will return 0 or -ENOMEM
 FIXME, use some real flag for selecting the extra index 
  Lookup for a directory item by name.
  @trans:	The transaction handle to use. Can be NULL if @mod is 0.
  @root:	The root of the target tree.
  @path:	Path to use for the search.
  @dir:	The inode number (objectid) of the directory.
  @name:	The name associated to the directory entry we are looking for.
  @name_len:	The length of the name.
  @mod:	Used to indicate if the tree search is meant for a read only
 		lookup, for a modification lookup or for a deletion lookup, so
 		its value should be 0, 1 or -1, respectively.
  Returns: NULL if the dir item does not exists, an error pointer if an error
  happened, or a pointer to a dir item if a dir item exists for the given name.
 Nothing found, we're safe 
 we found an item, look for our name in the item 
 our exact name was found 
	
	  see if there is room in the item to insert this
	  name
 plenty of insertion room 
  Lookup for a directory index item by name and index number.
  @trans:	The transaction handle to use. Can be NULL if @mod is 0.
  @root:	The root of the target tree.
  @path:	Path to use for the search.
  @dir:	The inode number (objectid) of the directory.
  @index:	The index number.
  @name:	The name associated to the directory entry we are looking for.
  @name_len:	The length of the name.
  @mod:	Used to indicate if the tree search is meant for a read only
 		lookup, for a modification lookup or for a deletion lookup, so
 		its value should be 0, 1 or -1, respectively.
  Returns: NULL if the dir index item does not exists, an error pointer if an
  error happened, or a pointer to a dir item if the dir index item exists and
  matches the criteria (name and index number).
  helper function to look at the directory item pointed to by 'path'
  this walks through all the entries in a dir item and finds one
  for a specific name.
  given a pointer into a directory item, delete it.  This
  handles items that have more than one entry in them.
 MARKER 
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2011 STRATO AG
  written by Arne Jansen <sensille@gmx.net>
  ulist is a generic data structure to hold a collection of unique u64
  values. The only operations it supports is adding to the list and
  enumerating it.
  It is possible to store an auxiliary value along with the key.
  A sample usage for ulists is the enumeration of directed graphs without
  visiting a node twice. The pseudo-code could look like this:
  ulist = ulist_alloc();
  ulist_add(ulist, root);
  ULIST_ITER_INIT(&uiter);
  while ((elem = ulist_next(ulist, &uiter)) {
  	for (all child nodes n in elem)
 		ulist_add(ulist, n);
 	do something useful with the node;
  }
  ulist_free(ulist);
  This assumes the graph nodes are addressable by u64. This stems from the
  usage for tree enumeration in btrfs, where the logical addresses are
  64 bit.
  It is also useful for tree enumeration which could be done elegantly
  recursively, but is not possible due to kernel stack limitations. The
  loop would be similar to the above.
  ulist_init - freshly initialize a ulist
  @ulist:	the ulist to initialize
  Note: don't use this function to init an already used ulist, use
  ulist_reinit instead.
  ulist_release - free up additionally allocated memory for the ulist
  @ulist:	the ulist from which to free the additional memory
  This is useful in cases where the base 'struct ulist' has been statically
  allocated.
  ulist_reinit - prepare a ulist for reuse
  @ulist:	ulist to be reused
  Free up all additional memory allocated for the list elements and reinit
  the ulist.
  ulist_alloc - dynamically allocate a ulist
  @gfp_mask:	allocation flags to for base allocation
  The allocated ulist will be returned in an initialized state.
  ulist_free - free dynamically allocated ulist
  @ulist:	ulist to free
  It is not necessary to call ulist_release before.
  ulist_add - add an element to the ulist
  @ulist:	ulist to add the element to
  @val:	value to add to ulist
  @aux:	auxiliary value to store along with val
  @gfp_mask:	flags to use for allocation
  Note: locking must be provided by the caller. In case of rwlocks write
        locking is needed
  Add an element to a ulist. The @val will only be added if it doesn't
  already exist. If it is added, the auxiliary value @aux is stored along with
  it. In case @val already exists in the ulist, @aux is ignored, even if
  it differs from the already stored value.
  ulist_add returns 0 if @val already exists in ulist and 1 if @val has been
  inserted.
  In case of allocation failure -ENOMEM is returned and the ulist stays
  unaltered.
  ulist_del - delete one node from ulist
  @ulist:	ulist to remove node from
  @val:	value to delete
  @aux:	aux to delete
  The deletion will only be done when BOTH val and aux matches.
  Return 0 for successful delete.
  Return > 0 for not found.
 Not found 
 Found and delete 
  ulist_next - iterate ulist
  @ulist:	ulist to iterate
  @uiter:	iterator variable, initialized with ULIST_ITER_INIT(&iterator)
  Note: locking must be provided by the caller. In case of rwlocks only read
        locking is needed
  This function is used to iterate an ulist.
  It returns the next element from the ulist or %NULL when the
  end is reached. No guarantee is made with respect to the order in which
  the elements are returned. They might neither be returned in order of
  addition nor in ascending order.
  It is allowed to call ulist_add during an enumeration. Newly added items
  are guaranteed to show up in the running enumeration.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2008 Oracle.  All rights reserved.
  Based on jffs2 zlib code:
  Copyright  2001-2007 Red Hat, Inc.
  Created by David Woodhouse <dwmw2@infradead.org>
 workspace buffer size for s390 zlib hardware support 
	
	  In case of s390 zlib hardware support, allocate lager workspace
	  buffer. If allocator fails, fall back to a single page buffer.
		
		  Get next input pages and copy the contents to
		  the workspace buffer if required.
 we're making it bigger, give up 
		 we need another page for writing out.  Test this
		  before the total_in so we will pull in a new page for
		  the stream end if required
 we're all done 
	
	  Call deflate with Z_FINISH flush parameter providing more output
	  space but no more input data, until it returns with Z_STREAM_END.
 get another page for the stream end 
	 If it's deflate, and it's got no preset dictionary, then
 we didn't make progress in this inflate call, we're done 
	 If it's deflate, and it's got no preset dictionary, then
	
	  this should only happen if zlib returned fewer bytes than we
	  expected.  btrfs_get_block is responsible for zeroing from the
	  end of the inline extent (destlen) to the end of the page
 SPDX-License-Identifier: GPL-2.0
  extent_map_tree_init - initialize extent map tree
  @tree:		tree to initialize
  Initialize the extent tree @tree.  Should be called for each new inode
  or other user of the extent_map interface.
  alloc_extent_map - allocate new extent map structure
  Allocate a new extent_map structure.  The new structure is
  returned with a reference count of one and needs to be
  freed using free_extent_map()
  free_extent_map - drop reference count of an extent_map
  @em:		extent map being released
  Drops the reference out on @em by one and free the structure
  if the reference count hits zero.
 simple helper to do math around the end of an extent, handling wrap 
  search through the tree for an extent_map with a given offset.  If
  it can't be found, try to find some neighboring extents
 check to see if two extent_map structs are adjacent and safe to merge 
	
	  don't merge compressed extents, we need to know their
	  actual size
	
	  We don't want to merge stuff that hasn't been written to the log yet
	  since it may not reflect exactly what is on disk, and that would be
	  bad.
	
	  We can't modify an extent map that is in the tree and that is being
	  used by another task, as it can cause that other task to see it in
	  inconsistent state during the merging. We always have 1 reference for
	  the tree and 1 for this task (which is unpinning the extent map or
	  clearing the logging flag), so anything > 2 means it's being used by
	  other tasks too.
  unpin_extent_cache - unpin an extent from the cache
  @tree:	tree to unpin the extent in
  @start:	logical offset in the file
  @len:	length of the extent
  @gen:	generation that this extent has been modified in
  Called after an extent has been written to disk properly.  Set the generation
  to the generation that actually added the file item to the inode so we know
  we need to sync this extent when we call fsync().
  Add new extent map to the extent tree
  @tree:	tree to insert new map in
  @em:		map to insert
  @modified:	indicate whether the given @em should be added to the
 	        modified list, which indicates the extent needs to be logged
  Insert @em into @tree or perform a simple forwardbackward merge with
  existing mappings.  The extent_map struct passed in will be inserted
  into the tree directly, with an additional reference taken, or a
  reference dropped if the merge attempt was successful.
  lookup_extent_mapping - lookup extent_map
  @tree:	tree to lookup in
  @start:	byte offset to start the search
  @len:	length of the lookup range
  Find and return the first extent_map struct in @tree that intersects the
  [start, len] range.  There may be additional objects in the tree that
  intersect, so check the object returned carefully to make sure that no
  additional lookups are needed.
  search_extent_mapping - find a nearby extent map
  @tree:	tree to lookup in
  @start:	byte offset to start the search
  @len:	length of the lookup range
  Find and return the first extent_map struct in @tree that intersects the
  [start, len] range.
  If one can't be found, any nearby extent may be returned
  remove_extent_mapping - removes an extent_map from the extent tree
  @tree:	extent tree to remove from
  @em:		extent map being removed
  Removes @em from @tree.  No reference counts are dropped, and no checks
  are done to see if the range is in use
  Helper for btrfs_get_extent.  Given an existing extent in the tree,
  the existing extent is the nearest extent to map_start,
  and an extent that you want to insert, deal with overlap and insert
  the best fitted new extent into the tree.
  Add extent mapping into em_tree
  @fs_info:  the filesystem
  @em_tree:  extent tree into which we want to insert the extent mapping
  @em_in:    extent we are inserting
  @start:    start of the logical range btrfs_get_extent() is requesting
  @len:      length of the logical range btrfs_get_extent() is requesting
  Note that @em_in's range may be different from [start, start+len),
  but they must be overlapped.
  Insert @em_in into @em_tree. In case there is an overlapping range, handle
  the -EEXIST by either:
  a) Returning the existing extent in @em_in if @start is within the
     existing em.
  b) Merge the existing extent with @em_in passed in.
  Return 0 on success, otherwise -EEXIST.
	 it is possible that someone inserted the extent into the tree
	  while we had the lock dropped.  It is also possible that
	  an overlapping map exists in the tree
		
		  existing will always be non-NULL, since there must be
		  extent causing the -EEXIST.
			
			  The existing extent map is the one nearest to
			  the [start, start + len) range which overlaps
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2008 Oracle.  All rights reserved.
  Extent buffer locking
  =====================
  We use a rw_semaphore for tree locking, and the semantics are exactly the
  same:
  - readerwriter exclusion
  - writerwriter exclusion
  - readerreader sharing
  - try-lock semantics for readers and writers
  The rwsem implementation does opportunistic spinning which reduces number of
  times the locking task needs to sleep.
  __btrfs_tree_read_lock - lock extent buffer for read
  @eb:		the eb to be locked
  @nest:	the nesting level to be used for lockdep
  This takes the read lock on the extent buffer, using the specified nesting
  level for lockdep purposes.
  Try-lock for read.
  Return 1 if the rwlock has been taken, 0 otherwise
  Try-lock for write.
  Return 1 if the rwlock has been taken, 0 otherwise
  Release read lock.
  __btrfs_tree_lock - lock eb for write
  @eb:		the eb to lock
  @nest:	the nesting to use for the lock
  Returns with the eb->lock write locked.
  Release the write lock.
  This releases any locks held in the path starting at level and going all the
  way up to the root.
  btrfs_search_slot will keep the lock held on higher nodes in a few corner
  cases, such as COW of the block at slot zero in the node.  This ignores
  those rules, and it should only be called when there are no more updates to
  be done higher up in the tree.
  Loop around taking references on and locking the root node of the tree until
  we end up with a lock on the root node.
  Return: root extent buffer with write lock held
  Loop around taking references on and locking the root node of the tree until
  we end up with a lock on the root node.
  Return: root extent buffer with read lock held
  DREW locks
  ==========
  DREW stands for double-reader-writer-exclusion lock. It's used in situation
  where you want to provide A-B exclusion but not AA or BB.
  Currently implementation gives more priority to reader. If a reader and a
  writer both race to acquire their respective sides of the lock the writer
  would yield its lock as soon as it detects a concurrent reader. Additionally
  if there are pending readers no new writers would be allowed to come in and
  acquire the lock.
 Return true if acquisition is successful, false otherwise 
 Ensure writers count is updated before we check for pending readers 
	
	  Ensure the pending reader count is perceieved BEFORE this reader
	  goes to sleep in case of active writers. This guarantees new writers
	  won't be allowed and that the current reader will be woken up when
	  the last active writer finishes its jobs.
	
	  atomic_dec_and_test implies a full barrier, so woken up writers
	  are guaranteed to see the decrement
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
  Transaction states and transitions
  No running transaction (fs tree blocks are not modified)
  |
  | To next stage:
  |  Call start_transaction() variants. Except btrfs_join_transaction_nostart().
  V
  Transaction N [[TRANS_STATE_RUNNING]]
  |
  | New trans handles can be attached to transaction N by calling all
  | start_transaction() variants.
  |
  | To next stage:
  |  Call btrfs_commit_transaction() on any trans handle attached to
  |  transaction N
  V
  Transaction N [[TRANS_STATE_COMMIT_START]]
  |
  | Will wait for previous running transaction to completely finish if there
  | is one
  |
  | Then one of the following happes:
  | - Wait for all other trans handle holders to release.
  |   The btrfs_commit_transaction() caller will do the commit work.
  | - Wait for current transaction to be committed by others.
  |   Other btrfs_commit_transaction() caller will do the commit work.
  |
  | At this stage, only btrfs_join_transaction() variants can attach
  | to this running transaction.
  | All other variants will wait for current one to finish and attach to
  | transaction N+1.
  |
  | To next stage:
  |  Caller is chosen to commit transaction N, and all other trans handle
  |  haven been released.
  V
  Transaction N [[TRANS_STATE_COMMIT_DOING]]
  |
  | The heavy lifting transaction work is started.
  | From running delayed refs (modifying extent tree) to creating pending
  | snapshots, running qgroups.
  | In short, modify supporting trees to reflect modifications of subvolume
  | trees.
  |
  | At this stage, all start_transaction() calls will wait for this
  | transaction to finish and attach to transaction N+1.
  |
  | To next stage:
  |  Until all supporting trees are updated.
  V
  Transaction N [[TRANS_STATE_UNBLOCKED]]
  |						    Transaction N+1
  | All needed trees are modified, thus we only    [[TRANS_STATE_RUNNING]]
  | need to write them back to disk and update	    |
  | super blocks.				    |
  |						    |
  | At this stage, new transaction is allowed to   |
  | start.					    |
  | All new start_transaction() calls will be	    |
  | attached to transid N+1.			    |
  |						    |
  | To next stage:				    |
  |  Until all tree blocks are super blocks are    |
  |  written to block devices			    |
  V						    |
  Transaction N [[TRANS_STATE_COMPLETED]]	    V
    All tree blocks and super blocks are written.  Transaction N+1
    This transaction is finished and all its	    [[TRANS_STATE_COMMIT_START]]
    data structures will be cleaned up.	    | Life goes on
		
		  If any block groups are found in ->deleted_bgs then it's
		  because the transaction was aborted and a commit did not
		  happen (things failed before writing the new superblock
		  and calling btrfs_finish_extent_commit()), so we can not
		  discard the physical locations of the block groups.
 We can free old roots now. 
	
	  We have to update the last_byte_to_unpin under the commit_root_sem,
	  at the same time we swap out the commit roots.
	 
	  This is because we must have a real view of the last spot the caching
	  kthreads were while caching.  Consider the following views of the
	  extent tree for a block group
	 
	  commit root
	  +----+----+----+----+----+----+----+
	  |\\\\|    |\\\\|\\\\|    |\\\\|\\\\|
	  +----+----+----+----+----+----+----+
	  0    1    2    3    4    5    6    7
	 
	  new commit root
	  +----+----+----+----+----+----+----+
	  |    |    |    |\\\\|    |    |\\\\|
	  +----+----+----+----+----+----+----+
	  0    1    2    3    4    5    6    7
	 
	  If the cache_ctl->progress was at 3, then we are only allowed to
	  unpin [0,1) and [2,3], because the caching thread has already
	  processed those extents.  We are not allowed to unpin [5,6), because
	  the caching thread will re-start it's search from 3, and thus find
	  the hole from [4,6) to add to the free space cache.
  To be called after doing the chunk btree updates right after allocating a new
  chunk (after btrfs_chunk_alloc_add_chunk_item() is called), when removing a
  chunk after all chunk btree updates and after finishing the second phase of
  chunk allocation (btrfs_create_pending_block_groups()) in case some block
  group had its chunk item insertion delayed to the second phase.
  either allocate a new transaction or hop into the existing one
 The file system has been taken offline. No new transactions. 
	
	  If we are ATTACH, we just want to catch the current transaction,
	  and commit it. If there is no transaction, just return ENOENT.
	
	  JOIN_NOLOCK only happens during the transaction commit, so
	  it is impossible that ->running_transaction is NULL
		
		  someone started a transaction after we unlocked.  Make sure
		  to redo the checks above
	
	  One for this trans handle, one so it will live on until we
	  commit the transaction.
	
	  although the tree mod log is per file system and not per transaction,
	  the log must never go across transaction boundaries.
  This does all the record keeping required to make sure that a shareable root
  is properly recorded in a given transaction.  This is required to make sure
  the old root from before we joined the transaction is deleted when the
  transaction commits.
		
		  see below for IN_TRANS_SETUP usage rules
		  we have the reloc mutex held now, so there
		  is only one writer in this function
		 make sure readers find IN_TRANS_SETUP before
		  they find our root->last_trans update
		 this is pretty tricky.  We don't want to
		  take the relocation lock in btrfs_record_root_in_trans
		  unless we're really doing the first setup for this root in
		  this transaction.
		 
		  Normally we'd use root->last_trans as a flag to decide
		  if we want to take the expensive mutex.
		 
		  But, we have to set root->last_trans before we
		  init the relocation root, otherwise, we trip over warnings
		  in ctree.c.  The solution used here is to flag ourselves
		  with root IN_TRANS_SETUP.  When this is 1, we're still
		  fixing up the reloc trees and everyone must wait.
		 
		  When this is zero, they can trust root->last_trans and fly
		  through btrfs_record_root_in_trans without having to take the
		  lock.  smp_wmb() makes sure that all the writes above are
		  done before we pop in the zero below
 Add ourselves to the transaction dropped list 
 Make sure we don't try to update the root at commit time 
	
	  see record_root_in_trans for comments about IN_TRANS_SETUP usage
	  and barriers
 wait for commit against the current transaction to become unblocked
  when this is done, it is safe to start a new transaction, but the current
  transaction might not be fully on disk.
	
	  Do the reservation before we join the transaction so we can do all
	  the appropriate flushing if need be.
		
		  We want to reserve all the bytes we may need all at once, so
		  we only do 1 enospc flushing cycle per transaction start.  We
		  accomplish this by simply assuming we'll do 2 x num_items
		  worth of delayed refs updates in this trans handle, and
		  refill that amount for whatever is missing in the reserve.
		
		  Do the reservation for the relocation root creation
		
		  Some people call with btrfs_start_transaction(root, 0)
		  because they can be throttled, but have some other mechanism
		  for reserving space.  We still want these guys to refill the
		  delayed block_rsv so just add 1 items worth of reservation
		  here.
	
	  If we are JOIN_NOLOCK we're already committing a transaction and
	  waiting on this guy, so we don't need to do the sb_start_intwrite
	  because we're already holding a ref.  We need this because we could
	  have raced in and did an fsync() on a file which can kick a commit
	  and then we deadlock with somebody doing a freeze.
	 
	  If we are ATTACH, it means we just want to catch the current
	  transaction and commit it, so we needn't do sb_start_intwrite(). 
	
	  If the space_info is marked ALLOC_FORCE then we'll get upgraded to
	  ALLOC_FORCE the first run through, and then we won't allocate for
	  anybody else who races in later.  We don't care about the return
	  value here.
	
	  btrfs_record_root_in_trans() needs to alloc new extents, and may
	  call btrfs_join_transaction() while we're also starting a
	  transaction.
	 
	  Thus it need to be called after current->journal_info initialized,
	  or we can deadlock.
		
		  The transaction handle is fully initialized and linked with
		  other structures so it needs to be ended in case of errors,
		  not just freed.
  Similar to regular join but it never starts a transaction when none is
  running or after waiting for the current one to finish.
  btrfs_attach_transaction() - catch the running transaction
  It is used when we want to commit the current the transaction, but
  don't want to start a new one.
  Note: If this function return -ENOENT, it just means there is no
  running transaction. But it is possible that the inactive transaction
  is still in the memory, not fully on disk. If you hope there is no
  inactive transaction in the fs when -ENOENT is returned, you should
  invoke
      btrfs_attach_transaction_barrier()
  btrfs_attach_transaction_barrier() - catch the running transaction
  It is similar to the above function, the difference is this one
  will wait for all the inactive transactions until they fully
  complete.
 Wait for a transaction commit to reach at least the given state. 
 find specified transaction 
		
		  The specified transaction doesn't exist, or we
		  raced with btrfs_commit_transaction
 find newest transaction that is committing | committed 
 nothing committing|committed 
  when btree blocks are allocated, they have some corresponding bits set for
  them in one of two extent_io trees.  This is used to make sure all of
  those extents are sent to disk but does not wait on them
		
		  convert_extent_bit can return -ENOMEM, which is most of the
		  time a temporary error. So when it happens, ignore the error
		  and wait for writeback of this range to finish - because we
		  failed to set the bit EXTENT_NEED_WAIT for the range, a call
		  to __btrfs_wait_marked_extents() would not know that
		  writeback for this range started and therefore wouldn't
		  wait for it to finish - we don't want to commit a
		  superblock that points to btree nodesleafs for which
		  writeback hasn't finished yet (and without errors).
		  We cleanup any entries left in the io tree when committing
		  the transaction (through extent_io_tree_release()).
  when btree blocks are allocated, they have some corresponding bits set for
  them in one of two extent_io trees.  This is used to make sure all of
  those extents are on disk for transaction or log commit.  We wait
  on all the pages and clear them from the dirty pages state tree
		
		  Ignore -ENOMEM errors returned by clear_extent_bit().
		  When committing the transaction, we'll remove any entries
		  left in the io tree. For a log commit, we don't remove them
		  after committing the log because the tree can be accessed
		  concurrently - we do it only at transaction commit time when
		  it's safe to do it (through extent_io_tree_release()).
  When btree blocks are allocated the corresponding extents are marked dirty.
  This function ensures such extents are persisted on disk for transaction or
  log commit.
  @trans: transaction whose dirty pages we'd like to write
  this is used to update the root pointer in the tree of tree roots.
  But, in the case of the extent allocation tree, updating the root
  pointer may allocate blocks which may change the root of the extent
  allocation tree.
  So, this loops and repeats and makes sure the cowonly root didn't
  change while the root pointer was being updated in the metadata.
  update all the cowonly tree roots on disk
  The error handling in this function may not be obvious. Any of the
  failures will cause the file system to go offline. We still need
  to clean up the delayed refs.
 Now flush any delayed refs generated by updating all of the roots 
		
		  We're writing the dirty block groups, which could generate
		  delayed refs, which could generate more dirty block groups,
		  so we want to keep this flushing in this loop to make sure
		  everything gets run.
 Update dev-replace pointer once everything is committed 
  dead roots are old snapshots that need to be deleted.  This allocates
  a dirty root struct and adds it into the list of dead roots that need to
  be deleted
  update all the cowonly tree roots on disk
 see comments in should_cow_block() 
  defrag a given btree.
  Every leaf in the btree is read and defragged.
  Do all special snapshot related qgroup dirty hack.
  Will do all needed qgroup inherit and dirty hack like switch commit
  roots inside one transaction and write all btree into disk, to make
  qgroup works.
	
	  Save some performance in the case that qgroups are not
	  enabled. If this check races with the ioctl, rescan will
	  kick in anyway.
	
	  Ensure dirty @src will be committed.  Or, after coming
	  commit_fs_roots() and switch_commit_roots(), any dirty but not
	  recorded root will never be updated again, causing an outdated root
	  item.
	
	  btrfs_qgroup_inherit relies on a consistent view of the usage for the
	  src root, so we must run the delayed refs here.
	 
	  However this isn't particularly fool proof, because there's no
	  synchronization keeping us from changing the tree after this point
	  before we do the qgroup_inherit, or even from making changes while
	  we're doing the qgroup_inherit.  But that's a problem for the future,
	  for now flush the delayed refs to narrow the race window where the
	  qgroup counters could end up wrong.
	
	  We are going to commit transaction, see btrfs_commit_transaction()
	  comment for reason locking tree_log_mutex
 Now qgroup are all updated, we can inherit it to new qgroups 
	
	  Now we do a simplified commit transaction, which will:
	  1) commit all subvolume and extent tree
	     To ensure all subvolume and extent tree have a valid
	     commit_root to accounting later insert_dir_item()
	  2) write all btree blocks onto disk
	     This is to make sure later btree modification will be cowed
	     Or commit_root can be populated and cause wrong qgroup numbers
	  In this simplified commit, we don't really care about other trees
	  like chunk and root tree, as they won't affect qgroup.
	  And we don't write super to avoid half committed status.
	
	  Force parent root to be updated, as we recorded it before so its
	  last_trans == cur_transid.
	  Or it won't be committed again onto disk after later
	  insert_dir_item()
  new snapshots need to be created at a very specific time in the
  transaction commit.  This does the actual creation.
  Note:
  If the error which may affect the commitment of the current transaction
  happens, we should return the error number. If the error which just affect
  the creation of the pending snapshots, just return 0.
	
	  Make qgroup to skip current new snapshot's qgroupid, as it is
	  accounted by later btrfs_qgroup_inherit().
	
	  insert the directory item
 -ENOMEM 
 check if there is a filedir which has the same name. 
	
	  pull in the delayed directory update
	  and the delayed inode item
	  otherwise we corrupt the FS during
	  snapshot
 Transaction aborted 
 clean up in any case 
 see comments in should_cow_block() 
 record when the snapshot was created in key.offset 
	
	  insert root backforward references
	
	  Do special qgroup accounting for snapshot, as we do some qgroup
	  snapshot hack to do fast snapshot.
	  To co-operate with that hack, we do hack again.
	  Or snapshot will be greatly slowed down by a subtree qgroup rescan
 We have check then name at the beginning, so it is impossible. 
  create all the snapshots we've scheduled for creation
  commit transactions asynchronously. once btrfs_commit_transaction_async
  returns, any subsequent transaction will not be allowed to join.
	
	  We've got freeze protection passed with the transaction.
	  Tell lockdep about it.
 take transaction reference 
	
	  Tell lockdep we've released the freeze rwsem, since the
	  async commit thread will be the one to unlock it.
	
	  Wait for the current transaction commit to start and block
	  subsequent transaction joins
	
	  If the transaction is removed from the list, it means this
	  transaction has been committed successfully, so it is impossible
	  to call the cleanup function.
	
	  Now that we know no one else is still using the transaction we can
	  remove the transaction from the list of transactions. This avoids
	  the transaction kthread from cleaning up the transaction while some
	  other task is still using it, which could result in a use-after-free
	  on things like log trees, as it forces the transaction kthread to
	  wait for this transaction to be cleaned up by us.
  Release reserved delayed ref space of all pending block groups of the
  transaction and remove them from the list
	
	  We use writeback_inodes_sb here because if we used
	  btrfs_start_delalloc_roots we would deadlock with fs freeze.
	  Currently are holding the fs freeze lock, if we do an async flush
	  we'll do btrfs_join_transaction() and deadlock because we need to
	  wait for the fs freeze lock.  Using the direct flushing we benefit
	  from already being in a transaction and our join_transaction doesn't
	  have to re-take the fs freeze lock.
 Stop the commit early if ->aborted is set 
	
	  We only want one transaction commit doing the flushing so we do not
	  waste a bunch of time on lock contention on the extent root node.
		
		  Make a pass through all the delayed refs we have so far.
		  Any running threads may add more while we are here.
		 this mutex is also taken before trying to set
		  block groups readonly.  We need to make sure
		  that nobody has set a block group readonly
		  after a extents from that block group have been
		  allocated for cache files.  btrfs_set_block_group_ro
		  will wait for the transaction to commit if it
		  finds BTRFS_TRANS_DIRTY_BG_RUN set.
		 
		  The BTRFS_TRANS_DIRTY_BG_RUN flag is also used to make sure
		  only one process starts all the block group IO.  It wouldn't
		  hurt to have more than one go through, but there's no
		  real advantage to it either.
		
		  The previous transaction was aborted and was already removed
		  from the list of transactions at fs_info->trans_list. So we
		  abort to prevent writing a new superblock that reflects a
		  corrupt state (pointing to trees with unwritten nodesleafs).
 some pending stuffs might be added after the previous flush. 
	
	  Wait for all ordered extents started by a fast fsync that joined this
	  transaction. Otherwise if this transaction commits before the ordered
	  extents complete we lose logged data after a power failure.
	
	  Ok now we need to make sure to block out any other joins while we
	  commit the transaction.  We could have started a join before setting
	  COMMIT_DOING so make sure to wait for num_writers to == 1 again.
	
	  the reloc mutex makes sure that we stop
	  the balancing code from coming in and moving
	  extents around in the middle of the commit
	
	  We needn't worry about the delayed items because we will
	  deal with them in create_pending_snapshot(), which is the
	  core function of the snapshot creation.
	
	  We insert the dir indexes of the snapshots and update the inode
	  of the snapshots' parents after the snapshot creation, so there
	  are some delayed items which are not dealt with. Now deal with
	  them.
	 
	  We needn't worry that this operation will corrupt the snapshots,
	  because all the tree which are snapshoted will be forced to COW
	  the nodes and leaves.
	
	  make sure none of the code above managed to slip in a
	  delayed item
	 btrfs_commit_tree_roots is responsible for getting the
	  various roots consistent with each other.  Every pointer
	  in the tree of tree roots has to point to the most up to date
	  root for every subvolume and other tree.  So, we have to keep
	  the tree logging code from jumping in and changing any
	  of the trees.
	 
	  At this point in the commit, there can't be any tree-log
	  writers, but a little lower down we drop the trans mutex
	  and let new people in.  By holding the tree_log_mutex
	  from now until after the super is written, we avoid races
	  with the tree-log code.
	
	  Since the transaction is done, we can apply the pending changes
	  before the next transaction.
	 commit_fs_roots gets rid of all the tree log roots, it is now
	  safe to free the root of tree log roots
	
	  Since fs roots are all committed, we can get a quite accurate
	  new_roots. So let's do quota accounting.
	
	  The tasks which save the space cache and inode cache may also
	  update ->aborted, check it.
		
		  reloc_mutex has been unlocked, tree_log_mutex is still held
		  but we can't jump to unlock_tree_log causing double unlock
	
	  At this point, we should have written all the tree blocks allocated
	  in this transaction. So it's now safe to free the redirtyied extent
	  buffers.
	
	  the super is written, we can safely allow the tree-loggers
	  to go about their business
	
	  We needn't acquire the lock here because there is no other task
	  which can change it.
	
	  We needn't acquire the lock here because there is no other task
	  which can change it.
  return < 0 if error
  0 if there are no more dead_roots at the time of call
  1 there are more to be processed, call me again
  The return value indicates there are certainly more snapshots to delete, but
  if there comes a new one during processing, it may return 0. We don't mind,
  because btrfs_commit_super will poke cleaner thread and it will process it a
  few seconds later.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) STRATO AG 2012.  All rights reserved.
  Device replace overview
  [Objective]
  To copy all extents (both new and on-disk) from source device to target
  device, while still keeping the filesystem read-write.
  [Method]
  There are two main methods involved:
  - Write duplication
    All new writes will be written to both target and source devices, so even
    if replace gets canceled, sources device still contains up-to-date data.
    Location:		handle_ops_on_dev_replace() from __btrfs_map_block()
    Start:		btrfs_dev_replace_start()
    End:		btrfs_dev_replace_finishing()
    Content:		Latest datametadata
  - Copy existing extents
    This happens by re-using scrub facility, as scrub also iterates through
    existing extents from commit root.
    Location:		scrub_write_block_to_dev_replace() from
    			scrub_block_complete()
    Content:		Datameta from commit root.
  Due to the content difference, we need to avoid nocow write when dev-replace
  is happening.  This is done by marking the block group read-only and waiting
  for NOCOW writes.
  After replace is done, the finishing part is done by swapping the target and
  source devices.
    Location:		btrfs_dev_replace_update_device_in_mapping_tree() from
    			btrfs_dev_replace_finishing()
		
		  We don't have a replace item or it's corrupted.  If there is
		  a replace target, fail the mount.
		
		  We don't have an active replace item but if there is a
		  replace target, fail the mount.
		
		  allow 'btrfs dev replace_cancel' if srctgt device is
		  missing
  Initialize a new device for device replace target from a given source dev
  and path.
  Return 0 and new device in @device_out, otherwise return < 0
  called from commit_transaction. Writes changed device replace state to
  disk.
		
		  need to delete old one and insert a new one.
		  Since no attempt is made to recover any old state, if the
		  dev_replace state is 'running', the data on the target
		  drive is lost.
		  It would be possible to recover the state: just make sure
		  that the beginning of the item is never changed and always
		  contains all the essential information. Then read this
		  minimal set of information and use it as a base for the
		  new state.
 need to insert a new item 
 Do not use "to_copy" on non zoned filesystem for now 
 Ensure we don't have pending new block group 
 Do not use "to_copy" on non zoned filesystem for now 
 We have more device extent to copy 
		
		  Has more stripes on this device. Keep this block group
		  readonly until we finish all the stripes.
 Last stripe on this device 
	
	  Here we commit the transaction to make sure commit_total_bytes
	  of all the devices are updated.
	
	  from now on, the writes to the srcdev are all duplicated to
	  go to the tgtdev as well (refer to btrfs_map_block()).
 Commit dev_replace state and reserve 1 item for it. 
 the disk copy procedure reuses the scrub code 
 don't warn if EINPROGRESS, someone else might be running scrub 
  blocked until all in-flight bios operations are finished.
  we have removed target device, it is safe to allow new bios request.
  When finishing the device replace, before swapping the source device with the
  target device we must update the chunk allocation state in the target device,
  as it is empty because replace works by directly copying the chunks and not
  through the normal chunk allocation path.
 don't allow cancel or unmount to disturb the finishing procedure 
 was the operation canceled, or is it finished? 
	
	  flush all outstanding IO and inode extent mappings before the
	  copy operation is declared as being finished
	
	  We have to use this loop approach because at this point src_device
	  has to be available for transaction commit to complete, yet new
	  chunks shouldn't be allocated on the device.
 Prevent write_all_supers() during the finishing procedure 
 Prevent new chunks being allocated on the source device 
	
	  Update allocation state in the new device and replace the old device
	  with the new one in the mapping tree.
	
	  Increment dev_stats_ccnt so that btrfs_run_dev_stats() will
	  update on-disk dev stats value during commit transaction
	
	  this is again a consistent state where no dev_replace procedure
	  is running, the target device is part of the filesystem, the
	  source device is not part of the filesystem anymore and its 1st
	  superblock is scratched out so that it is no longer marked to
	  belong to this filesystem.
 replace the sysfs entry 
 write back the superblocks 
  Read progress of device replace status according to the state and last
  stored position. The value format is the same as for
  btrfs_dev_replace::progress_1000
	 even if !dev_replace_is_valid, the values are good enough for
			
			  btrfs_dev_replace_finishing() will handle the
			  cleanup part
		
		  Scrub doing the replace isn't running so we need to do the
		  cleanup step of btrfs_dev_replace_finishing() here
 Scrub for replace must not be running in suspended state 
 resume dev_replace procedure that was interrupted by unmount 
	
	  This could collide with a paused balance, but the exclusive op logic
	  should never allow both to start and pause. We don't want to allow
	  dev-replace to start anyway.
		
		  return true even if tgtdev is missing (this is
		  something that can happen if the dev_replace
		  procedure is suspended by an umount and then
		  the tgtdev is missing (or "btrfs dev scan") was
		  not called and the filesystem is remounted
		  in degraded state. This does not stop the
		  dev_replace procedure. It needs to be canceled
		  manually if the cancellation is wanted.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) STRATO AG 2013.  All rights reserved.
 return -ENOENT for !found, < 0 for errors, or 0 if an item was found 
 Add an item for the type for the first time 
		
		  An item with that type already exists.
		  Extend the item and store the new subid at the end.
 1 - for the uuid item 
  Check if there's an matching subvolume for given UUID
  Return:
  0	check succeeded, the entry is not outdated
  > 0	if the check failed, the caller should remove the entry
  < 0	if an error occurred
					
					  this might look inefficient, but the
					  justification is that it is an
					  exception that check_func returns 1,
					  and that in the regular case only one
					  entry per UUID exists.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
  when auto defrag is enabled we
  queue up these defrag structs to remember which
  inodes need defragging passes
 objectid 
	
	  transid where the defrag was added, we search for
	  extents newer than this
 root objectid 
 last offset we were able to defrag 
 if we've wrapped around back to zero once already 
 pop a record for an inode into the defrag tree.  The lock
  must be held already
  If you're inserting a record for an older transid than an
  existing record, the transid already in the tree is lowered
  If an existing record is found the defrag item you
  pass in is freed
			 if we're reinserting an entry for
			  an old defrag run, make sure to
			  lower the transid of our existing record
  insert a defrag record for this inode if auto defrag is
  enabled
		
		  If we set IN_DEFRAG flag and evict the inode from memory,
		  and then re-read this inode, this new inode doesn't have
		  IN_DEFRAG flag. At the case, we may find the existed defrag.
  Requeue the defrag object. If there is a defrag object that points to
  the same inode in the tree, we will merge them together (by
  __btrfs_add_inode_defrag()) and free the one that we want to requeue.
	
	  Here we don't check the IN_DEFRAG flag, because we need merge
	  them together.
  pick the defragable inode that we want, if it doesn't exist, we will get
  the next one.
 get the inode 
 do a chunk of defrag 
	
	  if we filled the whole defrag batch, there
	  must be more work to do.  Queue this defrag
	  again
		
		  we didn't fill our defrag batch, but
		  we didn't start at zero.  Make sure we loop
		  around to the start of the file.
  run through the list of inodes in the FS that need
  defragging
 Pause the auto defragger. 
 find an inode to defrag 
	
	  during unmount, we use the transaction_wait queue to
	  wait for the defragger to stop
 simple helper to fault in pages and copy.  This should go away
  and be replaced with calls into generic code.
		
		  Copy data from userspace to the current page
 Flush processor's dcache for this page 
		
		  if we get a partial write, we can end up with
		  partially up to date pages.  These add
		  a lot of complexity, so make sure they don't
		  happen by forcing this copy to be retried.
		 
		  The rest of the btrfs_file_write code will fall
		  back to page at a time copies after we return 0.
  unlocks pages after btrfs_file_write is done with them
		 page checked is some magic around finding pages that
		  have been modified without going through btrfs_set_page_dirty
		  clear it here. There should be no need to mark the pages
		  accessed as prepare_pages should have marked them accessed
		  in prepare_pages via find_or_create_page()
  After btrfs_copy_from_user(), update the following things for delalloc:
  - Mark newly dirtied pages as DELALLOC in the io tree.
    Used to advise which range is to be written back.
  - Mark modified pages as UptodateDirty and not needing COW fixup
  - Update inode size for past EOF write
	
	  The pages may have already been dirty, clear out old accounting so
	  we can set things up properly
	
	  we've only changed i_size in ram, and we haven't updated
	  the disk i_size.  There is no need to log the inode
	  at this time.
  this drops all the extents in the cache that intersect the range
  [start, end].  Existing extents are split as required.
 Logic error 
 once for us 
 once for the tree
  this is very complex, but the basic idea is to drop all extents
  in the range start - end.  hint_block is filled in with a block number
  that would be a good hint to the block allocator for this file.
  If an extent intersects the range but is not entirely inside the range
  it is either truncated or split.  Anything entirely inside the range
  is deleted from the tree.
  Note: the VFS' inode number of bytes is not updated, it's up to the caller
  to deal with that. We set the field 'bytes_found' of the arguments structure
  with the number of allocated bytes found in the target range, so that the
  caller can update the inode's number of bytes in an atomic way when
  replacing extents in a range to avoid races with stat(2).
 Must always have a path if ->replace_extent is true 
 can't happen 
		
		  Don't skip extent items representing 0 byte lengths. They
		  used to be created (bug) if while punching holes we hit
		  -ENOSPC condition. So if we find one here, just ensure we
		  delete it, otherwise we would insert a new file extent item
		  with the same key (offset) as that 0 bytes length file
		  extent item in the call to setup_items_for_insert() later
		  in this function.
		
		      | - range to drop - |
		   | -------- extent -------- |
 -ENOMEM 
		
		  From here on out we will have actually dropped something, so
		  last_end can be updated.
		
		   | ---- range to drop ----- |
		       | -------- extent -------- |
		
		        | ---- range to drop ----- |
		   | -------- extent -------- |
		
		   | ---- range to drop ----- |
		     | ------ extent ------ |
 -ENOMEM 
		
		  Set path->slots[0] to first slot, so that after the delete
		  if items are move off from our leaf to its immediate left or
		  right neighbor leafs, we end up with a correct and adjusted
		  path->slots[0] for our insertion (if args->replace_extent).
	
	  If btrfs_del_items() was called, it might have deleted a leaf, in
	  which case it unlocked our path, so check path->locks[0] matches a
	  write lock.
  Mark extent in the range start - end as written.
  This changes extent type from 'pre-allocated' to 'regular'. If only
  part of extent is marked as written, the extent will be split into
  two or three.
  on error we return an unlocked page and the error value
  on success we return a locked page and 0
		
		  Since btrfs_readpage() will unlock the page before it
		  returns, there is a window where btrfs_releasepage() can be
		  called to release the page.  Here we check both inode
		  mapping and PagePrivate() to make sure the page was not
		  released.
		 
		  The private flag check is essential for subpage as we need
		  to store extra bitmap using page->private.
  this just gets pages into the page cache and locks them down.
  This function locks the extent and properly waits for data=ordered extents
  to finish before allowing the pages to be modified if need.
  The return value:
  1 - the extent is locked
  0 - the extent is not locked, and everything is OK
  -EAGAIN - need re-prepare the pages
  the other < 0 number - Something wrong happens
	
	  We should be called after prepare_pages() which should have locked
	  all pages in the range.
  Check if we can do nocow write into the range [@pos, @pos + @write_bytes)
  @pos:	 File offset
  @write_bytes: The length to write, will be updated to the nocow writeable
 		 range
  This function will flush ordered extents in the range to ensure proper
  nocow checks.
  Return:
  >0		and update @write_bytes if we can do nocow write
   0		if we can't do nocow write
  -EAGAIN	if we can't get the needed lock or there are ordered extents
  		for  (nowait == true) case
  <0		if other error happened
  NOTE: Callers need to release the lock by btrfs_check_nocow_unlock().
 We will allocate space in case nodatacow is not set, so bail 
		
		  There are holes in the range or parts of the range that must
		  be COWed (shared extents, RO block groups, etc), so just bail
		  out.
	
	  We reserve space for updating the inode when we reserve space for the
	  extent we are going to write, so we will enospc out there.  We don't
	  need to start yet another transaction to update the inode as we will
	  update the inode when we finish writing whatever data we write.
 Expand hole size to cover write data, preventing empty gap 
		
		  Fault pages before locking them in prepare_pages
		  to avoid recursive lock
			
			  If we don't have to COW at the offset, reserve
			  metadata only. write_bytes may get smaller than
			  requested here.
		
		  This is going to setup the pages array with the number of
		  pages we want, so we don't really need to worry about the
		  contents of pages from loop to loop
		
		  if we have trouble faulting in the pages, fall
		  back to one page at a time
 release everything except the sectors we dirtied 
		
		  If we have not locked the extent range, because the range's
		  start offset is >= i_size, we might still have a non-NULL
		  cached extent state, acquired while marking the extent range
		  as delalloc through btrfs_dirty_pages(). Therefore free any
		  possible cached extent state to avoid a memory leak.
 If the write DIO is within EOF, use a shared lock 
	
	  Re-check since file size may have changed just before taking the
	  lock or pos may have changed because of O_APPEND in generic_write_check()
	
	  We remove IOCB_DSYNC so that we don't deadlock when iomap_dio_rw()
	  calls generic_write_sync() (through iomap_dio_complete()), because
	  that results in calling fsync (btrfs_sync_file()) which will try to
	  lock the inode in exclusivewrite mode.
	
	  The iov_iter can be mapped to the same file range we are writing to.
	  If that's the case, then we will deadlock in the iomap code, because
	  it first calls our callback btrfs_dio_iomap_begin(), which will create
	  an ordered extent, and after that it will fault in the pages that the
	  iov_iter refers to. During the fault in we end up in the readahead
	  pages code (starting at btrfs_readahead()), which will lock the range,
	  find that ordered extent and then wait for it to complete (at
	  btrfs_lock_and_flush_ordered_range()), resulting in a deadlock since
	  obviously the ordered extent can never complete as we didn't submit
	  yet the respective bio(s). This always happens when the buffer is
	  memory mapped to the same file range, since the iomap DIO code always
	  invalidates pages in the target file range (after starting and waiting
	  for any writeback).
	 
	  So here we disable page faults in the iov_iter and then retry if we
	  got -EFAULT, faulting in the pages before the retry.
 No increment (+=) because iomap returns a cumulative value. 
		
		  We have more data left to write. Try to fault in as many as
		  possible of the remainder pages and retry. We do this without
		  releasing and locking again the inode, to prevent races with
		  truncate.
		 
		  Also, in case the iov refers to pages in the file range of the
		  file we want to write to (due to a mmap), we could enter an
		  infinite loop if we retry after faulting the pages in, since
		  iomap will invalidate any pages in the range early on, before
		  it tries to fault in the pages of the iov. So we keep track of
		  how much was left of iov in the previous EFAULT and fallback
		  to buffered IO in case we haven't made any progress.
	
	  Add back IOCB_DSYNC. Our caller, btrfs_file_write_iter(), will do
	  the fsync (call generic_write_sync()).
 If 'err' is -ENOTBLK then it means we must fallback to buffered IO. 
	
	  Ensure all data is persisted. We want the next direct IO read to be
	  able to read what was just written.
	
	  If the fs flips readonly due to some impossible error, although we
	  have opened a file as writable, we have to stop this write operation
	  to ensure consistency.
	
	  Set by setattr when we are about to truncate a file from a non-zero
	  size to a zero size.  This tries to flush down new bytes that may
	  have been written if the application were using truncate to replace
	  a file in place.
	
	  This is only called in fsync, which would do synchronous writes, so
	  a plug can merge adjacent IOs as much as possible.  Esp. in case of
	  multiple disks using raid profile, a large IO can be split to
	  several segments of stripe length (currently 64K).
	
	  If we are doing a fast fsync we can not bail out if the inode's
	  last_trans is <= then the last committed transaction, because we only
	  update the last_trans of the inode during ordered extent completion,
	  and for a fast fsync we don't wait for that, we only wait for the
	  writeback to complete.
  fsync call for both files and directories.  This logs the inode into
  the tree log instead of forcing full commits whenever possible.
  It needs to call filemap_fdatawait so that all ordered extent updates are
  in the metadata btree are up to date for copying to the log.
  It drops the inode mutex before doing the tree log commit.  This is an
  important optimization for directories because holding the mutex prevents
  new operations on the dir while we write to disk.
	
	  Always set the range to a full range, otherwise we can get into
	  several problems, from missing file extent items to represent holes
	  when not using the NO_HOLES feature, to log tree corruption due to
	  races between hole detection during logging and completion of ordered
	  extents outside the range, to missing checksums due to ordered extents
	  for which we flushed only a subset of their pages.
	
	  We write the dirty pages in the range and wait until they complete
	  out of the ->i_mutex. If so, we can flush the dirty pages by
	  multi-task, and make the performance up.  See
	  btrfs_wait_ordered_range for an explanation of the ASYNC check.
	
	  Always check for the full sync flag while holding the inode's lock,
	  to avoid races with other tasks. The flag must be either set all the
	  time during logging or always off all the time while logging.
	
	  Before we acquired the inode's lock and the mmap lock, someone may
	  have dirtied more pages in the target range. We need to make sure
	  that writeback for any such pages does not start while we are logging
	  the inode, because if it does, any of the following might happen when
	  we are not doing a full inode sync:
	 
	  1) We log an extent after its writeback finishes but before its
	     checksums are added to the csum tree, leading to -EIO errors
	     when attempting to read the extent after a log replay.
	 
	  2) We can end up logging an extent before its writeback finishes.
	     Therefore after the log replay we will have a file extent item
	     pointing to an unwritten extent (and no data checksums as well).
	 
	  So trigger writeback for any eventual new dirty pages and then we
	  wait for all ordered extents to complete below.
	
	  We have to do this here to avoid the priority inversion of waiting on
	  IO of a lower priority task while holding a transaction open.
	 
	  For a full fsync we wait for the ordered extents to complete while
	  for a fast fsync we wait just for writeback to complete, and then
	  attach the ordered extents to the transaction so that a transaction
	  commit waits for their completion, to avoid data loss if we fsync,
	  the current transaction commits before the ordered extents complete
	  and a power failure happens right after that.
	 
	  For zoned filesystem, if a write IO uses a ZONE_APPEND command, the
	  logical address recorded in the ordered extent may change. We need
	  to wait for the IO to stabilize the logical address.
		
		  Get our ordered extents as soon as possible to avoid doing
		  checksum lookups in the csum tree, and use instead the
		  checksums attached to the ordered extents.
		
		  We've had everything committed since the last time we were
		  modified so clear this flag in case it was set for whatever
		  reason, it's no longer relevant.
		
		  An ordered extent might have started before and completed
		  already with io errors, in which case the inode was not
		  updated and we end up here. So check the inode's mapping
		  for any errors that might have happened since we last
		  checked called fsync.
	
	  We use start here because we will need to wait on the IO to complete
	  in btrfs_sync_log, which could require joining a transaction (for
	  example checking cross references in the nocow path).  If we use join
	  here we could get into a situation where we're waiting on IO to
	  happen that is blocked on a transaction trying to commit.  With start
	  we inc the extwriter counter, so we wait for all extwriters to exit
	  before we start blocking joiners.  This comment is to keep somebody
	  from thinking they are super smart and changing this to
	  btrfs_join_transaction coughJosefcough.
 Fallthrough and commitfree transaction. 
	 we've logged all the items and now have a consistent
	  version of the file in the log.  It is possible that
	  someone will come in and modify the file, but that's
	  fine because the log is consistent on disk, and we
	  have references to all of the file's extents
	 
	  It is possible that someone will come in and log the
	  file again, but that will end up using the synchronization
	  inside btrfs_sync_log to keep things safe.
		
		  We should have dropped this offset, so if we find it then
		  something has gone horribly wrong.
  Find a hole extent on given inode and change startlen to the end of hole
  extent.(holevacuum extent whose em->start <= start &&
 	   em->start + em->len > start)
  When a hole extent is found, return 1 and modify startlen.
 Hole or vacuum extent(only exists in no-hole mode) 
	
	  For subpage case, if the range is not at page boundary, we could
	  have pages at the leadingtailing part of the range.
	  This could lead to dead loop since filemap_range_has_page()
	  will always return true.
	  So here we need to do extra page alignment for
	  filemap_range_has_page().
		
		  We need to make sure we have no ordered extents in this range
		  and nobody raced in and read a page in this range, if we did
		  we need to try again.
 If it's a hole, nothing more needs to be done. 
  The respective range must have been previously locked, as well as the inode.
  The end offset is inclusive (last byte of the range).
  @extent_info is NULL for fallocate's hole punching and non-NULL when replacing
  the file range with an extent.
  When not punching a hole, we don't want to end up in a state where we dropped
  extents without inserting a new one, so we must abort the transaction to avoid
  a corruption.
	
	  1 - update the inode
	  1 - removing the extents in the range
	  1 - adding the hole extent if no_holes isn't set or if we are
	      replacing the range with a new extent
 If we are punching a hole decrement the inode's byte count 
			
			  The only time we don't want to abort is if we are
			  attempting to clone a partial inline extent, in which
			  case we'll get EOPNOTSUPP.  However if we aren't
			  clone we need to abort no matter what, because if we
			  got EOPNOTSUPP via prealloc then we messed up and
			  need to abort.
				
				  If we failed then we didn't insert our hole
				  entries for the area we dropped, so now the
				  fs is corrupted, so we must abort the
				  transaction.
			
			  We are past the i_size here, but since we didn't
			  insert holes we need to clear the mapped area so we
			  know to not set disk_i_size in this area until a new
			  file extent is inserted here.
				
				  We couldn't clear our area, so we could
				  presumably adjust up and corrupt the fs, so
				  we need to abort.
 shouldn't happen 
	
	  If we were cloning, force the next fsync to be a full one since we
	  we replaced (or just dropped in the case of cloning holes when
	  NO_HOLES is enabled) file extent items and did not setup new extent
	  maps for the replacement extents (or holes).
	
	  If we are using the NO_HOLES feature we might have had already an
	  hole that overlaps a part of the region [lockstart, lockend] and
	  ends at (or beyond) lockend. Since we have no file extent items to
	  represent holes, drop_end can be less than lockend and so we must
	  make sure we have an extent map representing the existing hole (the
	  call to __btrfs_drop_extents() might have dropped the existing extent
	  map representing the existing hole), otherwise the fast fsync path
	  will not record the existence of the hole region
	  [existing_hole_start, lockend].
	
	  Don't insert file hole extent item if it's for a range beyond eof
	  (because it's useless) or if it represents a 0 bytes range (when
	  cur_offset == drop_end).
 Same comment as above. 
 See the comment in the loop above for the reasoning here. 
 Already in a large hole 
	
	  We needn't truncate any block which is beyond the end of the file
	  because we are sure there is no data there.
	
	  Only do this if we are in the same block and we aren't doing the
	  entire block.
 zero back part of the first block 
	 Check the aligned pages after the first unaligned page,
	  if offset != orig_start, which means the first unaligned page
	  including several following pages are already in holes,
 after truncate page, check hole again 
 Check the tail unaligned part is in a hole 
 zero the front end of the last page 
		
		  If we only end up zeroing part of a page, we still need to
		  update the inode item, so that all the time fields are
		  updated as well as the necessary btrfs inode in memory fields
		  for detecting, at fsync time, if the inode isn't yet in the
		  log tree or it's there but not up to date.
 Helper structure to record which range is already reserved 
  Helper function to add falloc range
  Caller should have locked the larger range of extent containing
  [start, len)
		
		  As fallocate iterates by bytenr order, we only need to check
		  the last range.
	
	  Avoid hole punching and extent allocation for some cases. More cases
	  could be considered, but these are unlikely common and we keep things
	  as simple as possible for now. Also, intentionally, if the target
	  range contains one or more prealloc extents together with regular
	  extents and holes, we drop all the existing extents and allocate a
	  new prealloc extent, so that we get a larger contiguous disk extent.
			
			  The whole range is already a prealloc extent,
			  do nothing except updating the inode's i_size if
			  needed.
		
		  Part of the range is already a prealloc extent, so operate
		  only on the remaining part of the range.
	
	  For unaligned ranges, check the pages at the boundaries, they might
	  map to an extent, in which case we need to partially zero them, or
	  they might map to a hole, in which case we need our allocation range
	  to cover them.
 btrfs_prealloc_file_range releases reserved space on error 
 Do not allow fallocate in ZONED mode 
 Make sure we aren't being give some crap mode 
	
	  Only trigger disk allocation, don't trigger qgroup reserve
	 
	  For qgroup space, it will be checked later.
	
	  TODO: Move these two operations after we have checked
	  accurate reserved space, or fallocate can still fail but
	  with page truncated or size expanded.
	 
	  But that's a minor problem and won't do much harm BTW.
		
		  If we are fallocating from the end of the file onward we
		  need to zero out the end of the block if i_size lands in the
		  middle of a block.
	
	  wait for ordered IO before we have any locks.  We'll loop again
	  below with the locks held.
		 the extent lock is ordered inside the running
		  transaction
			
			  we can't wait on the range with the transaction
			  running or with the extent lock held
 First, check if we exceed the qgroup limit 
			
			  Do not need to reserve unwritten extent for this
			  range, free reserved data space first, otherwise
			  it'll result in false ENOSPC error.
	
	  If ret is still 0, means we're OK to fallocate.
	  Or just cleanup the list and exit.
	
	  We didn't need to allocate any more space, but we still extended the
	  size of the file so we need to update i_size and the inode item.
 Let go of our reservation. 
	
	  offset can be negative, in this case we start finding DATAHOLE from
	  the very start of the file.
	
	  This is similar to what we do for direct IO writes, see the comment
	  at btrfs_direct_write(), but we also disable page faults in addition
	  to disabling them only at the iov_iter level. This is because when
	  reading from a hole or prealloc extent, iomap calls iov_iter_zero(),
	  which can still trigger page fault ins despite having set ->nofault
	  to true of our 'to' iov_iter.
	 
	  The difference to direct IO writes is that we deadlock when trying
	  to lock the extent range in the inode's tree during he page reads
	  triggered by the fault in (while for writes it is due to waiting for
	  our own ordered extent). This is because for direct IO reads,
	  btrfs_dio_iomap_begin() returns with the extent range locked, which
	  is only unlocked in the endio callback (end_bio_extent_readpage()).
 No increment (+=) because iomap returns a cumulative value. 
			
			  We didn't make any progress since the last attempt,
			  fallback to a buffered read for the remainder of the
			  range. This is just to avoid any possibility of looping
			  for too long.
			
			  We made some progress since the last retry or this is
			  the first time we are retrying. Fault in as many pages
			  as possible and retry.
	
	  So with compression we will find and lock a dirty page and clear the
	  first one as dirty, setup an async extent, and immediately return
	  with the entire range locked but with nobody actually marked with
	  writeback.  So we can't just filemap_write_and_wait_range() and
	  expect it to work since it will just kick off a thread to do the
	  actual work.  So we need to call filemap_fdatawrite_range _again_
	  since it will wait on the page lock, which won't be unlocked until
	  after the pages have been marked as writeback and so we're good to go
	  from there.  We have to do this otherwise we'll miss the ordered
	  extents and that results in badness.  Please Josef, do not think you
	  know better and pull this out at some point in the future, it is
	  right and you are wrong.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2012 Fusion-io  All rights reserved.
  Copyright (C) 2012 Intel Corp. All rights reserved.
 set when additional merges to this rbio are not allowed 
  set when this rbio is sitting in the hash, but it is just a cache
  of past RMW
  set when it is safe to trust the stripe_pages for caching
 Used by the raid56 code to lock stripes for readmodifywrite 
 Used by the raid56 code to lock stripes for readmodifywrite 
	 while we're doing rmw on a stripe
	  we put it into a hash table so we can
	  lock the stripe and merge more rbios
	  into it.
	
	  LRU list for the stripe cache
	
	  for scheduling work in the helper threads
	
	  bio list and bio_list_lock are used
	  to add more bios into the stripe
	  in hopes of avoiding the full rmw
	 also protected by the bio_list_lock, the
	  plug list is used by the plugging code
	  to collect partial bios while plugged.  The
	  stripe locking code also uses it to hand off
	  the stripe lock to the next pending IO
	
	  flags that tell us if it is safe to
	  merge with this bio
 size of each individual stripe on disk 
 number of data stripes (no pq) 
	
	  set if we're doing a parity rebuild
	  for a read from higher up, which is handled
	  differently from a parity rebuild as part of
	  rmw
 first bad stripe 
 second bad stripe (for raid6 use) 
	
	  number of pages needed to represent the full
	  stripe
	
	  size of all the bios in the bio_list.  This
	  helps us decide if the rbio maps to a full
	  stripe or not
	
	  these are two arrays of pointers.  We allocate the
	  rbio big enough to hold them both and setup their
	  locations when the rbio is allocated
	 pointers to pages that we allocated for
	  readingwriting stripes directly from the disk (including PQ)
	
	  pointers to the pages in the bio_list.  Stored
	  here for faster lookup
	
	  bitmap to record which horizontal stripe has data
 allocated with real_stripes-many pointers for finish_() calls 
 allocated with stripe_npages-many bits for finish_() calls 
  the stripe hash table is used for locking, and to collect
  bios in hopes of making a full stripe
	
	  The table is large, starting with order 4 and can go as high as
	  order 7 in case lock debugging is turned on.
	 
	  Try harder to allocate and fallback to vmalloc to lower the chance
	  of a failing mount.
  caching an rbio means to copy anything from the
  bio_pages array into the stripe_pages array.  We
  use the page uptodate bit in the stripe cache array
  to indicate if it has valid data
  once the caching is done, we set the cache ready
  bit.
  we hash on the first logical address of the stripe
	
	  we shift down quite a bit.  We're using byte
	  addressing, and most of the lower bits are zeros.
	  This tends to upset hash_64, and it consistently
	  returns just one or two different values.
	 
	  shifting off the lower bits fixes things.
  stealing an rbio means taking all the uptodate pages from the stripe
  array in the source rbio and putting them into the destination rbio
  merging means we take the bio_list from the victim and
  splice it into the destination.  The victim should
  be discarded afterwards.
  must be called with dest->rbio_list_lock held
  used to prune items that are in the cache.  The caller
  must hold the hash table lock.
	
	  check the bit again under the hash table lock.
	 hold the lock for the bucket because we may be
	  removing it from the hash table
	
	  hold the lock for the bio list because we need
	  to make sure the bio list is empty
		 if the bio list isn't empty, this rbio is
		  still involved in an IO.  We take it out
		  of the cache list, and drop the ref that
		  was held for the list.
		 
		  If the bio_list was empty, we also remove
		  the rbio from the hash_table, and drop
		  the corresponding ref
  prune a given rbio from the cache
  remove everything in the cache
  remove all cached entries and free the hash table
  used by unmount
  insert an rbio into the stripe cache.  It
  must have already been prepared by calling
  cache_rbio_pages
  If this rbio was already cached, it gets
  moved to the front of the lru.
  If the size of the rbio cache is too big, we
  prune an item.
 bump our ref if we were not in the list before 
  helper function to run the xor_blocks api.  It is only
  able to do MAX_XOR_BLOCKS at a time, so we need to
  loop through.
  Returns true if the bio list inside this rbio covers an entire stripe (no
  rmw required).
  returns 1 if it is safe to merge two rbios together.
  The merging is safe if the two rbios correspond to
  the same stripe and if they are both going in the same
  direction (read vs write), and if neither one is
  locked for final IO
  The caller is responsible for locking such that
  rmw_locked is safe to test
	
	  we can't merge with cached rbios, since the
	  idea is that when we merge the destination
	  rbio is going to run our IO for us.  We can
	  steal from cached rbios though, other functions
	  handle that.
 we can't merge with different operations 
	
	  We've need read the full stripe from the drive.
	  check and repair the parity and write the new results.
	 
	  We're not allowed to add any new bios to the
	  bio list here, anyone else that wants to
	  change this stripe needs to do their own rmw.
  these are just the pages from the rbio array, not from anything
  the FS sent down to us
  helper to index into the pstripe
  helper to index into the qstripe, returns null
  if there is no qstripe
  The first stripe in the table for a logical address
  has the lock.  rbios are added in one of three ways:
  1) Nobody has the stripe locked yet.  The rbio is given
  the lock and 0 is returned.  The caller must start the IO
  themselves.
  2) Someone has the stripe locked, but we're able to merge
  with the lock owner.  The rbio is freed and the IO will
  start automatically along with the existing rbio.  1 is returned.
  3) Someone has the stripe locked, but we're not able to merge.
  The rbio is added to the lock owner's plug list, or merged into
  an rbio already on the plug list.  When the lock owner unlocks,
  the next rbio on the list is run and the IO is started automatically.
  1 is returned
  If we return 0, the caller still owns the rbio and must continue with
  IO submission.  If we return 1, the caller must assume the rbio has
  already been freed.
 Can we steal this cached rbio's pages? 
 Can we merge into the lock owner? 
		
		  We couldn't merge with the running rbio, see if we can merge
		  with the pending ones.  We don't have to check for rmw_locked
		  because there is no way they are inside finish_rmw right now
		
		  No merging, put us on the tail of the plug list, our rbio
		  will be started with the currently running rbio unlocks
  called as rmw or parity rebuild is completed.  If the plug list has more
  rbios waiting for this stripe, the next one on the list will be started
		
		  if we're still cached and there is no other IO
		  to perform, just leave this rbio here for others
		  to steal from later
		
		  we use the plug list to hold all the rbios
		  waiting for the chance to lock this stripe.
		  hand the lock over to one of them.
  this frees the rbio and runs through all the bios in the
  bio_list and calls end_io on them
	
	  At this moment, rbio->bio_list is empty, however since rbio does not
	  always have RBIO_RMW_LOCKED_BIT set and rbio is still linked on the
	  hash list, rbio may be merged with others so that rbio->bio_list
	  becomes non-empty.
	  Once unlock_stripe() is done, rbio->bio_list will not be updated any
	  more and we can call bio_endio() on all queued bios.
  end io function used by finish_rmw.  When we finally
  get here, we've written a full stripe
 OK, we have read all the stripes we need to. 
  the readmodifywrite code wants to use the original bio for
  any pages it included, and then use the rbio for everything
  else.  This function decides if a given index (stripe number)
  and page number in that stripe fall inside the original bio
  or the rbio.
  if you set bio_list_only, you'll get a NULL back for any ranges
  that are outside the bio_list
  This doesn't take any refs on anything, you get a bare page pointer
  and the caller must bump refs as required.
  You must call index_rbio_pages once before you can trust
  the answers from this function.
  number of pages we need for the entire stripe across all the
  drives
  allocation and initial setup for the btrfs_raid_bio.  Not
  this does not allocate any pages for rbio->pages.
	
	  the stripe_pages, bio_pages, etc arrays point to the extra
	  memory we allocated past the end of the rbio
 allocate pages for all the stripes in the bio, including parity 
 only allocate pages for pq stripes 
  add a single page from a specific stripe into our list of bios for IO
  this will try to merge into existing bios if possible, and returns
  zero if all went well.
 if the device is missing, just fail this stripe 
 see if we can add this page onto our existing bio 
		
		  we can't merge these if they are from different
		  devices or if they are not contiguous
 put a new bio on the list 
  while we're doing the readmodifywrite cycle, we could
  have errors in reading pages off the disk.  This checks
  for errors and if we're not able to read the page it'll
  trigger parity reconstruction.  The rmw will be finished
  after we've reconstructed the failed stripes
  helper function to walk our bio list and populate the bio_pages array with
  the result.  This seems expensive, but it is faster than constantly
  searching through the bio list as we setup the IO in finish_rmw or stripe
  reconstruction.
  This must be called before you trust the answers from page_in_rbio
  this is called from one of two situations.  We either
  have a full stripe from the higher layers, or we've read all
  the missing bits off disk.
  This will calculate the parity and then send down any
  changed blocks.
	 at this point we either have a full stripe,
	  or we've read the full stripe from the drive.
	  recalculate the parity and write the new results.
	 
	  We're not allowed to add any new bios to the
	  bio list here, anyone else that wants to
	  change this stripe needs to do their own rmw.
	
	  now that we've set rmw_locked, run through the
	  bio list one last time and map the page pointers
	 
	  We don't cache full rbios because we're assuming
	  the higher layers are unlikely to use this area of
	  the disk again soon.  If they do use it again,
	  hopefully they will send another full bio.
 first collect one page from each data stripe 
 then add the parity stripe 
			
			  raid6, add the qstripe and call the
			  library function to fill in our pq
 raid5 
	
	  time to start writing.  Make bios for everything from the
	  higher layers (the bio_list in our rbio) and our pq.  Ignore
	  everything else.
  helper to find the stripe number for a given bio.  Used to figure out which
  stripe has failed.  This expects the bio to correspond to a physical disk,
  so it looks up based on physical sector numbers.
  helper to find the stripe number for a given
  bio (before mapping).  Used to figure out which stripe has
  failed.  This looks up based on logical block numbers.
  returns -EIO if we had too many failures
 we already know this stripe is bad, move on 
 first failure on this rbio 
 second failure on this rbio 
  helper to fail a stripe based on a physical disk
  bio.
  this sets each page in the bio uptodate.  It should only be used on private
  rbio pages, nothing that comes in from the higher layers
  end io for the read phase of the rmw cycle.  All the bios here are physical
  stripe bios we've read from the disk so we can recalculate the parity of the
  stripe.
  This will usually kick off finish_rmw once all the bios are read in, but it
  may trigger parity reconstruction if we had any errors along the way
	
	  this will normally call finish_rmw to start our write
	  but if there are any failed stripes we'll reconstruct
	  from parity first
  the stripe must be locked by the caller.  It will
  unlock after all the writes are done
	
	  build a list of bios to read all the missing parts of this
	  stripe
			
			  we want to find all the pages missing from
			  the rbio and read them from the disk.  If
			  page_in_rbio finds a page in the bio list
			  we don't need to read it off the stripe.
			
			  the bio cache may have handed us an uptodate
			  page.  If so, be happy and use it
		
		  this can happen if others have merged with
		  us, it means there is nothing left to read.
		  But if there are missing devices it may not be
		  safe to do the full stripe write yet.
	
	  The bioc may be freed once we submit the last bio. Make sure not to
	  touch it after that.
 the actual write will happen once the reads are done 
  if the upper layers pass in a full stripe, we thank them by only allocating
  enough pages to hold the parity, and sending it all down quickly.
  partial stripe writes get handed over to async helpers.
  We're really hoping to merge a few more writes into this
  rbio before calculating new parity
  sometimes while we were reading from the drive to
  recalculate parity, enough new bios come into create
  a full stripe.  So we do a check here to see if we can
  go directly to finish_rmw
 head off into rmw land if we don't have a full stripe 
  We use plugging call backs to collect full stripes.
  Any time we get a partial stripe write while plugged
  we collect it into a list.  When the unplug comes down,
  we sort the list by logical block number and merge
  everything we can into the same rbios
  rbios on the plug list are sorted for easier merging.
	
	  sort our plug list then try to merge
	  everything we can in hopes of creating full
	  stripes.
 we have a full stripe, send it down 
  if the unplug comes from schedule, we have to push the
  work off to a helper thread
  our main entry point for writes from the rest of the FS.
	
	  don't plug on full rbios, just get them out the door
	  as quickly as we can
  all parity reconstruction happens here.  We've read in everything
  we can find from the drives and this does the heavy lifting of
  sorting the good from the bad.
	
	  Store copy of pointers that does not get reordered during
	  reconstruction so that kunmap_local works.
		
		  Now we just use bitmap to mark the horizontal stripes in
		  which we have data when doing parity scrub.
		
		  Setup our array of pointers with pages from each stripe
		 
		  NOTE: store a duplicate array of pointers to preserve the
		  pointer order
			
			  if we're rebuilding a read, we have to use
			  pages from the bio list
 all raid6 handling here 
			
			  single failure, rebuild from parity raid5
			  style
					
					  Just the P stripe has failed, without
					  a bad data or Q stripe.
					  TODO, we should redo the xor here.
				
				  a single failure in raid6 is rebuilt
				  in the pstripe code below
 make sure our ps and qs are in order 
			 if the q stripe is failed, do a pstripe reconstruction
			  from the xors.
			  If both the q stripe and the P stripe are failed, we're
			  here due to a crc mismatch and we can't give them the
			  data they want
				
				  otherwise we have one bad data stripe and
				  a good P stripe.  raid5!
 rebuild from P stripe here (raid5 or raid6) 
 Copy parity block into failed block to start with 
 rearrange the pointer array 
 xor in the rest 
		 if we're doing this rebuild as part of an rmw, go through
		  and set all of our private rbio pages in the
		  failed stripes as uptodate.  This way finish_rmw will
		  know they can be trusted.  If this was a read reconstruction,
		  other endio functions will fiddle the uptodate bits
	
	  Similar to READ_REBUILD, REBUILD_MISSING at this point also has a
	  valid rbio which is consistent with ondisk content, thus such a
	  valid rbio can be cached to avoid further disk reads.
		
		  - In case of two failures, where rbio->failb != -1:
		 
		    Do not cache this rbio since the above read reconstruction
		    (raid6_datap_recov() or raid6_2data_recov()) may have
		    changed some content of stripes which are not identical to
		    on-disk content any more, otherwise, a later writerecover
		    may steal stripe_pages from this rbio and end up with
		    corruptions or rebuild failures.
		 
		  - In case of single failure, where rbio->failb == -1:
		 
		    Cache this rbio iff the above read reconstruction is
		    executed without problems.
  This is called only for stripes we've read from disk to
  reconstruct the parity.
	
	  we only read stripe pages off the disk, set them
	  up to date if there were no errors
  reads everything we need off the disk to reconstruct
  the parity. endio handlers trigger final reconstruction
  when the IO is done.
  This is used both for reads from the higher layers and for
  parity construction required to finish a rmw cycle.
	
	  read everything that hasn't failed.  Thanks to the
	  stripe cache, it is possible that some or all of these
	  pages are going to be uptodate.
			
			  the rmw code may have already read this
			  page in
		
		  we might have no bios to read just because the pages
		  were up to date, or we might have no bios to read because
		  the devices were gone.
	
	  The bioc may be freed once we submit the last bio. Make sure not to
	  touch it after that.
  the main entry point for reads from the higher layers.  This
  is really only called when the normal read path had a failure,
  so we assume the bio they send down corresponds to a failed part
  of the drive.
	
	  Loop retry:
	  for 'mirror == 2', reconstruct from all other stripes.
	  for 'mirror_num > 2', select a stripe to fail on every retry.
		
		  'mirror == 3' is to fail the p stripe and
		  reconstruct from the q stripe.  'mirror > 3' is to
		  fail a data stripe and reconstruct from p+q stripe.
	
	  __raid56_parity_recover will end the bio with
	  any errors it hits.  We don't want to return
	  its error value up the stack because our caller
	  will end up calling bio_endio with any nonzero
	  return
	
	  our rbio has been added to the list of
	  rbios that will be handled after the
	  currently lock owner is done
  The following code is used to scrubreplace the parity stripe
  Caller must have already increased bio_counter for getting @bioc.
  Note: We need make sure all the pages that add into the scrubreplace
  raid bio are correct and not be changed during the scrubreplace. That
  is those pages just hold metadata or file data with checksum.
	
	  This is a special bio which is used to hold the completion handler
	  and make the scrub rbio is similar to the other types
	
	  After mapping bioc with BTRFS_MAP_WRITE, parities have been sorted
	  to the end position, so this search can start from the first parity
	  stripe.
 Now we just support the sectorsize equals to page size 
	
	  We have already increased bio_counter when getting bioc, record it
	  so we can free it at rbio_orig_end_io().
 Used for both parity scrub and missing. 
  We just scrub the parity that we have correct data on the same horizontal,
  so we needn't allocate all pages for all the stripes.
	
	  Because the higher layers(scrubber) are unlikely to
	  use this area of the disk again soon, so don't cache
	  it.
 RAID6, allocate and map temp space for the Q stripe 
 Map the parity stripe just once 
 first collect one page from each data stripe 
 RAID6, call the library function to fill in our PQ 
 raid5 
 Check scrubbing parity and repair it 
 Parity is right, needn't writeback 
	
	  time to start writing.  Make bios for everything from the
	  higher layers (the bio_list in our rbio) and our pq.  Ignore
	  everything else.
 Every parity is right 
  While we're doing the parity check and repair, we could have errors
  in reading pages off the disk.  This checks for errors and if we're
  not able to read the page it'll trigger parity reconstruction.  The
  parity scrub will be finished after we've reconstructed the failed
  stripes
		
		  Because we can not use a scrubbing parity to repair
		  the data, so the capability of the repair is declined.
		  (In the case of RAID5, we can not repair anything)
		
		  If all data is good, only parity is correctly, just
		  repair the parity.
		
		  Here means we got one corrupted data stripe and one
		  corrupted parity on RAID6, if the corrupted parity
		  is scrubbing parity, luckily, use the other one to repair
		  the data, or we can not repair the data stripe.
  end io for the read phase of the rmw cycle.  All the bios here are physical
  stripe bios we've read from the disk so we can recalculate the parity of the
  stripe.
  This will usually kick off finish_rmw once all the bios are read in, but it
  may trigger parity reconstruction if we had any errors along the way
	
	  this will normally call finish_rmw to start our write
	  but if there are any failed stripes we'll reconstruct
	  from parity first
	
	  build a list of bios to read all the missing parts of this
	  stripe
			
			  we want to find all the pages missing from
			  the rbio and read them from the disk.  If
			  page_in_rbio finds a page in the bio list
			  we don't need to read it off the stripe.
			
			  the bio cache may have handed us an uptodate
			  page.  If so, be happy and use it
		
		  this can happen if others have merged with
		  us, it means there is nothing left to read.
		  But if there are missing devices it may not be
		  safe to do the full stripe write yet.
	
	  The bioc may be freed once we submit the last bio. Make sure not to
	  touch it after that.
 the actual write will happen once the reads are done 
 The following code is used for dev replace of a missing RAID 56 device. 
	
	  This is a special bio which is used to hold the completion handler
	  and make the scrub rbio is similar to the other types
	
	  When we get bioc, we have already increased bio_counter, record it
	  so we can free it at rbio_orig_end_io()
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
 returns NULL if the insertion worked, or it returns the node it did find
  in the tree
  look for a given offset in the tree, and if it can't be found return the
  first lesser offset
  look find the first ordered struct that has this offset, otherwise
  the first one less than this offset
  Allocate and add a new ordered_extent into the per-inode tree.
  The tree is given a single reference on the ordered extent that was
  inserted.
 For nocow write, we can release the qgroup rsv right now 
		
		  The ordered extent has reserved qgroup space, release now
		  and pass the reserved number for qgroup_record to free.
 one ref for the tree 
	
	  We don't need the count_max_extents here, we can assume that all of
	  that work has been done at higher layers, so this is truly the
	  smallest the extent is going to get.
  Add a struct btrfs_ordered_sum into the list of checksums to be inserted
  when an ordered extent is finished.  If the list covers more than one
  ordered extent, it is split across multiples.
  Mark all ordered extents io inside the specified range finished.
  @page:	 The invovled page for the opeartion.
 		 For uncompressed buffered IO, the page status also needs to be
 		 updated to indicate whether the pending ordered io is finished.
 		 Can be NULL for direct IO and compressed write.
 		 For these cases, callers are ensured they won't execute the
 		 endio function twice.
  @finish_func: The function to be executed when all the IO of an ordered
 		 extent are finished.
  This function is called for endio, thus the range must have ordered
  extent(s) coveri it.
 No ordered extents at all 
		
		  |<-- OE --->|  |
		 		  cur
		  Go to next OE.
 No more ordered extents, exit 
 Go to next ordered extent and continue 
		
		  |	|<--- OE --->|
		  cur
		  Go to the start of OE.
		
		  Now we are definitely inside one ordered extent.
		 
		  |<--- OE --->|
		 	|
		 	cur
			
			  Ordered (Private2) bit indicates whether we still
			  have pending io unfinished for the ordered extent.
			 
			  If there's no such bit, we need to skip to next range.
 Now we're fine to update the accounting 
		
		  All the IO of the ordered extent is finished, we need to queue
		  the finish_func to be executed.
  Finish IO for one ordered extent across a given range.  The range can only
  contain one ordered extent.
  @cached:	 The cached ordered extent. If not NULL, we can skip the tree
                search and use the ordered extent directly.
  		 Will be also used to store the finished ordered extent.
  @file_offset: File offset for the finished IO
  @io_size:	 Length of the finish IO range
  Return true if the ordered extent is finished in the range, and update
  @cached.
  Return false otherwise.
  NOTE: The range can NOT cross multiple ordered extents.
  Thus caller should ensure the range doesn't cross ordered extents.
		
		  Ensure only one caller can set the flag and finished_ret
		  accordingly
 test_and_set_bit implies a barrier 
  used to drop a reference on an ordered extent.  This will free
  the extent if the last reference is dropped
  remove an ordered extent from the tree.  No references are dropped
  and waiters are woken up.
 This is paired with btrfs_add_ordered_extent. 
	
	  The current running transaction is waiting on us, we need to let it
	  know that we're complete and wake it up.
		
		  The checks for trans are just a formality, it should be set,
		  but if it isn't we don't want to derefassert under the spin
		  lock, so be nice and check if trans is set, but ASSERT() so
		  if it isn't set a developer will notice.
  wait for all the ordered extents in a root.  This is done when balancing
  space between drives.
  Used to start IO or wait for a given ordered extent to finish.
  If wait is one, this effectively waits on page writeback for all the pages
  in the extent, and it waits on the io completion code to insert
  metadata into the btree corresponding to the extent
	
	  pages in the range can be dirty, clean or writeback.  We
	  start IO on any dirty ones so the wait doesn't stall waiting
	  for the flusher thread to find them
  Used to wait on ordered extents across a large range of bytes.
	 start IO across the range first to instantiate any delalloc
	  extents
	
	  If we have a writeback error don't return immediately. Wait first
	  for any ordered extents that haven't completed yet. This is to make
	  sure no one can dirty the same page ranges and call writepages()
	  before the ordered extents complete - to avoid failures (-EEXIST)
	  when adding the new ordered extents to the ordered tree.
		
		  If the ordered extent had an error save the error but don't
		  exit without waiting first for all other ordered extents in
		  the range to complete.
  find an ordered extent corresponding to file_offset.  return NULL if
  nothing is found, otherwise take a reference on the extent and return it
 Since the DIO code tries to lock a wide area we need to look for any ordered
  extents that exist in the range, rather than just the start of the range.
  Adds all ordered extents to the given list. The list ends up sorted by the
  file_offset of the ordered extents.
  lookup and return any extent before 'file_offset'.  NULL is returned
  if none is found
  Lookup the first ordered extent that overlaps the range
  [@file_offset, @file_offset + @len).
  The difference between this and btrfs_lookup_first_ordered_extent() is
  that this one won't return any ordered extent that does not overlap the range.
  And the difference against btrfs_lookup_ordered_extent() is, this function
  ensures the first ordered extent gets returned.
	
	  Here we don't want to use tree_search() which will use tree->last
	  and screw up the search order.
	  And __tree_search() can't return the adjacent ordered extents
	  either, thus here we do our own search.
			
			  Direct hit, got an ordered extent that starts at
			  @file_offset
 Empty tree 
 We got an entry around @file_offset, check adjacent entries 
 No ordered extent in the range 
  btrfs_flush_ordered_range - Lock the passed range and ensures all pending
  ordered extents in it are run to completion.
  @inode:        Inode whose ordered tree is to be searched
  @start:        Beginning of range to flush
  @end:          Last byte of range to lock
  @cached_state: If passed, will return the extent state responsible for the
  locked range. It's the caller's responsibility to free the cached state.
  This function always returns with the given range locked, ensuring after it's
  called no order extent can be pending.
			
			  If no external cached_state has been passed then
			  decrement the extra ref taken for cachedp since we
			  aren't exposing it outside of this function
	
	  The splitting extent is already counted and will be added again
	  in btrfs_add_ordered_extent_(). Subtract num_bytes to avoid
	  double counting.
 Remove from tree once 
 Re-insert the node 
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2011, 2012 STRATO.  All rights reserved.
  This is only the first step towards a full-features scrub. It reads all
  extent and super block and verifies the checksums. In case a bad checksum
  is found or the extent cannot be read, good data will be written back if
  any can be found.
  Future enhancements:
   - In case an unrepairable extent is encountered, track which files are
     affected and report them
   - track and record media errors, throw out bad devices
   - add a mode to also read unallocated space
  the following three values only influence the performance.
  The last one configures the number of parallel and outstanding IO
  operations. The first two values configure an upper limit for the number
  of (dynamically allocated) pages that are added to a bio.
 128k per bio 
 128k per bio 
 8MB per device in flight 
  the following value times PAGE_SIZE needs to be large enough to match the
  largest nodeleafsector size that shall be supported.
  Values larger than BTRFS_STRIPE_LEN are not supported.
 64k per nodeleafsector 
 extent flags 
 free mem on transition to zero 
 also sets header_error 
 The following is for the data used to check parity 
 It is for the data with checksum 
 Used for the chunks with parity stripe such RAID56 
 Work of parity check and repair 
 Mark the parity blocks which have data 
	
	  Mark the parity blocks which have data, but errors happen when
	  read data or check data
 State of IO submission throttling affecting the associated device 
 <= SCRUB_PAGES_PER_WR_BIO 
	
	  statistics
	
	  Use a ref counter to avoid use-after-free issues. Scrub workers
	  decrement bios_in_flight and workers_pending and then do a wakeup
	  on the list_wait wait queue. We must ensure the main scrub task
	  doesn't free the scrub context before or while the workers are
	  doing the wakeup() call.
  Insert new full stripe lock into full stripe locks tree
  Return pointer to existing or newly inserted full_stripe_lock structure if
  everything works well.
  Return ERR_PTR(-ENOMEM) if we failed to allocate memory
  NOTE: caller must hold full_stripe_locks_root->lock before calling this
  function
	
	  Insert new lock.
  Search for a full stripe lock of a block group
  Return pointer to existing full stripe lock if found
  Return NULL if not found
  Helper to get full stripe logical from a normal bytenr.
  Caller must ensure @cache is a RAID56 block group.
	
	  Due to chunk item size limit, full stripe length should not be
	  larger than U32_MAX. Just a sanity check here.
	
	  round_down() can only handle power of 2, while RAID56 full
	  stripe length can be 64KiB  n, so we need to manually round down.
  Lock a full stripe to avoid concurrency of recovery and read
  It's only used for profiles with parities (RAID56), for other profiles it
  does nothing.
  Return 0 if we locked full stripe covering @bytenr, with a mutex held.
  So caller must call unlock_full_stripe() at the same context.
  Return <0 if encounters error.
 Profiles not based on parity don't need full stripe lock 
 Now insert the full stripe lock 
  Unlock a full stripe.
  NOTE: Caller must ensure it's the same context calling corresponding
  lock_full_stripe().
  Return 0 if we unlock full stripe without problem.
  Return <0 for error
 If we didn't acquire full stripe lock, no need to continue 
 Unpaired unlock_full_stripe() detected 
 this can happen when scrub is cancelled 
	
	  this makes the path point to (inum INODE_ITEM ioff)
	
	  init_path might indirectly call vmalloc, or use GFP_KERNEL. Scrub
	  uses GFP_NOFS in this context, so we keep it consistent but it does
	  not seem to be strictly necessary.
	
	  we deliberately ignore the bit ipath might have been too small to
	  hold all of the paths here
  scrub_handle_errored_block gets called when either verification of the
  pages failed or the bio failed to read, e.g. with EIO. In the latter
  case, this function handles all pages in the bio, even though only one
  may be bad.
  The goal of this function is to repair the errored block by using the
  contents of one of the mirrors.
 holds one for each mirror 
		
		  if we find an error in a super block, we just report it.
		  They will get written with the next transaction commit
		  anyway
	
	  We must use GFP_NOFS because the scrub task might be waiting for a
	  worker task executing this function and in turn a transaction commit
	  might be waiting the scrub task to pause (which needs to wait for all
	  the worker tasks to complete before pausing).
	  We do allocations in the workers through insert_full_stripe_lock()
	  and scrub_add_page_to_wr_bio(), which happens down the call chain of
	  this function.
	
	  For RAID56, race can happen for a different device scrub thread.
	  For data corruption, Parity and Data threads will both try
	  to recovery the data.
	  Race can lead to doubly added csum error, or even unrecoverable
	  error.
	
	  read all mirrors one after the other. This includes to
	  re-read the extent or metadata block that failed (that was
	  the cause that this fixup code is called) another time,
	  sector by sector this time in order to know which sectors
	  caused IO errors and which ones are good (for all mirrors).
	  It is the goal to handle the situation when more than one
	  mirror contains IO errors, but the errors do not
	  overlap, i.e. the data can be repaired by selecting the
	  sectors from those mirrors without IO error on the
	  particular sectors. One example (with blocks >= 2  sectorsize)
	  would be that mirror #1 has an IO error on the first sector,
	  the second sector is good, and mirror #2 has an IO error on
	  the second sector, but the first sector is good.
	  Then the first sector of the first mirror can be repaired by
	  taking the first sector of the second mirror, and the
	  second sector of the second mirror can be repaired by
	  copying the contents of the 2nd sector of the 1st mirror.
	  One more note: if the sectors of one mirror contain IO
	  errors, the checksum cannot be verified. In order to get
	  the best data for repairing, the first attempt is to find
	  a mirror without IO errors and with a validated checksum.
	  Only if this is not possible, the sectors are picked from
	  mirrors with IO errors without considering the checksum.
	  If the latter is the case, at the end, the checksum of the
	  repaired area is verified in order to correctly maintain
	  the statistics.
 setup the context, map the logical blocks and alloc the pages 
 build and submit the bios for the failed mirror, check checksums 
		
		  the error disappeared after reading page by page, or
		  the area was part of a huge bio and other parts of the
		  bio caused IO errors, or the block layer merged several
		  read requests into one and the error is caused by a
		  different bio (usually one of the two latter cases is
		  the cause)
	
	  now build and submit the bios for the other mirrors, check
	  checksums.
	  First try to pick the mirror which is completely without IO
	  errors and also does not have a checksum error.
	  If one is found, and if a checksum is present, the full block
	  that is known to contain an error is rewritten. Afterwards
	  the block is known to be corrected.
	  If a mirror is found which is completely correct, and no
	  checksum is present, only those pages are rewritten that had
	  an IO error in the block to be repaired, since it cannot be
	  determined, which copy of the other pages is better (and it
	  could happen otherwise that a correct page would be
	  overwritten by a bad one).
 raid56's mirror can be more than BTRFS_MAX_MIRRORS 
 build and submit the bios, check checksums 
	
	  In case of IO errors in the area that is supposed to be
	  repaired, continue by picking good copies of those sectors.
	  Select the good sectors from mirrors to rewrite bad sectors from
	  the area to fix. Afterwards verify the checksum of the block
	  that is supposed to be repaired. This verification step is
	  only done for the purpose of statistic counting and for the
	  final scrub report, whether errors remain.
	  A perfect algorithm could make use of the checksum and try
	  all possible combinations of sectors from the different mirrors
	  until the checksum verification succeeds. For example, when
	  the 2nd sector of mirror #1 faces IO errors, and the 2nd sector
	  of mirror #2 is readable but the final checksum test fails,
	  then the 2nd sector of mirror #3 could be tried, whether now
	  the final checksum succeeds. But this would be a rare
	  exception and is therefore not implemented. At least it is
	  avoided that the good copy is overwritten.
	  A more useful improvement would be to pick the sectors
	  without IO error based on sector sizes (512 bytes on legacy
	  disks) instead of on sectorsize. Then maybe 512 byte of one
	  mirror could be repaired by taking 512 byte of a different
	  mirror, even if other 512 byte sectors in the same sectorsize
	  area are unreadable.
 skip no-io-error page in scrub 
			
			  In case of dev replace, if raid56 rebuild process
			  didn't work out correct data, then copy the content
			  in sblock_bad to make sure target device is identical
			  to source device, instead of writing garbage data in
			  sblock_for_recheck array to target device.
 try to find no-io-error page in mirrors 
			
			  did not find a mirror to fetch the page
			  from. scrub_write_page_to_dev_replace()
			  handles this case (page->io_error), by
			  filling the block with zeros before
			  submitting the write request
			
			  need to verify the checksum now that all
			  sectors on disk are repaired (the write
			  request for data to be repaired is on its way).
			  Just be lazy and use scrub_recheck_block()
			  which re-reads the data before the checksum
			  is verified, but most likely the data comes out
			  of the page cache.
 RAID56 
 The other RAID type 
	
	  note: the two members refs and outstanding_pages
	  are not used (and not set) in the blocks that are used for
	  the recheck procedure
		
		  With a length of sectorsize, each returned stripe represents
		  one mirror
 for missing devices, dev->bdev is NULL 
 All pages in sblock belong to the same stripe on the same device. 
  this function will check the on disk data for checksum errors, header
  errors and read IO errors. If any IO errors happen, the exact pages
  which are errored are marked as being bad. The goal is to enable scrub
  to take those pages that are not errored from all the mirrors so that
  the pages that are errored in the just handled mirror can be repaired.
 short cut for raid56 
	
	  This block is used for the check of the parity on the source device,
	  so the data needn't be written into the destination device.
	 process all writes in a single worker thread. Then the block layer
	  orders the requests before sending them to the driver which
	  doubled the write performance on spinning disks when measured
	
	  No need to initialize these stats currently,
	  because this function only use return value
	  instead of these stats value.
	 
	  Todo:
	  always use stats
	
	  In scrub_pages() and scrub_pages_for_parity() we ensure each spage
	  only contains one sector of data.
	
	  This is done in sectorsize steps even for metadata as there's a
	  constraint for nodesize to be aligned to sectorsize. This will need
	  to change so we don't misuse data and metadata units like that.
 Each member in pagev is just one block, not a full page 
	
	  we don't use the getter functions here, as we
	  a) don't have an extent buffer and
	  b) the page is already kmapped
		
		  if we find an error in a super block, we just report it.
		  They will get written with the next transaction commit
		  anyway
  Throttling of IO submission, bandwidth-limit based, the timeslice is 1
  second.  Limit can be set via sysfsUUIDdevinfodevidscrub_speed_max.
	
	  Slice is divided into intervals when the IO is submitted, adjust by
	  bwlimit and maximum of 64 intervals.
 Start new epoch, set deadline 
 Still in the time to send? 
 If current bio is within the limit, send it 
 We're over the limit, sleep until the rest of the slice 
 New request after deadline, start new epoch 
 Next call will start the deadline period 
	
	  grab a fresh bio or wait for one to become available
 one for the page added to the bio 
		
		  We shouldn't be scrubbing a missing device. Even for dev
		  replace, we should only get here for RAID 56. We either
		  managed to mount something with no mirrors remaining or
		  there's a bug in scrub_remap_extent()btrfs_map_block().
	 one ref inside this function, plus one for each page added to
		
		  Here we will allocate one page for one sector to scrub.
		  This is fine if PAGE_SIZE == sectorsize, but will cost
		  more memory for PAGE_SIZE > sectorsize case.
		
		  This case should only be hit for RAID 56 device replace. See
		  the comment in scrub_missing_raid56_pages() for details.
 last one frees, either here or in bio completion for last page 
 now complete the scrub_block items that have all pages completed 
		
		  if has checksum error, write via repair mechanism in
		  dev replace case, otherwise write here in dev replace
		  case.
  Find the desired csum for range [logical, logical + sectorsize), and store
  the csum into @csum.
  The search source is sctx->csum_list, which is a pre-populated list
  storing bytenr ordered csum ranges.  We're responsible to cleanup any range
  that is before @logical.
  Return 0 if there is no csum for the range.
  Return 1 if there is csum for the range and copied to @csum.
 The current csum range is beyond our range, no csum found 
		
		  The current sum is before our bytenr, since scrub is always
		  done in bytenr order, the csum will never be used anymore,
		  clean it up so that later calls won't bother with the range,
		  and continue search the next range.
 Now the csum range covers our bytenr, copy the csum 
 Cleanup the range if we're at the end of the csum range 
 scrub extent tries to collect up to 64 kB for each bio 
 push csums to sbio 
	 one ref inside this function, plus one for each page added to
 For scrub block 
 For scrub parity 
 Iterate over the stripe range in sectorsize steps 
 last one frees, either here or in bio completion for last page 
 push csums to sbio 
  Given a physical address, this will calculate it's
  logical offset. if this is a parity stripe, it will return
  the most left data stripe's logical offset.
  return 0 if it is a data stripe, 1 means parity stripe.
 Work out the disk rotation on this stripe-set 
 calculate which stripe this data locates 
 Check the comment in scrub_stripe() for why u32 is enough here 
	
	  Unlike chunk length, extent length should never go beyond
	  BTRFS_MAX_EXTENT_SIZE, thus u32 is enough here.
	
	  work on commit root. The related disk blocks are static as
	  long as COW is applied. This means, it is save to rewrite
	  them to repair disk errors without any race conditions
	
	  trigger the readahead for extent tree csum tree and wait for
	  completion. During readahead, the scrub is officially paused
	  to not hold off transaction commits
 FIXME it might be better to start readahead at commit root 
	
	  collect all data csums for the stripe to avoid seeking during
	  the scrub. This might currently (crc32) end up to be about 1MB
	
	  now find all extents for each stripe and scrub them
		
		  canceled?
		
		  check to see if we have to pause
 push queued extents 
 it is parity strip 
				 there's no smaller item, so stick with the
 out of this device extent 
			
			  If our block group was removed in the meanwhile, just
			  stop scrubbing since there is no point in continuing.
			  Continuing would prevent reusing its device extents
			  for new block groups for a long time.
			
			  trim extent to this stripe
					
					  loop until we find next data stripe
					  or we have finished all stripes.
 push queued extents 
		
		  Might have been an unused block group deleted by the cleaner
		  kthread or relocation.
		
		  get a reference on the corresponding block group to prevent
		  the chunk from going away while we scrub it
		 some chunks are removed but not committed to disk yet,
		
		  Make sure that while we are scrubbing the corresponding block
		  group doesn't get its logical address and its device extents
		  reused for another block group, which can possibly be of a
		  different type and different profile. We do this to prevent
		  false error detections and crashes due to bogus attempts to
		  repair extents.
		
		  we need call btrfs_inc_block_group_ro() with scrubs_paused,
		  to avoid deadlock caused by:
		  btrfs_inc_block_group_ro()
		  -> btrfs_wait_for_commit()
		  -> btrfs_commit_transaction()
		  -> btrfs_scrub_pause()
		
		  Don't do chunk preallocation for scrub.
		 
		  This is especially important for SYSTEM bgs, or we can hit
		  -EFBIG from btrfs_finish_chunk_alloc() like:
		  1. The only SYSTEM bg is marked RO.
		     Since SYSTEM bg is small, that's pretty common.
		  2. New SYSTEM bg will be allocated
		     Due to regular version will allocate new chunk.
		  3. New SYSTEM bg is empty and will get cleaned up
		     Before cleanup really happens, it's marked RO again.
		  4. Empty SYSTEM bg get scrubbed
		     We go back to 2.
		 
		  This can easily boost the amount of SYSTEM chunks if cleaner
		  thread can't be triggered fast enough, and use up all space
		  of btrfs_super_block::sys_chunk_array
		 
		  While for dev replace, we need to try our best to mark block
		  group RO, to prevent race between:
		  - Write duplication
		    Contains latest data
		  - Scrub copy
		    Contains data from commit tree
		 
		  If target block group is not marked RO, nocow writes can
		  be overwritten by scrub copy, causing data corruption.
		  So for dev-replace, it's not allowed to continue if a block
		  group is not RO.
			
			  btrfs_inc_block_group_ro return -ENOSPC when it
			  failed in creating new chunk for metadata.
			  It is not a problem for scrub, because
			  metadata are always cowed, and our scrub paused
			  commit_transactions.
		
		  Now the target block is marked RO, wait for nocow writes to
		  finish before dev-replace.
		  COW is fine, as COW never overwrites extents in commit tree.
		
		  flush, submit all pending read and write bios, afterwards
		  wait for them.
		  Note that in the dev replace case, a read request causes
		  write requests that are submitted in the read completion
		  worker. Therefore in the current situation, it is required
		  that all write requests are flushed, so that all read and
		  write requests are really completed when bios_in_flight
		  changes to 0.
		
		  must be called before we decrease @scrub_paused.
		  make sure we don't block transaction commit while
		  we are waiting pending workers finished.
		
		  We might have prevented the cleaner kthread from deleting
		  this block group if it was already unused because we raced
		  and set it to RO mode first. So add it back to the unused
		  list, otherwise it might not ever be deleted unless a manual
		  balance is triggered or it becomes used and unused again.
 Seed devices of a new filesystem has their own generation. 
  get a reference count on fs_info->scrub_workers. start worker if necessary
 Other thread raced in and created the workers for us 
		
		  in this case scrub is unable to calculate the checksum
		  the way scrub is implemented. Do not handle this
		  situation at all because it won't ever happen.
		
		  would exhaust the array bounds of pagev member in
		  struct scrub_block
 Allocate outside of device_list_mutex 
	
	  checking @scrub_pause_req here, we can avoid
	  race between committing transaction and scrubbing.
	
	  In order to avoid deadlock with reclaim when there is a transaction
	  trying to pause scrub, make sure we use GFP_NOFS for all the
	  allocations done at btrfs_scrub_pages() and scrub_pages_for_parity()
	  invoked by our callees. The pausing request is done when the
	  transaction commit starts, and it blocks the transaction until scrub
	  is paused (done at specific points at scrub_stripe() or right above
	  before incrementing fs_info->scrubs_running).
		
		  by holding device list mutex, we can
		  kick off writing super in log tree sync.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2014 Filipe David Borba Manana <fdmanana@gmail.com>
 Reset to defaults 
 Set NOCOMPRESS flag 
		
		  This is not strictly necessary as the property should be
		  valid, but in case it isn't, don't propagate it further.
		
		  Currently callers should be reserving 1 item for properties,
		  since we only have 1 property that we currently support.  If
		  we add more in the future we need to try and reserve more
		  space for them.  But we should also revisit how we do space
		  reservations if we do add more properties in the future.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
	
	  Search all extended backrefs in this item. We're only
	  looking through any collisions so most of the time this is
	  just going to compare against one buffer. If all is well,
	  we'll return success and the inode ref object.
 Returns NULL if no extref found 
	
	  Sanity check - did we find the right item for this name?
	  This should always succeed so error here will make the FS
	  readonly.
		
		  Common case only one ref in the item, remove the
		  whole item.
		
		  No refs were found, or we could not find the
		  name in our ref array. Find and remove the extended
		  inode ref then.
  btrfs_insert_inode_extref() - Inserts an extended inode ref into a tree.
  The caller must have checked against BTRFS_LINK_MAX already.
 Will return 0, -ENOMEM, -EMLINK, or -EEXIST or anything from the CoW path 
		 We ran out of space in the ref array. Need to
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) Qu Wenruo 2017.  All rights reserved.
  The module is used to catch unexpectedcorrupted tree block data.
  Such behavior can be caused either by a fuzzed image or bugs.
  The objective is to do leafnode validation checks when tree block is read
  from disk, and check every possible member, so other code won't
  need to checking them again.
  Due to the potential and unwanted damage, every checker needs to be
  carefully reviewed otherwise so it does not prevent mount of valid images.
  Error message should follow the following format:
  corrupt <type>: <identifier>, <reason>[, <bad_value>]
  @type:	leaf or node
  @identifier:	the necessary info to locate the leafnode.
  		It's recommended to decode key.objecitdoffset if it's
  		meaningful.
  @reason:	describe the error
  @bad_value:	optional, it's recommended to output bad value and its
 		expected value (range).
  Since comma is used to separate the components, only space is allowed
  inside each component.
  Append generic "corrupt leafnode root=%llu block=%llu slot=%d: " to @fmt.
  Allows callers to customize the output.
  Customized reporter for extent data item, since its key objectid and
  offset has its own meaning.
  Return 0 if the btrfs_file_extent_##name is aligned to @alignment
  Else return 1
  Customized report for dir_item, the only new important information is
  key->objectid, which represents inode number
  This functions checks prev_key->objectid, to ensure current key and prev_key
  share the same objectid as inode number.
  This is to detect missing INODE_ITEM in subvolume trees.
  Return true if everything is OK or we don't need to check.
  Return false if anything is wrong.
 No prev key, skip check 
 Only these key->types needs to be checked 
	
	  Only subvolume trees along with their reloc trees need this check.
	  Things like log tree doesn't follow this ino requirement.
 Error found 
	
	  Previous key must have the same key->objectid (ino).
	  It can be XATTR_ITEM, INODE_ITEM or just another EXTENT_DATA.
	  But if objectids mismatch, it means we have a missing
	  INODE_ITEM.
	
	  Make sure the item contains at least inline header, so the file
	  extent type is not some garbage.
	
	  Support for new compressionencryption must introduce incompat flag,
	  and must be caught in open_ctree().
 Inline extent must have 0 as key offset 
 Compressed inline extent has no on-disk size, skip it 
 Uncompressed inline extent size must match item size 
 Regular or preallocated extent has fixed item size 
 Catch extent end overflow 
	
	  Check that no two consecutive file extent items, in the same leaf,
	  present ranges that overlap each other.
 Inode item error output has the same format as dir_item_err() 
 For XATTR_ITEM, location key should be all 0 
 No such tree id 
 DIR_ITEMINDEXINODE_REF is not allowed to point to non-fs trees 
	
	  ROOT_ITEM with non-zero offset means this is a snapshot, created at
	  @offset transid.
	  Furthermore, for location key in DIR_ITEM, its offset is always -1.
	 
	  So here we only check offset for reloc tree whose key->offset must
	  be a valid tree.
 header itself should not cross item boundary 
 Location key check 
 dir type check 
 Namedata length check 
 header and namedata should not cross item boundary 
		
		  Special check for XATTRDIR_ITEM, as key->offset is name
		  hash, should match its name
	
	  Here we don't really care about alignment since extent allocator can
	  handle it.  We care more about the size.
 Only superblock eb is able to have such small offset 
		
		  Get the slot number by iterating through all slots, this
		  would provide better readability.
  The common chunk check which could also work on super block sys chunk array.
  Return -EUCLEAN if anything is corrupted.
  Return 0 if everything is OK.
  Enhanced version of chunk item checker.
  The common btrfs_check_chunk_valid() doesn't check item size since it needs
  to work on super block sys_chunk_array which doesn't have full item ptr.
 Let btrfs_check_chunk_valid() handle this error type 
	
	  For device total_bytes, we don't have reliable way to check it, as
	  it can be 0 for device removal. Device size check can only be done
	  by dev extents check.
	
	  Remaining members like io_aligntypegendev_group aren't really
	  utilized.  Skip them to make later usage of them easier.
 Here we use super block generation + 1 to handle log tree 
 Note for ROOT_TREE_DIR_ITEM, mkfs could set its transid 0 
	
	  For size and nbytes it's better not to be too strict, as for dir
	  item its sizenbytes can easily get wrong, but doesn't affect
	  anything in the fs. So here we skip the check.
	
	  S_IFMT is not bit mapped so we can't completely rely on
	  is_power_of_2has_single_bit_set, but it can save us from checking
	  FIFOCHRDIRREG.  Only needs to check BLK, LNK and SOCKS
	
	  For legacy root item, the members starting at generation_v2 will be
	  all filled with 0.
	  And since we allow geneartion_v2 as 0, it will still pass the check.
 Generation related 
 Alignment and level check 
 Flags check 
 Current pointer inside inline refs 
 Extent item end 
 Total refs in btrfs_extent_item 
 found total inline refs 
 key->objectid is the bytenr for both key types 
 key->offset is tree level for METADATA_ITEM_KEY 
	
	  EXTENTMETADATA_ITEM consists of:
	  1) One btrfs_extent_item
	     Records the total refs, type and generation of the extent.
	 
	  2) One btrfs_tree_block_info (for EXTENT_ITEM and tree backref only)
	     Records the first key and level of the tree block.
	 
	  2) Zero or more btrfs_extent_inline_ref(s)
	     Each inline ref has one btrfs_extent_inline_ref shows:
	     2.1) The ref type, one of the 4
	          TREE_BLOCK_REF	Tree block only
	          SHARED_BLOCK_REF	Tree block only
	          EXTENT_DATA_REF	Data only
	          SHARED_DATA_REF	Data only
	     2.2) Ref type specific data
	          Either using btrfs_extent_inline_ref::offset, or specific
	          data structure.
 Checks against extent_item 
 Check the special case of btrfs_tree_block_info 
 Check inline refs 
 inline_offset is subvolid of the owner, no need to check 
 Contains parent bytenr 
		
		  Contains owner subvolid, owner key objectid, adjusted offset.
		  The only obvious corruption can happen in that offset.
 Contains parent bytenr and ref count 
 No padding is allowed 
 Finally, check the inline refs against total refs 
		
		  We cannot check the extent_data_ref hash due to possible
		  overflow from the leaf due to hash collisions.
 namelen can't be 0, so item_size == sizeof() is also invalid 
		
		  NOTE: In theory we should record all found index numbers
		  to find any duplicated indexes, but that will be too time
		  consuming for inodes with too many hard links.
  Common point to switch the item-specific validation.
 No valid key type is 0, so all key should be larger than this key 
	
	  Extent buffers from a relocation tree have a owner field that
	  corresponds to the subvolume tree they are based on. So just from an
	  extent buffer alone we can not find out what is the id of the
	  corresponding subvolume tree, so we can not figure out if the extent
	  buffer corresponds to the root of the relocation tree or not. So
	  skip this check for relocation trees.
 These trees must never be empty 
 Unknown tree 
	
	  Check the following things to make sure this is a good leaf, and
	  leaf users won't need to bother with similar sanity checks:
	 
	  1) key ordering
	  2) item offset and size
	     No overlap, no hole, all inside the leaf.
	  3) item content
	     If possible, do comprehensive sanity check.
	     NOTE: All checks must only rely on the item data itself.
 Make sure the keys are in the right order 
		
		  Make sure the offset and ends are right, remember that the
		  item data starts at the end of the leaf and grows towards the
		  front.
		
		  Check to make sure that we don't point outside of the leaf,
		  just in case all the items are consistent to each other, but
		  all point outside of the leaf.
 Also check if the item pointer overlaps with btrfs item. 
			
			  Check if the item size and content meet other
			  criteria
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2008 Oracle.  All rights reserved.
  Btrfs LZO compression format
  Regular and inlined LZO compressed data extents consist of:
  1.  Header
      Fixed size. LZO_LEN (4) bytes long, LE32.
      Records the total size (including the header) of compressed data.
  2.  Segment(s)
      Variable size. Each segment includes one segment header, followed by data
      payload.
      One regular LZO compressed extent can have one or more segments.
      For inlined LZO compressed extent, only one segment is allowed.
      One segment represents at most one sector of uncompressed data.
  2.1 Segment header
      Fixed size. LZO_LEN (4) bytes long, LE32.
      Records the total size of the segment (not including the header).
      Segment header never crosses sector boundary, thus it's possible to
      have at most 3 padding zeros at the end of the sector.
  2.2 Data Payload
      Variable size. Size up limit should be lzo1x_worst_compress(sectorsize)
      which is 4419 for a 4KiB sectorsize.
  Example with 4K sectorsize:
  Page 1:
           0     0x2   0x4   0x6   0x8   0xa   0xc   0xe     0x10
  0x0000   |  Header   | SegHdr 01 | Data payload 01 ...     |
  ...
  0x0ff0   | SegHdr  N | Data payload  N     ...          |00|
                                                           ^^ padding zeros
  Page 2:
  0x1000   | SegHdr N+1| Data payload N+1 ...                |
 where decompressed data goes 
 where compressed data goes 
  Will do:
  - Write a segment header into the destination
  - Copy the compressed buffer into the destination
  - Make sure we have enough space in the last sector to fit a segment header
    If not, we will pad at most (LZO_LEN (4)) - 1 bytes of zeros.
  Will allocate new pages when needed.
	
	  We never allow a segment header crossing sector boundary, previous
	  run should ensure we have enough space left inside the sector.
 Allocate a new page 
 Copy compressed data 
 Allocate a new page 
	
	  Check if we can fit the next segment header into the remaining space
	  of the sector.
 The remaining size is not enough, pad it with zeros 
 Points to the file offset of input data 
 Points to the current output byte 
	
	  Skip the header for now, we will later come back and write the total
	  compressed size
 Get the input page first 
 Compress at most one sector of data each time 
		
		  Check if we're making it bigger after two sectors.  And if
		  it is so, give up.
 Check if we have reached page boundary 
 Store the size of all chunks of compressed data 
  Copy the compressed segment payload into @dest.
  For the payload there will be no padding, just need to do page switching.
 Compressed data length, can be unaligned 
 Offset inside the compressed data 
 Bytes decompressed so far 
	
	  LZO header length check
	 
	  The total length should not exceed the maximum extent length,
	  and all sectors should be used.
	  If this happens, it means the compressed extent is corrupted.
 Go through each lzo segment 
 Length of the compressed segment 
		
		  We should always have enough space for one segment header
		  inside current sector.
 Copy the compressed segment payload into workspace 
 Decompress the data 
 Copy the data into inode pages 
 All data read, exit 
 Check if the sector has enough space for a segment header 
 Skip the padding zeros 
	
	  the caller is already checking against PAGE_SIZE, but lets
	  move this check closer to the memcpymemset
	
	  btrfs_getblock is doing a zero on the tail of the page too,
	  but this will cover anything missing from the decompressed
	  data.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007,2008 Oracle.  All rights reserved.
	
	  csum type is validated at mount time
 csum type is validated at mount time 
  Return driver name if defined, otherwise the name that's also a valid driver
  name
 csum type is validated at mount time 
 this also releases the path 
  path release drops references on the extent buffers in the path
  and it drops any locks held by this path
  It is safe to call this on paths that no locks or extent buffers held.
  safely gets a reference on the root node of a tree.  A lock
  is not taken, so a concurrent writer may put a different node
  at the root of the tree.  See btrfs_lock_root_node for the
  looping required.
  The extent buffer returned by this has a reference taken, so
  it won't disappear.  It may stop being the root of the tree
  at any time because there are no locks held.
		
		  RCU really hurts here, we could free up the root node because
		  it was COWed but we may not get the new root node yet so do
		  the inc_not_zero dance and if it doesn't work then
		  synchronize_rcu and try again.
  Cowonly root (not-shareable trees, everything not subvolume or reloc roots),
  just get put onto a simple dirty list.  Transaction walks this list to make
  sure they get properly updated on disk.
 Want the extent tree to be the last on the list 
  used by snapshot creation to make a copy of a root for a tree with
  a given objectid.  The buffer with the new root node is returned in
  cow_ret, and this func returns zero on success or a negative error code.
  check if the tree block can be shared by multiple trees
	
	  Tree blocks not in shareable trees and tree roots are never shared.
	  If a block was allocated after the last snapshot and the block was
	  not allocated by tree relocation, we know the block is not shared.
	
	  Backrefs update rules:
	 
	  Always use full backrefs for extent pointers in tree block
	  allocated by tree relocation.
	 
	  If a shared tree block is no longer referenced by its owner
	  tree (btrfs_header_owner(buf) == root->root_key.objectid),
	  use full backrefs for extent pointers in tree block.
	 
	  If a tree block is been relocating
	  (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID),
	  use full backrefs for extent pointers in tree block.
	  The reason for this is some operations (such as drop tree)
	  are only allowed for blocks use full backrefs.
  does the dirty work in cow of a single block.  The parent block (if
  supplied) is updated to point to the new cow copy.  The new buffer is marked
  dirty and returned locked.  If you modify the block it needs to be marked
  dirty again.
  search_start -- an allocation hint for the new block
  empty_size -- a hint that you plan on doing more cow.  This is the size in
  bytes the allocator should try to find free next to the block it returns.
  This is just a hint and may be ignored by the allocator.
 cow is set to blocking by btrfs_init_new_buffer 
 Ensure we can see the FORCE_COW bit 
	
	  We do not need to cow a block if
	  1) this block is not created or changed in this transaction;
	  2) this block does not belong to TREE_RELOC tree;
	  3) the root is not forced COW.
	 
	  What is forced COW:
	     when we create snapshot during committing the transaction,
	     after we've finished copying src root, we must COW the shared
	     block to ensure the metadata consistency.
  cows a single block, see __btrfs_cow_block for the real work.
  This version of it has extra checks so that a block isn't COWed more than
  once per transaction, as long as it hasn't been written yet
	
	  Before CoWing this block for later modification, check if it's
	  the subtree root and do the delayed subtree trace if needed.
	 
	  Also We don't care about the error, as it's handled internally.
  helper function for defrag to decide if two blocks pointed to by a
  node are actually close by
  Compare two keys, on little-endian the disk order is same as CPU order and
  we can avoid the conversion.
  compare two keys in a memcmp fashion
  same as comp_keys only with two btrfs_key's
  this is used by the defrag code to go through all the
  leaves pointed to by a node and reallocate them so that
  disk order is close to key order
  search for key in the extent_buffer.  The items start at offset p,
  and they are item_size apart.
  the slot in the array is returned via slot, and it points to
  the place where you would insert key if it is not found in
  the array.
  Slot may point to total number of items if the key is bigger than
  all of the keys
  simple bin_search frontend that does the right thing for
  leaves vs nodes
 given a node and slot number, this reads the blocks it points to.  The
  extent buffer is returned with a reference taken (but unlocked).
  node level balancing, used to make sure nodes are in proper order for
  item deletion.  We balance from the top down, so we have to make sure
  that a deletion won't leave an node completely empty later on.
	
	  deal with the case where there is only one pointer in the root
	  by promoting the node below to a root
 promote the child to a root 
 once for the path 
 once for the root ptr 
 first, try to make some room in the middle buffer 
	
	  then try to empty the right most buffer into the middle
		
		  we're not allowed to leave a node with one item in the
		  tree during a delete.  A deletion from lower in the tree
		  could try to delete the only pointer in this node.
		  So, pull some keys from the left.
		  There has to be a left pointer at this point because
		  otherwise we would have pulled some pointers from the
		  right
 update the parent key to reflect our changes 
 update the path 
 left was locked after cow 
 double check we haven't messed things up 
 Node balancing for insertion.  Here we only split or push nodes around
  when they are completely full.  This is also done top down, so we
  have to be pessimistic.
 first, try to make some room in the middle buffer 
	
	  then try to empty the right most buffer into the middle
  readahead one full node of leaves, finding things that are close
  to the block in 'slot', and triggering ra on them.
	
	  Since the time between visiting leaves is much shorter than the time
	  between visiting nodes, limit read ahead of nodes to 1, to avoid too
	  much IO at once (possibly random).
  when we walk down the tree, it is usually safe to unlock the higher layers
  in the tree.  The exceptions are when our path goes through slot 0, because
  operations on the tree might require changing key pointers higher up in the
  tree.
  callers might also have set path->keep_locks, which tells this code to keep
  the lock if the path points to the last slot in the block.  This is part of
  walking through the tree, and selecting the next slot in the higher block.
  lowest_unlock sets the lowest level in the tree we're allowed to unlock.  so
  if lowest_unlock is 1, level 0 won't be unlocked
  helper function for btrfs_search_slot.  The goal is to find a block
  in cache without setting the path to blocking.  If we find the block
  we return zero and the path is unchanged.
  If we can't find the block, we set the path blocking and do some
  reada.  -EAGAIN is returned and the search must be repeated.
 first we do an atomic uptodate check 
			
			  Do extra check for first_key, eb can be stale due to
			  being cached, read from scrub, or have multiple
			  parents (shared tree blocks).
 now we're allowed to do a blocking uptodate check 
	
	  reduce lock contention at high levels
	  of the btree by dropping locks before
	  we read.  Don't release the lock on the current
	  level because we need to walk this node to figure
	  out which blocks to read.
		
		  If the read above didn't mark this buffer up to date,
		  it will never end up being up to date.  Set ret to EIO now
		  and give up so that our caller doesn't loop forever
		  on our EAGAINs.
  helper function for btrfs_search_slot.  This does all of the checks
  for node-level blocks and does any balancing required based on
  the ins_len.
  If no extra work was required, zero is returned.  If we had to
  drop the path, -EAGAIN is returned and btrfs_search_slot must
  start over
 We try very hard to do read locks on the root 
		
		  The commit roots are read only so we always do read locks,
		  and we always must hold the commit_root_sem when doing
		  searches on them, the only exception is send where we don't
		  want to block transaction commits for a long time, so
		  we need to clone the commit root in order to avoid races
		  with transaction commits that create a snapshot of one of
		  the roots used by a send operation.
		
		  Ensure that all callers have set skip_locking when
		  p->search_commit_root = 1.
	
	  If the level is set to maximum, we can skip trying to get the read
	  lock.
		
		  We don't know the level of the root node until we actually
		  have it read locked
 Whoops, must trade for write lock 
 The level might have changed, check again 
	
	  Callers are responsible for dropping b's references.
  btrfs_search_slot - look for a key in a tree and perform necessary
  modifications to preserve tree invariants.
  @trans:	Handle of transaction, used when modifying the tree
  @p:		Holds all btree nodes along the search path
  @root:	The root node of the tree
  @key:	The key we are looking for
  @ins_len:	Indicates purpose of search:
               >0  for inserts it's size of item inserted ()
               <0  for deletions
                0  for plain searches, not modifying the tree
               () If size of item inserted doesn't include
               sizeof(struct btrfs_item), then p->search_for_extension must
               be set.
  @cow:	boolean should CoW operations be performed. Must always be 1
 		when modifying the tree.
  If @ins_len > 0, nodes and leaves will be split as we walk down the tree.
  If @ins_len < 0, nodes will be merged as we walk down the tree (if possible)
  If @key is found, 0 is returned and you can find the item in the leaf level
  of the path (level 0)
  If @key isn't found, 1 is returned and the leaf level of the path (level 0)
  points to the slot where it should be inserted
  If an error is encountered while searching the tree a negative error number
  is returned
 everything at write_lock_level or lower must be write locked 
		 when we are removing items, we might have to go up to level
		  two as we update tree pointers  Make sure we keep write
		  for those levels as well
		
		  for inserting items, make sure we have a write lock on
		  level 1 so we can update keys
			
			  if we don't really need to cow this block
			  then we don't want to set the path blocking,
			  so we test it here
			
			  must have write locks on this node and the
			  parent
		
		  Leave path with blocking locks to avoid massive
		  lock context switch, this is made on purpose.
		
		  we have a lock on b and as long as we aren't changing
		  the tree, there is no way to for the items in b to change.
		  It is safe to drop the lock on our parent before we
		  go through the expensive btree search on b.
		 
		  If we're inserting or deleting (ins_len != 0), then we might
		  be changing slot zero, which may require changing the parent.
		  So, we can't drop the lock until after we know which slot
		  we're operating on.
		
		  If btrfs_bin_search returns an exact match (prev_cmp == 0)
		  we can safely assume the target key will always be in slot 0
		  on lower levels due to the invariants BTRFS' btree provides,
		  namely that a btrfs_key_ptr entry always points to the
		  lowest key in the child node, thus we can skip searching
		  lower levels
			
			  Item key already exists. In this case, if we are
			  allowed to insert the item (for example, in dir_item
			  case, item key collision is allowed), it will be
			  merged with the original item. Only the item size
			  grows, no new btrfs item will be added. If
			  search_for_extension is not set, ins_len already
			  accounts the size btrfs_item, deduct it here so leaf
			  space check will be correct.
		
		  Slot 0 is special, if we change the key we have to update
		  the parent pointer which means we must have a write lock on
		  the parent
  Like btrfs_search_slot, this looks for a key in the given tree. It uses the
  current state of the tree together with the operations recorded in the tree
  modification log to search for the key in a previous version of this tree, as
  denoted by the time_seq parameter.
  Naturally, there is no support for insert, delete or cow operations.
  The resulting path and return value will be set up as if we called
  btrfs_search_slot at that point in time with ins_len and cow both set to 0.
		
		  we have a lock on b and as long as we aren't changing
		  the tree, there is no way to for the items in b to change.
		  It is safe to drop the lock on our parent before we
		  go through the expensive btree search on b.
  helper to use instead of search slot if no exact match is needed but
  instead the next or previous item should be returned.
  When find_higher is true, the next higher item is returned, the next lower
  otherwise.
  When return_any and find_higher are both true, and no higher item is found,
  return the next lower instead.
  When return_any is true and find_higher is false, and no lower item is found,
  return the next higher instead.
  It returns 0 if any item is found, 1 if none is found (tree empty), and
  < 0 on error
	
	  a return value of 1 means the path is at the position where the
	  item should be inserted. Normally this is the next bigger item,
	  but in case the previous item is the last in a leaf, path points
	  to the first free slot in the previous leaf, i.e. at an invalid
	  item.
			
			  no higher item found, return the next
			  lower instead
			
			  no lower item found, return the next
			  higher instead
  Execute search and call btrfs_previous_item to traverse backwards if the item
  was not found.
  Return 0 if found, 1 if not found and < 0 if error.
  adjust the pointers going up the tree, starting at level
  making sure the right key of each node is points to 'key'.
  This is used after shifting pointers to the left, so it stops
  fixing up pointers when a given leafnode is not in slot 0 of the
  higher levels
  update item key.
  This function isn't completely safe. It's the caller's responsibility
  that the new key won't break the order
  Check key order of two sibling extent buffers.
  Return true if something is wrong.
  Return false if everything is fine.
  Tree-checker only works inside one tree block, thus the following
  corruption can not be detected by tree-checker:
  Leaf @left			| Leaf @right
  --------------------------------------------------------------
  | 1 | 2 | 3 | 4 | 5 | f6 |   | 7 | 8 |
  Key f6 in leaf @left itself is valid, but not valid when the next
  key in leaf @right is 7.
  This can only be checked at tree block merge time.
  And since tree checker has ensured all key order in each tree block
  is correct, we only need to bother the last key of @left and the first
  key of @right.
 No key to check in one of the tree blocks 
  try to push data from one node into the next node left in the
  tree.
  returns 0 if some ptrs were pushed left, < 0 if there was some horrible
  error, and > 0 if there was no room in the left hand block.
			 leave at least 8 pointers in the node if
			  we aren't going to empty it
 dst is the left eb, src is the middle eb 
		
		  Don't call btrfs_tree_mod_log_insert_move() here, key removal
		  was already fully logged by btrfs_tree_mod_log_eb_copy() above.
  try to push data from one node into the next node right in the
  tree.
  returns 0 if some ptrs were pushed, < 0 if there was some horrible
  error, and > 0 if there was no room in the right hand block.
  this will  only push up to 12 the contents of the left node over
 don't try to empty the node 
 dst is the right eb, src is the middle eb 
  helper function to insert a new root level in the tree.
  A new node is allocated, and a single item is inserted to
  point to the existing root
  returns zero on success or < 0 on failure.
 the super has an extra ref to root->node 
  worker function to insert a single pointer in a node.
  the node should have enough room for the pointer already
  slot and level indicate where you want the key to go, and
  blocknr is the block the key points to.
  split the node at the specified level in path in two.
  The path is corrected to point to the appropriate node after the split
  Before splitting this tries to make some room in the node by pushing
  left and right, if either one works, it returns right away.
  returns 0 on success and < 0 on failure
		
		  trying to split the root, lets make a new one
		 
		  tree mod log: We don't log_removal old root in
		  insert_new_root, because that root buffer will be kept as a
		  normal node. We are going to log removal of half of the
		  elements below with btrfs_tree_mod_log_eb_copy(). We're
		  holding a tree lock on the buffer, which is why we cannot
		  race with other tree_mod_log users.
  how many bytes are required to store the items in a leaf.  start
  and nr indicate which items in the leaf to check.  This totals up the
  space used both by the item structs and the item data
  The space between the end of the leaf items and
  the start of the leaf data.  IOW, how much room
  the leaf has left for both items and data
  min slot controls the lowest index we're willing to push to the
  right.  We'll push up to and including min_slot, but no lower
 push left to right 
 make room in the right data area 
 copy from the left data area 
 copy the items from left to right 
 update the item pointers 
 then fixup the leaf pointer in the path 
  push some data in the path leaf to the right, trying to free up at
  least data_size bytes.  returns zero if the push worked, nonzero otherwise
  returns 1 if the push failed because the other node didn't have enough
  room, 0 if everything worked out and < 0 if there were major errors.
  this will push starting from min_slot to the end of the leaf.  It won't
  push any slot lower than min_slot
	
	  slot + 1 is not valid or we fail to read the right node,
	  no big deal, just return.
 cow and double check 
		 Key greater than all keys in the leaf, right neighbor has
		  enough room for it and we're not emptying our leaf to delete
		  it, therefore use right neighbor to insert the new item and
  push some data in the path leaf to the left, trying to free up at
  least data_size bytes.  returns zero if the push worked, nonzero otherwise
  max_slot can put a limit on how far into the leaf we'll push items.  The
  item at 'max_slot' won't be touched.  Use (u32)-1 to make us do all the
  items
 push data from right to left 
 fixup right node 
 then fixup the leaf pointer in the path 
  push some data in the path leaf to the left, trying to free up at
  least data_size bytes.  returns zero if the push worked, nonzero otherwise
  max_slot can put a limit on how far into the leaf we'll push items.  The
  item at 'max_slot' won't be touched.  Use (u32)-1 to make us push all the
  items
	
	  slot - 1 is not valid or we fail to read the left node,
	  no big deal, just return.
 cow and double check 
 we hit -ENOSPC, but it isn't fatal here 
  split the path's leaf in two, making sure there is at least data_size
  available for the resulting leaf level of the path.
  double splits happen when we need to insert a big item in the middle
  of a leaf.  A double split can leave us with 3 mostly empty leaves:
  leaf: [ slots 0 - N] [ our target ] [ N + 1 - total in leaf ]
           A                 B                 C
  We avoid this by trying to push the items on either side of our target
  into the adjacent leaves.  If all goes well we can avoid the double split
  completely.
	
	  try to push all the items after our slot into the
	  right leaf
	
	  our goal is to get our slot at the start or end of a leaf.  If
	  we've done so we're done
 try to push all the items before our slot into the next leaf 
  split the path's leaf in two, making sure there is at least data_size
  available for the resulting leaf level of the path.
  returns 0 if all went well and < 0 on failure.
 first try to make some room by pushing left and right 
 did the pushes work? 
	
	  We have to about BTRFS_NESTING_NEW_ROOT here if we've done a double
	  split, because we're only allowed to have MAX_LOCKDEP_SUBCLASSES
	  subclasses, which is 8 at the time of this patch, and we've maxed it
	  out.  In the future we could add a
	  BTRFS_NESTING_SPLIT_THE_SPLITTENING if we need to, but for now just
	  use BTRFS_NESTING_NEW_ROOT.
		
		  We create a new leaf 'right' for the required ins_len and
		  we'll do btrfs_mark_buffer_dirty() on this leaf after copying
		  the content of ins_len to 'right'.
 if our item isn't there, return now 
 the leaf has  changed, it now has room.  return now 
 shift the items 
 write the data for the start of the original item 
 write the data for the new item 
  This function splits a single item into two items,
  giving 'new_key' to the new item and splitting the
  old one at split_offset (from the start of the item).
  The path may be released by this operation.  After
  the split, the path is pointing to the old item.  The
  new item is going to be in the same node as the old one.
  Note, the item being split must be smaller enough to live alone on
  a tree block with room for one extra struct btrfs_item
  This allows us to split the item in place, keeping a lock on the
  leaf the entire time.
  make the item pointed to by the path smaller.  new_size indicates
  how small to make it, and from_end tells us if we just chop bytes
  off the end of the item or if we shift the item to chop bytes off
  the front.
	
	  item0..itemN ... dataN.offset..dataN.size .. data0.size
 first correct the data pointers 
 shift the data 
  make the item pointed to by the path bigger, data_size is the added size.
	
	  item0..itemN ... dataN.offset..dataN.size .. data0.size
 first correct the data pointers 
 shift the data 
  setup_items_for_insert - Helper called before inserting one or more items
  to a leaf. Main purpose is to save stack depth by doing the bulk of the work
  in a function that doesn't call btrfs_search_slot
  @root:	root we are inserting items to
  @path:	points to the leafslot where we are going to insert new items
  @batch:      information about the batch of items to insert
	
	  Before anything else, update keys in the parent and other ancestors
	  if needed, then release the write locks on them, so that other tasks
	  can use them while we modify the leaf.
		
		  item0..itemN ... dataN.offset..dataN.size .. data0.size
 first correct the data pointers 
 shift the items 
 shift the data 
 setup the item for the new data 
  Insert a new item into a leaf.
  @root:      The root of the btree.
  @path:      A path pointing to the target leaf and slot.
  @key:       The key of the new item.
  @data_size: The size of the data associated with the new key.
  Given a key and some data, insert items into the tree.
  This does all the path init required, making room in the tree if needed.
  Given a key and some data, insert an item into the tree.
  This does all the path init required, making room in the tree if needed.
  This function duplicates an item, giving 'new_key' to the new item.
  It guarantees both items live in the same tree leaf and the new item is
  contiguous with the original item.
  This allows us to split a file extent in place, keeping a lock on the leaf
  the entire time.
  delete the pointer from a given node.
  the tree should have been previously balanced so the deletion does not
  empty a node.
 just turn the root into a leaf and break 
  a helper function to delete the leaf pointed to by path->slots[1] and
  path->nodes[1].
  This deletes the pointer in path->nodes[1] and frees the leaf
  block extent.  zero is returned if it all worked out, < 0 otherwise.
  The path must have already been setup for deleting the leaf, including
  all the proper balancing.  path->nodes[1] must be locked.
	
	  btrfs_free_extent is expensive, we want to make sure we
	  aren't holding any locks when we call it
  delete the item at the leaf level in path.  If that empties
  the leaf, remove it from the tree
 delete the leaf if we've emptied it 
 delete the leaf if it is mostly empty 
			 push_leaf_left fixes the path.
			  make sure the path still points to our leaf
			  for possible call to del_ptr below
				 if we're still in the path, make sure
				  we're dirty.  Otherwise, one of the
				  push_leaf functions must have already
				  dirtied this buffer
  search the tree again to find a leaf with lesser keys
  returns 0 if it found something or 1 if there are no lesser leaves.
  returns < 0 on io errors.
  This may release the path, and so you may lose any locks held at the
  time you call it.
	
	  We might have had an item with the previous key in the tree right
	  before we released our path. And after we released our path, that
	  item might have been pushed to the first slot (0) of the leaf we
	  were holding due to a tree balance. Alternatively, an item with the
	  previous key can exist as the only element of a leaf (big fat item).
	  Therefore account for these 2 cases, so that our callers (like
	  btrfs_previous_item) don't miss an existing item with a key matching
	  the previous key we computed above.
  A helper function to walk down the tree starting at min_key, and looking
  for nodes or leaves that are have a minimum transaction id.
  This is used by the btree defrag code, and tree logging
  This does not cow, but it does stuff the starting key it finds back
  into min_key, so you can call btrfs_search_slot with cow=1 on the
  key and get a writable path.
  This honors path->lowest_level to prevent descent past a given level
  of the tree.
  min_trans indicates the oldest transaction that you are interested
  in walking through.  Any nodes or leaves older than min_trans are
  skipped over (without reading them).
  returns zero if something useful was found, < 0 on error and 1 if there
  was nothing in the tree that matched the search criteria.
 at the lowest level, we're done, setup the path and exit 
		
		  check this node pointer against the min_trans parameters.
		  If it is too old, skip to the next one.
		
		  we didn't find a candidate key in this node, walk forward
		  and find another one
 save our key for returning back 
  this is similar to btrfs_next_leaf, but does not try to preserve
  and fixup the path.  It looks for and returns the next key in the
  tree based on the current path and the min_trans parameters.
  0 is returned if another key is found, < 0 if there are any errors
  and 1 is returned if there are no higher keys in the tree
  path->keep_locks should be set to 1 on the search made before
  calling this function.
	
	  by releasing the path above we dropped all our locks.  A balance
	  could have added more items next to the key that used to be
	  at the very end of the block.  So, check again here and
	  advance the path if there are now more items available.
	
	  So the above check misses one case:
	  - after releasing the path above, someone has removed the item that
	    used to be at the very end of the block, and balance between leafs
	    gets another one with bigger key.offset to replace it.
	 
	  This one should be returned as well, or we can get leaf corruption
	  later(esp. in __btrfs_drop_extents()).
	 
	  And a bit more explanation about this check,
	  with ret > 0, the key isn't found, the path points to the slot
	  where it should be inserted, so the path->slots[0] item must be the
	  bigger one.
		
		  Our current level is where we're going to start from, and to
		  make sure lockdep doesn't complain we need to drop our locks
		  and nodes from 0 to our current level.
				
				  If we don't get the lock, we may be racing
				  with push_leaf_left, holding that lock while
				  itself waiting for the leaf we've currently
				  locked. To solve this situation, we give up
				  on our lock and cycle.
  this uses btrfs_prev_leaf to walk backwards in the tree, and keeps
  searching until it gets past min_objectid or finds an item of 'type'
  returns 0 if something is found, 1 if nothing was found and < 0 on error
  search in extent tree to find a previous MetadataData extent item with
  min objecitd.
  returns 0 if something is found, 1 if nothing was found and < 0 on error
 SPDX-License-Identifier: GPL-2.0
  HOW DOES SPACE RESERVATION WORK
  If you want to know about delalloc specifically, there is a separate comment
  for that with the delalloc code.  This comment is about how the whole system
  works generally.
  BASIC CONCEPTS
    1) space_info.  This is the ultimate arbiter of how much space we can use.
    There's a description of the bytes_ fields with the struct declaration,
    refer to that for specifics on each field.  Suffice it to say that for
    reservations we care about total_bytes - SUM(space_info->bytes_) when
    determining if there is space to make an allocation.  There is a space_info
    for METADATA, SYSTEM, and DATA areas.
    2) block_rsv's.  These are basically buckets for every different type of
    metadata reservation we have.  You can see the comment in the block_rsv
    code on the rules for each type, but generally block_rsv->reserved is how
    much space is accounted for in space_info->bytes_may_use.
    3) btrfs_calc_size.  These are the worst case calculations we used based
    on the number of items we will want to modify.  We have one for changing
    items, and one for inserting new items.  Generally we use these helpers to
    determine the size of the block reserves, and then use the actual bytes
    values to adjust the space_info counters.
  MAKING RESERVATIONS, THE NORMAL CASE
    We call into either btrfs_reserve_data_bytes() or
    btrfs_reserve_metadata_bytes(), depending on which we're looking for, with
    num_bytes we want to reserve.
    ->reserve
      space_info->bytes_may_reserve += num_bytes
    ->extent allocation
      Call btrfs_add_reserved_bytes() which does
      space_info->bytes_may_reserve -= num_bytes
      space_info->bytes_reserved += extent_bytes
    ->insert reference
      Call btrfs_update_block_group() which does
      space_info->bytes_reserved -= extent_bytes
      space_info->bytes_used += extent_bytes
  MAKING RESERVATIONS, FLUSHING NORMALLY (non-priority)
    Assume we are unable to simply make the reservation because we do not have
    enough space
    -> __reserve_bytes
      create a reserve_ticket with ->bytes set to our reservation, add it to
      the tail of space_info->tickets, kick async flush thread
    ->handle_reserve_ticket
      wait on ticket->wait for ->bytes to be reduced to 0, or ->error to be set
      on the ticket.
    -> btrfs_async_reclaim_metadata_spacebtrfs_async_reclaim_data_space
      Flushes various things attempting to free up space.
    -> btrfs_try_granting_tickets()
      This is called by anything that either subtracts space from
      space_info->bytes_may_use, ->bytes_pinned, etc, or adds to the
      space_info->total_bytes.  This loops through the ->priority_tickets and
      then the ->tickets list checking to see if the reservation can be
      completed.  If it can the space is added to space_info->bytes_may_use and
      the ticket is woken up.
    -> ticket wakeup
      Check if ->bytes == 0, if it does we got our reservation and we can carry
      on, if not return the appropriate error (ENOSPC, but can be EINTR if we
      were interrupted.)
  MAKING RESERVATIONS, FLUSHING HIGH PRIORITY
    Same as the above, except we add ourselves to the
    space_info->priority_tickets, and we do not use ticket->wait, we simply
    call flush_space() ourselves for the states that are safe for us to call
    without deadlocking and hope for the best.
  THE FLUSHING STATES
    Generally speaking we will have two cases for each state, a "nice" state
    and a "ALL THE THINGS" state.  In btrfs we delay a lot of work in order to
    reduce the locking over head on the various trees, and even to keep from
    doing any work at all in the case of delayed refs.  Each of these delayed
    things however hold reservations, and so letting them run allows us to
    reclaim space so we can make new reservations.
    FLUSH_DELAYED_ITEMS
      Every inode has a delayed item to update the inode.  Take a simple write
      for example, we would update the inode item at write time to update the
      mtime, and then again at finish_ordered_io() time in order to update the
      isize or bytes.  We keep these delayed items to coalesce these operations
      into a single operation done on demand.  These are an easy way to reclaim
      metadata space.
    FLUSH_DELALLOC
      Look at the delalloc comment to get an idea of how much space is reserved
      for delayed allocation.  We can reclaim some of this space simply by
      running delalloc, but usually we need to wait for ordered extents to
      reclaim the bulk of this space.
    FLUSH_DELAYED_REFS
      We have a block reserve for the outstanding delayed refs space, and every
      delayed ref operation holds a reservation.  Running these is a quick way
      to reclaim space, but we want to hold this until the end because COW can
      churn a lot and we can avoid making some extent tree modifications if we
      are able to delay for as long as possible.
    ALLOC_CHUNK
      We will skip this the first time through space reservation, because of
      overcommit and we don't want to have a lot of useless metadata space when
      our worst case reservations will likely never come true.
    RUN_DELAYED_IPUTS
      If we're freeing inodes we're likely freeing checksums, file extent
      items, and extent tree items.  Loads of space could be freed up by these
      operations, however they won't be usable until the transaction commits.
    COMMIT_TRANS
      This will commit the transaction.  Historically we had a lot of logic
      surrounding whether or not we'd commit the transaction, but this waits born
      out of a pre-tickets era where we could end up committing the transaction
      thousands of times in a row without making progress.  Now thanks to our
      ticketing system we know if we're not making progress and can error
      everybody out after a few commits rather than burning the disk hoping for
      a different answer.
  OVERCOMMIT
    Because we hold so many reservations for metadata we will allow you to
    reserve more space than is currently free in the currently allocate
    metadata space.  This only happens with metadata, data does not allow
    overcommitting.
    You can see the current logic for when we allow overcommit in
    btrfs_can_overcommit(), but it only applies to unallocated space.  If there
    is no unallocated space to be had, all reservations are kept within the
    free space in the allocated metadata chunks.
    Because of overcommitting, you generally want to use the
    btrfs_can_overcommit() logic for metadata allocations, as it does the right
    thing with or without extra unallocated space.
  after adding space to the filesystem, we need to clear the full flags
  on all the space infos.
	
	  If we have dup, raid1 or raid10 then only half of the free
	  space is actually usable.  For raid56, the space info used
	  doesn't include the parity drive, so we don't have to
	  change the math
	
	  If we aren't flushing all things, let us overcommit up to
	  12th of the space. If we can flush, don't let us overcommit
	  too much, let it overcommit up to 18 of the space.
 Don't overcommit when in mixed mode 
  This is for space we already have accounted in space_info->bytes_may_use, so
  basically when we're returning space from block_rsv's.
 Check and see if our ticket can be satisfied now. 
 The free space could be negative in case of overcommit 
  shrink metadata reservation for delalloc
 Calc the number of the pages we need flush for space reservation 
		
		  to_reclaim is set to however much metadata we need to
		  reclaim, but reclaiming that much data doesn't really track
		  exactly.  What we really want to do is reclaim full inode's
		  worth of reservations, however that's not available to us
		  here.  We will take a fraction of the delalloc bytes for our
		  flushing loops and hope for the best.  Delalloc will expand
		  the amount we write to cover an entire dirty extent, which
		  will reclaim the metadata reservation for that range.  If
		  it's not enough subsequent flush stages will be more
		  aggressive.
	
	  If we are doing more ordered than delalloc we need to just wait on
	  ordered extents, otherwise we'll waste time trying to flush delalloc
	  that likely won't give us the space back we need.
		
		  We need to make sure any outstanding async pages are now
		  processed before we continue.  This is because things like
		  sync_inode() try to be smart and skip writing if the inode is
		  marked clean.  We don't use filemap_fwrite for flushing
		  because we want to control how many pages we write out at a
		  time, thus this is the only safe way to make sure we've
		  waited for outstanding compressed workers to have started
		  their jobs and thus have ordered extents set up properly.
		 
		  This exists because we do not want to wait for each
		  individual inode to finish its async work, we simply want to
		  start the IO on everybody, and then come back here and wait
		  for all of the async work to catch up.  Once we're done with
		  that we know we'll have ordered extents for everything and we
		  can decide if we wait for that or not.
		 
		  If we choose to replace this in the future, make absolutely
		  sure that the proper waiting is being done in the async case,
		  as there have been bugs in that area before.
		
		  We don't want to wait forever, if we wrote less pages in this
		  loop than we have outstanding, only wait for that number of
		  pages, otherwise we can wait for all async pages to finish
		  before continuing.
		
		  If we are for preemption we just want a one-shot of delalloc
		  flushing so we can stop flushing if we decide we don't need
		  to anymore.
  Try to flush some data based on policy set by @state. This is only advisory
  and may fail for various reasons. The caller is supposed to examine the
  state of @space_info to detect the outcome.
		
		  If we have pending delayed iputs then we could free up a
		  bunch of pinned space, so make sure we run the iputs before
		  we do our pinned bytes check below.
	
	  We may be flushing because suddenly we have less space than we had
	  before, and now we're well over-committed based on our current free
	  space.  If that's the case add in our overage so we make sure to put
	  appropriate pressure on the flushing state machine.
 If we're just plain full then async reclaim just slows us down. 
 The total flushable belongs to the global rsv, don't flush. 
	
	  128MiB is 14 of the maximum global rsv size.  If we have less than
	  that devoted to other reservations then there's no sense in flushing,
	  we don't have a lot of things that need flushing.
	
	  We have tickets queued, bail so we don't compete with the async
	  flushers.
	
	  If we have over half of the free space occupied by reservations or
	  pinned then we want to start flushing.
	 
	  We do not do the traditional thing here, which is to say
	 
	    if (used >= ((total_bytes + avail)  2))
	      return 1;
	 
	  because this doesn't quite work how we want.  If we had more than 50%
	  of the space_info used by bytes_used and we had 0 available we'd just
	  constantly run the background flusher.  Instead we want it to kick in
	  if our reclaimable space exceeds our clamped free space.
	 
	  Our clamping range is 2^1 -> 2^8.  Practically speaking that means
	  the following:
	 
	  Amount of RAM        Minimum threshold       Maximum threshold
	 
	         256GiB                     1GiB                  128GiB
	         128GiB                   512MiB                   64GiB
	          64GiB                   256MiB                   32GiB
	          32GiB                   128MiB                   16GiB
	          16GiB                    64MiB                    8GiB
	 
	  These are the range our thresholds will fall in, corresponding to how
	  much delalloc we need for the background flusher to kick in.
	
	  If we have more ordered bytes than delalloc bytes then we're either
	  doing a lot of DIO, or we simply don't have a lot of delalloc waiting
	  around.  Preemptive flushing is only useful in that it can free up
	  space before tickets need to wait for things to finish.  In the case
	  of ordered extents, preemptively waiting on ordered extents gets us
	  nothing, if our reservations are tied up in ordered extents we'll
	  simply have to slow down writers by forcing them to wait on ordered
	  extents.
	 
	  In the case that ordered is larger than delalloc, only include the
	  block reserves that we would actually be able to directly reclaim
	  from.  In this case if we're heavy on metadata operations this will
	  clearly be heavy enough to warrant preemptive flushing.  In the case
	  of heavy DIO or ordered reservations, preemptive flushing will just
	  waste time and cause us to slow down.
	 
	  We want to make sure we truly are maxed out on ordered however, so
	  cut ordered in half, and if it's still higher than delalloc then we
	  can keep flushing.  This is to avoid the case where we start
	  flushing, and now delalloc == ordered and we stop preemptively
	  flushing when we could still have several gigs of delalloc to flush.
  maybe_fail_all_tickets - we've exhausted our flushing, start failing tickets
  @fs_info - fs_info for this fs
  @space_info - the space info we were flushing
  We call this when we've exhausted our flushing ability and haven't made
  progress in satisfying tickets.  The reservation code handles tickets in
  order, so if there is a large ticket first and then smaller ones we could
  very well satisfy the smaller tickets.  This will attempt to wake up any
  tickets in the list to catch this case.
  This function returns true if it was able to make progress by clearing out
  other tickets, or if it stumbles across a ticket that was smaller than the
  first ticket.
		
		  We're just throwing tickets away, so more flushing may not
		  trip over btrfs_try_granting_tickets, so we need to call it
		  here to see if we can make progress with the next ticket in
		  the list.
  This is for normal flushers, we can wait all goddamned day if we want to.  We
  will loop and continuously try to flush as long as we are making progress.
  We count progress as clearing off tickets each time we have to loop.
		
		  We do not want to empty the system of delalloc unless we're
		  under heavy pressure, so allow one trip through the flushing
		  logic before we start doing a FLUSH_DELALLOC_FULL.
		
		  We don't want to force a chunk allocation until we've tried
		  pretty hard to reclaim space.  Think of the case where we
		  freed up a bunch of space and so have a lot of pinned space
		  to reclaim.  We would rather use that than possibly create a
		  underutilized metadata chunk.  So if this is our first run
		  through the flushing state machine skip ALLOC_CHUNK_FORCE and
		  commit the transaction.  If nothing has changed the next go
		  around then we can force a chunk allocation.
  This handles pre-flushing of metadata space before we get to the point that
  we need to start blocking threads on tickets.  The logic here is different
  from the other flush paths because it doesn't rely on tickets to tell us how
  much we need to flush, instead it attempts to keep us below the 80% full
  watermark of space by flushing whichever reservation pool is currently the
  largest.
		
		  We don't have a precise counter for the metadata being
		  reserved for delalloc, so we'll approximate it by subtracting
		  out the block rsv's space from the bytes_may_use.  If that
		  amount is higher than the individual reserves, then we can
		  assume it's tied up in delalloc reservations.
		
		  We don't want to include the global_rsv in our calculation,
		  because that's space we can't touch.  Subtract it from the
		  block_rsv_size for the next checks.
		
		  We really want to avoid flushing delalloc too much, as it
		  could result in poor allocation patterns, so only flush it if
		  it's larger than the rest of the pools combined.
		
		  We don't want to reclaim everything, just a portion, so scale
		  down the to_reclaim by 14.  If it takes us down to 0,
		  reclaim 1 items worth.
 We only went through once, back off our clamping. 
  FLUSH_DELALLOC_WAIT:
    Space is freed from flushing delalloc in one of two ways.
    1) compression is on and we allocate less space than we reserved
    2) we are overwriting existing space
    For #1 that extra space is reclaimed as soon as the delalloc pages are
    COWed, by way of btrfs_add_reserved_bytes() which adds the actual extent
    length to ->bytes_reserved, and subtracts the reserved space from
    ->bytes_may_use.
    For #2 this is trickier.  Once the ordered extent runs we will drop the
    extent in the range we are overwriting, which creates a delayed ref for
    that freed extent.  This however is not reclaimed until the transaction
    commits, thus the next stages.
  RUN_DELAYED_IPUTS
    If we are freeing inodes, we want to make sure all delayed iputs have
    completed, because they could have been on an inode with i_nlink == 0, and
    thus have been truncated and freed up space.  But again this space is not
    immediately re-usable, it comes in the form of a delayed ref, which must be
    run and then the transaction must be committed.
  COMMIT_TRANS
    This is where we reclaim all of the pinned space generated by running the
    iputs
  ALLOC_CHUNK_FORCE
    For data we start with alloc chunk force, however we could have been full
    before, and then the transaction commit could have freed new block groups,
    so if we now have space to allocate do the force chunk allocation.
 Something happened, fail everything and bail. 
 Something happened, fail everything and bail. 
			
			  Delete us from the list. After we unlock the space
			  info, we don't want the async reclaim job to reserve
			  space for this ticket. If that would happen, then the
			  ticket's task would not known that space was reserved
			  despite getting an error, resulting in a space leak
			  (bytes_may_use counter of our space_info).
  Do the appropriate flushing and waiting for a ticket
  @fs_info:    the filesystem
  @space_info: space info for the reservation
  @ticket:     ticket for the reservation
  @start_ns:   timestamp when the reservation started
  @orig_bytes: amount of bytes originally reserved
  @flush:      how much we can flush
  This does the work of figuring out how to flush for the ticket, waiting for
  the reservation, and returning the appropriate error if there is one.
		
		  We were a priority ticket, so we need to delete ourselves
		  from the list.  Because we could have other priority tickets
		  behind us that require less space, run
		  btrfs_try_granting_tickets() to see if their reservations can
		  now be made.
	
	  Check that we can't have an error set if the reservation succeeded,
	  as that would confuse tasks and lead them to error out without
	  releasing reserved space (if an error happens the expectation is that
	  space wasn't reserved at all).
  This returns true if this flush state will go through the ordinary flushing
  code.
	
	  If we're heavy on ordered operations then clamping won't help us.  We
	  need to clamp specifically to keep up with dirty'ing buffered
	  writers, because there's not a 1:1 correlation of writing delalloc
	  and freeing space, like there is with flushing delayed refs or
	  delayed nodes.  If we're already more ordered than delalloc then
	  we're keeping up, otherwise we aren't and should probably clamp.
  Try to reserve bytes from the block_rsv's space
  @fs_info:    the filesystem
  @space_info: space info we want to allocate from
  @orig_bytes: number of bytes we want
  @flush:      whether or not we can flush to make our reservation
  This will reserve orig_bytes number of bytes from the space info associated
  with the block_rsv.  If there is not enough space it will make an attempt to
  flush out space to make room.  It will do this by flushing delalloc if
  possible or committing the transaction.  If flush is 0 then no attempts to
  regain reservations will be made and this will fail if there is not enough
  space already.
	
	  We don't want NO_FLUSH allocations to jump everybody, they can
	  generally handle ENOSPC in a different way, so treat them the same as
	  normal flushers when it comes to skipping pending tickets.
	
	  Carry on if we have enough space (short-circuit) OR call
	  can_overcommit() to ensure we can overcommit to continue.
	
	  If we couldn't make a reservation then setup our reservation ticket
	  and kick the async worker if it's not already running.
	 
	  If we are a priority flusher then we just need to add our ticket to
	  the list and we will do our own flushing further down.
				
				  We were forced to add a reserve ticket, so
				  our preemptive flushing is unable to keep
				  up.  Clamp down on the threshold for the
				  preemptive flushing in order to keep up with
				  the workload.
		
		  We will do the space reservation dance during log replay,
		  which means we won't have fs_info->fs_root set, so don't do
		  the async reclaim as we will panic.
  Trye to reserve metadata bytes from the block_rsv's space
  @root:       the root we're allocating for
  @block_rsv:  block_rsv we're allocating for
  @orig_bytes: number of bytes we want
  @flush:      whether or not we can flush to make our reservation
  This will reserve orig_bytes number of bytes from the space info associated
  with the block_rsv.  If there is not enough space it will make an attempt to
  flush out space to make room.  It will do this by flushing delalloc if
  possible or committing the transaction.  If flush is 0 then no attempts to
  regain reservations will be made and this will fail if there is not enough
  space already.
  Try to reserve data bytes for an allocation
  @fs_info: the filesystem
  @bytes:   number of bytes we need
  @flush:   how we are allowed to flush
  This will reserve bytes from the data space info.  If there is not enough
  space then we will attempt to flush space as specified by flush.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2011 Fujitsu.  All rights reserved.
  Written by Miao Xie <miaox@cn.fujitsu.com>
 can be accessed 
		
		  It's possible that we're racing into the middle of removing
		  this node from the radix tree.  In this case, the refcount
		  was zero and it should never go back to one.  Just return
		  NULL like it was never in the radix at all; our release
		  function is in the process of removing it.
		 
		  Some implementations of refcount_inc refuse to bump the
		  refcount once it has hit zero.  If we don't do this dance
		  here, refcount_inc() may decide to just WARN_ONCE() instead
		  of actually bumping the refcount.
		 
		  If this node is properly in the radix, we want to bump the
		  refcount twice, once for the inode and once for this get
		  operation.
 Will return either the node or PTR_ERR(-ENOMEM) 
 cached in the btrfs inode and can be accessed 
  Call it when holding delayed_node->mutex
  If mod = 1, add this node into the prepared list.
 inserted into list 
 Call it when holding delayed_node->mutex 
 not in the list 
 not in the list 
		
		  Once our refcount goes to zero, nobody is allowed to bump it
		  back up.  We can delete it now.
  __btrfs_lookup_delayed_item - look up the delayed item by key
  @delayed_node: pointer to the delayed node
  @key:	  the key to look up
  @prev:	  used to store the prev item if the right item isn't found
  @next:	  used to store the next item if the right item isn't found
  Note: if we don't find the right item, we will return the prev item and
  the next item.
 atomic_dec_return implies a barrier 
 Not associated with any delayed_node 
	
	  Here we migrate space rsv from transaction rsv, since have already
	  reserved space when starting a transaction.  So no need to reserve
	  qgroup space here.
	
	  Check btrfs_delayed_item_reserve_metadata() to see why we don't need
	  to releasereserve qgroup space.
	
	  btrfs_dirty_inode will update the inode under btrfs_join_transaction
	  which doesn't reserve space for speed.  This is a problem since we
	  still need to reserve space for this update, so try to reserve the
	  space.
	 
	  Now if src_rsv == delalloc_block_rsv we'll let it just steal since
	  we always reserve enough to update the inode item.
 NO_FLUSH could only fail with -ENOSPC 
  Insert a single delayed item or a batch of delayed items that have consecutive
  keys if they exist.
	
	  Now release our path before releasing the delayed items and their
	  metadata reservations, so that we don't block other tasks for more
	  time than needed.
 FIXME: Is errno suitable? 
	
	  count the number of the dir index items that we can delete in batch
		
		  can't find the item which the node points to, so this node
		  is invalid, just drop it.
	
	  Delayed iref deletion is for the inode who has only one link,
	  so there is only one iref. The case that several irefs are
	  in the same item doesn't exist.
	
	  If we fail to update the delayed inode we need to abort the
	  transaction, because we could leave the inode with the improper
	  counts behind.
  Called when committing the transaction.
  Returns 0 on success.
  Returns < 0 on error and returns with an aborted transaction with any
  outstanding delayed items cleaned up.
 Will return 0 or -ENOMEM 
	
	  we have reserved enough space when we start a new transaction,
	  so reserving metadata failure is impossible
	
	  we have reserved enough space when we start a new transaction,
	  so reserving metadata failure is impossible.
	
	  Since we have held i_mutex of this directory, it is impossible that
	  a new directory index is added into the delayed node and index_cnt
	  is updated now. So we needn't lock the delayed node.
	
	  We can only do one readdir with delayed items at a time because of
	  item->readdir_list.
	
	  This delayed node is still cached in the btrfs inode, so refs
	  must be > 1 now, and we needn't check it is going to be freed
	  or not.
	 
	  Besides that, this function is used to read dir, we do not
	  insertdelete delayed items in this period. So we also needn't
	  requeue or dequeue this delayed node.
	
	  The VFS is going to do up_read(), so we need to downgrade back to a
	  read lock.
  btrfs_readdir_delayed_dir_index - read dir info stored in the delayed tree
	
	  Changing the data of the delayed item is impossible. So
	  we needn't lock them. And we have held i_mutex of the
	  directory, nobody can delete any directory indexes now.
	
	  we don't do delayed inode updates during log recovery because it
	  leads to enospc problems.  This means we also can't do
	  delayed inode refs
	
	  We don't reserve space for inode ref deletion is because:
	  - We ONLY do async inode ref deletion for the inode who has only
	    one link(i_nlink == 1), it means there is only one inode ref.
	    And in most case, the inode ref and the inode item are in the
	    same leaf, and we will deal with them at the same time.
	    Since we are sure we will reserve the space for the inode item,
	    it is unnecessary to reserve space for inode ref deletion.
	  - If the inode ref and the inode item are not in the same leaf,
	    We also needn't worry about enospc problem, because we reserve
	    much more space for the inode update than it needs.
	  - At the worst, we can steal some space from the global reservation.
	    It is very rare.
			
			  Don't increase refs in case the node is dead and
			  about to be removed from the tree in the loop below
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2009 Oracle.  All rights reserved.
  Relocation overview
  [What does relocation do]
  The objective of relocation is to relocate all extents of the target block
  group to other block groups.
  This is utilized by resize (shrink only), profile converting, compacting
  space, or balance routine to spread chunks over devices.
  		Before		|		After
  ------------------------------------------------------------------
   BG A: 10 data extents	| BG A: deleted
   BG B:  2 data extents	| BG B: 10 data extents (2 old + 8 relocated)
   BG C:  1 extents		| BG C:  3 data extents (1 old + 2 relocated)
  [How does relocation work]
  1.   Mark the target block group read-only
       New extents won't be allocated from the target block group.
  2.1  Record each extent in the target block group
       To build a proper map of extents to be relocated.
  2.2  Build data reloc tree and reloc trees
       Data reloc tree will contain an inode, recording all newly relocated
       data extents.
       There will be only one data reloc tree for one data block group.
       Reloc tree will be a special snapshot of its source tree, containing
       relocated tree blocks.
       Each tree referring to a tree block in target block group will get its
       reloc tree built.
  2.3  Swap source tree with its corresponding reloc tree
       Each involved tree only refers to new extents after swap.
  3.   Cleanup reloc trees and data reloc tree.
       As old extents in the target block group are still referenced by reloc
       trees, we need to clean them up before really freeing the target block
       group.
  The main complexity is in steps 2.2 and 2.3.
  The entry point of relocation is relocate_block_group() function.
  map address of tree root to tree
 Use rb_simle_node for searchinsert 
  present a tree block to process
 Use rb_simple_node for searchinsert 
 block group to relocate 
 extent tree 
 inode for moving data 
 tree blocks have been processed 
 map start of tree root to corresponding reloc tree 
 list of reloc trees 
 list of subvolume trees that get relocated 
 size of metadata reservation for merging reloc trees 
 size of relocated tree nodes 
 reserved size for block group relocation
 stages of data relocation 
  walk up backref nodes until reach node presents tree root
  walk down backref nodes to find start of next reference path
  update backref cache after a transaction commit
	
	  detached nodes are used to avoid unnecessary backref
	  lookup. transaction commit changes the extent tree.
	  so the detached nodes are no longer useful.
	
	  some nodes can be left in the pending list if there were
	  errors during processing the pending nodes.
	
	  Pair with set_bitclear_bit in clean_dirty_subvols and
	  btrfs_update_reloc_root. We need to see the updated bit before
	  trying to access reloc_root
  Check if this subvolume tree has valid reloc tree.
  Reloc tree after swap is considered dead, thus not considered as valid.
  This is enough for most callers, as they don't distinguish dead reloc root
  from no reloc root.  But btrfs_should_ignore_reloc_root() below is a
  special case.
 This root has been merged with its reloc tree, we can ignore it 
	
	  if there is reloc tree and it was created in previous
	  transaction backref lookup can find the reloc tree,
	  so backref node for the fs tree root is useless for
	  relocation.
  find reloc tree by address of tree root
  For useless nodes, do two major clean ups:
  - Cleanup the children edges and nodes
    If child node is also orphan (no parent) during cleanup, then the child
    node will also be cleaned up.
  - Freeing up leaves (level 0), keeps nodes detached
    For nodes, the node is still cached as "detached"
  Return false if @node is not in the @useless_nodes list.
  Return true if @node is in the @useless_nodes list.
 Only tree root nodes can be added to @useless_nodes 
 The node is the lowest node 
 Cleanup the lower edges 
 Child node is also orphan, queue for cleanup 
 Mark this block processed for relocation 
		
		  Backref nodes for tree leaves are deleted from the cache.
		  Backref nodes for upper level tree blocks are left in the
		  cache to avoid unnecessary backref lookup.
  Build backref tree for a given tree block. Root of the backref tree
  corresponds the tree block, leaves of the backref tree correspond roots of
  b-trees that reference the tree block.
  The basic idea of this function is check backrefs of a given block to find
  upper level blocks that reference the block, and then check backrefs of
  these upper level blocks recursively. The recursion stops when tree root is
  reached or backrefs for the block is cached.
  NOTE: if we find that backrefs for a block are cached, we know backrefs for
  all upper level blocks that directlyindirectly reference the block are also
  cached.
 For searching parent of TREE_BLOCK_REF 
 Breadth-first search to build backref cache 
		
		  The pending list isn't empty, take the first block to
		  process
 Finish the upper linkage of newly added edgesnodes 
  helper to add backref node for the newly created snapshot.
  the backref node is created by cloning backref node that
  corresponds to root of source tree
  helper to add 'address of tree root -> reloc tree' mapping
  helper to delete the 'address of tree root -> reloc tree'
  mapping
	
	  We only put the reloc root here if it's on the list.  There's a lot
	  of places where the pattern is to splice the rc->reloc_roots, process
	  the reloc roots, and then add the reloc root back onto
	  rc->reloc_roots.  If we call __del_reloc_root while it's off of the
	  list we don't want the reference being dropped, because the guy
	  messing with the list is in charge of the reference.
  helper to update the 'address of tree root -> reloc tree'
  mapping
 called by btrfs_init_reloc_root 
		
		  Set the last_snapshot field to the generation of the commit
		  root - like this ctree.c:btrfs_block_can_be_shared() behaves
		  correctly (returns true) when the relocation root is created
		  either inside the critical section of a transaction commit
		  (through transaction.c:qgroup_account_snapshot()) and when
		  it's created before the transaction commit is started.
		
		  called by btrfs_reloc_post_snapshot_hook.
		  the source tree is a reloc tree, all tree blocks
		  modified after it was created have RELOC flag
		  set in their headers. so it's OK to not update
		  the 'last_snapshot'.
	
	  We have changed references at this point, we must abort the
	  transaction if anything fails.
  create reloc tree for a given fs tree. reloc tree is just a
  snapshot of the fs tree with special root objectid.
  The reloc_root comes out of here with two references, one for
  root->reloc_root, and another for being on the rc->reloc_roots list.
	
	  The subvolume has reloc tree but the swap is finished, no need to
	  createupdate the dead reloc tree
	
	  This is subtle but important.  We do not do
	  record_root_in_transaction for reloc roots, instead we record their
	  corresponding fs root, and then here we update the last trans for the
	  reloc root.  This means that we have to do this for the entire life
	  of the reloc root, regardless of which stage of the relocation we are
	  in.
	
	  We are merging reloc roots, we do not need new reloc trees.  Also
	  reloc trees never need their own reloc tree.
 Pairs with create_reloc_root 
  update root item of reloc tree
	
	  We are probably ok here, but __del_reloc_root() will drop its ref of
	  the root.  We have the ref for root->reloc_root, but just in case
	  hold it while we update the reloc root.
 root->reloc_root will stay until current relocation finished 
		
		  Mark the tree as dead before we change reloc_root so
		  have_reloc_root will not touch it from now on.
  helper to find first cached inode with inode number >= objectid
  in a subvolume
  get new location of data
  update file extent items in the tree leaf to point to
  the new locations.
 reloc trees always use full backref 
		
		  if we are modifying block in fs tree, wait for readpage
		  to complete and drop the extent cache
			
			  Don't have to abort since we've not changed anything
			  in the file extent yet.
  try to replace tree blocks in fs tree with the new blocks
  in reloc tree. tree blocks haven't been modified since the
  reloc tree was create can be replaced.
  if a block was replaced, level of the block + 1 is returned.
  if no block got replaced, 0 is returned. if there are other
  errors, a negative error number is returned.
		
		  Info qgroup to trace both subtrees.
		 
		  We must trace both trees.
		  1) Tree reloc subtree
		     If not traced, we will leak data numbers
		  2) Fs subtree
		     If not traced, we will double count old data
		 
		  We don't scan the subtree right now, but only record
		  the swapped tree blocks.
		  The real subtree rescan is delayed until we have new
		  CoW on the subtree root node before transaction commit.
		
		  swap blocks in fs tree and reloc tree.
  helper to find next relocated block in reloc tree
  walk down reloc tree to find relocated block of lowest level
  invalidate extent cache for file extents whose key in range of
  [min_key, max_key)
 the lock_extent waits for readpage to complete 
  Insert current subvolume into reloc_control::dirty_subvol_roots
 @root must be a subvolume tree root with a valid reloc tree 
 Merged subvolume, cleanup its reloc root 
			
			  Need barrier to ensure clear_bit() only happens after
			  root->reloc_root = NULL. Pairs with have_reloc_root.
				
				  btrfs_drop_snapshot drops our ref we hold for
				  ->reloc_root.  If it fails however we must
				  drop the ref ourselves.
 Orphan reloc tree, just clean it up 
  merge the relocated tree blocks in reloc tree with corresponding
  fs tree.
	
	  In merge_reloc_root(), we modify the upper level pointer to swap the
	  tree blocks between reloc tree and subvolume tree.  Thus for tree
	  block COW, we COW at most from level 1 to root level for each tree.
	 
	  Thus the needed metadata size is at most root_level  nodesize,
	  and  2 since we have two trees to COW.
		
		  At this point we no longer have a reloc_control, so we can't
		  depend on btrfs_init_reloc_root to update our last_trans.
		 
		  But that's ok, we started the trans handle on our
		  corresponding fs_root, which means it's been added to the
		  dirty list.  At commit time we'll still call
		  btrfs_update_reloc_root() and update our root item
		  appropriately.
		
		  save the merging progress in the drop_progress.
		  this is OK since root refs == 1 in this case.
	
	  handle the case only one block in the fs tree need to be
	  relocated and the block is tree root.
			
			  Even if we have an error we need this reloc root
			  back on our list so we can clean up properly.
		
		  set reference count to 1, so btrfs_recover_relocation
		  knows it should resumes merging
		
		  Even if we have an error we need this reloc root back on our
		  list so we can clean up properly.
	
	  this serializes us with btrfs_record_root_in_transaction,
	  we have to make sure nobody is in the middle of
	  adding their roots to the list while we are
	  doing this splice
				
				  For recovery we read the fs roots on mount,
				  and if we didn't find the root then we marked
				  the reloc root as a garbage root.  For normal
				  relocation obviously the root should exist in
				  memory.  However there's no reason we can't
				  handle the error properly here just in case.
				
				  This is actually impossible without something
				  going really wrong (like weird race condition
				  or cosmic rays).
 Don't forget to queue this reloc root for cleanup 
 new reloc root may be added 
	
	  We used to have
	 
	  BUG_ON(!RB_EMPTY_ROOT(&rc->reloc_root_tree.rb_root));
	 
	  here, but it's wrong.  If we fail to start the transaction in
	  prepare_to_merge() we will have only 0 ref reloc roots, none of which
	  have actually been removed from the reloc_root_tree rb tree.  This is
	  fine because we're bailing here, and we hold a reference on the root
	  for the list that holds it, so these roots will be cleaned up when we
	  do the reloc_dirty_list afterwards.  Meanwhile the root->reloc_root
	  will be cleaned up on unmount.
	 
	  The remaining nodes will be cleaned up by free_reloc_control.
	
	  This should succeed, since we can't have a reloc root without having
	  already looked up the actual root and created the reloc root for this
	  root.
	 
	  However if there's some sort of corruption where we have a ref to a
	  reloc root without a corresponding root this could return ENOENT.
		
		  If there is no root, then our references for this block are
		  incomplete, as we should be able to walk all the way up to a
		  block that is owned by a root.
		 
		  This path is only for SHAREABLE roots, so if we come upon a
		  non-SHAREABLE root then we have backrefs that resolve
		  improperly.
		 
		  Both of these cases indicate file system corruption, or a bug
		  in the backref walking code.
		
		  We could have raced with another thread which failed, so
		  root->reloc_root may not be set, return ENOENT in this case.
			
			  We just created the reloc root, so we shouldn't have
			  ->new_bytenr set and this shouldn't be in the changed
			   list.  If it is then we have multiple roots pointing
			   at the same bytenr which indicates corruption, or
			   we've made a mistake in the backref walking code.
		
		  This can happen if there's fs corruption or if there's a bug
		  in the backref lookup code.
 setup backref node path for btrfs_reloc_cow_block 
  Select a tree root for relocation.
  Return NULL if the block is not shareable. We should use do_relocation() in
  this case.
  Return a tree root pointer if the block is shareable.
  Return -ENOENT if the block is root of reloc tree.
		
		  This can occur if we have incomplete extent refs leading all
		  the way up a particular path, in this case return -EUCLEAN.
 No other choice for non-shareable tree 
	
	  We are under a transaction here so we can only do limited flushing.
	  If we get an enospc just kick back -EAGAIN so we know to drop the
	  transaction and try to refill when we can flush all the things.
		
		  only one thread can access block_rsv at this point,
		  so we don't need hold lock to protect block_rsv.
		  we expand more reservation size here to allow enough
		  space for relocation and we will return earlier in
		  enospc case.
  relocate a block tree, and then update pointers in upper level
  blocks that reference the block to point to the new location.
  if called by link_to_upper, the block has already been relocated.
  in that case this function just updates pointers.
	
	  If we are lowest then this is the first time we're processing this
	  block, and thus shouldn't have an eb associated with it yet.
			
			  We've just COWed this block, it should have updated
			  the correct backref node entry.
	
	  We should have allocated all of our space in the block rsv and thus
	  shouldn't ENOSPC.
  mark a block and all blocks directlyindirectly reference the block
  as processed.
  helper function to relocate a tree block
	
	  If we fail here we want to drop our backref_node because we are going
	  to start over and regenerate the tree for it.
 See explanation in select_one_root for the -EUCLEAN case. 
			
			  This block was the root block of a root, and this is
			  the first time we're processing the block and thus it
			  should not have had the ->new_bytenr modified and
			  should have not been included on the changed list.
			 
			  However in the case of corruption we could have
			  multiple refs pointing to the same block improperly,
			  and thus we would trip over these checks.  ASSERT()
			  for the developer case, because it could indicate a
			  bug in the backref code, however error out for a
			  normal user in the case of corruption.
			
			  Another thread could have failed, need to check if we
			  have reloc_root actually set.
  relocate a list of blocks
 Kick in readahead for tree blocks with missing keys 
 Get first keys 
 Do tree relocation 
	
	  For subpage case, previous i_size may not be aligned to PAGE_SIZE.
	  This means the range [i_size, PAGE_END + 1) is filled with zeros by
	  btrfs_do_readpage() call of previously relocated file cluster.
	 
	  If the current cluster starts in the above range, btrfs_do_readpage()
	  will skip the read, and relocate_one_page() will later writeback
	  the padding zeros as new data, causing data corruption.
	 
	  Here we have to manually invalidate the range (i_size, PAGE_END + 1).
		
		  Subpage can't handle page with DIRTY but without UPTODATE
		  bit as it can lead to the following deadlock:
		 
		  btrfs_readpage()
		  | Page already locked
		  |- btrfs_lock_and_flush_ordered_range()
		     |- btrfs_start_ordered_extent()
		        |- extent_write_cache_pages()
		           |- lock_page()
		              We try to lock the page we already hold.
		 
		  Here we just writeback the whole data reloc inode, so that
		  we will be ensured to have no dirty range in the page, and
		  are safe to clear the uptodate bits.
		 
		  This shouldn't cause too much overhead, as we need to write
		  the data back anyway.
		
		  If page is freed we don't need to do anything then, as we
		  will re-read the whole page anyway.
  Allow error injection to test balancerelocation cancellation
 Last extent, use cluster end directly 
 Use next boundary start
	
	  Start from the cluster, as for subpage case, the cluster can start
	  inside the page.
 Reserve metadata for this range 
 Mark the range delalloc and dirty for later writeback 
		
		  Set the boundary if it's inside the page.
		  Data relocation requires the destination extents to have the
		  same size as the source.
		  EXTENT_BOUNDARY bit prevents current extent from being merged
		  with previous extent.
 Crossed extent end, go to next extent 
 Just finished the last extent of the cluster, exit. 
  helper to add a tree block to the list.
  the major work is getting the generation and level of the block
		
		  We're reading random blocks without knowing their owner ahead
		  of time.  This is ok most of the time, as all reloc roots and
		  fs roots have the same lock type.  However normal trees do
		  not, and the only way to know ahead of time is to read the
		  inline ref offset.  We know it's an fs root if
		 
		  1. There's more than one ref.
		  2. There's a SHARED_DATA_REF_KEY set.
		  3. FULL_BACKREF is set on the flags.
		 
		  Otherwise it's safe to assume that the ref offset == the
		  owner of this block, so we can use that when calling
		  read_tree_block.
  helper to add tree blocks for backref of type BTRFS_SHARED_DATA_REF_KEY
  Locate the free space cache EXTENT_DATA in root tree leaf and delete the
  cache inode, to avoid free space cache data extent blocking data relocation.
  helper to find all tree blocks that reference a given data extent
  helper to find next unprocessed extent
		
		  extent tree is not a ref_cow tree and has no reloc_root to
		  cleanup.  And callers are responsible to free the above
		  block rsv.
	
	  Even in the case when the relocation is cancelled, we should all go
	  through prepare_to_merge() and merge_reloc_roots().
	 
	  For error (including cancelled balance), prepare_to_merge() will
	  mark all reloc trees orphan, then queue them for cleanup in
	  merge_reloc_roots()
 get rid of pinned extents 
  helper to create inode for data relocation.
  the inode is in data relocation tree and its link count is 0
  Mark start of chunk relocation that is cancellable. Check if the cancellation
  has been requested meanwhile and don't start in that case.
  Return:
    0             success
    -EINPROGRESS  operation is already in progress, that's probably a bug
    -ECANCELED    cancellation request was set before the operation started
    -EAGAIN       can not start because there are ongoing send operations
 This should not happen 
		
		  On cancel, clear all requests but let the caller mark
		  the end after cleanup operations.
  Mark end of chunk relocation that is cancellable and wake any waiters.
 Requested after start, clear bit first so any waiters can continue 
  Print the block group being relocated
  function to relocate all extents in a block group.
		
		  We may have gotten ENOSPC after we already dirtied some
		  extents.  If writeout happens while we're relocating a
		  different block group we could end up hitting the
		  BUG_ON(rc->stage == UPDATE_DATA_PTRS) in
		  btrfs_reloc_cow_block.  Make sure we write everything out
		  properly so we don't trip over this problem, and then break
		  out of the loop if we hit an error.
  recover relocation interrupted by system crash.
  this function resumes merging reloc trees with corresponding fs trees.
  this is important for keeping the sharing of tree blocks
 cleanup orphan inode in data relocation tree 
  helper to add ordered checksum for data relocation.
  cloning checksum properly handles the nodatasum extents.
  it also saves CPU time to re-calculate the checksum.
		
		  We need to offset the new_bytenr based on where the csum is.
		  We need to do this because we will read in entire prealloc
		  extents but we may have written to say the middle of the
		  prealloc extent, so we need to make sure the csum goes with
		  the right disk offset.
		 
		  We can do this because the data reloc inode refers strictly
		  to the on disk bytes, so we don't have to worry about
		  disk_len vs real len like with real inodes since it's all
		  disk length.
  called before creating snapshot. it calculates metadata reservation
  required for relocating tree blocks in the snapshot
	
	  relocation is in the stage of merging trees. the space
	  used by merging a reloc tree is twice the size of
	  relocated tree nodes in the worst case. half for cowing
	  the reloc tree, half for cowing the fs tree. the space
	  used by cowing the reloc tree will be freed after the
	  tree is dropped. if we create snapshot, cowing the fs
	  tree may use more space than it frees. so we need
	  reserve extra space.
  called after snapshot is created. migrate block reservation
  and create reloc root for the newly created snapshot
  This is similar to btrfs_init_reloc_root(), we come out of here with two
  references held on the reloc_root, one for root->reloc_root and one for
  rc->reloc_roots.
 Pairs with create_reloc_root 
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
  Structure name                       Path
  --------------------------------------------------------------------------
  btrfs_supported_static_feature_attrs sysfsbtrfsfeatures
  btrfs_supported_feature_attrs	sysfsbtrfsfeatures and
 					sysfsbtrfs<uuid>features
  btrfs_attrs				sysfsbtrfs<uuid>
  devid_attrs				sysfsbtrfs<uuid>devinfo<devid>
  allocation_attrs			sysfsbtrfs<uuid>allocation
  qgroup_attrs				sysfsbtrfs<uuid>qgroups<level>_<qgroupid>
  space_info_attrs			sysfsbtrfs<uuid>allocation<bg-type>
  raid_attrs				sysfsbtrfs<uuid>allocation<bg-type><bg-profile>
  When built with BTRFS_CONFIG_DEBUG:
  btrfs_debug_feature_attrs		sysfsbtrfsdebug
  btrfs_debug_mount_attrs		sysfsbtrfs<uuid>debug
  discard_debug_attrs			sysfsbtrfs<uuid>debugdiscard
 For raid type sysfs entries 
 Nothing to do 
	
	  We don't want to do full transaction commit from inside sysfs
 Remove once support for zoned allocation is feature complete 
  Features which depend on feature bits and may differ between each fs.
  sysfsbtrfsfeatures      - all available features implemeted by this version
  sysfsbtrfsUUIDfeatures - features of the fs which are enabled or
                                can be changed on a mounted filesystem.
		
		  This "trick" only works as long as 'enum btrfs_csum_type' has
		  no holes in it
 4K sector size is also supported with 64K page size 
 Only sectorsize == PAGE_SIZE is now supported 
  Features which only depend on kernel version.
  These are listed in sysfsbtrfsfeatures along with
  btrfs_supported_feature_attrs.
  Discard statistics and tunables
  Per-filesystem debugging of discard (when mounted with discard=async).
  Path: sysfsbtrfs<uuid>debugdiscard
  Per-filesystem runtime debugging exported via sysfs.
  Path: sysfsbtrfsUUIDdebug
  Runtime debugging exported via sysfs, applies to all mounted filesystems.
  Path: sysfsbtrfsdebug
  Allocation information about block group profiles.
  Path: sysfsbtrfs<uuid>allocation<bg-type><bg-profile>
  Allocation information about block group types.
  Path: sysfsbtrfs<uuid>allocation<bg-type>
  Allocation information about block groups.
  Path: sysfsbtrfs<uuid>allocation
	
	  p_len is the len until the first occurrence of either
	  '\n' or '\0'
	
	  We don't want to do full transaction commit from inside sysfs
  Look for an exact string @string in @buffer with possible leading or
  trailing whitespace
 Skip leading whitespace 
 Match entire string, check if the rest is whitespace or empty 
  Per-filesystem information and stats.
  Path: sysfsbtrfs<uuid>
 when fs_devs is NULL it will remove all fsid kobject 
 safe max, 64 names  64 bytes 
  Create a sysfs entry for a given block group type at path
  sysfsbtrfsUUIDallocationdataTYPE
	
	  Setup a NOFS context because kobject_add(), deep in its call chain,
	  does GFP_KERNEL allocations, and we are often called in a context
	  where if reclaim is triggered we can deadlock (we are either holding
	  a transaction handle or some lock required for a transaction
	  commit).
	
	  We call this either on mount, or if we've created a block group for a
	  new index type while running (i.e. when restriping).  The running
	  case is tricky because we could race with other threads, so we need
	  to have this check to make sure we didn't already init the kobject.
	 
	  We don't have to protect on the free side because it only happens on
	  unmount.
  Remove sysfs directories for all block group types of a given space info and
  the space info as well
  Create a sysfs entry for a space info type at path
  sysfsbtrfsUUIDallocationTYPE
	
	  Seed fs_devices devices_kobj aren't used, fetch kobject from the
	  fs_info::fs_devices.
	
	  Print all at once so we get a snapshot of all values from the same
	  time. Keep them in sync and in order of definition of
	  btrfs_dev_stat_values.
  Information about one device.
  Path: sysfsbtrfs<uuid>devinfo<devid>
	
	  Make sure we use the fs_info::fs_devices to fetch the kobjects even
	  for the seed fs_devices
	
	  Sprouting changes fsid of the mounted filesystem, rename the fsid
	  directory
 sysfsbtrfs entry 
  Creates:
 		sysfsbtrfsUUID
  Can be called by the device discovery thread.
 Discard directory 
  Qgroup information.
  Path: sysfsbtrfs<uuid>qgroups<level>_<qgroupid>
 Called when qgroups get initialized, thus there is no need for locking 
  Change per-fs features in sysfsbtrfsUUIDfeatures to match current
  values in superblock. Call after any changes to incompatcompat_ro flags
	
	  See 14e46e04958df74 and e410e34fad913dd, feature bit updates are not
	  safe when called from some contexts (eg. balance)
	
	  FIXME: this is too heavy to update just one value, ideally we'd like
	  to use sysfs_update_group but some refactoring is needed first.
 SPDX-License-Identifier: GPL-2.0
 Key with offset of -1 found 
	
	  have to add the null termination to make sure that reconnect_path
	  gets the right len for strlen
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
  Macro templates that define helpers to readwrite extent buffer data of a
  given size, that are also used via ctree.h for access to item members by
  specialized helpers.
  Generic helpers:
  - btrfs_set_8 (for 8163264)
  - btrfs_get_8 (for 8163264)
  Generic helpers with a token (cached address of the most recently accessed
  page):
  - btrfs_set_token_8 (for 8163264)
  - btrfs_get_token_8 (for 8163264)
  The setget functions handle data spanning two pages transparently, in case
  metadata block size is larger than page.  Every pointer to metadata items is
  an offset into the extent buffer page array, cast to a specific type.  This
  gives us all the type checking.
  The extent buffer pages stored in the array pages do not form a contiguous
  phyusical range, but the API functions assume the linear offset to the range
  from 0 to metadata node size.
 SPDX-License-Identifier: GPL-2.0
  Implementation of the interface defined in struct fsverity_operations.
  The main question is how and where to store the verity descriptor and the
  Merkle tree. We store both in dedicated btree items in the filesystem tree,
  together with the rest of the inode metadata. This means we'll need to do
  extra work to encrypt them once encryption is supported in btrfs, but btrfs
  has a lot of careful code around i_size and it seems better to make a new key
  type than try and adjust all of our expectations for i_size.
  Note that this differs from the implementation in ext4 and f2fs, where
  this data is stored as if it were in the file, but past EOF. However, btrfs
  does not have a widespread mechanism for caching opaque metadata pages, so we
  do pretend that the Merkle tree pages themselves are past EOF for the
  purposes of caching them (as opposed to creating a virtual inode).
  fs verity items are stored under two different key types on disk.
  The descriptor items:
  [ inode objectid, BTRFS_VERITY_DESC_ITEM_KEY, offset ]
  At offset 0, we store a btrfs_verity_descriptor_item which tracks the
  size of the descriptor item and some extra data for encryption.
  Starting at offset 1, these hold the generic fs verity descriptor.
  The latter are opaque to btrfs, we just read and write them as a blob for
  the higher level verity code.  The most common descriptor size is 256 bytes.
  The merkle tree items:
  [ inode objectid, BTRFS_VERITY_MERKLE_ITEM_KEY, offset ]
  These also start at offset 0, and correspond to the merkle tree bytes.
  So when fsverity asks for page 0 of the merkle tree, we pull up one page
  starting at offset 0 for this key type.  These are also opaque to btrfs,
  we're blindly storing whatever fsverity sends down.
  Another important consideration is the fact that the Merkle tree data scales
  linearly with the size of the file (with 4K pagesblocks and SHA-256, it's
  ~1127th the size) so for large files, writing the tree can be a lengthy
  operation. For that reason, we guard the whole enable verity operation
  (between begin_enable_verity and end_enable_verity) with an orphan item.
  Again, because the data can be pretty large, it's quite possible that we
  could run out of space writing it, so we try our best to handle errors by
  stopping and rolling back rather than aborting the victim transaction.
  Compute the logical file offset where we cache the Merkle tree.
  @inode:  inode of the verity file
  For the purposes of caching the Merkle tree pages, as required by
  fs-verity, it is convenient to do size computations in terms of a file
  offset, rather than in terms of page indices.
  Use 64K to be sure it's past the last page in the file, even with 64K pages.
  That rounding operation itself can overflow loff_t, so we do it in u64 and
  check.
  Returns the file offset on success, negative error code on failure.
  Drop all the items for this inode with this key_type.
  @inode:     inode to drop items for
  @key_type:  type of items to drop (BTRFS_VERITY_DESC_ITEM or
              BTRFS_VERITY_MERKLE_ITEM)
  Before doing a verity enable we cleanup any existing verity items.
  This is also used to clean up if a verity enable failed half way through.
  Returns number of dropped items on success, negative error code on failure.
 1 for the item being dropped 
		
		  Walk backwards through all the items until we find one that
		  isn't from our key type or objectid
 No more keys of this type, we're done 
 No more keys of this type, we're done 
		
		  This shouldn't be a performance sensitive function because
		  it's not used as part of truncate.  If it ever becomes
		  perf sensitive, change this to walk forward and bulk delete
		  items
  Drop all verity items
  @inode:  inode to drop verity items for
  In most contexts where we are dropping verity items, we want to do it for all
  the types of verity items, not a particular one.
  Returns: 0 on success, negative error code on failure.
  Insert and write inode items with a given key type and offset.
  @inode:     inode to insert for
  @key_type:  key type to insert
  @offset:    item offset to insert at
  @src:       source data to write
  @len:       length of source data to write
  Write len bytes from src into items of up to 2K length.
  The inserted items will have key (ino, key_type, offset + off) where off is
  consecutively increasing from 0 up to the last item ending at offset + len.
  Returns 0 on success and a negative error code on failure.
 1 for the new item being inserted 
		
		  Insert 2K at a time mostly to be friendly for smaller leaf
		  size filesystems
  Read inode items of the given key type and offset from the btree.
  @inode:      inode to read items of
  @key_type:   key type to read
  @offset:     item offset to read from
  @dest:       Buffer to read into. This parameter has slightly tricky
               semantics.  If it is NULL, the function will not do any copying
               and will just return the size of all the items up to len bytes.
               If dest_page is passed, then the function will kmap_local the
               page and ignore dest, but it must still be non-NULL to avoid the
               counting-only behavior.
  @len:        length in bytes to read
  @dest_page:  copy into this page instead of the dest buffer
  Helper function to read items from the btree.  This returns the number of
  bytes read or < 0 for errors.  We can return short reads if the items don't
  exist on disk or aren't big enough to fill the desired length.  Supports
  reading into a provided buffer (dest) or into the page cache
  Returns number of bytes read or a negative error code on failure.
			
			  Once we've copied something, we want all of the items
			  to be sequential
			
			  Our initial offset might be in the middle of an
			  item.  Make sure it all makes sense.
 desc = NULL to just sum all the item lengths 
 Number of bytes in this item we want to copy 
 Offset from the start of item for copying 
			
			  We've reached the last slot in this leaf and we need
			  to go to the next leaf.
  Delete an fsverity orphan
  @trans:  transaction to do the delete in
  @inode:  inode to orphan
  Capture verity orphan specific logic that is repeated in the couple places
  we delete verity orphans. Specifically, handling ENOENT and ignoring inodes
  with 0 links.
  Returns zero on success or a negative error code on failure.
	
	  If the inode has no links, it is either already unlinked, or was
	  created with O_TMPFILE. In either case, it should have an orphan from
	  that other operation. Rather than reference count the orphans, we
	  simply ignore them here, because we only invoke the verity path in
	  the orphan logic when i_nlink is 1.
  Rollback in-progress verity if we encounter an error.
  @inode:  inode verity had an error for
  We try to handle recoverable errors while enabling verity by rolling it back
  and just failing the operation, rather than having an fs level error no
  matter what. However, any error in rollback is unrecoverable.
  Returns 0 on success, negative error code on failure.
	
	  1 for updating the inode flag
	  1 for deleting the orphan
  Finalize making the file a valid verity file
  @inode:      inode to be marked as verity
  @desc:       contents of the verity descriptor to write (not NULL)
  @desc_size:  size of the verity descriptor
  Do the actual work of finalizing verity after successfully writing the Merkle
  tree:
  - write out the descriptor items
  - mark the inode with the verity flag
  - delete the orphan item
  - mark the ro compat bit
  - clear the in progress bit
  Returns 0 on success, negative error code on failure.
 Write out the descriptor item 
 Write out the descriptor itself 
	
	  1 for updating the inode flag
	  1 for deleting the orphan
  fsverity op that begins enabling verity.
  @filp:  file to enable verity on
  Begin enabling fsverity for the file. We drop any existing verity items, add
  an orphan and set the in progress bit.
  Returns 0 on success, negative error code on failure.
	
	  This should almost never do anything, but theoretically, it's
	  possible that we failed to enable verity on a file, then were
	  interrupted or failed while rolling back, failed to cleanup the
	  orphan, and finally attempt to enable verity again.
 1 for the orphan item 
  fsverity op that ends enabling verity.
  @filp:              file we are finishing enabling verity on
  @desc:              verity descriptor to write out (NULL in error conditions)
  @desc_size:         size of the verity descriptor (variable with signatures)
  @merkle_tree_size:  size of the merkle tree in bytes
  If desc is null, then VFS is signaling an error occurred during verity
  enable, and we should try to rollback. Otherwise, attempt to finish verity.
  Returns 0 on success, negative error code on error.
  fsverity op that gets the struct fsverity_descriptor.
  @inode:     inode to get the descriptor of
  @buf:       output buffer for the descriptor contents
  @buf_size:  size of the output buffer. 0 to query the size
  fsverity does a two pass setup for reading the descriptor, in the first pass
  it calls with buf_size = 0 to query the size of the descriptor, and then in
  the second pass it actually reads the descriptor off disk.
  Returns the size on success or a negative error code on failure.
  fsverity op that reads and caches a merkle tree page.
  @inode:         inode to read a merkle tree page for
  @index:         page index relative to the start of the merkle tree
  @num_ra_pages:  number of pages to readahead. Optional, we ignore it
  The Merkle tree is stored in the filesystem btree, but its pages are cached
  with a logical position past EOF in the inode's mapping.
  Returns the page we read, or an ERR_PTR on error.
		
		  We only insert uptodate pages, so !Uptodate has to be
		  an error
	
	  Merkle item keys are indexed from byte 0 in the merkle tree.
	  They have the form:
	 
	  [ inode objectid, BTRFS_MERKLE_ITEM_KEY, offset in bytes ]
 Inserted and ready for fsverity 
 Did someone race us into inserting this page? 
  fsverity op that writes a Merkle tree block into the btree.
  @inode:          inode to write a Merkle tree block for
  @buf:            Merkle tree data block to write
  @index:          index of the block in the Merkle tree
  @log_blocksize:  log base 2 of the Merkle tree block size
  Note that the block size could be different from the page size, so it is not
  safe to assume that index is a page index.
  Returns 0 on success or negative error code on failure
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
 simple helper to search for an existing data extent at a given offset 
  helper function to lookup reference count and flags of a tree block.
  the head node for delayed ref is used to store the sum of all the
  reference count modifications queued up in the rbtree. the head
  node may also store the extent flags to set. This way you can check
  to see what the reference count and extent flags would be if all of
  the delayed refs are not processed.
	
	  If we don't have skinny metadata, don't bother doing anything
	  different
			
			  Mutex was contended, block until it's released and try
			  again
  Back reference rules.  Back refs have three main goals:
  1) differentiate between all holders of references to an extent so that
     when a reference is dropped we can make sure it was a valid reference
     before freeing the extent.
  2) Provide enough information to quickly find the holders of an extent
     if we notice a given block is corrupted or bad.
  3) Make it easy to migrate blocks for FS shrinking or storage pool
     maintenance.  This is actually the same as #2, but with a slightly
     different use case.
  There are two kinds of back refs. The implicit back refs is optimized
  for pointers in non-shared tree blocks. For a given pointer in a block,
  back refs of this kind provide information about the block's owner tree
  and the pointer's key. These information allow us to find the block by
  b-tree searching. The full back refs is for pointers in tree blocks not
  referenced by their owner trees. The location of tree block is recorded
  in the back refs. Actually the full back refs is generic, and can be
  used in all cases the implicit back refs is used. The major shortcoming
  of the full back refs is its overhead. Every time a tree block gets
  COWed, we have to update back refs entry for all pointers in it.
  For a newly allocated tree block, we use implicit back refs for
  pointers in it. This means most tree related operations only involve
  implicit back refs. For a tree block created in old transaction, the
  only way to drop a reference to it is COW it. So we can detect the
  event that tree block loses its owner tree's reference and do the
  back refs conversion.
  When a tree block is COWed through a tree, there are four cases:
  The reference count of the block is one and the tree is the block's
  owner tree. Nothing to do in this case.
  The reference count of the block is one and the tree is not the
  block's owner tree. In this case, full back refs is used for pointers
  in the block. Remove these full back refs, add implicit back refs for
  every pointers in the new block.
  The reference count of the block is greater than one and the tree is
  the block's owner tree. In this case, implicit back refs is used for
  pointers in the block. Add full back refs for every pointers in the
  block, increase lower level extents' reference counts. The original
  implicit back refs are entailed to the new block.
  The reference count of the block is greater than one and the tree is
  not the block's owner tree. Add implicit back refs for every pointer in
  the new block, increase lower level extents' reference count.
  Back Reference Key composing:
  The key objectid corresponds to the first byte in the extent,
  The key type is used to differentiate between types of back refs.
  There are different meanings of the key offset for different types
  of back refs.
  File extents can be referenced by:
  - multiple snapshots, subvolumes, or different generations in one subvol
  - different files inside a single subvolume
  - different offsets inside a file (bookend extents in file.c)
  The extent ref structure for the implicit back refs has fields for:
  - Objectid of the subvolume root
  - objectid of the file holding the reference
  - original offset in the file
  - how many bookend extents
  The key offset for the implicit back refs is hash of the first
  three fields.
  The extent ref structure for the full back refs has field for:
  - number of pointers in the tree leaf
  The key offset for the implicit back refs is the first byte of
  the tree leaf
  When a file extent is allocated, The implicit back refs is used.
  the fields are filled in:
      (root_key.objectid, inode objectid, offset in file, 1)
  When a file extent is removed file truncation, we find the
  corresponding implicit back refs and check the following fields:
      (btrfs_header_owner(leaf), inode objectid, offset in file)
  Btree extents can be referenced by:
  - Different subvolumes
  Both the implicit back refs and the full back refs for tree blocks
  only consist of key. The key offset for the implicit back refs is
  objectid of block's owner tree. The key offset for the full back refs
  is the first byte of parent block.
  When implicit back refs is used, information about the lowest key and
  level of the tree block are required. These information are stored in
  tree block info structure.
  is_data == BTRFS_REF_TYPE_BLOCK, tree block type is required,
  is_data == BTRFS_REF_TYPE_DATA, data type is requiried,
  is_data == BTRFS_REF_TYPE_ANY, either type is OK.
				
				  Every shared one has parent tree block,
				  which must be aligned to sector size.
				
				  Every shared one has parent tree block,
				  which must be aligned to sector size.
		
		  If type is invalid, we should have bailed out earlier than
		  this call.
  look for inline back ref. if back ref is found, ref_ret is set
  to the address of inline back ref, and 0 is returned.
  if back ref isn't found, ref_ret is set to the address where it
  should be inserted, and -ENOENT is returned.
  if insert is true and there are too many inline back refs, the path
  points to the extent item, and -EAGAIN is returned.
  NOTE: inline back refs are ordered in the same way that back ref
 	 items in the tree are ordered.
	
	  Owner is our level, so we can just add one to get the level for the
	  block we are interested in.
	
	  We may be a newly converted file system which still has the old fat
	  extent entries for metadata, so try and see if we have one of those.
		
		  To add new inline back ref, we have to make sure
		  there is no corresponding back ref item.
		  For simplicity, we just do not add new inline back
		  ref if there is any kind of item for this block
  helper to add new inline back ref
  helper to updateremove inline back ref
	
	  If type is invalid, we should have bailed out after
	  lookup_inline_extent_backref().
		
		  We're adding refs to a tree block we already own, this
		  should not happen at all.
 Skip any superblocks on this device. 
		
		  Superblock spans beginning of range.  Adjust start and
		  try again.
 Zone reset on a zoned filesystem 
 Send to replace target as well 
	
	  Avoid races with device replace and make sure our bioc has devices
	  associated to its stripes that don't go away while we are discarding.
 Tell the block device(s) that the sectors can be discarded 
		
		  Error can be -ENOMEM, -ENOENT (no such chunk mapping) or
		  -EOPNOTSUPP. For any such error, @num_bytes is not updated,
		  thus we can't continue anyway.
				
				  Logic errors or -ENOMEM, or -EIO, but
				  unlikely to happen.
				 
				  And since there are two loops, explicitly
				  go to out to avoid confusion.
			
			  Just in case we get back EOPNOTSUPP for some reason,
			  just ignore the return value so we don't screw up
			  people calling discard_extent.
 Can return -ENOMEM 
  __btrfs_inc_extent_ref - insert backreference for a given extent
  The counterpart is in __btrfs_free_extent(), with examples and more details
  how it works.
  @trans:	    Handle of transaction
  @node:	    The delayed ref node used to get the bytenrlength for
 		    extent whose references are incremented.
  @parent:	    If this is a shared extent (BTRFS_SHARED_DATA_REF_KEY
 		    BTRFS_SHARED_BLOCK_REF_KEY) then it holds the logical
 		    bytenr of the parent block. Since new extents are always
 		    created with indirect references, this will only be the case
 		    when relocating a shared extent. In that case, root_objectid
 		    will be BTRFS_TREE_RELOC_OBJECTID. Otherwise, parent must
 		    be 0
  @root_objectid:  The id of the root where this modification has originated,
 		    this can be either one of the well-known metadata trees or
 		    the subvolume id which references this extent.
  @owner:	    For data extents it is the inode number of the owning file.
 		    For metadata extents this parameter holds the level in the
 		    tree of the extent.
  @offset:	    For metadata extents the offset is ignored and is currently
 		    always passed as 0. For data extents it is the fileoffset
 		    this extent belongs to.
  @refs_to_add     Number of references to add
  @extent_op       Pointer to a structure, holding information necessary when
                   updating a tree block's flags
 this will setup the path even if it fails to insert the back ref 
	
	  Ok we had -EAGAIN which means we didn't have space to insert and
	  inline extent ref, so just update the reference count and add a
	  normal backref.
 now insert the actual backref 
 helper function to actually process a single delayed ref entry 
	
	  Select a delayed ref of type BTRFS_ADD_DELAYED_REF first.
	  This is to prevent a ref count from going down to zero, which deletes
	  the extent item from the extent tree, when there still are references
	  to add, which would fail because they would not find the extent item.
 Dropping this ref head update. 
	
	  We had csum deletions accounted for in our delayed refs rsv, we need
	  to drop the csum leaves for this update from our delayed_refs_rsv.
	
	  Need to drop our head ref lock and re-acquire the delayed ref lock
	  and then re-check to make sure nobody got added.
	
	  Grab the lock that says we are going to process all the refs for
	  this head
	
	  We may have dropped the spin lock to get the head mutex lock, and
	  that might have given someone else time to free the head.  If that's
	  true, it has been removed from our list and we can move on.
		
		  When we play the delayed ref, also correct the ref_mod on
		  head
		
		  Record the must_insert_reserved flag before we drop the
		  spin lock.
  Returns 0 on success or if called with an already aborted transaction.
  Returns -ENOMEM or -EIO on failure and will abort the transaction.
		
		  We need to try and merge adddrops of the same ref since we
		  can run into issues with relocate dropping the implicit ref
		  and then it being added back again before the drop can
		  finish.  If we merged anything we need to re-loop so we can
		  get a good ref.
		  Or we can get node references of the same type that weren't
		  merged when created due to bumps in the tree mod seq, and
		  we need to merge them to prevent adding an inline extent
		  backref before dropping it (triggering a BUG_ON at
		  insert_inline_extent_backref()).
			
			  Error, btrfs_run_delayed_refs_for_head already
			  unlocked everything so just bail out
			
			  Success, perform the usual cleanup of a processed
			  head
 We dropped our lock, we need to loop. 
		
		  Either success case or btrfs_run_delayed_refs_for_head
		  returned -EAGAIN, meaning we need to select another head
	
	  We don't want to include ref heads since we can have empty ref heads
	  and those will drastically skew our runtime down since we just do
	  accounting, no actual extent tree updates.
		
		  We weigh the current average higher than our current runtime
		  to avoid large swings in the average.
 div by 4 
  Normally delayed refs get processed in ascending bytenr order. This
  correlates in most cases to the order added. To expose dependencies on this
  order, we start to process the tree in the middle instead of the beginning
  this starts processing the delayed reference count updates and
  extent insertions we have queued up so far.  count can be
  0, which means to process everything in the tree at the start
  of the run (but not newly added entries), or it can be some target
  number you'd like to process.
  Returns 0 on success or if called with an aborted transaction
  Returns <0 on error and aborts the transaction
 We'll clean this up in btrfs_cleanup_transaction 
 Mutex was contended, block until it's released and retry. 
		
		  Mutex was contended, block until it's released and let
		  caller try again
	
	  XXX: We should replace this with a proper search function in the
	  future.
 If it's a shared ref we know a cross reference exists 
		
		  If our ref doesn't match the one we're currently looking at
		  then we have a cross reference.
 Corruption 
 If extent item has more than 1 inline ref then it's shared 
	
	  If extent created before last snapshot => it's shared unless the
	  snapshot has been deleted. Use the heuristic if strict is false.
 If this extent has SHARED_DATA_REF then it's shared 
 Logic error 
  this function must be called within transaction
	
	  pull in the free space cache (if any) so that our pin
	  removes the free space from the cache.  We have load_only set
	  to one because the slow code to read in the free extents does check
	  the pinned extents.
	
	  Make sure we wait until the cache is completely built in case it is
	  missing or is invalid and therefore needs to be rebuilt.
 remove us from the free space cache (if we're there at all) 
	
	  Make sure we wait until the cache is completely built in case it is
	  missing or is invalid and therefore needs to be rebuilt.
  Returns the free cluster for the given space info and sets empty_cluster to
  what it should be based on the mount options.
 Logic error 
		
		  If this space cluster has been marked as fragmented and we've
		  unpinned enough in this block group to potentially allow a
		  cluster to be created inside of it go ahead and clear the
		  fragmented check.
 Need reset before reusing in a zoned block group 
 Add to any tickets we may have 
	
	  Transaction is finished.  We don't need the lock anymore.  We
	  do need to clean up the block groups in case of a transaction
	  abort.
  Drop one or more refs of @node.
  1. Locate the extent refs.
     It's either inline in EXTENTMETADATA_ITEM or in keyed SHARED_ item.
     Locate it, then reduce the refs number or remove the ref line completely.
  2. Update the refs count in EXTENTMETADATA_ITEM
  Inline backref case:
  in extent tree we have:
  	item 0 key (13631488 EXTENT_ITEM 1048576) itemoff 16201 itemsize 82
 		refs 2 gen 6 flags DATA
 		extent data backref root FS_TREE objectid 258 offset 0 count 1
 		extent data backref root FS_TREE objectid 257 offset 0 count 1
  This function gets called with:
     node->bytenr = 13631488
     node->num_bytes = 1048576
     root_objectid = FS_TREE
     owner_objectid = 257
     owner_offset = 0
     refs_to_drop = 1
  Then we should get some like:
  	item 0 key (13631488 EXTENT_ITEM 1048576) itemoff 16201 itemsize 82
 		refs 1 gen 6 flags DATA
 		extent data backref root FS_TREE objectid 258 offset 0 count 1
  Keyed backref case:
  in extent tree we have:
 	item 0 key (13631488 EXTENT_ITEM 1048576) itemoff 3971 itemsize 24
 		refs 754 gen 6 flags DATA
 	[...]
 	item 2 key (13631488 EXTENT_DATA_REF <HASH>) itemoff 3915 itemsize 28
 		extent data backref root FS_TREE objectid 866 offset 0 count 1
  This function get called with:
     node->bytenr = 13631488
     node->num_bytes = 1048576
     root_objectid = FS_TREE
     owner_objectid = 866
     owner_offset = 0
     refs_to_drop = 1
  Then we should get some like:
 	item 0 key (13631488 EXTENT_ITEM 1048576) itemoff 3971 itemsize 24
 		refs 753 gen 6 flags DATA
  And that (13631488 EXTENT_DATA_REF <HASH>) gets removed.
		
		  Either the inline backref or the SHARED_DATA_REF
		  SHARED_BLOCK_REF is found
		 
		  Here is a quick path to locate EXTENTMETADATA_ITEM.
		  It's possible the EXTENTMETADATA_ITEM is near current slot.
 Quick path didn't find the EXTEMTMETADATA_ITEM 
 Must be SHARED_ item, remove the backref first 
 Slow path to locate EXTENTMETADATA_ITEM 
				
				  Couldn't find our skinny metadata item,
				  see if we have ye olde extent item.
		
		  In the case of inline back ref, reference count will
		  be updated by remove_extent_backref
 In this branch refs == 1 
				
				  No inline ref, we must be at SHARED_ item,
				  And it's single ref, it must be:
				  |	extent_slot	  ||extent_slot + 1|
				  [ EXTENTMETADATA_ITEM ][ SHARED_ ITEM ]
	
	  Leaf dump can take up a lot of log buffer, so we only do full leaf
	  dump for debug build.
  when we free an block, it is possible (and likely) that we free the last
  delayed ref for that extent as well.  This searches the delayed ref tree for
  a given extent, and if there are no other delayed refs to be processed, it
  removes it from the tree.
	
	  waiting for the lock here would deadlock.  If someone else has it
	  locked they are already in the process of dropping it anyway
 -ENOMEM 
		
		  If this is a leaf and there are tree mod log users, we may
		  have recorded mod log operations that point to this leaf.
		  So we must make sure no one reuses this leaf's extent before
		  mod log operations are applied to a node, otherwise after
		  rewinding a node using the mod log operations we get an
		  inconsistent btree, as the leaf's extent may now be used as
		  a node or leaf for another different btree.
		  We are safe from races here because at this point no other
		  node or root points to this extent buffer, so if after this
		  check a new tree mod log user joins, it will not be able to
		  find a node pointing to this leaf and record operations that
		  point to this leaf.
		
		  Deleting the buffer, clear the corrupt flag since it doesn't
		  matter anymore.
 Can return -ENOMEM 
	
	  tree log blocks never actually go into the extent allocation
	  tree, just update pinning info and exit early.
 unlocks the pinned mutex 
 We should only have one-level nested. 
  Structure used internally for find_free_extent() function.  Wraps needed
  parameters.
 Basic allocation info 
 Where to start the search inside the bg 
 For clustered allocation 
 Allocation is called for tree-log 
 Allocation is called for data relocation 
 RAID index, converted from flags 
	
	  Current loop number, check find_free_extent_update_loop() for details
	
	  Whether we're refilling a cluster, if true we need to re-search
	  current block group but don't try to refill the cluster again.
	
	  Whether we're updating free space cache, if true we need to re-search
	  current block group but don't try updating free space cache again.
 If current block group is cached 
 Max contiguous hole found 
 Total free space from free space cache, not always contiguous 
 Found result 
 Hint where to start looking for an empty space 
 Allocation policy 
  Helper function for find_free_extent().
  Return -ENOENT to inform caller that we need fallback to unclustered mode.
  Return -EAGAIN to inform caller that we need to re-search this block group
  Return >0 to inform caller that we find nothing
  Return 0 means we have found a location and set ffe_ctl->found_offset.
 We have a block, we're done 
	
	  If we are on LOOP_NO_EMPTY_SIZE, we can't set up a new clusters, so
	  lets just skip it and let the allocator find whatever block it can
	  find. If we reach this point, we will have tried the cluster
	  allocator plenty of times and not have found anything, so we are
	  likely way too fragmented for the clustering stuff to find anything.
	 
	  However, if the cluster is taken from the current block group,
	  release the cluster first, so that we stand a better chance of
	  succeeding in the unclustered allocation.
 This cluster didn't work out, free it and start over 
 Now pull our allocation out of this cluster 
 We found one, proceed 
	
	  At this point we either didn't find a cluster or we weren't able to
	  allocate a block from our cluster.  Free the cluster we've been
	  trying to use, and go to the next block group.
  Return >0 to inform caller that we find nothing
  Return 0 when we found an free extent and set ffe_ctrl->found_offset
  Return -EAGAIN to inform caller that we need to re-search this block group
	
	  We are doing an unclustered allocation, set the fragmented flag so
	  we don't bother trying to setup a cluster again until we get more
	  space.
	
	  If we didn't find a chunk, and we haven't failed on this block group
	  before, and this block group is in the middle of caching and we are
	  ok with waiting, then go ahead and wait for progress to be made, and
	  set @retry_unclustered to true.
	 
	  If @retry_unclustered is true then we've already waited on this
	  block group once and should move on to the next block group.
 We want to try and use the cluster allocator, so lets look there 
 ret == -ENOENT case falls through 
  Tree-log block group locking
  ============================
  fs_info::treelog_bg_lock protects the fs_info::treelog_bg which
  indicates the starting address of a block group, which is reserved only
  for tree-log metadata.
  Lock nesting
  ============
  space_info::lock
    block_group::lock
      fs_info::treelog_bg_lock
  Simple allocator for sequential-only block group. It only allows sequential
  allocation. No need to play with trees. This function also reserves the
  bytes as in btrfs_add_reserved_bytes.
	
	  Do not allow non-tree-log blocks in the dedicated tree-log block
	  group, and vice versa.
	
	  Do not allow non-relocation blocks in the dedicated relocation block
	  group, and vice versa.
 Check RO and no space case before trying to activate it 
	
	  Do not allow currently using block group to be tree-log dedicated
	  block group.
	
	  Do not allow currently used block group to be the data relocation
	  dedicated block group.
			
			  With sequential allocator, free space is always
			  contiguous
	
	  We do not check if found_offset is aligned to stripesize. The
	  address is anyway rewritten when using zone append writing.
 Nothing to do 
 Nothing to do 
		
		  If we can't allocate a new chunk we've already looped through
		  at least once, move on to the NO_EMPTY_SIZE case.
 Give up here 
  Return >0 means caller needs to re-search for free extent
  Return 0 means we have the needed free extent.
  Return <0 means we failed to locate any free extent.
		
		  If we have enough free space left in an already active block
		  group and we can't activate any other zone now, retry the
		  active ones with a smaller allocation size.  Returning early
		  from here will tell btrfs_reserve_extent() to haven the
		  size.
	
	  LOOP_CACHING_NOWAIT, search partially cached block groups, kicking
	 			caching kthreads as we move along
	  LOOP_CACHING_WAIT, search everything, and wait if our bg is caching
	  LOOP_ALLOC_CHUNK, force a chunk allocation and try again
	  LOOP_NO_EMPTY_SIZE, set empty_size and empty_cluster to 0 and try
	 		       again
			
			  We want to skip the LOOP_CACHING_WAIT step if we
			  don't have any uncached bgs and we've already done a
			  full search through.
 Do not bail out on ENOSPC since we can do more. 
			
			  Don't loop again if we already have no empty_size and
			  no empty_cluster.
	
	  If our free space is heavily fragmented we may not be able to make
	  big contiguous allocations, so instead of doing the expensive search
	  for free space, simply return ENOSPC with our max_extent_size so we
	  can go ahead and search for a more manageable chunk.
	 
	  If our max_extent_size is large enough for our allocation simply
	  disable clustering since we will likely not be able to find enough
	  space to create a cluster and induce latency trying.
			
			  We still set window_start so we can keep track of the
			  last place we found an allocation to try and save
			  some time.
  walks the btree of allocated extents and find a hole of a given size.
  The key ins is changed to record the hole:
  ins->objectid == start position
  ins->flags = BTRFS_EXTENT_ITEM_KEY
  ins->offset == the size of the hole.
  Any available blocks before search_start are skipped.
  If there is no suitable free space, we will record the max size of
  the free space extent currently.
  The overall logic and call chain:
  find_free_extent()
  |- Iterate through all block groups
  |  |- Get a valid block group
  |  |- Try to do clustered allocation in that block group
  |  |- Try to do unclustered allocation in that block group
  |  |- Check if the result is valid
  |  |  |- If valid, then exit
  |  |- Jump to next block group
  |
  |- Push harder to find free extents
     |- If not found, re-iterate all block groups
 For clustered allocation 
 For clustered allocation 
		
		  we don't want to use the block group if it doesn't match our
		  allocation bits, or if its not cached.
		 
		  However if we are re-searching with an ideal block group
		  picked out then we don't care that the block group is cached.
				
				  someone is removing this block group,
				  we can't jump into the have_block_group
				  target because our list pointers are not
				  valid
 If the block group is read-only, we can skip it entirely. 
		
		  this can happen if we end up cycling through all the
		  raid types, but we want to make sure we only allocate
		  for the proper type.
			
			  if they asked for extra copies and this block group
			  doesn't provide them, bail.  This does allow us to
			  fill raid0 from raid1.
			
			  This block group has different flags than we want.
			  It's possible that we have MIXED_GROUP flag but no
			  block group is mixed.  Just skip such block group.
			
			  If we get ENOMEM here or something else we want to
			  try other block groups, because it may not be fatal.
			  However if we can't find anything else we need to
			  save our return here so that we return the actual
			  error that caused problems, not ENOSPC.
 Checks 
 move on to the next group 
 we are all good, lets return 
		
		  Use ffe_ctl->total_free_space as fallback if we can't find
		  any contiguous hole.
  btrfs_reserve_extent - entry point to the extent allocator. Tries to find a
 			  hole that is at least as big as @num_bytes.
  @root           -	The root that will contain this extent
  @ram_bytes      -	The amount of space in ram that @num_bytes take. This
 			is used for accounting purposes. This value differs
 			from @num_bytes only in the case of compressed extents.
  @num_bytes      -	Number of bytes to allocate on-disk.
  @min_alloc_size -	Indicates the minimum amount of space that the
 			allocator should try to satisfy. In some cases
 			@num_bytes may be larger than what is required and if
 			the filesystem is fragmented then allocation fails.
 			However, the presence of @min_alloc_size gives a
 			chance to try and satisfy the smaller allocation.
  @empty_size     -	A hint that you plan on doing more COW. This is the
 			size in bytes the allocator should try to find free
 			next to the block it returns.  This is just a hint and
 			may be ignored by the allocator.
  @hint_byte      -	Hint to the allocator to start searching above the byte
 			address passed. It might be ignored.
  @ins            -	This key is modified to record the found hole. It will
 			have the following values:
 			ins->objectid == start position
 			ins->flags = BTRFS_EXTENT_ITEM_KEY
 			ins->offset == the size of the hole.
  @is_data        -	Boolean flag indicating whether an extent is
 			allocated for data (true) or metadata (false)
  @delalloc       -	Boolean flag indicating whether this allocation is for
 			delalloc or not. If 'true' data_rwsem of block groups
 			is going to be acquired.
  Returns 0 when an allocation succeeded or < 0 when an error occurred. In
  case -ENOSPC is returned then @ins->offset will contain the size of the
  largest available hole the allocator managed to find.
 -ENOENT, logic error 
 -ENOENT, logic error 
  this is used by the tree logging recovery code.  It records that
  an extent has been allocated and makes sure to clear the free
  space cache bits as well
	
	  Mixed block groups will exclude before processing the log so we only
	  need to do the exclude dance if this fs isn't mixed.
	
	  Extra safety check in case the extent tree is corrupted and extent
	  allocator chooses to use a tree block which is already used and
	  locked.
	
	  This needs to stay, because we could allocate a freed block from an
	  old tree into a new tree, so we need to make sure this new block is
	  set to the appropriate level and owner.
		
		  we allow two log transactions at a time, use different
		  EXTENT bit to differentiate dirty pages.
 this returns a buffer locked for blocking 
  finds a free extent and does all the dirty work required for allocation
  returns the tree buffer or an ERR_PTR on error.
 We don't lock the tree block, it's OK to be racy here 
 We don't care about errors in readahead. 
  helper to process tree block while walking down the tree.
  when wc->stage == UPDATE_BACKREF, this function updates
  back refs for pointers in the block.
  NOTE: return value 1 means we should stop walking down.
	
	  when reference count of tree block is 1, it won't increase
	  again. once full backref flag is set, we never clear it.
 wc->stage == UPDATE_BACKREF 
 -ENOMEM 
 -ENOMEM 
 -ENOMEM 
	
	  the block is shared by multiple trees, so it's not good to
	  keep the tree lock
  This is used to verify a ref exists for this root to deal with a bug where we
  would have a drop_progress key that hadn't been updated properly.
  helper to process tree block pointer.
  when wc->stage == DROP_REFERENCE, this function checks
  reference count of the block pointed to. if the block
  is shared and we need update back refs for the subtree
  rooted at the block, this function changes wc->stage to
  UPDATE_BACKREF. if the block is shared and there is no
  need to update back, this function drops the reference
  to the block.
  NOTE: return value 1 means we should stop walking down.
	
	  if the lower level block was created before the snapshot
	  was created, we know there is no need to update back refs
	  for the subtree
		
		  If we had a drop_progress we need to verify the refs are set
		  as expected.  If we find our ref then we know that from here
		  on out everything should be correct, and we can clear the
		  ->restarted flag.
		
		  Reloc tree doesn't contribute to qgroup numbers, and we have
		  already accounted them at merge time (replace_path),
		  thus we could skip expensive subtree trace here.
		
		  We need to update the next key in our walk control so we can
		  update the drop_progress key accordingly.  We don't care if
		  find_next_key doesn't find a key because that means we're at
		  the end and are going to clean up now.
  helper to process tree block while walking up the tree.
  when wc->stage == DROP_REFERENCE, this function drops
  reference count on the block.
  when wc->stage == UPDATE_BACKREF, this function changes
  wc->stage back to DROP_REFERENCE if we changed wc->stage
  to UPDATE_BACKREF previously while processing the block.
  NOTE: return value 1 means we should stop walking up.
		
		  check reference count again if the block isn't locked.
		  we should start walking down the tree again if reference
		  count is one.
 wc->stage == DROP_REFERENCE 
 -ENOMEM 
 make block locked assertion in btrfs_clean_tree_block happy 
  drop a subvolume tree.
  this function traverses the tree freeing any blocks that only
  referenced by the tree.
  when a shared tree block is found. this function decreases its
  reference count by one. if update_ref is true, this function
  also make sure backrefs for the shared block and all lower level
  blocks are properly updated.
  If called with for_reloc == 0, may exit early with -EAGAIN
	
	  Use join to avoid potential EINTR from transaction start. See
	  wait_reserve_ticket and the whole reservation callchain.
	
	  This will help us catch people modifying the fs tree while we're
	  dropping it.  It is unsafe to mess with the fs tree while it's being
	  dropped as we unlock the root node and parent nodes as we walk down
	  the tree, assuming nothing will change.  If something does change
	  then we'll have stale information and drop references to blocks we've
	  already dropped.
		
		  unlock our path, this is safe because only this
		  function is allowed to delete this snapshot
		       
			 Use join to avoid potential EINTR from transaction
			 start. See wait_reserve_ticket and the whole
			 reservation callchain.
			 if we fail to delete the orphan item this time
			  around, it'll get picked up the next time.
			 
			  The most common failure here is just -ENOENT.
	
	  This subvolume is going to be completely dropped, and won't be
	  recorded as dirty roots, thus pertrans meta rsv will not be freed at
	  commit transaction time.  So free it here manually.
	
	  So if we need to stop dropping the snapshot for whatever reason we
	  need to make sure to add it back to the dead root list so that we
	  keep trying to do the work later.  This also cleans up roots if we
	  don't have it in the radix (like when we recover after a power fail
	  or unmount) so we don't leak memory.
  drop subtree rooted at tree block 'node'.
  NOTE: this function will unlock and release tree block 'node'
  only used by relocation code
  helper to account the unused space of all the readonly block group in the
  space_info. takes mirrors into account.
 It's df, we don't care if it's racy 
  It used to be that old block groups would be left around forever.
  Iterating over them would be enough to trim unused space.  Since we
  now automatically remove them, we also need to iterate over unallocated
  space.
  We don't want a transaction for this since the discard may take a
  substantial amount of time.  We don't require that a transaction be
  running, but we do need to take a running transaction into account
  to ensure that we're not discarding chunks that were released or
  allocated in the current transaction.
  Holding the chunks lock will prevent other threads from allocating
  or releasing chunks, but it won't prevent a running transaction
  from committing and releasing the memory that the pending chunks
  list head uses.  For that, we need to take a reference to the
  transaction and hold the commit root sem.  We only need to hold
  it while performing the free space search since we have already
  held back allocations.
 Discard not supported = nothing to do. 
 Not writable = nothing to do. 
 No free space = nothing to do. 
 Check if there are any CHUNK_ bits left 
 Ensure we skip the reserved area in the first 1M 
		
		  If find_first_clear_extent_bit find a range that spans the
		  end of the device it will set end to -1, in this case it's up
		  to the caller to trim the value to the size of the device.
 We didn't find any extents 
  Trim the whole filesystem by:
  1) trimming the free space in each block group
  2) trimming the unallocated space on each device
  This will also continue trimming even if a block group or device encounters
  an error.  The return value will be the last error, or 0 if nothing bad
  happens.
	
	  Check range overflow if range->len is set.
	  The default range->len is U64_MAX.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
  Set inode's size according to filesystem options
  @inode:      inode we want to update the disk_i_size for
  @new_i_size: i_size we want to set to, 0 if we use i_size
  With NO_HOLES set this simply sets the disk_is_size to whatever i_size_read()
  returns as it is perfectly fine with a file that has holes without hole file
  extent items.
  However without NO_HOLES we need to only return the area that is contiguous
  from the 0 offset of the file.  Otherwise we could end up adjust i_size up
  to an extent that has a gap in between.
  Finally new_i_size should only be set in the case of truncate where we're not
  ready to use i_size_read() as the limiter yet.
  Mark range within a file as having a new extent inserted
  @inode: inode being modified
  @start: start file offset of the file extent we've inserted
  @len:   logical length of the file extent item
  Call when we are inserting a new file extent where there was none before.
  Does not need to call this in the case where we're replacing an existing file
  extent, however if not sure it's fine to call this multiple times.
  The start and len must match the file extent item, so thus must be sectorsize
  aligned.
  Marks an inode range as not having a backing extent
  @inode: inode being modified
  @start: start file offset of the file extent we've inserted
  @len:   logical length of the file extent item
  Called when we drop a file extent, for example when we truncate.  Doesn't
  need to be called for cases where we're replacing a file extent, like when
  we've COWed a file extent.
  The start and len must match the file extent item, so thus must be sectorsize
  aligned.
 Can't happen 
  Find checksums for logical bytenr range [disk_bytenr, disk_bytenr + len) and
  estore the result to @dst.
  Return >0 for the number of sectors we found.
  Return 0 for the range [disk_bytenr, disk_bytenr + sectorsize) has no csum
  for it. Caller may want to try next sector until one range is hit.
  Return <0 for fatal error.
 Check if the current csum item covers disk_bytenr 
 Current item doesn't contain the desired range, search again 
  Locate the file_offset of @cur_disk_bytenr of a @bio.
  Bio of btrfs represents read range of
  [bi_sector << 9, bi_sector << 9 + bi_size).
  Knowing this, we can iterate through each bvec to locate the page belong to
  @cur_disk_bytenr and get the file offset.
  @inode is used to determine if the bvec page really belongs to @inode.
  Return 0 if we can't find the file offset
  Return >0 if we find the file offset and restore it to @file_offset_ret
  Lookup the checksum for the read bio in csum tree.
  @inode: inode that the bio is for.
  @bio: bio to look up.
  @dst: Buffer of size nblocks  btrfs_super_csum_size() used to return
        checksum (nblocks = bio->bi_iter.bi_size  fs_info->sectorsize). If
        NULL, the checksum buffer is allocated and returned in
        btrfs_bio(bio)->csum instead.
  Return: BLK_STS_RESOURCE if allocating memory fails, BLK_STS_OK otherwise.
	
	  This function is only called for read bio.
	 
	  This means two things:
	  - All our csums should only be in csum tree
	    No ordered extents csums, as ordered extents are only for write
	    path.
	  - No need to bother any other info from bvec
	    Since we're looking up csums, the only important info is the
	    disk_bytenr and the length, which can be extracted from bi_iter
	    directly.
	
	  If requested number of sectors is larger than one leaf can contain,
	  kick the readahead for csum tree.
	
	  the free space stuff is only read when it hasn't been
	  updated in the current transaction.  So, we can safely
	  read from the commit root and sidestep a nasty deadlock
	  between reading the free space cache and updating the csum tree.
		
		  Although both cur_disk_bytenr and orig_disk_bytenr is u64,
		  we're calculating the offset to the bio start.
		 
		  Bio size is limited to UINT_MAX, thus unsigned int is large
		  enough to contain the raw result, not to mention the right
		  shifted result.
			
			  Either we hit a critical error or we didn't find
			  the csum.
			  Either way, we put zero into the csums dst, and skip
			  to the next sector.
			
			  For data reloc inode, we need to mark the range
			  NODATASUM so that balance won't report false csum
			  error.
  btrfs_csum_one_bio - Calculates checksums of the data contained inside a bio
  @inode:	 Owner of the data inside the bio
  @bio:	 Contains the data to be checksummed
  @file_start:  offset in file this bio begins to describe
  @contig:	 Boolean. If true1 means all bio vecs in this bio are
 		 contiguous and they begin at @file_start in the file. False0
 		 means this bio can contain potentially discontiguous bio vecs
 		 so the logical offset of each should be calculated separately.
 shut up gcc 
			
			  The bio range is not covered by any ordered extent,
			  must be a code logic error.
 -ENOMEM 
 Logic error 
  helper function for csum removal, this expects the
  key to describe the csum pointed to by the path, and it expects
  the csum to overlap the range [bytenr, len]
  The csum should not be entirely contained in the range and the
  range should not be entirely contained in the csum.
  This calls btrfs_truncate_item with the correct args based on the
  overlap, and fixes up the key as required.
		
		          [ bytenr - len ]
		          [   ]
		    [csum     ]
		    A simple truncate off the end of the item
		
		          [ bytenr - len ]
		                  [ ]
		                  [csum     ]
		  we need to truncate from the beginning of the csum
  deletes the csum items from the csum tree for a given
  range of bytes.
 this csum ends before we start, we're done 
 delete the entire item, it is inside our range 
			
			  Check how many csum items preceding this one in this
			  leaf correspond to our range and then delete them all
			  at once.
			
			         [ bytenr - len ]
			      [csum                ]
			 
			  Our bytes are in the middle of the csum,
			  we need to split this item and insert a new one.
			 
			  But we can't drop the path because the
			  csum could change, get removed, extended etc.
			 
			  The trick here is the max size of a csum item leaves
			  enough room in the tree block for a single
			  item header.  So, we split the item in place,
			  adding a new header pointing to the existing
			  bytes.  Then we loop around again and we have
			  a nicely formed csum item that we can neatly
			  truncate.
			
			  btrfs_split_item returns -EAGAIN when the
			  item changed size or key
 we found one, but it isn't big enough yet 
 already at max size, make a new one 
 We didn't find a csum item, insert one. 
	
	  At this point, we know the tree has a checksum item that ends at an
	  offset matching the start of the checksum range we want to insert.
	  We try to extend that item as much as possible and then add as many
	  checksums to it as they fit.
	 
	  First check if the leaf has enough free space for at least one
	  checksum. If it has go directly to the item extension code, otherwise
	  release the path and do a search for insertion before the extension.
		
		  A log tree can already have checksum items with a subset of
		  the checksums we are trying to log. This can happen after
		  doing a sequence of partial writes into prealloc extents and
		  fsyncs in between, with a full fsync logging a larger subrange
		  of an extent for which a previous fast fsync logged a smaller
		  subrange. And this happens in particular due to merging file
		  extent items when we complete an ordered extent for a range
		  covered by a prealloc extent - this is done at
		  btrfs_mark_extent_written().
		 
		  So if we try to extend the previous checksum item, which has
		  a range that ends at the start of the range we want to insert,
		  make sure we don't extend beyond the start offset of the next
		  checksum item. If we are at the last item in the leaf, then
		  forget the optimization of extending and add a new checksum
		  item - it is not worth the complexity of releasing the path,
		  getting the first key for the next leaf, repeat the btree
		  search, etc, because log trees are temporary anyway and it
		  would only save a few bytes of leaf space.
		
		  Initialize orig_start and block_len with the same values
		  as in inode.c:btrfs_get_extent().
  Returns the end offset (non inclusive) of the file extent item the given path
  points to. If it points to an inline extent, the returned offset is rounded
  up to the sector size.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
  Read a root item from the tree. In case we detect a root item smaller then
  sizeof(root_item), we know it's an old version of the root structure and
  initialize all new fields to zero. The same happens if we detect mismatching
  generation numbers as then we know the root was once mounted with an older
  kernel that was not aware of the root item structure change.
 Clear all members from generation_v2 onwards. 
  btrfs_find_root - lookup the root by the key.
  root: the root of the root tree
  search_key: the key to search
  path: the path we search
  root_item: the root item of the tree we look for
  root_key: the root key of the tree we look for
  If ->offset of 'search_key' is -1ULL, it means we are not sure the offset
  of the search key, just lookup the root with the highest offset for a
  given objectid.
  If we find something return 0, otherwise > 0, < 0 on error.
 the search key is exact 
 Logical error 
  copy the data in 'item' into the btree
	
	  If this is the first time we update the root item which originated
	  from an older kernel, we need to enlarge the item size to make room
	  for the added fields.
	
	  Update generation_v2 so at the next mount we know the new root
	  fields are valid.
	
	  Make sure generation v1 and v2 match. See update_root for details.
 drop the root item for 'key' from the tree root 
  add a btrfs_root_ref item.  type is either BTRFS_ROOT_REF_KEY
  or BTRFS_ROOT_BACKREF_KEY.
  The dirid, sequence, name and name_len refer to the directory entry
  that is referencing the root.
  For a forward ref, the root_id is the id of the tree referencing
  the root and ref_id is the id of the subvol  or snapshot.
  For a back ref the root_id is the id of the subvol or snapshot and
  ref_id is the id of the tree referencing it.
  Will return 0, -ENOMEM, or anything from the CoW path
  Old btrfs forgets to init root_item->flags and root_item->byte_limit
  for subvolumes. To work around this problem, we steal a bit from
  root_item->inode_item->flags, and use it to indicate if those fields
  have been properly initialized.
  btrfs_subvolume_reserve_metadata() - reserve space for subvolume operation
  root: the root of the parent directory
  rsv: block reservation
  items: the number of items that we need do reservation
  use_global_rsv: allow fallback to the global block reservation
  This function is used to reserve the space for snapshotsubvolume
  creation and deletion. Those operations are different with the
  common filedirectory operations, they change two fsfile trees
  and root tree, the number of items that the qgroup reserves is
  different with the free space reservation. So we can not use
  the space reservation mechanism in start_transaction().
 One for parent inode, two for dir entries 
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2008 Red Hat.  All rights reserved.
	
	  We are often under a trans handle at this point, so we need to make
	  sure NOFS is set to keep us from deadlocking.
 We inline CRCs for the free disk space cache 
  inode is an optional sink: if it is NULL, btrfs_remove_free_space_inode
  handles lookup, otherwise it takes ownership and iputs the inode.
  Don't reuse an inode pointer after passing it into this function.
 One for the block groups ref 
 One for the lookup ref 
 1 for slack space, 1 for updating the inode 
		
		  now that we've truncated the cache away, its no longer
		  setup or written
	
	  We skip the throttling logic for free space cache inodes, so we don't
	  need to check for -EAGAIN.
 Make sure we can fit our crcs and generation into the first page 
	
	  Skip the csum areas.  If we don't check crcs then we just have a
	  64bit chunk at the front of the first page.
	
	  Skip the crc area.  If we don't check crcs then we just have a 64bit
	  chunk at the front of the first page.
 No more pages to map 
 map the next page 
	
	  If we aren't at the start of the current page, unmap this one and
	  map the next one if there is any left.
	
	  If we're not on the boundary we know we've modified the page and we
	  need to crc the page.
	
	  We are trying to keep the total amount of memory used per 1GiB of
	  space to be MAX_CACHE_BYTES_PER_GIG.  However, with a reclamation
	  mechanism of pulling extents >= FORCE_EXTENT_THRESHOLD out of
	  bitmaps, we may end up using more memory than this.
	
	  we want the extent entry threshold to always be at most 12 the max
	  bytes we can have, or whatever is less than that.
 Nothing in the space cache, goodbye 
	
	  We add the bitmaps at the end of the entries in order that
	  the bitmap entries are added to the cache.
	
	  Because we could potentially discard our loaded free space, we want
	  to load everything into a temporary structure first, and then if it's
	  valid copy it all into the actual free space ctl.
	
	  If this block group has been marked to be cleared for one reason or
	  another then we can't trust the on disk cache, so just return.
	
	  We must pass a path with search_commit_root set to btrfs_iget in
	  order to avoid a deadlock when allocating extents for the tree root.
	 
	  When we are COWing an extent buffer from the tree root, when looking
	  for a free extent, at extent-tree.c:find_free_extent(), we can find
	  block group without its free space cache loaded. When we find one
	  we must load its space cache which requires reading its free space
	  cache's inode item from the root tree. If this inode item is located
	  in the same leaf that we started COWing before, then we end up in
	  deadlock on the extent buffer (trying to read lock it when we
	  previously write locked it).
	 
	  It's safe to read the inode item using the commit root because
	  block groups, once loaded, stay in memory forever (until they are
	  removed) as well as their space caches once loaded. New block groups
	  once created get their ->cached field set to BTRFS_CACHE_FINISHED so
	  we will never try to read their inode item while the fs is mounted.
 We may have converted the inode and made the cache invalid. 
		
		  ret == 1 means we successfully loaded the free space cache,
		  so we need to re-set it here.
 This cache is bogus, make sure it gets cleared 
 Get the cluster for this block_group if it exists 
 Write out the extent entries 
	
	  Make sure we don't miss any range that was removed from our rbtree
	  because trimming is running. Otherwise after a umount+mount (or crash
	  after committing the transaction) we would leak free space and get
	  an inconsistent free space cache report from fsck.
	
	  We want to add any pinned extents to our free space cache
	  so we don't leak the space
	 
	  We shouldn't have switched the pinned extents yet so this is the
	  right one
 This pinned extent is out of our range 
 Write out the bitmaps 
 Flush the dirty pages in the cache file. 
 Update the cache item to tell everyone this cache file is valid. 
 the dirty list is protected by the dirty_bgs_lock 
 the disk_cache_state is protected by the block group lock 
		
		  only mark this as written if we didn't get put back on
		  the dirty list while waiting for IO.   Otherwise our
		  cache state won't be right, and we won't get written again
  Write out cached info to an inode
  @root:        root the inode belongs to
  @inode:       freespace inode we are writing out
  @ctl:         free space cache we are going to write out
  @block_group: block_group for this cache if it belongs to a block_group
  @io_ctl:      holds context for the io
  @trans:       the trans handle
  This function writes out a free space cache struct to disk for quick recovery
  on mount.  This will return 0 if it was successful in writing the cache out,
  or an errno if it was not.
 Lock all pages first so we can lock the extent safely. 
 Write out the extent entries in the free space cache 
	
	  Some spaces that are freed in the current transaction are pinned,
	  they will be added into free space cache after the transaction is
	  committed, we shouldn't lose them.
	 
	  If this changes while we are working we'll get added back to
	  the dirty list and redo it.  No locking needed
	
	  At last, we write out all the bitmaps and keep cache_writeout_mutex
	  locked while doing it because a concurrent trim can be manipulating
	  or freeing the bitmap.
 Zero out the rest of the pages just to make sure 
 Everything is written out, now we dirty the pages in the file. 
	
	  Release the pages and unlock the extent, we will flush
	  them out later
	
	  at this point the pages are under IO and we're happy,
	  The caller is responsible for waiting on them and updating
	  the cache and the inode
	
	  if ret == 0 the caller is expected to call btrfs_wait_cache_io
	  to wait for IO and put the inode
			
			  we could have a bitmap entry and an extent entry
			  share the same offset.  If this is the case, we want
			  the extent entry to always be found first if we do a
			  linear search through the tree, since we want to have
			  the quickest allocation time, and allocating from an
			  extent is faster than allocating from a bitmap.  So
			  if we're inserting a bitmap and we find an entry at
			  this offset, we want to go right, or after this entry
			  logically.  If we are inserting an extent and we've
			  found a bitmap, we want to go left, or before
			  logically.
  searches the tree for the given offset.
  fuzzy - If this is set, then we are trying to make an allocation, and we just
  want a section that has at least bytes size and comes at or after the given
  offset.
 find entry that is closest to the 'offset' 
		
		  bitmap entry and extent entry may share same offset,
		  in that case, bitmap entry comes after extent entry.
			
			  if previous extent entry covers the offset,
			  we should return it instead of the bitmap entry
 find last entry before the 'offset' 
  If we can not find suitable extent, we will use bytes to record
  the size of the max extent.
	
	  Skip searching the bitmap if we don't have a contiguous section that
	  is large enough for this allocation.
 Cache the size of the max extent in bytes 
		 make sure the space returned is big enough
		  to match our requested alignment
	
	  Normally when this is called, the bitmap is completely empty. However,
	  if we are blowing up the free space cache for one reason or another
	  via __btrfs_remove_free_space_cache(), then it may not be freed and
	  we may leave stats on the table.
	
	  We need to search for bits in this bitmap.  We could only cover some
	  of the extent in this bitmap thanks to how we add space, so we need
	  to search for as much as it as we can and clear that amount, and then
	  go searching for the next bit.
 We may have found more bits than what we need 
 Cannot clear past the end of the bitmap 
		
		  no entry after this bitmap, but we still have bytes to
		  remove, so something has gone wrong.
		
		  if the next entry isn't a bitmap we need to return to let the
		  extent stuff do its work.
		
		  Ok the next item is a bitmap, but it may not actually hold
		  the information for the rest of this free space stuff, so
		  look for it, and if we don't find it return so we can try
		  everything over again.
	
	  This is a tradeoff to make bitmap trim state minimal.  We mark the
	  whole bitmap untrimmed if at any point we add untrimmed regions.
	
	  We set some bytes, we have no idea what the max extent size is
	  anymore.
 This is a way to reclaim large regions from the bitmaps. 
	
	  If we are below the extents threshold then we can add this as an
	  extent, and don't have to deal with the bitmap
		
		  If this block group has some small extents we don't want to
		  use up all of our free slots in the cache with them, we want
		  to reserve them to larger extents, however if we have plenty
		  of cache left then go ahead an dadd them, no sense in adding
		  the overhead of a bitmap if we don't have to.
	
	  The original block groups from mkfs can be really small, like 8
	  megabytes, so don't bother with a bitmap for those entries.  However
	  some block groups can be smaller than what a bitmap would cover but
	  are still large enough that they could overflow the 32k memory limit,
	  so allow those block groups to still be allowed to have a bitmap
	  entry.
	
	  Since we link bitmaps right into the cluster we need to see if we
	  have a cluster here, and if so and it has our bitmap we need to add
	  the free space to that bitmap.
 no pre-allocated info, allocate a new one 
 allocate the bitmap 
  Free space merging rules:
   1) Merge trimmed areas together
   2) Let untrimmed areas coalesce with trimmed areas
   3) Always pull neighboring regions from bitmaps
  The above rules are for when we merge free space based on btrfs_trim_state.
  Rules 2 and 3 are subtle because they are suboptimal, but are done for the
  same reason: to promote larger extent regions which makes life easier for
  find_free_extent().  Rule 2 enables coalescing based on the common path
  being returning free space from btrfs_finish_extent_commit().  So when free
  space is trimmed, it will prevent aggregating trimmed new region and
  untrimmed regions in the rb_tree.  Rule 3 is purely to obtain larger extents
  and provide find_free_extent() with the largest extents possible hoping for
  the reuse path.
	
	  first we want to see if there is free space adjacent to the range we
	  are adding, if there is remove that struct and add a new one to
	  cover the entire range
 See try_merge_free_space() comment. 
 See try_merge_free_space() comment. 
 See try_merge_free_space() comment. 
 If we're on a boundary, try the previous logical bitmap. 
 See try_merge_free_space() comment. 
  We prefer always to allocate from extent entries, both for clustered and
  non-clustered allocation requests. So when attempting to add a new extent
  entry, try to see if there's adjacent free space in bitmap entries, and if
  there is, migrate that space from the bitmaps to the extent.
  Like this we get better chances of satisfying space allocation requests
  because we attempt to satisfy them based on a single cache entry, and never
  on 2 or more entries - even if the entries represent a contiguous free space
  region (e.g. 1 extent entry + 1 bitmap entry starting where the extent entry
  ends).
	
	  Only work with disconnected entries, as we can change their offset,
	  and must be extent entries.
	
	  There was no extent directly to the left or right of this new
	  extent then we know we're going to have to allocate a new extent, so
	  before we do that see if we need to drop this into a bitmap
	
	  Only steal free space from adjacent bitmaps if we're sure we're not
	  going to add the new free space to existing bitmap entries - because
	  that would mean unnecessary work that would be reverted. Therefore
	  attempt to steal space from bitmaps if we're adding an extent entry.
	
	  If the block group is read-only, we should account freed space into
	  bytes_readonly.
 All the region is now unusable. Mark it as unused and reclaim 
  This is a subtle distinction because when adding free space back in general,
  we want it to be added as untrimmed for async. But in the case where we add
  it on loading of a block group, we want to consider it trimmed.
		
		  This can happen with conventional zones when replaying log.
		  Since the allocation info of tree-log nodes are not recorded
		  to the extent-tree, calculate_alloc_pointer() failed to
		  advance the allocation pointer after last allocated tree log
		  node blocks.
		 
		  This function is called from
		  btrfs_pin_extent_for_log_replay() when replaying the log.
		  Advance the pointer not to overwrite the tree-log nodes.
		
		  oops didn't find an extent that matched the space we wanted
		  to remove, look for a bitmap instead
			
			  If we found a partial bit of our free space in a
			  bitmap but then couldn't find the other part this may
			  be a problem, so WARN about it.
 Not enough bytes in this entry to satisfy us 
 all done 
	
	  Zoned btrfs does not use free space tree and cluster. Just print
	  out the free space after the allocation offset.
	
	  we only want to have 32k of ram per block group for keeping
	  track of free space, and if we pass 12 of that we want to
	  start converting things over to using bitmaps
  for a given cluster, put all of its extents back into the free
  space cache.  If the block group passed doesn't match the block group
  pointed to by the cluster, someone else raced in and freed the
  cluster already.  In that case, we just return without changing anything
 Merging treats extents as if they were new 
 As we insert directly, update these statistics 
  btrfs_is_free_space_trimmed - see if everything is trimmed
  @block_group: block_group of interest
  Walk @block_group's free space rb_tree to determine if everything is trimmed.
  given a cluster, put all of its extents back into the free space
  cache.  If a block group is passed, this function will only free
  a cluster that belongs to the passed block group.
  Otherwise, it'll get a reference on the block group pointed to by the
  cluster and remove the cluster from it.
 first, get a safe pointer to the block group 
 someone else has already freed it don't redo their work 
 now return any extents the cluster had on it 
 finally drop our ref 
  given a cluster, try to allocate 'bytes' from it, returns 0
  if it couldn't find anything suitably large, or a logical disk offset
  if things worked out
	
	  Don't bother looking for a cluster in this bitmap if it's heavily
	  fragmented.
 -EEXIST; Logic error 
  This searches the block group for just extents to fill the cluster with.
  Try to find a cluster with at least bytes total bytes, at least one
  extent of cont1_bytes, and other clusters of at least min_bytes.
	
	  We don't want bitmaps, so just move along until we find a normal
	  extent entry.
	
	  now we've found our entries, pull them out of the free space
	  cache and put them into the cluster rbtree
 -EEXIST; Logic error 
  This specifically looks for bitmaps that may work in the cluster, we assume
  that we have already failed to find extents that will work.
	
	  The bitmap that covers offset won't be in the list unless offset
	  is just its start offset.
	
	  The bitmaps list has all the bitmaps that record free space
	  starting after offset, so no more search is required.
  here we try to find a cluster of blocks in a block group.  The goal
  is to find at least bytes+empty_size.
  We might not find them all in one contiguous area.
  returns zero and sets up cluster if things worked out, otherwise
  it returns -enospc
	
	  Choose the minimum extent size we'll require for this
	  cluster.  For SSD_SPREAD, don't allow any fragmentation.
	  For metadata, allow allocates with smaller extents.  For
	  data, keep it dense.
	
	  If we know we don't have enough space to make a cluster don't even
	  bother doing all the work to try and find one.
 someone already found a cluster, hooray 
 Clear our temporary list 
  simple code to zero out a cluster
  If @async is set, then we will trim 1 region and return.
 Skip bitmaps and if async, already trimmed entries 
			
			  Let bytes = BTRFS_MAX_DISCARD_SIZE + X.
			  If X < BTRFS_ASYNC_DISCARD_MIN_FILTER, we won't trim
			  X when we come back around.  So trim it now.
  If we break out of trimming a bitmap prematurely, we should reset the
  trimming bit.  In a rather contrieved case, it's possible to race here so
  reset the state to BTRFS_TRIM_STATE_UNTRIMMED.
  start = start of bitmap
  end = near end of bitmap
  Thread 1:			Thread 2:
  trim_bitmaps(start)
 				trim_bitmaps(end)
 				end_trimming_bitmap()
  reset_trimming_bitmap()
  If @async is set, then we will trim 1 region and return.
		
		  Bitmaps are marked trimmed lossily now to prevent constant
		  discarding of the same bitmap (the reason why we are bound
		  by the filters).  So, retrim the block group bitmaps when we
		  are preparing to punt to the unused_bgs list.  This uses
		  @minlen to determine if we are in BTRFS_DISCARD_INDEX_UNUSED
		  which is the only discard index which sets minlen to 0.
		
		  Async discard bitmap trimming begins at by setting the start
		  to be key.objectid and the offset_to_bitmap() aligns to the
		  start of the bitmap.  This lets us know we are fully
		  scanning the bitmap rather than only some portion of it.
			
			  We lossily consider a bitmap trimmed if we only skip
			  over regions <= BTRFS_ASYNC_DISCARD_MIN_FILTER.
		
		  We already trimmed a region, but are using the locking above
		  to reset the trim_state.
		
		  Let bytes = BTRFS_MAX_DISCARD_SIZE + X.
		  If X < @minlen, we won't trim X when we come back around.
		  So trim it now.  We differ here from trimming extents as we
		  don't keep individual state per bit.
 If we ended in the middle of a bitmap, reset the trimming flag 
	
	  update_super_roots will appropriately set or unset
	  super_copy->cache_generation based on SPACE_CACHE and
	  BTRFS_FS_CLEANUP_SPACE_CACHE_V1. For this reason, we need a
	  transaction commit whether we are enabling space cache v1 and don't
	  have any other work to do, or are disabling it and removing free
	  space inodes.
  Use this if you need to make a bitmap or extent entry specifically, it
  doesn't do any of the merging that add_free_space does, this acts a lot like
  how the free space cache loading stuff works, so you can get really weird
  configurations.
  Checks to see if the given range is in the free space cache.  This is really
  just used to check the absence of space, so if there is free space in the
  range at all we will return 1.
 CONFIG_BTRFS_FS_RUN_SANITY_TESTS 
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
			
			  offset is supposed to be a tree block which
			  must be aligned to nodesize.
			
			  offset is supposed to be a tree block which
			  must be aligned to nodesize.
  Helper to output refs and locking status of extent buffer.  Useful to debug
  race condition related problems.
 SPDX-License-Identifier: GPL-2.0
	
	  We round up to the block size at eof when determining which
	  extents to clone above, but shouldn't round up the file size.
	
	  We have flushed and locked the ranges of the source and destination
	  inodes, we also have locked the inodes, so we are safe to do a
	  reservation here. Also we must not do the reservation while holding
	  a transaction open, otherwise we would deadlock.
	
	  After dirtying the page our caller will need to start a transaction,
	  and if we are low on metadata free space, that can cause flushing of
	  delalloc for all inodes in order to get metadata space released.
	  However we are holding the range locked for the whole duration of
	  the clonededupe operation, so we may deadlock if that happens and no
	  other task releases enough space. So mark this inode as not being
	  possible to flush to avoid such deadlock. We will clear that flag
	  when we finish cloning all extents, since a transaction is started
	  after finding each extent to clone.
	
	  If our inline data is smaller then the blockpage size, then the
	  remaining of the blockpage is equivalent to zeroes. We had something
	  like the following done:
	 
	  $ xfs_io -f -c "pwrite -S 0xab 0 500" file
	  $ sync  # (or fsync)
	  $ xfs_io -c "falloc 0 4K" file
	  $ xfs_io -c "pwrite -S 0xcd 4K 4K"
	 
	  So what's in the range [500, 4095] corresponds to zeroes.
  Deal with cloning of inline extents. We try to copy the inline extent from
  the source inode to destination inode when possible. When not possible we
  copy the inline extent's data into the respective page of the inode.
			
			  There's an implicit hole at file offset 0, copy the
			  inline extent's data to the page.
		
		  If it's an inline extent replace it with the source inline
		  extent, otherwise copy the source inline extent data into
		  the respective page at the destination inode.
	
	  We have no extent items, or we have an extent at offset 0 which may
	  or may not be inlined. All these cases are dealt the same way.
		
		  At the destination offset 0 we have either a hole, a regular
		  extent or an inline extent larger then the one we want to
		  clone. Deal with all these cases by copying the inline extent
		  data into the respective page at the destination inode.
	
	  Release path before starting a new transaction so we don't hold locks
	  that would confuse lockdep.
	
	  If we end up here it means were copy the inline extent into a leaf
	  of the destination inode. We know we will drop or adjust at most one
	  extent item in the destination root.
	 
	  1 unit - adjusting old extent (we may have to split it)
	  1 unit - add new extent
	  1 unit - inode update
		
		  No transaction here means we copied the inline extent into a
		  page of the destination inode.
		 
		  1 unit to update inode item
	
	  Release our path because we don't need it anymore and also because
	  copy_inline_to_page() needs to reserve data and metadata, which may
	  need to flush delalloc when we are low on available space and
	  therefore cause a deadlock if writeback of an inline extent needs to
	  write to the same leaf or an ordered extent completion needs to write
	  to the same leaf.
  btrfs_clone() - clone a range from inode file to another
  @src: Inode to clone from
  @inode: Inode to clone to
  @off: Offset within source to start clone from
  @olen: Original length, passed by user, of range to clone
  @olen_aligned: Block-aligned value of olen
  @destoff: Offset within @inode to start clone
  @no_time_update: Whether to update mtimectime on the target inode
 Clone data 
 Note the key will change type as we walk through the tree 
		
		  First search, if no extent item that starts at offset off was
		  found but the previous item is an extent item, it's possible
		  it might overlap our target range, therefore process it.
 Take upper bound, may be compressed 
		
		  The first search might have left us at an extent item that
		  ends before our target range's start, can happen if we have
		  holes and NO_HOLES feature enabled.
		
		  Deal with a hole that doesn't have an extent item that
		  represents it (NO_HOLES feature enabled).
		  This hole is either in the middle of the cloning range or at
		  the beginning (fully overlaps it or partially overlaps it).
			
			     a  | --- range to clone ---|  b
			  | ------------- extent ------------- |
 Subtract range b 
 Subtract range a 
			
			  Inline extents always have to start at file offset 0
			  and can never be bigger then the sector size. We can
			  never clone only parts of an inline extent, since all
			  reflink operations must start at a sector size aligned
			  offset, and the length must be aligned too or end at
			  the i_size (which implies the whole inlined data).
		
		  If this is a new extent update the last_reflink_trans of both
		  inodes. This is used by fsync to make sure it does not log
		  multiple checksum items with overlapping ranges. For older
		  extents we don't need to do it since inode logging skips the
		  checksums for older extents. Also ignore holes and inline
		  extents because they don't have checksums in the csum tree.
		
		  We have an implicit hole that fully or partially overlaps our
		  cloning range at its end. This means that we either have the
		  NO_HOLES feature enabled or the implicit hole happened due to
		  mixing buffered and direct IO writes against this file.
		
		  When using NO_HOLES and we are cloning a range that covers
		  only a hole (no extents) into a range beyond the current
		  i_size, punching a hole in the target range will not create
		  an extent map defining a hole, because the range starts at or
		  beyond current i_size. If the file previously had an i_size
		  greater than the new i_size set by this clone operation, we
		  need to make sure the next fsync is a full fsync, so that it
		  detects and logs a hole covering a range from the current
		  i_size to the new i_size. If the clone range covers extents,
		  besides a hole, then we know the full sync flag was already
		  set by previous calls to btrfs_replace_file_extents() that
		  replaced file extent items.
	
	  Lock destination range to serialize with concurrent readpages() and
	  source range to serialize with relocation.
	
	  VFS's generic_remap_file_range_prep() protects us from cloning the
	  eof block into the middle of a file, which would result in corruption
	  if the file size is not blocksize aligned. So we don't need to check
	  for that case here.
		
		  We may have truncated the last block if the inode's size is
		  not sector size aligned, so we need to wait for writeback to
		  complete before proceeding further, otherwise we can race
		  with cloning and attempt to increment a reference to an
		  extent that no longer exists (writeback completed right after
		  we found the previous extent covering eof and before we
		  attempted to increment its reference count).
	
	  Lock destination range to serialize with concurrent readpages() and
	  source range to serialize with relocation.
	
	  We may have copied an inline extent into a page of the destination
	  range, so wait for writeback to complete before truncating pages
	  from the page cache. This is a rare case.
	
	  Truncate page cache pages so that future reads will see the cloned
	  data immediately and not the previous data.
 Don't make the dst file partly checksummed 
	
	  Now that the inodes are locked, we need to start writeback ourselves
	  and can not rely on the writeback from the VFS's generic helper
	  generic_remap_file_range_prep() because:
	 
	  1) For compression we must call filemap_fdatawrite_range() range
	     twice (btrfs_fdatawrite_range() does it for us), and the generic
	     helper only calls it once;
	 
	  2) filemap_fdatawrite_range(), called by the generic helper only
	     waits for the writeback to complete, i.e. for IO to be done, and
	     not for the ordered extents to complete. We need to wait for them
	     to complete so that new file extent items are in the fs tree.
	
	  Since we don't lock ranges, wait for ongoing lockless dio writes (as
	  any in progress could create its ordered extents after we wait for
	  existing ordered extents below).
	
	  Workaround to make sure NOCOW buffered write reach disk as NOCOW.
	 
	  Btrfs' back references do not have a block level granularity, they
	  work at the whole extent level.
	  NOCOW buffered write without data space reserved may not be able
	  to fall back to CoW due to lack of data space, thus could cause
	  data loss.
	 
	  Here we take a shortcut by flushing the whole inode, so that all
	  nocow write should reach disk as nocow before we increase the
	  reference of the extent. We could do better by only flushing NOCOW
	  data, but that needs extra accounting.
	 
	  Also we don't need to check ASYNC_EXTENT, as async extent will be
	  CoWed anyway, not affecting nocow part.
	
	  If either the source or the destination file was opened with O_SYNC,
	  O_DSYNC or has the S_SYNC attribute, fsync both the destination and
	  source filesranges, so that after a successful return (0) followed
	  by a power failure results in the reflinked data to be readable from
	  both filesranges.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2007 Oracle.  All rights reserved.
  btrfs_end_io_wq structs are used to do processing in task context when an IO
  is complete.  This is used during reads to verify checksums, and it is used
  by writes to insert metadata for new file extents after IO is complete.
  async submit bios are used to offload expensive checksumming
  onto the worker threads.  They checksum file and metadata bios
  just before they are sent down the IO stack.
 Optional parameter for submit_bio_start used by direct io 
  Lockdep class keys for extent_buffer->lock's in this root.  For a given
  eb, the lockdep key is determined by the btrfs_root it belongs to and
  the level the eb occupies in the tree.
  Different roots are used for different purposes and may nest inside each
  other and they require separate keysets.  As lockdep keys should be
  static, assign keysets according to the purpose of the root as indicated
  by btrfs_root->root_key.objectid.  This ensures that all special purpose
  roots have separate keysets.
  Lock-nesting across peer nodes is always done with the immediate parent
  node locked thus preventing deadlock.  As lockdep doesn't know this, use
  subclass to avoid triggering lockdep warning in such cases.
  The key is set by the readpage_end_io_hook after the buffer has passed
  csum validation but before the pages are unlocked.  It is also set by
  btrfs_init_new_buffer on freshly allocated blocks.
  We also add a check to make sure the highest level of the tree is the
  same as our lockdep setup here.  If BTRFS_MAX_LEVEL changes, this code
  needs update as well.
 root objectid 
 Longest entry: btrfs-free-space-00 
 find the matching keyset, id 0 is the default entry 
  Compute the csum of a btree block and store the result to provided buffer.
  we can't consider a given block up to date unless the transid of the
  block matches the transid in the parent node's pointer.  This is how we
  detect blocks that either didn't get written at all or got written
  in the wrong place.
  Return 0 if the superblock checksum type matches the checksum value of that
  algorithm. Pass the raw disk superblock data.
	
	  The super_block structure does not span the whole
	  BTRFS_SUPER_INFO_SIZE range, we expect that the unused space is
	  filled with zeros and is included in the checksum.
	
	  For live tree block (new tree blocks in current transaction),
	  we need proper lock context to avoid race, which is impossible here.
	  So we only checks tree blocks which is read from disk, whose
	  generation <= fs_info->last_trans_committed.
 We have @first_key, so this @eb must have at least one item 
  helper to read a given tree block, doing retries as required when
  the checksums don't match and we have alternate mirrors to try.
  @parent_transid:	expected transid, skip check if 0
  @level:		expected level, mandatory check
  @first_key:		expected key of first slot, skip check if NULL
 Checksum all dirty extent buffers in one bio_vec 
 A dirty eb shouldn't disappear from buffer_radix 
  Checksum a dirty tree block before IO.  This has extra checks to make sure
  we only fill in the checksum field in the first page of a multi-page block.
  For subpage extent buffers we need bvec to also read the offset in the page.
	
	  Please do not consolidate these warnings into a single if.
	  It is useful to know what went wrong.
	
	  Checking the incompat flag is only valid for the current fs. For
	  seed devices it's forbidden to have their uuid changed so reading
	  ->fsid in this case is fine
 Do basic extent buffer checks at read time 
	
	  If this is a leaf block and it is corrupt, set the corrupt bit so
	  that we don't try and read the other copies of this block, just
	  return -EIO.
	
	  We don't allow bio merge for subpage metadata read, so we should
	  only get one eb for each endio hook.
	
	  When we are reading one tree block, eb must have been inserted into
	  the radix tree. If not, something is wrong.
 Subpage read must finish in page read 
	
	  end_bio_extent_readpage decrements io_pages in case of error,
	  make sure it has something to decrement.
	
	  The pending IO might have been the only thing that kept this buffer
	  in memory.  Make sure we have a ref for all this other checks
		
		  our io error hook is going to dec the io pages
		  again, we have to make sure it has something
		  to decrement
  In order to insert checksums into the metadata in large chunks, we wait
  until bio submission time.   All the pages in the bio are checksummed and
  sums are attached onto the ordered extent record.
  At IO completion time the csums attached on the ordered extent record are
  inserted into the tree.
 If an error occurred we just want to clean up the bio and move on 
	
	  All of the bios that pass through here are from async helpers.
	  Use REQ_CGROUP_PUNT to issue them from the owning cgroup's context.
	  This changes nothing when cgroups aren't in use.
	
	  when we're called for a write, we're already in the async
	  submission context.  Just jump into btrfs_map_bio
		
		  called for a read, do the setup so that checksum validation
		  can happen in the async kernel threads
		
		  kthread helpers are used to submit writes so that
		  checksumming can happen in parallel across all CPUs
	
	  we can't safely write a btree page from here,
	  we haven't done the locking hook
	
	  Buffers may be managed in a filesystem specific way.
	  We must have no buffers or drop them.
 this is a bit racy, but that's ok 
  Read tree block at logical address @bytenr and do variant basic but critical
  verification.
  @owner_root:		the objectid of the root owner for this block.
  @parent_transid:	expected transid of this tree block, skip check if 0
  @level:		expected level, mandatory check
  @first_key:		expected key in slot 0, skip check if NULL
 Should only be used by the testing infrastructure 
 We don't use the stripesize in selftest, set it as sectorsize 
	
	  We're holding a transaction handle, so use a NOFS memory allocation
	  context to avoid deadlock if reclaim happens.
	
	  DON'T set SHAREABLE bit for log trees.
	 
	  Log trees are not exposed to user space thus can't be snapshotted,
	  and they go away before a real commit is actually done.
	 
	  They do store pointers to file data extents, and those reference
	  counts still get updated (along with back refs to the log tree).
  Initialize subvolume root in-memory structure
  @anon_dev:	anonymous device to attach to the root, if zero, allocate new
	
	  We might be called under a transaction (e.g. indirect backref
	  resolution) which could deadlock if it triggers memory reclaim
	
	  Don't assign anonymous block device to roots that are not exposed to
	  userspace, the id pool is limited to 1M
 The caller is responsible to call btrfs_free_fs_root 
  Get an in-memory reference of a root structure.
  For essential trees like rootextent tree, we grab it from fs_info directly.
  For subvolume trees, we check the cached filesystem roots first. If not
  found, then read it from disk and add it to cached fs roots.
  Caller should release the root by calling btrfs_put_root() after the usage.
  NOTE: Reloc and log trees can't be read by this function as they share the
 	 same root objectid.
  @objectid:	root id
  @anon_dev:	preallocated anonymous block device number for new roots,
  		pass 0 for new allocation.
  @check_ref:	whether to check root item references, If true, return -ENOENT
 		for orphan roots
 Shouldn't get preallocated anon_dev for cached roots 
  Get in-memory reference of a root structure
  @objectid:	tree objectid
  @check_ref:	if set, verify that the tree exists and the item has at least
 		one reference
  Get in-memory reference of a root structure, created as new, optionally pass
  the anonymous block device id
  @objectid:	tree objectid
  @anon_dev:	if zero, allocate a new anonymous block device or use the
 		parameter value
  btrfs_get_fs_root_commit_root - return a root for the given objectid
  @fs_info:	the fs_info
  @objectid:	the objectid we need to lookup
  This is exclusively used for backref walking, and exists specifically because
  of how qgroups does lookups.  Qgroups will do a backref lookup at delayed ref
  creation time, which means we may have to read the tree_root in order to look
  up a fs root that is not in memory.  If the root is not in memory we will
  read the tree root commit root and look up the fs root from there.  This is a
  temporary root, it will not be inserted into the radix tree as it doesn't
  have the most uptodate information, it'll simply be discarded once the
  backref code is finished using the root.
	
	  This can return -ENOENT if we ask for a root that doesn't exist, but
	  since this is called via the backref walking code we won't be looking
	  up a root that doesn't exist, unless there's corruption.  So if root
	  != NULL just return it.
  called by the kthread helper functions to finally call the bio end_io
  functions.  This is where read checksum verification actually happens
 Make the cleaner go to sleep early. 
		
		  Do not do anything if we might cause open_ctree() to block
		  before we have finished mounting the filesystem.
		
		  Avoid the problem that we change the status of the fs
		  during the above check and trylock.
		
		  The defragger has dealt with the RO remount and umount,
		  needn't do anything special here.
		
		  Acquires fs_info->reclaim_bgs_lock to avoid racing
		  with relocation (btrfs_relocate_chunk) and relocation
		  acquires fs_info->cleaner_mutex (btrfs_relocate_block_group)
		  after acquiring fs_info->reclaim_bgs_lock. So we
		  can't hold, nor need to, fs_info->cleaner_mutex when deleting
		  unused block groups.
		
		  Reclaim block groups in the reclaim_bgs list after we deleted
		  all unused block_groups. This possibly gives us some more free
		  space.
 If the file system is aborted, this will always fail. 
  This will find the highest generation in the array of root backups.  The
  index of the highest array is returned, or -EINVAL if we can't find
  anything.
  We check to make sure the array is valid by comparing the
  generation of the latest  root in the array with the generation
  in the super block.  If they don't match we pitch it.
  copy all the root pointers into the super backup array.
  this will bump the backup pointer by one when it is
  done
	
	  make sure all of our padding and empty slots get zero filled
	  regardless of which ones we use today
	
	  we might commit during log recovery, which happens before we set
	  the fs_root.  Make sure it is valid before we fill it in.
	
	  if we don't copy this out to the super_copy, it won't get remembered
	  for the next commit
  read_backup_root - Reads a backup root based on the passed priority. Prio 0
  is the newest, prio 123 are 2nd newest3rd newest4th (oldest) backup roots
  fs_info - filesystem whose backup roots need to be read
  priority - priority of backup root required
  Returns backup root index on success and -EINVAL otherwise.
	
	  Fixme: the total bytes and num_devices need to match or we should
	  need a fsck
 helper to cleanup workers 
	
	  Now that all other work queues are destroyed, we can safely destroy
	  the queues used for metadata IO, since tasks from those other work
	  queues can do metadata IO operations.
 helper to cleanup tree roots 
	
	  we set the i_size on the btree inode to the max possible int.
	  the real end of the address space is determined by all of
	  the devices in the system
	
	  endios are largely parallel and should have a very
	  low idle thresh
 returns with log_tree_root freed on success 
 Initialize fs_info for all devices in any case 
 If IGNOREDATACSUMS is set don't bother reading the csum root. 
	
	  This tree can share blocks with some other fs tree during relocation
	  and we need a proper setup by btrfs_get_fs_root
  Real super block validation
  NOTE: super csum type and incompat features will not be checked here.
  @sb:		super block to check
  @mirror_num:	the super block number to check its bytenr:
  		0	the primary (1st) sb
  		1, 2	2nd and 3rd backup copy
  	       -1	skip bytenr check
	
	  Check sectorsize and nodesize first, other check will need it.
	  Check all possible sectorsize(4K, 8K, 16K, 32K, 64K) here.
	
	  For 4K page size, we only support 4K sector size.
	  For 64K page size, we support 64K and 4K sector sizes.
 Root alignment check 
	
	  Hint to catch really bogus numbers, bitflips or so, more exact checks are
	  done later
	
	  Obvious sys_chunk_array corruptions, it must hold at least one key
	  and one chunk
	
	  The generation is a global counter, we'll trust it more than the others
	  but it's still possible that it's the one that's wrong.
  Validation of super block at mount time.
  Some checks already done early at mount time, like csum type and incompat
  flags will be skipped.
  Validation of super block at write time.
  Some checks like bytenr check will be skipped as their values will be
  overwritten soon.
  Extra checks like csum type and incompat flags will be done here.
			
			  Don't use the log in recovery mode, it won't be
			  valid
 We can't trust the free space cache either 
		
		  No need to hold btrfs_root::objectid_mutex since the fs
		  hasn't been fully initialised and we are the only user
 All successful 
 Always begin writing backup roots after the one being used 
 div by 64 
 readahead state 
 Usable values until the real ones are cached from the superblock 
	
	  1st step is to iterate through the existing UUID tree and
	  to delete all entries that contain outdated data.
	  2nd step is to add all missing entries to the UUID tree.
 fs_info->update_uuid_tree_gen remains 0 in all error case 
  Some options only have meaning at mount time and shouldn't persist across
  remounts, or be displayed. Clear these at the end of mount and remount
  code paths.
  Mounting logic specific to read-write file systems. Shared by open_ctree
  and btrfs_remount when remounting from read-only to read-write.
	
	  btrfs_find_orphan_roots() is responsible for finding all the dead
	  roots (with 0 refs), flag them with BTRFS_ROOT_DEAD_TREE and load
	  them into the fs_info->fs_roots_radix tree. This must be done before
	  calling btrfs_orphan_cleanup() on the tree root. If we don't do it
	  first, then btrfs_orphan_cleanup() will delete a dead root's orphan
	  item before the root's tree is deleted - this means that if we unmount
	  or crash before the deletion completes, on the next mount we will not
	  delete what remains of the tree because the orphan item does not
	  exists anymore, which is what tells us we have a pending deletion.
 These need to be init'ed before we start creating inodes and such. 
	
	  Read super block and check the signature bytes only
	
	  Verify the type first, if that or the checksum value are
	  corrupted, we'll find out
	
	  We want to check superblock checksum, the type is stored inside.
	  Pass the whole disk block of size BTRFS_SUPER_INFO_SIZE (4k).
	
	  super_copy is zeroed at allocation time and we never touch the
	  following bytes up to INFO_SIZE, the checksum is calculated from
	  the whole block of INFO_SIZE
 check FS state, whether FS is broken. 
	
	  In the long term, we'll store the compression type in the super
	  block, and it'll be used for per file compression control.
	
	  Flag our filesystem as having big metadata blocks if they are bigger
	  than the page size.
 Set up fs_info before parsing mount options 
	
	  mixed block groups end up with duplicate but slightly offset
	  extent buffers for the same range.  It leads to corruptions
	
	  Needn't use the lock because there is no other task which will
	  update the flag.
	
	  At this point we know all the devices that make this filesystem,
	  including the seed devices but we don't know yet if the replace
	  target is required. So free devices that are not part of this
	  filesystem but skip the replace target device which is checked
	  below in btrfs_init_dev_replace().
	
	  Get zone type information of zoned block devices. This will also
	  handle emulation of a zoned filesystem if a regular device has the
	  zoned incompat feature flag set.
	
	  If we have a uuid root and we're not being told to rescan we need to
	  check the generation here so we can set the
	  BTRFS_FS_UPDATE_UUID_TREE_GEN bit.  Otherwise we could commit the
	  transaction during a balance or the log replay without updating the
	  uuid generation, and then if we crash we would rescan the uuid tree,
	  even though it was perfectly fine.
	
	  Mount does not set all options immediately, we can do it now and do
	  not have to wait for transaction commit
 do not make disk changes in broken FS or nologreplay is given 
	
	  make sure we're done with the btree inode before we stop our
	  kthreads
	 we would like to check all the supers, but that would make
	  a btrfs mount succeed after a mkfs from a different FS.
	  So, we need to add a special mount option to scan for
	  later supers, using BTRFS_SUPER_MIRROR_MAX instead
  Write superblock @sb to the @device. Do not wait for completion, all the
  pages we use for writing are locked.
  Write @max_mirrors copies of the superblock, where 0 means default that fit
  the expected device size at commit time. Note that max_mirrors must be
  same for write and wait phases.
  Return number of errors when page is not found or submission fails.
 Bump the refcount for wait_dev_supers() 
		
		  Directly use bios here instead of relying on the page cache
		  to do IO, so we don't lose the ability to do integrity
		  checking.
		
		  We FUA only the first super block.  The others we allow to
		  go down lazy and there's a short window where the on-disk
		  copies might still contain the older version.
  Wait for write completion of superblocks done by write_dev_supers,
  @max_mirrors same for write and wait phases.
  Return number of errors when page is not found or not marked up to
  date.
 Page is submitted locked and unlocked once the IO completes 
 Drop our reference 
 Drop the reference from the writing run 
 log error, force error return 
  endio for the write_dev_flush, this will wake anyone waiting
  for the barrier when it is done
  Submit a flush request to the device if it supports it. Error handling is
  done in the waiting counterpart.
	
	  When a disk has write caching disabled, we skip submission of a bio
	  with flush and sync requests before writing the superblock, since
	  it's not needed. However when the integrity checker is enabled, this
	  results in reports that there are metadata blocks referred by a
	  superblock that were not properly flushed. So don't skip the bio
	  submission only when the integrity checker is enabled for the sake
	  of simplicity, since this is a debug tool and not meant for use in
	  non-debug builds.
  If the flush bio has been submitted by write_dev_flush, wait for it.
  send an empty flush down to each device in parallel,
  then wait for them
 send down all the barriers 
 wait for all the barriers 
		
		  At some point we need the status of all disks
		  to arrive at the volume status. So error checking
		  is being pushed to a separate loop.
	
	  max_mirrors == 0 indicates we're from commit_transaction,
	  not from fsync where the tree roots in fs_info have not
	  been consistent on disk.
 FUA is masked off if unsupported and can't be the reason 
 Drop a fs root from the radix tree and free it. 
 Avoid to grab roots in dead_roots 
 grab all the search result for later use 
 release the uncleaned roots due to error 
 wait until ongoing cleanup work done 
	
	  We don't want the cleaner to start new transactions, add more delayed
	  iputs, etc. while we're closing. We can't use kthread_stop() yet
	  because that frees the task_struct, and the transaction kthread might
	  still try to wake up the cleaner.
 wait for the qgroup rescan worker to stop 
 wait for the uuid_scan task to finish 
 avoid complains from lockdep et al., set sem back to initial state 
 pause restriper - we want to resume on mount 
 wait for any defraggers to finish 
 clear out the rbtree of defraggable inodes 
 Cancel or finish ongoing discard work 
		
		  The cleaner kthread is stopped, so do one final pass over
		  unused block groups.
		
		  There might be existing delayed inode workers still running
		  and holding an empty delayed inode item. We must wait for
		  them to complete first because they can create a transaction.
		  This happens when someone calls btrfs_balance_delayed_items()
		  and then a transaction commit runs the same delayed nodes
		  before any delayed worker has done something with the nodes.
		  We must wait for any worker here and not at transaction
		  commit time since that could cause a deadlock.
		  This is a very rare case.
	
	  we must make sure there is not any read request to
	  submit after we stopping all workers.
 We shouldn't have any transaction open at this point 
	
	  We must free the block groups after dropping the fs_roots as we could
	  have had an IO error and have left over tree log blocks that aren't
	  cleaned up until the fs roots are freed.  This makes the block group
	  accounting appear to be wrong because there's pending reserved bytes,
	  so make sure we do the block group cleanup afterwards.
	
	  This is a fast path so only do this check if we have sanity tests
	  enabled.  Normal people shouldn't be using unmapped buffers as dirty
	  outside of the sanity tests.
	
	  Since btrfs_mark_buffer_dirty() can be called with item pointer set
	  but item data not updated.
	  So here we should only check item pointers, not item data.
	
	  looks as though older kernels can get into trouble with
	  this code, they end up stuck in balance_dirty_pages forever
 cleanup FS via transaction 
	
	  This will just short circuit the ordered completion stuff which will
	  make sure the ordered extent gets properly cleaned up.
	
	  We need this here because if we've been flipped read-only we won't
	  get sync() from the umount, so we need to make sure any ordered
	  extents that haven't had their dirty pages IO start writeout yet
	  actually get run and error out properly.
		
		  Make sure we get a live inode and that it'll not disappear
		  meanwhile.
		
		  The btrfs_finish_extent_commit() may get the same range as
		  ours between find_first_extent_bit and clear_extent_dirty.
		  Hence, hold the unused_bg_unpin_mutex to avoid double unpin
		  the same extent range.
	
	  Refer to the definition of io_bgs member for details why it's safe
	  to use it without any locking
			
			  We wait for 0 num_writers since we don't hold a trans
			  handle open currently for this transaction.
 Corruption 
 SPDX-License-Identifier: GPL-2.0
  This contains the logic to handle async discard.
  Async discard manages trimming of free space outside of transaction commit.
  Discarding is done by managing the block_groups on a LRU list based on free
  space recency.  Two passes are used to first prioritize discarding extents
  and then allow for trimming in the bitmap the best opportunity to coalesce.
  The block_groups are maintained on multiple lists to allow for multiple
  passes with different discard filter requirements.  A delayed work item is
  used to manage discarding with timeout determined by a max of the delay
  incurred by the iops rate limit, the byte rate limit, and the max delay of
  BTRFS_DISCARD_MAX_DELAY.
  Note, this only keeps track of block_groups that are explicitly for data.
  Mixed block_groups are not supported.
  The first list is special to manage discarding of fully free block groups.
  This is necessary because we issue a final trim for a full free block group
  after forgetting it.  When a block group becomes unused, instead of directly
  being added to the unused_bgs list, we add it to this first list.  Then
  from there, if it becomes fully discarded, we place it onto the unused_bgs
  list.
  The in-memory free space cache serves as the backing state for discard.
  Consequently this means there is no persistence.  We opt to load all the
  block groups in as not discarded, so the mount case degenerates to the
  crashing case.
  As the free space cache uses bitmaps, there exists a tradeoff between
  easeefficiency for find_free_extent() and the accuracy of discard state.
  Here we opt to let untrimmed regions merge with everything while only letting
  trimmed regions merge with other trimmed regions.  This can cause
  overtrimming, but the coalescing benefit seems to be worth it.  Additionally,
  bitmap state is tracked as a whole.  If we're able to fully trim a bitmap,
  the trimmed flag is set on the bitmap.  Otherwise, if an allocation comes in,
  this resets the state and we will retry trimming the whole bitmap.  This is a
  tradeoff between discard state accuracy and the cost of accounting.
 This is an initial delay to give some chance for block reuse 
 Target completion latency of discarding all discardable extents 
 Montonically decreasing minimum length filters after index 0 
  find_next_block_group - find block_group that's up next for discarding
  @discard_ctl: discard control
  @now: current time
  Iterate over the discard lists to find the next block_group up for
  discarding checking the discard_eligible_time of block_group.
  Wrap find_next_block_group()
  @discard_ctl:   discard control
  @discard_state: the discard_state of the block_group after state management
  @discard_index: the discard_index of the block_group after state management
  @now:           time when discard was invoked, in ns
  This wraps find_next_block_group() and sets the block_group to be in use.
  discard_state's control flow is managed here.  Variables related to
  discard_state are reset here as needed (eg discard_cursor).  @discard_state
  and @discard_index are remembered as it may change while we're discarding,
  but we want the discard to execute in the context determined here.
  btrfs_discard_check_filter - updates a block groups filters
  @block_group: block group of interest
  @bytes: recently freed region size after coalescing
  Async discard maintains multiple lists with progressively smaller filters
  to prioritize discarding based on size.  Should a free space that matches
  a larger filter be returned to the free_space_cache, prioritize that discard
  by moving @block_group to the proper filter.
  btrfs_update_discard_index - moves a block group along the discard lists
  @discard_ctl: discard control
  @block_group: block_group of interest
  Increment @block_group's discard_index.  If it falls of the list, let it be.
  Otherwise add it back to the appropriate list.
  btrfs_discard_cancel_work - remove a block_group from the discard lists
  @discard_ctl: discard control
  @block_group: block_group of interest
  This removes @block_group from the discard lists.  If necessary, it waits on
  the current work and then reschedules the delayed work.
  btrfs_discard_queue_work - handles queuing the block_groups
  @discard_ctl: discard control
  @block_group: block_group of interest
  This maintains the LRU order of the discard lists.
		
		  A single delayed workqueue item is responsible for
		  discarding, so we can manage the bytes rate limit by keeping
		  track of the previous discard.
		
		  This timeout is to hopefully prevent immediate discarding
		  in a recently allocated block group.
  btrfs_discard_schedule_work - responsible for scheduling the discard work
  @discard_ctl:  discard control
  @override:     override the current timer
  Discards are issued by a delayed workqueue item.  @override is used to
  update the current delay as the baseline delay interval is reevaluated on
  transaction commit.  This is also maxed with any other rate limit.
  btrfs_finish_discard_pass - determine next step of a block_group
  @discard_ctl: discard control
  @block_group: block_group of interest
  This determines the next step for a block group after it's finished going
  through a pass on a discard list.  If it is unused and fully trimmed, we can
  mark it unused and send it to the unused_bgs path.  Otherwise, pass it onto
  the appropriate filter list or let it fall off.
  btrfs_discard_workfn - discard work function
  @work: work
  This finds the next block_group to start discarding and then discards a
  single region.  It does this in a two-pass fashion: first extents and second
  bitmaps.  Completely discarded block groups are sent to the unused_bgs path.
 Perform discarding 
		
		  Use the previous levels minimum discard length as the max
		  length filter.  In the case something is added to make a
		  region go beyond the max filter, the entire bitmap is set
		  back to BTRFS_TRIM_STATE_UNTRIMMED.
 Determine next steps for a block_group 
  btrfs_run_discard_work - determines if async discard should be running
  @discard_ctl: discard control
  Checks if the file system is writeable and BTRFS_FS_DISCARD_RUNNING is set.
  btrfs_discard_calc_delay - recalculate the base delay
  @discard_ctl: discard control
  Recalculate the base delay which is based off the total number of
  discardable_extents.  Clamp this between the lower_limit (iops_limit or 1ms)
  and the upper_limit (BTRFS_DISCARD_MAX_DELAY_MSEC).
	
	  The following is to fix a potential -1 discrepenancy that we're not
	  sure how to reproduce. But given that this is the only place that
	  utilizes these numbers and this is only called by from
	  btrfs_finish_extent_commit() which is synchronized, we can correct
	  here.
  btrfs_discard_update_discardable - propagate discard counters
  @block_group: block_group of interest
  This propagates deltas of counters up to the discard_ctl.  It maintains a
  current counter and a previous counter passing the delta up to the global
  stat.  Then the current counter value becomes the previous counter value.
  btrfs_discard_punt_unused_bgs_list - punt unused_bgs list to discard lists
  @fs_info: fs_info of interest
  The unused_bgs list needs to be punted to the discard lists because the
  order of operations is changed.  In the normal synchronous discard path, the
  block groups are trimmed via a single large trim in transaction commit.  This
  is ultimately what we are trying to avoid with asynchronous discard.  Thus,
  it must be done before going down the unused_bgs path.
 We enabled async discard, so punt all to the queue 
  btrfs_discard_purge_list - purge discard lists
  @discard_ctl: discard control
  If we are disabling async discard, we may have intercepted block groups that
  are completely free and ready for the unused_bgs path.  As discarding will
  now happen in transaction commit or not at all, we can safely mark the
  corresponding block groups as unused and they will be sent on their merry
  way to the unused_bgs list.
 SPDX-License-Identifier: GPL-2.0
	
	  This is used for BTRFS_MOD_LOG_KEY_ and BTRFS_MOD_LOG_MOVE_KEYS
	  operations.
 This is used for BTRFS_MOD_LOG_KEY and BTRFS_MOD_LOG_ROOT_REPLACE. 
 Those are used for op == BTRFS_MOD_LOG_KEY_{REPLACE,REMOVE}. 
 This is used for op == BTRFS_MOD_LOG_MOVE_KEYS. 
 This is used for op == BTRFS_MOD_LOG_ROOT_REPLACE. 
  Pull a new tree mod seq number for our operation.
  This adds a new blocker to the tree mod log's blocker list if the @elem
  passed does not already have a sequence number set. So when a caller expects
  to record tree modifications, it should ensure to set elem->seq to zero
  before calling btrfs_get_tree_mod_seq.
  Returns a fresh, unused tree log modification sequence number, even if no new
  blocker was added.
			
			  Blocker with lower sequence number exists, we cannot
			  remove anything from the log.
	
	  Anything that's lower than the lowest existing (read: blocked)
	  sequence number can be removed from the tree.
  Key order of the log:
        nodeleaf start address -> sequence
  The 'start address' is the logical address of the new root node for root
  replace operations, or the logical address of the affected block for all
  other operations.
  Determines if logging can be omitted. Returns true if it can. Otherwise, it
  returns false with the tree_mod_log_lock acquired. The caller must hold
  this until all tree mod log insertions are recorded in the rb tree and then
  write unlock fs_info::tree_mod_log_lock.
 Similar to tree_mod_dont_log, but doesn't acquire any locks. 
	
	  When we override something during the move, we log these removals.
	  This can only happen when we move towards the beginning of the
	  buffer, i.e. dst_slot < src_slot.
 We want the node with the highest seq 
 We want the node with the smallest seq 
  This returns the element from the log with the smallest time sequence
  value that's in the log (the oldest log item). Any element with a time
  sequence lower than min_seq will be ignored.
  This returns the element from the log with the largest time sequence
  value that's in the log (the most recent log item). Any element with
  a time sequence lower than min_seq will be ignored.
  Returns the logical address of the oldest predecessor of the given root.
  Entries older than time_seq are ignored.
	
	  The very last operation that's logged for a root is the replacement
	  operation (if it is replaced at all). This has the logical address
	  of the new root, making it the very first operation that's logged
	  for this root.
		
		  If there are no tree operation for the oldest root, we simply
		  return it. This should only happen if that (old) root is at
		  level 0.
		
		  If there's an operation that's not a root replacement, we
		  found the oldest version of our root. Normally, we'll find a
		  BTRFS_MOD_LOG_KEY_REMOVE_WHILE_FREEING operation here.
 If there's no old root to return, return what we found instead 
  tm is a pointer to the first operation to rewind within eb. Then, all
  previous operations will be rewound (until we reach something older than
  time_seq).
		
		  All the operations are recorded with the operator used for
		  the modification. As we're going backwards, we do the
		  opposite of each operation here.
 if a move operation is needed it's in the log 
			
			  This operation is special. For roots, this must be
			  handled explicitly before rewinding.
			  For non-roots, this operation may exist if the node
			  was a root: root A -> child B; then A gets empty and
			  B is promoted to the new root. In the mod log, we'll
			  have a root-replace operation for B, a tree block
			  that is no root. We simply ignore that operation.
  Called with eb read locked. If the buffer cannot be rewound, the same buffer
  is returned. If rewind operations happen, a fresh buffer is returned. The
  returned buffer is always read-locked. If the returned buffer is not the
  input buffer, the lock on the input buffer is released and the input buffer
  is freed (its refcount is decremented).
  Rewind the state of @root's root node to the given @time_seq value.
  If there are no changes, the current root->root_node is returned. If anything
  changed in between, there's a fresh buffer allocated on which the rewind
  operations are done. In any case, the returned buffer is read locked.
  Returns NULL on error (with no locks held).
			
			  After the lookup for the most recent tree mod operation
			  above and before we locked and cloned the extent buffer
			  'old', a new tree mod log operation may have been added.
			  So lookup for a more recent one to make sure the number
			  of mod log operations we replay is consistent with the
			  number of items we have in the cloned extent buffer,
			  otherwise we can hit a BUG_ON when rewinding the extent
			  buffer.
  Return the lowest sequence number in the tree modification log.
  Return the sequence number of the oldest tree modification log user, which
  corresponds to the lowest sequence number of all existing users. If there are
  no users it returns 0.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) STRATO AG 2011.  All rights reserved.
  This module can be used to catch cases when the btrfs kernel
  code executes write requests to the disk that bring the file
  system in an inconsistent state. In such a state, a power-loss
  or kernel panic event would cause that the data on disk is
  lost or at least damaged.
  Code is added that examines all block write requests during
  runtime (including writes of the super block). Three rules
  are verified and an error is printed on violation of the
  rules:
  1. It is not allowed to write a disk block which is
     currently referenced by the super block (either directly
     or indirectly).
  2. When a super block is written, it is verified that all
     referenced (directly or indirectly) blocks fulfill the
     following requirements:
     2a. All referenced blocks have either been present when
         the file system was mounted, (i.e., they have been
         referenced by the super block) or they have been
         written since then and the write completion callback
         was called and no write error was indicated and a
         FLUSH request to the device where these blocks are
         located was received and completed.
     2b. All referenced blocks need to have a generation
         number which is equal to the parent's number.
  One issue that was found using this module was that the log
  tree on disk became temporarily corrupted because disk blocks
  that had been in use for the log tree had been freed and
  reused too early, while being referenced by the written super
  block.
  The search term in the kernel log that can be used to filter
  on the existence of detected integrity issues is
  "btrfs: attempt".
  The integrity check is enabled via mount options. These
  mount options are only supported if the integrity check
  tool is compiled by defining BTRFS_FS_CHECK_INTEGRITY.
  Example #1, apply integrity checks to all metadata:
  mount devsdb1 mnt -o check_int
  Example #2, apply integrity checks to all metadata and
  to data extents:
  mount devsdb1 mnt -o check_int_data
  Example #3, apply integrity checks to all metadata and dump
  the tree that the super block references to kernel messages
  each time after a super block was written:
  mount devsdb1 mnt -o check_int,check_int_print_mask=263
  If the integrity check tool is included and activated in
  the mount options, plenty of kernel memory is used, and
  plenty of additional CPU cycles are spent. Enabling this
  functionality is not intended for normal use. In most
  cases, unless you are a btrfs developer who needs to verify
  the integrity of (super)-block write requests, do not
  enable the config option BTRFS_FS_CHECK_INTEGRITY to
  include and compile the integrity check tool.
  Expect millions of lines of information in the kernel log with an
  enabled check_int_print_mask. Therefore set LOG_BUF_SHIFT in the
  kernel config to at least 26 (which is 64MB). Usually the value is
  limited to 21 (which is 2MB) in initKconfig. The file needs to be
  changed like this before LOG_BUF_SHIFT can be set to a high value:
  config LOG_BUF_SHIFT
        int "Kernel log buffer size (16 => 64KB, 17 => 128KB)"
        range 12 30
#define BTRFSIC_TREE_DUMP_MAX_INDENT_LEVEL (200 - 6)	 in characters,
  The definition of the bitmask fields for the print_mask.
  They are specified with the mount option check_integrity_print_mask.
 only used for debug purposes 
 if it is meta-data, not data-data 
 if it is one of the superblocks 
 if is done by lower subsystem 
 error was indicated to endio 
	unsigned int never_written:1;	 block was added because it was
					  referenced, not because it was
	unsigned int mirror_num;	 large enough to hold
 key, physical byte num on disk 
 logical byte num on disk 
	struct btrfs_disk_key disk_key;	 extra info to print in case of
 list node 
 list node 
 the following two lists contain block_link items 
 list 
 list 
 only valid if !never_written 
  Elements of this type are allocated dynamically and required because
  each block object can refer to and can be ref from multiple blocks.
  The key to lookup them in the hashtable is the dev_bytenr of
  the block ref to plus the one from the block referred from.
  The fact that they are searchable via a hashtable and that a
  ref_cnt is maintained is not required for the btrfs integrity
  check algorithm itself, it is only used to make the output more
  beautiful in case that an error is detected (an error is defined
  as a write operation to a block while that block is still referenced).
 only used for debug purposes 
 list node 
 list node 
 list node 
 only used for debug purposes 
 list node 
 virtual bytenr 
 physical bytenr on device 
 This structure is used to implement recursion without occupying
 Some state per mounted filesystem 
 super block bytenr is always the unmapped device bytenr 
 for superblock, only the dev_bytenr makes sense 
 select the one with the highest generation field 
 the one for the initial block is freed in the caller 
 Pages must be unmapped in reverse order 
  Test whether the disk block contains a tree block (leaf or node)
  (note that this test fails for the super block)
 not metadata 
 is metadata 
 it would not be safe to go on 
		
		  Clear all references of this block. Do not free
		  the block itself even if is not referenced anymore
		  because it still carries valuable information
		  like whether it was ever written and IO completed.
 unknown 
 unknown 
				
				  disk block is overwritten with extent
				  data (not meta data) and we are configured
				  to not include extent data: take the
				  chance and free the block's memory
 block has not been found in hash table 
 ignore that written D block 
			 this is getting ugly for the
 unknown 
 unknown 
	 mutex is not held! This is not save if IO is not yet completed
			block->flush_gen = 0;  FUA completed means block is
 for FLUSH, this releases the block 
		
		  Note that this situation can happen and does not
		  indicate an error in regular cases. It happens
		  when disk blocks are freed and later reused.
		  The check-integrity module is not aware of any
		  block free operations, it just recognizes block
		  write operations. Therefore it keeps the linkage
		  information for a block until a block is
		  rewritten. This can temporarily cause incorrect
		  and even circular linkage information. This
		  causes no harm unless such blocks are referenced
		  by the most recent super block.
	
	  This algorithm is recursive because the amount of used stack
	  space is very small and the max recursion depth is limited.
 refer to comment at "abort cyclic linkage (case 1)" 
	
	  This algorithm is recursive because the amount of used stack space
	  is very small and the max recursion depth is limited.
	
	  Should better fill an on-stack buffer with a complete line and
	  dump it at once when it is time to print a newline character.
	
	  This algorithm is recursive because the amount of used stack space
	  is very small and the max recursion depth is limited.
	 since btrfsic_submit_bio() is also called before
	
	  Don't care about keeping the lists' state up to date,
	  just free all memory that was allocated dynamically.
	  Free the blocks and the block_links.
 SPDX-License-Identifier: GPL-2.0
	
	  If we didn't get into open_ctree our allocated_ebs will not be
	  initialized, so just skip this.
	 tells writepage not to lock the state bits for this range
	  it still does the unlocking
 tells the submit_bio code to use REQ_SYNC 
 Caller should ensure the bio has at least some range added 
 Cleanup unsubmitted bios 
  Submit bio from extent page data via submit_one_bio
  Return 0 if everything is OK.
  Return <0 for error.
		
		  Clean up of epd->bio is handled by its endio function.
		  And endio is either triggered by successful bio execution
		  or the error handler of submit bio hook.
		  So at this point, no matter what happened, we don't need
		  to clean up epd->bio.
	
	  Make sure all delayed rcu free are flushed before we
	  destroy caches.
  For the file_extent_tree, we want to hold the inode lock when we lookup and
  update the disk_i_size, but lockdep will complain because our io_tree we hold
  the tree lock and get the inode lock when setting delalloc.  These two things
  are unrelated, so make a class for the file_extent_tree so we don't get the
  two locking patterns mixed up.
	
	  Do a single barrier for the waitqueue_active check here, the state
	  of the waitqueue should not change once extent_io_tree_release is
	  called.
		
		  btree io trees aren't supposed to have tasks waiting for
		  changes in the flags of extent states ever.
	
	  The given mask might be not appropriate for the slab allocator,
	  drop the unsupported bits
  Search @tree for an entry that contains @offset. Such entry would have
  entry->start <= offset && entry->end >= offset.
  @tree:       the tree to search
  @offset:     offset that should fall within an entry in @tree
  @next_ret:   pointer to the first entry whose range ends after @offset
  @prev_ret:   pointer to the first entry whose range begins before @offset
  @p_ret:      pointer where new node should be anchored (used when inserting an
 	        entry in the tree)
  @parent_ret: points to entry which would have been the parent of the entry,
                containing @offset
  This function returns a pointer to the entry that contains @offset byte
  address. If no such entry exists, then NULL is returned and the other
  pointer arguments to the function are filled, otherwise the found entry is
  returned and other pointers are left untouched.
  utility function to look for merge candidates inside a given range.
  Any extents with matching state are merged together into a single
  extent in the tree.  Extents with EXTENT_IO in their state field
  are not merged because the end_io handlers need to be able to do
  operations on them without sleeping (or doing allocationssplits).
  This should be called with the tree lock held.
  insert an extent_state struct into the tree.  'bits' are set on the
  struct before it is inserted.
  This may return -EEXIST if the extent is already there, in which case the
  state struct is freed.
  The tree lock is not taken internally.  This is a utility function and
  probably isn't what you want to call (see setclear_extent_bit).
  split a given extent state struct in two, inserting the preallocated
  struct 'prealloc' as the newly created second half.  'split' indicates an
  offset inside 'orig' where it should be split.
  Before calling,
  the tree has 'orig' at [orig->start, orig->end].  After calling, there
  are two extent state structs in the tree:
  prealloc: [orig->start, split - 1]
  orig: [ split, orig->end ]
  The tree locks are not taken by this function. They need to be held
  by the caller.
  utility function to clear some bits in an extent state struct.
  it will optionally wake up anyone waiting on this state (wake == 1).
  If no bits are set on the state struct after clearing things, the
  struct is freed and removed from the tree
  clear some bits on a range in the tree.  This may require splitting
  or inserting elements in the tree, so the gfp mask is used to
  indicate which allocations or sleeping are allowed.
  pass 'wake' == 1 to kick any sleepers, and 'delete' == 1 to remove
  the given range from the tree regardless of state (ie for truncate).
  the range [start, end] is inclusive.
  This takes the tree lock, and returns 0 on success and < 0 on error.
		
		  Don't care for allocation failure here because we might end
		  up not needing the pre-allocated extent state at all, which
		  is the case if we only have in the tree extent states that
		  cover our input range and don't cover too any other range.
		  If we end up needing a new extent state we allocate it later.
	
	  this search will find the extents that end after
	  our range starts
 the state doesn't have the wanted bits, go ahead 
	
	      | ---- desired range ---- |
	   | state | or
	   | ------------- state -------------- |
	 
	  We need to split the extent we found, and may flip
	  bits on second half.
	 
	  If the extent we found extends past our range, we
	  just split and search again.  It'll get split again
	  the next time though.
	 
	  If the extent we found is inside our range, we clear
	  the desired bit on it.
	
	  | ---- desired range ---- |
	                         | state |
	  We need to split the extent, and clear the bit
	  on the first half
  waits for one or more bits to clear on a range in the state tree.
  The range [start, end] is inclusive.
  The tree lock is taken by this function
		
		  this search will find all the extents that end after
		  our range starts
  set some bits on a range in the tree.  This may require allocations or
  sleeping, so the gfp mask is used to indicate what is allowed.
  If any of the exclusive bits are set, this will fail with -EEXIST if some
  part of the range already has the desired bits set.  The start of the
  existing range is returned in failed_start in this case.
  [start, end] is inclusive This takes the tree lock.
		
		  Don't care for allocation failure here because we might end
		  up not needing the pre-allocated extent state at all, which
		  is the case if we only have in the tree extent states that
		  cover our input range and don't cover too any other range.
		  If we end up needing a new extent state we allocate it later.
	
	  this search will find all the extents that end after
	  our range starts.
	
	  | ---- desired range ---- |
	  | state |
	 
	  Just lock what we found and keep going
	
	      | ---- desired range ---- |
	  | state |
	    or
	  | ------------- state -------------- |
	 
	  We need to split the extent we found, and may flip bits on
	  second half.
	 
	  If the extent we found extends past our
	  range, we just split and search again.  It'll get split
	  again the next time though.
	 
	  If the extent we found is inside our range, we set the
	  desired bit on it.
		
		  If this extent already has all the bits we want set, then
		  skip it, not necessary to split it or do anything with it.
	
	  | ---- desired range ---- |
	      | state | or               | state |
	 
	  There's a hole, we need to insert something in it and
	  ignore the extent we found.
		
		  Avoid to free 'prealloc' if it can be merged with
		  the later extent.
	
	  | ---- desired range ---- |
	                         | state |
	  We need to split the extent, and set the bit
	  on the first half
  convert_extent_bit - convert all bits in a given range from one bit to
  			another
  @tree:	the io tree to search
  @start:	the start offset in bytes
  @end:	the end offset in bytes (inclusive)
  @bits:	the bits to set in this range
  @clear_bits:	the bits to clear in this range
  @cached_state:	state that we're going to cache
  This will go through and set bits for the given range.  If any states exist
  already in this range they are set with the given bit and cleared of the
  clear_bits.  This is only meant to be used by things that are mergeable, ie
  converting from say DELALLOC to DIRTY.  This is not meant to be used with
  boundary bits like LOCK.
  All allocations are done with GFP_NOFS.
		
		  Best effort, don't worry if extent state allocation fails
		  here for the first iteration. We might have a cached state
		  that matches exactly the target range, in which case no
		  extent state allocations are needed. We'll only know this
		  after locking the tree.
	
	  this search will find all the extents that end after
	  our range starts.
	
	  | ---- desired range ---- |
	  | state |
	 
	  Just lock what we found and keep going
	
	      | ---- desired range ---- |
	  | state |
	    or
	  | ------------- state -------------- |
	 
	  We need to split the extent we found, and may flip bits on
	  second half.
	 
	  If the extent we found extends past our
	  range, we just split and search again.  It'll get split
	  again the next time though.
	 
	  If the extent we found is inside our range, we set the
	  desired bit on it.
	
	  | ---- desired range ---- |
	      | state | or               | state |
	 
	  There's a hole, we need to insert something in it and
	  ignore the extent we found.
		
		  Avoid to free 'prealloc' if it can be merged with
		  the later extent.
	
	  | ---- desired range ---- |
	                         | state |
	  We need to split the extent, and set the bit
	  on the first half
 wrappers around setclear extent bit 
	
	  We don't support EXTENT_LOCKED yet, as current changeset will
	  record any bits changed, so for EXTENT_LOCKED case, it will
	  either fail with -EEXIST or changeset will record the whole
	  range.
	
	  Don't support EXTENT_LOCKED case, same reason as
	  set_record_extent_bits().
  either insert or lock state struct between start and end use mask to tell
  us if waiting is desired.
 Pages should be in the extent_io_tree 
 Pages should be in the extent_io_tree 
 find the first state struct with 'bits' set after 'start', and
  return it.  tree->lock must be held.  NULL will returned if
  nothing was found after 'start'
	
	  this search will find all the extents that end after
	  our range starts.
  Find the first offset in the io tree with one or more @bits set.
  Note: If there are multiple bits set in @bits, any of them will match.
  Return 0 if we find something, and update @start_ret and @end_ret.
  Return 1 if we found nothing.
  Find a contiguous area of bits
  @tree:      io tree to check
  @start:     offset to start the search from
  @start_ret: the first offset we found with the bits set
  @end_ret:   the final contiguous range of the bits that were set
  @bits:      bits to look for
  set_extent_bit and clear_extent_bit can temporarily split contiguous ranges
  to set bits appropriately, and then merge them again.  During this time it
  will drop the tree->lock, so use this helper if you want to find the actual
  contiguous area for given bits.  We will search to the first bit we find, and
  then walk down the tree until we find a non-contiguous area.  The area
  returned will be the full contiguous area with the bits set.
  Find the first range that has @bits not set. This range could start before
  @start.
  @tree:      the tree to search
  @start:     offset atafter which the found extent should start
  @start_ret: records the beginning of the range
  @end_ret:   records the end of the range (inclusive)
  @bits:      the set of bits which must be unset
  Since unallocated range is also considered one which doesn't have the bits
  set it's possible that @end_ret contains -1, this happens in case the range
  spans (last_range_end, end of device]. In this case it's up to the caller to
  trim @end_ret to the appropriate size.
 Find first extent with bits cleared 
			
			  Tree is completely empty, send full range and let
			  caller deal with it
			
			  We are past the last allocated chunk, set start at
			  the end of the last extent.
		
		  At this point 'node' either contains 'start' or start is
		  before 'node'
				
				  |--range with bits sets--|
				     |
				     start
				
				  'start' falls within a range that doesn't
				  have the bits set, so take its start as
				  the beginning of the desired range
				 
				  |--range with bits cleared----|
				       |
				       start
			
			  |---prev range---|---holeunset---|---node range---|
			                           |
			                         start
			 
			                         or
			 
			  |---holeunset--||--first node--|
			  0   |
			     start
	
	  Find the longest stretch from start until an entry which has the
	  bits set
  find a contiguous range of bytes in the file marked as delalloc, not
  more than 'max_bytes'.  start and end are used to return the range,
  true is returned if we find something, false if nothing was in the tree
	
	  this search will find all the extents that end after
	  our range starts.
  Process one page for __process_pages_contig().
  Return >0 if we hit @page == @locked_page.
  Return 0 if we updated the page status.
  Return -EGAIN if the we need to try again.
  (For PAGE_LOCK case but got dirty page or page not belong to mapping)
			
			  Only if we're going to lock these pages, we can find
			  nothing at @index.
		
		  Update @processed_end. I know this is awful since it has
		  two different return value patterns (inclusive vs exclusive).
		 
		  But the exclusive pattern is necessary if @start is 0, or we
		  underflow and check against processed_end won't work as
		  expected.
  Find and lock a contiguous range of bytes in the file marked as delalloc, no
  more than @max_bytes.
  @start:	The original start bytenr to search.
 		Will store the extent range start bytenr.
  @end:	The original end bytenr of the search range
 		Will store the extent range end bytenr.
  Return true if we find a delalloc range which starts inside the original
  range, and @start@end will store the delalloc range startend.
  Return false if we can't find any delalloc range which starts inside the
  original range, and @start@end will be the non-delalloc range startend.
 Caller should pass a valid @end to indicate the search range end 
 The range should at least cover part of the page 
 step one, find a bunch of delalloc bytes starting at start 
 @delalloc_end can be -1, never go beyond @orig_end 
	
	  start comes from the offset of locked_page.  We have to lock
	  pages in order, so we can't process delalloc bytes before
	  locked_page
	
	  make sure to limit the number of pages we try to lock down
 step two, lock all the pages after the page that has start 
		 some of the pages are gone, lets avoid looping by
		  shortening the size of the delalloc range we're searching
 step three, lock the state bits for the whole range 
 then test to make sure it is all still delalloc 
  count the number of bytes in the tree that have a given bit(s)
  set.  This can be fairly slow, except for EXTENT_DIRTY which is
  cached.  The total number found is returned.
	
	  this search will find all the extents that end after
	  our range starts.
  set the private field for a given byte offset in the tree.  If there isn't
  an extent_state there already, this does nothing.
	
	  this search will find all the extents that end after
	  our range starts.
	
	  this search will find all the extents that end after
	  our range starts.
  searches a range in the state tree for a given mask.
  If 'filled' == 1, this returns 1 only if every extent in the tree
  has the bits set.  Otherwise, 1 is returned if any bit in the
  range is found set.
  this bypasses the standard btrfs submit functions deliberately, as
  the standard behavior is to write all copies in a raid setup. here we only
  want to write the one bad copy. so we do the mapping for ourselves and issue
  submit_bio directly.
  to avoid any synchronization issues, wait for the data after writing, which
  actually prevents the read that triggered the error from finishing.
  currently, there can be no more than two copies of every data bit. thus,
  exactly one rewrite is required.
	
	  Avoid races with device replace and make sure our bioc has devices
	  associated to its stripes that don't go away while we are doing the
	  read repair operation.
		
		  Note that we don't use BTRFS_MAP_WRITE because it's supposed
		  to update all raid stripes, but here we just want to correct
		  bad stripe, thus BTRFS_MAP_READ is abused to only get the bad
		  stripe's dev and sector.
 try to remap that extent elsewhere? 
  each time an IO finishes, we do a fast check in the IO failure tree
  to see if we need to process or clean up an io_failure_record
  Can be called when
  - hold extent lock
  - under ordered extent
  - the inode is freeing
		
		  when data can be on disk more than twice, add to failrec here
		  (e.g. with a list for failed_mirror) to make
		  clean_io_failure() clean all those errors at once.
 Set the bits in the private failure tree 
 Set the bits in the inode's tree 
		
		  we only have a single copy of the data, so don't bother with
		  all the retry and error correction code that follows. no
		  matter what the error is, it is very likely to persist.
 The failure record should only contain one sector 
	
	  There are two premises:
	  a) deliver good data to the caller
	  b) correct the bad sectors on disk
	 
	  Since we're only doing repair for one sector, we only need to get
	  a good copy of the failed sector and if we succeed, we have setup
	  everything for repair_io_failure to do the rest for us.
 We're here because we had some read errors or csum mismatch 
	
	  We only get called on buffered IO, thus page must be mapped and bio
	  must not be cloned.
 Iterate through all the sectors in the range 
			
			  This sector has no error, just end the page read
			  and unlock the range.
			
			  We have submitted the read repair, the page release
			  will be handled by the endio function of the
			  submitted repair bio.
			  Thus we don't need to do any thing here.
		
		  Repair failed, just record the error but still continue.
		  Or the remaining sectors will not be properly unlocked.
 lots and lots of room for performance fixes in the end_bio funcs 
  after a writepage IO is done, we need to:
  clear the uptodate bits on error
  clear the writeback bits in the extent tree for this IO
  end_page_writeback if the page has no more pending IO
  Scheduling is not allowed, so the extent state tree is expected
  to have one and only one object corresponding to this IO.
 Our readwrite should always be sector aligned. 
  Record previously processed extent range
  For endio_readpage_release_extent() to handle a full extent range, reducing
  the extent io operations.
 Start of the range in @inode 
 End of the range in @inode 
  Try to release processed extent range
  May not release the extent range right now if the current range is
  contiguous to processed extent.
  Will release processed extent when any of @inode, @uptodate, the range is
  no longer contiguous to the processed range.
  Passing @inode == NULL will force processed extent to be released.
 The first extent, initialize @processed 
	
	  Contiguous to processed extent, just uptodate the end.
	 
	  Several things to notice:
	 
	  - bio can be merged as long as on-disk bytenr is contiguous
	    This means we can have page belonging to other inodes, thus need to
	    check if the inode still matches.
	  - bvec can contain range beyond current page for multi-page bvec
	    Thus we need to do processed->end + 1 >= start check
	
	  Now we don't have range contiguous to the processed range, release
	  the processed range now.
 Update processed to current range 
  Find extent buffer for a givne bytenr.
  This is for end_bio_extent_readpage(), thus we can't do any unsafe locking
  in endio context.
	
	  For regular sectorsize, we can use page->private to grab extent
	  buffer
 For subpage case, we need to lookup buffer radix tree 
  after a readpage IO is done, we need to:
  clear the uptodate bits on error
  set the uptodate bits if things worked
  set the page up to date if all extents in the tree are uptodate
  clear the lock bit in the extent tree
  unlock the page if there are no other extents locked for it
  Scheduling is not allowed, so the extent state tree is expected
  to have one and only one object corresponding to this IO.
	
	  The offset to the beginning of a bio, since one bio can never be
	  larger than UINT_MAX, u32 here is enough.
		
		  We always issue full-sector reads, but if some block in a
		  page fails to read, blk_update_request() will advance
		  bv_offset and adjust bv_len to compensate.  Print a warning
		  for unaligned offsets, and an error if they don't add up to
		  a full sector.
			
			  btrfs_submit_read_repair() will handle all the good
			  and bad sectors, we just continue to the next bvec.
			
			  Zero out the remaining part if this range straddles
			  i_size.
			 
			  Here we should only zero the range inside the bvec,
			  not touch anything else.
			 
			  NOTE: i_size is exclusive while end is inclusive.
 Update page status and unlock 
 Release the last extent 
  Initialize the members up to but not including 'bio'. Use after allocating a
  new bio by bio_alloc_bioset as it does not initialize the bytes outside of
  'bio' because use of __GFP_ZERO is not supported.
  Allocate a btrfs_io_bio, with @nr_iovecs as maximum number of iovecs.
  The bio allocation is backed by bioset and does not fail.
 Bio allocation backed by a bioset does not fail 
 this will never fail when it's backed by a bioset 
  Attempt to add a page to bio
  @bio:	destination bio
  @page:	page to add to the bio
  @disk_bytenr:  offset of the new bio or to check whether we are adding
                 a contiguous page to the previous one
  @pg_offset:	starting offset in the page
  @size:	portion of page that we want to write
  @prev_bio_flags:  flags of previous bio to see if we can merge the current one
  @bio_flags:	flags of the current bio to see if we can merge them
  Attempt to add a page to bio considering stripe alignment etc.
  Return >= 0 for the number of bytes added to the bio.
  Can return 0 if the current bio is already at stripezone boundary.
  Return <0 for error.
 The limit should be calculated when bio_ctrl->bio is allocated 
	
	  If real_size is 0, never call bio_add__page(), as even size is 0,
	  bio will still execute its endio function on the page!
	
	  Pages for compressed extent are never submitted to disk directly,
	  thus it has no real boundary, just set them to U32_MAX.
	 
	  The split happens for real compressed bio, which happens in
	  btrfs_submit_compressed_readwrite().
 Ordered extent not yet created, so we're good 
	
	  For compressed page range, its disk_bytenr is always @disk_bytenr
	  passed in, no matter if we have added any range into previous bio.
  @opf:	bio REQ_OP_ and REQ_ flags as one value
  @wbc:	optional writeback control for io accounting
  @page:	page to add to the bio
  @disk_bytenr: logical bytenr where the write will be
  @size:	portion of page that we want to write to
  @pg_offset:	offset of the new bio or to check whether we are adding
               a contiguous page to the previous one
  @bio_ret:	must be valid pointer, newly allocated bio will be stored there
  @end_io_func:     end_io callback for new bio
  @mirror_num:	     desired mirror to readwrite
  @prev_bio_flags:  flags of previous bio to see if we can merge the current one
  @bio_flags:	flags of the current bio to see if we can merge them
 Allocate new bio if needed 
		
		  We must go through btrfs_bio_add_page() to ensure each
		  page range won't cross various boundaries.
 Metadata page range should never be split 
 At least we added some page, update the account 
 We have reached boundary, submit right now 
 The bio should contain some page(s) 
	
	  If the page is mapped to btree inode, we should hold the private
	  lock to prevent race.
	  For cloned or dummy extent buffers, their pages are not mapped and
	  will not race with any other ebs.
 Already mapped, just free prealloc 
 Has preallocated memory for subpage 
 Do new allocation to attach subpage 
  basic readpage implementation.  Locked extent state structs are inserted
  into the tree that are removed when the IO is done (by the end_io
  handlers)
  XXX JDM: This needs looking at to ensure proper page locking
  return 0 on success, otherwise return error
		
		  If we have a file range that points to a compressed extent
		  and it's followed by a consecutive file range that points
		  to the same compressed extent (possibly with a different
		  offset andor length, so it either points to the whole extent
		  or only part of it), we must make sure we do not submit a
		  single bio to populate the pages for the 2 ranges because
		  this makes the compressed extent read zero out the pages
		  belonging to the 2nd range. Imagine the following scenario:
		 
		   File layout
		   [0 - 8K]                     [8K - 24K]
		     |                               |
		     |                               |
		  points to extent X,         points to extent X,
		  offset 4K, length of 8K     offset 0, length 16K
		 
		  [extent X, compressed length = 4K uncompressed length = 16K]
		 
		  If the bio to read the compressed extent covers both ranges,
		  it will decompress extent X into the pages belonging to the
		  first range and then it will stop, zeroing out the remaining
		  pages that belong to the other range that points to extent X.
		  So here we make sure we submit 2 bios, one for the first
		  range and another one for the third range. Both will target
		  the same physical extent from disk, but we can't currently
		  make the compressed bio endio callback populate the pages
		  for both ranges because each compressed bio is tightly
		  coupled with a single extent map, and each range can have
		  an extent map with a different offset value relative to the
		  uncompressed data of our extent and different lengths. This
		  is a corner case so we prioritize correctness over
		  non-optimal behavior (submitting 2 bios for the same extent).
 we've found a hole, just zero and go on 
 the get_extent function already copied into the page 
		 we have an inline extent but it didn't get marked up
		  to date.  Error out
  helper for __extent_writepage, doing all of the delayed allocation setup.
  This returns 1 if btrfs_run_delalloc_range function did all the work required
  to write the page (copy into inline extent).  In this case the IO has
  been started and the page is already unlocked.
  This returns 0 if all went well (page still locked)
  This returns < 0 if there were errors (page still locked)
		
		  delalloc_end is already one less than the total length, so
		  we don't subtract one from PAGE_SIZE
	 did the fill delalloc function already unlock and start
	  the IO?
		
		  we've unlocked the page, so we can't update
		  the mapping's writeback index, just update
		  nr_to_write.
  Find the first byte we need to write.
  For subpage, one page can contain several sectors, and
  __extent_writepage_io() will just grab all extent maps in the page
  range and try to submit all non-inlinenon-compressed extents.
  This is a big problem for subpage, we shouldn't re-submit already written
  data at all.
  This function will lookup subpage dirty bit to find which range we really
  need to submit.
  Return the next dirty range in [@start, @end).
  If no dirty range is found, @start will be page_offset(page) + PAGE_SIZE.
 Declare as unsigned long so we can use bitmap ops 
	
	  For regular sector size == page size case, since one page only
	  contains one sector, we return the page offset directly.
 We should have the page locked, but just in case 
  helper for __extent_writepage.  This calls the writepage start hooks,
  and does the loop to map the page into extents and bios.
  We return 1 if the IO is started and the page is unlocked,
  0 if all went well (page still locked)
  < 0 if there were errors (page still locked)
 Fixup worker will requeue 
	
	  we don't want to touch the inode after unlocking the page,
	  so we update the mapping writeback index now
			
			  This range is beyond i_size, thus we don't need to
			  bother writing back.
			  But we still need to clear the dirty subpage bit, or
			  the next time the page gets dirtied, we will try to
			  writeback the sectors with subpage dirty bits,
			  causing writeback without ordered extent.
		
		  Note that em_end from extent_map_end() and dirty_range_end from
		  find_next_dirty_byte() are all exclusive
		
		  compressed and inline extents are written through other
		  paths in the FS
		
		  Although the PageDirty bit is cleared before entering this
		  function, subpage dirty bit is not cleared.
		  So clear subpage dirty bit here so next time we won't submit
		  page for range already written to disk.
	
	  If we finish without problem, we should not only clear page dirty,
	  but also empty subpage dirty bits
  the writepage semantics are similar to regular writepage.  extent
  records are inserted to lock ranges in the tree, and as dirty areas
  are found, they are marked writeback.  Then the lock bits are removed
  and the end_io handler clears the writeback ranges
  Return 0 if everything goes well.
  Return <0 for error.
 make sure the mapping tag for page dirty gets cleared 
	
	  Here we used to have a check for PageError() and then set @ret and
	  call end_extent_writepage().
	 
	  But in fact setting @ret here will cause different error paths
	  between subpage and regular sectorsize.
	 
	  For regular page size, we never submit current page, but only add
	  current page to current bio.
	  The bio submission can only happen in next page.
	  Thus if we hit the PageError() branch, @ret is already set to
	  non-zero value and will not get updated for regular sectorsize.
	 
	  But for subpage case, it's possible we submit part of current page,
	  thus can get PageError() set by submitted bio of the same page,
	  while our @ret is still 0.
	 
	  So here we unify the behavior and don't set @ret.
	  Error can still be properly passed to higher layer as page will
	  be set error, here we just don't handle the IO failure.
	 
	  NOTE: This is just a hotfix for subpage.
	  The root fix will be properly ending ordered extent when we hit
	  an error during writeback.
	 
	  But that needs a bigger refactoring, as we not only need to grab the
	  submitted OE, but also need to know exactly at which bytenr we hit
	  the error.
	  Currently the full page based __extent_writepage_io() is not
	  capable of that.
		
		  If epd->extent_locked, it's from extent_write_locked_range(),
		  the page can either be locked by lock_page() or
		  process_one_page().
		  Let btrfs_page_unlock_writer() handle both cases.
  Lock extent buffer status and pages for writeback.
  May try to flush write bio if we can't get the lock.
  Return  0 if the extent buffer doesn't need to be submitted.
            (E.g. the extent buffer is not dirty)
  Return >0 is the extent buffer is submitted to bio.
  Return <0 if something went wrong, no page is locked.
	
	  We need to do this to prevent races in people who check if the eb is
	  under IO since we can end up having no IO bits set for a short period
	  of time.
	
	  Either we don't need to submit any tree block, or we're submitting
	  subpage eb.
	  Subpage metadata doesn't use page locking at all, so we can skip
	  the page locking.
 Unlock already locked pages 
	
	  Clear EXTENT_BUFFER_WRITEBACK and wake up anyone waiting on it.
	  Also set back EXTENT_BUFFER_DIRTY so future attempts to this eb can
	  be made and undo everything done before.
	
	  If we error out, we should add back the dirty_metadata_bytes
	  to make it consistent.
	
	  If writeback for a btree extent that doesn't belong to a log tree
	  failed, increment the counter transaction->eb_write_errors.
	  We do this because while the transaction is running and before it's
	  committing (when we call filemap_fdata[write|wait]_range against
	  the btree inode), we might have
	  btree_inode->i_mapping->a_ops->writepages() called by the VM - if it
	  returns an error or an error happens during writeback, when we're
	  committing the transaction we wouldn't know about it, since the pages
	  can be no longer dirty nor marked anymore for writeback (if a
	  subsequent modification to the extent buffer didn't happen before the
	  transaction commit), which makes filemap_fdata[write|wait]_range not
	  able to find the pages tagged with SetPageError at transaction
	  commit time. So if this happens we must abort the transaction,
	  otherwise we commit a super block with btree roots that point to
	  btree nodesleafs whose content on disk is invalid - either garbage
	  or the content of some nodeleaf from a past generation that got
	  cowed or deleted and is no longer valid.
	 
	  Note: setting AS_EIOAS_ENOSPC in the btree inode's i_mapping would
	  not be enough - we need to distinguish between log tree extents vs
	  non-log tree extents, and the next filemap_fdatawait_range() call
	  will catch and clear such errors in the mapping - and that call might
	  be from a log sync and not from a transaction commit. Also, checking
	  for the eb flag EXTENT_BUFFER_WRITE_ERR at transaction commit time is
	  not done and would not be reliable - the eb might have been released
	  from memory and reading it back again means that flag would not be
	  set (since it's a runtime flag, not persisted on disk).
	 
	  Using the flags below in the btree inode also makes us achieve the
	  goal of AS_EIOAS_ENOSPC when writepages() returns success, started
	  writeback for all dirty pages and before filemap_fdatawait_range()
	  is called, the writeback for all dirty pages had already finished
	  with errors - because we were not using AS_EIOAS_ENOSPC,
	  filemap_fdatawait_range() would return success, as it could not know
	  that writeback errors happened (the pages were no longer tagged for
	  writeback).
 unexpected, logic error 
  The endio specific version which won't touch any unsafe spinlock in endio
  context.
  The endio function for subpage extent buffer write.
  Unlike end_bio_extent_buffer_writepage(), we only call end_page_writeback()
  after all extent buffers in the page has finished their writeback.
 Iterate through all extent buffers in the range 
			
			  Here we can't use find_extent_buffer(), as it may
			  try to lock eb->refs_lock, which is not safe in endio
			  context.
			
			  free_extent_buffer() will grab spinlock which is not
			  safe in endio context. Thus here we manually dec
			  the ref.
 Set btree blocks beyond nritems with 0 to avoid stale content 
		
		  Leaf:
		  header 0 1 2 .. N ... data_N .. data_2 data_1 data_0
  Unlike the work in write_one_eb(), we rely completely on extent locking.
  Page locking is only utilized at minimum to keep the VMM code happy.
 clear_page_dirty_for_io() in subpage helper needs page locked 
 Check if this is the last dirty bit to update nr_written 
	
	  Submission finished without problem, if no range of the page is
	  dirty anymore, we have submitted a page.  Update nr_written in wbc.
  Submit one subpage btree page.
  The main difference to submit_eb_page() is:
  - Page locking
    For subpage, we don't rely on page locking at all.
  - Flush write bio
    We only flush bio if we may be unable to fit current extent buffers into
    current bio.
  Return >=0 for the number of submitted extent buffers.
  Return <0 for fatal error.
 Lock and write each dirty extent buffers in the range 
		
		  Take private lock to ensure the subpage won't be detached
		  in the meantime.
		
		  Here we just want to grab the eb without touching extra
		  spin locks, so call find_extent_buffer_nolock().
		
		  The eb has already reached 0 refs thus find_extent_buffer()
		  doesn't return it. We don't need to write back such eb
		  anyway.
 We hit error, end bio for the submitted extent buffers 
  Submit all page(s) of one extent buffer.
  @page:	the page of one extent buffer
  @eb_context:	to determine if we need to submit this page, if current page
 		belongs to this eb, we don't need to submit
  The caller should pass each page in their bytenr order, and here we use
  @eb_context to determine if we have submitted pages of one extent buffer.
  If we have, we just skip until we hit a new page that doesn't belong to
  current @eb_context.
  If not, we submit all the page(s) of the extent buffer.
  Return >0 if we have submitted the extent buffer successfully.
  Return 0 if we don't need to submit the page, as it's already submitted by
  previous call.
  Return <0 for fatal error.
	
	  Shouldn't happen and normally this would be a BUG_ON but no point
	  crashing the machine for something we can survive anyway.
		
		  If for_sync, this hole will be filled with
		  trasnsaction commit.
 Impiles write in zoned mode 
 Mark the last eb in a block group 
 Inclusive 
 Start from prev offset 
		
		  Start from the beginning does not need to cycle over the
		  range, mark it as scanned.
			
			  the filesystem may choose to bump up nr_to_write.
			  We have to make sure to honor the new nr_to_write
			  at any time
		
		  We hit the last page and there is more work to be done: wrap
		  back to the start of the file
	
	  If something went wrong, don't allow any metadata write bio to be
	  submitted.
	 
	  This would prevent use-after-free if we had dirty pages not
	  cleaned up, which can still happen by fuzzed images.
	 
	  - Bad extent tree
	    Allowing existing tree block to be allocated for other trees.
	 
	  - Log tree operations
	    Exiting tree blocks get allocated to log tree, bumps its
	    generation, then get cleaned in tree re-balance.
	    Such tree block will not be written back, since it's clean,
	    thus no WRITTEN flag set.
	    And after log writes back, this tree block is not traced by
	    any dirty extent_io_tree.
	 
	  - Offending tree block gets re-dirtied from its original owner
	    Since it has bumped generation, no WRITTEN flag, it can be
	    reused without COWing. This tree block will not be traced
	    by btrfs_transaction::dirty_pages.
	 
	    Now such dirty tree block will not be cleaned by any dirty
	    extent io tree. Thus we don't want to submit such wild eb
	    if the fs already has error.
  Walk the list of dirty pages of the given address space and write all of them.
  @mapping: address space structure to write
  @wbc:     subtract the number of written pages from @wbc->nr_to_write
  @epd:     holds context for the write, namely the bio
  If a page is already under IO, write_cache_pages() skips it, even
  if it's dirty.  This is desirable behaviour for memory-cleaning writeback,
  but it is INCORRECT for data-integrity system calls such as fsync().  fsync()
  and msync() need to guarantee that all the data which was dirty at the time
  the call was made get new IO started against them.  If wbc->sync_mode is
  WB_SYNC_ALL then we were called for data integrity and we must wait for
  existing IO to complete.
 Inclusive 
	
	  We have to hold onto the inode so that ordered extents can do their
	  work when the IO finishes.  The alternative to this is failing to add
	  an ordered extent if the igrab() fails there and that is a huge pain
	  to deal with, so instead just hold onto the inode throughout the
	  writepages operation.  If it fails here we are freeing up the inode
	  anyway and we'd rather not waste our time writing out stuff that is
	  going to be truncated anyway.
 Start from prev offset 
		
		  Start from the beginning does not need to cycle over the
		  range, mark it as scanned.
	
	  We do the tagged writepage as long as the snapshot flush bit is set
	  and we are the first one who do the filemap_flush() on this inode.
	 
	  The nr_to_write == LONG_MAX is needed to make sure other flushers do
	  not race in and drop the bit.
			
			  At this point we hold neither the i_pages lock nor
			  the page lock: the page may be truncated or
			  invalidated (changing page->mapping to NULL),
			  or even swizzled back from swapper_space to
			  tmpfs file mapping
			
			  the filesystem may choose to bump up nr_to_write.
			  We have to make sure to honor the new nr_to_write
			  at any time
		
		  We hit the last page and there is more work to be done: wrap
		  back to the start of the file
		
		  If we're looping we could run into a page that is locked by a
		  writer and that writer could be waiting on writeback for a
		  page in our current bio, and thus deadlock, so flush the
		  write bio here.
  Submit the pages in the range to bio for call sites which delalloc range has
  already been ran (aka, ordered extent inserted) and all pages are still
  locked.
 We're called from an async helper function 
		
		  All pages in the range are locked since
		  btrfs_run_delalloc_range(), thus there is no way to clear
		  the page dirty flag.
	
	  Allow only a single thread to do the reloc work in zoned mode to
	  protect the write pointer updates.
  basic invalidatepage code, this waits on any locked or writeback
  ranges corresponding to the page, and then deletes any extent state
  records from the tree
 This function is only called for the btree inode 
	
	  Currently for btree io tree, only EXTENT_LOCKED is utilized,
	  so here we only need to unlock the extent range to free any
	  existing extent state.
  a helper for releasepage, this tests for areas of the page that
  are locked or under IO and drops the related state bits if it is safe
  to drop the page.
		
		  At this point we can safely clear everything except the
		  locked bit, the nodatasum bit and the delalloc new bit.
		  The delalloc new bit will be cleared by ordered extent
		  completion.
		 if clear_extent_bit failed for enomem reasons,
		  we can't allow the release to continue.
  a helper for releasepage.  As long as there are no locked extents
  in the range corresponding to the page, both state records and extent
  map records are removed
			
			  If it's not in the list of modified extents, used
			  by a fast fsync, we can remove it. If it's being
			  logged we can safely remove it since fsync took an
			  extra reference on the em.
			
			  If it's in the list of modified extents, remove it
			  only if its generation is older then the current one,
			  in which case we don't need it for a fast fsync.
			  Otherwise don't remove it, we could be racing with an
			  ongoing fast fsync that could miss the new extent.
			
			  We only remove extent maps that are not in the list of
			  modified extents or that are in the list but with a
			  generation lower then the current generation, so there
			  is no need to set the full fsync flag on the inode (it
			  hurts the fsync performance for workloads with a data
			  size that exceeds or is close to the system's memory).
 once for the rb tree 
 once for us 
 Allow large-extent preemption. 
  helper function for fiemap, which doesn't want to see any holes.
  This maps until we find something past 'last'
 if this isn't a hole return it 
 this is a hole, advance to the next extent 
  To cache previous fiemap extent
  Will be used for merging fiemap extent
  Helper to submit fiemap extent.
  Will try to merge current fiemap extent specified by @offset, @phys,
  @len and @flags with cached one.
  And only when we fails to merge, cached one will be submitted as
  fiemap extent.
  Return value is the same as fiemap_fill_next_extent().
	
	  Sanity check, extent_fiemap() should have ensured that new
	  fiemap extent won't overlap with cached one.
	  Not recoverable.
	 
	  NOTE: Physical address can overlap, due to compression
	
	  Only merges fiemap extents if
	  1) Their logical addresses are continuous
	 
	  2) Their physical addresses are continuous
	     So truly compressed (physical size smaller than logical size)
	     extents won't get merged with each other
	 
	  3) Share same flags except FIEMAP_EXTENT_LAST
	     So regular extent won't get merged with prealloc extent
 Not mergeable, need to submit cached one 
  Emit last fiemap cache
  The last fiemap cache may still be cached in the following case:
  0		      4k		    8k
  |<- Fiemap range ->|
  |<------------  First extent ----------->|
  In this case, the first extent range will be cached but not emitted.
  So we must emit it before ending extent_fiemap().
	
	  We can't initialize that to 'start' as this could miss extents due
	  to extent item merging
	
	  lookup the last file extent.  We're not using i_size here
	  because there might be preallocation past i_size
 No extents, but there might be delalloc bits 
 have to trust i_size as the end 
		
		  remember the start of the last extent.  There are a
		  bunch of different factors that go into the length of the
		  extent, so its much less complex to remember where it started
	
	  we might have some extents allocated but more delalloc past those
	  extents.  so, we trust isize unless the start of the last extent is
	  beyond isize
 break if the extent we found is outside the range 
		
		  get_extent may return an extent that starts before our
		  requested range.  We have to make sure the ranges
		  we return to fiemap always move forward and don't
		  overlap, so adjust the offsets here
		
		  record the offset from the start of the extent
		  for adjusting the disk offset below.  Only do this if the
		  extent isn't compressed since our in ram offset may be past
		  what we have actually allocated on disk.
		
		  bump off for our next call to get_extent
			
			  As btrfs supports shared space, this information
			  can be exported to userspace tools via
			  flag FIEMAP_EXTENT_SHARED.  If fi_extents_max == 0
			  then we're just getting a count and we can skip the
			  lookup stuff.
 now scan forward to see if this is really the last extent. 
		
		  Even there is no eb refs here, we may still have
		  end_page_read() call relying on page::private.
	
	  For mapped eb, we're going to change the page private, which should
	  be done under the private_lock.
		
		  We do this since we'll remove the pages after we've
		  removed the eb from the radix tree, so we could race
		  and have this page now attached to the new eb.  So
		  only clear page_private if it's still connected to
		  this eb.
			
			  We need to make sure we haven't be attached
			  to a new eb.
	
	  For subpage, we can have dummy eb with page private.  In this case,
	  we can directly detach the private as such page is only attached to
	  one dummy eb, no sharing.
	
	  We can only detach the page private if there are no other ebs in the
	  page range and no unfinished IO.
 Release all pages attached to the extent buffer 
 One for when we allocated the page 
  Helper for releasing the extent buffer.
	
	  Set UNMAPPED before calling btrfs_release_extent_buffer(), as
	  btrfs_release_extent_buffer() have different behavior for
	  UNMAPPED subpage extent buffer.
	
	  The TREE_REF bit is first set when the extent_buffer is added
	  to the radix tree. It is also reset, if unset, when a new reference
	  is created by find_extent_buffer.
	 
	  It is only cleared in two cases: freeing the last non-tree
	  reference to the extent_buffer when its STALE bit is set or
	  calling releasepage when the tree reference is the only reference.
	 
	  In both cases, care is taken to ensure that the extent_buffer's
	  pages are not under io. However, releasepage can be concurrently
	  called with creating new references, which is prone to race
	  conditions between the calls to check_buffer_tree_ref in those
	  codepaths and clearing TREE_REF in try_release_extent_buffer.
	 
	  The actual lifetime of the extent_buffer in the radix tree is
	  adequately protected by the refcount, but the TREE_REF bit and
	  its corresponding reference are not. To protect against this
	  class of races, we call check_buffer_tree_ref from the codepaths
	  which trigger io after they set eb->io_pages. Note that once io is
	  initiated, TREE_REF can no longer be cleared, so that is the
	  moment at which any such race is best fixed.
	
	  Lock our eb's refs_lock to avoid races with free_extent_buffer().
	  When we get our eb it might be flagged with EXTENT_BUFFER_STALE and
	  another task running free_extent_buffer() might have seen that flag
	  set, eb->refs == 2, that the buffer isn't under IO (dirty and
	  writeback flags not set) and it's still in the tree (flag
	  EXTENT_BUFFER_TREE_REF set), therefore being in the process of
	  decrementing the extent buffer's reference count twice.  So here we
	  could race and increment the eb's reference count, clear its stale
	  flag, mark it as dirty and drop our reference before the other task
	  finishes executing free_extent_buffer, which would later result in
	  an attempt to free an extent buffer that is dirty.
	
	  For subpage case, we completely rely on radix tree to ensure we
	  don't try to insert two ebs for the same bytenr.  So here we always
	  return NULL and just continue.
 Page not yet attached to an extent buffer 
	
	  We could have already allocated an eb for this page and attached one
	  so lets see if we can get a ref on the existing eb, and if we can we
	  know it's good and we can just return that one, else we know we can
	  just overwrite page->private.
		
		  Preallocate page->private for subpage case, so that we won't
		  allocate memory with private_lock hold.  The memory will be
		  freed by attach_extent_buffer_page() or freed manually if
		  we exit earlier.
		 
		  Although we have ensured one subpage eb can only have one
		  page, but it may change in the future for 16K page size
		  support, so we still preallocate the memory in the loop.
 Should not fail, as we have preallocated the memory 
		
		  To inform we have extra eb under allocation, so that
		  detach_extent_buffer_page() won't release the page private
		  when the eb hasn't yet been inserted into radix tree.
		 
		  The ref will be decreased when the eb released the page, in
		  detach_extent_buffer_page().
		  Thus needs no special handling in error path.
		
		  We can't unlock the pages just yet since the extent buffer
		  hasn't been properly inserted in the radix tree, this
		  opens a race with btree_releasepage which can free a page
		  while we are still filling in all pages for the buffer and
		  we could crash.
 add one reference for the tree 
	
	  Now it's safe to unlock the pages because any calls to
	  btree_releasepage will correctly detect that a page belongs to a
	  live buffer and won't free them prematurely.
 Should be safe to release our pages at this point 
	
	  I know this is terrible, but it's temporary until we stop tracking
	  the uptodate bits and such for the extent buffers.
 btree_clear_page_dirty() needs page locked 
		
		  For subpage case, we can have other extent buffers in the
		  same page, and in clear_subpage_extent_buffer_dirty() we
		  have to clear page dirty without subpage lock held.
		  This can cause race where our page gets dirty cleared after
		  we just set it.
		 
		  Thankfully, clear_subpage_extent_buffer_dirty() has locked
		  its page for other reasons, we can use page lock to prevent
		  the above race.
		
		  In the endio function, if we hit something wrong we will
		  increase the io_pages, so here we need to decrease it for
		  error path.
			
			  WAIT_NONE is only utilized by readahead. If we can't
			  acquire the lock atomically it means either the eb
			  is being read out or under modification.
			  Either way the eb will be or has been cached,
			  readahead can exit safely.
	
	  We need to firstly lock all pages to make sure that
	  the uptodate bit of our pages won't be affected by
	  clear_extent_buffer_uptodate().
	
	  It is possible for releasepage to clear the TREE_REF bit before we
	  set io_pages. See check_buffer_tree_ref for a more detailed comment.
				
				  We failed to submit the bio so it's the
				  caller's responsibility to perform cleanup
				  i.e unlock pageset error bit.
  Check if the [start, start + len) range is valid before readingwriting
  the eb.
  NOTE: @start and @len are offset inside the eb, not logical address.
  Caller should not touch the dstsrc memory if this function returns error.
 start, start + len should not go beyond eb->len nor overflow 
  Check that the extent buffer is uptodate.
  For regular sector size == PAGE_SIZE case, check if @page is uptodate.
  For subpage case, check if the range covered by the eb has EXTENT_UPTODATE.
  eb_bitmap_offset() - calculate the page and offset of the byte containing the
  given bit number
  @eb: the extent buffer
  @start: offset of the bitmap item in the extent buffer
  @nr: bit number
  @page_index: return index of the page in the extent buffer that contains the
  given bit number
  @page_offset: return offset into the page given by page_index
  This helper hides the ugliness of finding the byte in an extent buffer which
  contains a given bit.
	
	  The byte we want is the offset of the extent buffer + the offset of
	  the bitmap item in the extent buffer + the offset of the byte in the
	  bitmap item.
  extent_buffer_test_bit - determine whether a bit in a bitmap item is set
  @eb: the extent buffer
  @start: offset of the bitmap item in the extent buffer
  @nr: bit number to test
  extent_buffer_bitmap_set - set an area of a bitmap
  @eb: the extent buffer
  @start: offset of the bitmap item in the extent buffer
  @pos: bit number of the first bit
  @len: number of bits to set
  extent_buffer_bitmap_clear - clear an area of a bitmap
  @eb: the extent buffer
  @start: offset of the bitmap item in the extent buffer
  @pos: bit number of the first bit
  @len: number of bits to clear
 Already beyond page end 
 Found one 
		
		  Unlike try_release_extent_buffer() which uses page->private
		  to grab buffer, for subpage case we rely on radix tree, thus
		  we need to ensure radix tree consistency.
		 
		  We also want an atomic snapshot of the radix tree, thus go
		  with spinlock rather than RCU.
 No more eb in the page range after or at cur 
		
		  The same as try_release_extent_buffer(), to ensure the eb
		  won't disappear out from under us.
		
		  If tree ref isn't set then we know the ref on this eb is a
		  real ref, so just return, this eb will likely be freed soon
		  anyway.
		
		  Here we don't care about the return value, we will always
		  check the page private at the end.  And
		  release_extent_buffer() will release the refs_lock.
	
	  Finally to check if we have cleared page private, as if we have
	  released all ebs in the page, the page private should be cleared now.
	
	  We need to make sure nobody is changing page->private, as we rely on
	  page->private as the pointer to extent buffer.
	
	  This is a little awful but should be ok, we need to make sure that
	  the eb doesn't disappear out from under us while we're looking at
	  this page.
	
	  If tree ref isn't set then we know the ref on this eb is a real ref,
	  so just return, this page will likely be freed soon anyway.
  btrfs_readahead_tree_block - attempt to readahead a child block
  @fs_info:	the fs_info
  @bytenr:	bytenr to read
  @owner_root: objectid of the root that owns this eb
  @gen:	generation for the uptodate check, can be 0
  @level:	level for the eb
  Attempt to readahead a tree block at @bytenr.  If @gen is 0 then we do a
  normal uptodate check of the eb, without checking the generation.  If we have
  to read the block we will not block on anything.
  btrfs_readahead_node_child - readahead a node's child block
  @node:	parent node we're reading from
  @slot:	slot in the parent node for the child we want to read
  A helper for btrfs_readahead_tree_block, we simply read the bytenr pointed at
  the slot in the node provided.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2009 Oracle.  All rights reserved.
  delayed back reference update tracking.  For subvolume trees
  we queue up extent allocations and backref maintenance for
  delayed processing.   This avoids deep call chains where we
  add extents in the middle of btrfs_search_slot, and it allows
  us to buffer up frequently modified backrefs in an rb tree instead
  of hammering updates on the extent allocation tree.
	
	  Since the global reserve is just kind of magic we don't really want
	  to rely on it to save our bacon, so if our size is more than the
	  delayed_refs_rsv and the global rsv then it's time to think about
	  bailing.
  Release a ref head's reservation
  @fs_info:  the filesystem
  @nr:       number of items to drop
  This drops the delayed ref head's count from the delayed refs rsv and frees
  any excess reservation we had.
  btrfs_update_delayed_refs_rsv - adjust the size of the delayed refs rsv
  @trans - the trans that may have generated delayed refs
  This is to be called anytime we may have adjusted trans->delayed_ref_updates,
  it'll calculate the additional size and add it to the delayed_refs_rsv.
  Transfer bytes to our delayed refs rsv
  @fs_info:   the filesystem
  @src:       source block rsv to transfer from
  @num_bytes: number of bytes to transfer
  This transfers up to the num_bytes amount from the src rsv to the
  delayed_refs_rsv.  Any extra bytes are returned to the space info.
  Refill based on our delayed refs usage
  @fs_info: the filesystem
  @flush:   control how we can flush for this reservation.
  This will refill the delayed block_rsv up to 1 items size worth of space and
  will return -ENOSPC if we can't make the reservation.
  compare two delayed tree backrefs with same bytenr and type
  compare two delayed data backrefs with same bytenr and type
 insert a new ref to head ref rbtree 
  Find a head entry based on bytenr. This returns the delayed ref head if it
  was able to find one, or NULL if nothing was in that spot.  If return_bigger
  is given, the next bigger entry is returned if no exact match is found.
			
			  Can't have multiples of the same ref on a tree block.
 We don't have too many refs to merge for data. 
  Helper to insert the ref_node to the tail or merge with tail.
  Return 0 for insert.
  Return >0 for merge.
 Now we are sure we can merge 
 Need to change action 
 remove existing tail if its ref_mod is zero 
  helper function to update the accounting in the head ref
  existing and update must have the same bytenr
		 if the extent was freed and then
		  reallocated before the delayed ref
		  entries were processed, we can end up
		  with an existing head ref without
		  the must_insert_reserved flag set.
		  Set it again here
		
		  update the num_bytes so we make sure the accounting
		  is done correctly
	
	  update the reference mod on the head to reflect this new operation,
	  only need the lock for this case cause we could be processing it
	  currently, for refs we just added we know we're a-ok.
	
	  If we are going to from a positive ref mod to a negative or vice
	  versa we need to make sure to adjust pending_csums accordingly.
 If reserved is provided, it must be a data extent. 
	
	  The head node stores the sum of all the mods, so dropping a ref
	  should drop the sum in the head node by one.
	
	  BTRFS_ADD_DELAYED_EXTENT means that we need to update the reserved
	  accounting when the extent is finally added, or if a later
	  modification deletes the delayed ref without ever inserting the
	  extent into the extent allocation tree.  ref->must_insert_reserved
	  is the flag used to record that accounting mods are required.
	 
	  Once we record must_insert_reserved, switch the action to
	  BTRFS_ADD_DELAYED_REF because other special casing is not required.
  helper function to actually insert a head node into the rbtree.
  this does all the dirty work in terms of maintaining the correct
  overall modification count.
 Record qgroup extent info if provided 
		
		  we've updated the existing ref, free the newly
		  allocated ref
  init_delayed_ref_common - Initialize the structure which represents a
 			     modification to a an extent.
  @fs_info:    Internal to the mounted filesystem mount structure.
  @ref:	The structure which is going to be initialized.
  @bytenr:	The logical address of the extent for which a modification is
 		going to be recorded.
  @num_bytes:  Size of the extent whose modification is being recorded.
  @ref_root:	The id of the root where this modification has originated, this
 		can be either one of the well-known metadata trees or the
 		subvolume id which references this extent.
  @action:	Can be one of BTRFS_ADD_DELAYED_REFBTRFS_DROP_DELAYED_REF or
 		BTRFS_ADD_DELAYED_EXTENT
  @ref_type:	Holds the type of the extent which is being recorded, can be
 		one of BTRFS_SHARED_BLOCK_REF_KEYBTRFS_TREE_BLOCK_REF_KEY
 		when recording a metadata extent or BTRFS_SHARED_DATA_REF_KEY
 		BTRFS_EXTENT_DATA_REF_KEY when recording data extent
  add a delayed tree ref.  This does all of the accounting required
  to make sure the delayed ref is eventually processed before this
  transaction commits.
	
	  insert both the head node and the new ref without dropping
	  the spin lock
	
	  Need to update the delayed_refs_rsv with any changes we may have
	  made.
  add a delayed data ref. it's similar to btrfs_add_delayed_tree_ref.
	
	  insert both the head node and the new ref without dropping
	  the spin lock
	
	  Need to update the delayed_refs_rsv with any changes we may have
	  made.
	
	  Need to update the delayed_refs_rsv with any changes we may have
	  made.
  This does a simple search for the head node for a given extent.  Returns the
  head node if found, or NULL if not.
 SPDX-License-Identifier: GPL-2.0
 Maximum number of zones to report per blkdev_report_zones() call 
 Invalid allocation pointer value for missing devices 
 Pseudo write pointer value for conventional zone 
  Location of the first zone of superblock logging zone pairs.
  - primary superblock:    0B (zone 0)
  - first copy:          512G (zone starting at that offset)
  - second copy:           4T (zone starting at that offset)
 Number of superblock log zones 
  Minimum of active zones we need:
  - BTRFS_SUPER_MIRROR_MAX zones for superblock mirrors
  - 3 zones to ensure at least one zone per SYSTEM, META and DATA block group
  - 1 zone for tree-log dedicated block group
  - 1 zone for relocation
  Maximum supported zone size. Currently, SMR disks have a zone size of
  256MiB, and we are expecting ZNS drives to be in the 1-4GiB range. We do not
  expect the zone size to become larger than 8GiB in the near future.
	
	  Possible states of log buffer zones
	 
	            Empty[0]  In use[0]  Full[0]
	  Empty[1]                   x        0
	  In use[1]        0          x        0
	  Full[1]          1          1        C
	 
	  Log position:
	    : Special case, no superblock is written
	    0: Use write pointer of zones[0]
	    1: Use write pointer of zones[1]
	    C: Compare super blocks from zones[0] and zones[1], use the latest
	       one determined by generation
	    x: Invalid state
 Special case to distinguish no superblock to read 
 Compare two super blocks 
  Get the first zone number of the superblock mirror
  Emulate blkdev_report_zones() for a non-zoned device. It slices up the block
  device into static sized chunks and fake a conventional zone on each of
  them.
 The emulated zone size is determined from the size of device extent 
 No dev extents at all? Not good 
 fs_info->zone_size might not set yet. Use the incomapt flag here. 
 We can skip reading of zone info for missing devices 
	
	  Cannot use btrfs_is_zoned here, since fs_info::zone_size might not
	  yet be set.
 Check if it's power of 2 (see is_power_of_2) 
 We reject devices with a zone size larger than 8GB 
 Get zones type 
 Validate superblock log 
		
		  If zones[0] is conventional, always use the beginning of the
		  zone to record superblock. No need to validate in that case.
 Just in case 
 Count zoned devices 
		
		  A Host-Managed zoned device must be used as a zoned device.
		  A Host-Aware zoned device and a non-zoned devices can be
		  treated as a zoned device, if ZONED flag is enabled in the
		  superblock.
 No zoned block device found on ZONED filesystem 
	
	  stripe_size is always aligned to BTRFS_STRIPE_LEN in
	  btrfs_create_chunk(). Since we want stripe_len == zone_size,
	  check the alignment here.
	
	  Check mount options here, because we might change fs_info->zoned
	  from fs_info->zone_size.
	
	  Space cache writing is not COWed. Disable that to avoid write errors
	  in sequential zones.
		
		  For READ, we want the previous one. Move write pointer to
		  the end of a zone, if it is at the head of a zone.
	
	  For a zoned filesystem on a non-zoned block device, use the same
	  super block locations as regular filesystem. Doing so, the super
	  block can always be retrieved and the zoned flag of the volume
	  detected from the super block information.
 Advance the next zone 
			
			  No room left to write new superblock. Since
			  superblock is written with REQ_SYNC, it is safe to
			  finish the zone now.
			 
			  If the write pointer is exactly at the capacity,
			  explicit ZONE_FINISH is not necessary.
 All the zones are FULL. Should not reach here. 
  btrfs_find_allocatable_zones - find allocatable zones within a given region
  @device:	the device to allocate a region on
  @hole_start: the position of the hole to allocate the region
  @num_bytes:	size of wanted region
  @hole_end:	the end of the hole
  @return:	position of allocatable zones
  Allocatable region should not contain any superblock locations.
 Check if zones in the region are all empty 
 We also need to exclude regular superblock positions 
 We can use any number of zones 
 Active zone left? 
 Someone already set the bit 
 We can use any number of zones 
 All the zones are conventional 
 All the zones are sequential and empty 
 Free regions should be empty 
  Calculate an allocation pointer from the extent allocation information
  for a block group consist of conventional zones. It is pointed to the
  end of the highest addressed extent in the block group as an allocation
  offset.
 We should not find the exact match 
 Sanity check 
 Get the chunk mapping 
		
		  This zone will be used for allocation, so mark this zone
		  non-empty.
		
		  The group is mapped to a sequential zone. Get the zone write
		  pointer to determine the allocation offset within the zone.
 Partially used zone 
		
		  Consider a zone as active if we can allow any number of
		  active zones.
		
		  Avoid calling calculate_alloc_pointer() for new BG. It
		  is no use for new BG. It must be always 0.
		 
		  Also, we have a lock chain of extent buffer lock ->
		  chunk mutex.  For new BG, this function is called from
		  btrfs_make_block_group() which is already taking the
		  chunk mutex. Thus, we cannot call
		  calculate_alloc_pointer() which takes extent buffer
		  locks to avoid deadlock.
 Zone capacity is always zone size in emulation 
 single 
 non-single profiles are not supported yet 
 An extent is allocated after the write pointer 
 We only need ->free_space in ALLOC_SEQ block groups 
	
	  Using REQ_OP_ZONE_APPNED for relocation can break assumptions on the
	  extent layout the relocation code has.
	  Furthermore we have set aside own block-group from which only the
	  relocation "process" can allocate and make sure only one process at a
	  time can add pages to an extent that gets relocated, so it's safe to
	  use regular REQ_OP_WRITE for this special case.
 Zoned devices should not have partitions. So, we can assume it is 0 
 Missing device 
 Failing device 
  Synchronize write pointer in a zone at @physical_start on @tgt_dev, by
  filling zeros between @physical_pos to a write pointer of dev-replace
  source device.
 We only support single profile for now 
  Activate block group and underlying device zones
  @block_group: the block group to activate
  Return: true on success, false otherwise
 Currently support SINGLE profile only 
 No space left 
 Cannot activate the zone 
 Successfully activated all the zones 
 For the active block group list 
 Currently support SINGLE profile only 
 Check if we have unwritten allocated space 
 Ensure all writes in this block group finish 
 No need to wait for NOCOW writers. Zoned mode does not allow that. 
	
	  Bail out if someone already deactivated the block group, or
	  allocated space is left in the block group.
 For active_bg_list 
 Non-single profiles are not supported yet 
 Check if there is a device with active zones left 
 We should have consumed all the free space 
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2015 Facebook.  All rights reserved.
 Flip it to the other format and check that for good measure. 
	
	  Align some operations to a page to flush out bugs in the extent
	  buffer bitmap handling of highmem.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2013 Fusion IO.  All rights reserved.
  Build the most complicated map of extents the earth has ever seen.  We want
  this so we can test all of the corner cases of btrfs_get_extent.  Here is a
  diagram of how the extents will look though this may not be possible we still
  want to make sure everything acts normally (the last number is not inclusive)
  [0 - 5][5 -  6][     6 - 4096     ][ 4096 - 4100][4100 - 8195][8195 - 12291]
  [hole ][inline][hole but no extent][  hole   ][   regular ][regular1 split]
  [12291 - 16387][16387 - 24579][24579 - 28675][ 28675 - 32771][32771 - 36867 ]
  [    hole    ][regular1 split][   prealloc ][   prealloc1  ][prealloc1 written]
  [36867 - 45059][45059 - 53251][53251 - 57347][57347 - 61443][61443- 69635]
  [  prealloc1  ][ compressed  ][ compressed1 ][    regular  ][ compressed1]
  [69635-73731][   73731 - 86019   ][86019-90115]
  [  regular  ][ hole but no extent][  regular  ]
 First we want a hole 
	
	  Now we want an inline extent, I don't think this is possible but hey
	  why not?  Also keep in mind if we have an inline extent it counts as
	  the whole first page.  If we were to expand it we would have to cow
	  and we wouldn't have an inline extent anymore.
 Now another hole 
 Now for a regular extent 
	
	  Now for 3 extents that were split from a hole punch so we test
	  offsets properly.
 Now for a unwritten prealloc extent 
	
	  We want to jack up disk_bytenr a little more so the em stuff doesn't
	  merge our records.
	
	  Now for a partially written prealloc extent, basically the same as
	  the hole punch example above.  Ram_bytes never changes when you mark
	  extents written btw.
 Now a normal compressed extent 
 No merges 
 Now a split compressed extent 
 Now extents that have a hole but no hole extent 
 First with no extents 
	
	  All of the magic numbers are based on the mapping setup in
	  setup_file_extents, so if you change anything there you need to
	  update the comment and update the expected values below.
	
	  We don't test anything else for inline since it doesn't get set
	  unless we have a page for it to write into.  Maybe we should change
	  this?
 Regular extent 
 The next 3 are split extents 
 Prealloc extent 
 The next 3 are a half written prealloc extent 
 Now for the compressed extent 
 Split compressed extent 
 A hole between regular extents but no hole extent 
	
	  Currently we just return a length that we requested rather than the
	  length of the actual hole, if this changes we'll have to change this
	  test.
	
	  Need a blank inode item here just so we don't confuse
	  btrfs_get_extent.
 [BTRFS_MAX_EXTENT_SIZE] 
 [BTRFS_MAX_EXTENT_SIZE][sectorsize] 
 [BTRFS_MAX_EXTENT_SIZE2][sectorsize HOLE][the rest] 
 [BTRFS_MAX_EXTENT_SIZE][sectorsize] 
	
	  [BTRFS_MAX_EXTENT_SIZE+sectorsize][sectorsize HOLE][BTRFS_MAX_EXTENT_SIZE+sectorsize]
	
	 [BTRFS_MAX_EXTENT_SIZE+sectorsize][sectorsize][BTRFS_MAX_EXTENT_SIZE+sectorsize]
 [BTRFS_MAX_EXTENT_SIZE+4k][4K HOLE][BTRFS_MAX_EXTENT_SIZE+4k] 
	
	  Refill the hole again just for good measure, because I thought it
	  might fail and I'd rather satisfy my paranoia at this point.
 Empty 
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2013 Fusion IO.  All rights reserved.
  This test just does basic sanity checking, making sure we can add an extent
  entry and remove space from either end and the middle, and make sure we can
  remove space that covers adjacent extent entries.
 First just make sure we can remove an entire entry 
 Ok edge and middle cases now 
 Cleanup 
	
	  The first bitmap we have starts at offset 0 so the next one is just
	  at the end of the first bitmap.
 Test a bit straddling two bitmaps 
 This is the high grade jackassery 
	
	  First let's do something simple, an extent at the same offset as the
	  bitmap, but the free space completely in the extent and then
	  completely in the bitmap.
 Now to add back the extent entry and remove from the bitmap 
	
	  Ok so a little more evil, extent entry and bitmap at the same offset,
	  removing an overlapping chunk.
 Now with the extent entry offset into the bitmap 
	
	  This has blown up in the past, the extent entry starts before the
	  bitmap entry, but we're trying to remove an offset that falls
	  completely within the bitmap range and is in both the extent entry
	  and the bitmap entry, looks like this
	 
	    [ extent ]
	       [ bitmap ]
	         [ del ]
	
	  This blew up before, we have part of the free space in a bitmap and
	  then the entirety of the rest of the space in an extent.  This used
	  to return -EAGAIN back from btrfs_remove_extent, make sure this
	  doesn't happen.
 Used by test_steal_space_from_bitmap_to_extent(). 
 Used by test_steal_space_from_bitmap_to_extent(). 
 Used by test_steal_space_from_bitmap_to_extent(). 
	
	  Now lets confirm that there's absolutely no free space left to
	  allocate.
 And any allocation request, no matter how small, should fail now. 
 And no extent nor bitmap entries in the cache anymore. 
  Before we were able to steal free space from a bitmap entry to an extent
  entry, we could end up with 2 entries representing a contiguous free space.
  One would be an extent entry and the other a bitmap entry. Since in order
  to allocate space to a caller we use only 1 entry, we couldn't return that
  whole range to the caller if it was requested. This forced the caller to
  either assume ENOSPC or perform several smaller space allocations, which
  wasn't optimal as they could be spread all over the block group while under
  concurrency (extra overhead and fragmentation).
  This stealing approach is beneficial, since we always prefer to allocate
  from extent entries, both for clustered and non-clustered allocation
  requests.
	
	  For this test, we want to ensure we end up with an extent entry
	  immediately adjacent to a bitmap entry, where the bitmap starts
	  at an offset where the extent entry ends. We keep adding and
	  removing free space to reach into this state, but to get there
	  we need to reach a point where marking new free space doesn't
	  result in adding new extent entries or merging the new space
	  with existing extent entries - the space ends up being marked
	  in an existing bitmap that covers the new free space range.
	 
	  To get there, we need to reach the threshold defined set at
	  cache->free_space_ctl->extents_thresh, which currently is
	  256 extents on a x86_64 system at least, and a few other
	  conditions (check free_space_cache.c). Instead of making the
	  test much longer and complicated, use a "use_bitmap" operation
	  that forces use of bitmaps as soon as we have at least 1
	  extent entry.
	
	  Extent entry covering free space range [128Mb - 256Kb, 128Mb - 128Kb[
 Bitmap entry covering free space range [128Mb + 512Kb, 256Mb[ 
	
	  Now make only the first 256Kb of the bitmap marked as free, so that
	  we end up with only the following ranges marked as free space:
	 
	  [128Mb - 256Kb, 128Mb - 128Kb[
	  [128Mb + 512Kb, 128Mb + 768Kb[
 Confirm that only those 2 ranges are marked as free. 
	
	  Confirm that the bitmap range [128Mb + 768Kb, 256Mb[ isn't marked
	  as free anymore.
	
	  Confirm that the region [128Mb + 256Kb, 128Mb + 512Kb[, which is
	  covered by the bitmap, isn't marked as free.
	
	  Confirm that the region [128Mb, 128Mb + 256Kb[, which is covered
	  by the bitmap too, isn't marked as free either.
	
	  Now lets mark the region [128Mb, 128Mb + 512Kb[ as free too. But,
	  lets make sure the free space cache marks it as free in the bitmap,
	  and doesn't insert a new extent entry to represent this region.
 Confirm the region is marked as free. 
	
	  Confirm that no new extent entries or bitmap entries were added to
	  the cache after adding that free space region.
	
	  Now lets add a small free space region to the right of the previous
	  one, which is not contiguous with it and is part of the bitmap too.
	  The goal is to test that the bitmap entry space stealing doesn't
	  steal this space region.
	
	  Confirm that no new extent entries or bitmap entries were added to
	  the cache after adding that free space region.
	
	  Now mark the region [128Mb - 128Kb, 128Mb[ as free too. This will
	  expand the range covered by the existing extent entry that represents
	  the free space [128Mb - 256Kb, 128Mb - 128Kb[.
 Confirm the region is marked as free. 
	
	  Confirm that our extent entry didn't stole all free space from the
	  bitmap, because of the small 4Kb free space region.
	
	  So now we have the range [128Mb - 256Kb, 128Mb + 768Kb[ as free
	  space. Without stealing bitmap free space into extent entry space,
	  we would have all this free space represented by 2 entries in the
	  cache:
	 
	  extent entry covering range: [128Mb - 256Kb, 128Mb[
	  bitmap entry covering range: [128Mb, 128Mb + 768Kb[
	 
	  Attempting to allocate the whole free space (1Mb) would fail, because
	  we can't allocate from multiple entries.
	  With the bitmap free space stealing, we get a single extent entry
	  that represents the 1Mb free space, and therefore we're able to
	  allocate the whole free space at once.
	
	  All that remains is a sectorsize free space region in a bitmap.
	  Confirm.
	
	  Now test a similar scenario, but where our extent entry is located
	  to the right of the bitmap entry, so that we can check that stealing
	  space from a bitmap to the front of an extent entry works.
	
	  Extent entry covering free space range [128Mb + 128Kb, 128Mb + 256Kb[
 Bitmap entry covering free space range [0, 128Mb - 512Kb[ 
	
	  Now make only the last 256Kb of the bitmap marked as free, so that
	  we end up with only the following ranges marked as free space:
	 
	  [128Mb + 128b, 128Mb + 256Kb[
	  [128Mb - 768Kb, 128Mb - 512Kb[
 Confirm that only those 2 ranges are marked as free. 
	
	  Confirm that the bitmap range [0, 128Mb - 768Kb[ isn't marked
	  as free anymore.
	
	  Confirm that the region [128Mb - 512Kb, 128Mb[, which is
	  covered by the bitmap, isn't marked as free.
	
	  Now lets mark the region [128Mb - 512Kb, 128Mb[ as free too. But,
	  lets make sure the free space cache marks it as free in the bitmap,
	  and doesn't insert a new extent entry to represent this region.
 Confirm the region is marked as free. 
	
	  Confirm that no new extent entries or bitmap entries were added to
	  the cache after adding that free space region.
	
	  Now lets add a small free space region to the left of the previous
	  one, which is not contiguous with it and is part of the bitmap too.
	  The goal is to test that the bitmap entry space stealing doesn't
	  steal this space region.
	
	  Now mark the region [128Mb, 128Mb + 128Kb[ as free too. This will
	  expand the range covered by the existing extent entry that represents
	  the free space [128Mb + 128Kb, 128Mb + 256Kb[.
 Confirm the region is marked as free. 
	
	  Confirm that our extent entry didn't stole all free space from the
	  bitmap, because of the small 2  sectorsize free space region.
	
	  So now we have the range [128Mb - 768Kb, 128Mb + 256Kb[ as free
	  space. Without stealing bitmap free space into extent entry space,
	  we would have all this free space represented by 2 entries in the
	  cache:
	 
	  extent entry covering range: [128Mb, 128Mb + 256Kb[
	  bitmap entry covering range: [128Mb - 768Kb, 128Mb[
	 
	  Attempting to allocate the whole free space (1Mb) would fail, because
	  we can't allocate from multiple entries.
	  With the bitmap free space stealing, we get a single extent entry
	  that represents the 1Mb free space, and therefore we're able to
	  allocate the whole free space at once.
	
	  All that remains is 2  sectorsize free space region
	  in a bitmap. Confirm.
	
	  For ppc64 (with 64k page size), bytes per bitmap might be
	  larger than 1G.  To make bitmap test available in ppc64,
	  alloc dummy block group whose size cross bitmaps.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2013 Fusion IO.  All rights reserved.
 Shouldn't happen but that kind of thinking creates CVE's 
 Will be freed by btrfs_free_fs_roots 
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2017 Oracle.  All rights reserved.
  Test scenario:
  Suppose that no extent map has been loaded into memory yet, there is a file
  extent [0, 16K), followed by another file extent [16K, 20K), two dio reads
  are entering btrfs_get_extent() concurrently, t1 is reading [8K, 16K), t2 is
  reading [0, 8K)
      t1                            t2
   btrfs_get_extent()              btrfs_get_extent()
     -> lookup_extent_mapping()      ->lookup_extent_mapping()
     -> add_extent_mapping(0, 16K)
     -> return em
                                     ->add_extent_mapping(0, 16K)
                                     -> #handle -EEXIST
 Add [0, 16K) 
 Add [16K, 20K) following [0, 16K)  
 avoid merging 
 Add [0, 8K), should return [0, 16K) instead. 
  Test scenario:
  Reading the inline ending up with EEXIST, ie. read an inline
  extent and discard page cache and read it again.
 Add [0, 1K) 
 Add [4K, 8K) following [0, 1K)  
 Add [0, 1K) 
 Add [4K, 8K) 
 Add [0, 16K) 
	
	  Since bytes within em are contiguous, em->block_start is identical to
	  em->start.
  Test scenario:
  Suppose that no extent map has been loaded into memory yet.
  There is a file extent [0, 16K), two jobs are running concurrently
  against it, t1 is buffered writing to [4K, 8K) and t2 is doing dio
  read from [0, 4K) or [8K, 12K) or [12K, 16K).
  t1 goes ahead of t2 and adds em [4K, 8K) into tree.
          t1                       t2
   cow_file_range()	     btrfs_get_extent()
                             -> lookup_extent_mapping()
    -> add_extent_mapping()
                             -> add_extent_mapping()
 Add [0K, 8K) 
 Add [8K, 32K) 
 avoid merging 
 Add [0K, 32K) 
  Test scenario:
  Suppose that no extent map has been loaded into memory yet.
  There is a file extent [0, 32K), two jobs are running concurrently
  against it, t1 is doing dio write to [8K, 32K) and t2 is doing dio
  read from [0, 4K) or [4K, 8K).
  t1 goes ahead of t2 and splits em [0, 32K) to em [0K, 8K) and [8K 32K).
          t1                                t2
   btrfs_get_blocks_direct()	       btrfs_get_blocks_direct()
    -> btrfs_get_extent()              -> btrfs_get_extent()
        -> lookup_extent_mapping()
        -> add_extent_mapping()            -> lookup_extent_mapping()
           # load [0, 32K)
    -> btrfs_new_extent_direct()
        -> btrfs_drop_extent_cache()
           # split [0, 32K)
        -> add_extent_mapping()
           # add [8K, 32K)
                                           -> add_extent_mapping()
                                              # handle -EEXIST when adding
                                              # [0, 32K)
 Assume we won't have more than 5 physical stripes 
 Physical to logical addresses 
 Start at 4GiB logical address 
 For us 
 For the tree 
			
			  Test a chunk with 2 data stripes one of which
			  intersects the physical address of the super block
			  is correctly recognised.
			
			  Test that out-of-range physical addresses are
			  ignored
 SINGLE chunk type 
	
	  Note: the fs_info is not set up completely, we only need
	  fs_info::fsid for the tracepoint.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2013 Fusion IO.  All rights reserved.
 In this test we need at least 2 file extents at its maximum size 
	
	  Passing NULL as we don't have fs_info but tracepoints are not used
	  at this point
	
	  First go through and create and mark all of our pages dirty, we pin
	  everything to make sure our pages don't get evicted and screw up our
	  test.
	 Test this scenario
	  |--- delalloc ---|
	  |---  search  ---|
	
	  Test this scenario
	 
	  |--- delalloc ---|
	            |--- search ---|
 locked_page was unlocked above 
	
	  Test this scenario
	  |--- delalloc ---|
	                     |--- search ---|
	
	  Test this scenario
	  [------- delalloc -------|
	  [max_bytes]|-- search--|
	 
	  We are re-using our test_start from above since it works out well.
	
	  Now to test where we run into a page that is no longer dirty in the
	  range we want to find.
 We unlocked it in the previous test 
	
	  Currently if we fail to find dirty pages in the delalloc range we
	  will adjust max_bytes down to PAGE_SIZE and then re-search.  If
	  this changes at any point in the future we will need to fix this
	  tests expected behavior.
 Straddling pages test 
	
	  Generate a wonky pseudo-random bit pattern for the sake of not using
	  something repetitive that could miss some hypothetical off-by-n bug.
	
	  Test again for case where the tree block is sectorsize aligned but
	  not nodesize aligned.
 Test correct handling of empty tree 
	
	  Set 1M-4M allocdiscard and 32M-64M thus leaving a hole between
	  4M-32M
 Now add 32M-64M so that we have a hole between 4M-32M 
	
	  Request first hole starting at 12M, we should get 4M-32M
	
	  Search in the middle of allocated range, should get the next one
	  available, which happens to be unallocated -> 4M-32M
	
	  Set 64M-72M with CHUNK_ALLOC flag, then search for CHUNK_TRIMMED flag
	  being unset in this range, we should get the entry in range 64M-72M
	
	  Search in the middle of set range whose immediate neighbour doesn't
	  have the bits set so it must be returned
	
	  Search beyond any known range, shall return after last known range
	  and end should be -1
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2013 Facebook.  All rights reserved.
	
	  Since the test trans doesn't have the complicated delayed refs,
	  we can only call btrfs_qgroup_account_extent() directly to test
	  quota.
  Add a ref for two different roots to make sure the shared value comes out
  right, also remove one of the roots and make sure the exclusive count is
  adjusted properly.
	
	  We have BTRFS_FS_TREE_OBJECTID created already from the
	  previous test.
 We are using this root as our extent root 
	
	  Some of the paths we test assume we have a filled out fs_info, so we
	  just need to add the root in there so we don't panic.
	
	  Can't use bytenr 0, some things freak out
	  coughbackref walking codecough
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2013 Fusion IO.  All rights reserved.
	
	  Passing NULL trans here should be safe because we have plenty of
	  space in this leaf to split the item without having to split the
	  leaf.
	
	  Read the first slot, it should have the original key and contain only
	  'mary had a little'
 Do it again so we test memmoving the other items in the leaf 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2011 Novell Inc.
  Copyright (C) 2016 Red Hat, Inc.
		
		  One of the ancestor path elements in an absolute path
		  lookup in ovl_lookup_layer() could have been opaque and
		  that will stop further lookup in lower layers (d->stop=true)
		  But we have found an absolute redirect in decendant path
		  element and that should force continue lookup in lower
		  layers (reset d->stop).
	
	  A non-dir origin may be disconnected, which is fine, because
	  we only need it for its unique inode number.
 Don't decode a deleted empty directory 
 Check if directory belongs to the layer we are decoding from 
  Check validity of an overlay file handle buffer.
  Return 0 for a valid file handle.
  Return -ENODATA for "origin unknown".
  Return <0 for an invalid file handle.
 Treat larger version and unknown flags as "origin unknown" 
 Treat endianness mismatch as "origin unknown" 
 Zero size value means "copied up but origin unknown" 
	
	  Make sure that the stored uuid matches the uuid of the lower
	  layer where file handle will be decoded.
	  In case of uuid=off option just make sure that stored uuid is null.
		
		  Treat stale file handle to lower file as "origin unknown".
		  upper file handle could become stale when upper file is
		  unlinked and this information is needed to handle stale
		  index entries correctly.
 Recheck condition under lock 
 Don't support traversing automounts and other weirdness 
	
	  This dentry should be a regular file if previous layer lookup
	  found a metacopy dentry.
 Caught in a trap of overlapping layers 
 Counting down from the end, since the prefix can change 
 Verify we did not go off the rails 
		
		  If lower fs uuid is not unique among lower fs we cannot match
		  fh->uuid to layer.
  Verify that @fh matches the file handle stored in xattr @name.
  Return 0 on match, -ESTALE on mismatch, < 0 on error.
  Verify that @real dentry matches the file handle stored in xattr @name.
  If @set is true and there is no stored file handle, encode @real and store
  file handle in xattr @name.
  Return 0 on match, -ESTALE on mismatch, -ENODATA on no xattr, < 0 on error.
 Get upper dentry from index 
  Verify that an index entry name matches the origin file handle stored in
  OVL_XATTR_ORIGIN and that origin file handle can be decoded to lower path.
  Return 0 on match, -ESTALE on mismatch or stale origin, < 0 on error.
	
	  Whiteout index entries are used as an indication that an exported
	  overlay file handle should be treated as stale (i.e. after unlink
	  of the overlay inode). These entries contain no origin xattr.
	
	  Verifying directory index entries are not stale is expensive, so
	  only verify stale dir index if NFS export is enabled.
	
	  Directory index entries should have 'upper' xattr pointing to the
	  real upper dir. Non-dir index entries are hardlinks to the upper
	  real inode. For non-dir index, we can read the copy up origin xattr
	  directly from the index dentry, but for dir index we first need to
	  decode the upper directory.
		
		  Directory index entries with no 'upper' xattr need to be
		  removed. When dir index entry has a stale 'upper' xattr,
		  we assume that upper dir was removed and we treat the dir
		  index as orphan entry that needs to be whited out.
 Check if non-dir index is orphan and don't warn before cleaning it 
  Lookup in indexdir for the index entry of a lower real inode or a copy up
  origin inode. The index entry name is the hex representation of the lower
  inode file handle.
  If the index dentry in negative, then either no lower aliases have been
  copied up yet, or aliases have been copied up in older kernels and are
  not indexed.
  If the index dentry for a copy up origin inode is positive, but points
  to an inode different than the upper inode, then either the upper inode
  has been copied up and not indexed or it was indexed, but since then
  index dir was cleared. Either way, that index cannot be used to indentify
  the overlay inode.
 Lookup index by file handle for NFS export 
		
		  When index lookup is called with !verify for decoding an
		  overlay file handle, a whiteout index implies that decode
		  should treat file handle as stale and no need to print a
		  warning about it.
		
		  Index should always be of the same file type as origin
		  except for the case of a whiteout index. A whiteout
		  index should only exist if all lower aliases have been
		  unlinked, which means that finding a lower origin on lookup
		  whose index is a whiteout should be treated as an error.
 Verify that dir index 'upper' xattr points to upper dir 
  Returns next layer in stack starting from top.
  Returns -1 if this is the last layer.
 Fix missing 'origin' xattr 
			
			  Lookup copy up origin by decoding origin file handle.
			  We may get a disconnected dentry, which is fine,
			  because we only need to hold the origin inode in
			  cache and use its inode number.  We may even get a
			  connected dentry, that is not under any of the lower
			  layers root.  That is also fine for using it's inode
			  number - it's the same as if we held a reference
			  to a dentry in lower layer that was moved under us.
		
		  If no origin fh is stored in upper of a merge dir, store fh
		  of lower dir and set upper parent "impure".
		
		  When "verify_lower" feature is enabled, do not merge with a
		  lower dir that does not match a stored origin xattr. In any
		  case, only verified origin is used for index lookup.
		 
		  For non-dir dentry, if index=on, then ensure origin
		  matches the dentry found using path based lookup,
		  otherwise error out.
			
			  Do not store intermediate metacopy dentries in
			  lower chain, except top most lower metacopy dentry.
			  Continue the loop so that if there is an absolute
			  redirect on this dentry, poe can be reset to roe.
		
		  Following redirects can have security consequences: it's like
		  a symlink into the lower layer without the permission checks.
		  This is only a problem if the upper layer is untrusted (e.g
		  comes from an USB drive).  This can allow a non-readable file
		  or directory to become readable.
		 
		  Only following redirects when redirects are enabled disables
		  this attack vector when not necessary.
 Find the current layer on the root dentry 
	
	  For regular non-metacopy upper dentries, there is no lower
	  path based lookup, hence ctr will be zero. If a dentry is found
	  using ORIGIN xattr on upper, install it in stack.
	 
	  For metacopy dentry, path based lookup will find lower dentries.
	  Just make sure a corresponding data dentry has been found.
	
	  Always lookup index if there is no-upperdentry.
	 
	  For the case of upperdentry, we have set origin by now if it
	  needed to be set. There are basically three cases.
	 
	  For directories, lookup index by lower inode and verify it matches
	  upper inode. We only trust dir index if we verified that lower dir
	  matches origin, otherwise dir index entries may be inconsistent
	  and we ignore them.
	 
	  For regular upper, we already set origin if upper had ORIGIN
	  xattr. There is no verification though as there is no path
	  based dentry lookup in lower in this case.
	 
	  For metacopy upper, we set a verified origin already if index
	  is enabled and if upper had an ORIGIN xattr.
	 
	
	  If dentry is negative, then lower is positive iff this is a
	  whiteout.
 Negative upper -> positive lower 
 Positive upper -> have to look up lower to see whether it exists 
				
				  Assume something is there, we just couldn't
				  access it.
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2011 Novell Inc.
 It's an overlay file 
 Handle recursion 
 Hack!  Reuse ofs->layers as a vfsmount array before freeing it 
 Sync real dirty inodes in upper filesystem (if it exists) 
	
	  We have to always set the err, because the return value isn't
	  checked in syncfs, and instead indirectly return an error via
	  the sb's writeback errseq, which VFS inspects after this call.
	
	  Not called for sync(2) call or an emergency sync (SB_I_SKIP_SYNC).
	  All the super blocks will be iterated, including upper_sb.
	 
	  If this is a syncfs(2) call, then we do need to call
	  sync_filesystem() on upper_sb, but enough if we do it when being
	  called with wait == 1.
  ovl_statfs
  @sb: The overlayfs super block
  @buf: The struct kstatfs to fill in with stats
  Get the filesystem statistics.  As writes always target the upper layer
  filesystem pass the statfs to the upper filesystem (if it exists)
 Will this overlay be forced to mountremount ro? 
  ovl_show_options
  Prints the mount options for a given superblock.
  Returns zero; does not fail.
		
		  Does not make sense to have redirect creation without
		  redirect following.
 Workdirindex are useless in non-upper mount 
	
	  This is to make the logic below simpler.  It doesn't make any other
	  difference, since config->redirect_dir is only used for upper.
 Resolve metacopy -> redirect_dir dependency 
			
			  There was an explicit redirect_dir=... that resulted
			  in this conflict.
 Automatically enable redirect otherwise. 
 Resolve nfs_export -> index dependency 
			
			  There was an explicit index=off that resulted
			  in this conflict.
 Automatically enable index otherwise. 
 Resolve nfs_export -> !metacopy dependency 
			
			  There was an explicit metacopy=on that resulted
			  in this conflict.
			
			  There was an explicit nfs_export=on that resulted
			  in this conflict.
 Resolve userxattr -> !redirect && !metacopy dependency 
		
		  Silently disable default setting of redirect and metacopy.
		  This shall be the default in the future as well: these
		  options must be explicitly enabled if used together with
		  userxattr.
 Weird filesystem returning with hashed negative (kernfs)? 
		
		  Try to remove POSIX ACL xattrs from workdir.  We are good if:
		 
		  a) success (there was a POSIX ACL xattr and was removed)
		  b) -ENODATA (there was no POSIX ACL xattr)
		  c) -EOPNOTSUPP (POSIX ACL xattrs are not supported)
		 
		  There are various other error values that could effectively
		  mean that the xattr doesn't exist (e.g. -ERANGE is returned
		  if the xattr name is too long), but the set of filesystems
		  allowed as upper are limited to "normal" ones, where checking
		  for the above two errors is sufficient.
 Clear any inherited mode bits 
	
	  The inodes index feature and NFS export need to encode and decode
	  file handles, so they require that all layers support them.
	
	  Decoding origin file handle is required for persistent st_ino.
	  Without persistent st_ino, xino=auto falls back to xino=off.
 Check if lower fs has 32bit inode numbers 
 Workdir should not be subdir of upperdir and vice versa 
 Check that everything is OK before copy-up 
	
	  Check if sgid bit needs to be cleared (actual setacl operation will
	  be done with mounter's capabilities and so that won't do it for us).
 catch all 
  Determine how we treat concurrent use of upperdirworkdir based on the
  index feature. This is papering over mount leaks of container runtimes,
  for example, an old overlay mount is leaked and now its upperdir is
  attempted to be used as a lower layer in a new overlay mount.
 Upperdir path should not be ro 
 Don't inherit atime flags 
	
	  Inherit SB_NOSEC flag from upperdir.
	 
	  This optimization changes behavior when a security related attribute
	  (suidsgidsecurity.) is changed on an underlying layer.  This is
	  okay because we don't yet have guarantees in that case, but it will
	  need careful treatment once we want to honour changes to underlying
	  filesystems.
  Returns 1 if RENAME_WHITEOUT is supported, 0 if not supported and
  negative values if error is encountered.
 Name is inline and stable - using snapshot as a copy helper 
 Best effort cleanup of whiteout and temp file 
  Creates $workdirworkincompatvolatiledirty file if it is not already
  present.
	
	  Upper should support d_type, else whiteouts are visible.  Given
	  workdir and upper are on same fs, we can do iterate_dir() on
	  workdir. This check requires successful creation of workdir in
	  previous step.
 Check if upperwork fs supports O_TMPFILE 
 Check if upperwork fs supports RENAME_WHITEOUT 
	
	  Check if upperwork fs supports (trusted|user).overlay. xattr
		
		  xattr support is required for persistent st_ino.
		  Without persistent st_ino, xino=auto falls back to xino=off.
	
	  We allowed sub-optimal upper fs configuration and don't want to break
	  users over kernel upgrade, but we never allowed remote upper fs, so
	  we can enforce strict requirements for remote upper fs.
	
	  For volatile mount, create a incompatvolatiledirty file to keep
	  track of it.
 Check if upperwork fs supports file handles 
 Check if upper fs has 32bit inode numbers 
 NFS export of rw mount depends on index 
 Verify lower root is upper root origin 
 index dir will act also as workdir 
		
		  Verify upper root is exclusively associated with index dir.
		  Older kernels stored upper fh in ".overlay.origin"
		  xattr. If that xattr exists, verify that it is a match to
		  upper dir file handle. In any case, verify or set xattr
		  ".overlay.upper" to indicate that index may have
		  directory entries.
 Cleanup badstaleorphan index entries 
	
	  We allow using single lower with null uuid for index and nfs_export
	  for example to support those features with single lower squashfs.
	  To avoid regressions in setups of overlay with re-formatted lower
	  squashfs, do not allow decoding origin with lower null uuid unless
	  user opted-in to one of the new features that require following the
	  lower inode of non-dir upper.
		
		  We use uuid to associate an overlay lower file handle with a
		  lower layer, so we can accept lower fs with null uuid as long
		  as all lower layers with null uuid are on the same fs.
		  if we detect multiple lower fs with the same uuid, we
		  disable lower file handle decoding on all of them.
 Get a unique fsid for the layer 
 idxfsid 0 are reserved for upper fs even with lower only overlay 
	
	  All lower layers that share the same fs as upper layer, use the same
	  pseudo_dev as upper layer.  Allocate fs[0].pseudo_dev even for lower
	  only overlay to simplify ovl_fs_free().
	  is_lower will be set if upper fs is shared with a lower layer.
		
		  Check if lower root conflicts with this overlay layers before
		  checking if it is in-use as upperdirworkdir of "another"
		  mount, because we do not bother to check in ovl_is_inuse() if
		  the upperdirworkdir is in fact in-use by our
		  upperdirworkdir.
		
		  Make lower layers RO.  That way fchmodfchown on lower file
		  will fail instead of modifying lower fs.
	
	  When all layers on same fs, overlay can use real inode numbers.
	  With mount option "xino=<on|auto>", mounter declares that there are
	  enough free high bits in underlying fs to hold the unique fsid.
	  If overlayfs does encounter underlying inodes using the high xino
	  bits reserved for fsid, it emits a warning and uses the original
	  inode number or a non persistent inode number allocated from a
	  dedicated range.
		
		  This is a roundup of number of bits needed for encoding
		  fsid, where fsid 0 is reserved for upper fs (even with
		  lower only overlay) +1 extra bit is reserved for the non
		  persistent inode number range that is used for resolving
		  xino lower bits overflow.
  Check if this layer root is a descendant of:
  - another layer of this overlayfs instance
  - upperwork dir of any overlayfs instance
 Walk back ancestors to root (inclusive) looking for traps 
  Check if any of the layers or work dirs overlap.
		
		  Checking workbasedir avoids hitting ovl_is_inuse(parent) of
		  this instance and covers overlapping work and index dirs,
		  unless work or index dir have been moved since created inside
		  workbasedir.  In that case, we already have their traps in
		  inode cache and we will catch that case on lookup.
 Root inode uses upper st_inoi_ino 
 Root is always merge -> can have whiteouts 
 Is there a reason anyone would want not to share whiteouts? 
 Layer 0 is reserved for upper even if there's no upper 
 Assume underlaying fs uses 32bit inodes unless proven otherwise 
 allocdestroy_inode needed for setting up traps in inode cache 
 If the upper fs is nonexistent, we mark overlayfs ro too 
 Force ro mount with no index dir 
 Show index=off in procmounts for forced ro mount 
 Never override disk quota limits or use reserved space 
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2011 Novell Inc.
 Truncate should trigger data copy up as well 
		
		  We might have to translate ovl file into real file object
		  once use cases emerge.  For now, simply don't let underlying
		  filesystem rely on attr->ia_file
		
		  If open(O_TRUNC) is done, VFS calls ->setattr with ATTR_OPEN
		  set.  Overlayfs does not pass O_TRUNC flag to underlying
		  filesystem during open -> do not pass ATTR_OPEN.  This
		  disables optimization in fuse which assumes open(O_TRUNC)
		  already set file size to 0.  But we never passed O_TRUNC to
		  fuse.  So by clearing ATTR_OPEN, fuse will be forced to send
		  setattr request to server.
		
		  When all layers are on the same fs, all real inode
		  number are unique, so we use the overlay st_dev,
		  which is friendly to du -x.
		
		  All inode numbers of underlying fs should not be using the
		  high xinobits, so we use high xinobits to partition the
		  overlay st_ino address space. The high bits holds the fsid
		  (upper fsid is 0). The lowest xinobit is reserved for mapping
		  the non-persistent inode numbers range in case of overflow.
		  This way all overlay inode numbers are unique and use the
		  overlay st_dev.
 The inode could not be mapped to a unified st_ino address space 
		
		  Always use the overlay st_dev for directories, so 'find
		  -xdev' will scan the entire overlay mount and won't cross the
		  overlay mount boundaries.
		 
		  If not all layers are on the same fs the pair {real st_ino;
		  overlay st_dev} is not unique, so use the non persistent
		  overlay st_ino for directories.
		
		  For non-samefs setup, if we cannot map all layers st_ino
		  to a unified address space, we need to make sure that st_dev
		  is unique per underlying fs, so we use the unique anonymous
		  bdev assigned to the underlying fs.
 Report the effective immutableappend-only STATX flags 
	
	  For non-dir or same fs, we use st_ino of the copy up origin.
	  This guaranties constant st_devst_ino across copy up.
	  With xino feature and non-samefs, we use st_ino of the copy up
	  origin masked with high bits that represent the layer id.
	 
	  If lower filesystem supports NFS file handles, this also guaranties
	  persistent st_ino across mount cycle.
			
			  Lower hardlinks may be broken on copy up to different
			  upper files, so we cannot use the lower origin st_ino
			  for those different files, even for the same fs case.
			 
			  Similarly, several redirected dirs can point to the
			  same dir on a lower layer. With the "verify_lower"
			  feature, we do not use the lower origin st_ino, if
			  we haven't verified that this redirect is unique.
			 
			  With inodes index enabled, it is safe to use st_ino
			  of an indexed origin. The index validates that the
			  upper hardlink is not broken and that a redirected
			  dir is the only redirect to that origin.
			
			  If we are querying a metacopy dentry and lower
			  dentry is data dentry, then use the blocks we
			  queried just now. We don't have to do additional
			  vfs_getattr(). If lower itself is metacopy, then
			  additional vfs_getattr() is unavoidable.
			
			  If lower is not same as lowerdata or if there was
			  no origin on upper, we can end up here.
	
	  It's probably not worth it to count subdirs to get the
	  correct link count.  nlink=1 seems to pacify 'find' and
	  other utilities.
	
	  Return the overlay inode nlinks for indexed upper inodes.
	  Overlay inode nlink counts the union of the upper hardlinks
	  and non-covered lower hardlinks. It does not include the upper
	  index hardlink.
 Careful in RCU walk mode 
	
	  Check overlay inode with the creds of task and underlying inode
	  with creds of mounter
 Make sure mounter can read file for copy up later 
 copy cmtime 
 Never list private (.overlay) 
 List all non-trusted xattrs 
 list other trusted for superuser only 
 filter out private xattrs 
 underlying fs providing us with an broken xattr list? 
  Work around the fact that security_file_ioctl() takes a file argument.
  Introducing security_inode_fileattr_getset() hooks would solve this issue
  properly.
		
		  Store immutableappend-only flags in xattr and clear them
		  in upper fileattr (in case they were set by older kernel)
		  so children of "ovl-immutable" directories lower aliases of
		  "ovl-immutable" hardlinks could be copied up.
		  Clear xattr when flags are cleared.
		
		  Merge real inode flags with inode flags read from
		  overlay.protattr xattr
 Update ctime 
 Convert inode protection flags to fileattr flags 
 For O_DIRECT dentry_open() checks f_mapping->a_ops->direct_IO 
  It is possible to stack overlayfs instance on top of another
  overlayfs instance as lower layer. We need to annotate the
  stackable i_mutex locks according to stack level of the super
  block instance. An overlayfs instance can never be in stack
  depth 0 (there is always a real fs below it).  An overlayfs
  inode lock will use the lockdep annotation ovl_i_mutex_key[depth].
  For example, here is a snip from proclockdep_chains after
  dir_iterate of nested overlayfs:
  [...] &ovl_i_mutex_dir_key[depth]   (stack_depth=2)
  [...] &ovl_i_mutex_dir_key[depth]#2 (stack_depth=1)
  [...] &type->i_mutex_dir_key        (stack_depth=0)
  Locking order w.r.t ovl_want_write() is important for nested overlayfs.
  This chain is valid:
  - inode->i_rwsem			(inode_lock[2])
  - upper_mnt->mnt_sb->s_writers	(ovl_want_write[0])
  - OVL_I(inode)->lock			(ovl_inode_lock[2])
  - OVL_I(lowerinode)->lock		(ovl_inode_lock[1])
  And this chain is valid:
  - inode->i_rwsem			(inode_lock[2])
  - OVL_I(inode)->lock			(ovl_inode_lock[2])
  - lowerinode->i_rwsem		(inode_lock[1])
  - OVL_I(lowerinode)->lock		(ovl_inode_lock[1])
  But lowerinode->i_rwsem SHOULD NOT be acquired while ovl_want_write() is
  held, because it is in reverse order of the non-nested case using the same
  upper fs:
  - inode->i_rwsem			(inode_lock[1])
  - upper_mnt->mnt_sb->s_writers	(ovl_want_write[0])
  - OVL_I(inode)->lock			(ovl_inode_lock[1])
	
	  When d_ino is consistent with st_ino (samefs or i_ino has enough
	  bits to encode layer), set the same value used for st_ino to i_ino,
	  so inode number exposed via proclocks and a like will be
	  consistent with d_ino and st_ino values. An i_ino value inconsistent
	  with d_ino also causes nfsd readdirplus to fail.
	
	  For directory inodes on non-samefs with xino disabled or xino
	  overflow, we allocate a non-persistent inode number, to be used for
	  resolving st_ino collisions in ovl_map_dev_ino().
	 
	  To avoid ino collision with legitimate xino values from upper
	  layer (fsid 0), use the lowest xinobit to map the non
	  persistent inode numbers to the unified st_ino address space.
  With inodes index enabled, an overlay inode nlink counts the union of upper
  hardlinks and non-covered lower hardlinks. During the lifetime of a non-pure
  upper inode, the following nlink modifying operations can happen:
  1. Lower hardlink copy up
  2. Upper hardlink created, unlinked or renamed over
  3. Lower hardlink whiteout or renamed over
  For the first, copy up case, the union nlink does not change, whether the
  operation succeeds or fails, but the upper inode nlink may change.
  Therefore, before copy up, we store the union nlink value relative to the
  lower inode nlink in the index inode xattr .overlay.nlink.
  For the second, upper hardlink case, the union nlink should be incremented
  or decremented IFF the operation succeeds, aligned with nlink change of the
  upper inode. Therefore, before linkunlinkrename, we store the union nlink
  value relative to the upper inode nlink in the index inode.
  For the last, lower cover up case, we simplify things by preceding the
  whiteout or cover up with copy up. This makes sure that there is an index
  upper inode where the nlink xattr can be stored before the copied up upper
  entry is unlink.
  On-disk format for indexed nlink:
  nlink relative to the upper inode - "U[+-]NUM"
  nlink relative to the lower inode - "L[+-]NUM"
	
	  For directories, @strict verify from lookup path performs consistency
	  checks, so NULL lowerupper in dentry must match NULL lowerupper in
	  inode. Non @strict verify from NFS handle decode path passes NULL for
	  'unknown' lowerupper.
 Real lower dir moved to upper layer under us? 
 Lookup of an uncovered redirect origin? 
	
	  Allow non-NULL lower inode in ovl_inode even if lowerdentry is NULL.
	  This happens when finding a copied up overlay inode for a renamed
	  or hardlinked overlay dentry and lower dentry cannot be followed
	  by origin because lower fs does not support file handles.
	
	  Allow non-NULL __upperdentry in inode even if upperdentry is NULL.
	  This happens when finding a lower alias for a copied up hard link.
  Create an inode cache entry for layer root dir, that will intentionally
  fail ovl_verify_inode(), so any lookup that will find some layer root
  will fail.
 Conflicting layer roots? 
  Does overlay inode need to be hashed by lower inode?
 No, if pure upper 
 Yes, if already indexed 
 Yes, if won't be copied up 
 No, if lower hardlink is or will be broken on copy up 
 No, if non-indexed upper with NFS export 
 Otherwise, hash by lower inode for fsnotify 
	
	  Copy up origin (lower) may exist for non-indexed upper, but we must
	  not use lower as hash key if this is a broken hardlink.
			
			  Verify that the underlying files stored in the inode
			  match those in the dentry.
 Recalculate nlink for non-dir due to indexing 
 Lower hardlink that will be broken on copy up 
 Check for non-merge dir that may have whiteouts 
 Check for immutableappend-only inode flags in xattr 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2011 Novell Inc.
 counter is allowed to wrap, since temp dentries are ephemeral 
 caller holds i_mutex on workdir 
 Caller must hold i_mutex on both workdir and dir 
	
	  vfs_mkdir() may succeed and leave the dentry passed
	  to it unhashed and negative. If that happens, try to
	  lookup a new hashed and positive dentry.
 mkdir is special... 
		
		  Not quite sure if non-instantiated dentry is legal or not.
		  VFS doesn't seem to care so check and warn here.
	
	  Fail with -EIO when trying to create opaque dir and upper doesn't
	  support xattrs. ovl_rename() calls ovl_set_opaque_xerr(-EXDEV) to
	  return a specific error for noxattr case.
  Common operations required to be done after creation of file on upper.
  If @hardlink is false, then @inode is a pre-allocated inode, we may or
  may not use to instantiate the new dentry.
		
		  ovl_obtain_alias() can be called after ovl_create_real()
		  and before we get here, so we may get an inode from cache
		  with the same real upperdentry that is not the inode we
		  pre-allocated.  In this case we will use the cached inode
		  to instantiate the new dentry.
		 
		  XXX: if we ever use ovl_obtain_alias() to decode directory
		  file handles, need to use ovl_get_inode_locked() and
		  d_instantiate_new() here to prevent from creating two
		  hashed directory inode aliases.
 Force lookup of new upper hardlink to find its lower 
 Setting opaque here is just an optimization, allow to fail 
 dentry's upper doesn't match now, get rid of it 
	
	  mode could have been mutilated due to umask (e.g. sgid directory)
	
	  When linking a file with copy up origin into a new parent, mark the
	  new parent dir "impure".
 Preallocate inode to be used by ovl_get_inode() 
 Did we end up using the preallocated inode? 
 Don't allow creation of "whiteout" on overlay 
	
	  Keeping this dentry hashed would mean having to release
	  upperpathlowerpath, which could only be done if we are the
	  sole user of this dentry.  Too tricky...  Just unhash for
	  now.
 Try to find another, hashed alias 
	
	  Changes to underlying layers may cause i_nlink to lose sync with
	  reality.  In this case prevent the link count from going to zero
	  prematurely.
 No need to clean pure upper removed by vfs_rmdir() 
	
	  Copy ctime
	 
	  Note: we fail to update ctime if there was no copy-up, only a
	  whiteout
 If path is too long, fall back to userspace move 
 Absolute redirect: finished 
	
	  For non-dir hardlinked files, we need absolute redirects
	  in general as two upper hardlinks could be in different
	  dirs. We could put a relative redirect now and convert
	  it to absolute redirect later. But when nlink > 1 and
	  indexing is on, that means relative redirect needs to be
	  converted to absolute during copy up of another lower
	  hardllink as well.
	 
	  So without optimizing too much, just check if lower is
	  a hard link or not. If lower is hard link, put absolute
	  redirect.
 Fall back to userspace copy-up 
 Don't copy up directory trees 
 Whiteout source 
 Switch whiteouts 
		
		  When moving a merge dir or non-dir with copy up origin into
		  a new parent, we are marking the new parent dir "impure".
		  When ovl_iterate() iterates an "impure" upper dir, it will
		  lookup the origin inodes of the entries to fill d_ino.
 copy ctime: 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2017 Red Hat, Inc.
 No atime modificaton nor notify on underlying 
 Has it been copied up since we'd opened it? 
 Did the flags change since open? 
 No longer need these flags, so don't pass them on to underlying fs 
	
	  The two special cases below do not need to involve real fs,
	  so we can optimizing concurrent callers.
	
	  Overlay file f_pos is the master copy that is preserved
	  through copy up and modified on readwrite, but only real
	  fs knows how to SEEK_HOLESEEK_DATA and real fs may impose
	  limitations that are more strict than ->s_maxbytes for specific
	  files, so we use the real file to perform seeks.
 Actually acquired in ovl_write_iter() 
 Update mode 
 Update size 
 Pacify lockdep, same trick as done in aio_write() 
  Calling iter_file_splice_write() directly from overlay's f_op may deadlock
  due to lock order inversion between pipe->mutex in iter_file_splice_write()
  and file_start_write(real.file) in ovl_write_iter().
  So do everything ovl_write_iter() does and call iter_file_splice_write() on
  the real file.
 Update mode 
 Update size 
 Don't sync lower file for fear of receiving EROFS error 
 Update size 
 Update size 
	
	  Don't copy up because of a dedupe request, this wouldn't make sense
	  most of the time (data would be duplicated instead of deduplicated).
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2011 Novell Inc.
  Copyright (C) 2016 Red Hat, Inc.
  Check if underlying fs supports file handles and try to determine encoding
  type, in order to deduce maximum inode number used by fs.
  Return 0 if file handles are not supported.
  Return 1 (FILEID_INO32_GEN) if fs uses the default 32bit inode encoding.
  Return -1 if fs uses a non default encoding with unknown inode size.
 Index all files on copy up. For now only enabled for NFS export 
 Verify lower origin on lookup. For now only enabled for NFS export 
		
		  Non-dir dentry can hold lower dentry of its copy up origin.
  ovl_dentry_lower() could return either a data dentry or metacopy dentry
  depending on what is stored in lowerstack[0]. At times we need to find
  lower dentry which has data (and not metacopy dentry). This helper
  returns the lower data dentry.
 Return inode which contains lower data. Do not return metacopy 
 Return real inode which contains data. Does not return metacopy inode 
  For hard links and decoded file handles, it's possible for ovl_dentry_upper()
  to return positive, while there's no actual upper alias for the inode.
  Copy up code needs to know about the existence of the upper alias, so it
  can't use ovl_dentry_upper().
	
	  Pairs with smp_wmb() in ovl_set_upperdata(). Main user of
	  ovl_has_upperdata() is ovl_copy_up_meta_inode_data(). Make sure
	  if setting of OVL_UPPERDATA is visible, then effects of writes
	  before that are visible too.
	
	  Pairs with smp_rmb() in ovl_has_upperdata(). Make sure
	  if OVL_UPPERDATA flag is visible, then effects of write operations
	  before it are visible as well.
 Caller should hold ovl_inode->lock 
	
	  Make sure upperdentry is consistent before making it visible
	
	  Version is used by readdir code to keep cache consistent.
	  For merge dirs (or dirs with origin) all changes need to be noted.
	  For non-merge dirs, cache contains only impure entries (i.e. ones
	  which have been copied up and have origins), so only need to note
	  changes to impure entries.
 Copy mtimectime 
 O_NOATIME is an optimization, don't fail if not permitted 
 Caller should hold ovl_inode->lock 
	
	  Check if copy-up has happened as well as for upper alias (in
	  case of hard links) is there.
	 
	  Both checks are lockless:
	   - false negatives: will recheck under oi->lock
	   - false positives:
	     + ovl_dentry_upper() uses memory barriers to ensure the
	       upper dentry is up-to-date
	     + ovl_dentry_has_upper_alias() relies on locking of
	       upper parent i_rwsem to prevent reordering copy-up
	       with rename.
 Already copied up 
 Zero size value means "copied up but origin unknown" 
	
	  Do not fail when upper doesn't support xattrs.
	  Upper inodes won't have origin nor redirect xattr anyway.
 Reserved for future flags 
	
	  Initialize inode flags from overlay.protattr xattr and upper inode
	  flags.  If upper inode has those fileattr flags set (i.e. from old
	  kernel), we do not clear them on ovl_get_inode(), but we will clear
	  them on next fileattr_set().
	
	  Do not allow to set protection flags when upper doesn't support
	  xattrs, because we do not set those fileattr flags on upper inode.
	  Remove xattr if it exist and all protection flags are cleared.
 Mask out the fileattr flags that should not be set in upper inode 
  Caller must hold a reference to inode to prevent it from being freed while
  it is marked inuse.
  Does this overlay dentry need to be indexed on copy up?
 Index all files for NFS export and consistency verification 
 Index only lower hardlinks on copy up 
 Caller must hold OVL_I(inode)->lock 
		
		  We either have a bug with persistent union nlink or a lower
		  hardlink was added while overlay is mounted. Adding a lower
		  hardlink and then unlinking all overlay hardlinks would drop
		  overlay nlink to zero before all upper inodes are unlinked.
		  As a safety measure, when that situation is detected, set
		  the overlay nlink to the index inode nlink minus one for the
		  index entry itself.
 Whiteout orphan index to block future open by handle 
 Cleanup orphan index entries 
  Operations that change overlay inode and upper inode nlink need to be
  synchronized with copy up for persistent nlink accounting.
	
	  With inodes index is enabled, we store the union overlay nlink
	  in an xattr on the index inode. When whiting out an indexed lower,
	  we need to decrement the overlay persistent nlink, but before the
	  first copy up, we have no upper index inode to store the xattr.
	 
	  As a workaround, before whiteoutrename over an indexed lower,
	  copy up to create the upper index. Creating the upper index will
	  initialize the overlay nlink, so it could be dropped if unlink
	  or rename succeeds.
	 
	  TODO: implement metadata only index copy up when called with
	        ovl_copy_up_flags(dentry, O_PATH).
	
	  The overlay inode nlink should be incrementeddecremented IFF the
	  upper operation succeeds, along with nlink change of upper inode.
	  Therefore, before linkunlinkrename, we store the union nlink
	  value relative to the upper inode nlink in an upper inode xattr.
 Workdir should not be the same as upperdir 
 Workdir should not be subdir of upperdir and vice versa 
 err < 0, 0 if no metacopy xattr, 1 if metacopy xattr found 
 Only regular files can have metacopy xattr 
		
		  getxattr on user. may fail with EACCES in case there's no
		  read permission on the inode.  Not much we can do, other than
		  tell the caller that this is not a metacopy inode.
  ovl_sync_status() - Check fs sync status for volatile mounts
  Returns 1 if this is not a volatile mount and a real sync is required.
  Returns 0 if syncing can be skipped because mount is volatile, and no errors
  have occurred on the upperdir since the mount.
  Returns -errno if it is a volatile mount, and the error that occurred since
  the last mount. If the error code changes, it'll return the latest error
  code.
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2011 Novell Inc.
 underlying fs providing us with an broken xattr list? 
 Discard 
 Ignore failure to copy unknown xattrs 
 Ntfs-3g returns -EINVAL for "no fileattr support" 
	
	  We cannot set immutable and append-only flags on upper inode,
	  because we would not be able to link upper inode to upper dir
	  not set overlay private xattr on upper inode.
	  Store these flags in overlay.protattr xattr instead.
 Don't bother copying flags if none are set 
 Try to use clone_file_range to clone up within the same fs 
 Couldn't clone, so now we try to copy the data 
 Check if lower fs supports seek operation 
		
		  Fill zero for hole will cost unnecessary disk space
		  and meanwhile slow down the copy-up speed, so we do
		  an optimization for hole during copy-up, it relies
		  on SEEK_DATA implementation in lower fs so if lower
		  fs does not support it, copy-up will behave as before.
		 
		  Detail logic of hole detection as below:
		  When we detect next data position is larger than current
		  position we will skip that hole, otherwise we copy
		  data in the size of OVL_COPY_UP_CHUNK_SIZE. Actually,
		  it may not recognize all kind of holes and sometimes
		  only skips partial of hole area. However, it will be
		  enough for most of the use cases.
 Make sure the real fid stays 32bit aligned 
	
	  We encode a non-connectable file handle for non-dir, because we
	  only need to find the lower inode number and we don't want to pay
	  the price or reconnecting the dentry.
	
	  When we will want to decode an overlay dentry from this handle
	  and all layers are on the same fs, if we get a disconncted real
	  dentry when we decode fid, the only way to tell if we should assign
	  it to upperdentry or to lowerstack is by checking this flag.
	
	  When lower layer doesn't support export operations store a 'null' fh,
	  so we can use the overlay.origin xattr to distignuish between a copy
	  up and a pure upper inode.
	
	  Do not fail when upper doesn't support xattrs.
 Ignore -EPERM from setting "user." on symlinkspecial 
 Store file handle of @upper dir in @index dir entry 
  Create and install index entry.
  Caller must hold i_mutex on indexdir.
	
	  For now this is only used for creating index entry for directories,
	  because non-dir are copied up directly to index and then hardlinked
	  to upper dir.
	 
	  TODO: implement create index for non-dir, so we can call it when
	  encoding file handle for non-dir in case index does not exist.
 Directory not expected to be indexed before copy up 
 Mark parent "impure" because it may now contain non-pure upper 
 Restore timestamps on parent (best effort) 
	
	  Copy up data first and then xattrs. Writing data after
	  xattrs will remove security.capability xattr automatically.
		
		  Copy the fileattr inode flags that are the source of already
		  copied i_flags
	
	  Store identifier of lower inode in upper inode xattr to
	  allow lookup of the copy up origin inode.
	 
	  Don't set origin when we are breaking the association with a lower
	  hard link.
  Copyup using workdir to prepare temp file.  Used when copying up directories,
  special files or when upper fs doesn't support O_TMPFILE.
 Can't properly set mode on creation because of the umask 
 workdir and destdir could be the same when copying up to indexdir 
 Copyup using O_TMPFILE which does not require cross dir locking 
  Copy up a single dentry
  All renames start with copy up of source if necessary.  The actual
  rename will only proceed once the copy up was successful.  Copy up uses
  upper parent i_mutex for exclusion.  Since rename can change d_parent it
  is possible that the copy up will lock the old parent.  At that point
  the file will have already been copied up anyway.
	
	  Indexed non-dir is copied up directly to the index entry and then
	  hardlinked to upper dir. Indexed dir is copied up to indexdir,
	  then index entry is created and then copied up dir installed.
	  Copying dir up to indexdir instead of workdir simplifies locking.
 Disconnected dentry must be copied up to index dir 
		
		  Mark parent "impure" because it may now contain non-pure
		  upper
 Should we copyup with O_TMPFILE or with workdir? 
 Initialize nlink for copy up of disconnected dentry 
 Restore timestamps on parent (best effort) 
 Copy up data of an inode which was copied up metadata only in the past. 
	
	  Writing to upper file will clear security.capability xattr. We
	  don't want that to happen for normal copy-up operation.
 maybe truncate regular file. this has no effect on dirs 
 err < 0: interrupted, err > 0: raced with another copy-up 
	
	  With NFS export, copy up can get called for a disconnected non-dir.
	  In this case, we will copy up lower inode to index dir without
	  linking it to upper dir.
 find the topmost dentry not yet copied up 
 Copy up of disconnected dentry does not set upper alias 
 SPDX-License-Identifier: GPL-2.0-only
  Overlayfs NFS export support.
  Amir Goldstein <amir73il@gmail.com>
  Copyright (C) 2017-2018 CTERA Networks. All Rights Reserved.
  Before encoding a non-upper directory file handle from real layer N, we need
  to check if it will be possible to reconnect an overlay dentry from the real
  lower decoded dentry. This is done by following the overlay ancestry up to a
  "layer N connected" ancestor and verifying that all parents along the way are
  "layer N connectable". If an ancestor that is NOT "layer N connectable" is
  found, we need to copy up an ancestor, which is "layer N connectable", thus
  making that ancestor "layer N connected". For example:
  layer 1: a
  layer 2: abc
  The overlay dentry a is NOT "layer 2 connectable", because if dir a is
  copied up and renamed, upper dir a will be indexed by lower dir a from
  layer 1. The dir a from layer 2 will never be indexed, so the algorithm ()
  in ovl_lookup_real_ancestor() will not be able to lookup a connected overlay
  dentry from the connected lower dentry abc.
  To avoid this problem on decode time, we need to copy up an ancestor of
  abc, which is "layer 2 connectable", on encode time. That ancestor is
  ab. After copy up (and index) of ab, it will become "layer 2 connected"
  and when the time comes to decode the file handle from lower dentry abc,
  ovl_lookup_real_ancestor() will find the indexed ancestor ab and decoding
  a connected overlay dentry will be accomplished.
  () the algorithm in ovl_lookup_real_ancestor() can be improved to lookup an
  entry a in the lower layers above layer N and find the indexed dir a from
  layer 1. If that improvement is made, then the check for "layer N connected"
  will need to verify there are no redirects in lower layers above N. In the
  example above, a will be "layer 2 connectable". However, if layer 2 dir a
  is a target of a layer 1 redirect, then a will NOT be "layer 2 connectable":
  layer 1: A (redirect = a)
  layer 2: abc
 Return the lowest layer for encoding a connectable file handle 
 We can get overlay root from root of any layer 
	
	  If it's an unindexed merge dir, then it's not connectable with any
	  lower layer
 We can get upperoverlay path from indexedlower dentry 
  @dentry is "connected" if all ancestors up to root or a "connected" ancestor
  have the same uppermost lower layer as the origin's layer. We may need to
  copy up a "connectable" ancestor to make it "connected". A "connected" dentry
  cannot become non "connected", so cache positive result in dentry flags.
  Return the connected origin layer or < 0 on error.
 Find the topmost origin layer connectable ancestor of @dentry 
		
		  If @parent is not origin layer connectable, then copy up
		  @next which is origin layer connectable and we are done.
 If @parent is connected or indexed we are done 
  We only need to encode origin if there is a chance that the same object was
  encoded pre copy up and then we need to stay consistent with the same
  encoding also after copy up. If non-pure upper is not indexed, then it was
  copied up before NFS export was enabled. In that case we don't need to worry
  about staying consistent with pre copy up encoding and we encode an upper
  file handle. Overlay root dentry is a private case of non-indexed upper.
  The following table summarizes the different file handle encodings used for
  different overlay object types:
   Object type		| Encoding
  --------------------------------
   Pure upper		| U
   Non-indexed upper	| U
   Indexed upper	| L ()
   Non-upper		| L ()
  U = upper file handle
  L = lower file handle
  () Connecting an overlay dir from real lower dentry is not always
  possible when there are redirects in lower layers and non-indexed merge dirs.
  To mitigate those case, we may copy up the lower dir ancestor before encode
  a lower dir file handle.
  Return 0 for upper file handle, > 0 for lower file handle or < 0 on error.
 Upper file handle for pure upper 
	
	  Upper file handle for non-indexed upper.
	 
	  Root is never indexed, so if there's an upper layer, encode upper for
	  root.
	
	  Decoding a merge dir, whose origin's ancestor is under a redirected
	  lower dir or under a non-indexed upper is not always possible.
	  ovl_connect_layer() will try to make origin's layer "connected" by
	  copying up a "connectable" ancestor.
 Lower file handle for indexed and non-upper dirnon-dir 
	
	  Check if we should encode a lower or upper file handle and maybe
	  copy up an ancestor to make lower file handle connectable.
 Encode an upper or lower file handle 
 TODO: encode connectable file handles 
  Find or instantiate an overlay dentry from real dentries and index.
 We get overlay directory dentries with ovl_lookup_real() 
 Get the upper or lower dentry in stach whose on layer @idx 
  Lookup a child overlay dentry to get a connected overlay dentry whose real
  dentry is @real. If @real is on upper layer, we lookup a child overlay
  dentry with the same name as the real dentry. Otherwise, we need to consult
  index for lookup.
	
	  Lookup child overlay dentry by real name. The dir mutex protects us
	  from racing with overlay rename. If the overlay dentry that is above
	  real has already been moved to a parent that is not under the
	  connected overlay dir, we return -ECHILD and restart the lookup of
	  connected real path from the top.
	
	  We also need to take a snapshot of real dentry name to protect us
	  from racing with underlying layer rename. In this case, we don't
	  care about returning ESTALE, only from dereferencing a free name
	  pointer because we hold no lock on the real dentry.
  Lookup an indexed or hashed overlay dentry by real inode.
	
	  Decoding upper dir from index is expensive, so first try to lookup
	  overlay dentry in inodedcache.
	
	  For decoded lower dir file handle, lookup index by origin to check
	  if lower dir was copied up and andor removed.
 Get connected upper overlay dir from index 
		
		  ovl_lookup_real() in lower layer may call recursively once to
		  ovl_lookup_real() in upper layer. The first level call walks
		  back lower parents to the topmost indexed parent. The second
		  recursive call walks back from indexed upper to the topmost
		  connectedhashed upper parent (or up to root).
  Lookup an indexed or hashed overlay dentry, whose real dentry is an
  ancestor of @real.
 Find the topmost indexed or hashed ancestor 
		
		  Lookup a matching overlay dentry in inodedentry
		  cache or in index by real inode.
		
		  If @real has been moved out of the layer root directory,
		  we will eventully hit the real fs root. This cannot happen
		  by legit overlay rename, so we return error in that case.
  Lookup a connected overlay dentry whose real dentry is @real.
  If @real is on upper layer, we lookup a child overlay dentry with the same
  path the real dentry. Otherwise, we need to consult index for lookup.
 Find the topmost dentry not yet connected 
			
			  If real has been moved out of 'real_connected',
			  we will not find 'real_connected' and hit the layer
			  root. In that case, we need to restart connecting.
			  This game can go on forever in the worst case. We
			  may want to consider taking s_vfs_rename_mutex if
			  this happens more than once.
			
			  If real file has been moved out of the layer root
			  directory, we will eventully hit the real fs root.
			  This cannot happen by legit overlay rename, so we
			  return error in that case.
			
			  Lookup of child in overlay can fail when racing with
			  overlay rename of child away from 'connected' parent.
			  In this case, we need to restart the lookup from the
			  top, because we cannot trust that 'real_connected' is
			  still an ancestor of 'real'. There is a good chance
			  that the renamed overlay ancestor is now in cache, so
			  ovl_lookup_real_ancestor() will find it and we can
			  continue to connect exactly from where lookup failed.
  Get an overlay dentry from upperlower real dentries and index.
	
	  Obtain a disconnected overlay dentry from a non-dir real dentry
	  and index.
 Removed empty directory? 
	
	  If real dentry is connected and hashed, get a connected overlay
	  dentry whose real dentry is @real.
 First lookup overlay inode in inode cache by origin fh 
 Then lookup indexed upperwhiteout by origin fh 
 Then try to get a connected upper dir by index 
 Find origin.dentry again with ovl_acceptable() layer check 
 Get a connected non-upper dir or disconnected non-dir 
 If on-wire inner fid is aligned - nothing to do 
 Copy unaligned inner fh into aligned buffer 
 We may have needed to re-align OVL_FILEID_V0 
	
	  ovl_fh_to_dentry() returns connected dir overlay dentries and
	  ovl_fh_to_parent() is not implemented, so we should not get here.
	
	  ovl_fh_to_dentry() returns connected dir overlay dentries, so we
	  should not get here.
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2011 Novell Inc.
 Don't care if not doing ovl_iter() 
 Always recalc d_ino when remapping lower inode numbers 
 Always recalc d_ino for parent 
 If this is lower, then native d_ino will do 
	
	  Recalc d_ino for '.' and for all entries if dir is impure (contains
	  copied up entries)
 Defer setting d_ino for upper entry to ovl_iterate() 
 is_real can only become false when dir is copied up 
			
			  Insert lowest layer entries before upper ones, this
			  allows offsets to be reasonably constant
 Cursor is safe since the cache is stable 
 Map inode number to lower fs unique range 
	
	  The lowest xinobit is reserved for mapping the non-peresistent inode
	  numbers range, but this range is only exposed via st_ino, not here.
  Set d_ino for upper entries. Non-upper entries should always report
  the uppermost real inode ino and should not call this function.
  When not all layer are on same fs, report real ino also for upper.
  When all layers are on the same fs, and upper has a reference to
  copy up origin, call vfs_getattr() on the overlay entry to make
  sure that d_ino will be consistent with st_ino from stat(2).
 we shall not be moved 
 Mark a stale entry 
		
		  Directory inode is always on overlay st_dev.
		  Non-dir with ovl_same_dev() could be on pseudo st_dev in case
		  of xino bits overflow.
 Impure cache is not refcounted, free it here 
		
		  A good opportunity to get rid of an unneeded "impure" flag.
		  Removing the "impure" xattr is best effort.
	
	  Only upper dir can be impure, but if we are in the middle of
	  iterating a lower real dir, dir could be copied up and marked
	  impure. We only want the impure cache if we started iterating
	  a real upper dir to begin with.
		
		  If parent is merge, then need to adjust d_ino for '..', if
		  dir is impure then need to adjust d_ino for copied up
		  entries.
 ovl_cache_update_ino() sets is_whiteout on stale entry 
  Like ovl_real_fdget(), returns upperfile if dir was copied up since open.
  Unlike ovl_real_fdget(), this caches upperfile in file->private_data.
  TODO: use same abstract type for file->private_data of dir and file so
  upperfile could also be cached for files as well.
	
	  Need to check if we started out being a lower dir, but got copied up
 Nothing to sync for lower 
		
		  Select whiteouts in upperdir, they should
		  be cleared when deleting this directory.
 Even if d_type is not supported, DT_DIR is returned for . and .. 
  Returns 1 if d_type is supported, 0 not supportedunknown. Negative values
  if error is encountered.
	
	  The "workincompat" directory is treated specially - if it is not
	  empty, instead of printing a generic error and mounting read-only,
	  we will error about incompat features and fail the mount.
	 
	  When called from ovl_indexdir_cleanup(), path->dentry->d_name.name
	  starts with '#'.
 Cleanup leftover from index createcleanup attempt 
 Cleanup stale index entries 
			
			  Abort mount to avoid corrupting the index if
			  an incompatible index entry was found or on out
			  of memory.
			
			  Whiteout orphan index to block future open by
			  handle after overlay nlink dropped to zero.
 Cleanup orphan index entries 
 SPDX-License-Identifier: GPL-2.0+
  NILFS ioctl operations.
  Copyright (C) 2007, 2008 Nippon Telegraph and Telephone Corporation.
  Written by Koji Sato.
 capable() 
 copy_from_user(), copy_to_user() 
 compat_ptr() 
 mnt_want_write_file(), mnt_drop_write_file() 
  nilfs_ioctl_wrap_copy - wrapping function of getset metadata info
  @nilfs: nilfs object
  @argv: vector of arguments from userspace
  @dir: set of direction flags
  @dofunc: concrete function of getset metadata info
  Description: nilfs_ioctl_wrap_copy() getssets metadata info by means of
  calling dofunc() function on the basis of @argv argument.
  Return Value: On success, 0 is returned and requested metadata info
  is copied into userspace. On error, one of the following
  negative error codes is returned.
  %-EINVAL - Invalid arguments from userspace.
  %-ENOMEM - Insufficient amount of memory available.
  %-EFAULT - Failure during execution of requested operation.
	
	  Reject pairs of a start item position (argv->v_index) and a
	  total count (argv->v_nmembs) which leads position 'pos' to
	  overflow by the increment at the end of the loop.
  nilfs_fileattr_get - ioctl to support lsattr
  nilfs_fileattr_set - ioctl to support chattr
  nilfs_ioctl_getversion - get info about a file's version (generation number)
  nilfs_ioctl_change_cpmode - change checkpoint mode (checkpointsnapshot)
  @inode: inode object
  @filp: file object
  @cmd: ioctl's request code
  @argp: pointer on argument from userspace
  Description: nilfs_ioctl_change_cpmode() function changes mode of
  given checkpoint between checkpoint and snapshot state. This ioctl
  is used in chcp and mkcp utilities.
  Return Value: On success, 0 is returned and mode of a checkpoint is
  changed. On error, one of the following negative error codes
  is returned.
  %-EPERM - Operation not permitted.
  %-EFAULT - Failure during checkpoint mode changing.
 never fails 
  nilfs_ioctl_delete_checkpoint - remove checkpoint
  @inode: inode object
  @filp: file object
  @cmd: ioctl's request code
  @argp: pointer on argument from userspace
  Description: nilfs_ioctl_delete_checkpoint() function removes
  checkpoint from NILFS2 file system. This ioctl is used in rmcp
  utility.
  Return Value: On success, 0 is returned and a checkpoint is
  removed. On error, one of the following negative error codes
  is returned.
  %-EPERM - Operation not permitted.
  %-EFAULT - Failure during checkpoint removing.
 never fails 
  nilfs_ioctl_do_get_cpinfo - callback method getting info about checkpoints
  @nilfs: nilfs object
  @posp: pointer on array of checkpoint's numbers
  @flags: checkpoint mode (checkpoint or snapshot)
  @buf: buffer for storing checkponts' info
  @size: size in bytes of one checkpoint info item in array
  @nmembs: number of checkpoints in array (numbers and infos)
  Description: nilfs_ioctl_do_get_cpinfo() function returns info about
  requested checkpoints. The NILFS_IOCTL_GET_CPINFO ioctl is used in
  lscp utility and by nilfs_cleanerd daemon.
  Return value: count of nilfs_cpinfo structures in output buffer.
  nilfs_ioctl_get_cpstat - get checkpoints statistics
  @inode: inode object
  @filp: file object
  @cmd: ioctl's request code
  @argp: pointer on argument from userspace
  Description: nilfs_ioctl_get_cpstat() returns information about checkpoints.
  The NILFS_IOCTL_GET_CPSTAT ioctl is used by lscp, rmcp utilities
  and by nilfs_cleanerd daemon.
  Return Value: On success, 0 is returned, and checkpoints information is
  copied into userspace pointer @argp. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-EFAULT - Failure during getting checkpoints statistics.
  nilfs_ioctl_do_get_suinfo - callback method getting segment usage info
  @nilfs: nilfs object
  @posp: pointer on array of segment numbers
  @flags: not used
  @buf: buffer for storing suinfo array
  @size: size in bytes of one suinfo item in array
  @nmembs: count of segment numbers and suinfos in array
  Description: nilfs_ioctl_do_get_suinfo() function returns segment usage
  info about requested segments. The NILFS_IOCTL_GET_SUINFO ioctl is used
  in lssu, nilfs_resize utilities and by nilfs_cleanerd daemon.
  Return value: count of nilfs_suinfo structures in output buffer.
  nilfs_ioctl_get_sustat - get segment usage statistics
  @inode: inode object
  @filp: file object
  @cmd: ioctl's request code
  @argp: pointer on argument from userspace
  Description: nilfs_ioctl_get_sustat() returns segment usage statistics.
  The NILFS_IOCTL_GET_SUSTAT ioctl is used in lssu, nilfs_resize utilities
  and by nilfs_cleanerd daemon.
  Return Value: On success, 0 is returned, and segment usage information is
  copied into userspace pointer @argp. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-EFAULT - Failure during getting segment usage statistics.
  nilfs_ioctl_do_get_vinfo - callback method getting virtual blocks info
  @nilfs: nilfs object
  @posp: not used
  @flags: not used
  @buf: buffer for storing array of nilfs_vinfo structures
  @size: size in bytes of one vinfo item in array
  @nmembs: count of vinfos in array
  Description: nilfs_ioctl_do_get_vinfo() function returns information
  on virtual block addresses. The NILFS_IOCTL_GET_VINFO ioctl is used
  by nilfs_cleanerd daemon.
  Return value: count of nilfs_vinfo structures in output buffer.
  nilfs_ioctl_do_get_bdescs - callback method getting disk block descriptors
  @nilfs: nilfs object
  @posp: not used
  @flags: not used
  @buf: buffer for storing array of nilfs_bdesc structures
  @size: size in bytes of one bdesc item in array
  @nmembs: count of bdescs in array
  Description: nilfs_ioctl_do_get_bdescs() function returns information
  about descriptors of disk block numbers. The NILFS_IOCTL_GET_BDESCS ioctl
  is used by nilfs_cleanerd daemon.
  Return value: count of nilfs_bdescs structures in output buffer.
  nilfs_ioctl_get_bdescs - get disk block descriptors
  @inode: inode object
  @filp: file object
  @cmd: ioctl's request code
  @argp: pointer on argument from userspace
  Description: nilfs_ioctl_do_get_bdescs() function returns information
  about descriptors of disk block numbers. The NILFS_IOCTL_GET_BDESCS ioctl
  is used by nilfs_cleanerd daemon.
  Return Value: On success, 0 is returned, and disk block descriptors are
  copied into userspace pointer @argp. On error, one of the following
  negative error codes is returned.
  %-EINVAL - Invalid arguments from userspace.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-EFAULT - Failure during getting disk block descriptors.
  nilfs_ioctl_move_inode_block - prepare datanode block for moving by GC
  @inode: inode object
  @vdesc: descriptor of virtual block number
  @buffers: list of moving buffers
  Description: nilfs_ioctl_move_inode_block() function registers datanode
  buffer in the GC pagecache and submit read request.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - Requested block doesn't exist.
  %-EEXIST - Blocks conflict is detected.
  nilfs_ioctl_move_blocks - move valid inode's blocks during garbage collection
  @sb: superblock object
  @argv: vector of arguments from userspace
  @buf: array of nilfs_vdesc structures
  Description: nilfs_ioctl_move_blocks() function reads valid datanode
  blocks that garbage collector specified with the array of nilfs_vdesc
  structures and stores them into page caches of GC inodes.
  Return Value: Number of processed nilfs_vdesc structures or
  error code, otherwise.
			
			  Add the inode to GC inode list. Garbage Collection
			  is serialized and no two processes manipulate the
			  list simultaneously.
 The inode still remains in GC inode list 
  nilfs_ioctl_delete_checkpoints - delete checkpoints
  @nilfs: nilfs object
  @argv: vector of arguments from userspace
  @buf: array of periods of checkpoints numbers
  Description: nilfs_ioctl_delete_checkpoints() function deletes checkpoints
  in the period from p_start to p_end, excluding p_end itself. The checkpoints
  which have been already deleted are ignored.
  Return Value: Number of processed nilfs_period structures or
  error code, otherwise.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-EINVAL - invalid checkpoints.
  nilfs_ioctl_free_vblocknrs - free virtual block numbers
  @nilfs: nilfs object
  @argv: vector of arguments from userspace
  @buf: array of virtual block numbers
  Description: nilfs_ioctl_free_vblocknrs() function frees
  the virtual block numbers specified by @buf and @argv->v_nmembs.
  Return Value: Number of processed virtual block numbers or
  error code, otherwise.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - The virtual block number have not been allocated.
  nilfs_ioctl_mark_blocks_dirty - mark blocks dirty
  @nilfs: nilfs object
  @argv: vector of arguments from userspace
  @buf: array of block descriptors
  Description: nilfs_ioctl_mark_blocks_dirty() function marks
  metadata file or data blocks as dirty.
  Return Value: Number of processed block descriptors or
  error code, otherwise.
  %-ENOMEM - Insufficient memory available.
  %-EIO - IO error
  %-ENOENT - the specified block does not exist (hole block)
 XXX: use macro or inline func to check liveness 
 skip dead block 
		
		  can safely abort because checkpoints can be removed
		  independently.
		
		  can safely abort because DAT file is updated atomically
		  using a copy-on-write technique.
		
		  can safely abort because the operation is nondestructive.
  nilfs_ioctl_clean_segments - clean segments
  @inode: inode object
  @filp: file object
  @cmd: ioctl's request code
  @argp: pointer on argument from userspace
  Description: nilfs_ioctl_clean_segments() function makes garbage
  collection operation in the environment of requested parameters
  from userspace. The NILFS_IOCTL_CLEAN_SEGMENTS ioctl is used by
  nilfs_cleanerd daemon.
  Return Value: On success, 0 is returned or error code, otherwise.
	
	  argv[4] points to segment numbers this ioctl cleans.  We
	  use kmalloc() for its buffer because memory used for the
	  segment numbers is enough small.
	
	  nilfs_ioctl_move_blocks() will call nilfs_iget_for_gc(),
	  which will operates an inode list without blocking.
	  To protect the list from concurrent operations,
	  nilfs_ioctl_move_blocks should be atomic operation.
  nilfs_ioctl_sync - make a checkpoint
  @inode: inode object
  @filp: file object
  @cmd: ioctl's request code
  @argp: pointer on argument from userspace
  Description: nilfs_ioctl_sync() function constructs a logical segment
  for checkpointing.  This function guarantees that all modified data
  and metadata are written out to the device when it successfully
  returned.
  Return Value: On success, 0 is retured. On errors, one of the following
  negative error code is returned.
  %-EROFS - Read only filesystem.
  %-EIO - IO error
  %-ENOSPC - No space left on device (only in a panic state).
  %-ERESTARTSYS - Interrupted.
  %-ENOMEM - Insufficient memory available.
  %-EFAULT - Failure during execution of requested operation.
  nilfs_ioctl_resize - resize NILFS2 volume
  @inode: inode object
  @filp: file object
  @argp: pointer on argument from userspace
  Return Value: On success, 0 is returned or error code, otherwise.
  nilfs_ioctl_trim_fs() - trim ioctl handle function
  @inode: inode object
  @argp: pointer on argument from userspace
  Description: nilfs_ioctl_trim_fs is the FITRIM ioctl handle function. It
  checks the arguments from userspace and calls nilfs_sufile_trim_fs, which
  performs the actual trim operation.
  Return Value: On success, 0 is returned or negative error code, otherwise.
  nilfs_ioctl_set_alloc_range - limit range of segments to be allocated
  @inode: inode object
  @argp: pointer on argument from userspace
  Description: nilfs_ioctl_set_alloc_range() function defines lower limit
  of segments in bytes and upper limit of segments in bytes.
  The NILFS_IOCTL_SET_ALLOC_RANGE is used by nilfs_resize utility.
  Return Value: On success, 0 is returned or error code, otherwise.
  nilfs_ioctl_get_info - wrapping function of get metadata info
  @inode: inode object
  @filp: file object
  @cmd: ioctl's request code
  @argp: pointer on argument from userspace
  @membsz: size of an item in bytes
  @dofunc: concrete function of getting metadata info
  Description: nilfs_ioctl_get_info() gets metadata info by means of
  calling dofunc() function.
  Return Value: On success, 0 is returned and requested metadata info
  is copied into userspace. On error, one of the following
  negative error codes is returned.
  %-EINVAL - Invalid arguments from userspace.
  %-ENOMEM - Insufficient amount of memory available.
  %-EFAULT - Failure during execution of requested operation.
  nilfs_ioctl_set_suinfo - set segment usage info
  @inode: inode object
  @filp: file object
  @cmd: ioctl's request code
  @argp: pointer on argument from userspace
  Description: Expects an array of nilfs_suinfo_update structures
  encapsulated in nilfs_argv and updates the segment usage info
  according to the flags in nilfs_suinfo_update.
  Return Value: On success, 0 is returned. On error, one of the
  following negative error codes is returned.
  %-EPERM - Not enough permissions
  %-EFAULT - Error copying input data
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-EINVAL - Invalid values in input (segment number, flags or nblocks)
 never fails 
 SPDX-License-Identifier: GPL-2.0+
  NILFS inode file
  Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.
  Written by Amagai Yoshiji.
  Revised by Ryusuke Konishi.
  struct nilfs_ifile_info - on-memory private data of ifile
  @mi: on-memory private data of metadata file
  @palloc_cache: persistent object allocator cache of ifile
  nilfs_ifile_create_inode - create a new disk inode
  @ifile: ifile inode
  @out_ino: pointer to a variable to store inode number
  @out_bh: buffer_head contains newly allocated disk inode
  Return Value: On success, 0 is returned and the newly allocated inode
  number is stored in the place pointed by @ino, and buffer_head pointer
  that contains newly allocated disk inode structure is stored in the
  place pointed by @out_bh
  On error, one of the following negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOSPC - No inode left.
	req.pr_entry_nr = 0;  
			        0 says find free inode from beginning
			        of a group. dull code!!
  nilfs_ifile_delete_inode - delete a disk inode
  @ifile: ifile inode
  @ino: inode number
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - The inode number @ino have not been allocated.
  nilfs_ifile_count_free_inodes - calculate free inodes count
  @ifile: ifile inode
  @nmaxinodes: current maximum of available inodes count [out]
  @nfreeinodes: free inodes count [out]
  nilfs_ifile_read - read or get ifile inode
  @sb: super block instance
  @root: root object
  @inode_size: size of an inode
  @raw_inode: on-disk ifile inode
  @inodep: buffer to store the inode
 SPDX-License-Identifier: GPL-2.0+
  NILFS pathname lookup operations.
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Modified for NILFS by Amagai Yoshiji and Ryusuke Konishi.
   linuxfsext2namei.c
  Copyright (C) 1992, 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
   from
   linuxfsminixnamei.c
   Copyright (C) 1991, 1992  Linus Torvalds
   Big-endian to little-endian byte-swappingbitmaps by
         David S. Miller (davem@caip.rutgers.edu), 1995
  Methods themselves.
  By the time this is called, we already have created
  the directory cache entry for the new file, but it
  is so far negative - it has no inode.
  If the create succeeds, we fill in the inode information
  with d_instantiate().
 slow symlink 
 mark_inode_dirty(inode); 
 page_symlink() do this 
	
	  Like most other Unix systems, set the ctime for inodes on a
	  rename.
  Export operations
 SPDX-License-Identifier: GPL-2.0+
  NILFS module and super block management.
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Written by Ryusuke Konishi.
   linuxfsext2super.c
  Copyright (C) 1992, 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
   from
   linuxfsminixinode.c
   Copyright (C) 1991, 1992  Linus Torvalds
   Big-endian to little-endian byte-swappingbitmaps by
         David S. Miller (davem@caip.rutgers.edu), 1995
 nilfs_sufile_resize(), nilfs_sufile_set_alloc_range() 
  __nilfs_error() - report failure condition on a filesystem
  __nilfs_error() sets an ERROR_FS flag on the superblock as well as
  reporting an error message.  This function should be called when
  NILFS detects incoherences or defects of meta data on disk.
  This implements the body of nilfs_error() macro.  Normally,
  nilfs_error() should be used.  As for sustainable errors such as a
  single-shot IO error, nilfs_err() should be used instead.
  Callers should not add a trailing newline since this will do it.
			
			  sbp[0] points to newer log than sbp[1],
			  so copy sbp[0] to sbp[1] to take over sbp[0].
		
		  The latest segment becomes trailable from the position
		  written in superblock.
 update GC protection for recent segments 
 nilfs->ns_sem must be locked by the caller. 
 nilfs->ns_sem must be locked by the caller. 
 nilfs->ns_sem must be locked by the caller. 
 make sure store to ns_flushed_device cannot be reordered 
  nilfs_cleanup_super() - write filesystem state for cleanup
  @sb: super block instance to be unmounted or degraded to read-only
  This function restores state flags in the on-disk super block.
  This will set "clean" flag (i.e. NILFS_VALID_FS) unless the
  filesystem was not clean previously.
			
			  make the "clean" flag also to the opposite
			  super block if both super blocks point to
			  the same checkpoint.
  nilfs_move_2nd_super - relocate secondary super block
  @sb: super block instance
  @sb2off: new offset of the secondary super block (in bytes)
 array index of the secondary superblock 
 nilfs->ns_sem must be locked by the caller. 
 super block location is unchanged 
 Get new super block buffer 
 secondary super block will be restored to index 1 
  nilfs_resize_fs - resize the filesystem
  @sb: super block instance
  @newsize: new size of the filesystem (in bytes)
	
	  Write lock is required to protect some functions depending
	  on the number of segments, the number of reserved segments,
	  and so forth.
		
		  Drop NILFS_RESIZE_FS flag for compatibility with
		  mount-time resize which may be implemented in a
		  future release.
	
	  Reset the range of allocatable segments last.  This order
	  is important in the case of expansion because the secondary
	  superblock must be protected from log write until migration
	  completes.
 This function is called when super block should be written back 
 already attached checkpoint 
 Mark super block clean 
	
	  Compute all of the segment blocks
	 
	  The blocks before first segment and after last segment
	  are excluded.
	
	  Compute the overhead
	 
	  When distributing meta data blocks outside segment structure,
	  We must count them as the overhead.
			
			  If nilfs_palloc_count_max_entries() returns
			  -ERANGE error code then we simply treat
			  curent inodes count as maximum possible and
			  zero as free inodes value.
 Ordered data semantics 
 Strict in-order semantics 
 nilfs->ns_sem must be locked by the caller. 
 synchronize sbp[1] with sbp[0] 
 FS independent flags 
  nilfs_tree_is_busy() - try to shrink dentries of a checkpoint
  @root_dentry: root dentry of the tree to be shrunk
  This function returns true if the tree was in-use.
 protect recent checkpoints 
  nilfs_fill_super() - initialize a super block instance
  @sb: super_block
  @data: mount options
  @silent: silent mode flag
  This function is called exclusively by nilfs->ns_mount_mutex.
  So, the recovery process is protected from other simultaneous mounts.
 Shutting down log writer 
		
		  Remounting a valid RW partition RDONLY, so set
		  the RDONLY flag and then mark the partition as valid again.
		
		  Mounting a RDONLY partition read-write, so reread and
		  store the current valid flag.  (It may have been changed
		  by fsck since we originally mounted the partition.)
  nilfs_identify - pre-read mount options needed to identify mount instance
  @data: mount options
  @sd: nilfs_super_data
	
	  once the super is inserted into the list by sget, s_umount
	  will protect the lockfs code from trying to start a snapshot
	  while we are mounting
 New superblock instance created 
			
			  Try remount to setup mount states if the current
			  tree is not mounted and only snapshots use this sb.
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 SPDX-License-Identifier: GPL-2.0+
  NILFS inode operations.
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Written by Ryusuke Konishi.
  struct nilfs_iget_args - arguments used during comparison between inodes
  @ino: inode number
  @cno: checkpoint number
  @root: pointer on NILFS root object (mounted checkpoint)
  @for_gc: inode for GC flag
  nilfs_get_block() - get a file block on the filesystem (callback function)
  @inode - inode struct of the target file
  @blkoff - file block number
  @bh_result - buffer head to be mapped on
  @create - indicate whether allocating the block or not when it has not
       been allocated yet.
  This function does not issue actual read request of the specified data
  block. It is done by VFS.
 found 
 data block was not found 
				
				  The get_block() function could be called
				  from multiple callers for an inode.
				  However, the page having this block must
				  be locked in this case.
 never fails 
 Error handling should be detailed 
 Disk block number must be changed to proper value 
		
		  not found is not error (e.g. hole); must return without
		  the mapped state flag.
  nilfs_readpage() - implement readpage() method of nilfs_aops {}
  address_space_operations.
  @file - file struct of the file to be read
  @page - the page to be read
		
		  It means that filesystem was remounted in read-only
		  mode because of error or metadata corruption. But we
		  have dirty pages that try to be flushed in background.
		  So, here we simply discard this dirty page.
		
		  This page is locked by callers, and no other thread
		  concurrently marks its buffers dirty since they are
		  only dirtied through routines in fsbuffer.c in
		  which call sites of mark_buffer_dirty are protected
		  by page lock.
 Do not mark hole blocks dirty 
 Needs synchronization with the cleaner 
 .releasepage		= nilfs_releasepage, 
 reference count of i_bh inherits from nilfs_mdt_read_block() 
 No lock is needed; iget() ensures it. 
 ii->i_file_acl = 0; 
 ii->i_dir_acl = 0; 
		
		  Never occur.  When supporting nilfs_init_acl(),
		  proper cancellation of above jobs should be considered.
	iput(inode);  
		        raw_inode will be deleted through
		        nilfs_evict_inode().
 this inode is deleted 
 No lock is needed; iget() ensures it. 
 zero-fill unused portion in the case of super root block 
	
	  When extending inode, nilfs->ns_inode_size should be checked
	  for substitutions of appended fields.
		
		  XXX: call with has_bmap = 0 is a workaround to avoid
		  deadlock of bmap.  This delays update of i_bmap to just
		  before writing.
 64MB for 4KB block 
 never fails 
	
	  May construct a logical segment and may fail in sync mode.
	  But truncate has no return value.
	
	  Free resources allocated in nilfs_read_inode(), here.
 never fails 
 TODO: some of the following operations may fail.  
	
	  May construct a logical segment and may fail in sync mode.
	  But delete_inode has no return value.
 snapshot is not writable 
		
		  Because this routine may race with nilfs_dispose_list(),
		  we have to check NILFS_I_QUEUED here, too.
			
			  This will happen when somebody is freeing
			  this inode.
			return -EINVAL; 
					  NILFS_I_DIRTY may remain for
					  freeing inode.
  nilfs_dirty_inode - reflect changes on given inode to an inode block.
  @inode: inode of the file to be registered.
  nilfs_dirty_inode() loads a inode block containing the specified
  @inode and copies data from a nilfs_inode to a corresponding inode
  entry in the inode block. This operation is excluded from the segment
  construction. This function can be called both as a single operation
  and as a part of indivisible file operations.
 never fails 
 End of the current extent 
		
		  Limit the number of blocks that we look up so as
		  not to get into the next delayed allocation extent.
 error 
 HOLE 
 End of the current extent 
 The current extent goes on 
 Terminate the current extent 
 Start another extent 
 Start a new extent 
 If ret is 1 then we just hit the end of the extent array 
 SPDX-License-Identifier: GPL-2.0+
  NILFS segment constructor.
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Written by Ryusuke Konishi.
  Segment constructor
 Size of locally allocated inode vector 
#define SC_MAX_SEGDELTA 64   
			       Upper limit of the number of segments
			       appended in collection retry loop
 Construction mode 
 Make a logical segment having a super root 
	SC_LSEG_DSYNC,	
			  Flush data blocks of a given file and make
			  a logical segment without a super root.
	SC_FLUSH_FILE,	
			  Flush data files, leads to segment writes without
			  creating a checkpoint.
	SC_FLUSH_DAT,	
			  Flush DAT file.  This also creates segments
			  without a checkpoint.
 Stage numbers of dirty block collection 
 Collecting dirty blocks for GC 
 Super root 
 Data sync blocks 
  nilfs_sc_cstage_inc(), nilfs_sc_cstage_set(), nilfs_sc_cstage_get() are
  wrapper functions of stage count (nilfs_sc_info->sc_stage.scnt). Users of
  the variable must use them because transition of stage count must involve
  trace events (trace_nilfs2_collection_stage_transition).
  nilfs_sc_cstage_get() isn't required for the above purpose because it doesn't
  produce tracepoint events. It is provided just for making the intention
  clear.
 State flags of collection 
 Collecting node blocks 
 IFILE stage has started 
 segment usages has been freed 
 Operations depending on the construction mode and file type 
  Other definitions
		
		  If journal_info field is occupied by other FS,
		  it is saved and will be restored on
		  nilfs_transaction_commit().
  nilfs_transaction_begin - start indivisible file operations.
  @sb: super block
  @ti: nilfs_transaction_info
  @vacancy_check: flags for vacancy rate checks
  nilfs_transaction_begin() acquires a readerwriter semaphore, called
  the segment semaphore, to make a segment construction and write tasks
  exclusive.  The function is used with nilfs_transaction_commit() in pairs.
  The region enclosed by these two functions can be nested.  To avoid a
  deadlock, the semaphore is only acquired or released in the outermost call.
  This function allocates a nilfs_transaction_info struct to keep context
  information on it.  It is initialized and hooked onto the current task in
  the outermost call.  If a pre-allocated struct is given to @ti, it is used
  instead; otherwise a new struct is assigned from a slab.
  When @vacancy_check flag is set, this function will check the amount of
  free space, and will wait for the GC to reclaim disk space if low capacity.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error code is returned.
  %-ENOMEM - Insufficient memory available.
  %-ENOSPC - No space left on device
  nilfs_transaction_commit - commit indivisible file operations.
  @sb: super block
  nilfs_transaction_commit() releases the read semaphore which is
  acquired by nilfs_transaction_begin(). This is only performed
  in outermost call of this function.  If a commit flag is set,
  nilfs_transaction_commit() sets a timer to start the segment
  constructor.  If a sync flag is set, it starts construction
  directly.
  nilfs_segctor_reset_segment_buffer - reset the current segment buffer
  @sci: nilfs_sc_info
		return -E2BIG; 
				 The current segment is filled up
				 (internal code)
  Functions for making segment summary and payloads
 Size of finfo and binfo is enough small against blocksize 
 skip finfo 
 Substitution to vblocknr is delayed until update_blocknr() 
  Callback functions that enumerate, mark, and collect dirty blocks
		
		  A valid range is given for sync-ing data pages. The
		  range is rounded to per-page; extra dirty buffers
		  may be included if blocksize < pagesize.
 XXX: this interface will be changed 
		
		  The following code is duplicated with cpfile.  But, it is
		  needed to collect the checkpoint even if it was not newly
		  created.
 Remaining number of blocks within segment buffer 
 always receive -E2BIG or true error 
 dispose node list 
 Collect node 
 always receive -E2BIG or true error if n > rest 
 Pre-processes 
 sci->sc_stage.dirty_file_ptr = NILFS_I(inode); 
 XXX: required ? 
 Creating a checkpoint 
 Appending a super root 
 End of a logical segment 
  nilfs_segctor_begin_construction - setup segment buffer to make a new log
  @sci: nilfs_sc_info
  @nilfs: nilfs object
 Start from the head of a new full segment 
 Continue logs 
	
	  Since the segment specified with nextnum might be allocated during
	  the previous construction, the buffer including its segusage may
	  not be dirty.  The following call ensures that the buffer is dirty
	  and will pin the buffer on memory until the sufile is written.
 extend segment info 
 map this buffer to region of segment on-disk 
 allocate the next next full segment 
 never fails 
 never fails 
 Case 1: The first segment failed 
			
			  Case 1a:  Partial segment appended into an existing
			  segment
 Case 1b:  New full segment 
 never fails 
 Case 2: extended segment (!= next) failed 
 always succeed because the segusage is dirty 
 always succeed because the segusage is dirty 
 always succeed 
 Collection retry loop 
 The current segment is filled up 
 do not happen 
 The caller must release old_bh 
 file blocks 
		
		  For split b-tree node pages, this function may be called
		  twice.  We ignore the 2nd or later calls by this check.
		
		  For b-tree node pages, this function may be called twice
		  or more because they might be split in a segment.
			
			  For pages holding split b-tree node buffers, dirty
			  flag on the buffers may be cleared discretely.
			  In that case, the page is once redirtied for
			  remaining buffers, and it must be cancelled if
			  all the buffers get cleaned later.
 do not happen 
		
		  We assume that the buffers which belong to the same page
		  continue over the buffer list.
		  Under this assumption, the last BHs of pages is
		  identifiable by the discontinuity of bh->b_page
		  (page != fs_page).
		 
		  For B-tree node blocks, however, this assumption is not
		  guaranteed.  The cleanup code of B-tree node pages needs
		  special care.
	
	  Since pages may continue over multiple segment buffers,
	  end of the last page must be checked outside of the loop.
 Always redirty the buffer to avoid race condition
			
			  Defer calling iput() to avoid deadlocks if
			  i_nlink == 0 or mount is not yet finished.
  Main procedure of segment constructor
 Update time stamp 
 Avoid empty segment 
 Write partial segments 
			
			  At this point, we avoid double buffering
			  for blocksize < pagesize because page dirty
			  flag is turned off during write and dirty
			  buffers are not properly collected for
			  pages crossing over segments.
  nilfs_segctor_start_timer - set timer of background write
  @sci: nilfs_sc_info
  If the timer has already been set, it ignores the new request.
  This function MUST be called within a section locking the segment
  semaphore.
  nilfs_flush_segment - trigger a segment construction for resource control
  @sb: super block
  @ino: inode number of the file to be flushed out.
 assign bit 0 to data files 
  nilfs_construct_segment - construct a logical segment
  @sb: super block
  Return Value: On success, 0 is returned. On errors, one of the following
  negative error code is returned.
  %-EROFS - Read only filesystem.
  %-EIO - IO error
  %-ENOSPC - No space left on device (only in a panic state).
  %-ERESTARTSYS - Interrupted.
  %-ENOMEM - Insufficient memory available.
 A call inside transactions causes a deadlock. 
  nilfs_construct_dsync_segment - construct a data-only logical segment
  @sb: super block
  @inode: inode whose data blocks should be written out
  @start: start byte offset
  @end: end byte offset (inclusive)
  Return Value: On success, 0 is returned. On errors, one of the following
  negative error code is returned.
  %-EROFS - Read only filesystem.
  %-EIO - IO error
  %-ENOSPC - No space left on device (only in a panic state).
  %-ERESTARTSYS - Interrupted.
  %-ENOMEM - Insufficient memory available.
 data file only 
 DAT only 
  nilfs_segctor_accept - record accepted sequence count of log-write requests
  @sci: segment constructor object
  nilfs_segctor_notify - notify the result of request to caller threads
  @sci: segment constructor object
  @mode: mode of log forming
  @err: error code to be notified
 Clear requests (even when the construction failed) 
 re-enable timer if checkpoint creation was not done 
  nilfs_segctor_construct - form logs and write them to disk
  @sci: segment constructor object
  @mode: mode of log forming
	
	  Unclosed segment should be retried.  We do this using sc_timer.
	  Timeout of sc_timer will invoke complete construction which leads
	  to close the current logical segment.
  nilfs_segctor_thread - main loop of the segment constructor thread.
  @arg: pointer to a struct nilfs_sc_info.
  nilfs_segctor_thread() initializes a timer and serves as a daemon
  to execute segment constructions.
 start sync. 
 for nilfs_segctor_start_thread() 
 end sync. 
 for nilfs_segctor_kill_thread() 
  Setup & clean-up functions
	
	  The segctord thread was stopped and its timer was removed.
	  But some tasks remain.
  nilfs_segctor_destroy - destroy the segment constructor.
  @sci: nilfs_sc_info
  nilfs_segctor_destroy() kills the segctord thread and frees
  the nilfs_sc_info struct.
  Caller must hold the segment semaphore.
  nilfs_attach_log_writer - attach log writer
  @sb: super block instance
  @root: root object of the current filesystem tree
  This allocates a log writer object, initializes it, and starts the
  log writer.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error code is returned.
  %-ENOMEM - Insufficient memory available.
		
		  This happens if the filesystem was remounted
		  readwrite after nilfs_error degenerated it into a
		  read-only mount.
  nilfs_detach_log_writer - destroy log writer
  @sb: super block instance
  This kills log writer daemon, frees the log writer object, and
  destroys list of dirty files.
 Force to free the list of dirty files 
 SPDX-License-Identifier: GPL-2.0+
  NILFS directory entry operations
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Modified for NILFS by Amagai Yoshiji.
   linuxfsext2dir.c
  Copyright (C) 1992, 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
   from
   linuxfsminixdir.c
   Copyright (C) 1991, 1992  Linus Torvalds
   ext2 directory handling functions
   Big-endian to little-endian byte-swappingbitmaps by
         David S. Miller (davem@caip.rutgers.edu), 1995
  All code that works with directory layout had been switched to pagecache
  and moved here. AV
  nilfs uses block-sized chunks. Arguably, sector-sized ones would be
  more robust, but we have what we have
  Return the offset into page `page_nr' of the last valid
  byte in that page, plus one.
 do not happen 
 Too bad, we had an error 
  NOTE! unlike strncmp, nilfs_match returns 1 for success, 0 for failure.
  len <= NILFS_NAME_LEN and de != NULL are guaranteed by caller.
  p is at least 6 bytes before the end of page
 	nilfs_find_entry()
  finds an entry in the specified directory with the wanted name. It
  returns the page in which the entry was found, and the entry itself
  (as a parameter - res_dir). Page is returned mapped and unlocked.
  Entry is guaranteed to be valid.
 OFFSET_CACHE 
 next page is past the blocks we've got 
 Releases the page 
 	Parent is locked.
	
	  We take care of directory expansion in the same loop.
	  This code plays outside i_size, so it locks the page
	  to protect that region.
 We hit i_size 
 OFFSET_CACHE 
  nilfs_delete_entry deletes a directory entry by merging it with the
  previous entry. Page is up-to-date. Releases the page.
  Set the first fragment of directory.
  routine to check that the specified directory is empty (for rmdir)
 check for . and .. 
 CONFIG_COMPAT 
 SPDX-License-Identifier: GPL-2.0+
  NILFS segment usage file.
  Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.
  Written by Koji Sato.
  Revised by Ryusuke Konishi.
  struct nilfs_sufile_info - on-memory private data of sufile
  @mi: on-memory private data of metadata file
  @ncleansegs: number of clean segments
  @allocmin: lower limit of allocatable segment range
  @allocmax: upper limit of allocatable segment range
 number of clean segments 
 lower limit of allocatable segment range 
 upper limit of allocatable segment range 
  nilfs_sufile_get_ncleansegs - return the number of clean segments
  @sufile: inode of segment usage file
  nilfs_sufile_updatev - modify multiple segment usages at a time
  @sufile: inode of segment usage file
  @segnumv: array of segment numbers
  @nsegs: size of @segnumv array
  @create: creation flag
  @ndone: place to store number of modified segments on @segnumv
  @dofunc: primitive operation for the update
  Description: nilfs_sufile_updatev() repeatedly calls @dofunc
  against the given array of segments.  The @dofunc is called with
  buffers of a header block and the sufile block in which the target
  segment usage entry is contained.  If @ndone is given, the number
  of successfully modified segments from the head is stored in the
  place @ndone points to.
  Return Value: On success, zero is returned.  On error, one of the
  following negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - Given segment usage is in hole block (may be returned if
             @create is zero)
  %-EINVAL - Invalid segment usage number
 get different block 
  nilfs_sufile_set_alloc_range - limit range of segment to be allocated
  @sufile: inode of segment usage file
  @start: minimum segment number of allocatable region (inclusive)
  @end: maximum segment number of allocatable region (inclusive)
  Return Value: On success, 0 is returned.  On error, one of the
  following negative error codes is returned.
  %-ERANGE - invalid segment region
  nilfs_sufile_alloc - allocate a segment
  @sufile: inode of segment usage file
  @segnump: pointer to segment number
  Description: nilfs_sufile_alloc() allocates a clean segment.
  Return Value: On success, 0 is returned and the segment number of the
  allocated segment is stored in the place pointed by @segnump. On error, one
  of the following negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOSPC - No clean segment left.
				
				  wrap around in the limited region.
				  if allocation started from
				  sui->allocmin, this never happens.
 never happens 
 found a clean segment 
 no segments left 
 make the segment garbage 
  nilfs_sufile_mark_dirty - mark the buffer having a segment usage dirty
  @sufile: inode of segment usage file
  @segnum: segment number
  nilfs_sufile_set_segment_usage - set usage of a segment
  @sufile: inode of segment usage file
  @segnum: segment number
  @nblocks: number of live blocks in the segment
  @modtime: modification time (option)
  nilfs_sufile_get_stat - get segment usage statistics
  @sufile: inode of segment usage file
  @sustat: pointer to a structure of segment usage statistics
  Description: nilfs_sufile_get_stat() returns information about segment
  usage.
  Return Value: On success, 0 is returned, and segment usage information is
  stored in the place pointed by @sustat. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  nilfs_sufile_truncate_range - truncate range of segment array
  @sufile: inode of segment usage file
  @start: start segment number (inclusive)
  @end: end segment number (inclusive)
  Return Value: On success, 0 is returned.  On error, one of the
  following negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-EINVAL - Invalid number of segments specified
  %-EBUSY - Dirty or active segments are present in the range
 hole 
 make hole 
  nilfs_sufile_resize - resize segment array
  @sufile: inode of segment usage file
  @newnsegs: new number of segments
  Return Value: On success, 0 is returned.  On error, one of the
  following negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOSPC - Enough free space is not left for shrinking
  %-EBUSY - Dirty or active segments exist in the region to be truncated
 newnsegs < nsegs  {
  nilfs_sufile_get_suinfo -
  @sufile: inode of segment usage file
  @segnum: segment number to start looking
  @buf: array of suinfo
  @sisz: byte size of suinfo
  @nsi: size of suinfo array
  Description:
  Return Value: On success, 0 is returned and .... On error, one of the
  following negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
 hole 
  nilfs_sufile_set_suinfo - sets segment usage info
  @sufile: inode of segment usage file
  @buf: array of suinfo_update
  @supsz: byte size of suinfo_update
  @nsup: size of suinfo_update array
  Description: Takes an array of nilfs_suinfo_update structs and updates
  segment usage accordingly. Only the fields indicated by the sup_flags
  are updated.
  Return Value: On success, 0 is returned. On error, one of the
  following negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-EINVAL - Invalid values in input (segment number, flags or nblocks)
			
			  Active flag is a virtual flag projected by running
			  nilfs kernel code - drop it not to write it to
			  disk.
 get different block 
  nilfs_sufile_trim_fs() - trim ioctl handle function
  @sufile: inode of segment usage file
  @range: fstrim_range structure
  start:	First Byte to trim
  len:		number of Bytes to trim from start
  minlen:	minimum extent length in Bytes
  Decription: nilfs_sufile_trim_fs goes through all segments containing bytes
  from start to start+len. start is rounded up to the next block boundary
  and start+len is rounded down. For each clean segment blkdev_issue_discard
  function is invoked.
  Return Value: On success, 0 is returned or negative error code, otherwise.
	
	  range->len can be very large (actually, it is set to
	  ULLONG_MAX by default) - truncate upper end of the range
	  carefully so as not to overflow.
 hole 
 start new extent 
 add to previous extent 
 discard previous extent 
 start new extent 
 discard last extent 
  nilfs_sufile_read - read or get sufile inode
  @sb: super block instance
  @susize: size of a segment usage entry
  @raw_inode: on-disk sufile inode
  @inodep: buffer to store the inode
 SPDX-License-Identifier: GPL-2.0+
  Bufferpage management specific to NILFS
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Written by Ryusuke Konishi and Seiji Kihara.
  nilfs_forget_buffer - discard dirty state
  @bh: buffer head of the buffer to be discarded
  nilfs_copy_buffer -- copy buffer data and flags
  @dbh: destination buffer
  @sbh: source buffer
  nilfs_page_buffers_clean - check if a page has dirty buffers or not.
  @page: page to be checked
  nilfs_page_buffers_clean() returns zero if the page has dirty buffers.
  Otherwise, it returns non-zero value.
  nilfs_copy_page -- copy the page with buffers
  @dst: destination page
  @src: source page
  @copy_dirty: flag whether to copy dirty states on the page's buffer heads.
  This function is for both data pages and btnode pages.  The dirty flag
  should be treated by caller.  The page must not be under io.
  Both src and dst page must be locked
 No empty page is added to the page cache 
  nilfs_copy_back_pages -- copy back pages to original cache from shadow cache
  @dmap: destination page cache
  @smap: source page cache
  No pages must be added to the cache during this process.
  This must be ensured by the caller.
 overwrite existing page in the destination cache 
 Do we not need to remove page from smap here? 
 move the page to the destination cache 
 Probably -ENOMEM 
  nilfs_clear_dirty_pages - discard dirty pages in address space
  @mapping: address space with dirty pages for discarding
  @silent: suppress [true] or print [false] warning messages
  nilfs_clear_dirty_page - discard dirty page
  @page: dirty page that will be discarded
  @silent: suppress [true] or print [false] warning messages
  NILFS2 needs clear_page_dirty() in the following two cases:
  1) For B-tree node pages and data pages of the datgcdat, NILFS2 clears
     page dirty flags when it copies back pages from the shadow cache
     (gcdat->{i_mapping,i_btnode_cache}) to its original cache
     (dat->{i_mapping,i_btnode_cache}).
  2) Some B-tree operations like insertion or deletion may dispose buffers
     in dirty state, and this needs to cancel the dirty state of their pages.
  nilfs_find_uncommitted_extent - find extent of uncommitted data
  @inode: inode
  @start_blk: start block offset (in)
  @blkoff: start offset of the found extent (out)
  This function searches an extent of buffers marked "delayed" which
  starts from a block offset equal to or larger than @start_blk.  If
  such an extent was found, this will store the start offset in
  @blkoff and return its length in blocks.  Otherwise, zero is
  returned.
 SPDX-License-Identifier: GPL-2.0+
  Dummy inodes to buffer blocks for garbage collection
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Written by Seiji Kihara, Amagai Yoshiji, and Ryusuke Konishi.
  Revised by Ryusuke Konishi.
  This file adds the cache of on-disk blocks to be moved in garbage
  collection.  The disk blocks are held with dummy inodes (called
  gcinodes), and this file provides lookup function of the dummy
  inodes and their buffer read function.
  Buffers and pages held by the dummy inodes will be released each
  time after they are copied to a new log.  Dirty blocks made on the
  current generation and the blocks to be moved by GC never overlap
  because the dirty blocks make a new generation; they rather must be
  written individually.
  nilfs_gccache_submit_read_data() - add data buffer and submit read request
  @inode - gc inode
  @blkoff - dummy offset treated as the key for the page cache
  @pbn - physical block number of the block
  @vbn - virtual block number of the block, 0 for non-virtual block
  @out_bh - indirect pointer to a buffer_head struct to receive the results
  Description: nilfs_gccache_submit_read_data() registers the data buffer
  specified by @pbn to the GC pagecache with the key @blkoff.
  This function sets @vbn (@pbn if @vbn is zero) in b_blocknr of the buffer.
  Return Value: On success, 0 is returned. On Error, one of the following
  negative error code is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - The block specified with @pbn does not exist.
 -EIO, -ENOMEM, -ENOENT 
  nilfs_gccache_submit_read_node() - add node buffer and submit read request
  @inode - gc inode
  @pbn - physical block number for the block
  @vbn - virtual block number for the block
  @out_bh - indirect pointer to a buffer_head struct to receive the results
  Description: nilfs_gccache_submit_read_node() registers the node buffer
  specified by @vbn to the GC pagecache.  @pbn can be supplied by the
  caller to avoid translation of the disk block address.
  Return Value: On success, 0 is returned. On Error, one of the following
  negative error code is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
 internal code (cache hit) 
  nilfs_remove_all_gcinodes() - remove all unprocessed gc inodes
 SPDX-License-Identifier: GPL-2.0+
  Meta data file for NILFS
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Written by Ryusuke Konishi.
 nilfs_palloc_destroy_cache() 
 Caller exclude read accesses using page lock 
 set_buffer_new(bh); 
 internal code 
 mode == READ 
 internal code 
 abort readahead if bmap lookup failed 
  nilfs_mdt_get_block - read or create a buffer on meta data file.
  @inode: inode of the meta data file
  @blkoff: block offset
  @create: create flag
  @init_block: initializer used for newly allocated block
  @out_bh: output of a pointer to the buffer_head
  nilfs_mdt_get_block() looks up the specified buffer and tries to create
  a new buffer if @create is not zero.  On success, the returned buffer is
  assured to be either existing or formatted using a buffer lock on success.
  @out_bh is substituted only when zero is returned.
  Return Value: On success, it returns 0. On error, the following negative
  error code is returned.
  %-ENOMEM - Insufficient memory available.
  %-EIO - IO error
  %-ENOENT - the specified block does not exist (hole block)
  %-EROFS - Read only filesystem (for create mode)
 Should be rewritten with merging nilfs_mdt_read_block() 
 create = 0;   
  nilfs_mdt_find_block - find and get a buffer on meta data file.
  @inode: inode of the meta data file
  @start: start block offset (inclusive)
  @end: end block offset (inclusive)
  @blkoff: block offset
  @out_bh: place to store a pointer to buffer_head struct
  nilfs_mdt_find_block() looks up an existing block in range of
  [@start, @end] and stores pointer to a buffer head of the block to
  @out_bh, and block offset to @blkoff, respectively.  @out_bh and
  @blkoff are substituted only when zero is returned.
  Return Value: On success, it returns 0. On error, the following negative
  error code is returned.
  %-ENOMEM - Insufficient memory available.
  %-EIO - IO error
  %-ENOENT - no block was found in the range
  nilfs_mdt_delete_block - make a hole on the meta data file.
  @inode: inode of the meta data file
  @block: block offset
  Return Value: On success, zero is returned.
  On error, one of the following negative error code is returned.
  %-ENOMEM - Insufficient memory available.
  %-EIO - IO error
  nilfs_mdt_forget_block - discard dirty state and try to remove the page
  @inode: inode of the meta data file
  @block: block offset
  nilfs_mdt_forget_block() clears a dirty flag of the specified buffer, and
  tries to release the page including the buffer from a page cache.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error code is returned.
  %-EBUSY - page has an active buffer.
  %-ENOENT - page cache has no page addressed by the offset.
		
		  It means that filesystem was remounted in read-only
		  mode because of error or metadata corruption. But we
		  have dirty pages that try to be flushed in background.
		  So, here we simply discard this dirty page.
  nilfs_mdt_clear - do cleanup for the metadata file
  @inode: inode of the metadata file
  nilfs_mdt_destroy - release resources used by the metadata file
  @inode: inode of the metadata file
 kfree(NULL) is safe 
  nilfs_mdt_setup_shadow_map - setup shadow map and bind it to metadata file
  @inode: inode of the metadata file
  @shadow: shadow mapping
  nilfs_mdt_save_to_shadow_map - copy bmap and dirty pages to shadow map
  @inode: inode of the metadata file
 already frozen 
 drop ref-count to make it releasable 
  nilfs_mdt_restore_from_shadow_map - restore dirty pages and bmap state
  @inode: inode of the metadata file
  nilfs_mdt_clear_shadow_map - truncate pages in shadow map caches
  @inode: inode of the metadata file
 SPDX-License-Identifier: GPL-2.0+
  NILFS B-tree node cache
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Originally written by Seiji Kihara.
  Fully revised by Ryusuke Konishi for stabilization and simplification.
 internal code 
 blocknr is a virtual block number 
 internal code 
 mode == READ 
 internal code 
 set block address for read 
 set back to the given block address 
  nilfs_btnode_delete - delete B-tree node buffer
  @bh: buffer to be deleted
  nilfs_btnode_delete() invalidates the specified buffer and delete the page
  including the buffer if the page gets unbusy.
  nilfs_btnode_prepare_change_key
   prepare to move contents of the block for old key to one of new key.
   the old buffer will not be removed, but might be reused for new buffer.
   it might return -ENOMEM because of memory allocation errors,
   and might return -EIO because of disk read errors.
 BUG_ON(oldkey != obh->b_page->index); 
		
		  Note: page->index will not change to newkey until
		  nilfs_btnode_commit_change_key() will be called.
		  To protect the page in intermediate state, the page lock
		  is held.
 fallback to copy mode 
  nilfs_btnode_commit_change_key
   commit the change_key operation prepared by prepare_change_key().
 blocksize == pagesize 
 will decrement bh->b_count 
  nilfs_btnode_abort_change_key
   abort the change_key operation prepared by prepare_change_key().
 blocksize == pagesize 
 SPDX-License-Identifier: GPL-2.0+
  NILFS regular file handling primitives including fsync().
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Written by Amagai Yoshiji and Ryusuke Konishi.
	
	  Called from fsync() system call
	  This is the only entry point that can catch write and synch
	  timing for both data blocks and intermediate blocks.
	 
	  This function should be implemented when the writeback function
	  will be implemented.
 -ENOSPC 
 make the VM retry the fault 
	
	  check to see if the page is mapped already (no holes)
	
	  fill hole blocks
 never returns -ENOMEM, but may return -ENOSPC 
  We have mostly NULL's here: the current defaults are ok for
  the nilfs filesystem.
 CONFIG_COMPAT 
 .release	= nilfs_release_file, 
 end of file 
 SPDX-License-Identifier: GPL-2.0+
  the_nilfs shared structure.
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Written by Ryusuke Konishi.
  alloc_nilfs - allocate a nilfs object
  @sb: super block instance
  Return Value: On success, pointer to the_nilfs is returned.
  On error, NULL is returned.
  destroy_nilfs - destroy nilfs object
  @nilfs: nilfs object to be released
  nilfs_store_log_cursor - load log cursor from a super block
  @nilfs: nilfs object
  @sbp: buffer storing super block to be read
  nilfs_store_log_cursor() reads the last position of the log
  containing a super root from a given super block, and initializes
  relevant information on the nilfs object preparatory for log
  scanning and recovery.
  load_nilfs - load and recover the nilfs
  @nilfs: the_nilfs structure to be released
  @sb: super block instance used to recover past segment
  load_nilfs() searches and load the latest super root,
  attaches the last segment, and does recovery if needed.
  The caller must call this exclusively for simultaneous mounts.
		
		  restore super block with its spare and reconfigure
		  relevant states of the nilfs object.
 verify consistency between two super blocks 
 drop clean flag to allow roll-forward and recovery 
 set "clean" flag 
 page cache limit 
 bmap size limit 
  nilfs_nrsvsegs - calculate the number of reserved segments
  @nilfs: nilfs object
  @nsegs: total number of segments
	
	  Compare two super blocks and set 1 in swp if the secondary
	  super block is valid and newer.  Otherwise, set 0 in swp.
  init_nilfs - initialize a NILFS instance.
  @nilfs: the_nilfs structure
  @sb: super block
  @data: mount options
  init_nilfs() performs common initialization per block device (e.g.
  reading the super block, getting disk layout information, initializing
  shared fields in the_nilfs).
  Return Value: On success, 0 is returned. On error, a negative error
  code is returned.
			
			  Not to failed_sbh; sbh is released automatically
			  when reloading fails.
 SPDX-License-Identifier: GPL-2.0+
  NILFS recovery logic
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Written by Ryusuke Konishi.
  Segment check result
 work structure for recovery 
	ino_t ino;		
				  Inode number of the file that this block
				  belongs to
 block number 
 virtual block number 
 File offset of the data block (per block) 
  nilfs_compute_checksum - compute checksum of blocks continuously
  @nilfs: nilfs object
  @bhs: buffer head of start block
  @sum: place to store result
  @offset: offset bytes in the first block
  @check_bytes: number of bytes to be checked
  @start: DBN of start block
  @nblock: number of blocks to be checked
  nilfs_read_super_root_block - read super root block
  @nilfs: nilfs object
  @sr_block: disk block number of the super root block
  @pbh: address of a buffer_head pointer to return super root buffer
  @check: CRC check flag
  nilfs_read_log_header - read summary header of the specified log
  @nilfs: nilfs object
  @start_blocknr: start block number of the log
  @sum: pointer to return segment summary structure
  nilfs_validate_log - verify consistency of log
  @nilfs: nilfs object
  @seg_seq: sequence number of segment
  @bh_sum: buffer head of summary block
  @sum: segment summary struct
 This limits the number of blocks read in the CRC check 
  nilfs_read_summary_info - read an item on summary blocks of a log
  @nilfs: nilfs object
  @pbh: the current buffer head on summary blocks [in, out]
  @offset: the current byte offset on summary blocks [in, out]
  @bytes: byte size of the item to be read
  nilfs_skip_summary_info - skip items on summary blocks of a log
  @nilfs: nilfs object
  @pbh: the current buffer head on summary blocks [in, out]
  @offset: the current byte offset on summary blocks [in, out]
  @bytes: byte size of the item to be skipped
  @count: number of items to be skipped
  nilfs_scan_dsync_log - get block information of a log written for data sync
  @nilfs: nilfs object
  @start_blocknr: start block number of the log
  @sum: log summary information
  @head: list head to add nilfs_recovery_block struct
 INIT_LIST_HEAD(&rb->list); 
 always 0 for data sync logs 
 brelse(NULL) is just ignored 
	
	  Releasing the next segment of the latest super root.
	  The next segment is invalidated by this recovery.
	
	  Collecting segments written after the latest super root.
	  These are marked dirty to avoid being reallocated in the next write.
 Allocate new segments for recovery 
 No need to recover sufile because it will be destroyed on error 
 iput(NULL) is just ignored 
  nilfs_do_roll_forward - salvage logical segments newer than the latest
  checkpoint
  @nilfs: nilfs object
  @sb: super block instance
  @ri: pointer to a nilfs_recovery_info
 Startingending DBN of full segment 
 list of data blocks to be recovered 
 scanning data-sync segments 
 Found a valid partial segment; do recovery actions 
 Fall through to try_next_pseg 
 Looking to the next full segment 
  nilfs_salvage_orphan_logs - salvage logs written after the latest checkpoint
  @nilfs: nilfs object
  @sb: super block instance
  @ri: pointer to a nilfs_recovery_info struct to store search results.
  Return Value: On success, 0 is returned.  On error, one of the following
  negative error code is returned.
  %-EINVAL - Inconsistent filesystem state.
  %-EIO - IO error
  %-ENOSPC - No space left on device (only in a panic state).
  %-ERESTARTSYS - Interrupted.
  %-ENOMEM - Insufficient memory available.
  nilfs_search_super_root - search the latest valid super root
  @nilfs: the_nilfs
  @ri: pointer to a nilfs_recovery_info struct to store search results.
  nilfs_search_super_root() looks for the latest super-root from a partial
  segment pointed by the superblock.  It sets up struct the_nilfs through
  this search. It fills nilfs_recovery_info (ri) required for recovery.
  Return Value: On success, 0 is returned.  On error, one of the following
  negative error code is returned.
  %-EINVAL - No valid segment found
  %-EIO - IO error
  %-ENOMEM - Insufficient memory available.
 range of full segment (block number) 
 Calculate range of segment 
 Read ahead segment 
 A valid partial segment 
			
			  This will never happen because a superblock
			  (last_segment) always points to a pseg with
			  a super root.
 A valid super root was found. 
 nilfs->ns_cno = ri->ri_cno + 1 
 Standing on a course, or met an inconsistent state 
 Off the trail 
			
			  This can happen if a checkpoint was written without
			  barriers, or as a result of an IO failure.
 Looking to the next full segment 
 found a valid super root 
 Updating pointers relating to the latest checkpoint 
 SPDX-License-Identifier: GPL-2.0+
  Sysfs support implementation.
  Copyright (C) 2005-2014 Nippon Telegraph and Telephone Corporation.
  Copyright (C) 2014 HGST, Inc., a Western Digital Company.
  Written by Vyacheslav Dubeyko <Vyacheslav.Dubeyko@hgst.com>
 sysfs<nilfs> 
                         NILFS snapshot attrs                          
                     NILFS mounted snapshots attrs                     
                       NILFS checkpoints attrs                         
                         NILFS segments attrs                          
                         NILFS segctor attrs                           
                         NILFS superblock attrs                        
                         NILFS device attrs                            
                         NILFS feature attrs                           
 SPDX-License-Identifier: GPL-2.0+
  NILFS direct block pointer.
  Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.
  Written by Koji Sato.
 sequential access 
 block group 
 ptr must be a pointer to a buffer head. 
 no need to allocate any resource for conversion 
 delete 
 free resources 
 convert 
 SPDX-License-Identifier: GPL-2.0+
  NILFS segment buffer
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Written by Ryusuke Konishi.
 The region to be submitted 
  nilfs_segbuf_map_cont - map a new log behind a given log
  @segbuf: new segment buffer
  @prev: segment buffer containing a log to be continued
  Setup segment summary
  CRC calculation routines
  Iterators for segment buffers
  nilfs_add_checksums_on_logs - add checksums on the logs
  @logs: list of segment buffers storing target logs
  @seed: checksum seed value
  BIO operations
  nilfs_alloc_seg_bio - allocate a new bio for writing log
  @nilfs: nilfs object
  @start: start block number of the bio
  @nr_vecs: request size of page vector.
  Return Value: On success, pointer to the struct bio is returned.
  On error, NULL is returned.
 bio is FULL 
 never submit current bh 
  nilfs_segbuf_write - submit write requests of a log
  @segbuf: buffer storing a log to be written
  @nilfs: nilfs object
  Return Value: On Success, 0 is returned. On Error, one of the following
  negative error code is returned.
  %-EIO - IO error
  %-ENOMEM - Insufficient memory available.
		
		  Last BIO is always sent through the following
		  submission.
  nilfs_segbuf_wait - wait for completion of requested BIOs
  @segbuf: segment buffer
  Return Value: On Success, 0 is returned. On Error, one of the following
  negative error code is returned.
  %-EIO - IO error
 SPDX-License-Identifier: GPL-2.0+
  NILFS B-tree.
  Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.
  Written by Koji Sato.
  B-tree node operations
 Assume the buffer heads corresponding to left and right are locked. 
 Assume that the buffer heads corresponding to left and right are locked. 
 Assume that the buffer head corresponding to node is locked. 
 Assume that the buffer head corresponding to node is locked. 
 binary search 
 adjust index 
  nilfs_btree_node_broken - verify consistency of btree node
  @node: btree node block to be examined
  @size: node size (in bytes)
  @inode: host inode of btree
  @blocknr: block number
  Return Value: If node is broken, 1 is returned. Otherwise, 0 is returned.
  nilfs_btree_root_broken - verify consistency of btree root node
  @node: btree root node to be examined
  @inode: host inode of btree
  Return Value: If node is broken, 1 is returned. Otherwise, 0 is returned.
 parent node 
 max nof blocks to read ahead 
 current index on the parent node 
 nof children in the parent node 
 read ahead sibling nodes 
 insert 
  nilfs_btree_get_next_key - get next valid key from btree path array
  @btree: bmap struct of btree
  @path: array of nilfs_btree_path struct
  @minlevel: start level
  @nextkey: place to store the next valid key
  Return Value: If a next key was found, 0 is returned. Otherwise,
  -ENOENT is returned.
 Next index is already set to bp_index for leaf nodes. 
 Next key is in this node 
 For non-leaf nodes, next index is stored at bp_index + 1. 
 look-up right sibling node 
 root 
 move insert point 
 move insert point 
 left sibling 
 parent 
 sequential access 
 near 
 block group 
 allocate a new ptr for data block 
 left sibling 
 right sibling 
 split 
 root 
 grow 
 a newly-created node block and a data block are added 
 success 
 error 
 left sibling 
 continue; 
 right sibling 
				
				  When merging right sibling node
				  into the current node, pointer to
				  the right sibling node must be
				  terminated instead.  The adjustment
				  below is required for that.
 continue; 
 no siblings 
 the only child of the root node 
 child of the root node is deleted 
 success 
 error 
 for data 
 cannot find near ptr 
 success 
 error 
 free resources 
 ptr must be a pointer to a buffer head. 
 convert and insert 
 create child node at level 1 
 create root node at level 2 
 create root node at level 1 
  nilfs_btree_convert_and_insert -
  @bmap:
  @key:
  @ptr:
  @keys:
  @ptrs:
  @n:
 success 
 error 
 on-disk format 
 on-disk format 
 on-disk format 
 SPDX-License-Identifier: GPL-2.0+
  NILFS block mapping.
  Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.
  Written by Koji Sato.
  nilfs_bmap_lookup_at_level - find a data block or node block
  @bmap: bmap
  @key: key
  @level: level
  @ptrp: place to store the value associated to @key
  Description: nilfs_bmap_lookup_at_level() finds a record whose key
  matches @key in the block at @level of the bmap.
  Return Value: On success, 0 is returned and the record associated with @key
  is stored in the place pointed by @ptrp. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - A record associated with @key does not exist.
  nilfs_bmap_insert - insert a new key-record pair into a bmap
  @bmap: bmap
  @key: key
  @rec: record
  Description: nilfs_bmap_insert() inserts the new key-record pair specified
  by @key and @rec into @bmap.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-EEXIST - A record associated with @key already exist.
  nilfs_bmap_seek_key - seek a valid entry and return its key
  @bmap: bmap struct
  @start: start key number
  @keyp: place to store valid key
  Description: nilfs_bmap_seek_key() seeks a valid key on @bmap
  starting from @start, and stores it to @keyp if found.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - No valid entry was found
  nilfs_bmap_delete - delete a key-record pair from a bmap
  @bmap: bmap
  @key: key
  Description: nilfs_bmap_delete() deletes the key-record pair specified by
  @key from @bmap.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - A record associated with @key does not exist.
  nilfs_bmap_truncate - truncate a bmap to a specified key
  @bmap: bmap
  @key: key
  Description: nilfs_bmap_truncate() removes key-record pairs whose keys are
  greater than or equal to @key from @bmap.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  nilfs_bmap_clear - free resources a bmap holds
  @bmap: bmap
  Description: nilfs_bmap_clear() frees resources associated with @bmap.
  nilfs_bmap_propagate - propagate dirty state
  @bmap: bmap
  @bh: buffer head
  Description: nilfs_bmap_propagate() marks the buffers that directly or
  indirectly refer to the block specified by @bh dirty.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  nilfs_bmap_lookup_dirty_buffers -
  @bmap: bmap
  @listp: pointer to buffer head list
  nilfs_bmap_assign - assign a new block number to a block
  @bmap: bmap
  @bh: pointer to buffer head
  @blocknr: block number
  @binfo: block information
  Description: nilfs_bmap_assign() assigns the block number @blocknr to the
  buffer specified by @bh.
  Return Value: On success, 0 is returned and the buffer head of a newly
  create buffer and the block information associated with the buffer are
  stored in the place pointed by @bh and @binfo, respectively. On error, one
  of the following negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  nilfs_bmap_mark - mark block dirty
  @bmap: bmap
  @key: key
  @level: level
  Description: nilfs_bmap_mark() marks the block specified by @key and @level
  as dirty.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  nilfs_bmap_test_and_clear_dirty - test and clear a bmap dirty state
  @bmap: bmap
  Description: nilfs_test_and_clear() is the atomic operation to test and
  clear the dirty state of @bmap.
  Return Value: 1 is returned if @bmap is dirty, or 0 if clear.
  Internal use only
  nilfs_bmap_read - read a bmap from an inode
  @bmap: bmap
  @raw_inode: on-disk inode
  Description: nilfs_bmap_read() initializes the bmap @bmap.
  Return Value: On success, 0 is returned. On error, the following negative
  error code is returned.
  %-ENOMEM - Insufficient amount of memory available.
  nilfs_bmap_write - write back a bmap to an inode
  @bmap: bmap
  @raw_inode: on-disk inode
  Description: nilfs_bmap_write() stores @bmap in @raw_inode.
 SPDX-License-Identifier: GPL-2.0+
  NILFS disk address translation.
  Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.
  Written by Koji Sato.
  struct nilfs_dat_info - on-memory private data of DAT file
  @mi: on-memory private data of metadata file
  @palloc_cache: persistent object allocator cache of DAT file
  @shadow: shadow map of DAT file
  nilfs_dat_mark_dirty -
  @dat: DAT file inode
  @vblocknr: virtual block number
  Description:
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  nilfs_dat_freev - free virtual block numbers
  @dat: DAT file inode
  @vblocknrs: array of virtual block numbers
  @nitems: number of virtual block numbers
  Description: nilfs_dat_freev() frees the virtual block numbers specified by
  @vblocknrs and @nitems.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - The virtual block number have not been allocated.
  nilfs_dat_move - change a block number
  @dat: DAT file inode
  @vblocknr: virtual block number
  @blocknr: block number
  Description: nilfs_dat_move() changes the block number associated with
  @vblocknr to @blocknr.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
	
	  The given disk block number (blocknr) is not yet written to
	  the device at this point.
	 
	  To prevent nilfs_dat_translate() from returning the
	  uncommitted block number, this makes a copy of the entry
	  buffer and redirects nilfs_dat_translate() to the copy.
  nilfs_dat_translate - translate a virtual block number to a block number
  @dat: DAT file inode
  @vblocknr: virtual block number
  @blocknrp: pointer to a block number
  Description: nilfs_dat_translate() maps the virtual block number @vblocknr
  to the corresponding block number.
  Return Value: On success, 0 is returned and the block number associated
  with @vblocknr is stored in the place pointed by @blocknrp. On error, one
  of the following negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - A block number associated with @vblocknr does not exist.
 last virtual block number in this block 
  nilfs_dat_read - read or get dat inode
  @sb: super block instance
  @entry_size: size of a dat entry
  @raw_inode: on-disk dat inode
  @inodep: buffer to store the inode
 SPDX-License-Identifier: GPL-2.0+
  NILFS checkpoint file.
  Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.
  Written by Koji Sato.
 block number from the beginning of the file 
 offset in block 
  nilfs_cpfile_find_checkpoint_block - find and get a buffer on cpfile
  @cpfile: inode of cpfile
  @start_cno: start checkpoint number (inclusive)
  @end_cno: end checkpoint number (inclusive)
  @cnop: place to store the next checkpoint number
  @bhp: place to store a pointer to buffer_head struct
  Return Value: On success, it returns 0. On error, the following negative
  error code is returned.
  %-ENOMEM - Insufficient memory available.
  %-EIO - IO error
  %-ENOENT - no block exists in the range.
  nilfs_cpfile_get_checkpoint - get a checkpoint
  @cpfile: inode of checkpoint file
  @cno: checkpoint number
  @create: create flag
  @cpp: pointer to a checkpoint
  @bhp: pointer to a buffer head
  Description: nilfs_cpfile_get_checkpoint() acquires the checkpoint
  specified by @cno. A new checkpoint will be created if @cno is the current
  checkpoint number and @create is nonzero.
  Return Value: On success, 0 is returned, and the checkpoint and the
  buffer head of the buffer on which the checkpoint is located are stored in
  the place pointed by @cpp and @bhp, respectively. On error, one of the
  following negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - No such checkpoint.
  %-EINVAL - invalid checkpoint.
 a newly-created checkpoint 
  nilfs_cpfile_put_checkpoint - put a checkpoint
  @cpfile: inode of checkpoint file
  @cno: checkpoint number
  @bh: buffer head
  Description: nilfs_cpfile_put_checkpoint() releases the checkpoint
  specified by @cno. @bh must be the buffer head which has been returned by
  a previous call to nilfs_cpfile_get_checkpoint() with @cno.
  nilfs_cpfile_delete_checkpoints - delete checkpoints
  @cpfile: inode of checkpoint file
  @start: start checkpoint number
  @end: end checkpoint number
  Description: nilfs_cpfile_delete_checkpoints() deletes the checkpoints in
  the period from @start to @end, excluding @end itself. The checkpoints
  which have been already deleted are ignored.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-EINVAL - invalid checkpoints.
 skip hole 
 make hole 
 checkpoint number 0 is invalid 
 No snapshots (started from a hole block) 
 Terminator 
 reach end of the snapshot list 
  nilfs_cpfile_get_cpinfo -
  @cpfile:
  @cno:
  @ci:
  @nci:
  nilfs_cpfile_delete_checkpoint -
  @cpfile:
  @cno:
 checkpoint number 0 is invalid 
 checkpoint number 0 is invalid 
  nilfs_cpfile_is_snapshot -
  @cpfile: inode of checkpoint file
  @cno: checkpoint number
  Description:
  Return Value: On success, 1 is returned if the checkpoint specified by
  @cno is a snapshot, or 0 if not. On error, one of the following negative
  error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - No such checkpoint.
	
	  CP number is invalid if it's zero or larger than the
	  largest existing one.
  nilfs_cpfile_change_cpmode - change checkpoint mode
  @cpfile: inode of checkpoint file
  @cno: checkpoint number
  @mode: mode of checkpoint
  Description: nilfs_change_cpmode() changes the mode of the checkpoint
  specified by @cno. The mode @mode is NILFS_CHECKPOINT or NILFS_SNAPSHOT.
  Return Value: On success, 0 is returned. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  %-ENOENT - No such checkpoint.
			
			  Current implementation does not have to protect
			  plain read-only mounts since they are exclusive
			  with a readwrite mount and are protected from the
			  cleaner.
  nilfs_cpfile_get_stat - get checkpoint statistics
  @cpfile: inode of checkpoint file
  @cpstat: pointer to a structure of checkpoint statistics
  Description: nilfs_cpfile_get_stat() returns information about checkpoints.
  Return Value: On success, 0 is returned, and checkpoints information is
  stored in the place pointed by @cpstat. On error, one of the following
  negative error codes is returned.
  %-EIO - IO error.
  %-ENOMEM - Insufficient amount of memory available.
  nilfs_cpfile_read - read or get cpfile inode
  @sb: super block instance
  @cpsize: size of a checkpoint entry
  @raw_inode: on-disk cpfile inode
  @inodep: buffer to store the inode
 SPDX-License-Identifier: GPL-2.0+
  NILFS datinode allocator
  Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.
  Originally written by Koji Sato.
  Two allocators were unified by Ryusuke Konishi and Amagai Yoshiji.
  nilfs_palloc_groups_per_desc_block - get the number of groups that a group
 					descriptor block can maintain
  @inode: inode of metadata file using this allocator
  nilfs_palloc_groups_count - get maximum number of groups
  @inode: inode of metadata file using this allocator
 log2(8) ));
  nilfs_palloc_init_blockgroup - initialize private variables for allocator
  @inode: inode of metadata file using this allocator
  @entry_size: size of the persistent object
		
		  Number of blocks in a group including entry blocks
		  and a bitmap block
		
		  Number of blocks per descriptor including the
		  descriptor block
  nilfs_palloc_group - get group number and offset from an entry number
  @inode: inode of metadata file using this allocator
  @nr: serial number of the entry (e.g. inode number)
  @offset: pointer to store offset number in the group
  nilfs_palloc_desc_blkoff - get block offset of a group descriptor block
  @inode: inode of metadata file using this allocator
  @group: group number
  nilfs_palloc_desc_blkoff() returns block offset of the descriptor
  block which contains a descriptor of the specified group.
  nilfs_palloc_bitmap_blkoff - get block offset of a bitmap block
  @inode: inode of metadata file using this allocator
  @group: group number
  nilfs_palloc_bitmap_blkoff() returns block offset of the bitmap
  block used to allocatedeallocate entries in the specified group.
  nilfs_palloc_group_desc_nfrees - get the number of free entries in a group
  @desc: pointer to descriptor structure for the group
  @lock: spin lock protecting @desc
  nilfs_palloc_group_desc_add_entries - adjust count of free entries
  @desc: pointer to descriptor structure for the group
  @lock: spin lock protecting @desc
  @n: delta to be added
  nilfs_palloc_entry_blkoff - get block offset of an entry block
  @inode: inode of metadata file using this allocator
  @nr: serial number of the entry (e.g. inode number)
  nilfs_palloc_desc_block_init - initialize buffer of a group descriptor block
  @inode: inode of metadata file
  @bh: buffer head of the buffer to be initialized
  @kaddr: kernel address mapped for the page including the buffer
		
		  The following code must be safe for change of the
		  cache contents during the get block call.
  nilfs_palloc_delete_block - delete a block on the persistent allocator file
  @inode: inode of metadata file using this allocator
  @blkoff: block offset
  @prev: nilfs_bh_assoc struct of the last used buffer
  @lock: spin lock protecting @prev
  nilfs_palloc_get_desc_block - get buffer head of a group descriptor block
  @inode: inode of metadata file using this allocator
  @group: group number
  @create: create flag
  @bhp: pointer to store the resultant buffer head
  nilfs_palloc_get_bitmap_block - get buffer head of a bitmap block
  @inode: inode of metadata file using this allocator
  @group: group number
  @create: create flag
  @bhp: pointer to store the resultant buffer head
  nilfs_palloc_delete_bitmap_block - delete a bitmap block
  @inode: inode of metadata file using this allocator
  @group: group number
  nilfs_palloc_get_entry_block - get buffer head of an entry block
  @inode: inode of metadata file using this allocator
  @nr: serial number of the entry (e.g. inode number)
  @create: create flag
  @bhp: pointer to store the resultant buffer head
  nilfs_palloc_delete_entry_block - delete an entry block
  @inode: inode of metadata file using this allocator
  @nr: serial number of the entry
  nilfs_palloc_block_get_group_desc - get kernel address of a group descriptor
  @inode: inode of metadata file using this allocator
  @group: group number
  @bh: buffer head of the buffer storing the group descriptor block
  @kaddr: kernel address mapped for the page including the buffer
  nilfs_palloc_block_get_entry - get kernel address of an entry
  @inode: inode of metadata file using this allocator
  @nr: serial number of the entry (e.g. inode number)
  @bh: buffer head of the buffer storing the entry block
  @kaddr: kernel address mapped for the page including the buffer
  nilfs_palloc_find_available_slot - find available slot in a group
  @bitmap: bitmap of the group
  @target: offset number of an entry in the group (start point)
  @bsize: size in bits
  @lock: spin lock protecting @bitmap
 wrap around 
  nilfs_palloc_rest_groups_in_desc_block - get the remaining number of groups
 					    in a group descriptor block
  @inode: inode of metadata file using this allocator
  @curr: current group number
  @max: maximum number of groups
  nilfs_palloc_count_desc_blocks - count descriptor blocks number
  @inode: inode of metadata file using this allocator
  @desc_blocks: descriptor blocks number [out]
  nilfs_palloc_mdt_file_can_grow - check potential opportunity for
 					MDT file growing
  @inode: inode of metadata file using this allocator
  @desc_blocks: known current descriptor blocks count
  nilfs_palloc_count_max_entries - count max number of entries that can be
 					described by descriptor blocks count
  @inode: inode of metadata file using this allocator
  @nused: current number of used entries
  @nmaxp: max number of entries [out]
  nilfs_palloc_prepare_alloc_entry - prepare to allocate a persistent object
  @inode: inode of metadata file using this allocator
  @req: nilfs_palloc_req structure exchanged for the allocation
 wrap around 
 found a free entry 
 no entries left 
  nilfs_palloc_commit_alloc_entry - finish allocation of a persistent object
  @inode: inode of metadata file using this allocator
  @req: nilfs_palloc_req structure exchanged for the allocation
  nilfs_palloc_commit_free_entry - finish deallocating a persistent object
  @inode: inode of metadata file using this allocator
  @req: nilfs_palloc_req structure exchanged for the removal
  nilfs_palloc_abort_alloc_entry - cancel allocation of a persistent object
  @inode: inode of metadata file using this allocator
  @req: nilfs_palloc_req structure exchanged for the allocation
  nilfs_palloc_prepare_free_entry - prepare to deallocate a persistent object
  @inode: inode of metadata file using this allocator
  @req: nilfs_palloc_req structure exchanged for the removal
  nilfs_palloc_abort_free_entry - cancel deallocating a persistent object
  @inode: inode of metadata file using this allocator
  @req: nilfs_palloc_req structure exchanged for the removal
  nilfs_palloc_freev - deallocate a set of persistent objects
  @inode: inode of metadata file using this allocator
  @entry_nrs: array of entry numbers to be deallocated
  @nitems: number of entries stored in @entry_nrs
 Get the first entry number of the group 
 This entry is in the same block 
 Test if the entry block is empty or not 
 Go on to the next entry block 
 SPDX-License-Identifier: GPL-2.0
  Process version 2 NFS requests.
  Copyright (C) 1995-1997 Olaf Kirch <okir@monad.swb.de>
  Get a file's attributes
  N.B. After this call resp->fh needs an fh_put
  Set a file's attributes
  N.B. After this call resp->fh needs an fh_put
	
	  NFSv2 does not differentiate between "set-[ac]time-to-now"
	  which only requires access, and "set-[ac]time-to-X" which
	  requires ownership.
	  So if it looks like it might be "set both to the same time which
	  is close to now", and if setattr_prepare fails, then we
	  convert to "set to now" instead of "set to explicit time"
	 
	  We only call setattr_prepare as the last test as technically
	  it is not an interface that we should be using.
		
		  Looks probable.
		 
		  Now just make sure time is in the right ballpark.
		  Solaris, at least, doesn't seem to care what the time
		  request is.  We require it be within 30 minutes of now.
			
			  Turn off ATTR_[AM]TIME_SET but leave ATTR_[AM]TIME.
			  This will cause notify_change to set these times
			  to "now"
 Obsolete, replaced by MNTPROC_MNT. 
  Look up a path name component
  Note: the dentry in the resp->fh may be negative if the file
  doesn't exist yet.
  N.B. After this call resp->fh needs an fh_put
  Read a symlink.
 Read the symlink. 
  Read a portion of a file.
  N.B. After this call resp->fh needs an fh_put
	 Obtain buffer pointer for payload. 19 is 1 word for
	  status, 17 words for fattr, and 1 word for the byte count.
 Reserved 
  Write data to a file
  N.B. After this call resp->fh needs an fh_put
  CREATE processing is complicated. The keyword here is `overloaded.'
  The parent directory is kept locked between the check for existence
  and the actual create() call in compliance with VFS protocols.
  N.B. After this call _both_ argp->fh and resp->fh need an fh_put
 First verify the parent file handle 
 must fh_put dirfhp even on error 
 Check for NFSD_MAY_WRITE in nfsd_create if necessary 
		
		  If the new file handle wasn't verified, we can't tell
		  whether the file exists or not. Time to bail ...
 Unfudge the mode bits 
			 no type, so if target exists, assume same as that,
 reserve rdev for later checking 
					 this is probably a permission check..
					  at least IRIX implements perm checking on
					    echo thing > device-special-file-or-pipe
					  by doing a CREATE with type==0
 ??? 
	 Special treatment for non-regular files according to the
	  gospel of sun micro
 If you think you've seen the worst, grok this. 
 Okay, char or block special 
 we've used the SIZE information, so discard it 
 Make sure the type and device matches 
 File doesn't exist. Create it and set attrs 
		 File already exists. We ignore all attributes except
		  size, so that creat() behaves exactly like
		  open(..., O_CREAT|O_TRUNC|O_WRONLY).
 We don't really need to unlock, as fh_put does it. 
 Unlink. -SIFDIR means file must not be a directory 
  Make directory. This operation is not idempotent.
  N.B. After this call resp->fh needs an fh_put
  Remove a directory
 Reserve room for the NULL ptr & eof flag (-2 words) 
	 This is xdr_init_encode(), but it assumes that
  Read a portion of a directory.
  Get file system info
  NFSv2 Server procedures.
  Only the results of non-idempotent operations are cached.
 status 
 filehandle 
 attributes 
  Map errnos to NFS errnos.
 SPDX-License-Identifier: GPL-2.0
 Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de> 
 discard any old override before preparing the new set 
 Each thread allocates its own gi, no race 
  Copyright (c) 2004 The Regents of the University of Michigan.
  Copyright (c) 2012 Jeff Layton <jlayton@redhat.com>
  All rights reserved.
  Andy Adamson <andros@citi.umich.edu>
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions
  are met:
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions and the following disclaimer.
  2. Redistributions in binary form must reproduce the above copyright
     notice, this list of conditions and the following disclaimer in the
     documentation andor other materials provided with the distribution.
  3. Neither the name of the University nor the names of its
     contributors may be used to endorse or promote products derived
     from this software without specific prior written permission.
  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 Declarations 
 Globals 
  If we had an error generating the recdir name for the legacy tracker
  then warn the admin. If the error doesn't appear to be transient,
  then disable recovery tracking.
	
	  if the algorithm just doesn't exist, then disable the recovery
	  tracker altogether. The crypto libs will generally return this if
	  FIPS is enabled as well.
 lock the parent 
		
		  In the 4.1 case, where we're called from
		  reclaim_complete(), records from the previous reboot
		  may still be left, so this is OK.
		 
		  In the 4.0 case, we should never get here; but we may
		  as well be forgiving and just succeed silently.
 Keep trying; maybe the others are OK: 
 Keep trying, success or failure: 
 Keep trying; maybe the others are OK: 
  Hold reference to the recovery directory.
 XXX: The legacy code won't work in a container 
  Change the NFSv4 recovery directory to recdir.
 did we already find that this client is stable? 
 look for it in the reclaim hashtable otherwise 
 Globals 
 per-net-ns structure for holding cld upcall info 
	
	  -EAGAIN occurs when pipe is closed and reopened while there are
	   upcalls queued.
 copy just the xid so we can try to find that 
	
	  copy the status so we know whether to remove the upcall from the
	  list (for -EINPROGRESS, we just want to make sure the xid is
	  valid, not remove the upcall from the list)
 walk the list and find corresponding xid 
 couldn't find upcall? 
 errno >= 0 means we got a downcall 
 Initialize rpc_pipefs pipe for communication with client tracking daemon 
 FIXME: hard cap on number in flight? 
 Ask daemon to create a new record 
 Don't upcall if it's already stored 
 Ask daemon to create a new record 
 Don't upcall if it's already stored 
 Ask daemon to create a new record 
 Don't upcall if it's already removed 
  For older nfsdcld's that do not allow us to "slurp" the clients
  from the tracking database during startup.
  Check for presence of a record, and update its timestamp
 Don't upcall if one was already stored during this grace pd 
  For newer nfsdcld's that allow us to "slurp" the clients
  from the tracking database during startup.
  Check for presence of a record in the reclaim_str_hashtbl
 did we already find that this client is stable? 
 look for it in the reclaim hashtable otherwise 
 did we already find that this client is stable? 
 look for it in the reclaim hashtable otherwise 
 For older nfsdcld's that need cm_gracetime 
  For newer nfsdcld's that do not need cm_gracetime.  We also need to call
  nfs4_release_reclaim() to clear out the reclaim_str_hashtbl.
	
	  rpc pipe upcalls take 30 seconds to time out, so we don't want to
	  queue an upcall unless we know that nfsdcld is running (because we
	  want this to fail fast so that nfsd4_client_tracking_init() can try
	  the next client tracking method).  nfsdcld should already be running
	  before nfsd is started, so the wait here is for nfsdcld to open the
	  pipefs file we just created.
 For older nfsdcld's 
 For newer nfsdcld's 
 v2 createcheck ops include the principal, if available 
 upcall via usermodehelper 
 just return nothing if output was truncated 
 +1 is for '' between "topdir" and "recdir" 
 just return nothing if output will be truncated 
 prefix + YN character + terminating NULL 
 just return nothing if output was truncated 
 prefix + max width of int64_t string + terminating NULL 
 just return nothing if output was truncated 
	
	  Disable the upcall mechanism if we're getting an ENOENT or EACCES
	  error. The admin can re-enable it on the fly by using sysfs
	  once the problem has been fixed.
 +1 for terminating NULL 
 XXX: The usermode helper s not working in container yet. 
	
	  With v4.0 clients, there's little difference in outcome between a
	  create and check operation, and we can end up calling into this
	  function multiple times per client (once for each openowner). So,
	  for v4.0 clients skip upcalling once the client has been recorded
	  on stable storage.
	 
	  For v4.1+ clients, the outcome of the two operations is different,
	  so we must ensure that we upcall for the create operation. v4.1+
	  clients call this on RECLAIM_COMPLETE though, so we should only end
	  up doing a single create upcall per client.
 FIXME: better way to determine max size? 
 just run the init if it the method is already decided 
 First, try to use nfsdcld 
	
	  Next, try the UMH upcall.
	
	  Finally, See if the recoverydir exists and is a directory.
	  If it is, then use the legacy ops.
 SPDX-License-Identifier: GPL-2.0
  File operations used by nfsd. Some of these have been ripped from
  other parts of the kernel because they weren't exported, others
  are partial duplicates with added or changed functionality.
  Note that several functions dget() the dentry upon which they want
  to act, most notably those that create directory entries. Response
  dentry's are dput()'d if necessary in the release callback.
  So if you notice code paths that apparently fail to dput() the
  dentry, don't worry--they have been taken care of.
  Copyright (C) 1995-1999 Olaf Kirch <okir@monad.swb.de>
  Zerocpy NFS support (C) 2002 Hirokazu Takahashi <taka@valinux.co.jp>
 CONFIG_NFSD_V3 
 CONFIG_NFSD_V4 
  Called from nfsd_lookup and encode_dirent. Check if we have crossed 
  a mount point.
  Returns -EAGAIN or -ETIMEDOUT leaving dpp and expp unchanged,
   or nfs_ok having possibly changed dpp and expp
 This is only a mountpoint in some other namespace 
		
		  We normally allow NFS clients to continue
		  "underneath" a mountpoint that is not exported.
		  The exception is V4ROOT, where no traversal is ever
		  allowed without an explicit export of the new
		  directory.
 successfully crossed mount point 
		
		  This is subtle: path.dentry is not on path.mnt
		  at this point.  The only reason we are safe is that
		  original mnt is pinned down by exp, so we should
		  put path before putting exp
  For nfsd purposes, we treat V4ROOT exports as though there was an
  export at every directory.
  We return:
  '1' if this dentry must be an export point,
  '2' if it might be, if there is really a mount here, and
  '0' if there is no chance of an export point here.
		
		  Might only be a mountpoint in a different namespace,
		  but we need to check.
 Lookup the name, but don't follow links 
 .. == . just like at  
 checking mountpoint crossing is very different when stepping up 
		
		  In the nfsd4_open() case, this may be held across
		  subsequent open and delegation acquisition which may
		  need to take the child's i_mutex:
			
			  We don't need the i_mutex after all.  It's
			  still possible we could open this (regular
			  files can be mountpoints too), but the
			  i_mutex is just there to prevent renames of
			  something that we might be about to delegate,
			  and a mountpoint won't be renamed:
  Look up one component of a pathname.
  N.B. After this call _both_ fhp and resfh need an fh_put
  If the lookup would cross a mountpoint, and the mounted filesystem
  is exported to the client with NFSEXP_NOHIDE, then the lookup is
  accepted as it stands and the mounted directory is
  returned. Otherwise the covered directory is returned.
  NOTE: this mountpoint crossing is not supported properly by all
    clients and is explicitly disallowed for NFSv3
       NeilBrown <neilb@cse.unsw.edu.au>
	
	  Note: we compose the file handle now, but as the
	  dentry may be negative, it may need to be updated.
  Commit metadata changes to stable storage.
  Go over the attributes and take care of the small differences between
  NFS semantics and what Linux expects.
 sanitize the mode change 
 Revoke setuidsetgid on chown 
 we're setting mode too, just clear the sid bits 
 set ATTR_KILL_ bits and let VFS handle it 
  Set various file attributes.  After this call fhp needs an fh_put.
	
	  If utimes(2) and friends are called with times not NULL, we should
	  not set NFSD_MAY_WRITE bit. Otherwise fh_verify->nfsd_permission
	  will return EACCES, when the caller's effective UID does not match
	  the owner of the file, and the caller is not privileged. In this
	  situation, we should return EPERM(notify_change will return this).
 Callers that do fh_verify should do the fh_want_write: 
 Get inode 
 Ignore any mode updates on symlinks 
	
	  The size case is special, it changes the file in addition to the
	  attributes, and file systems don't expect it to be mixed with
	  "random" attribute changes.  We thus split out the size change
	  into a separate call to ->setattr, and do the rest as a separate
	  setattr call.
		
		  RFC5661, Section 18.30.4:
		    Changing the size of a file with SETATTR indirectly
		    changes the time_modify and change attributes.
		 
		  (and similar for the older RFCs)
		
		  Avoid the additional setattr call below if the only other
		  attribute that the client sends is the mtime, as we update
		  it as part of the size change above.
  NFS junction information is stored in an extended attribute.
  nfsd4_is_junction - Test if an object could be an NFS junction
  @dentry: object to test
  Returns 1 if "dentry" appears to contain NFS junction information.
  Otherwise 0 is returned.
 S_IFREG , NFSD_MAY_SATTR);
	
	  Limit copy to 4MB to prevent indefinitely blocking an nfsd
	  thread and client rpc slot.  The choice of 4MB is somewhat
	  arbitrary.  We might instead base this on rwsize, or make it
	  tunable, or use a time instead of a byte limit, or implement
	  asynchronous copy.  In theory a client could also recognize a
	  limit like this and pipeline multiple COPY requests.
 defined(CONFIG_NFSD_V4) 
  Check server access rights to a file system object
	 Some clients - Solaris 2.6 at least, make an access call
	  to the server to check for access for things like devnull
	  (which really, the server doesn't care about).  So
	  We provide simple access checking for them, looking
	  mainly at mode bits, and we make sure to ignore read-only
	  filesystem checks
			 the following error codes just mean the access was not allowed,
 simply don't "or" in the access bit. 
 CONFIG_NFSD_V3 
  Open an existing file or directory.
  The may_flags argument indicates the type of open (readwritelock)
  and additional flags.
  N.B. After this call fhp needs an fh_put
 NOMEM or WOULDBLOCK 
	
	  If we get here, then the client has already done an "open",
	  and (hopefully) checked permission - so allow OWNER_OVERRIDE
	  in case a chmod has now revoked permission.
	 
	  Arguably we should also allow the owner override for
	  directories, but we never have and it doesn't seem to have
	  caused anyone a problem.  If we were to change this, note
	  also that our filldir callbacks would need a variant of
	  lookup_one_len that doesn't check permissions.
  Grab and keep cached pages associated with a file in the svc_rqst
  so that they can be passed to the network sendmsgsendpage routines
  directly. They will be released after the sending has completed.
  Gathered writes: If another process is currently writing to the file,
  there's a high chance this is another nfsd (triggered by a bulk write
  from a client's biod). Rather than syncing the file with each write
  request, we sleep for 10 msec.
  I don't know if this roughly approximates C. Juszak's idea of
  gathered writes, but it's a nice and simple solution (IMHO), and it
  seems to work:-)
  Note: we do this only in the NFSv2 case, since v3 and higher have a
  better tool (separate unstable writes and commits) for solving this
  problem.
		
		  We want throttling in balance_dirty_pages()
		  and shrink_inactive_list() to only consider
		  the backingdev we are writing to, so that nfs to
		  localhost doesn't cause nfsd to lock up due to all
		  the client's dirty pages or its congested queue.
  Read data from a file. count must contain the requested read count
  on entry. On return, count contains the number of bytes actually read.
  N.B. After this call fhp needs an fh_put
  Write data to a file.
  The stable flag requests synchronous writes.
  N.B. After this call fhp needs an fh_put
  Commit all pending writes to stable storage.
  Note: we only guarantee that data that lies within the range specified
  by the 'offset' and 'count' parameters will be synced.
  Unfortunately we cannot lock the file to make sure we return full WCC
  data to the client, as locking happens lower down in the filesystem.
 CONFIG_NFSD_V3 
	
	  Mode has already been set earlier in create:
	
	  Setting uidgid works only for root.  Irix appears to
	  send along the gid on create when it tries to implement
	  setgid directories via NFS:
 Callers expect file metadata to be committed here 
 HPUX client sometimes creates a file in mode 000, and sets size to 0.
  setting size to 0 may fail for some specific file systems by the permission
  checking which requires WRITE permission but the mode is 000.
  we ignore the resizing(to 0) on the just new created file, since the size is
  0 after file created.
  call this only after vfs_create() is called.
 The parent directory should already be locked: 
	
	  nfsd_create_setattr already committed the child.  Transactional
	  filesystems had a chance to commit changes for both parent and
	  child simultaneously making the following commit_metadata a
	  noop.
	
	  Update the file handle to get the new inode info.
  Create a filesystem object (regular, directory, special).
  Note that the parent directory is left locked.
  N.B. Every call to nfsd_create needs an fh_put for _both_ fhp and resfhp
	
	  We unconditionally drop our ref to dchild as fh_compose will have
	  already grabbed its own ref for it.
  NFSv3 and NFSv4 version of nfsd_create
	
	  Compose the response file handle.
 If file doesn't exist, check for permissions to create one 
		 solaris7 gets confused (bugid 4218508) if these have
		  the high bit set, as do xfs filesystems without the
		  "bigtime" feature.  So just clear the high bits. If this is
		  ever changed to use different attrs for storing the
		  verifier, then do_open_lookup() will also need to be fixed
		  accordingly.
				 in nfsv4, we need to treat this case a little
				  differently.  we don't want to truncate the
				  file now; this would be wrong if the OPEN
				  fails for some other reason.  furthermore,
				  if the size is nonzero, we should ignore it
				  according to spec!
 Cram the verifier into atimemtime 
 XXX someone who knows this better please fix it for nsec  
	
	  nfsd_create_setattr already committed the child
	  (and possibly also the parent).
	
	  Update the filehandle to get the new inode info.
 CONFIG_NFSD_V3 
  Read a symlink. On entry, lenp must contain the maximum path length that
  fits into the buffer. On return, it contains the true length.
  N.B. After this call fhp needs an fh_put
  Create a symlink and look up its inode
  N.B. After this call _both_ fhp and resfhp need an fh_put
  Create a hardlink
  N.B. After this call _both_ ffhp and tfhp need an fh_put
  Rename a file
  N.B. After this call _both_ ffhp and tfhp need an fh_put
	 cannot use fh_lock as we need deadlock protective ordering
	
	  We cannot rely on fh_unlock on the two filehandles,
	  as that would do the wrong thing if the two directories
	  were the same, so again we do it by hand.
	
	  If the target dentry has cached open files, then we need to try to
	  close them prior to doing the rename. Flushing delayed fput
	  shouldn't be done with locks held however, so we delay it until this
	  point and then reattempt the whole shebang.
  Unlink a file or directory
  N.B. After this call fhp needs an fh_put
 truncate the inode here 
		 name is mounted-on. There is no perfect
		  error status.
  We do this buffering because we must not call back into the file
  system's ->lookup() method from the filldir callback. That may well
  deadlock a number of file systems.
  This is based heavily on the implementation of same in XFS.
 will be cleared on successful read 
 We bailed out early 
  Read entries from a directory.
  The  NFSv34 verifier we ignore for now.
 NFSv2 only supports 32 bit cookies 
 can still be found in ->err 
  Get file system stats
  N.B. After this call fhp needs an fh_put
  Helper function to translate error numbers. In the case of xattr operations,
  some error codes need to be translated outside of the standard translations.
  ENODATA needs to be translated to nfserr_noxattr.
  E2BIG to nfserr_xattr2big.
  Additionally, vfs_listxattr can return -ERANGE. This means that the
  file has too many extended attributes to retrieve inside an
  XATTR_LIST_MAX sized buffer. This is a bug in the xattr implementation:
  filesystems will allow the adding of extended attributes until they hit
  their own internal limit. This limit may be larger than XATTR_LIST_MAX.
  So, at that point, the attributes are present and valid, but can't
  be retrieved using listxattr, since the upper level xattr code enforces
  the XATTR_LIST_MAX limit.
  This bug means that we need to deal with listxattr returning -ERANGE. The
  best mapping is to return TOOSMALL.
  Retrieve the specified user extended attribute. To avoid always
  having to allocate the maximum size (since we are not getting
  a maximum size from the RPC), do a probe + alloc. Hold a reader
  lock on i_rwsem to prevent the extended attribute from changing
  size while we're doing this.
	
	  Zero-length attribute, just return.
  Retrieve the xattr names. Since we can't know how many are
  user extended attributes, we must get all attributes here,
  and have the XDR encode filter out the "user." ones.
  While this could always just allocate an XATTR_LIST_MAX
  buffer, that's a waste, so do a probe + allocate. To
  avoid any changes between the probe and allocate, wrap
  this in inode_lock.
	
	  We're holding i_rwsem - use GFP_NOFS.
  Removexattr and setxattr need to call fh_lock to both lock the inode
  and set the change attribute. Since the top-level vfs_removexattr
  and vfs_setxattr calls already do their own inode_lock calls, call
  the _locked variant. Pass in a NULL pointer for delegated_inode,
  and let the client deal with NFS4ERR_DELAY (same as with e.g.
  setattr and remove).
  Check for a user's access permissions to this inode.
	 Normally we reject any writesattr etc access on a read-only file
	  system.  But if it is IRIX doing check on write-access for a 
	  device special file, we ignore rofs.
 (acc & NFSD_MAY_WRITE) &&  IS_IMMUTABLE(inode))
		 If we cannot rely on authentication in NLM requests,
		  just allow locks, otherwise require read permission, or
		  ownership
	
	  The file owner always gets access permission for accesses that
	  would normally be checked at open time. This is to make
	  file access work even when the client has done a fchmod(fd, 0).
	 
	  However, `cp foo bar' should fail nevertheless when bar is
	  readonly. A sensible way to do this might be to reject all
	  attempts to truncate a read-only file, because a creat() call
	  always implies file truncation.
	  ... but this isn't really fair.  A process may reasonably call
	  ftruncate on an open file descriptor on a file with perm 000.
	  We must trust the client to do permission checking - using "ACCESS"
	  with NFSv3.
 This assumes  NFSD_MAY_{READ,WRITE,EXEC} == MAY_{READ,WRITE,EXEC} 
 Allow read access to binaries even when mode 111 
 SPDX-License-Identifier: GPL-2.0
  This file contains all the stubs needed when communicating with lockd.
  This level of indirection is necessary so we can run nfsd+lockd without
  requiring the nfs client to be compiled inloaded, and vice versa.
  Copyright (C) 1996, Olaf Kirch <okir@monad.swb.de>
  Note: we hold the dentry use count while the file is open.
 must initialize before using! but maxsize doesn't matter 
 	 We return nlm error codes as nlm doesn't know
	  about nfsd, but nfsd does know about nlm..
 open file for locking 
 close file 
 SPDX-License-Identifier: GPL-2.0-only
  Syscall interface to knfsd.
  Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>
 	We have a single directory with several nodes in it.
	
	  The below MUST come last.  Otherwise we leave a hole in nfsd_files[]
	  with !CONFIG_NFSD_V4 and simple_fill_super() goes oops
  write() for these nodes.
		 An attempt to read a transaction file without writing
		  causes a 0-byte write so that the file can return
		  state information
 CONFIG_SUNRPC_GSS or CONFIG_SUNRPC_GSS_MODULE 
----------------------------------------------------------------------------
  payload - write methods
  write_unlock_ip - Release all locks used by a client
  Experimental.
  Input:
 			buf:	'\n'-terminated C string containing a
 				presentation format IP address
 			size:	length of C string in @buf
  Output:
 	On success:	returns zero if all specified locks were released;
 			returns one if one or more locks were not released
 	On error:	return code is negative errno value
 sanity check 
  write_unlock_fs - Release all locks on a local file system
  Experimental.
  Input:
 			buf:	'\n'-terminated C string containing the
 				absolute pathname of a local file system
 			size:	length of C string in @buf
  Output:
 	On success:	returns zero if all specified locks were released;
 			returns one if one or more locks were not released
 	On error:	return code is negative errno value
 sanity check 
	
	  XXX: Needs better sanity checking.  Otherwise we could end up
	  releasing locks on the wrong file system.
	 
	  For example:
	  1.  Does the path refer to a directory?
	  2.  Is that directory a mount point, or
	  3.  Is that directory the root of an exported file system?
  write_filehandle - Get a variable-length NFS file handle by path
  On input, the buffer contains a '\n'-terminated C string comprised of
  three alphanumeric words separated by whitespace.  The string may
  contain escape sequences.
  Input:
 			buf:
 				domain:		client domain name
 				path:		export pathname
 				maxsize:	numeric maximum size of
 						@buf
 			size:	length of C string in @buf
  Output:
 	On success:	passed-in buffer filled with '\n'-terminated C
 			string containing a ASCII hex text version
 			of the NFS file handle;
 			return code is the size in bytes of the string
 	On error:	return code is negative errno value
 we have all the words, they are in buf.. 
  write_threads - Start NFSD, or report the current number of running threads
  Input:
 			buf:		ignored
 			size:		zero
  Output:
 	On success:	passed-in buffer filled with '\n'-terminated C
 			string numeric value representing the number of
 			running NFSD threads;
 			return code is the size in bytes of the string
 	On error:	return code is zero
  OR
  Input:
 			buf:		C string containing an unsigned
 					integer value representing the
 					number of NFSD threads to start
 			size:		non-zero length of C string in @buf
  Output:
 	On success:	NFS service is started;
 			passed-in buffer filled with '\n'-terminated C
 			string numeric value representing the number of
 			running NFSD threads;
 			return code is the size in bytes of the string
 	On error:	return code is zero or a negative errno value
  write_pool_threads - Set or report the current number of threads per pool
  Input:
 			buf:		ignored
 			size:		zero
  OR
  Input:
  			buf:		C string containing whitespace-
  					separated unsigned integer values
 					representing the number of NFSD
 					threads to start in each pool
 			size:		non-zero length of C string in @buf
  Output:
 	On success:	passed-in buffer filled with '\n'-terminated C
 			string containing integer values representing the
 			number of NFSD threads in each pool;
 			return code is the size in bytes of the string
 	On error:	return code is zero or a negative errno value
	 if size > 0, look for an array of number of threads per node
	  and apply them  then write out number of threads per node as reply
		
		  NFS is shut down.  The admin can start it by
		  writing to the threads file but NOT the pool_threads
		  file, sorry.  Report zero threads.
 fewer numbers than pools 
 syntax error 
		
		  special case for backward compatability.
		  +4.0 is never reported, it is implied by
		  +4, unless -4.0 is present.
			 Cannot change versions without updating
			  nn->nfsd_serv->sv_xdrsize, and reallocing
			  rq_argp and rq_resp
					
					  Either we have +4 and no minors are enabled,
					  or we have -4 and at least one minor is enabled.
					  In either case, propagate 'cmd' to all minors.
		 If all get turned off, turn them back on, as
		  having no versions is BAD
 Now write current state into reply buffer 
  write_versions - Set or report the available NFS protocol versions
  Input:
 			buf:		ignored
 			size:		zero
  Output:
 	On success:	passed-in buffer filled with '\n'-terminated C
 			string containing positive or negative integer
 			values representing the current status of each
 			protocol version;
 			return code is the size in bytes of the string
 	On error:	return code is zero or a negative errno value
  OR
  Input:
  			buf:		C string containing whitespace-
  					separated positive or negative
  					integer values representing NFS
  					protocol versions to enable ("+n")
  					or disable ("-n")
 			size:		non-zero length of C string in @buf
  Output:
 	On success:	status of zero or more protocol versions has
 			been updated; passed-in buffer filled with
 			'\n'-terminated C string containing positive
 			or negative integer values representing the
 			current status of each protocol version;
 			return code is the size in bytes of the string
 	On error:	return code is zero or a negative errno value
  Zero-length write.  Return a list of NFSD's current listener
  transports.
  A single 'fd' number was written, in which case it must be for
  a socket of a supported familyprotocol, and we use it as an
  nfsd listener.
 Decrease the count, but don't shut down the service 
  A transport listener is added by writing it's transport name and
  a port number.
 Decrease the count, but don't shut down the service 
  write_ports - Pass a socket file descriptor or transport name to listen on
  Input:
 			buf:		ignored
 			size:		zero
  Output:
 	On success:	passed-in buffer filled with a '\n'-terminated C
 			string containing a whitespace-separated list of
 			named NFSD listeners;
 			return code is the size in bytes of the string
 	On error:	return code is zero or a negative errno value
  OR
  Input:
 			buf:		C string containing an unsigned
 					integer value representing a bound
 					but unconnected socket that is to be
 					used as an NFSD listener; listen(3)
 					must be called for a SOCK_STREAM
 					socket, otherwise it is ignored
 			size:		non-zero length of C string in @buf
  Output:
 	On success:	NFS service is started;
 			passed-in buffer filled with a '\n'-terminated C
 			string containing a unique alphanumeric name of
 			the listener;
 			return code is the size in bytes of the string
 	On error:	return code is a negative errno value
  OR
  Input:
 			buf:		C string containing a transport
 					name and an unsigned integer value
 					representing the port to listen on,
 					separated by whitespace
 			size:		non-zero length of C string in @buf
  Output:
 	On success:	returns zero; NFS service is started
 	On error:	return code is a negative errno value
  write_maxblksize - Set or report the current NFS blksize
  Input:
 			buf:		ignored
 			size:		zero
  OR
  Input:
  			buf:		C string containing an unsigned
  					integer value representing the new
  					NFS blksize
 			size:		non-zero length of C string in @buf
  Output:
 	On success:	passed-in buffer filled with '\n'-terminated C string
 			containing numeric value of the current NFS blksize
 			setting;
 			return code is the size in bytes of the string
 	On error:	return code is zero or a negative errno value
		 force bsize into allowed range and
		  required alignment.
  write_maxconn - Set or report the current max number of connections
  Input:
 			buf:		ignored
 			size:		zero
  OR
  Input:
  			buf:		C string containing an unsigned
  					integer value representing the new
  					number of max connections
 			size:		non-zero length of C string in @buf
  Output:
 	On success:	passed-in buffer filled with '\n'-terminated C string
 			containing numeric value of max_connections setting
 			for this net namespace;
 			return code is the size in bytes of the string
 	On error:	return code is zero or a negative errno value
		
		  Some sanity checking.  We don't have a reason for
		  these particular numbers, but problems with the
		  extremes are:
		 	- Too short: the briefest network outage may
		 	  cause clients to lose all their locks.  Also,
		 	  the frequent polling may be wasteful.
		 	- Too long: do you really want reboot recovery
		 	  to take more than an hour?  Or to make other
		 	  clients wait an hour before being able to
		 	  revoke a dead client's locks?
  write_leasetime - Set or report the current NFSv4 lease time
  Input:
 			buf:		ignored
 			size:		zero
  OR
  Input:
 			buf:		C string containing an unsigned
 					integer value representing the new
 					NFSv4 lease expiry time
 			size:		non-zero length of C string in @buf
  Output:
 	On success:	passed-in buffer filled with '\n'-terminated C
 			string containing unsigned integer value of the
 			current lease expiry time;
 			return code is the size in bytes of the string
 	On error:	return code is zero or a negative errno value
  write_gracetime - Set or report current NFSv4 grace period time
  As above, but sets the time of the NFSv4 grace period.
  Note this should never be set to less than the previous
  lease-period time, but we don't try to enforce this.  (In the common
  case (a new boot), we don't know what the previous lease time was
  anyway.)
  write_recoverydir - Set or report the pathname of the recovery directory
  Input:
 			buf:		ignored
 			size:		zero
  OR
  Input:
 			buf:		C string containing the pathname
 					of the directory on a local file
 					system containing permanent NFSv4
 					recovery data
 			size:		non-zero length of C string in @buf
  Output:
 	On success:	passed-in buffer filled with '\n'-terminated C string
 			containing the current recovery pathname setting;
 			return code is the size in bytes of the string
 	On error:	return code is zero or a negative errno value
  write_v4_end_grace - release grace period for nfsd's v4.x lock manager
  Input:
 			buf:		ignored
 			size:		zero
  OR
  Input:
  			buf:		any value
 			size:		non-zero length of C string in @buf
  Output:
 			passed-in buffer filled with "Y" or "N" with a newline
 			and NULL-terminated C string. This indicates whether
 			the grace period has ended in the current net
 			namespace. Return code is the size in bytes of the
 			string. Writing a string that starts with 'Y', 'y', or
 			'1' to the file will end the grace period for nfsd's v4
 			lock manager.
----------------------------------------------------------------------------
 	populating the filesystem.
 Basically copying rpc_get_inode. 
 Following advice from simple_fill_super documentation: 
 from __rpc_unlink 
 I think this can't happen? 
 XXX: cut'n'paste from simple_fill_super; figure out if we could share
 on success, returns positive number unique to that client. 
 XXX: tossing errors? 
 Taken from __rpc_rmdir: 
 Per-export io stats use same ops as exports file 
 CONFIG_SUNRPC_GSS or CONFIG_SUNRPC_GSS_MODULE 
 last one  {""}
 CONFIG_PROC_FS 
 default lease time 
 Statistics 
 lockd->nfsd callbacks 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2016 Tom Haynes <loghyr@primarydata.com>
  The following implements a super-simple flex-file server
  where the NFSv4.1 mds is also the ds. And the storage is
  the same. I.e., writing to the mds via a NFSv4.1 WRITE
  goes to the same location as the NFSv3 WRITE.
	
	  The super simple flex file server has 1 mirror, 1 data server,
	  and 1 file handle. So instead of 4 allocs, do 1 for now.
	  Zero it out for the stateid - don't want junk in there!
	
	  Avoid layout commit, try to force the IO to the DS,
	  and for fun, cause all IOMODE_RW layout segments to
	  effectively be WRITE only.
 Do not allow a IOMODE_READ segment to have write pemissions 
 Give whole file layout segments 
 SPDX-License-Identifier: GPL-2.0
  XDR support for nfsd
  Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>
  Mapping of S_IF types to NFS file types
  Basic NFSv2 data types (RFC 1094 Section 2.3)
  svcxdr_encode_stat - Encode an NFSv2 status code
  @xdr: XDR stream
  @status: status value to encode
  Return values:
    %false: Send buffer space was exhausted
    %true: Success
  svcxdr_decode_fhandle - Decode an NFSv2 file handle
  @xdr: XDR stream positioned at an encoded NFSv2 FH
  @fhp: OUT: filled-in server file handle
  Return values:
   %false: The encoded file handle was not valid
   %true: @fhp has been initialized
	
	  Some Sun clients put 0xffff in the mode field when they
	  mean 0xffffffff.
		
		  Passing the invalid value useconds=1000000 for mtime
		  is a Sun convention for "set both mtime and atime to
		  current server time".  It's needed to make permissions
		  checks for the "touch" program across v2 mounts to
		  Solaris and Irix boxes work correctly. See description of
		  sattr in section 6.1 of "NFS Illustrated" by
		  Brent Callaghan, Addison-Wesley, ISBN 0-201-32750-5
  svcxdr_encode_fattr - Encode NFSv2 file attributes
  @rqstp: Context of a completed RPC transaction
  @xdr: XDR stream
  @fhp: File handle to encode
  @stat: Attributes to encode
  Return values:
    %false: Send buffer space was exhausted
    %true: Success
  XDR decode functions
 totalcount is ignored 
 beginoffset is ignored 
 totalcount is ignored 
 opaque data 
  XDR encode functions
 no more entries 
  nfssvc_encode_nfscookie - Encode a directory offset cookie
  @resp: readdir result context
  @offset: offset cookie to encode
  The buffer space for the offset cookie has already been reserved
  by svcxdr_encode_entry_common().
 fileid 
 name 
 cookie 
  nfssvc_encode_entry - encode one NFSv2 READDIR entry
  @data: directory context
  @name: name of the object to be encoded
  @namlen: length of that name, in bytes
  @offset: the offset of the previous entry
  @ino: the fileid of this entry
  @d_type: unused
  Return values:
    %0: Entry was successfully encoded.
    %-EINVAL: An encoding problem occured, secondary status code in resp->common.err
  On exit, the following fields are updated:
    - resp->xdr
    - resp->common.err
    - resp->cookie_offset
 The offset cookie for the previous entry 
  XDR release functions
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2016 Tom Haynes <loghyr@primarydata.com>
	
	  Unlike nfsd4_encode_user, we know these will
	  always be stringified.
 8 + len for recording the length, name, and padding 
 The layout segment 
 stripe unit of 1 
 single mirror 
 single data server 
 efficiency 
 single file handle 
 No stats collect hint 
 len + padding for two strings 
	
	  Fill in the overall length and number of volumes at the beginning
	  of the layout.
 1 netaddr 
 1 versions 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2014 Christoph Hellwig.
 pNFS device ID to export fsid mapping 
 retain the whole layout segment on a split. 
		
		  Anything left? If not, then call it done. Note that we don't
		  take the spinlock since this is an optimization and nothing
		  should get added until the cb counter goes to zero.
 Poll the client until it's done with the layout 
 Client gets 2 lease periods to return it 
 10 mili-seconds 
		
		  Unknown error or non-responding client, we'll need to fence.
	
	  We don't want the locks code to timeout the lease for us;
	  we'll remove it ourself if a layout isn't returned
	  in time:
   Common NFSv4 ACL handling code.
   Copyright (c) 2002, 2003 The Regents of the University of Michigan.
   All rights reserved.
   Marius Aamodt Eriksen <marius@umich.edu>
   Jeff Sedlak <jsedlak@umich.edu>
   J. Bruce Fields <bfields@umich.edu>
   Redistribution and use in source and binary forms, with or without
   modification, are permitted provided that the following conditions
   are met:
   1. Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
   2. Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation andor other materials provided with the distribution.
   3. Neither the name of the University nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.
   THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
   WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
   MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
   DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
   FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
   CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
   SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
   BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
   LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
   NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
   SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 mode bit translations: 
 flags used to simulate posix default ACLs 
 XXX: modify functions to return NFS errors; they're only ever
 We only map from NFSv4 to POSIX ACLs when setting ACLs, when we err on the
  side of being more restrictive, so the mode bit mapping below is
  pessimistic.  An optimistic version would be needed to handle DENY's,
  but we expect to coalesce all ALLOWs and DENYs before mapping to mode
 allocate for worst case: one (deny, allow) pair each: 
	
	  Only pas.users and pas.groups need initialization; previous
	  posix_acl_valid() calls ensure that the other fields will be
	  initialized in the following loop.  But, just to placate gcc:
 We'll only care about effective permissions: 
 We assume the acl has been verified with posix_acl_valid. 
 We could deny everything not granted by the owner: 
	
	  but it is equivalent (and simpler) to deny only what is not
	  granted by later entries:
	 In the case of groups, we apply allow ACEs first, then deny ACEs,
 allow ACEs 
 deny ACEs 
	 We just do a bubble sort; easy to do in place, and we're not
	 posix_acl_valid requires that users and groups be in order
 no users or groups 
  While processing the NFSv4 ACE, this maintains bitmasks representing
  which permission bits have been allowed and which denied to a given
  While processing the NFSv4 ACE, this maintains the partial permissions
 Deny unused in this case 
	
	  In the worst case, each individual acl could be for a distinct
	  named user or group, but we don't know which, so we allocate
	  enough space for either:
	
	  ACLs with no ACEs are treated differently in the inheritable
	  and effective cases: when there are no inheritable ACEs,
	  calls ->set_acl with a NULL ACL structure.
	
	  When there are no effective ACEs, the following will end
	  up setting a 3-element effective posix ACL with all
	  permissions zero.
 Note we also include a MASK ACE in this case: 
 Allow all bits in the mask not already denied: 
 Deny all bits in the mask not already allowed: 
 Not found: 
 Not found: 
		
		  Note that when only one of FILE_INHERIT or DIRECTORY_INHERIT
		  is set, we're effectively turning on the other.  That's OK,
		  according to rfc 3530.
 Get inode 
  return the size of the struct nfs4_acl required to represent an acl
  with @entries entries.
 SPDX-License-Identifier: GPL-2.0
  Request reply cache. This is currently a global cache, but this may
  change in the future and be a per-client cache.
  This code is heavily inspired by the 44BSD implementation, although
  it does things a bit differently.
  Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>
  We use this value to determine the number of hash buckets from the max
  cache size, the idea being that when the cache is at its maximum number
  of entries, then this should be the average number of entries per bucket.
  Put a cap on the size of the DRC based on the amount of available
  low memory in the machine.
   64MB:    8192
  128MB:   11585
  256MB:   16384
  512MB:   23170
    1GB:   32768
    2GB:   46340
    4GB:   65536
    8GB:   92681
   16GB:  131072
  ...with a hard cap of 256k entries. In the worst case, each entry will be
  ~1k, so the above numbers should give a rough max of the amount of memory
  used in k.
  XXX: these limits are per-container, so memory used will increase
  linearly with number of containers.  Maybe that's OK.
  Compute the number of hash buckets we need. Divide the max cachesize by
  the "target" max bucket size, and round up to next power of two.
  Move cache entry to end of LRU list, and queue the cleaner to run if it's
  not already scheduled.
		
		  Don't free entries attached to calls that are still
		  in-progress, but do keep scanning the list.
  Walk the LRU list and prune off entries that are older than RC_EXPIRE.
  Also prune the oldest ones when the total exceeds the max number of entries.
  Walk an xdr_buf and get a CRC for at most the first RC_CSUMLEN bytes
 rq_arg.head first 
 Continue into page array 
  Search the request hash for an entry that matches the given rqstp.
  Must be called with cache_lock held. Returns the found entry or
  inserts an empty key on failure.
 tally hash chain length stats 
 prefer to keep the smallest cachesize possible here 
  nfsd_cache_lookup - Find an entry in the duplicate reply cache
  @rqstp: Incoming Call to find
  Try to find an entry matching the current call in the cache. When none
  is found, we try to grab the oldest expired entry off the LRU list. If
  a suitable one isn't there, then drop the cache_lock and allocate a
  new one, then search again in case one got inserted while this thread
  didn't hold the lock.
  Return values:
    %RC_DOIT: Process the request normally
    %RC_REPLY: Reply from cache
    %RC_DROPIT: Do not process the request further
	
	  Since the common case is a cache miss followed by an insert,
	  preallocate an entry.
 We found a matching entry which is either in progress or done. 
 Request being processed 
	 From the hall of fame of impractical attacks:
 Compose RPC reply header 
 should not happen 
  nfsd_cache_update - Update an entry in the duplicate reply cache.
  @rqstp: svc_rqst with a finished Reply
  @cachetype: which cache to update
  @statp: Reply's status code
  This is called from nfsd_dispatch when the procedure has been
  executed and the complete reply is in rqstp->rq_res.
  We're copying around data here rather than swapping buffers because
  the toplevel loop requires max-sized buffers, which would be a waste
  of memory for a cache with a max reply size of 100 bytes (diropokres).
  If we should start to use different types of cache entries tailored
  specifically for attrstat and fh's, we may save even more space.
  Also note that a cachetype of RC_NOCACHE can legally be passed when
  nfsd failed to encode a reply that otherwise would have been cached.
  In this case, nfsd_cache_update is called with statp == NULL.
 Don't cache excessive amounts of data and XDR failures 
  Copy cached reply to current reply buffer. Should always fit.
  FIXME as reply is in a page, we should just attach the page, and
  keep a refcount....
  Note that fields may be added, removed or reordered in the future. Programs
  scraping this file for info should test the labels to ensure they're
  getting the correct field.
 SPDX-License-Identifier: GPL-2.0
  XDR support for nfsdprotocol version 3.
  Copyright (C) 1995, 1996, 1997 Olaf Kirch <okir@monad.swb.de>
  2003-08-09 Jamie Lokier: Use htonl() for nanoseconds, not htons()!
  Force construction of an empty post-op attr
  time_delta. {1, 0} means the server is accurate only
  to the nearest second.
  Mapping of S_IF types to NFS file types
  Basic NFSv3 data types (RFC 1813 Sections 2.5 and 2.6)
  svcxdr_decode_nfs_fh3 - Decode an NFSv3 file handle
  @xdr: XDR stream positioned at an undecoded NFSv3 FH
  @fhp: OUT: filled-in server file handle
  Return values:
   %false: The encoded file handle was not valid
   %true: @fhp has been initialized
  svcxdr_encode_nfsstat3 - Encode an NFSv3 status code
  @xdr: XDR stream
  @status: status value to encode
  Return values:
    %false: Send buffer space was exhausted
    %true: Success
 used 
 rdev 
 fileid 
  svcxdr_encode_post_op_attr - Encode NFSv3 post-op attributes
  @rqstp: Context of a completed RPC transaction
  @xdr: XDR stream
  @fhp: File handle to encode
  Return values:
    %false: Send buffer space was exhausted
    %true: Success
	
	  The inode may be NULL if the call failed because of a
	  stale file handle. In this case, no attributes are
	  returned.
  Encode weak cache consistency data
 before 
 after 
  Fill in the pre_op attr for the wcc data
 Grab the times from inode anyway 
  Fill in the post_op attr for the wcc data
  XDR decode functions
 opaque data 
 request sanity 
 request sanity 
 Valid XDR but illegal file types 
 dircount is ignored 
  XDR encode functions
 GETATTR 
 SETATTR, REMOVE, RMDIR 
 LOOKUP 
 ACCESS 
 READLINK 
 READ 
 WRITE 
 CREATE, MKDIR, SYMLINK, MKNOD 
 RENAME 
 LINK 
 READDIR 
 no more entries 
			
			  Don't return filehandle for ".." if we're at
			  the filesystem or export root:
  nfs3svc_encode_cookie3 - Encode a directory offset cookie
  @resp: readdir result context
  @offset: offset cookie to encode
  The buffer space for the offset cookie has already been reserved
  by svcxdr_encode_entry3_common().
 fileid 
 name 
 cookie 
  nfs3svc_encode_entry3 - encode one NFSv3 READDIR entry
  @data: directory context
  @name: name of the object to be encoded
  @namlen: length of that name, in bytes
  @offset: the offset of the previous entry
  @ino: the fileid of this entry
  @d_type: unused
  Return values:
    %0: Entry was successfully encoded.
    %-EINVAL: An encoding problem occured, secondary status code in resp->common.err
  On exit, the following fields are updated:
    - resp->xdr
    - resp->common.err
    - resp->cookie_offset
 The offset cookie for the previous entry 
  nfs3svc_encode_entryplus3 - encode one NFSv3 READDIRPLUS entry
  @data: directory context
  @name: name of the object to be encoded
  @namlen: length of that name, in bytes
  @offset: the offset of the previous entry
  @ino: the fileid of this entry
  @d_type: unused
  Return values:
    %0: Entry was successfully encoded.
    %-EINVAL: An encoding problem occured, secondary status code in resp->common.err
  On exit, the following fields are updated:
    - resp->xdr
    - resp->common.err
    - resp->cookie_offset
 The offset cookie for the previous entry 
 total bytes 
 free bytes 
 user available bytes 
 total inodes 
 free inodes 
 user available inodes 
 mean unchanged time 
 FSSTAT 
 FSINFO 
 PATHCONF 
 COMMIT 
  XDR release functions
 SPDX-License-Identifier: GPL-2.0
  Process version 3 NFS requests.
  Copyright (C) 1996, 1997, 1998 Olaf Kirch <okir@monad.swb.de>
 NF3NON 
 NF3REG 
 NF3DIR 
 NF3BLK 
 NF3CHR 
 NF3LNK 
 NF3SOCK 
 NF3FIFO 
  NULL call.
  Get a file's attributes
  Set a file's attributes
  Look up a path name component
  Check file access
  Read a symlink.
 Read the symlink. 
  Read a portion of a file.
	 Obtain buffer pointer for payload.
	  1 (status) + 22 (post_op_attr) + 1 (count) + 1 (eof)
	  + 1 (xdr opaque byte count) = 26
  Write data to a file
  With NFSv3, CREATE processing is a lot easier than with NFSv2.
  At least in theory; we'll see how it fares in practice when the
  first reports about SunOS compatibility problems start to pour in...
 Unfudge the mode bits 
 Now create the file and set attributes 
  Make directory. This operation is not idempotent.
  Make socketfifodevice.
  Remove filefifosocket etc.
 Unlink. -S_IFDIR means file must not be a directory 
  Remove a directory
 Reserve room for the NULL ptr & eof flag (-2 words) 
	 This is xdr_init_encode(), but it assumes that
  Read a portion of a directory.
 Recycle only pages that were part of the reply 
  Read a portion of a directory, including file handles and attrs.
  For now, we choose to ignore the dircount parameter.
 Recycle only pages that were part of the reply 
  Get file system stats
  Get file system info
	 Check special features of the file system. May request
	  different readwrite sizes for file systems known to have
 Note that we don't care for remote fs's here 
  Get pathconf info for the specified file
 Set default pathconf 
 at least 
 at least 
 Note that we don't care for remote fs's here 
  Commit a file (range) to stable storage.
  NFSv3 Server procedures.
  Only the results of non-idempotent operations are cached.
 status
 filehandle with length 
 attributes 
 post attributes - conditional 
 WCC attributes 
   Copyright (c) 2001 The Regents of the University of Michigan.
   All rights reserved.
   Kendrick Smith <kmsmith@umich.edu>
   Andy Adamson <andros@umich.edu>
   Redistribution and use in source and binary forms, with or without
   modification, are permitted provided that the following conditions
   are met:
   1. Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
   2. Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation andor other materials provided with the distribution.
   3. Neither the name of the University nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.
   THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
   WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
   MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
   DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
   FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
   CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
   SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
   BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
   LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
   NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
   SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 Index of predefined Linux callback client operations 
 args 
 minorversion 0 only 
 res 
  Encodedecode NFSv4 CB basic data types
  Basic NFSv4 callback data types are defined in section 15 of RFC
  3530: "Network File System (NFS) version 4 Protocol" and section
  20 of RFC 5661: "Network File System (NFS) Version 4 Minor Version
  1 Protocol"
 	nfs_cb_opnum4
 	enum nfs_cb_opnum4 {
 		OP_CB_GETATTR		= 3,
 		  ...
 	};
  nfs_fh4
 	typedef opaque nfs_fh4<NFS4_FHSIZE>;
  stateid4
 	struct stateid4 {
 		uint32_t	seqid;
 		opaque		other[12];
 	};
  sessionid4
 	typedef opaque sessionid4[NFS4_SESSIONID_SIZE];
  nfsstat4
  If we cannot translate the error, the recovery routines should
  handle it.
  Note: remaining NFSv4 error codes have values > 10000, so should
  not conflict with native Linux error codes.
  CB_COMPOUND4args
 	struct CB_COMPOUND4args {
 		utf8str_cs	tag;
 		uint32_t	minorversion;
 		uint32_t	callback_ident;
 		nfs_cb_argop4	argarray<>;
 	};
 empty tag 
 argarray element count 
  Update argarray element count
  CB_COMPOUND4res
 	struct CB_COMPOUND4res {
 		nfsstat4	status;
 		utf8str_cs	tag;
 		nfs_cb_resop4	resarray<>;
 	};
 Ignore the tag 
  CB_RECALL4args
 	struct CB_RECALL4args {
 		stateid4	stateid;
 		bool		truncate;
 		nfs_fh4		fh;
 	};
 truncate 
  CB_SEQUENCE4args
 	struct CB_SEQUENCE4args {
 		sessionid4		csa_sessionid;
 		sequenceid4		csa_sequenceid;
 		slotid4			csa_slotid;
 		slotid4			csa_highest_slotid;
 		bool			csa_cachethis;
 		referring_call_list4	csa_referring_call_lists<>;
 	};
 csa_sequenceid 
 csa_slotid 
 csa_highest_slotid 
 csa_cachethis 
 csa_referring_call_lists 
  CB_SEQUENCE4resok
 	struct CB_SEQUENCE4resok {
 		sessionid4	csr_sessionid;
 		sequenceid4	csr_sequenceid;
 		slotid4		csr_slotid;
 		slotid4		csr_highest_slotid;
 		slotid4		csr_target_highest_slotid;
 	};
 	union CB_SEQUENCE4res switch (nfsstat4 csr_status) {
 	case NFS4_OK:
 		CB_SEQUENCE4resok	csr_resok4;
 	default:
 		void;
 	};
  Our current back channel implmentation supports a single backchannel
  with a single slot.
	
	  If the server returns different values for sessionID, slotID or
	  sequence number, the server is looney tunes.
	
	  FIXME: process highest slotid and target highest slotid
  NFSv4.0 and NFSv4.1 XDR encode functions
  NFSv4.0 callback argument types are defined in section 15 of RFC
  3530: "Network File System (NFS) version 4 Protocol" and section 20
  of RFC 5661:  "Network File System (NFS) Version 4 Minor Version 1
  Protocol".
  NB: Without this zero space reservation, callbacks over krb5p fail
  20.2. Operation 4: CB_RECALL - Recall a Delegation
  NFSv4.0 and NFSv4.1 XDR decode functions
  NFSv4.0 callback result types are defined in section 15 of RFC
  3530: "Network File System (NFS) version 4 Protocol" and section 20
  of RFC 5661:  "Network File System (NFS) Version 4 Minor Version 1
  Protocol".
  20.2. Operation 4: CB_RECALL - Recall a Delegation
  CB_LAYOUTRECALL4args
 	struct layoutrecall_file4 {
 		nfs_fh4         lor_fh;
 		offset4         lor_offset;
 		length4         lor_length;
 		stateid4        lor_stateid;
 	};
 	union layoutrecall4 switch(layoutrecall_type4 lor_recalltype) {
 	case LAYOUTRECALL4_FILE:
 		layoutrecall_file4 lor_layout;
 	case LAYOUTRECALL4_FSID:
 		fsid4              lor_fsid;
 	case LAYOUTRECALL4_ALL:
 		void;
 	};
 	struct CB_LAYOUTRECALL4args {
 		layouttype4             clora_type;
 		layoutiomode4           clora_iomode;
 		bool                    clora_changed;
 		layoutrecall4           clora_recall;
 	};
 CONFIG_NFSD_PNFS 
  struct write_response4 {
 	stateid4	wr_callback_id<1>;
 	length4		wr_count;
 	stable_how4	wr_committed;
 	verifier4	wr_writeverf;
  };
  union offload_info4 switch (nfsstat4 coa_status) {
 	case NFS4_OK:
 		write_response4	coa_resok4;
 	default:
 	length4		coa_bytes_copied;
  };
  struct CB_OFFLOAD4args {
 	nfs_fh4		coa_fh;
 	stateid4	coa_stateid;
 	offload_info4	coa_offload_info;
  };
 We always return success if bytes were written 
  RPC procedure tables
  Note on the callback rpc program version number: despite language in rfc
  5661 section 18.36.3 requiring servers to use 4 in this field, the
  official xdr descriptions for both 4.0 and 4.1 specify version 1, and
  in practice that appears to be what implementations use.  The section
  18.36.3 language is expected to be fixed in an erratum.
	
	  nfsd4_lease is set to at most one hour in __nfsd4_write_time,
	  so we can use 32-bit math on it. Warn if that assumption
	  ever stops being true.
 Create RPC client 
	 XXX: release method to ensure we set the cb channel down if
  Poke the callback thread to process any updates to the callback
  parameters, and send a null probe.
  There's currently a single callback channel slot.
  If the slot is available, then mark it busy.  Otherwise, set the
  thread for sleeping on the callback RPC wait queue.
 Race breaker 
  TODO: cb_sequence should support referring call lists, cachethis, multiple
  slots, and mark callback channel down on communication errors.
	
	  cb_seq_status is only set in decode_cb_sequence4res,
	  and so will remain 1 if an rpc level failure occurs.
		
		  If the backchannel connection was shut down while this
		  task was queued, we need to resubmit it after setting up
		  a new backchannel connection.
		 
		  Note that if we lost our callback connection permanently
		  the submission code will error out, so we don't need to
		  handle that case here.
		
		  No need for lock, access serialized in nfsd4_cb_prepare
		 
		  RFC5661 20.9.3
		  If CB_SEQUENCE returns an error, then the state of the slot
		  (sequence ID, cached reply) MUST NOT change.
 must be called under the state lock 
	
	  Note this won't actually result in a null callback;
	  instead, nfsd4_run_cb_null() will detect the killed
	  client, destroy the rpc client, and stop:
 requires cl_lock: 
  Note there isn't a lot of locking in this code; instead we depend on
  the fact that it is run from the callback_wq, which won't run two
  work items at once.  So, for example, callback_wq handles all access
  of cl_cb_client and all calls to rpc_create or rpc_shutdown_client.
	
	  This is either an update, or the client dying; in either case,
	  kill the old client:
	
	  Only serialized callback code is allowed to clear these
	  flags; main nfsd code can only set them:
 Callback channel broken, or client killed; give up: 
	
	  Don't send probe messages for 4.1 or later.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2011 Bryan Schumaker <bjschuma@netapp.com>
  Uses debugfs to create fault injection points for client testing
 Deal with any embedded newlines in the string 
 on success, claim we got the whole input 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2014-2016 Christoph Hellwig.
	
	  Some clients barf on non-zero block numbers for NONE or INVALID
	  layouts, so make sure to zero the whole structure.
			
			  Crack monkey special case from section 2.3.1.
	
	  Pretend that we send notification to the client.  This is a blatant
	  lie to force recent Linux clients to cache our device IDs.
	  We rarely ever change the device ID, so the harm of leaking deviceids
	  for a while isn't too bad.  Unfortunately RFC5661 is a complete mess
	  in this regard, but I filed errata 4119 for this a while ago, and
	  hopefully the Linux client will eventually start caching deviceids
	  without this again.
 CONFIG_NFSD_BLOCKLAYOUT 
  We use the client ID as a unique key for the reservations.
  This allows us to easily fence a client when recalls fail.
	
	  Pretend that we send notification to the client.  This is a blatant
	  lie to force recent Linux clients to cache our device IDs.
	  We rarely ever change the device ID, so the harm of leaking deviceids
	  for a while isn't too bad.  Unfortunately RFC5661 is a complete mess
	  in this regard, but I filed errata 4119 for this a while ago, and
	  hopefully the Linux client will eventually start caching deviceids
	  without this again.
 CONFIG_NFSD_SCSILAYOUT 
 SPDX-License-Identifier: GPL-2.0
  NFS server file handle treatment.
  Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>
  Portions Copyright (C) 1999 G. Allen Morris III <gam3@acm.org>
  Extensive rewrite by Neil Brown <neilb@cse.unsw.edu.au> Southern-Spring 1999
  ... and again Southern-Winter 2001 to support export_operations
  our acceptability function.
  if NOSUBTREECHECK, accept anything
  if not, require that we can walk up to exp->ex_dentry
  doing some checks on the 'x' bits
 make sure parents give x permission to user 
 Type check. The correct error return for type mismatches does not seem to be
  generally agreed upon. SunOS seems to use EISDIR if file isn't S_IFREG; a
  comment in the NFSv3 spec says this is incorrect (implementation notes for
  the write call).
 the caller doesn't care 
	
	  v4 has an error more specific than err_notdir which we should
	  return in preference to err_notdir:
 We don't require gss requests to use low ports: 
 Check if the request originated from a secure port. 
 Set user creds for this exportpoint 
	
	  v2v3 clients have no need for the V4ROOT export--they use
	  the mount protocl instead; also, further V4ROOT checks may be
	  in v4-specific code, in which case v2v3 clients could bypass
	  them.
	
	  We're exposing only the directories and symlinks that have to be
	  traversed on the way to real exports:
	
	  A pseudoroot export gives permission to access only one
	  single directory; the kernel has to make another upcall
	  before granting access to anything else under it:
  Use the given filehandle to look up the corresponding export and
  dentry.  On success, the results are used to set fh_export and
  fh_dentry.
 deprecated, convert to type 3 
		
		  struct knfsd_fh uses host-endian fields, which are
		  sometimes used to hold net-endian values. This
		  confuses sparse, so we must use __force here to
		  keep it from complaining.
		 Elevate privileges so that the lack of 'r' or 'x'
		  permission on some parent directory will
		  not stop exportfs_decode_fh from being able
		  to reconnect a directory into the dentry cache.
		  The same problem can affect "SUBTREECHECK" exports,
		  but as nfsd_acceptable depends on correct
		  access control settings being in effect, we cannot
		  fix that case easily.
	
	  Look up the dentry using the NFS file handle.
  fh_verify - filehandle lookup and access checking
  @rqstp: pointer to current rpc request
  @fhp: filehandle to be verified
  @type: expected type of object pointed to by filehandle
  @access: type of access needed to object
  Look up a dentry from the on-the-wire filehandle, check the client's
  access to the export, and set the current task's credentials.
  Regardless of success or failure of fh_verify(), fh_put() should be
  called on @fhp when the caller is finished with the filehandle.
  fh_verify() may be called multiple times on a given filehandle, for
  example, when processing an NFSv4 compound.  The first call will look
  up a dentry using the on-the-wire filehandle.  Subsequent calls will
  skip the lookup and just perform the other checks and possibly change
  the current task's credentials.
  @type specifies the type of object expected using one of the S_IF
  constants defined in includelinuxstat.h.  The caller may use zero
  to indicate that it doesn't care, or a negative integer to indicate
  that it expects something not of the given type.
  @access is formed from the NFSD_MAY_ constants defined in
  fsnfsdvfs.h.
	
	  We still have to do all these permission checks, even when
	  fh_dentry is already set:
	  	- fh_verify may be called multiple times with different
	  	  "access" arguments (e.g. nfsd_proc_create calls
	  	  fh_verify(...,NFSD_MAY_EXEC) first, then later (in
	  	  nfsd_create) calls fh_verify(...,NFSD_MAY_CREATE).
	 	- in the NFSv4 case, the filehandle may have been filled
	 	  in by fh_compose, and given a dentry, but further
	 	  compound operations performed with that filehandle
	 	  still need permissions checks.  In the worst case, a
	 	  mountpoint crossing may have changed the export
	 	  options, and we may now need to use a different uid
	 	  (for example, if different id-squashing options are in
	 	  effect on the new filesystem).
	
	  pseudoflavor restrictions are not enforced on NLM,
	  which clients virtually always use auth_sys for,
	  even while using RPCSEC_GSS for NFS.
	
	  Clients may expect to be able to use auth_sys during mount,
	  even if they use gss for everything else; see section 2.3.2
	  of rfc 2623.
 Finally, check access permissions. 
  Compose a file handle for an NFS reply.
  Note that when first composed, the dentry may not yet have
  an inode.  In this case a call to fh_update should be made
  before the fh goes out on the wire ...
		
		  As the fsid -> filesystem mapping was guided by
		  user-space, there is no guarantee that the filesystem
		  actually supports that fsid type. If it doesn't we
		  loop around again without ref_fh set.
 for newer device numbers, we must use a newer fsid format 
	 ref_fh is a reference file handle.
	  if it is non-null and for the same filesystem, then we should compose
	  a filehandle which is of the same version, where possible.
	 Choose filehandle version and fsid type based on
	  the reference filehandle (if it is in the same export)
	  or the export options.
 If we have a ref_fh, then copy the fh_no_wcc setting from it. 
 our internal copy 
  Update file handle information after changing a dentry.
  This is only called by nfsd_create, nfsd_create_v3 and nfsd_proc_create
  Release a file handle.
  Shorthand for dprintk()'s
	 either a UUID type filehandle, or the filehandle doesn't
	  match the export.
 SPDX-License-Identifier: GPL-2.0
  Process version 3 NFSACL requests.
  Copyright (C) 2002-2003 Andreas Gruenbacher <agruen@suse.de>
 FIXME: nfsacl.h is a broken header 
  NULL call.
  Get the Access andor Default ACL of a file.
 Solaris returns the inode's minimum ACL. 
		 Check how Solaris handles requests for the Default ACL
 resp->acl_{access,default} are released in nfs3svc_release_getacl. 
  Set the Access andor Default ACL of a file.
	 argp->acl_{access,default} may have been allocated in
  XDR decode functions
  XDR encode functions
 GETACL 
 SETACL 
  XDR release functions
 status
 attributes 
 post attributes - conditional 
 Access Control List 
 SPDX-License-Identifier: GPL-2.0
  Central processing for nfsd.
  Authors:	Olaf Kirch (okir@monad.swb.de)
  Copyright (C) 1995, 1996, 1997 Olaf Kirch <okir@monad.swb.de>
  nfsd_mutex protects nn->nfsd_serv -- both the pointer itself and the members
  of the svc_serv struct. In particular, ->sv_nrthreads but also to some
  extent ->sv_temp_socks and ->sv_permsocks. It also protects nfsdstats.th_cnt
  If (out side the lock) nn->nfsd_serv is non-NULL, then it must point to a
  properly initialised 'struct svc_serv' with ->sv_nrthreads > 0. That number
  of nfsd threads must exist and each must listed in ->sp_all_threads in each
  entry of ->sv_pools[].
  Transitions of the thread count between zero and non-zero are of particular
  interest since the svc_serv needs to be created and initialized at that
  point, or freed.
  Finally, the nfsd_mutex also protects some of the global variables that are
  accessed when nfsd starts and that are settable via the write_ routines in
  nfsctl.c. In particular:
 	user_recovery_dirname
 	user_lease_time
 	nfsd_versions
  nfsd_drc_lock protects nfsd_drc_max_pages and nfsd_drc_pages_used.
  nfsd_drc_max_pages limits the total amount of memory available for
  version 4.1 DRC caches.
  nfsd_drc_pages_used tracks the current version 4.1 DRC memory usage.
 defined(CONFIG_NFSD_V2_ACL) || defined(CONFIG_NFSD_V3_ACL) 
 program number 
 nr of entries in nfsd_version 
 version table 
 program name 
 authentication class 
 version table 
 export authentication 
 All compiled versions are enabled by default 
 All minor versions are enabled by default 
  Maximum number of nfsd processes
		
		  This is opaque to client, so no need to byte-swap. Use
		  __force to keep sparse happy. y2038 time_t overflow is
		  irrelevant in this usage
 Only used under nfsd_mutex, so this atomic may be overkill: 
 check if the notifier still has clients 
	
	  write_ports can create the server without actually starting
	  any threads--if we get shut down before any threads are
	  started, then nfsd_last_thread will be run before any of this
	  other initialization has been done except the rpcb information.
  Each session guarantees a negotiated per slot memory cache for replies
  which in turn consumes memory beyond the v2v3v4.0 server. A dedicated
  NFSv4.1 server might want to use more memory for a DRC than a machine
  with mutiple services.
  Impose a hard limit on the number of pages for the DRC which varies
  according to the machines free pages. This is of course only a default.
  For now this is a #defined shift which could be under admin control
  in the future.
	
	  Aim for 14096 of memory per thread This gives 1MB on 4Gig
	  machines, but only uses 32K on 128M machines.  Bottom out at
	  8K on 32M and smaller.  Of course, this is only a default.
 Kill outstanding nfsd threads 
 Wait for shutdown of nfsd_serv to complete 
 check if the notifier is already set 
 enforce a global maximum number of threads 
 total too large: scale down requested numbers 
	
	  There must always be a thread in pool 0; the admin
	  can't shut down NFS completely using pool_threads.
 apply the new numbers 
  Adjust the number of threads and return the new number of threads.
  This is also the function that starts the server if necessary, if
  this is the first time nrservs is nonzero.
	 We are holding a reference to nn->nfsd_serv which
	  we don't want to count in the return value,
	  so subtract 1
 Release server 
  This is the NFS server kernel thread
 Lock module and set up kernel thread 
	 At this point, the thread shares current->fs
	  with the init process. We need to create files with the
	
	  thread is spawned with all signals set to SIG_IGN, re-enable
	  the ones that will bring down the thread
	
	  The main request loop
 Update sv_maxconn if it has changed 
		
		  Find a socket with data available and call its
		  recvfrom routine.
 Clear signals before calling svc_exit_thread() 
 Release the thread 
 Release module 
  nfsd_dispatch - Process an NFS or NFSACL Request
  @rqstp: incoming request
  @statp: pointer to location of accept_stat field in RPC Reply buffer
  This RPC dispatcher integrates the NFS server's duplicate reply cache.
  Return values:
   %0: Processing complete; do not send a Reply
   %1: Processing complete; send Reply in rqstp->rq_res
	
	  Give the xdr decoder a chance to change this if it wants
	  (necessary in the NFSv4.0 compound case)
	
	  Need to grab the location to store the status, as
	  NFSv4 does some encoding while processing
  nfssvc_decode_voidarg - Decode void arguments
  @rqstp: Server RPC transaction context
  @xdr: XDR stream positioned at arguments to decode
  Return values:
    %false: Arguments were not valid
    %true: Decoding was successful
  nfssvc_encode_voidres - Encode void results
  @rqstp: Server RPC transaction context
  @xdr: XDR stream into which to encode results
  Return values:
    %false: Local error while encoding
    %true: Encoding was successful
 bump up the psudo refcount while traversing 
 this function really, really should have been called svc_put() 
 SPDX-License-Identifier: GPL-2.0
  procfs-based user access to knfsd statistics
  procnetrpcnfsd
  Format:
 	rc <hits> <misses> <nocache>
 			Statistsics for the reply cache
 	fh <stale> <deprecated filehandle cache stats>
 			statistics for filehandle lookup
 	io <bytes-read> <bytes-written>
 			statistics for IO throughput
 	th <threads> <deprecated thread usage histogram stats>
 			number of threads
 	ra <deprecated ra-cache stats>
 	plus generic RPC stats (see netsunrpcstats.c)
  Copyright (C) 1995, 1996, 1997 Olaf Kirch <okir@monad.swb.de>
 thread usage: 
 deprecated thread usage histogram stats 
 deprecated ra-cache stats 
 show my rpc info 
 Show count for individual nfsv4 operations 
 Writing operation numbers 0 1 2 also for maintaining uniformity 
  Open file cache.
  (c) 2015 - Jeff Layton <jeff.layton@primarydata.com>
 FIXME: dynamically size this for the machine somehow? 
 We only care about NFSD_MAY_READWRITE for this cache 
 Avoid soft lockup race with nfsd_file_mark_put() 
 allocate a new nfm 
		
		  If the add was successful, then return the object.
		  Otherwise, we need to put the reference we hold on the
		  nfm_mark. The fsnotify code will take a reference and put
		  it on failure, so we can't just free it directly. It's also
		  not safe to call fsnotify_destroy_mark on it as the
		  mark->group will be NULL. Thus, we can't let the nfm_ref
		  counter drive the destruction at this point.
  Return true if the file was unhashed.
 keep final reference for nfsd_file_lru_dispose 
  Note this can deadlock with nfsd_file_cache_purge.
	
	  Do a lockless refcount check. The hashtable holds one reference, so
	  we look to see if anything else has a reference, or if any have
	  been put since the shrinker last ran. Those don't get unhashed and
	  released.
	 
	  Note that in the put path, we set the flag and then decrement the
	  counter. Here we check the counter and then test and clear the flag.
	  That order is deliberate to ensure that we can do this locklessly.
	
	  Don't throw out files that are still undergoing IO or
	  that have uncleared errors pending.
  nfsd_file_close_inode_sync - attempt to forcibly close a nfsd_file
  @inode: inode of the file to attempt to remove
  Walk the whole hash bucket, looking for any files that correspond to "inode".
  If any do, then unhash them and put the hashtable reference to them and
  destroy any that had their last reference put. Also ensure that any of the
  fputs also have their final __fput done as well.
  nfsd_file_close_inode - attempt a delayed close of a nfsd_file
  @inode: inode of the file to attempt to remove
  Walk the whole hash bucket, looking for any files that correspond to "inode".
  If any do, then unhash them and put the hashtable reference to them and
  destroy any that had their last reference put.
  nfsd_file_delayed_close - close unused nfsd_files
  @work: dummy
  Walk the LRU list and close any entries that have not been used since
  the last scan.
  Note this can deadlock with nfsd_file_cache_purge.
 Only close files for F_SETLEASE leases 
 Should be no marks on non-regular files 
 don't close files if this was not the last link 
  Note this can deadlock with nfsd_file_lru_cb.
			
			  Deadlock detected! Something marked this entry as
			  unhased, but hasn't removed it from the hash list.
	
	  make sure all callers of nfsd_file_lru_cb are done before
	  calling nfsd_file_cache_purge
  nfsd_file_is_cached - are there any cached open files for this fh?
  @inode: inode of the file to check
  Scan the hashtable for open files that match this fh. Returns true if there
  are any, and false if not.
 FIXME: skip this if fh_dentry is already set? 
 Did construction of this file fail? 
 Take reference for the hashtable 
	
	  If construction failed, or we raced with a call to unlink()
	  then unhash.
  Note that fields may be added, removed or reordered in the future. Programs
  scraping this file for info should test the labels to ensure they're
  getting the correct field.
	
	  No need for spinlocks here since we're not terribly interested in
	  accuracy. We do take the nfsd_mutex simply to ensure that we
	  don't end up racing with server shutdown
   Server-side procedures for NFSv4.
   Copyright (c) 2002 The Regents of the University of Michigan.
   All rights reserved.
   Kendrick Smith <kmsmith@umich.edu>
   Andy Adamson   <andros@umich.edu>
   Redistribution and use in source and binary forms, with or without
   modification, are permitted provided that the following conditions
   are met:
   1. Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
   2. Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation andor other materials provided with the distribution.
   3. Neither the name of the University nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.
   THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
   WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
   MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
   DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
   FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
   CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
   SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
   BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
   LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
   NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
   SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 default to 15 mins 
		
		  XXX: We should really fail the whole open, but we may
		  already have created a new file, so it may be too
		  late.  For now this seems the least of evils:
  if error occurs when setting the acl, just clear the acl bit
  in the returned attr bitmap.
		
		  We should probably fail the whole open at this point,
		  but we've already created the file, so it's too late;
		  So this seems the least of evils:
	
	  Using err_symlink as our catch-all case may look odd; but
	  there's no other obvious error for this case in 4.0, and we
	  happen to know that it will cause the linux v4 client to do
	  the right thing on attempts to open something other than a
	  regular file.
		 FIXME: check session persistence and pnfs flags.
		  The nfsv4.1 spec requires the following semantics:
		 
		  Persistent   | pNFS   | Server REQUIRED | Client Allowed
		  Reply Cache  | server |                 |
		  -------------+--------+-----------------+--------------------
		  no           | no     | EXCLUSIVE4_1    | EXCLUSIVE4_1
		               |        |                 | (SHOULD)
		               |        | and EXCLUSIVE4  | or EXCLUSIVE4
		               |        |                 | (SHOULD NOT)
		  no           | yes    | EXCLUSIVE4_1    | EXCLUSIVE4_1
		  yes          | no     | GUARDED4        | GUARDED4
		  yes          | yes    | GUARDED4        | GUARDED4
		
		  Note: create modes (UNCHECKED,GUARDED...) are the same
		  in NFSv4 as in v3 except EXCLUSIVE4_1.
		
		  Following rfc 3530 14.2.16, and rfc 5661 18.16.4
		  use the returned bitmask to indicate which attributes
		  we used to store the verifier:
		
		  Note this may exit with the parent still locked.
		  We will hold the lock until nfsd4_open's final
		  lookup, to prevent renames or unlinks until we've had
		  a chance to an acquire a delegation if appropriate.
	 We don't know the target directory, and therefore can not
	 set the change info
	
	  In the delegation case, the client is telling us about an
	  open that it already performed locally, some time ago.  We
	  should let it succeed now if possible.
	 
	  In the case of a CLAIM_FH open, on the other hand, the client
	  may be counting on us to enforce permissions (the Linux 4.1
	  client uses this for normal opens, for example).
 This check required by spec. 
	
	  RFC5661 18.51.3
	  Before RECLAIM_COMPLETE done, server should deny new lock
 check seqid for replay. set nfs4_owner 
	 Openowner is now set, so sequence id will get bumped.  Now we need
	
	  nfsd4_process_open2() does the actual opening of the file.  If
	  successful, it (1) truncates the file if open->op_truncate was
	  set, (2) sets open->op_stateid, (3) sets open->op_delegation.
  OPEN is the only seqid-mutating operation whose decoding can fail
  with a seqid-mutating error (specifically, decoding of user names in
  the attributes).  Therefore we have to do some processing to look up
  the stateowner so that we can bump the seqid.
  filehandle-manipulating ops.
  misc nfsv4 ops
	
	  If we do a zero copy read, then a client will see read data
	  that reflects the state of the file after performing the
	  following compound.
	 
	  To ensure proper ordering, we therefore turn off zero copy if
	  the client wants us to do more in this compound:
 check stateid 
 no need to check permission - this will be done in nfsd_readdir() 
 See rfc 5661 section 2.6.3.1.1.8 
 fix up for NFS-specific error code 
 only 1 thread should stop the copy 
  setup a work entry in the ssc delayed unmount list.
 found a match 
  wait - and try again 
 allow 20secs for mountunmount for now - revisit 
 return vfsmount in ss_mnt 
 set nsui_vfsmount, clear busy flag and wakeup waiters 
  Support one copy source server for now.
 Construct the raw data for the vfs_kern_mount call 
 2 for ipv6 endsep and startsep. 3 for ":" and trailing '0'
 Set the server:<export> for the vfs_kern_mount call 
 Use an 'internal' mount: SB_KERNMOUNT -> MNT_INTERNAL 
  Verify COPY destination stateid.
  Connect to the source server with NFSv4.1.
  Create the source struct file for nfsd_copy_range.
  Called with COPY cstate:
     SAVED_FH: source filehandle
     CURRENT_FH: destination filehandle
 Verify the destination stateid and set dst struct file
			
			  vfsmount can be shared by multiple exports,
			  decrement refcnt. If the count drops to 1 it
			  will be unmounted when nsui_expire expires.
 CONFIG_NFSD_V4_2_INTER_SSC 
 CONFIG_NFSD_V4_2_INTER_SSC 
 See RFC 7862 p.67: 
 for a non-zero asynchronous copy do a commit of data 
	 for async copy, we ignore the error, client can always retry
	  to get the error
 Inter server SSC 
 for inter, file_src doesn't exist yet 
 Inter server SSC 
 Inter server SSC 
	 For now, only return one server address in cpn_src, the
	  address used by the client to connect to this server.
	
	  Note:  This call does change file->f_pos, but nothing in NFSD
	         should ever file->f_pos.
 This routine never returns NFS_OK!  If there are no other errors, it
  will return NFSERR_SAME or NFSERR_NOT_SAME depending on whether the
  attributes matched.  VERIFY is implemented by mapping NFSERR_SAME
  to NFS_OK after the call; NVERIFY by mapping NFSERR_NOT_SAME to NFS_OK.
	 count in words:
	    bitmap_len(1) + bitmap(2) + attr_len(1) = 4
	
	  If nfsd4_encode_fattr() ran out of space, assume that's because
	  the attributes are longer (hence different) than those given:
 skip bitmap 
	
	  Verify minlength and range as per RFC5661:
	   o  If loga_length is less than loga_minlength,
	      the metadata server MUST return NFS4ERR_INVAL.
	   o  If the sum of loga_offset and loga_minlength exceeds
	      NFS4_UINT64_MAX, and loga_minlength is not
	      NFS4_UINT64_MAX, the error NFS4ERR_INVAL MUST result.
	   o  If the sum of loga_offset and loga_length exceeds
	      NFS4_UINT64_MAX, and loga_length is not NFS4_UINT64_MAX,
	      the error NFS4ERR_INVAL MUST result.
 fixup error code as per RFC5661 
 LAYOUTCOMMIT does not require any serialization 
 CONFIG_NFSD_PNFS 
	
	  Get the entire list, then copy out only the user attributes
	  in the encode function.
  NULL call.
  Enforce NFSv4.1 COMPOUND ordering rules:
  Also note, enforced elsewhere:
 	- SEQUENCE other than as first op results in
 	  NFS4ERR_SEQUENCE_POS. (Enforced in nfsd4_sequence().)
 	- BIND_CONN_TO_SESSION must be the only op in its compound.
 	  (Enforced in nfsd4_bind_conn_to_session().)
 	- DESTROY_SESSION must be the final operation in a compound, if
 	  sessionid's in SEQUENCE and DESTROY_SESSION are the same.
 	  (Enforced in nfsd4_destroy_session().)
 These ordering requirements don't apply to NFSv4.0: 
 This is weird, but OK, not our problem: 
	
	  So first_op is something allowed outside a session, like
	  EXCHANGE_ID; but then it has to be the only op in the
	  compound:
	
	  Most ops check wronsec on our own; only the putfh-like ops
	  have special rules.
	
	  rfc 5661 2.6.3.1.1.6: don't bother erroring out a
	  put-filehandle operation if we're not going to use the
	  result:
	
	  Rest of 2.6.3.1.1: certain operations will return WRONGSEC
	  errors themselves as necessary; others should check for them
	  now:
	 traverse all operation and if it's a COPY compound, mark the
	  source filehandle to skip verification
  COMPOUND call.
 reserve space for: NFS status code 
 reserve space for: taglen, tag, and opcnt 
	
	  Don't use the deferral mechanism for NFSv4; compounds make it
	  too hard to avoid non-idempotency problems.
	
	  According to RFC3010, this takes precedence over all other errors.
		
		  The XDR decode routines may have pre-set op->status;
		  for example, if there is a miscellaneous XDR error
		  it will be set to nfserr_bad_xdr.
 If op is non-idempotent 
			
			  Don't execute this op if we couldn't encode a
			  succesful reply:
			
			  Plus if there's another operation, make sure
			  we'll have space to at least encode an error:
 Only from SEQUENCE 
 Reset deferral mechanism for RPC deferrals 
 We'll fall back on returning no lockowner if run out of space: 
 ac_supported, ac_resp_access 
  Note since this is an idempotent operation we won't insist on failing
  the op prematurely if the estimate is too large.  We may turn off splice
  reads unnecessarily.
	
	  Largest of remaining attributes are 16 bytes (e.g.,
	  supported_attributes)
 bitmask, length 
	
	  If we detect that the file changed during hole encoding, then we
	  recover by encoding the remaining reply as data. This means we need
	  to set aside enough room to encode two data segments.
 eir_clientid, eir_sequenceid \
 eir_flags, spr_how \
 spo_must_enforce & _allow with bitmap \
eir_server_owner.so_minor_id \
 eir_server_owner.so_major_id<> \
 eir_server_scope<> \
 eir_server_impl_id array length \
 ignored eir_server_impl_id contents )  sizeof(__be32);
 bctsr_sessid \
 bctsr_dir, use_conn_in_rdma_mode )  sizeof(__be32);
 sessionid \
 csr_sequence, csr_flags \
 wr_callback  +
 wr_callback  +
 wr_count  +
 wr_committed  +
 cr_consecutive  +
 cr_synchronous )  sizeof(__be32);
 osr_count  +
 osr_complete<1> optional 0 for now )  sizeof(__be32);
 cnr_lease_time  +
 We support one cnr_source_server  +
 cnr_stateid seq  +
 cnr_stateid  +
 num cnr_source_server +
 nl4_type  +
 nl4 size  +
nl4_loc + nl4_loc_sz )
 gd_layout_type +
 gd_notify_types )  sizeof(__be32);
  At this stage we don't really know what layout driver will handle the request,
  so we need to define an arbitrary upper bound here.
 logr_return_on_close  +
 nr of layouts  +
 locr_newsize  +
 ns_size )  sizeof(__be32);
 lrs_stateid  +
 CONFIG_NFSD_PNFS 
 NFSv4.1 operations 
 CONFIG_NFSD_PNFS 
 NFSv4.2 operations 
  nfsd4_spo_must_allow - Determine if the compound op contains an
  operation that is allowed to be sent with machine credentials
  @rqstp: a pointer to the struct svc_rqst
  Checks to see if the compound contains a spo_must_allow op
  and confirms that it was sent with the proper machine creds.
   Mapping of UIDGIDs to name and vice versa.
   Copyright (c) 2002, 2003 The Regents of the University of
   Michigan.  All rights reserved.
   Marius Aamodt Eriksen <marius@umich.edu>
   Redistribution and use in source and binary forms, with or without
   modification, are permitted provided that the following conditions
   are met:
   1. Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
   2. Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation andor other materials provided with the distribution.
   3. Neither the name of the University nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.
   THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
   WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
   MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
   DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
   FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
   CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
   SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
   BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
   LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
   NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
   SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  Turn off idmapping when using AUTH_SYS.
  Cache entry
  XXX we know that IDMAP_NAMESZ < PAGE_SIZE, but it's ugly to rely on
  that.
 User  Group 
 Common entry handling 
  ID -> Name cache
 Flip LSB for usergroup 
 Authentication name 
 Type 
 ID 
 expiry 
 Name 
  Name -> ID cache
 Authentication name 
 Type 
 Name 
 expiry 
 ID 
  Exported API
 too long to represent a 32-bit id: 
 Just to make sure it's null-terminated: 
		
		  otherwise, fall through and try idmapping, for
		  backwards compatibility with clients sending names:
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2014-2016 Christoph Hellwig.
 we always return a single extent 
 single signature 
	
	  Fill in the overall length and number of volumes at the beginning
	  of the layout.
 SPDX-License-Identifier: GPL-2.0
  NFS exporting and validation.
  We maintain a list of clients, each of which has a list of
  exports. To export an fs to a given client, you first have
  to create the client entry with NFSCTL_ADDCLIENT, which
  creates a client control block and adds it to the hash
  table. Then, you call NFSCTL_EXPORT for each fs.
  Copyright (C) 1995, 1996 Olaf Kirch, <okir@monad.swb.de>
  We have two caches.
  One maps client+vfsmnt+dentry to export options - the export map
  The other maps client+filehandle-fragment to export options. - the expkey map
  The export options are actually stored in the first map, and the
  second map contains a reference to the entry in the first map.
 client fsidtype \xfsid 
 client fsidtype fsid expiry [path] 
 invalid type 
 OK, we seem to have a valid key 
 now we want a pathname, or empty meaning NEGATIVE  
	
	  Take the nfsd_mutex here to ensure that the file cache is not
	  destroyed while we're in the middle of flushing.
  client path 
 is this correct? 
	
	  We currently export only dirs, regular files, and (for v4
	  pseudoroot) symlinks.
	
	  Mountd should never pass down a writeable V4ROOT export, but,
	  just to make sure:
	 There are two requirements on a filesystem to be exportable.
	  1:  We must be able to identify the filesystem from a number.
	        either a device number (so FS_REQUIRES_DEV needed)
	        or an FSID number (so NFSEXP_FSID or ->uuid is needed).
	  2:  We must be able to find an inode from a filehandle.
	        This means that s_export_op must be set.
	  3: We must not currently be on an idmapped mount.
 more than one fsloc 
 listsize 
 colon separated host list 
 slash separated path component list 
 migrated 
 more than one secinfo 
		
		  XXX: It would be nice to also check whether this
		  pseudoflavor is supported, so we can discover the
		  problem at export time instead of when a client fails
		  to authenticate.
 Only some flags are allowed to differ between flavors: 
 CONFIG_NFSD_V4 
 more than one uuid 
 expect a 16 byte uuid encoded as \xXXXX... 
 client path expiry [flags anonuid anongid fsid] 
 client 
 path 
 expiry 
 flags 
 anon uid 
 anon gid 
 fsid 
				 quietly ignore unknown words and anything
				  following. Newer user-space can try to set
				  new values, then see what the result was.
		
		  No point caching this if it would immediately expire.
		  Also, this protects exportfs's dummy export from the
		  anon_uidanon_gid checks:
		
		  For some reason exportfs has been passing down an
		  invalid (-1) uid & gid on the "dummy" export which it
		  uses to test export support.  To make sure exportfs
		  sees errors from check_export we therefore need to
		  delay these checks till after check_export:
	
	  The export_stats file uses the same ops as the exports file.
	  We use the file's name to determine the reported info per export.
	  There is no rename in nsfdfs, so d_name.name is stable.
  Find the export entry for a given dentry.
  Obtain the root fh on behalf of a client.
  This could be done in user space, but I feel that it adds some safety
  since its harder to fool a kernel module than a user space program.
 NB: we probably ought to check that it's NUL-terminated 
	
	  fh must be initialized before calling fh_compose
 legacy gss-only clients are always OK: 
 ip-address based client; check sec= export option: 
 defaults in absence of sec= options: 
	 If the compound op contains a spo_must_allowed op,
	  it will be sent with integrityprotection which
	  will have to be expressly allowed on mounts that
	  don't support it
  Uses rq_client and rq_gssclient to find an export; uses rq_client (an
  auth_unix client) if it's available and has secinfo information;
  otherwise, will try to use rq_gssclient.
  Called from functions that handle requests; functions that do work on
  behalf of mountd are passed a single client name to use, and should
  use exp_get_by_name() or exp_find().
 First try the auth_unix client: 
 If it has secinfo, assume there are no gss... clients 
 Otherwise, try falling back on gss client 
 First try the auth_unix client: 
 If it has secinfo, assume there are no gss... clients 
 Otherwise, try falling back on gss client 
  Called when we need the filehandle for the root of the pseudofs,
  for a given NFSv4 client.   The root is defined to be the
  export point with fsid==0
  Initialize the exports module.
  Flush exports table - called when last nfsd thread is killed
  Shutdown the exports module.
  Copyright (c) 2001 The Regents of the University of Michigan.
  All rights reserved.
  Kendrick Smith <kmsmith@umich.edu>
  Andy Adamson <kandros@umich.edu>
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions
  are met:
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions and the following disclaimer.
  2. Redistributions in binary form must reproduce the above copyright
     notice, this list of conditions and the following disclaimer in the
     documentation andor other materials provided with the distribution.
  3. Neither the name of the University nor the names of its
     contributors may be used to endorse or promote products derived
     from this software without specific prior written permission.
  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 all fields zero 
 forward declarations 
 Locking: 
  Currently used for the del_recall_lru and file hash table.  In an
  effort to decrease the scope of the client_mutex, this spinlock may
  eventually cover more:
  A waitqueue for all in-progress 4.0 CLOSE operations that are waiting for
  the refcount on the open stateid to drop.
  A waitqueue where a writer to clients#ctl destroying a client can
  wait for cl_rpc_users to drop to 0 and then for the client to be
  unhashed.
 must be called under the client_lock 
 Dequeue all blocked locks 
 Now free them 
	
	  Since this is just an optimization, we don't try very hard if it
	  turns out not to succeed. We'll requeue it on NFS4ERR_DELAY, and
	  just quit trying on anything else.
  We store the NONE, READ, WRITE, and BOTH bits separately in the
  st_{access,deny}_bmap field of the stateid, in order to track not
  only what share bits are currently in force, but also what
  combinations of share bits previous opens have used.  This allows us
  to enforce the recommendation of rfc 3530 14.2.19 that the server
  return an error if the client attempt to downgrade to a combination
  of share bits not explicable by closing some of its previous opens.
  XXX: This enforcement is actually incomplete, since we don't keep
  track of accessdeny bit combinations; so, e.g., we allow:
 	OPEN allow read, deny write
 	OPEN allow both, deny none
 	DOWNGRADE allow read, deny none
  which we should reject.
 set share access for a given stateid 
 clear share access for a given stateid 
 test whether a given stateid has access 
 set share deny for a given stateid 
 clear share deny for a given stateid 
 test whether a given stateid is denying specific access 
  Open owner state (share locks)
 hash tables for lock and open owners 
 hash table for nfs4_file 
 XXX: why not (here & in file cache) use inode? 
 Does this access mode make sense? 
 Does it conflict with a deny mode already set? 
 Common case is that there is no deny mode. 
 Does this deny mode make sense? 
  Allocate a new opendelegation state counter. This is needed for
  pNFS for proper return on close semantics.
  Note that we only allocate it for pNFS-enabled exports, otherwise
  all pointers to struct nfs4_clnt_odstate are always NULL.
 Reserving 0 for start of file in nfsdfs "states" file: 
 Will be incremented before return to client: 
	
	  It shouldn't be a problem to reuse an opaque stateid value.
	  I don't think it is for 4.1.  But with 4.0 I worry that, for
	  example, a stray write retransmission could be accepted by
	  the server when it should have been rejected.  Therefore,
	  adopt a trick from the sctp code to attempt to maximize the
	  amount of time until an id is reused, by ensuring they always
	  "increase" (mod INT_MAX):
  Create a unique stateid_t to represent each COPY.
  When we recall a delegation, we should be careful not to hand it
  out again straight away.
  To ensure this we keep a pair of bloom filters ('new' and 'old')
  in which the filehandles of recalled delegations are "stored".
  If a filehandle appear in either filter, a delegation is blocked.
  When a delegation is recalled, the filehandle is stored in the "new"
  filter.
  Every 30 seconds we swap the filters and clear the "new" one,
  unless both are empty of course.
  Each filter is 256 bits.  We hash the filehandle to 32bit and use the
  low 3 bytes as hash-table indices.
  'blocked_delegations_lock', which is always taken in block_delegations(),
  is used to manage concurrent access.  Testing does not need the lock
  except when swapping the two filters.
 index into 'set' 
	
	  delegation seqid's are never incremented.  The 4.1 special
	  meaning of seqid 0 isn't meaningful, really, but let's avoid
	  0 anyway just for consistency and use 1:
  nfs4_delegation_exists - Discover if this delegation already exists
  @clp:     a pointer to the nfs4_client we're granting a delegation to
  @fp:      a pointer to the nfs4_file we're granting a delegation on
  Return:
       On success: true iff an existing delegation is found
  hash_delegation_locked - Add a delegation to the appropriate lists
  @dp:     a pointer to the nfs4_delegation we are adding.
  @fp:     a pointer to the nfs4_file we're granting a delegation on
  Return:
       On success: NULL if the delegation was successfully hashed.
       On error: -EAGAIN if one was previously granted to this
                  nfs4_client for this nfs4_file. Delegation is not hashed.
 Ensure that deleg break won't try to requeue it 
  SETCLIENTID state 
  A stateid that had a deny mode associated with it is being released
  or downgraded. Recalculate the deny mode on the file.
 Recalculate per-file deny mode if there was a change 
 release all access and file references for a given stateid 
  Put the persistent reference to an already unhashed generic stateid, while
  holding the cl_lock. If it's the last reference, then put it onto the
  reaplist for later destruction.
  Free a list of generic stateids that were collected earlier after being
  fully unhashed.
  Bump the seqid on cstate->replay_owner, and clear replay_owner if it
  won't be used for replay.
  The protocol defines ca_maxresponssize_cached to include the size of
  the rpc header, but all we need to cache is the data starting after
  the end of the initial SEQUENCE operation--the rest we regenerate
  each time.  Therefore we can advertise a ca_maxresponssize_cached
  value that is the number of bytes in our cache plus a few additional
  bytes.  In order to stay on the safe side, and not promise more than
  we can cache, those additional bytes must be the minimum possible: 24
  bytes of rpc header (xid through accept state, with AUTH_NULL
  verifier), 12 for the compound header (with zero-length tag), and 44
  for the SEQUENCE op response:
  We don't actually need to cache the rpc and session headers, so we
  can allocate a little less for each slot:
  XXX: If we run out of reserved DRC memory we could (up to a point)
  re-negotiate active sessions and reduce their slot usage to make
  room for new connections. For now we just fail the create session.
		 We have handed out more space than we chose in
		  set_max_drc() to allow.  That isn't really a
		  problem as long as that doesn't make us think we
		  have lots more due to integer overflow.
	
	  Never use more than a fraction of the remaining memory,
	  unless it's the only way to give this client a slot.
	  The chosen fraction is either 18 or 1number of threads,
	  whichever is smaller.  This ensures there are adequate
	  slots to support multiple clients per thread.
	  Give the client one slot even if that would require
	  over-allocation--it is better than failure.
 allocate each struct nfsd4_slot and data cache in one piece 
 oops; xprt is already down: 
 We may have gained or lost a callback channel: 
 must be called under client_lock 
		
		  This is a little silly; with sessions there's no real
		  use for the callback address.  Use the peer address
		  as a reasonable default for now, but consider fixing
		  the rpc client not to require an address in the
		  future:
 caller must hold client_lock 
 Search in the appropriate list 
 caller must hold client_lock 
 SETCLIENTID and SETCLIENTID_CONFIRM Helper functions 
	
	  We're assuming the clid was not given out from a boot
	  precisely 2^32 (about 136 years) before this one.  That seems
	  a safe assumption:
  XXX Should we use a slab cache ?
  This type of memory management is somewhat inefficient, but we use it
  anyway since SETCLIENTID is not a common operation.
 must be called under the client_lock 
 Mark the client as expired! 
 Make it invisible 
 Should be no openowners at this point 
  RFC 3530 language requires clid_inuse be returned when the
  "principal" associated with a requests differs from that previously
  used.  We use uid, gid's, and gss principal string as our best
  approximation.  We also don't want to allow non-gss use of a client
  established using gss: in theory cr_principal should catch that
  change, but in practice cr_principal can be null even in the gss case
  since gssd doesn't always pass down a principal string.
 Is cr_flavor one of the gss "pseudoflavors"?: 
 XXX: check that cr_targ_princ fields match ? 
	
	  This is opaque to client, so no need to byte-swap. Use
	  __force to keep sparse happy
 XXX: or SEQ_SKIP? 
	
	  Note: a lock stateid isn't really the same thing as a lock,
	  it's the locking state held by one owner on a file, and there
	  may be multiple (or no) lock ranges associated with it.
	  (Same for the matter is true of open stateids.)
 XXX: open stateid? 
 Kinda dead code as long as we only support read delegs: 
 XXX: lease time, whether it's being recalled. 
 XXX: What else would be useful? 
 XXX: or SEQ_SKIP? 
 XXX: copy stateids? 
 XXX: alternatively, we could getdrop in seq startstop 
  Normally we refuse to destroy clients that are in use, but here the
  administrator is telling us to just do it.  We also want to wait
  so the caller has a guarantee that the client's locks are gone by
  the time the write returns:
 Currently, we only support tcp and tcp6 for the callback channel 
  Cache a reply. nfsd4_check_resp_size() has bounded the cache size.
  Encode the replay sequence operation from the slot values.
  If cachethis is FALSE encode the uncached rep error on the next
  operation which sets resp->p and increments resp->opcnt for
  nfs4svc_encode_compoundres.
 Encode the replayed sequence operation 
		
		  The original operation wasn't a solo sequence--we
		  always cache those--so this retry must not match the
		  original:
  The sequence operation is not cached because we can use the slot and
  session values.
  Set the exchange_id flags returned by the server.
 Referrals are supported, Migration is not. 
 set the wire flags to return to client. 
		
		  Sometimes userspace doesn't give us a principal.
		  Which is a bug, really.  Anyway, we can't enforce
		  MACH_CRED in that case, better to give up now:
 checked by xdr code 
 Cases below refer to rfc 5661 section 18.35.4: 
 buggy client 
 case 9 
 case 8 
 case 6 
 case 3 
 case 2 
 case 5, client reboot 
 case 7 
 case 4, possible retry or client restart 
 case 1, new owner ID 
 The slot is in use, and no response has been sent. 
 Note unsigned 32-bit arithmetic handles wraparound: 
  Cache the create session result into the create session single DRC
  slot cache by saving the xdr structure. sl_seqid has been set.
  Do this for solo or embedded create session operations.
 credential,verifier: AUTH_NULL, length 0  \
 MIN tag is length with zero, only length  \
 version, opcount, opcode  \
 seqid, slotID, slotID, cache  \
 verifier: AUTH_NULL, length 0 \
 status  \
 MIN tag is length with zero, only length  \
 opcount, opcode, opstatus \
 seqid, slotID, slotID, slotID, status  \
	
	  Note decreasing slot size below client's request may make it
	  difficult for client to function correctly, whereas
	  decreasing the number of slots will (just?) affect
	  performance.  When short on memory we therefore prefer to
	  decrease number of slots instead of their size.  Clients that
	  request larger slots than they need will get poor results:
	  Note that we always allow at least one slot, because our
	  accounting is soft and provides no guarantees either way.
  Server's NFSv4.1 backchannel support is AUTH_SYS-only for now.
  These are based on similar macros in linuxsunrpcmsg_prot.h .
		
		  GSS case: the spec doesn't allow us to return this
		  error.  But it also doesn't allow us not to support
		  GSS.
		  I'd rather this fail hard than return some error the
		  client might think it can already handle:
 an unconfirmed replay returns misordered 
 Persistent sessions are not supported 
 Upshifting from TCP to RDMA is not supported 
 cache solo and embedded create sessions under the client_lock 
 init connection and backchannel 
 Following the last paragraph of RFC 5661 Section 18.34.3: 
 oops; xprt is already down: 
	
	  If there's an error then the reply can have fewer ops than
	  the call.
	
	  But if we cached a reply with more ops than the call you're
	  sending us now, then this new call is clearly not really a
	  replay of the old one:
 This is the only check explicitly called by spec: 
	
	  There may be more comparisons we could actually do, but the
	  spec doesn't require us to catch every case where the calls
	  don't match (that would require caching the call as well as
	  the reply), so we don't bother.
	
	  Will be either used or freed by nfsd4_sequence_check_conn
	  below.
	 We do not negotiate the number of slots yet, so set the
	  maxslots to the session maxreqs which is used to encode
		 Return the cached reply status and set cstate->status
 Success! bump slot seqid 
 Drop session reference that was taken in nfsd4_sequence() 
		
		  We don't take advantage of the rca_one_fs case.
		  That's OK, it's optional, we can safely ignore it.
		
		  The following error isn't really legal.
		  But we only get here if the client just explicitly
		  destroyed the client.  Surely it no longer cares what
		  error it gets back on an operation for the dead
		  client.
	
	  We try hard to give out unique clientid's, so if we get an
	  attempt to confirm the same clientid with a different cred,
	  the client may be buggy; this should never happen.
	 
	  Nevertheless, RFC 7530 recommends INUSE for this case:
 OPEN Share state helper functions 
 ignore lock owners 
 Lock the stateid st_mutex, and deal with races with CLOSE 
 We are moving these outside of the spinlocks to avoid the warnings 
 Handle races with CLOSE 
 To keep mutex tracking happy 
  In the 4.0 case we need to keep the owners around a little while to handle
  CLOSE replay. We still do need to release any file access that is held by
  them before returning however.
	
	  We know that we hold one reference via nfsd4_close, and another
	  "persistent" reference for the client. If the refcount is higher
	  than 2, then there are still calls in progress that are using this
	  stateid. We can't put the sc_file reference until they are finished.
	  Wait for the refcount to drop to 2. Since it has been unhashed,
	  there should be no danger of the refcount going back up again at
	  this point.
 search file_hashtbl[] for file 
  Called to check deny when READ with all zero stateid or
  WRITE with all zero or all one stateid
 Check for conflicting share reservations 
	
	  We can't do this in nfsd_break_deleg_cb because it is
	  already holding inode->i_lock.
	 
	  If the dl_time != 0, then we know that it has already been
	  queued for a lease break. Don't queue it again.
		
		  Race: client probably got cb_recall before open reply
		  granting delegation.
	
	  We're assuming the state code never drops its reference
	  without first removing the lease.  Since we're in this lease
	  callback (and since the lease code is serialized by the
	  i_lock) we know the server hasn't removed the lease yet, and
	  we know it's safe to take a reference.
 Called from break_lease() with i_lock held. 
	
	  We don't want the locks code to timeout the lease for us;
	  we'll remove it ourself if a delegation isn't returned
	  in time:
 Note rq_prog == NFS_ACL_PROGRAM is also possible: 
	
	  We're in the 4.0 case (otherwise the SEQUENCE op would have
	  set cstate->clp), so session = false:
	
	  In case we need it later, after we've already created the
	  file and don't want to risk a further failure:
 Replace unconfirmed owners without checking for replay. 
	
	  Are we trying to set a deny mode that would conflict with
	  current access?
 set access to the file 
 Set access bits in stateid 
 Set new deny mask 
 test and set deny mode 
 Should we give out recallable state?: 
	
	  In the sessions case, since we don't have to establish a
	  separate connection for callbacks, we assume it's OK
	  until we hear otherwise:
	
	  There could be multiple filehandles (hence multiple
	  nfs4_files) referencing this file, but that's not too
	  common; let's just give up in that case rather than
	  trying to go look up all the clients using that other
	  nfs4_file as well:
	
	  If there's a close in progress, make sure that we see it
	  clear any fi_fds[] entries before we see it decrement
	  i_writecount:
 There may be non-NFSv4 writers 
	
	  It's possible there are non-NFSv4 write opens in progress,
	  but if they haven't incremented i_writecount yet then they
	  also haven't called break lease yet; so, they'll break this
	  lease soon enough.  So, all that's left to check for is NFSv4
	  opens:
 it's an open  &&
	
	  There's a small chance that we could be racing with another
	  NFSv4 open.  However, any open that hasn't added itself to
	  the fi_stateids list also hasn't called break_lease yet; so,
	  they'll break this lease soon enough.
	
	  The fi_had_conflict and nfs_get_existing_delegation checks
	  here are just optimizations; we'll need to recheck them at
	  the end:
		
		  We probably could attempt another open and get a read
		  delegation, but for now, don't bother until the
		  client actually sends us one.
		 increment early to prevent fi_deleg_file from being
  Attempt to hand out a delegation.
  Note we don't support write delegations, and won't until the vfs has
  proper support for them.
			
			  Let's not give out any delegations till everyone's
			  had the chance to reclaim theirs, and until
			  NLM locks have all been reclaimed:
 4.1 client asking for a delegation? 
	 Otherwise the client must be confused wanting a delegation
	  it already has, therefore we don't return
	  NFS4_OPEN_DELEGATE_NONE_EXT and reason.
	
	  Lookup file; if found, lookup stateid and check open request,
	  and check for delegations in the process of being recalled.
	  If not found, create the nfs4_file struct
	
	  OPEN the file, or upgrade an existing OPEN.
	  If truncate fails, the OPEN fails.
	 
	  stp is already locked.
 Stateid was found, this is an OPEN upgrade 
	
	 Attempt to hand out a delegation. No error return, because the
	 OPEN succeeds even if we fail.
 4.1 client trying to upgradedowngrade delegation? 
	
	 To finish the open response, we just need to set the rflags.
 do nothing if grace period already ended 
	
	  If the server goes down again right now, an NFSv4
	  client will still be allowed to reclaim after it comes back up,
	  even if it hasn't yet had a chance to reclaim state this time.
	 
	
	  At this point, NFSv4 clients can still reclaim.  But if the
	  server crashes, any that have not yet reclaimed will be out
	  of luck on the next boot.
	 
	  (NFSv4.1+ clients are considered to have reclaimed once they
	  call RECLAIM_COMPLETE.  NFSv4.0 clients are considered to
	  have reclaimed after their first OPEN.)
	
	  At this point, and once lockd andor any other containers
	  exit their grace period, further reclaims will fail and
	  regular locking can resume.
  If we've waited a lease period but there are still clients trying to
  reclaim, wait a little longer to give them a chance to finish.
	
	  If we've given them two lease times to reclaim, and they're
	  still not done, give up:
  This is called when nfsd is being shutdown, after all inter_ssc
  cleanup were done, to destroy the ssc delayed unmount list.
 mark being unmount 
 waiters need to start from begin of list 
 wakeup ssc_connect waiters 
	
	  It's possible for a client to try and acquire an already held lock
	  that is being held for a long time, and then lose interest in it.
	  So, we clean out any un-revisited request after a lease period
	  under the assumption that the client is no longer interested.
	 
	  RFC5661, sec. 9.6 states that the client must not rely on getting
	  notifications and must continue to poll for locks, even when the
	  server supports them. Thus this shouldn't lead to clients blocking
	  indefinitely once the lock does become free.
 service the server-to-server copy delayed unmount list 
 For lock stateid's, we test the parent open, not the lock: 
		 Answer in remaining cases depends on existence of
 (flags & RD_STATE) && ZERO_STATEID(stateid) 
	
	  When sessions are used the stateid generation number is ignored
	  when it is zero.
 If the client sends us a stateid from the future, it's buggy: 
	
	  However, we could see a stateid from the past, even from a
	  non-buggy client.  For example, if the client sends a lock
	  while some IO is outstanding, the lock may bump si_generation
	  while the IO is still in flight.  The client could avoid that
	  situation by waiting for responses on all the IO requests,
	  but better performance may result in retrying IO that
	  receives an old_stateid error if requests are rarely
	  reordered in flight:
	
	   only return revoked delegations if explicitly asked.
	   otherwise we report revoked or bad_stateid status.
  A READ from an inter server to server COPY will have a
  copy stateid. Look up the copy notify stateid from the
  idr structure and take a reference on it.
  Checks for stateid operations
  Test if the stateid is valid
 Default falls through and returns nfserr_bad_stateid 
  Checks for sequence id mutating operations. 
 We don't yet support WANT bits: 
  nfs4_unlock_state() called after encode
	
	  Technically we don't _really_ have to increment or copy it, since
	  it should just be gone after this operation and we clobber the
	  copied value below, but we continue to do so here just to ensure
	  that racing ops see that there was a state change.
	 v4.1+ suggests that we send a special stateid in here, since the
	  clients should just ignore this anyway. Since this is not useful
	  for v4.0 clients either, we set it to the special close_stateid
	  universally.
	 
	  See RFC5661 section 18.2.4, and RFC7530 section 16.2.5
 put reference from nfs4_preprocess_seqid_op 
 last octet in a range 
  TODO: Linux file offsets are _signed_ 64-bit quantities, which means that
  we can't properly handle lock requests that go beyond the (2^63 - 1)-th
  byte, because of sign extension problems.  Since NFSv4 calls for 64-bit
  locking, this prevents us from being completely protocol-compliant.  The
  real solution to this problem is to start using unsigned file offsets in
  the VFS, but this is a very deep change!
 An empty list means that something else is going to be using it 
 We just don't care that much 
  Alloc a lock owner structure.
  Called in nfsd4_lock - therefore, OPEN and OPEN_CONFIRM (if needed) has 
  occurred. 
  strhashval = ownerstr_hashval
 If ost is not hashed, ost->st_locks will not be valid 
 To keep mutex tracking happy 
 with an existing lockowner, seqids must be the same 
   LOCK operation 
 See rfc 5661 18.10.3: given clientid is ignored: 
 validate and update open stateid and open seqid 
 success! 
 conflock holds conflicting lock 
 dequeue it if we queued it before 
 Bump seqid manually if the 4.0 replay owner is openowner 
		
		  If this is a new, never-before-used stateid, and we are
		  returning an error, then just go ahead and release it.
  The NFSv4 spec allows a client to do a LOCKT without holding an OPEN,
  so we do a temporary open here just to get an open file to pass to
  vfs_test_lock.
 to block new leases till after test_lock: 
  LOCKT operation
  returns
  	true:  locks held by lockowner
  	false: no locks held by lockowner
 Any valid lock stateid should have some sort of access 
 Find the matching lock stateowner 
 see if there are still any locks associated with it 
  failure => all reset bets are off, nfserr_no_grace...
  The caller is responsible for freeing name.data if NULL is returned (it
  will be freed in nfs4_remove_reclaim_record in the normal case).
  Since the lifetime of a delegation isn't limited to that of an open, a
  client may quite reasonably hang on to a delegation as long as it has
  the inode cached.  This becomes an obvious problem the first time a
  client's inode cache approaches the size of the server's total memory.
  For now we avoid this problem by imposing a hard limit on the number
  of delegations, which varies according to the server's memory size.
	
	  Allow at most 4 delegations per megabyte of RAM.  Quick
	  estimates suggest that in the worst case (where every delegation
	  is for a different inode), a delegation could take about 1.5K,
	  giving a worst case usage of about 6% of memory.
 initialization to perform when the nfsd service is started: 
  functions to set current state id
  functions to consume current state id
   Server-side XDR for NFSv4
   Copyright (c) 2002 The Regents of the University of Michigan.
   All rights reserved.
   Kendrick Smith <kmsmith@umich.edu>
   Andy Adamson   <andros@umich.edu>
   Redistribution and use in source and binary forms, with or without
   modification, are permitted provided that the following conditions
   are met:
   1. Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
   2. Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation andor other materials provided with the distribution.
   3. Neither the name of the University nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.
   THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
   WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
   MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
   DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
   FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
   CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
   SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
   BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
   LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
   NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
   SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  As per referral draft, the fsid for a referral MUST be different from the fsid of the containing
  directory in order to indicate to the client that a filesystem boundary is present
  We use a fixed fsid for a referral
  svcxdr_tmpalloc - allocate memory to be freed after compound processing
  @argp: NFSv4 compound argument structure
  @len: length of buffer to allocate
  Allocates a buffer of size @len to be freed when processing the compound
  operation described in @argp finishes.
  For xdr strings that need to be passed to other kernel api's
  as null-terminated strings.
  Note null-terminating in place usually isn't safe since the
  buffer might end on a page boundary.
	
	  The location of the decoded data item is stable,
	  so @p is OK to use. This is the common case.
  NFSv4 basic data type decoders
  This helper handles variable-length opaques which belong to protocol
  elements that this implementation does not support.
  nfsd4_decode_bitmap4 - Decode an NFSv4 bitmap4
  @argp: NFSv4 compound argument structure
  @bmval: pointer to an array of u32's to decode into
  @bmlen: size of the @bmval array
  The server needs to return nfs_ok rather than nfserr_bad_xdr when
  encountering bitmaps containing bits it does not recognize. This
  includes bits in bitmap words past WORDn, where WORDn is the last
  bitmap WORD the implementation currently supports. Thus we are
  careful here to simply ignore bits in bitmap words that this
  implementation has yet to support explicitly.
  Return values:
    %nfs_ok: @bmval populated successfully
    %nfserr_bad_xdr: the encoded bitmap was invalid
 request sanity 
 A counted array of nfsace4's 
		
		  Even with 4-byte names there wouldn't be
		  space for that many aces; something fishy is
		  going on:
 request sanity: did attrlist4 contain the expected number of words? 
 CONFIG_NFSD_PNFS 
 Defined in Appendix A of RFC 5531 
 machine name 
 gcbp_handle_from_server 
 gcbp_handle_from_client 
 a counted array of callback_sec_parms4 items 
 callback_sec_params4 
 Is this legal? Be generous, take it to mean AUTH_NONE: 
 void 
  NFSv4 operation argument decoders
 open_downgrade 
 Note: unlike access bits, deny bits may be zero. 
 void 
 deleg_want is ignored 
 deleg_want is ignored 
 Also used for NVERIFY 
	 For convenience's sake, we compare raw xdr'd attributes in
  This implementation currently does not support SP4_SSV.
  This decoder simply skips over these arguments.
 ssp_ops 
 ssp_hash_algs<> 
 ssp_encr_algs<> 
		 Note that RFC 8881 places no length limit on
		  nii_domain, but this implementation permits no
		 Note that RFC 8881 places no length limit on
		  nii_name, but this implementation permits no
 headerpadsz is ignored 
 CONFIG_NFSD_PNFS 
 XXX: not jukebox? 
 currently support for 1 inter-server source server 
 ca_consecutive: we always do consecutive copies 
 intra-server copy 
 decode all the supplied server addresses but use only the first 
 XXX: jukebox? 
  XDR data that is more than PAGE_SIZE in size is normally part of a
  read or write. However, the size of extended attributes is limited
  by the maximum request size, and then further limited by the underlying
  filesystem limits. This can exceed PAGE_SIZE (currently, XATTR_SIZE_MAX
  is 64k). Since there is no kvec- or page-based interface to xattrs,
  and we're not dealing with contiguous pages, we need to do some copying.
  Decode data into buffer.
		
		  We're in luck, the head has enough space. Just return
		  the head, no need for copying.
  Get a user extended attribute name from the XDR buffer.
  It will not have the "user." prefix, so prepend it.
  Lastly, check for nul characters in the name.
	
	  Copy the extended attribute name over while checking for 0
	  characters.
  A GETXATTR op request comes without a length specifier. We just set the
  maximum length for the reply based on XATTR_SIZE_MAX and the maximum
  channel reply size. nfsd_getxattr will probe the length of the xattr,
  check it against getxa_len, and allocate + return the value.
	
	  If the cookie  is too large to have even one user.x attribute
	  plus trailing '\0' left in a maximum size buffer, it's invalid.
 Always need at least 2 words (length and one character) 
 new operations for NFSv4.1 
 new operations for NFSv4.2 
 RFC 8276 extended atributes operations 
 opcnt, status 
	
	  NFS4ERR_RESOURCE is a more helpful error than GARBAGE_ARGS
	  here, so we return success at the xdr level so that
	  nfsd4_proc can handle this is an NFS-level error.
		
		  We'll try to cache the result in the DRC if any one
		  op in the compound wants to be cached:
		
		  OP_LOCK and OP_LOCKT may return a conflicting lock.
		  (Special case because it will just skip encoding this
		  if it runs out of xdr buffer space, and it is the only
		  operation that behaves this way.)
 Sessions make the DRC unnecessary: 
  ctime (in NFSv4, time_metadata) is not writeable, and the client
  doesn't really care what resolution could theoretically be stored by
  the filesystem.
  The client cares how close together changes can be while still
  guaranteeing ctime changes.  For most filesystems (which have
  timestamps with nanosecond fields) that is limited by the resolution
  of the time returned from current_time() (which I'm assuming to be
  1HZ).
 Encode as an array of strings the string given with components
  separated @sep, escaped with esc_enter and esc_exit.
 We will fill this in with @count later 
 try to parse as esc_start, ..., esc_end, sep 
 find esc_exit or end of string ;
 find sep or end of string ;
 Encode as an array of strings the string given with components
  separated @sep.
  encode a location element of a fs_locations structure
  Encode a path in RFC3530 'pathname4' format
	 First walk the path up to the nfsd root, and store the
	  dentriespath components in an array.
   encode a fs_locations structure
	
	  For now we use a 0 here to indicate the null translation; in
	  the future we may place a call to translation code here.
 lfs 
 pi 
 As per referral draft:  
  Note: @fhp can be NULL; in this case, we might have to compose the filehandle
  ourselves.
 CONFIG_NFSD_V4_SECURITY_LABEL 
 to be backfilled later 
		
		  Get parent's attributes if not ignoring crossmount
		  and this is the root of a cross-mounted filesystem.
 CONFIG_NFSD_PNFS 
 CONFIG_NFSD_V4_SECURITY_LABEL 
	
	  In the case of a mountpoint, the client may be asking for
	  attributes that are only properties of the underlying filesystem
	  as opposed to the cross-mounted file system. In such a case,
	  we will not follow the cross mount and will fill the attribtutes
	  directly from the mountpoint dentry.
		
		  Why the heck aren't we just using nfsd_lookup??
		  Different "."".." handling?  Something else?
		  At least, add a comment here to explain....
 bmval0 
 bmval1 
 attribute length 
 no htonl 
 In nfsv4, "." and ".." never make it onto the wire.. 
 mark entry present 
 offset of next entry 
 name length & name 
		
		  If the client requested the RDATTR_ERROR attribute,
		  we stuff the error code into this attribute
		  and continue.  If this attribute was not requested,
		  then in accordance with the spec, we fail the
		  entire READDIR operation(!)
	
	  RFC 3530 14.2.24 describes rd_dircount as only a "hint", and
	  notes that it could be zero. If it is zero, then the server
	  should enforce only the rd_maxcount value.
 Upshifting from TCP to RDMA is not supported 
 Including all fields other than the name, a LOCK4denied structure requires
   8(clientid) + 4(namelen) + 8(offset) + 8(length) + 4(type) = 32 bytes.
		
		  Don't fail to return the result just because we can't
		  return the conflicting open:
 non - nfsv4 lock in conflict, no clientid nor owner 
 clientid 
 length of owner name 
		
		  TODO: ACE's in delegations
 XXX: is NULL principal ok? 
		
		  TODO: space_limit's in delegations
		
		  TODO: ACE's in delegations
 XXX: is NULL principal ok? 
 4.1 
 deleg signaling not supported yet: 
 XXX save filehandle here 
 Make sure there will be room for padding if needed 
 Use rest of head for padding and remaining ops: 
	
	  nfsd_splice_actor may have already messed with the
	  page length; reset it so as not to confuse
	  xdr_truncate_encode in our caller.
 eof flag and byte count 
	
	  XXX: By default, vfs_readlink() will truncate symlinks if they
	  would overflow the buffer.  Is this kosher in NFSv4?  If not, one
	  easy fix is: if vfs_readlink() precisely fills the buffer, assume
	  that truncation occurred, and return NFS4ERR_RESOURCE.
 XXX: Following NFSv3, we ignore the READDIR verifier for now. 
	
	  Number of bytes left for directory entries allowing for the
	  final 8 bytes of the readdir and a following failed op:
	
	  Note the rfc defines rd_maxcount as the size of the
	  READDIR4resok structure, which includes the verifier above
	  and the 8 bytes encoded at the end of this function:
 RFC 3530 14.2.24 allows us to ignore dircount when it's 0: 
 nothing encoded; which limit did we hit?: 
 It was the fault of rd_maxcount: 
 We ran out of buffer space: 
 no more entries 
 Handling of some defaults in absence of real secinfo: 
 to be backfilled later 
  The SETATTR encode routine is special -- it always encodes a bitmap,
  regardless of the error status.
 eir_clientid  +
 eir_sequenceid  +
 eir_flags  +
 spr_how );
 spo_must_enforce bitmap: 
 spo_must_allow bitmap: 
 so_minor_id  +
 so_major_id.len  +
 eir_server_scope.len  +
 eir_server_impl_id.count (0) );
 The server_owner struct 
 Minor id 
 major id 
 Server scope 
 Implementation id 
 zero length nfs_impl_id4 array 
 headerpadsz 
 headerpadsz 
 Note slotid's are numbered from zero: 
 sr_highest_slotid 
 sr_target_highest_slotid 
 DRC cache data pointer 
 If maxcount is 0 then just update notifications 
			
			  We don't bother to burden the layout drivers with
			  enforcing gd_maxcount, just tell the client to
			  come back with a bigger buffer if it's not enough.
 bitmap length 
 notifications ;
 we always set return-on-close 
 we always return a single layout 
 CONFIG_NFSD_PNFS 
		 netid_len, netid, uaddr_len, uaddr (port included
		  in RPCBIND_MAXUADDRLEN)
 netid len  +
 uaddr len  +
 cr_consecutive 
 Content type, offset, byte count 
 Content type, offset, byte count 
 eof flag, segment count 
 8 sec, 4 nsec 
 cnr_lease_time 
 cnr_stateid 
 cnr_src.nl_nsvr 
  Encode kmalloc-ed buffer in to XDR stream.
			
			  We're done, with a length that wasn't page
			  aligned, so possibly not word aligned. Pad
			  any trailing bytes with 0.
  See if there are cookie values that can be rejected outright.
	
	  If the cookie is larger than the maximum number we can fit
	  in either the buffer we just got back from vfs_listxattr, or,
	  XDR-encoded, in the return buffer, it's invalid.
	
	  Reserve space for the cookie and the name array count. Record
	  the offsets to save them later.
		
		  Check if this is a "user." attribute, skip it if not.
				
				  Can't even fit the first attribute name.
	
	  If there were user attributes to copy, but we didn't copy
	  any, the offset was too large (e.g. the cookie was invalid).
  Note: nfsd4_enc_ops vector is shared for v4.0 and v4.1
  since we don't need to filter out obsolete ops as this is
  done in the decoding phase.
 NFSv4.1 operations 
 NFSv4.2 operations 
 RFC 8276 extended atributes operations 
  Calculate whether we still have space to encode repsize bytes.
  There are two considerations:
      - For NFS versions >=4.1, the size of the reply must stay within
        session limits
      - For all NFS versions, we must stay within limited preallocated
        buffer space.
  This is called before the operation is processed, so can only provide
  an upper estimate.  For some nonidempotent operations (such as
  getattr), it's not necessarily a problem if that estimate is wrong,
  as we can fail it after processing without significant side effects.
 nfsd4_check_resp_size guarantees enough room for error status 
		
		  The operation may have already been encoded or
		  partially encoded.  No op returns anything additional
		  in the case of one of these three errors, so we can
		  just truncate back to after the status.  But it's a
		  bug if we had to do this on a non-idempotent op:
 Note that op->status is already in network byte order: 
  Encode the reply stored in the stateowner reply cache 
  XDR note: do not encode rp->rp_buflen: the buffer contains the
  previously sent already encoded operation.
 already xdr'ed 
 svcxdr_tmp_alloc 
	
	  Send buffer space for the following items is reserved
	  at the top of nfsd4_proc_compound().
 SPDX-License-Identifier: GPL-2.0
 SPDX-License-Identifier: GPL-2.0
  Process version 2 NFSACL requests.
  Copyright (C) 2002-2003 Andreas Gruenbacher <agruen@suse.de>
 FIXME: nfsacl.h is a broken header 
  NULL call.
  Get the Access andor Default ACL of a file.
 Solaris returns the inode's minimum ACL. 
		 Check how Solaris handles requests for the Default ACL
 resp->acl_{access,default} are released in nfssvc_release_getacl. 
  Set the Access andor Default ACL of a file.
	 argp->acl_{access,default} may have been allocated in
  Check file attributes
  Check file access
  XDR decode functions
  XDR encode functions
 GETACL 
 ACCESS 
  XDR release functions
 status
 attributes 
 post attributes - conditional 
 Access Control List 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2017-2018 HUAWEI, Inc.
              https:www.huawei.com
 based on the end of qn is accurate and it must have the trailing '\0' 
	
	  on-disk error, let's only BUG_ON in the debugging mode.
	  otherwise, it will return 1 to just skip the invalid name
	  and go on (in consideration of the lookup performance).
 qd could not have trailing '\0' 
 However it is absolutely safe if < qd->end 
 See comments in __d_alloc on the terminating NUL character 
 since the 1st dirent has been evaluated previously 
 string comparison without already matched prefix 
 string comparison without already matched prefix 
 free if the candidate is valid 
 the target page has been mapped 
 NOTE: i_mutex is already held by vfs 
 dentry must be unhashed in lookup, no need to worry about 
 file name exceeds fs limit 
 false uninitialized warnings on gcc 4.8.x 
 negative dentry 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2017-2018 HUAWEI, Inc.
              https:www.huawei.com
  Copyright (C) 2021, Alibaba Cloud
 should already be PageUptodate 
 there is no hole in flatmode 
 2 - inode inline B: inode, [xattrs], inline last blk... 
 inline data should be located in one meta block 
 leave out-of-bound access unmapped 
 chunk index 
 block map 
 handle block map 
 parse chunk indexes 
 primary device by default 
  since we dont have write or truncate flows, so no inode
  locking needs to be held at the moment.
 no need taking (shared) inode lock since it's a ro filesystem 
 for uncompressed (aligned) files and raw access for other files 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2017-2018 HUAWEI, Inc.
              https:www.huawei.com
 to allow for x86 boot sectors and other oddities. 
 zero out everything except vfs_inode 
 be careful of RCU symlink path 
 check if current kernel meets all mandatory requirements 
 read variable-sized metadata, offset will be aligned by 4-byte 
 9(512 bytes) + LOG_SECTORS_PER_BLOCK == LOG_BLOCK_SIZE 
 -E2BIG 
 parse on-disk compression configurations 
 handle multiple devices 
 set up default EROFS parameters 
 0 - busy 
 Check for potential overflow in debug mode 
 get the root inode 
 sb->s_umount is already locked, SB_ACTIVE and SB_BORN are not set 
  could be triggered after deactivate_locked_super()
  is called, thus including umount and failed to initialize.
 called when ->s_root is non-NULL 
 Ensure all RCU free inodes  pclusters are safe to be destroyed. 
 get filesystem statistics 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2017-2018 HUAWEI, Inc.
              https:www.huawei.com
 the only user of kunmap() is 'init_inode_xattrs' 
 the most case is that xattrs of this inode are initialized. 
		
		  paired with smp_mb() at the end of the function to ensure
		  fields will only be observed after the bit is set.
 someone has initialized xattrs for us? 
	
	  bypass all xattr operations if ->xattr_isize is not greater than
	  sizeof(struct erofs_xattr_ibody_header), in detail:
	  1) it is not enough to contain erofs_xattr_ibody_header then
	     ->xattr_isize should be 0 (it means no xattr);
	  2) it is just to contain erofs_xattr_ibody_header, which is on-disk
	     undefined right now (maybe use later with some new sb feature).
 xattr ondisk layout error 
 read in shared xattr array (non-atomic, see kmalloc below) 
 let's skip ibody header 
 cannot be unaligned 
 paired with smp_mb() at the beginning of the function. 
  the general idea for these return values is
  if    0 is returned, go on processing the current xattr;
        1 (> 0) is returned, skip this round to process the next xattr;
     -err (< 0) is returned, an error (maybe ENOXATTR) occurred
                             and need to be handled
  Regardless of success or failure, `xattr_foreach' will end up with
  `ofs' pointing to the next xattr item rather than an arbitrary position.
 0. fixup blkaddr, ofs, ipage 
	
	  1. read xattr entry to the memory,
	     since we do EROFS_XATTR_ALIGN
	     therefore entry should be in the page
 xattr on-disk corruption: xattr entry beyond xattr_isize 
 handle entry 
 2. handle xattr name (ofs will finally be at the end of name) 
 handle name 
 3. handle xattr value 
 xattrs should be 4-byte aligned (on-disk constraint) 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2017-2018 HUAWEI, Inc.
              https:www.huawei.com
  Copyright (C) 2021, Alibaba Cloud
  if inode is successfully read, return its inode page (or sometimes
  the inode payload page if it's an extended inode) in order to fill
  inline data if possible.
 check if the inode acrosses page boundary 
 extended inode has its own timestamp 
 total blocks for compressed files 
 fill chunked inode summary info 
 use build time for compact inodes 
 measure inode.i_blocks as generic filesystems 
 if it cannot be handled with fast symlink scheme 
 inline symlink data shouldn't cross page boundary as well 
 read inode base data from disk 
 setup the new inode 
  erofs nid is 64bits, but i_ino is 'unsigned long', therefore
  we should do more for 32-bit platform to find the right inode.
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2017-2018 HUAWEI, Inc.
              https:www.huawei.com
 since the on-disk name could not have the trailing '\0' 
 the last dirent in the block? 
 a corrupted entry is found 
 stopped by some reason 
 search dirents at the arbitrary position 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2018 HUAWEI, Inc.
              https:www.huawei.com
 global shrink count (for all mounted EROFS instances) 
 decrease refcount paired by erofs_workgroup_put 
 prefer to relax rcu read side 
	
	  Bump up a reference count before making this visible
	  to others for the XArray in order to avoid potential
	  UAF without serialized by xa_lock.
 try to legitimize the current in-tree one 
	
	  If managed cache is on, refcount of workgroups
	  themselves could be < 0 (freezed). In other words,
	  there is no guarantee that all refcounts > 0.
	
	  Note that all cached pages should be unattached
	  before deleted from the XArray. Otherwise some
	  cached pages could be still attached to the orphan
	  old workgroup when the new one is available in the tree.
	
	  It's impossible to fail after the workgroup is freezed,
	  however in order to avoid some race conditions, add a
	  DBG_BUGON to observe this in advance.
 last refcount should be connected with its managed pslot.  
 try to shrink each valid workgroup 
 protected by 'erofs_sb_list_lock' 
 protects the mounted 'erofs_sb_list' 
 clean up all remaining workgroups in memory 
 Iterate over all mounted superblocks and try to shrink them 
		
		  We move the ones we do to the end of the list, so we stop
		  when we see one we have already done.
 Get the next list element before we move this one 
		
		  Move this one to the end of the list to provide some
		  fairness.
 !CONFIG_EROFS_FS_ZIP 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2018 HUAWEI, Inc.
              https:www.huawei.com
  since pclustersize is variable for big pcluster feature, introduce slab
  pools implementation for different pcluster sizes.
 how to allocate cached pages for a pcluster 
 don't allocate any cached pages 
	
	  try to use cached IO if page allocation succeeds or fallback
	  to in-place IO instead to avoid any direct reclaim.
  tagged pointer with 1-bit tag for all compressed pages
  tag 0 - the page is just found with an extra page reference
	
	  no need to spawn too many threads, limiting threads could minimum
	  scheduling overhead, perhaps per-CPU threads should be better?
	
	  The current collection was the tail of an exist chain, in addition
	  that the previous processed chained collections are all decided to
	  be hooked up to it.
	  A new chain will be created for the remaining collections which are
	  not processed yet, therefore different from COLLECT_PRIMARY_FOLLOWED,
	  the next collection cannot reuse the whole page safely in
	  the following scenario:
	   ________________________________________________________________
	  |      tail (partial) page     |       head (partial) page       |
	  |   (belongs to the next cl)   |   (belongs to the current cl)   |
	  |_______PRIMARY_FOLLOWED_______|________PRIMARY_HOOKED___________|
	
	  a weak form of COLLECT_PRIMARY_FOLLOWED, the difference is that it
	  could be dispatched into bypass queue later due to uptodated managed
	  pages. All related online pages cannot be reused for inplace IO (or
	  pagevec) since it can be directly decoded without IO submission.
	
	  The current collection has been linked with the owned chain, and
	  could also be linked with the remaining collections, which means
	  if the processing page is the tail page of the collection, thus
	  the current collection can safely use the whole page (since
	  the previous collection is under control) for in-place IO, as
	  illustrated below:
	   ________________________________________________________________
	  |  tail (partial) page |          head (partial) page           |
	  |  (of the current cl) |      (of the previous collection)      |
	  |  PRIMARY_FOLLOWED or |                                        |
	  |_____PRIMARY_HOOKED___|____________PRIMARY_FOLLOWED____________|
	 
	  [  () the above page can be used as inplace IO.               ]
 a pointer used to pick up inplace IO pages 
 used for applying cache strategy on the fly 
 the compressed page was loaded before 
 IO is needed, no possible to decompress directly 
 DONTALLOC 
	
	  don't do inplace IO if all compressed pages are available in
	  managed cache since it can be moved to the bypass queue instead.
 called by erofs_shrinker to get rid of all compressed_pages 
	
	  refcount of workgroup is now freezed as 1,
	  therefore no need to worry about available decompression users.
 block other users from reclaiming or migrating the page 
 barrier is implied in the following 'unlock_page' 
 0 - busy 
 page_type must be Z_EROFS_PAGE_TYPE_EXCLUSIVE 
 callers must be with collection lock held 
 give priority for inplaceio 
 type 1, nil pcluster (this pcluster doesn't belong to any chain.) 
 so we can attach this pcluster to our submission chain. 
	
	  type 2, link to the end of an existing open chain, be careful
	  that its submission is controlled by the original attached chain.
 type 3, it belongs to a chain, but it isn't the end of the chain 
 to avoid unexpected loop formed by corrupted images 
 used to check tail merging loop due to corrupted images 
 no available pcluster, let's allocate one 
 new pclusters should be claimed as type 1, primary and followed 
	
	  lock all primary followed works before visible to others
	  and mutex_trylock never fails for a new pcluster.
 used to check tail merging loop due to corrupted images 
 must be Z_EROFS_PCLUSTER_TAIL or pointed to previous collection 
 since file-backed online pages are traversed in reverse order 
  keep in mind that no referenced pclusters will be freed
  only after a RCU grace period.
	
	  if all pending pages are added, don't hold its reference
	  any longer if the pcluster isn't hosted by ourselves.
 register locked file pages as online pages in pack 
 lucky, within the range of the current map_blocks 
 didn't get a valid collection previously (very rare) 
 go ahead the next map_blocks 
 preload all compressed pages (maybe downgrade role if necessary) 
	
	  Ensure the current partial page belongs to this submit chain rather
	  than other concurrent submit chains or the noio(bypass) chain since
	  those chains are handled asynchronously thus the page cannot be used
	  for inplace IO or pagevec (should be processed in strict order.)
 let's derive page type 
 should allocate an additional short-lived page for pagevec 
 bump up the number of spiltted parts of a page 
 also update nr_pages 
 can be used for verification 
 if some error occurred while processing this page 
 wake up the caller thread for sync decompression 
 Use workqueue and sync decompression for atomic contexts only 
 fallback to global pagemap for the lowmem scenario 
 all pages in pagevec ought to be valid 
		
		  currently EROFS doesn't support multiref(dedup),
		  so here erroring out one multiref page.
 all compressed pages ought to be valid 
			
			  only if non-head page can be selected
			  for inplace decompression
 PG_error needs checking for all non-managed pages 
 must handle all compressed pages before ending pages 
 recycle all individual short-lived pages 
 recycle all individual short-lived pages 
 all cl locks MUST be taken before the following line 
 all cl locks SHOULD be released right now 
 no possible that 'owned' equals Z_EROFS_WORK_TPTR_TAIL 
 no possible that 'owned' equals NULL 
 process the target tagged pointer 
	
	  preallocated cached pages, which is used to avoid direct reclaim
	  otherwise, it will go inplace IO path instead.
	
	  file-backed online pages in plcuster are all locked steady,
	  therefore it is impossible for `mapping' to be NULL.
 ought to be unmanaged pages 
 directly return for shortlived page as well 
 only true if page reclaim goes wrong, should never happen 
 the page is still in manage cache 
			
			  impossible to be !PagePrivate(page) for
			  the current restriction as well if
			  the page is already in compressed_pages[].
 no need to submit io if it is already up-to-date 
	
	  the managed page has been truncated, it's unsafe to
	  reuse this one, let's allocate a new cache-managed page.
 turn into temporary page if fails (1 ref) 
 drop a refcount added by allocpage (then we have 2 refs here) 
 the only exit (for tracing and debugging) 
 define decompression jobqueue types 
	
	  if managed cache is enabled, bypass jobqueue is needed,
	  no need to read from device for all pclusters in this queue.
 bio is NULL initially, so no need to initialize last_{index,bdev} 
 by default, all need io submission 
 no possible 'owned_head' equals the following 
 no device id here, thus it will always succeed 
 close the main owned chain at first 
	
	  although background is preferred, no one is pending for submission.
	  don't issue workqueue for decompression but drop it directly instead.
 handle bypass queue (no io pclusters) immediately 
 wait until all bios are completed 
 handle synchronous decompress queue in the caller context 
  Since partial uptodate is still unimplemented for now, we have to use
  approximate readmore strategies as a start.
 expend ra for the trailing edge if readahead 
 if some compressed cluster ready, need submit them anyway 
 traversal in reverse order 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2018-2019 HUAWEI, Inc.
              https:www.huawei.com
		
		  paired with smp_mb() at the end of the function to ensure
		  fields will only be observed after the bit is set.
 paired with smp_mb() at the beginning of the function 
 compression extent information gathered 
 vcnt - 1 (Z_EROFS_VLE_CLUSTER_TYPE_NONHEAD) item 
 figure out lookahead_distance: delta[1] if needed 
		
		  since the last lcluster in the pack is special,
		  of which lo saves delta[1] rather than delta[0].
		  Hence, get delta[0] by the previous lcluster indirectly.
 figout out blkaddr (pblk) for HEAD lclusters 
 bigpcluster shouldn't have plain d0 == 1 
 used to align to 32-byte (compacted_2b) alignment 
 load extent head logical cluster if needed 
	
	  If the 1st NONHEAD lcluster has already been handled initially wo
	  valid compressedlcs, which means at least it mustn't be CBLKCNT, or
	  an internal implemenatation error is detected.
	 
	  The following code can also handle it properly anyway, but let's
	  BUG_ON in the debugging mode only for developers to notice that.
		
		  if the 1st NONHEAD lcluster is actually PLAIN or HEAD type
		  rather than CBLKCNT, it's a 1 lcluster-sized pcluster.
 handle the last EOF pcluster (no next HEAD lcluster) 
 go on until the next HEAD lcluster 
 when trying to read beyond EOF, leave it unmapped 
 m.lcn should be >= 1 if endoff < m.clusterofs 
 get the corresponding first chunk 
 aggressively BUG_ON iff CONFIG_EROFS_FS_DEBUG is on 
		
		  No strict rule how to describe extents for post EOF, yet
		  we need do like below. Otherwise, iomap itself will get
		  into an endless loop on post EOF.
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2019 HUAWEI, Inc.
              https:www.huawei.com
 history window size 
 set to maximum value by default 
 reserved case 
  Fill all gaps with bounce pages if it's a sparse page list. Also check if
  all physical pages are consecutive, which can be seen for moderate CR.
 'valid' bounced can only be tested after a complete round 
 Or copy compressed data which can be overlapped to per-CPU buffer 
 decompression inplace is only safe when 0padding is enabled 
 legacy format could compress extra data in a pcluster. 
 one optimized fast path only for non bigpcluster cases yet 
 general decoding path which can be used for all cases 
 SPDX-License-Identifier: GPL-2.0-or-later
 considering the LZMA performance, no need to use a lockless list for now 
 there should be no running fs instance 
 by default, use # of possible CPUs instead 
 in case 2 z_erofs_load_lzma_config() race to avoid deadlock 
 1. collectisolate all streams for the following check 
 2. walk each isolated stream and grow max dict_size if needed 
 3. push back all to the global list and update max dict_size 
 1. get the exact LZMA compressed size 
 2. get an available lzma context 
 3. multi-call decompress 
		
		  Handle overlapping: Use bounced buffer if the compressed
		  data is under processing; Otherwise, Use short-lived pages
		  from the on-stack pagepool where pages share with the same
		  request.
 4. push back LZMA stream context to the global list 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Gao Xiang <xiang@kernel.org>
  For low-latency decompression algorithms (e.g. lz4), reserve consecutive
  per-CPU virtual memory (in pages) in advance to store such inplace IO
  data if inplace decompression is failed (due to unmet inplace margin for
  example).
 check if the per-CPU buffer is too small 
 (for sparse checker) pretend pcb->lock is still taken 
 the next step: support per-CPU page buffers hotplug 
 avoid shrinking pcpubuf, since no idea how many fses rely on 
 SPDX-License-Identifier: GPL-2.0
  linuxfsext2acl.c
  Copyright (C) 2001-2003 Andreas Gruenbacher, <agruen@suse.de>
  Convert from filesystem to in-memory representation.
  Convert from in-memory to filesystem representation.
  inode->i_mutex: don't care
  inode->i_mutex: down
  Initialize the ACLs of a new inode. Called from ext2_new_inode.
  dir->i_mutex: down
  inode->i_mutex: up (access to inode is still exclusive)
 SPDX-License-Identifier: GPL-2.0
  linuxfsext2xattr_user.c
  Handler for extended user attributes.
  Copyright (C) 2001 by Andreas Gruenbacher, <a.gruenbacher@computer.org>
 SPDX-License-Identifier: GPL-2.0
  linuxfsext2ioctl.c
  Copyright (C) 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
 Is it quota file? Do not allow user to mess with it 
		
		  need to allocate reservation structure for this inode
		  before set the window size
		
		  XXX What lock should protect the rsv_goal_size?
		  Accessed in ext2_get_block only.  ext3 uses i_truncate.
 These are just misnamed, they actually getput fromto user an int 
 SPDX-License-Identifier: GPL-2.0
  linuxfsext2namei.c
  Rewrite to pagecache. Almost all code had been changed, so blame me
  if the things go wrong. Please, send bug reports to
  viro@parcelfarce.linux.theplanet.co.uk
  Stuff here is basically a glue between the VFS and generic UNIXish
  filesystem that keeps everything in pagecache. All knowledge of the
  directory layout is in fsext2dir.c - it turned out to be easily separatable
  and it's easier to debug that way. In principle we might want to
  generalize that a bit and turn it into a library. Or not.
  The only non-static object here is ext2_dir_inode_operations.
  TODO: get rid of kmap() use, add readahead.
  Copyright (C) 1992, 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
   from
   linuxfsminixnamei.c
   Copyright (C) 1991, 1992  Linus Torvalds
   Big-endian to little-endian byte-swappingbitmaps by
         David S. Miller (davem@caip.rutgers.edu), 1995
  Methods themselves.
  By the time this is called, we already have created
  the directory cache entry for the new file, but it
  is so far negative - it has no inode.
  If the create succeeds, we fill in the inode information
  with d_instantiate(). 
 slow symlink 
 fast symlink 
	
	  Like most other Unix systems, set the ctime for inodes on a
 	  rename.
 SPDX-License-Identifier: GPL-2.0-only
   linuxfsext2super.c
  Copyright (C) 1992, 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
   from
   linuxfsminixinode.c
   Copyright (C) 1991, 1992  Linus Torvalds
   Big-endian to little-endian byte-swappingbitmaps by
         David S. Miller (davem@caip.rutgers.edu), 1995
  This must be called with sbi->s_lock held.
 leave es->s_feature_compat flags alone 
 es->s_uuid will be set by e2fsck if empty 
	
	  The rest of the superblock fields should be zero, and if not it
	  means they are likely already in use, so leave them alone.  We
	  can leave it up to e2fsck to clean up any inconsistencies there.
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
	
	  ext2_iget isn't quite right if the inode is currently unallocated!
	  However ext2_iget currently does appropriate checks to handle stale
	  inodes so everything is OK.
 we didn't find the right inode.. 
 Default location 
 handled by get_sb_block() instead of here 
 sb_block = match_int(&args[0]); 
  Maximal file size.  There is a direct, and {,double-,triple-}indirect
  block limit, and also a limit of (2^32 - 1) 512-byte sectors in i_blocks.
  We need to be 1 filesystem block less than the 2^32 sector limit.
	 This is calculated to be the largest file size for a
	  dense, file such that the total number of
	  sectors in the file, including data and all indirect blocks,
	  does not exceed 2^32 -1
	  __u32 i_blocks representing the total number of
	  512 bytes blocks of the file
 total blocks in file system block size 
 Compute how many blocks we can address by block tree 
 Does block tree limit file size? 
 How many metadata blocks are needed for addressing upper_limit? 
 indirect blocks 
 double indirect blocks 
 tripple indirect blocks for the rest 
	
	  See what the current blocksize for the device is, and
	  use that as the blocksize.  Otherwise (or if the blocksize
	  is smaller than the default) use the default.
	  This is important for devices that have a hardware
	  sectorsize that is larger than the default.
	
	  If the superblock doesn't start on a hardware sector boundary,
	  calculate the offset.  
	
	  Note: s_es must be initialized as soon as possible because
	        some ext2 macro-instructions depend on its value
 Set defaults before we parse the mount options 
	
	  Check feature flags regardless of the revision level, since we
	  previously didn't change the revision level when setting the flags,
	  so there is a chance incompat flags are set on a rev 0 filesystem.
 If the blocksize doesn't match, re-read the thing.. 
 per filesystem reservation list head & lock 
	
	  Add a single, static dummy reservation to the start of the
	  reservation window list --- it gives us a placeholder for
	  append-at-start-of-list which makes the allocation logic
	  _much_ simpler.
	
	  set up enough so that it can read an inode
		
		  Oh, dear.  A previous attempt to write the
		  superblock failed.  This could happen because the
		  USB device was yanked out.  Or it could happen to
		  be a transient write error and maybe the block will
		  be remapped.  Nothing we can do but to retry the
		  write and hope for the best.
 unlock before we do IO 
  In the second extended file system, it is not necessary to
  write the super block since we use a mapping of the
  disk super block in a buffer.
  However, this function is still used to set the fs valid
  flags to 0.  We need to set this flag to 0 since the fs
  may have been checked while mounted and e2fsck may have
  set s_state to EXT2_VALID_FS after some corrections.
	
	  Write quota structures to quota file, sync_blockdev() will write
	  them to disk later
	
	  Open but unlinked files present? Keep EXT2_VALID_FS flag cleared
	  because we have unattached inodes and thus filesystem is not fully
	  consistent.
 Set EXT2_FS_VALID flag 
 Just write sb to clear EXT2_VALID_FS flag 
		
		  OK, we are remounting a valid rw partition rdonly, so set
		  the rdonly flag and then mark the partition as valid again.
		
		  Mounting a RDONLY partition read-write, so reread and
		  store the current valid flag.  (It may have been changed
		  by e2fsck since we originally mounted the partition.)
		
		  Compute the overhead (FS structures). This is constant
		  for a given filesystem unless the number of block groups
		  changes so we cache the previous value until it does.
		
		  All of the blocks before first_data_block are
		  overhead
		
		  Add the overhead attributed to the superblock and
		  block group descriptors.  If the sparse superblocks
		  feature is turned on, then not all groups have this.
		
		  Every block group has an inode bitmap, a block
		  bitmap, and an inode table.
 Read data from quotafile - avoid pagecache and such because we cannot afford
  acquiring the locks... As quota files are never truncated and quota code
  itself serializes the operations (and no one else should touch the files)
 A hole? 
 Write to quotafile 
 SPDX-License-Identifier: GPL-2.0
   linuxfsext2ialloc.c
  Copyright (C) 1992, 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
   BSD ufs-inspired inode and directory allocation by 
   Stephen Tweedie (sct@dcs.ed.ac.uk), 1993
   Big-endian to little-endian byte-swappingbitmaps by
         David S. Miller (davem@caip.rutgers.edu), 1995
  ialloc.c contains the inodes allocation and deallocation routines
  The free inodes are managed by bitmaps.  A file system contains several
  blocks groups.  Each group contains 1 bitmap block for blocks, 1 bitmap
  block for inodes, N blocks for the inode table and data blocks.
  The file system contains group descriptors which are located after the
  super block.  Each descriptor contains the number of the bitmap block and
  the free blocks count in the block.
  Read the inode allocation bitmap for a given block_group, reading
  into the specified slot in the superblock's bitmap cache.
  Return buffer_head of bitmap on success or NULL.
  NOTE! When we get the inode, we're the only people
  that have access to it, and as such there are no
  race conditions we have to worry about. The inode
  is not on the hash-lists, and it cannot be reached
  through the filesystem because the directory entry
  has been deleted earlier.
  HOWEVER: we must make sure that we get no aliases,
  which means that we have to call "clear_inode()"
  _before_ we mark the inode not in use in the inode
  bitmaps. Otherwise a newly created file might use
  the same inode number (not actually the same pointer
  though), and then we'd have two inodes sharing the
  same inode number and space on the harddisk.
	
	  Note: we must free any quota before locking the superblock,
	  as writing the quota to disk may need the lock as well.
 Quota is already initialized in iput() 
 Ok, now we can actually update the inode bitmaps.. 
  We perform asynchronous prereading of the new inode's inode block when
  we create the inode, in the expectation that the inode will be written
  back soon.  There are two reasons:
  - When creating a large number of files, the async prereads will be
    nicely merged into large reads
  - When writing out a large number of inodes, we don't need to keep on
    stalling the writes while we read the inode block.
  FIXME: ext2_get_group_desc() needs to be simplified.
	
	  Figure out the offset within the block group inode table
  There are two policies for allocating an inode.  If the new inode is
  a directory, then a forward search is made for a block group with both
  free space and a low directory-to-inode ratio; if that fails, then of
  the groups with above-average free space, that group with the fewest
  directories already is chosen.
  For other inodes, search forward from the parent directory\'s block
  group to find a free inode.
  Orlov's allocator for directories. 
  We always try to spread first-level directories.
  If there are blockgroups with both free inodes and free blocks counts 
  not worse than average we return one with smallest directory count. 
  Otherwise we simply return a random group. 
  For the rest rules look so: 
  It's OK to put directory into a group unless 
  it has too many directories already (max_dirs) or 
  it has too few free inodes left (min_inodes) or 
  it has too few free blocks left (min_blocks) or 
  it's already running too large debt (max_debt). 
  Parent's group is preferred, if it doesn't satisfy these 
  conditions we search cyclically through the rest. If none 
  of the groups look good we just look for a group with more 
  free inodes than average (starting at parent's group). 
  Debt is incremented each time we allocate a directory and decremented 
  when we allocate an inode, within 0--255. 
 percpu_counters are approximate... 
		
		  The free-inodes counter is approximate, and for really small
		  filesystems the above test can fail to find any blockgroups
	
	  Try to place the inode in its parent directory
	
	  We're going to place this inode in a different blockgroup from its
	  parent.  We want to cause files in a common directory to all land in
	  the same blockgroup.  But we want files which are in a different
	  directory which shares a blockgroup with our parent to land in a
	  different blockgroup.
	 
	  So add our directory's i_ino into the starting point for the hash.
	
	  Use a quadratic hash to find a group with a free inode and some
	  free blocks.
	
	  That failed: try linear search for a free inode, even if that group
	  has no free blocks.
			
			  Rare race: find_group_xx() decided that there were
			  free inodes in this group, but by the time we tried
			  to allocate one, they're all gone.  This can also
			  occur because the counters which find_group_orlov()
			  uses are approximate.  So just go and search the
			  next block group.
 we lost this inode 
 this group is exhausted, try next group 
 try to find free inode in the same group 
	
	  Scanned all blockgroups.
 Called at mount-time, super-block is locked 
 SPDX-License-Identifier: GPL-2.0
  linuxfsext2xattr.c
  Copyright (C) 2001-2003 Andreas Gruenbacher <agruen@suse.de>
  Fix by Harrison Xing <harrison@mountainviewdata.com>.
  Extended attributes for symlinks and special files added per
   suggestion of Luka Renko <luka.renko@hermes.si>.
  xattr consolidation Copyright (c) 2004 James Morris <jmorris@redhat.com>,
   Red Hat Inc.
  Extended attributes are stored on disk blocks allocated outside of
  any inode. The i_file_acl field is then made to point to this allocated
  block. If all extended attributes of an inode are identical, these
  inodes may share the same extended attribute block. Such situations
  are automatically detected by keeping a cache of recent attribute block
  numbers and hashes over the block's contents in memory.
  Extended attribute block layout:
    +------------------+
    | header           |
    | entry 1          | |
    | entry 2          | | growing downwards
    | entry 3          | v
    | four null bytes  |
    | . . .            |
    | value 1          | ^
    | value 3          | | growing upwards
    | value 2          | |
    +------------------+
  The block header is followed by multiple entry descriptors. These entry
  descriptors are variable in size, and aligned to EXT2_XATTR_PAD
  byte boundaries. The entry descriptors are sorted by attribute name,
  so that two extended attribute blocks can be compared efficiently.
  Attribute values are aligned to the end of the block, stored in
  no specific order. They are also padded to EXT2_XATTR_PAD byte
  boundaries. No additional gaps are left between them.
  Locking strategy
  ----------------
  EXT2_I(inode)->i_file_acl is protected by EXT2_I(inode)->xattr_sem.
  EA blocks are only changed if they are exclusive to an inode, so
  holding xattr_sem also means that nothing but the EA block's reference
  count will change. Multiple writers to an EA block are synchronized
  by the bh lock. No more than a single bh lock is held at any time
  to avoid deadlocks.
  ext2_xattr_get()
  Copy an extended attribute into the buffer
  provided, or compute the buffer size required.
  Buffer is NULL to compute the size of the buffer required.
  Returns a negative error number on failure, or the number of bytes
  used  required on success.
 find named attribute 
 return value of attribute 
  ext2_xattr_list()
  Copy a list of attribute names into the buffer
  provided, or compute the buffer size required.
  Buffer is NULL to compute the size of the buffer required.
  Returns a negative error number on failure, or the number of bytes
  used  required on success.
 check the on-disk data structure 
 list the attribute names 
 total size 
  Inode operation listxattr()
  d_inode(dentry)->i_mutex: don't care
  If the EXT2_FEATURE_COMPAT_EXT_ATTR feature of this file system is
  not set, set it.
  ext2_xattr_set()
  Create, replace or remove an extended attribute for this inode.  Value
  is NULL to remove an existing extended attribute, and non-NULL to
  either replace an existing extended attribute, or create a new extended
  attribute. The flags XATTR_REPLACE and XATTR_CREATE
  specify that an extended attribute must exist and must not exist
  previous to the call, respectively.
  Returns 0, or a negative error number on failure.
	
	  header -- Points either into bh, or to a temporarily
	            allocated buffer.
	  here -- The named entry found, or the place for inserting, within
	          the block pointed to by header.
	  last -- Points right after the last named entry within the block
	          pointed to by header.
	  min_offs -- The offset of the first value (values are aligned
	              towards the end of the block).
	  end -- Points right after the block pointed to by header.
 The inode already has an extended attribute block. 
		
		  Find the named attribute. If not found, 'here' will point
		  to entry where the new attribute should be inserted to
		  maintain sorting.
 Check whether we have enough space left. 
 We will use a new extended attribute block. 
 Request to remove a nonexistent attribute? 
 Request to create an existing attribute? 
 Here we know that we can set the new attribute. 
 assert(header == HDR(bh)); 
			
			  This must happen under buffer lock for
			  ext2_xattr_set2() to reliably detect modified block
 keep the buffer locked while modifying it. 
 Allocate a buffer where we construct the new block. 
 Iff we are modifying the block in-place, bh is locked here. 
 Insert the new name. 
				 The old and the new value have the same
 Clear pad bytes. 
 Remove the old value. 
 Adjust all value offsets. 
 Remove the old name. 
 Insert the new value. 
 Clear the pad bytes. 
 This block is now empty. 
 we were modifying in-place. 
 we were modifying in-place. 
  Second half of ext2_xattr_set(): Update the file system.
 We found an identical block in the cache. 
				 The old block is released after updating
			 Keep this block. No need to lock the block as we
 We need to allocate a new block 
 Update the inode. 
		 In case sync failed due to ENOSPC the inode was actually
		  written (only some dirty data were not) so we just proceed
		
		  If there was an old block and we are no longer using it,
		  release the old block.
			
			  This must happen under buffer lock for
			  ext2_xattr_set2() to reliably detect freed block
 Free the old block. 
			 We let our caller release old_bh, so we
 Decrement the refcount only. 
  ext2_xattr_delete_inode()
  Free extended attribute resources associated with this inode. This
  is called immediately before an inode is freed.
	
	  We are the only ones holding inode reference. The xattr_sem should
	  better be unlocked! We could as well just not acquire xattr_sem at
	  all but this makes the code more futureproof. OTOH we need trylock
	  here to avoid false-positive warning from lockdep about reclaim
	  circular dependency.
		
		  This must happen under buffer lock for ext2_xattr_set2() to
		  reliably detect freed block
  ext2_xattr_cache_insert()
  Create a new entry in the extended attribute cache, and insert
  it unless such an entry is already in the cache.
  Returns 0, or a negative error number on failure.
  ext2_xattr_cmp()
  Compare two extended attribute blocks for equality.
  Returns 0 if the blocks are equal, 1 if they differ, and
  a negative error number on errors.
  ext2_xattr_cache_find()
  Find an identical extended attribute block.
  Returns a locked buffer head to the block found, or NULL if such
  a block was not found or an error occurred.
 never share 
			
			  We have to be careful about races with freeing or
			  rehashing of xattr block. Once we hold buffer lock
			  xattr block's state is stable so we can check
			  whether the block got freed  rehashed or not.
			  Since we unhash mbcache entry under buffer lock when
			  freeing  rehashing xattr block, checking whether
			  entry is still hashed is reliable.
  ext2_xattr_hash_entry()
  Compute the hash of an extended attribute.
  ext2_xattr_rehash()
  Re-compute the extended attribute hash value after an entry has changed.
 Block is not shared if an entry's hash value == 0 
 SPDX-License-Identifier: GPL-2.0
   linuxfsext2inode.c
  Copyright (C) 1992, 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
   from
   linuxfsminixinode.c
   Copyright (C) 1991, 1992  Linus Torvalds
   Goal-directed block allocation by Stephen Tweedie
  	(sct@dcs.ed.ac.uk), 1993, 1998
   Big-endian to little-endian byte-swappingbitmaps by
         David S. Miller (davem@caip.rutgers.edu), 1995
   64-bit file support on 64-bit platforms by Jakub Jelinek
  	(jj@sunsite.ms.mff.cuni.cz)
   Assorted race fixes, rewrite of ext2_get_block() by Al Viro, 2000
  Test whether an inode is a fast symlink.
  Called at the last iput() if i_nlink is zero.
 set dtime 
 truncate to 0 
 	ext2_block_to_path - parse the block number into array of offsets
 	@inode: inode in question (we are only interested in its superblock)
 	@i_block: block number to be parsed
 	@offsets: array to store the offsets in
       @boundary: set this non-zero if the referred-to block is likely to be
              followed (on disk) by an indirect block.
 	To store the locations of file's data ext2 uses a data structure common
 	for UNIX filesystems - tree of pointers anchored in the inode, with
 	data blocks at leaves and indirect blocks in intermediate nodes.
 	This function translates the block number into path in that tree -
 	return value is the path length and @offsets[n] is the offset of
 	pointer to (n+1)th node in the nth one. If @block is out of range
 	(negative or too large) warning is printed and zero returned.
 	Note: function doesn't find node addresses, so no IO is needed. All
 	we need to know is the capacity of indirect blocks (taken from the
 	inode->i_sb).
  Portability note: the last comparison (check that we fit into triple
  indirect block) is spelled differently, because otherwise on an
  architecture with 32-bit longs and 8Kb pages we might get into trouble
  if our filesystem had 8Kb blocks. We might use long long, but that would
  kill us on x86. Oh, well, at least the sign propagation does not matter -
  i_block would have to be negative in the very beginning, so we would not
  get there at all.
 	ext2_get_branch - read the chain of indirect blocks leading to data
 	@inode: inode in question
 	@depth: depth of the chain (1 - direct pointer, etc.)
 	@offsets: offsets of pointers in inodeindirect blocks
 	@chain: place to store the result
 	@err: here we store the error value
 	Function fills the array of triples <key, p, bh> and returns %NULL
 	if everything went OK or the pointer to the last filled triple
 	(incomplete one) otherwise. Upon the return chain[i].key contains
 	the number of (i+1)-th block in the chain (as it is stored in memory,
 	i.e. little-endian 32-bit), chain[i].p contains the address of that
 	number (it points into struct inode for i==0 and into the bh->b_data
 	for i>0) and chain[i].bh points to the buffer_head of i-th indirect
 	block for i>0 and NULL for i==0. In other words, it holds the block
 	numbers of the chain, addresses they were taken from (and where we can
 	verify that chain did not change) and buffer_heads hosting these
 	numbers.
 	Function stops when it stumbles upon zero pointer (absent block)
 		(pointer to last triple returned, @err == 0)
 	or when it gets an IO error reading an indirect block
 		(ditto, @err == -EIO)
 	or when it notices that chain had been changed while it was reading
 		(ditto, @err == -EAGAIN)
 	or when it reads all @depth-1 indirect blocks successfully and finds
 	the whole chain, all way to the data (returns %NULL, err == 0).
 i_data is not going away, no lock needed 
 	ext2_find_near - find a place for allocation with sufficient locality
 	@inode: owner
 	@ind: descriptor of indirect block.
 	This function returns the preferred place for block allocation.
 	It is used when heuristic for sequential allocation fails.
 	Rules are:
 	  + if there is a block to the left of our position - allocate near it.
 	  + if pointer will live in indirect block - allocate near that block.
 	  + if pointer will live in inode - allocate in the same cylinder group.
  In the latter case we colour the starting block by the callers PID to
  prevent it from clashing with concurrent allocations for a different inode
  in the same block group.   The PID is used here so that functionally related
  files will be close-by on-disk.
 	Caller must make sure that @ind is valid and will stay that way.
 Try to find previous block 
 No such thing, so let's try location of indirect block 
	
	  It is going to be referred from inode itself? OK, just put it into
	  the same cylinder group then.
 	ext2_find_goal - find a preferred place for allocation.
 	@inode: owner
 	@block:  block we want
 	@partial: pointer to the last triple within a chain
 	Returns preferred place for a block (the goal).
	
	  try the heuristic for sequential allocation,
	  failing that at least try to get decent locality.
 	ext2_blks_to_allocate: Look up the block map and count the number
 	of direct blocks need to be allocated for the given branch.
  	@branch: chain of indirect blocks
 	@k: number of blocks need for indirect blocks
 	@blks: number of data blocks to be mapped.
 	@blocks_to_boundary:  the offset in the indirect block
 	return the number of direct blocks to allocate.
	
	  Simple case, [t,d]Indirect block(s) has not allocated yet
	  then it's clear blocks on that path have not allocated
 right now don't hanel cross boundary allocation 
 	ext2_alloc_blocks: multiple allocate blocks needed for a branch
 	@indirect_blks: the number of blocks need to allocate for indirect
 			blocks
 	@blks: the number of blocks need to allocate for direct blocks
 	@new_blocks: on return it will store the new block numbers for
 	the indirect blocks(if needed) and the first direct block,
	
	  Here we try to allocate the requested multiple blocks at once,
	  on a best-effort basis.
	  To build a branch, we should allocate blocks for
	  the indirect blocks(if not allocated yet), and at least
	  the first direct block of this branch.  That's the
	  minimum number of blocks need to allocate(required)
 allocating blocks for indirect blocks and direct blocks 
 allocate blocks for indirect blocks 
 save the new block number for the first direct block 
 total number of blocks allocated for direct blocks 
 	ext2_alloc_branch - allocate and set up a chain of blocks.
 	@inode: owner
 	@indirect_blks: depth of the chain (number of blocks to allocate)
 	@blks: number of allocated direct blocks
 	@goal: preferred place for allocation
 	@offsets: offsets (in the blocks) to store the pointers to next.
 	@branch: place to store the chain in.
 	This function allocates @num blocks, zeroes out all but the last one,
 	links them into chain and (if we are synchronous) writes them to disk.
 	In other words, it prepares a branch that can be spliced onto the
 	inode. It stores the information about that chain in the branch[], in
 	the same format as ext2_get_branch() would do. We are calling it after
 	we had read the existing part of chain and partial points to the last
 	triple of that (one with zero ->key). Upon the exit we have the same
 	picture as after the successful ext2_get_block(), except that in one
 	place chain is disconnected - branch->p is still zero (we did not
 	set the last link), but branch->key contains the number that should
 	be placed into branch->p to fill that gap.
 	If allocation fails we free all blocks we've allocated (and forget
 	their buffer_heads) and return the error value the from failed
 	ext2_alloc_block() (normally -ENOSPC). Otherwise we set the chain
 	as described above and return 0.
	
	  metadata blocks and data blocks are allocated.
		
		  Get buffer_head for parent block, zero it out
		  and set the pointer to new one, then send
		  parent to disk.
			
			  End of chain, update the last new metablock of
			  the chain to point to the new allocated
			  data blocks numbers
		 We used to sync bh here if IS_SYNC(inode).
		  But we now rely upon generic_write_sync()
		  and b_inode_buffers.  But not for directories.
  ext2_splice_branch - splice the allocated branch onto inode.
  @inode: owner
  @block: (logical) number of block we are adding
  @where: location of missing link
  @num:   number of indirect blocks we are adding
  @blks:  number of direct blocks we are adding
  This function fills the missing link and does all housekeeping needed in
  inode (->i_blocks, etc.). In case of success we end up with the full
  chain to new block and return 0.
 XXX LOCKING probably should have i_meta_lock ?
 That's it 
	
	  Update the host buffer_head or inode to point to more just allocated
	  direct blocks blocks
	
	  update the most recently allocated logical & physical block
	  in i_block_alloc_info, to assist find the proper goal block for next
	  allocation
 We are done with atomic stuff, now do the rest of housekeeping 
 had we spliced it onto indirect block? 
  Allocation strategy is simple: if we have to allocate something, we will
  have to go the whole way to leaf. So let's do it before attaching anything
  to tree, set linkage between the newborn blocks, write them if sync is
  required, recheck the path, free and repeat if check fails, otherwise
  set the last missing link (that will protect us from any truncate-generated
  removals - all blocks on the path are immune now) and possibly force the
  write on the parent block.
  That has a nice additional property: no special recovery from the failed
  allocations is needed - we simply release blocks and do not touch anything
  reachable from inode.
  `handle' can be NULL if create == 0.
  return > 0, # of blocks mapped or allocated.
  return = 0, if plain lookup failed.
  return < 0, error case.
 Simplest case - block found, no allocation needed 
map more blocks
				
				  Indirect block might be removed by
				  truncate while we were reading it.
				  Handling of that case: forget what we've
				  got now, go to reread.
 Next simple case - plain lookup or failed read of indirect block 
	
	  If the indirect block is missing while we are reading
	  the chain(ext2_get_branch() returns -EAGAIN err), or
	  if the chain has been changed after we grab the semaphore,
	  (either because another process truncated this branch, or
	  another get_block allocated this branch) re-grab the chain to see if
	  the request block has been allocated or not.
	 
	  Since we already block the truncateother get_block
	  at this point, we will have the current copy of the chain when we
	  splice the branch into the tree.
	
	  Okay, we need to do block allocation.  Lazily initialize the block
	  allocation info here if necessary
 the number of blocks need to allocate for [d,t]indirect blocks 
	
	  Next look up the indirect map to count the total number of
	  direct blocks to allocate for this branch.
	
	  XXX ???? Block out ext2_truncate while we alter the tree
		
		  We must unmap blocks before zeroing so that writeback cannot
		  overwrite zeros with stale data from block device page cache.
		
		  block must be initialised before we put it in the tree
		  so that it's not found by another thread before it's
		  initialised
 Clean up and exit 
 the whole chain 
  Probably it should be a library function... search for first non-zero word
  or memcmp with zero_page, whatever is better for particular architecture.
  Linus?
 	ext2_find_shared - find the indirect blocks for partial truncation.
 	@inode:	  inode in question
 	@depth:	  depth of the affected branch
 	@offsets: offsets of pointers in that branch (see ext2_block_to_path)
 	@chain:	  place to store the pointers to partial indirect blocks
 	@top:	  place to the (detached) top of branch
 	This is a helper function used by ext2_truncate().
 	When we do truncate() we may have to clean the ends of several indirect
 	blocks but leave the blocks themselves alive. Block is partially
 	truncated if some data below the new i_size is referred from it (and
 	it is on the path to the first completely truncated data block, indeed).
 	We have to free the top of that path along with everything to the right
 	of the path. Since no allocation past the truncation point is possible
 	until ext2_truncate() finishes, we may safely do the latter, but top
 	of branch may require special attention - pageout below the truncation
 	point might try to populate it.
 	We atomically detach the top of branch from the tree, store the block
 	number of its root in @top, pointers to buffer_heads of partially
 	truncated blocks - in @chain[].bh and pointers to their last elements
 	that should not be removed - in @chain[].p. Return value is the pointer
 	to last filled element of @chain.
 	The work left to caller to do the actual freeing of subtrees:
 		a) free the subtree starting from @top
 		b) free the subtrees whose roots are stored in
 			(@chain[i].p+1 .. end of @chain[i].bh->b_data)
 		c) free the subtrees growing from the inode past the @chain[0].p
 			(no partially truncated stuff there).
	
	  If the branch acquired continuation since we've looked at it -
	  fine, it should all survive and (new) top doesn't belong to us.
	
	  OK, we've found the last block that must survive. The rest of our
	  branch should be detached before unlocking. However, if that rest
	  of branch is all ours and does not grow immediately from the inode
	  it's easier to cheat and just decrement partial->p.
 	ext2_free_data - free a list of data blocks
 	@inode:	inode we are dealing with
 	@p:	array of block numbers
 	@q:	points immediately past the end of array
 	We are freeing all blocks referred from that array (numbers are
 	stored as little-endian 32-bit) and updating @inode->i_blocks
 	appropriately.
 accumulate blocks to free if they're contiguous 
 	ext2_free_branches - free an array of branches
 	@inode:	inode we are dealing with
 	@p:	array of block numbers
 	@q:	pointer immediately past the end of array
 	@depth:	depth of the branches to free
 	We are freeing all blocks referred from these branches (numbers are
 	stored as little-endian 32-bit) and updating @inode->i_blocks
 	appropriately.
			
			  A read failure? Report error and clear slot
			  (should be rare).
 mapping->invalidate_lock must be held when calling this function 
	
	  From here we block out all ext2_get_block() callers who want to
	  modify the block allocation tree.
 Kill the top of shared branch (already detached) 
 Clear the ends of indirect blocks on the shared branch 
 Kill the remaining (whole) subtrees 
	
	  Figure out the offset within the block group inode table
	 We now have enough fields to check if the inode was active or not.
	  This is needed because nfsd might try to access dead inodes
	  the test is that same one that e2fsck uses
	  NeilBrown 1999oct15
 this inode is deleted 
	
	  NOTE! The in-memory inode i_data array is in little-endian order
	  even on big-endian machines: we do NOT byteswap the block numbers!
	 For fields not not tracking in the in-memory inode,
  Fix up interoperability with old kernels. Otherwise, old inodes get
  re-used with the upper 16 bits of the uidgid intact
			        If this is the first large file
				 created, add a flag to the superblock.
 SPDX-License-Identifier: GPL-2.0
   linuxfsext2dir.c
  Copyright (C) 1992, 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
   from
   linuxfsminixdir.c
   Copyright (C) 1991, 1992  Linus Torvalds
   ext2 directory handling functions
   Big-endian to little-endian byte-swappingbitmaps by
         David S. Miller (davem@caip.rutgers.edu), 1995
  All code that works with directory layout had been switched to pagecache
  and moved here. AV
  Tests against MAX_REC_LEN etc were put in place for 64k block
  sizes; if that is not possible on this arch, we can skip
  those tests and speed things up.
  ext2 uses block-sized chunks. Arguably, sector-sized ones would be
  more robust, but we have what we have
  Return the offset into page `page_nr' of the last valid
  byte in that page, plus one.
 Too bad, we had an error 
  Calls to ext2_get_page()ext2_put_page() must be nested according to the
  rules documented in kmap_local_page()kunmap_local().
  NOTE: ext2_find_entry() and ext2_dotdot() act as a call to ext2_get_page()
  and should be treated as a call to ext2_get_page() for nesting purposes.
  NOTE! unlike strncmp, ext2_match returns 1 for success, 0 for failure.
  len <= EXT2_NAME_LEN and de != NULL are guaranteed by caller.
  p is at least 6 bytes before the end of page
 	ext2_find_entry()
  finds an entry in the specified directory with the wanted name. It
  returns the page in which the entry was found (as a parameter - res_page),
  and the entry itself. Page is returned mapped and unlocked.
  Entry is guaranteed to be valid.
  On Success ext2_put_page() should be called on res_page.
  NOTE: Calls to ext2_get_page()ext2_put_page() must be nested according to
  the rules documented in kmap_local_page()kunmap_local().
  ext2_find_entry() and ext2_dotdot() act as a call to ext2_get_page() and
  should be treated as a call to ext2_get_page() for nesting purposes.
 OFFSET_CACHE 
 next page is past the blocks we've got 
  Return the '..' directory entry and the page in which the entry was found
  (as a parameter - p).
  On Success ext2_put_page() should be called on p.
  NOTE: Calls to ext2_get_page()ext2_put_page() must be nested according to
  the rules documented in kmap_local_page()kunmap_local().
  ext2_find_entry() and ext2_dotdot() act as a call to ext2_get_page() and
  should be treated as a call to ext2_get_page() for nesting purposes.
 	Parent is locked.
	
	  We take care of directory expansion in the same loop.
	  This code plays outside i_size, so it locks the page
	  to protect that region.
 We hit i_size 
 OFFSET_CACHE 
  ext2_delete_entry deletes a directory entry by merging it with the
  previous entry. Page is up-to-date.
  Set the first fragment of directory.
  routine to check that the specified directory is empty (for rmdir)
 check for . and .. 
 SPDX-License-Identifier: GPL-2.0
   linuxfsext2balloc.c
  Copyright (C) 1992, 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
   Enhanced block allocation by Stephen Tweedie (sct@redhat.com), 1993
   Big-endian to little-endian byte-swappingbitmaps by
         David S. Miller (davem@caip.rutgers.edu), 1995
  balloc.c contains the blocks allocation and deallocation routines
  The free blocks are managed by bitmaps.  A file system contains several
  blocks groups.  Each group contains 1 bitmap block for blocks, 1 bitmap
  block for inodes, N blocks for the inode table and data blocks.
  The file system contains group descriptors which are located after the
  super block.  Each descriptor contains the number of the bitmap block and
  the free blocks count in the block.  The descriptors are loaded in memory
  when a file system is mounted (see ext2_fill_super).
 check whether block bitmap block number is set 
 bad block bitmap 
 check whether the inode bitmap block number is set 
 bad block bitmap 
 check whether the inode table block number is set 
 good bitmap for inode tables 
  Read the bitmap for a given block_group,and validate the
  bits for blockinodeinode tables are set in the bitmaps
  Return buffer_head on success or NULL in case of failure.
	
	  file system mounted not to panic on error, continue with corrupt
	  bitmap
  The reservation window structure operations
  --------------------------------------------
  Operations include:
  dump, find, add, remove, is_empty, find_next_reservable_window, etc.
  We use a red-black tree to represent per-filesystem reservation
  windows.
  __rsv_window_dump() -- Dump the filesystem block allocation reservation map
  @root:		root of per-filesystem reservation rb tree
  @verbose:		verbose mode
  @fn:			function which wishes to dump the reservation map
  If verbose is turned on, it will print the whole block reservation
  windows(start, end). Otherwise, it will only print out the "bad" windows,
  those windows that overlap with their immediate neighbors.
  goal_in_my_reservation()
  @rsv:		inode's reservation window
  @grp_goal:		given goal block relative to the allocation block group
  @group:		the current allocation block group
  @sb:			filesystem super block
  Test if the given goal block (group relative) is within the file's
  own block reservation window range.
  If the reservation window is outside the goal allocation group, return 0;
  grp_goal (given goal block) could be -1, which means no specific
  goal block. In this case, always return 1.
  If the goal block is within the reservation window, return 1;
  otherwise, return 0;
  search_reserve_window()
  @root:		root of reservation tree
  @goal:		target allocation block
  Find the reserved window which includes the goal, or the previous one
  if the goal is not in any window.
  Returns NULL if there are no windows or if all windows start after the goal.
	
	  We've fallen off the end of the tree: the goal wasn't inside
	  any particular node.  OK, the previous node must be to one
	  side of the interval containing the goal.  If it's the RHS,
	  we need to back up one.
  ext2_rsv_window_add() -- Insert a window to the block reservation rb tree.
  @sb:			super block
  @rsv:		reservation window to add
  Must be called with rsv_lock held.
  rsv_window_remove() -- unlink a window from the reservation rb tree
  @sb:			super block
  @rsv:		reservation window to remove
  Mark the block reservation window as not allocated, and unlink it
  from the filesystem reservation window rb tree. Must be called with
  rsv_lock held.
  rsv_is_empty() -- Check if the reservation window is allocated.
  @rsv:		given reservation window to check
  returns 1 if the end block is EXT2_RESERVE_WINDOW_NOT_ALLOCATED.
 a valid reservation end block could not be 0 
  ext2_init_block_alloc_info()
  @inode:		file inode structure
  Allocate and initialize the  reservation window structure, and
  link the window to the ext2 inode structure at last
  The reservation window structure is only dynamically allocated
  and linked to ext2 inode the first time the open file
  needs a new block. So, before every ext2_new_block(s) call, for
  regular files, we should check whether the reservation window
  structure exists or not. In the latter case, this function is called.
  Fail to do so will result in block reservation being turned off for that
  open file.
  This function is called from ext2_get_blocks_handle(), also called
  when setting the reservation window size through ioctl before the file
  is open for write (needs block allocation).
  Needs truncate_mutex protection prior to calling this function.
	 	
		  if filesystem is mounted with NORESERVATION, the goal
		  reservation window size is set to zero to indicate
		  block reservation is off
  ext2_discard_reservation()
  @inode:		inode
  Discard(free) block reservation window on last file close, or truncate
  or at last iput().
  It is being called in three cases:
  	ext2_release_file(): last writer closes the file
  	ext2_clear_inode(): last iput(), when nobody links to this file.
  	ext2_truncate(): when the block indirect map is about to change.
  ext2_free_blocks() -- Free given blocks and update quota and i_blocks
  @inode:		inode
  @block:		start physical block to free
  @count:		number of blocks to free
	
	  Check to see if we are freeing blocks across a group
	  boundary.
  bitmap_search_next_usable_block()
  @start:		the starting block (group relative) of the search
  @bh:			bufferhead contains the block group bitmap
  @maxblocks:		the ending block (group relative) of the reservation
  The bitmap search --- search forward through the actual bitmap on disk until
  we find a bit free.
  find_next_usable_block()
  @start:		the starting block (group relative) to find next
  			allocatable block in bitmap.
  @bh:			bufferhead contains the block group bitmap
  @maxblocks:		the ending block (group relative) for the search
  Find an allocatable block in a bitmap.  We perform the "most
  appropriate allocation" algorithm of looking for a free block near
  the initial goal; then for a free byte somewhere in the bitmap;
  then for any free bit in the bitmap.
		
		  The goal was occupied; search forward for a free 
		  block within the next XX blocks.
		 
		  end_goal is more or less random, but it has to be
		  less than EXT2_BLOCKS_PER_GROUP. Aligning up to the
		  next 64-bit boundary is simple..
  ext2_try_to_allocate()
  @sb:			superblock
  @group:		given allocation block group
  @bitmap_bh:		bufferhead holds the block bitmap
  @grp_goal:		given target block within the group
  @count:		target number of blocks to allocate
  @my_rsv:		reservation window
  Attempt to allocate blocks within a give range. Set the range of allocation
  first, then find the first free bit(s) from the bitmap (within the range),
  and at last, allocate the blocks by claiming the found free bit as allocated.
  To set the range of this allocation:
  	if there is a reservation window, only try to allocate block(s)
  	from the file's own reservation window;
  	Otherwise, the allocation range starts from the give goal block,
  	ends at the block group's last block.
  If we failed to allocate the desired block then we may end up crossing to a
  new bitmap.
 we do allocation within the reservation window if we have a window 
  	find_next_reservable_window():
 		find a reservable space within the given range.
 		It does not allocate the reservation window for now:
 		alloc_new_reservation() will do the work later.
  	@search_head: the head of the searching list;
 		This is not necessarily the list head of the whole filesystem
 		We have both head and start_block to assist the search
 		for the reservable space. The list starts from head,
 		but we will shift to the place where start_block is,
 		then start from there, when looking for a reservable space.
 	@sb: the super block.
  	@start_block: the first block we consider to start the real search from
  	@last_block:
 		the maximum block number that our goal reservable space
 		could start from. This is normally the last block in this
 		group. The search will end when we found the start of next
 		possible reservable space is out of this boundary.
 		This could handle the cross boundary reservation window
 		request.
  	basically we search from the given range, rather than the whole
  	reservation double linked list, (start_block, last_block)
  	to find a free region that is of my size and has not
  	been reserved.
 TODO: make the start of the reservation window byte-aligned 
 cur = start_block & ~7;
		 TODO?
		  in the case we could not find a reservable space
		  that is what is expected, during the re-search, we could
		  remember what's the largest reservable space we could have
		  and return that one.
		 
		  For now it will fail if we could not find the reservable
		  space with expected-size (or more)...
 fail 
		
		  Reached the last reservation, we can just append to the
		  previous one.
			
			  Found a reserveable space big enough.  We could
			  have a reservation across the group boundary here
	
	  we come here either :
	  when we reach the end of the whole list,
	  and there is empty reservable space after last entry in the list.
	  append it to the end of the list.
	 
	  or we found one reservable space in the middle of the list,
	  return the reservation window that we could append to.
	  succeed.
	
	  Let's book the whole available window for now.  We will check the
	  disk bitmap later and then, if there are free blocks then we adjust
	  the window size if it's larger than requested.
	  Otherwise, we will remove this node from the tree next time
	  call find_next_reservable_window.
  	alloc_new_reservation()--allocate a new reservation window
 		To make a new reservation, we search part of the filesystem
 		reservation list (the list that inside the group). We try to
 		allocate a new reservation window near the allocation goal,
 		or the beginning of the group, if there is no goal.
 		We first find a reservable space after the goal, then from
 		there, we check the bitmap for the first free block after
 		it. If there is no free block until the end of group, then the
 		whole group is full, we failed. Otherwise, check if the free
 		block is inside the expected reservable space, if so, we
 		succeed.
 		If the first free block is outside the reservable space, then
 		start from the first free block, we search for next available
 		space, and go on.
 	on succeed, a new reservation will be found and inserted into the list
 	It contains at least one free block, and it does not overlap with other
 	reservation windows.
 	failed: we failed to find a reservation window in this group
 	@my_rsv: the reservation
 	@grp_goal: The goal (group-relative).  It is where the search for a
 		free reservable space should start from.
 		if we have a goal(goal >0 ), then start from there,
 		no goal(goal = -1), we start from the first block
 		of the group.
 	@sb: the super block
 	@group: the group we are trying to allocate in
 	@bitmap_bh: the block group block bitmap
		
		  if the old reservation is cross group boundary
		  and if the goal is inside the old reservation window,
		  we will come here when we just failed to allocate from
		  the first part of the window. We still have another part
		  that belongs to the next group. In this case, there is no
		  point to discard our window and try to allocate a new one
		  in this group(which will fail). we should
		  keep the reservation window, just simply move on.
		 
		  Maybe we could shift the start block of the reservation
		  window to the first block of next group.
			
			  if the previously allocation hit ratio is
			  greater than 12, then we double the size of
			  the reservation window the next time,
			  otherwise we keep the same size window
	
	  shift the search start to the window near the goal block
	
	  find_next_reservable_window() simply finds a reservable window
	  inside the given range(start_block, group_end_block).
	 
	  To make sure the reservation window has a free bit inside it, we
	  need to check the bitmap after we found a reservable window.
	
	  On success, find_next_reservable_window() returns the
	  reservation window where there is a reservable space after it.
	  Before we reserve this reservable space, we need
	  to make sure there is at least a free block inside this region.
	 
	  Search the first free bit on the block bitmap.  Search starts from
	  the start block of the reservable space we just found.
		
		  no free block left on the bitmap, no point
		  to reserve the space. return failed.
 failed 
	
	  check if the first free block is within the
	  free space we just reserved
 success 
	
	  if the first free bit we found is out of the reservable space
	  continue search for next reservable space,
	  start from where the free block is,
	  we also shift the list head to where we stopped last time
  try_to_extend_reservation()
  @my_rsv:		given reservation window
  @sb:			super block
  @size:		the delta to extend
  Attempt to expand the reservation window large enough to have
  required number of free blocks
  Since ext2_try_to_allocate() will always allocate blocks within
  the reservation window range, if the window size is too small,
  multiple blocks allocation has to stop at the end of the reservation
  window. To make this more efficient, given the total number of
  blocks needed and the current size of the window, we try to
  expand the reservation window size if necessary on a best-effort
  basis before ext2_new_blocks() tries to allocate blocks.
  ext2_try_to_allocate_with_rsv()
  @sb:			superblock
  @group:		given allocation block group
  @bitmap_bh:		bufferhead holds the block bitmap
  @grp_goal:		given target block within the group
  @count:		target number of blocks to allocate
  @my_rsv:		reservation window
  This is the main function used to allocate a new block and its reservation
  window.
  Each time when a new block allocation is need, first try to allocate from
  its own reservation.  If it does not have a reservation window, instead of
  looking for a free bit on bitmap first, then look up the reservation list to
  see if it is inside somebody else's reservation window, we try to allocate a
  reservation window for it starting from the goal first. Then do the block
  allocation within the reservation window.
  This will avoid keeping on searching the reservation list again and
  again when somebody is looking for a free block (without
  reservation), and there are lots of free blocks, but they are all
  being reserved.
  We use a red-black tree for the per-filesystem reservation list.
	
	  we don't deal with reservation when
	  filesystem is mounted without reservation
	  or the file is not a regular file
	  or last attempt to allocate a block with reservation turned on failed
	
	  grp_goal is a group relative block number (if there is a goal)
	  0 <= grp_goal < EXT2_BLOCKS_PER_GROUP(sb)
	  first block is a filesystem wide block number
	  first block is the block number of the first block in this group
	
	  Basically we will allocate a new block from inode's reservation
	  window.
	 
	  We need to allocate a new reservation window, if:
	  a) inode does not have a reservation window; or
	  b) last attempt to allocate a block from existing reservation
	     failed; or
	  c) we come here with a goal and with a reservation window
	 
	  We do not need to allocate a new reservation window if we come here
	  at the beginning with a goal and the goal is inside the window, or
	  we don't have a goal but already have a reservation window.
	  then we could go to allocate from the reservation window directly.
 failed 
 succeed 
  ext2_has_free_blocks()
  @sbi:		in-core super block structure.
  Check if filesystem has at least 1 free block available for allocation.
  Returns 1 if the passed-in block region is valid; 0 if some part overlaps
  with filesystem metadata blocks.
 Ensure we do not step over superblock 
  ext2_new_blocks() -- core block(s) allocation function
  @inode:		file inode
  @goal:		given target block(filesystem wide)
  @count:		target number of blocks to allocate
  @errp:		error code
  ext2_new_blocks uses a goal block to assist allocation.  If the goal is
  free, or there is a free block within 32 blocks of the goal, that block
  is allocated.  Otherwise a forward search is made for a free block; within 
  each block group the search first looks for an entire free byte in the block
  bitmap, and then for any free bit if that fails.
  This function also updates quota and i_blocks field.
 blockgroup relative goal block 
 blockgroup-relative allocated block
 filesyetem-wide allocated block 
 blockgroup iteration index 
 number of free blocks in a group 
	
	  Check quota for allocation of this block.
	
	  Allocate a block from reservation only when
	  filesystem is mounted with reservation(default,-o reservation), and
	  it's a regular file, and
	  the desired window size is greater than 0 (One could use ioctl
	  command EXT2_IOC_SETRSVSZ to set the window size to 0 to turn off
	  reservation on that particular file)
	
	  First, test whether the goal block is free.
	
	  if there is not enough free blocks to make a new resevation
	  turn off reservation for this allocation
		
		  In case we retry allocation (due to fs reservation not
		  working out or fs corruption), the bitmap_bh is non-null
		  pointer and we have to release it before calling
		  read_block_bitmap().
	
	  Now search the rest of the groups.  We assume that
	  group_no and gdp correctly point to the last group visited.
		
		  skip this group (and avoid loading bitmap) if there
		  are no free blocks
		
		  skip this group if the number of
		  free blocks is less than half of the reservation
		  window size.
		
		  try to allocate block(s) from this group, without a goal(-1).
	
	  We may end up a bogus earlier ENOSPC error due to
	  filesystem is "full" of reservations, but
	  there maybe indeed free blocks available on disk
	  In this case, we just forget about the reservations
	  just do block allocation as without reservations.
 No space left on the device 
		
		  ext2_try_to_allocate marked the blocks we allocated as in
		  use.  So we may want to selectively mark some of the blocks
		  as free
	
	  Undo the block allocation
  EXT2FS_DEBUG  
 	ext2_bg_has_super - number of blocks used by the superblock in group
 	@sb: superblock for filesystem
 	@group: group number to check
 	Return the number of blocks used by the superblock (primary or backup)
 	in this group.  Currently this will be only 0 or 1.
 	ext2_bg_num_gdb - number of blocks used by the group table in group
 	@sb: superblock for filesystem
 	@group: group number to check
 	Return the number of blocks used by the group descriptor table
 	(primary or backup) in this group.  In the future there may be a
 	different number of descriptor blocks in each group.
 SPDX-License-Identifier: GPL-2.0
   linuxfsext2file.c
  Copyright (C) 1992, 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
   from
   linuxfsminixfile.c
   Copyright (C) 1991, 1992  Linus Torvalds
   ext2 fs regular file handling primitives
   64-bit file support on 64-bit platforms by Jakub Jelinek
  	(jj@sunsite.ms.mff.cuni.cz)
 skip atime 
  The lock ordering for ext2 DAX fault paths is:
  mmap_lock (MM)
    sb_start_pagefault (vfs, freeze)
      address_space->invalidate_lock
        address_space->i_mmap_rwsem or page_lock (mutually exclusive in DAX)
          ext2_inode_info->truncate_mutex
  The default page_lock and i_size verification done by non-DAX fault paths
  is sufficient because ext2 doesn't support hole punching.
	
	  .huge_fault is not supported for DAX because allocation in ext2
	  cannot be reliably aligned to huge page sizes and so pmd faults
	  will always fail and fail back to regular faults.
  Called when filp is released. This happens when all file descriptors
  for a single struct file are closed. Note that different open() calls
  for the same file yield different struct file structures.
 We don't really know where the IO error happened... 
 SPDX-License-Identifier: GPL-2.0
   linuxfsext2symlink.c
  Only fast symlinks left here - the rest is done by generic code. AV, 1999
  Copyright (C) 1992, 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
   from
   linuxfsminixsymlink.c
   Copyright (C) 1991, 1992  Linus Torvalds
   ext2 symlink handling code
 SPDX-License-Identifier: GPL-2.0
  linuxfsext2xattr_trusted.c
  Handler for trusted extended attributes.
  Copyright (C) 2003 by Andreas Gruenbacher, <a.gruenbacher@computer.org>
 SPDX-License-Identifier: GPL-2.0
  linuxfsext2xattr_security.c
  Handler for storing security labels as extended attributes.
 SPDX-License-Identifier: GPL-2.0
  linuxfshfsplusxattr_user.c
  Vyacheslav Dubeyko <slava@dubeyko.com>
  Handler for user extended attributes.
 SPDX-License-Identifier: GPL-2.0
   linuxfshfsplusioctl.c
  Copyright (C) 2003
  Ethan Benson <erbenson@alaska.net>
  partially derived from linuxfsext2ioctl.c
  Copyright (C) 1993, 1994, 1995
  Remy Card (card@masi.ibp.fr)
  Laboratoire MASI - Institut Blaise Pascal
  Universite Pierre et Marie Curie (Paris VI)
  hfsplus ioctls
  "Blessing" an HFS+ filesystem writes metadata to the superblock informing
  the platform firmware which file to boot from
 Directory containing the bootable system 
	
	  Bootloader. Just using the inode here breaks in the case of
	  hard links - the firmware wants the ID of the hard link file,
	  but the inode points at the indirect inode
 Per spec, the OS X system folder - same as finder_info[0] here 
 SPDX-License-Identifier: GPL-2.0
   linuxfshfspluswrapper.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
  Handling of HFS wrappers around HFS+ volumes
  hfsplus_submit_bio - Perform block IO
  @sb: super block of volume for IO
  @sector: block to read or write, for blocks of HFSPLUS_SECTOR_SIZE bytes
  @buf: buffer for IO
  @data: output pointer for location of requested data
  @op: direction of IO
  @op_flags: request op flags
  The unit of IO is hfsplus_min_io_size(sb), which may be bigger than
  HFSPLUS_SECTOR_SIZE, and @buf must be sized accordingly. On reads
  @data will return a pointer to the start of the requested sector,
  which may not be the same location as @buf.
  If @sector is not aligned to the bdev logical block size it will
  be rounded down. For writes this means that @buf should contain data
  that starts at the rounded-down address. As long as the data was
  read using hfsplus_submit_bio() and the same buffer is used things
  will work correctly.
	
	  Align sector to hardware sector size and find offset. We
	  assume that io_size is a power of two, which _should_
	  be true.
 default values 
 Find the volume header and fill in some minimum bits in superblock 
 Takes in super block, returns true if good data read 
		
		  Check for a partition block.
		 
		  (should do this only for cdromloop though)
	
	  Block size must be at least as large as a sector and a multiple of 2.
	
	  Align block size to block offset.
 SPDX-License-Identifier: GPL-2.0-only
   linuxfshfsplussuper.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
	
	  Explicitly write out the special metadata inodes.
	 
	  While these special inodes are marked as hashed and written
	  out peridocically by the flusher threads we redirty them
	  during writeout of normal inodes, and thus the life lock
	  prevents us from getting the latest state to disk.
 nothing 
 temporarily use utf8 to correctly find the hidden dir below 
 Grab the volume header 
 Copy parts of the volume header into the superblock 
 Set up operations so we can load metadata 
 nothing 
 Load metadata objects (BTrees) 
 Load the root directory 
		
		  H+LX == hfsplusutils, H+Lx == this driver, H+lx is unused
		  all three are registered with Apple for our use
 Operation is not supported. 
				
				  Try to delete anyway without
				  error analysis.
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 SPDX-License-Identifier: GPL-2.0
  linuxfshfsplusxattr.c
  Vyacheslav Dubeyko <slava@dubeyko.com>
  Logic of processing extended attributes
 The end of the node contains list of record offsets 
		
		  This state means that another thread is in process
		  of AttributesFile creation. Theoretically, it is
		  possible to be here. But really __setxattr() method
		  first of all calls hfs_find_init() for lookup in
		  B-tree of CatalogFile. This method locks mutex of
		  CatalogFile's B-tree. As a result, if some thread
		  is inside AttributedFile creation operation then
		  another threads will be waiting unlocking of
		  CatalogFile's B-tree's mutex. However, if code will
		  change then we will return error code (-EAGAIN) from
		  here. Really, it means that first try to set of xattr
		  fails with error but second attempt will have success.
	
	  Don't allow retrieving properly prefixed attributes
	  by prepending them with "osx."
	
	  osx is the namespace we use to indicate an unprefixed
	  attribute on the filesystem (like the ones that OS X
	  creates), so we pass the name through unmodified (after
	  ensuring it doesn't conflict with another namespace).
	
	  Don't allow setting properly prefixed attributes
	  by prepending them with "osx."
	
	  osx is the namespace we use to indicate an unprefixed
	  attribute on the filesystem (like the ones that OS X
	  creates), so we pass the name through unmodified (after
	  ensuring it doesn't conflict with another namespace).
 SPDX-License-Identifier: GPL-2.0
  linuxfshfsplustables.c
  Various data tables
   Unicode case folding table taken from Apple Technote #1150
   (HFS Plus Volume Format)
   The lower case table consists of a 256-entry high-byte table followed by
   some number of 256-entry subtables. The high-byte table contains either an
   offset to the subtable for characters with that high byte or zero, which
   means that there are no case mappings or ignored characters in that block.
   Ignored characters are mapped to zero.
 High-byte indices ( == 0 iff no case mapping and no ignorables )
 0  0x0100, 0x0200, 0x0000, 0x0300, 0x0400, 0x0500, 0x0000, 0x0000,
 1  0x0600, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 2  0x0700, 0x0800, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 3  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 4  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 5  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 6  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 7  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 8  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 9  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 A  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 B  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 C  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 D  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 E  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 F  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
 Table 1 (for high byte 0x00)
 0  0xFFFF, 0x0001, 0x0002, 0x0003, 0x0004, 0x0005, 0x0006, 0x0007,
 1  0x0010, 0x0011, 0x0012, 0x0013, 0x0014, 0x0015, 0x0016, 0x0017,
 2  0x0020, 0x0021, 0x0022, 0x0023, 0x0024, 0x0025, 0x0026, 0x0027,
 3  0x0030, 0x0031, 0x0032, 0x0033, 0x0034, 0x0035, 0x0036, 0x0037,
 4  0x0040, 0x0061, 0x0062, 0x0063, 0x0064, 0x0065, 0x0066, 0x0067,
 5  0x0070, 0x0071, 0x0072, 0x0073, 0x0074, 0x0075, 0x0076, 0x0077,
 6  0x0060, 0x0061, 0x0062, 0x0063, 0x0064, 0x0065, 0x0066, 0x0067,
 7  0x0070, 0x0071, 0x0072, 0x0073, 0x0074, 0x0075, 0x0076, 0x0077,
 8  0x0080, 0x0081, 0x0082, 0x0083, 0x0084, 0x0085, 0x0086, 0x0087,
 9  0x0090, 0x0091, 0x0092, 0x0093, 0x0094, 0x0095, 0x0096, 0x0097,
 A  0x00A0, 0x00A1, 0x00A2, 0x00A3, 0x00A4, 0x00A5, 0x00A6, 0x00A7,
 B  0x00B0, 0x00B1, 0x00B2, 0x00B3, 0x00B4, 0x00B5, 0x00B6, 0x00B7,
 C  0x00C0, 0x00C1, 0x00C2, 0x00C3, 0x00C4, 0x00C5, 0x00E6, 0x00C7,
 D  0x00F0, 0x00D1, 0x00D2, 0x00D3, 0x00D4, 0x00D5, 0x00D6, 0x00D7,
 E  0x00E0, 0x00E1, 0x00E2, 0x00E3, 0x00E4, 0x00E5, 0x00E6, 0x00E7,
 F  0x00F0, 0x00F1, 0x00F2, 0x00F3, 0x00F4, 0x00F5, 0x00F6, 0x00F7,
 Table 2 (for high byte 0x01)
 0  0x0100, 0x0101, 0x0102, 0x0103, 0x0104, 0x0105, 0x0106, 0x0107,
 1  0x0111, 0x0111, 0x0112, 0x0113, 0x0114, 0x0115, 0x0116, 0x0117,
 2  0x0120, 0x0121, 0x0122, 0x0123, 0x0124, 0x0125, 0x0127, 0x0127,
 3  0x0130, 0x0131, 0x0133, 0x0133, 0x0134, 0x0135, 0x0136, 0x0137,
 4  0x0140, 0x0142, 0x0142, 0x0143, 0x0144, 0x0145, 0x0146, 0x0147,
 5  0x0150, 0x0151, 0x0153, 0x0153, 0x0154, 0x0155, 0x0156, 0x0157,
 6  0x0160, 0x0161, 0x0162, 0x0163, 0x0164, 0x0165, 0x0167, 0x0167,
 7  0x0170, 0x0171, 0x0172, 0x0173, 0x0174, 0x0175, 0x0176, 0x0177,
 8  0x0180, 0x0253, 0x0183, 0x0183, 0x0185, 0x0185, 0x0254, 0x0188,
 9  0x025B, 0x0192, 0x0192, 0x0260, 0x0263, 0x0195, 0x0269, 0x0268,
 A  0x01A0, 0x01A1, 0x01A3, 0x01A3, 0x01A5, 0x01A5, 0x01A6, 0x01A8,
 B  0x01B0, 0x028A, 0x028B, 0x01B4, 0x01B4, 0x01B6, 0x01B6, 0x0292,
 C  0x01C0, 0x01C1, 0x01C2, 0x01C3, 0x01C6, 0x01C6, 0x01C6, 0x01C9,
 D  0x01D0, 0x01D1, 0x01D2, 0x01D3, 0x01D4, 0x01D5, 0x01D6, 0x01D7,
 E  0x01E0, 0x01E1, 0x01E2, 0x01E3, 0x01E5, 0x01E5, 0x01E6, 0x01E7,
 F  0x01F0, 0x01F3, 0x01F3, 0x01F3, 0x01F4, 0x01F5, 0x01F6, 0x01F7,
 Table 3 (for high byte 0x03)
 0  0x0300, 0x0301, 0x0302, 0x0303, 0x0304, 0x0305, 0x0306, 0x0307,
 1  0x0310, 0x0311, 0x0312, 0x0313, 0x0314, 0x0315, 0x0316, 0x0317,
 2  0x0320, 0x0321, 0x0322, 0x0323, 0x0324, 0x0325, 0x0326, 0x0327,
 3  0x0330, 0x0331, 0x0332, 0x0333, 0x0334, 0x0335, 0x0336, 0x0337,
 4  0x0340, 0x0341, 0x0342, 0x0343, 0x0344, 0x0345, 0x0346, 0x0347,
 5  0x0350, 0x0351, 0x0352, 0x0353, 0x0354, 0x0355, 0x0356, 0x0357,
 6  0x0360, 0x0361, 0x0362, 0x0363, 0x0364, 0x0365, 0x0366, 0x0367,
 7  0x0370, 0x0371, 0x0372, 0x0373, 0x0374, 0x0375, 0x0376, 0x0377,
 8  0x0380, 0x0381, 0x0382, 0x0383, 0x0384, 0x0385, 0x0386, 0x0387,
 9  0x0390, 0x03B1, 0x03B2, 0x03B3, 0x03B4, 0x03B5, 0x03B6, 0x03B7,
 A  0x03C0, 0x03C1, 0x03A2, 0x03C3, 0x03C4, 0x03C5, 0x03C6, 0x03C7,
 B  0x03B0, 0x03B1, 0x03B2, 0x03B3, 0x03B4, 0x03B5, 0x03B6, 0x03B7,
 C  0x03C0, 0x03C1, 0x03C2, 0x03C3, 0x03C4, 0x03C5, 0x03C6, 0x03C7,
 D  0x03D0, 0x03D1, 0x03D2, 0x03D3, 0x03D4, 0x03D5, 0x03D6, 0x03D7,
 E  0x03E0, 0x03E1, 0x03E3, 0x03E3, 0x03E5, 0x03E5, 0x03E7, 0x03E7,
 F  0x03F0, 0x03F1, 0x03F2, 0x03F3, 0x03F4, 0x03F5, 0x03F6, 0x03F7,
 Table 4 (for high byte 0x04)
 0  0x0400, 0x0401, 0x0452, 0x0403, 0x0454, 0x0455, 0x0456, 0x0407,
 1  0x0430, 0x0431, 0x0432, 0x0433, 0x0434, 0x0435, 0x0436, 0x0437,
 2  0x0440, 0x0441, 0x0442, 0x0443, 0x0444, 0x0445, 0x0446, 0x0447,
 3  0x0430, 0x0431, 0x0432, 0x0433, 0x0434, 0x0435, 0x0436, 0x0437,
 4  0x0440, 0x0441, 0x0442, 0x0443, 0x0444, 0x0445, 0x0446, 0x0447,
 5  0x0450, 0x0451, 0x0452, 0x0453, 0x0454, 0x0455, 0x0456, 0x0457,
 6  0x0461, 0x0461, 0x0463, 0x0463, 0x0465, 0x0465, 0x0467, 0x0467,
 7  0x0471, 0x0471, 0x0473, 0x0473, 0x0475, 0x0475, 0x0476, 0x0477,
 8  0x0481, 0x0481, 0x0482, 0x0483, 0x0484, 0x0485, 0x0486, 0x0487,
 9  0x0491, 0x0491, 0x0493, 0x0493, 0x0495, 0x0495, 0x0497, 0x0497,
 A  0x04A1, 0x04A1, 0x04A3, 0x04A3, 0x04A5, 0x04A5, 0x04A7, 0x04A7,
 B  0x04B1, 0x04B1, 0x04B3, 0x04B3, 0x04B5, 0x04B5, 0x04B7, 0x04B7,
 C  0x04C0, 0x04C1, 0x04C2, 0x04C4, 0x04C4, 0x04C5, 0x04C6, 0x04C8,
 D  0x04D0, 0x04D1, 0x04D2, 0x04D3, 0x04D4, 0x04D5, 0x04D6, 0x04D7,
 E  0x04E0, 0x04E1, 0x04E2, 0x04E3, 0x04E4, 0x04E5, 0x04E6, 0x04E7,
 F  0x04F0, 0x04F1, 0x04F2, 0x04F3, 0x04F4, 0x04F5, 0x04F6, 0x04F7,
 Table 5 (for high byte 0x05)
 0  0x0500, 0x0501, 0x0502, 0x0503, 0x0504, 0x0505, 0x0506, 0x0507,
 1  0x0510, 0x0511, 0x0512, 0x0513, 0x0514, 0x0515, 0x0516, 0x0517,
 2  0x0520, 0x0521, 0x0522, 0x0523, 0x0524, 0x0525, 0x0526, 0x0527,
 3  0x0530, 0x0561, 0x0562, 0x0563, 0x0564, 0x0565, 0x0566, 0x0567,
 4  0x0570, 0x0571, 0x0572, 0x0573, 0x0574, 0x0575, 0x0576, 0x0577,
 5  0x0580, 0x0581, 0x0582, 0x0583, 0x0584, 0x0585, 0x0586, 0x0557,
 6  0x0560, 0x0561, 0x0562, 0x0563, 0x0564, 0x0565, 0x0566, 0x0567,
 7  0x0570, 0x0571, 0x0572, 0x0573, 0x0574, 0x0575, 0x0576, 0x0577,
 8  0x0580, 0x0581, 0x0582, 0x0583, 0x0584, 0x0585, 0x0586, 0x0587,
 9  0x0590, 0x0591, 0x0592, 0x0593, 0x0594, 0x0595, 0x0596, 0x0597,
 A  0x05A0, 0x05A1, 0x05A2, 0x05A3, 0x05A4, 0x05A5, 0x05A6, 0x05A7,
 B  0x05B0, 0x05B1, 0x05B2, 0x05B3, 0x05B4, 0x05B5, 0x05B6, 0x05B7,
 C  0x05C0, 0x05C1, 0x05C2, 0x05C3, 0x05C4, 0x05C5, 0x05C6, 0x05C7,
 D  0x05D0, 0x05D1, 0x05D2, 0x05D3, 0x05D4, 0x05D5, 0x05D6, 0x05D7,
 E  0x05E0, 0x05E1, 0x05E2, 0x05E3, 0x05E4, 0x05E5, 0x05E6, 0x05E7,
 F  0x05F0, 0x05F1, 0x05F2, 0x05F3, 0x05F4, 0x05F5, 0x05F6, 0x05F7,
 Table 6 (for high byte 0x10)
 0  0x1000, 0x1001, 0x1002, 0x1003, 0x1004, 0x1005, 0x1006, 0x1007,
 1  0x1010, 0x1011, 0x1012, 0x1013, 0x1014, 0x1015, 0x1016, 0x1017,
 2  0x1020, 0x1021, 0x1022, 0x1023, 0x1024, 0x1025, 0x1026, 0x1027,
 3  0x1030, 0x1031, 0x1032, 0x1033, 0x1034, 0x1035, 0x1036, 0x1037,
 4  0x1040, 0x1041, 0x1042, 0x1043, 0x1044, 0x1045, 0x1046, 0x1047,
 5  0x1050, 0x1051, 0x1052, 0x1053, 0x1054, 0x1055, 0x1056, 0x1057,
 6  0x1060, 0x1061, 0x1062, 0x1063, 0x1064, 0x1065, 0x1066, 0x1067,
 7  0x1070, 0x1071, 0x1072, 0x1073, 0x1074, 0x1075, 0x1076, 0x1077,
 8  0x1080, 0x1081, 0x1082, 0x1083, 0x1084, 0x1085, 0x1086, 0x1087,
 9  0x1090, 0x1091, 0x1092, 0x1093, 0x1094, 0x1095, 0x1096, 0x1097,
 A  0x10D0, 0x10D1, 0x10D2, 0x10D3, 0x10D4, 0x10D5, 0x10D6, 0x10D7,
 B  0x10E0, 0x10E1, 0x10E2, 0x10E3, 0x10E4, 0x10E5, 0x10E6, 0x10E7,
 C  0x10F0, 0x10F1, 0x10F2, 0x10F3, 0x10F4, 0x10F5, 0x10C6, 0x10C7,
 D  0x10D0, 0x10D1, 0x10D2, 0x10D3, 0x10D4, 0x10D5, 0x10D6, 0x10D7,
 E  0x10E0, 0x10E1, 0x10E2, 0x10E3, 0x10E4, 0x10E5, 0x10E6, 0x10E7,
 F  0x10F0, 0x10F1, 0x10F2, 0x10F3, 0x10F4, 0x10F5, 0x10F6, 0x10F7,
 Table 7 (for high byte 0x20)
 0  0x2000, 0x2001, 0x2002, 0x2003, 0x2004, 0x2005, 0x2006, 0x2007,
 1  0x2010, 0x2011, 0x2012, 0x2013, 0x2014, 0x2015, 0x2016, 0x2017,
 2  0x2020, 0x2021, 0x2022, 0x2023, 0x2024, 0x2025, 0x2026, 0x2027,
 3  0x2030, 0x2031, 0x2032, 0x2033, 0x2034, 0x2035, 0x2036, 0x2037,
 4  0x2040, 0x2041, 0x2042, 0x2043, 0x2044, 0x2045, 0x2046, 0x2047,
 5  0x2050, 0x2051, 0x2052, 0x2053, 0x2054, 0x2055, 0x2056, 0x2057,
 6  0x2060, 0x2061, 0x2062, 0x2063, 0x2064, 0x2065, 0x2066, 0x2067,
 7  0x2070, 0x2071, 0x2072, 0x2073, 0x2074, 0x2075, 0x2076, 0x2077,
 8  0x2080, 0x2081, 0x2082, 0x2083, 0x2084, 0x2085, 0x2086, 0x2087,
 9  0x2090, 0x2091, 0x2092, 0x2093, 0x2094, 0x2095, 0x2096, 0x2097,
 A  0x20A0, 0x20A1, 0x20A2, 0x20A3, 0x20A4, 0x20A5, 0x20A6, 0x20A7,
 B  0x20B0, 0x20B1, 0x20B2, 0x20B3, 0x20B4, 0x20B5, 0x20B6, 0x20B7,
 C  0x20C0, 0x20C1, 0x20C2, 0x20C3, 0x20C4, 0x20C5, 0x20C6, 0x20C7,
 D  0x20D0, 0x20D1, 0x20D2, 0x20D3, 0x20D4, 0x20D5, 0x20D6, 0x20D7,
 E  0x20E0, 0x20E1, 0x20E2, 0x20E3, 0x20E4, 0x20E5, 0x20E6, 0x20E7,
 F  0x20F0, 0x20F1, 0x20F2, 0x20F3, 0x20F4, 0x20F5, 0x20F6, 0x20F7,
 Table 8 (for high byte 0x21)
 0  0x2100, 0x2101, 0x2102, 0x2103, 0x2104, 0x2105, 0x2106, 0x2107,
 1  0x2110, 0x2111, 0x2112, 0x2113, 0x2114, 0x2115, 0x2116, 0x2117,
 2  0x2120, 0x2121, 0x2122, 0x2123, 0x2124, 0x2125, 0x2126, 0x2127,
 3  0x2130, 0x2131, 0x2132, 0x2133, 0x2134, 0x2135, 0x2136, 0x2137,
 4  0x2140, 0x2141, 0x2142, 0x2143, 0x2144, 0x2145, 0x2146, 0x2147,
 5  0x2150, 0x2151, 0x2152, 0x2153, 0x2154, 0x2155, 0x2156, 0x2157,
 6  0x2170, 0x2171, 0x2172, 0x2173, 0x2174, 0x2175, 0x2176, 0x2177,
 7  0x2170, 0x2171, 0x2172, 0x2173, 0x2174, 0x2175, 0x2176, 0x2177,
 8  0x2180, 0x2181, 0x2182, 0x2183, 0x2184, 0x2185, 0x2186, 0x2187,
 9  0x2190, 0x2191, 0x2192, 0x2193, 0x2194, 0x2195, 0x2196, 0x2197,
 A  0x21A0, 0x21A1, 0x21A2, 0x21A3, 0x21A4, 0x21A5, 0x21A6, 0x21A7,
 B  0x21B0, 0x21B1, 0x21B2, 0x21B3, 0x21B4, 0x21B5, 0x21B6, 0x21B7,
 C  0x21C0, 0x21C1, 0x21C2, 0x21C3, 0x21C4, 0x21C5, 0x21C6, 0x21C7,
 D  0x21D0, 0x21D1, 0x21D2, 0x21D3, 0x21D4, 0x21D5, 0x21D6, 0x21D7,
 E  0x21E0, 0x21E1, 0x21E2, 0x21E3, 0x21E4, 0x21E5, 0x21E6, 0x21E7,
 F  0x21F0, 0x21F1, 0x21F2, 0x21F3, 0x21F4, 0x21F5, 0x21F6, 0x21F7,
 Table 9 (for high byte 0xFE)
 0  0xFE00, 0xFE01, 0xFE02, 0xFE03, 0xFE04, 0xFE05, 0xFE06, 0xFE07,
 1  0xFE10, 0xFE11, 0xFE12, 0xFE13, 0xFE14, 0xFE15, 0xFE16, 0xFE17,
 2  0xFE20, 0xFE21, 0xFE22, 0xFE23, 0xFE24, 0xFE25, 0xFE26, 0xFE27,
 3  0xFE30, 0xFE31, 0xFE32, 0xFE33, 0xFE34, 0xFE35, 0xFE36, 0xFE37,
 4  0xFE40, 0xFE41, 0xFE42, 0xFE43, 0xFE44, 0xFE45, 0xFE46, 0xFE47,
 5  0xFE50, 0xFE51, 0xFE52, 0xFE53, 0xFE54, 0xFE55, 0xFE56, 0xFE57,
 6  0xFE60, 0xFE61, 0xFE62, 0xFE63, 0xFE64, 0xFE65, 0xFE66, 0xFE67,
 7  0xFE70, 0xFE71, 0xFE72, 0xFE73, 0xFE74, 0xFE75, 0xFE76, 0xFE77,
 8  0xFE80, 0xFE81, 0xFE82, 0xFE83, 0xFE84, 0xFE85, 0xFE86, 0xFE87,
 9  0xFE90, 0xFE91, 0xFE92, 0xFE93, 0xFE94, 0xFE95, 0xFE96, 0xFE97,
 A  0xFEA0, 0xFEA1, 0xFEA2, 0xFEA3, 0xFEA4, 0xFEA5, 0xFEA6, 0xFEA7,
 B  0xFEB0, 0xFEB1, 0xFEB2, 0xFEB3, 0xFEB4, 0xFEB5, 0xFEB6, 0xFEB7,
 C  0xFEC0, 0xFEC1, 0xFEC2, 0xFEC3, 0xFEC4, 0xFEC5, 0xFEC6, 0xFEC7,
 D  0xFED0, 0xFED1, 0xFED2, 0xFED3, 0xFED4, 0xFED5, 0xFED6, 0xFED7,
 E  0xFEE0, 0xFEE1, 0xFEE2, 0xFEE3, 0xFEE4, 0xFEE5, 0xFEE6, 0xFEE7,
 F  0xFEF0, 0xFEF1, 0xFEF2, 0xFEF3, 0xFEF4, 0xFEF5, 0xFEF6, 0xFEF7,
 Table 10 (for high byte 0xFF)
 0  0xFF00, 0xFF01, 0xFF02, 0xFF03, 0xFF04, 0xFF05, 0xFF06, 0xFF07,
 1  0xFF10, 0xFF11, 0xFF12, 0xFF13, 0xFF14, 0xFF15, 0xFF16, 0xFF17,
 2  0xFF20, 0xFF41, 0xFF42, 0xFF43, 0xFF44, 0xFF45, 0xFF46, 0xFF47,
 3  0xFF50, 0xFF51, 0xFF52, 0xFF53, 0xFF54, 0xFF55, 0xFF56, 0xFF57,
 4  0xFF40, 0xFF41, 0xFF42, 0xFF43, 0xFF44, 0xFF45, 0xFF46, 0xFF47,
 5  0xFF50, 0xFF51, 0xFF52, 0xFF53, 0xFF54, 0xFF55, 0xFF56, 0xFF57,
 6  0xFF60, 0xFF61, 0xFF62, 0xFF63, 0xFF64, 0xFF65, 0xFF66, 0xFF67,
 7  0xFF70, 0xFF71, 0xFF72, 0xFF73, 0xFF74, 0xFF75, 0xFF76, 0xFF77,
 8  0xFF80, 0xFF81, 0xFF82, 0xFF83, 0xFF84, 0xFF85, 0xFF86, 0xFF87,
 9  0xFF90, 0xFF91, 0xFF92, 0xFF93, 0xFF94, 0xFF95, 0xFF96, 0xFF97,
 A  0xFFA0, 0xFFA1, 0xFFA2, 0xFFA3, 0xFFA4, 0xFFA5, 0xFFA6, 0xFFA7,
 B  0xFFB0, 0xFFB1, 0xFFB2, 0xFFB3, 0xFFB4, 0xFFB5, 0xFFB6, 0xFFB7,
 C  0xFFC0, 0xFFC1, 0xFFC2, 0xFFC3, 0xFFC4, 0xFFC5, 0xFFC6, 0xFFC7,
 D  0xFFD0, 0xFFD1, 0xFFD2, 0xFFD3, 0xFFD4, 0xFFD5, 0xFFD6, 0xFFD7,
 E  0xFFE0, 0xFFE1, 0xFFE2, 0xFFE3, 0xFFE4, 0xFFE5, 0xFFE6, 0xFFE7,
 F  0xFFF0, 0xFFF1, 0xFFF2, 0xFFF3, 0xFFF4, 0xFFF5, 0xFFF6, 0xFFF7,
 base table 
 char table 0x0___ 
 char table 0x00__ 
 char values 0x00c_ 
 char values 0x00d_ 
 char values 0x00e_ 
 char values 0x00f_ 
 char table 0x01__ 
 char values 0x010_ 
 char values 0x011_ 
 char values 0x012_ 
 char values 0x013_ 
 char values 0x014_ 
 char values 0x015_ 
 char values 0x016_ 
 char values 0x017_ 
 char values 0x01a_ 
 char values 0x01b_ 
 char values 0x01c_ 
 char values 0x01d_ 
 char values 0x01e_ 
 char values 0x01f_ 
 char table 0x02__ 
 char values 0x020_ 
 char values 0x021_ 
 char table 0x03__ 
 char values 0x031_ 
 char values 0x034_ 
 char values 0x037_ 
 char values 0x038_ 
 char values 0x039_ 
 char values 0x03a_ 
 char values 0x03b_ 
 char values 0x03c_ 
 char values 0x03d_ 
 char table 0x04__ 
 char values 0x040_ 
 char values 0x041_ 
 char values 0x043_ 
 char values 0x045_ 
 char values 0x047_ 
 char values 0x04c_ 
 char values 0x04d_ 
 char values 0x04e_ 
 char values 0x04f_ 
 char table 0x09__ 
 char values 0x092_ 
 char values 0x093_ 
 char values 0x095_ 
 char values 0x09b_ 
 char values 0x09c_ 
 char values 0x09d_ 
 char table 0x0a__ 
 char values 0x0a5_ 
 char table 0x0b__ 
 char values 0x0b4_ 
 char values 0x0b5_ 
 char values 0x0b9_ 
 char values 0x0bc_ 
 char table 0x0c__ 
 char values 0x0c4_ 
 char values 0x0cc_ 
 char table 0x0d__ 
 char values 0x0d4_ 
 char table 0x0e__ 
 char values 0x0e3_ 
 char values 0x0eb_ 
 char table 0x0f__ 
 char values 0x0f4_ 
 char values 0x0f5_ 
 char values 0x0f6_ 
 char values 0x0f7_ 
 char values 0x0f8_ 
 char values 0x0f9_ 
 char values 0x0fa_ 
 char values 0x0fb_ 
 char table 0x1___ 
 char table 0x1e__ 
 char values 0x1e0_ 
 char values 0x1e1_ 
 char values 0x1e2_ 
 char values 0x1e3_ 
 char values 0x1e4_ 
 char values 0x1e5_ 
 char values 0x1e6_ 
 char values 0x1e7_ 
 char values 0x1e8_ 
 char values 0x1e9_ 
 char values 0x1ea_ 
 char values 0x1eb_ 
 char values 0x1ec_ 
 char values 0x1ed_ 
 char values 0x1ee_ 
 char values 0x1ef_ 
 char table 0x1f__ 
 char values 0x1f0_ 
 char values 0x1f1_ 
 char values 0x1f2_ 
 char values 0x1f3_ 
 char values 0x1f4_ 
 char values 0x1f5_ 
 char values 0x1f6_ 
 char values 0x1f7_ 
 char values 0x1f8_ 
 char values 0x1f9_ 
 char values 0x1fa_ 
 char values 0x1fb_ 
 char values 0x1fc_ 
 char values 0x1fd_ 
 char values 0x1fe_ 
 char values 0x1ff_ 
 char table 0x3___ 
 char table 0x30__ 
 char values 0x304_ 
 char values 0x305_ 
 char values 0x306_ 
 char values 0x307_ 
 char values 0x309_ 
 char values 0x30a_ 
 char values 0x30b_ 
 char values 0x30c_ 
 char values 0x30d_ 
 char values 0x30f_ 
 char table 0xf___ 
 char table 0xfb__ 
 char values 0xfb1_ 
 char values 0xfb2_ 
 char values 0xfb3_ 
 char values 0xfb4_ 
 decomposed characters 
 base 
 hangul marker 
 0x0300 
 0x0301 
 0x0302 
 0x0303 
 0x0304 
 0x0306 
 0x0307 
 0x0308 
 0x0309 
 0x030a 
 0x030b 
 0x030c 
 0x030d 
 0x030f 
 0x0311 
 0x0313 
 0x0314 
 0x031b 
 0x0323 
 0x0324 
 0x0325 
 0x0327 
 0x0328 
 0x032d 
 0x032e 
 0x0330 
 0x0331 
 0x0342 
 0x0345 
 0x05b7 
 0x05b8 
 0x05b9 
 0x05bc 
 0x05bf 
 0x05c1 
 0x05c2 
 0x093c 
 0x09bc 
 0x09be 
 0x09d7 
 0x0a3c 
 0x0b3c 
 0x0b3e 
 0x0b56 
 0x0b57 
 0x0bbe 
 0x0bd7 
 0x0c56 
 0x0cc2 
 0x0cd5 
 0x0cd6 
 0x0d3e 
 0x0d57 
 0x0e32 
 0x0eb2 
 0x0f71 
 0x0f80 
 0x0fb5 
 0x0fb7 
 0x3099 
 0x309a 
 0x0041 0x0300 
 0x0045 0x0300 
 0x0049 0x0300 
 0x004f 0x0300 
 0x0055 0x0300 
 0x0057 0x0300 
 0x0059 0x0300 
 0x0061 0x0300 
 0x0065 0x0300 
 0x0069 0x0300 
 0x006f 0x0300 
 0x0075 0x0300 
 0x0077 0x0300 
 0x0079 0x0300 
 0x00a8 0x0300 
 0x0391 0x0300 
 0x0395 0x0300 
 0x0397 0x0300 
 0x0399 0x0300 
 0x039f 0x0300 
 0x03a5 0x0300 
 0x03a9 0x0300 
 0x03b1 0x0300 
 0x03b5 0x0300 
 0x03b7 0x0300 
 0x03b9 0x0300 
 0x03bf 0x0300 
 0x03c5 0x0300 
 0x03c9 0x0300 
 0x1fbf 0x0300 
 0x1ffe 0x0300 
 0x0041 0x0301 
 0x0043 0x0301 
 0x0045 0x0301 
 0x0047 0x0301 
 0x0049 0x0301 
 0x004b 0x0301 
 0x004c 0x0301 
 0x004d 0x0301 
 0x004e 0x0301 
 0x004f 0x0301 
 0x0050 0x0301 
 0x0052 0x0301 
 0x0053 0x0301 
 0x0055 0x0301 
 0x0057 0x0301 
 0x0059 0x0301 
 0x005a 0x0301 
 0x0061 0x0301 
 0x0063 0x0301 
 0x0065 0x0301 
 0x0067 0x0301 
 0x0069 0x0301 
 0x006b 0x0301 
 0x006c 0x0301 
 0x006d 0x0301 
 0x006e 0x0301 
 0x006f 0x0301 
 0x0070 0x0301 
 0x0072 0x0301 
 0x0073 0x0301 
 0x0075 0x0301 
 0x0077 0x0301 
 0x0079 0x0301 
 0x007a 0x0301 
 0x00a8 0x0301 
 0x00c6 0x0301 
 0x00d8 0x0301 
 0x00e6 0x0301 
 0x00f8 0x0301 
 0x0391 0x0301 
 0x0395 0x0301 
 0x0397 0x0301 
 0x0399 0x0301 
 0x039f 0x0301 
 0x03a5 0x0301 
 0x03a9 0x0301 
 0x03b1 0x0301 
 0x03b5 0x0301 
 0x03b7 0x0301 
 0x03b9 0x0301 
 0x03bf 0x0301 
 0x03c5 0x0301 
 0x03c9 0x0301 
 0x0413 0x0301 
 0x041a 0x0301 
 0x0433 0x0301 
 0x043a 0x0301 
 0x1fbf 0x0301 
 0x1ffe 0x0301 
 0x0041 0x0302 
 0x0043 0x0302 
 0x0045 0x0302 
 0x0047 0x0302 
 0x0048 0x0302 
 0x0049 0x0302 
 0x004a 0x0302 
 0x004f 0x0302 
 0x0053 0x0302 
 0x0055 0x0302 
 0x0057 0x0302 
 0x0059 0x0302 
 0x005a 0x0302 
 0x0061 0x0302 
 0x0063 0x0302 
 0x0065 0x0302 
 0x0067 0x0302 
 0x0068 0x0302 
 0x0069 0x0302 
 0x006a 0x0302 
 0x006f 0x0302 
 0x0073 0x0302 
 0x0075 0x0302 
 0x0077 0x0302 
 0x0079 0x0302 
 0x007a 0x0302 
 0x0041 0x0303 
 0x0045 0x0303 
 0x0049 0x0303 
 0x004e 0x0303 
 0x004f 0x0303 
 0x0055 0x0303 
 0x0056 0x0303 
 0x0059 0x0303 
 0x0061 0x0303 
 0x0065 0x0303 
 0x0069 0x0303 
 0x006e 0x0303 
 0x006f 0x0303 
 0x0075 0x0303 
 0x0076 0x0303 
 0x0079 0x0303 
 0x0041 0x0304 
 0x0045 0x0304 
 0x0047 0x0304 
 0x0049 0x0304 
 0x004f 0x0304 
 0x0055 0x0304 
 0x0061 0x0304 
 0x0065 0x0304 
 0x0067 0x0304 
 0x0069 0x0304 
 0x006f 0x0304 
 0x0075 0x0304 
 0x00c6 0x0304 
 0x00e6 0x0304 
 0x0391 0x0304 
 0x0399 0x0304 
 0x03a5 0x0304 
 0x03b1 0x0304 
 0x03b9 0x0304 
 0x03c5 0x0304 
 0x0418 0x0304 
 0x0423 0x0304 
 0x0438 0x0304 
 0x0443 0x0304 
 0x0041 0x0306 
 0x0045 0x0306 
 0x0047 0x0306 
 0x0049 0x0306 
 0x004f 0x0306 
 0x0055 0x0306 
 0x0061 0x0306 
 0x0065 0x0306 
 0x0067 0x0306 
 0x0069 0x0306 
 0x006f 0x0306 
 0x0075 0x0306 
 0x0391 0x0306 
 0x0399 0x0306 
 0x03a5 0x0306 
 0x03b1 0x0306 
 0x03b9 0x0306 
 0x03c5 0x0306 
 0x0410 0x0306 
 0x0415 0x0306 
 0x0416 0x0306 
 0x0418 0x0306 
 0x0423 0x0306 
 0x0430 0x0306 
 0x0435 0x0306 
 0x0436 0x0306 
 0x0438 0x0306 
 0x0443 0x0306 
 0x0041 0x0307 
 0x0042 0x0307 
 0x0043 0x0307 
 0x0044 0x0307 
 0x0045 0x0307 
 0x0046 0x0307 
 0x0047 0x0307 
 0x0048 0x0307 
 0x0049 0x0307 
 0x004d 0x0307 
 0x004e 0x0307 
 0x0050 0x0307 
 0x0052 0x0307 
 0x0053 0x0307 
 0x0054 0x0307 
 0x0057 0x0307 
 0x0058 0x0307 
 0x0059 0x0307 
 0x005a 0x0307 
 0x0061 0x0307 
 0x0062 0x0307 
 0x0063 0x0307 
 0x0064 0x0307 
 0x0065 0x0307 
 0x0066 0x0307 
 0x0067 0x0307 
 0x0068 0x0307 
 0x006d 0x0307 
 0x006e 0x0307 
 0x0070 0x0307 
 0x0072 0x0307 
 0x0073 0x0307 
 0x0074 0x0307 
 0x0077 0x0307 
 0x0078 0x0307 
 0x0079 0x0307 
 0x007a 0x0307 
 0x017f 0x0307 
 0x0306 0x0307 
 0x0041 0x0308 
 0x0045 0x0308 
 0x0048 0x0308 
 0x0049 0x0308 
 0x004f 0x0308 
 0x0055 0x0308 
 0x0057 0x0308 
 0x0058 0x0308 
 0x0059 0x0308 
 0x0061 0x0308 
 0x0065 0x0308 
 0x0068 0x0308 
 0x0069 0x0308 
 0x006f 0x0308 
 0x0074 0x0308 
 0x0075 0x0308 
 0x0077 0x0308 
 0x0078 0x0308 
 0x0079 0x0308 
 0x018f 0x0308 
 0x019f 0x0308 
 0x0259 0x0308 
 0x0275 0x0308 
 0x0399 0x0308 
 0x03a5 0x0308 
 0x03b9 0x0308 
 0x03c5 0x0308 
 0x03d2 0x0308 
 0x0406 0x0308 
 0x0410 0x0308 
 0x0415 0x0308 
 0x0416 0x0308 
 0x0417 0x0308 
 0x0418 0x0308 
 0x041e 0x0308 
 0x0423 0x0308 
 0x0427 0x0308 
 0x042b 0x0308 
 0x0430 0x0308 
 0x0435 0x0308 
 0x0436 0x0308 
 0x0437 0x0308 
 0x0438 0x0308 
 0x043e 0x0308 
 0x0443 0x0308 
 0x0447 0x0308 
 0x044b 0x0308 
 0x0456 0x0308 
 0x0041 0x0309 
 0x0045 0x0309 
 0x0049 0x0309 
 0x004f 0x0309 
 0x0055 0x0309 
 0x0059 0x0309 
 0x0061 0x0309 
 0x0065 0x0309 
 0x0069 0x0309 
 0x006f 0x0309 
 0x0075 0x0309 
 0x0079 0x0309 
 0x0041 0x030a 
 0x0055 0x030a 
 0x0061 0x030a 
 0x0075 0x030a 
 0x0077 0x030a 
 0x0079 0x030a 
 0x004f 0x030b 
 0x0055 0x030b 
 0x006f 0x030b 
 0x0075 0x030b 
 0x0423 0x030b 
 0x0443 0x030b 
 0x0041 0x030c 
 0x0043 0x030c 
 0x0044 0x030c 
 0x0045 0x030c 
 0x0047 0x030c 
 0x0049 0x030c 
 0x004b 0x030c 
 0x004c 0x030c 
 0x004e 0x030c 
 0x004f 0x030c 
 0x0052 0x030c 
 0x0053 0x030c 
 0x0054 0x030c 
 0x0055 0x030c 
 0x005a 0x030c 
 0x0061 0x030c 
 0x0063 0x030c 
 0x0064 0x030c 
 0x0065 0x030c 
 0x0067 0x030c 
 0x0069 0x030c 
 0x006a 0x030c 
 0x006b 0x030c 
 0x006c 0x030c 
 0x006e 0x030c 
 0x006f 0x030c 
 0x0072 0x030c 
 0x0073 0x030c 
 0x0074 0x030c 
 0x0075 0x030c 
 0x007a 0x030c 
 0x01b7 0x030c 
 0x0292 0x030c 
 0x00a8 0x030d 
 0x0308 0x030d 
 0x0391 0x030d 
 0x0395 0x030d 
 0x0397 0x030d 
 0x0399 0x030d 
 0x039f 0x030d 
 0x03a5 0x030d 
 0x03a9 0x030d 
 0x03b1 0x030d 
 0x03b5 0x030d 
 0x03b7 0x030d 
 0x03b9 0x030d 
 0x03bf 0x030d 
 0x03c5 0x030d 
 0x03c9 0x030d 
 0x03d2 0x030d 
 0x0041 0x030f 
 0x0045 0x030f 
 0x0049 0x030f 
 0x004f 0x030f 
 0x0052 0x030f 
 0x0055 0x030f 
 0x0061 0x030f 
 0x0065 0x030f 
 0x0069 0x030f 
 0x006f 0x030f 
 0x0072 0x030f 
 0x0075 0x030f 
 0x0474 0x030f 
 0x0475 0x030f 
 0x0041 0x0311 
 0x0045 0x0311 
 0x0049 0x0311 
 0x004f 0x0311 
 0x0052 0x0311 
 0x0055 0x0311 
 0x0061 0x0311 
 0x0065 0x0311 
 0x0069 0x0311 
 0x006f 0x0311 
 0x0072 0x0311 
 0x0075 0x0311 
 0x0391 0x0313 
 0x0395 0x0313 
 0x0397 0x0313 
 0x0399 0x0313 
 0x039f 0x0313 
 0x03a9 0x0313 
 0x03b1 0x0313 
 0x03b5 0x0313 
 0x03b7 0x0313 
 0x03b9 0x0313 
 0x03bf 0x0313 
 0x03c1 0x0313 
 0x03c5 0x0313 
 0x03c9 0x0313 
 0x0391 0x0314 
 0x0395 0x0314 
 0x0397 0x0314 
 0x0399 0x0314 
 0x039f 0x0314 
 0x03a1 0x0314 
 0x03a5 0x0314 
 0x03a9 0x0314 
 0x03b1 0x0314 
 0x03b5 0x0314 
 0x03b7 0x0314 
 0x03b9 0x0314 
 0x03bf 0x0314 
 0x03c1 0x0314 
 0x03c5 0x0314 
 0x03c9 0x0314 
 0x004f 0x031b 
 0x0055 0x031b 
 0x006f 0x031b 
 0x0075 0x031b 
 0x0041 0x0323 
 0x0042 0x0323 
 0x0044 0x0323 
 0x0045 0x0323 
 0x0048 0x0323 
 0x0049 0x0323 
 0x004b 0x0323 
 0x004c 0x0323 
 0x004d 0x0323 
 0x004e 0x0323 
 0x004f 0x0323 
 0x0052 0x0323 
 0x0053 0x0323 
 0x0054 0x0323 
 0x0055 0x0323 
 0x0056 0x0323 
 0x0057 0x0323 
 0x0059 0x0323 
 0x005a 0x0323 
 0x0061 0x0323 
 0x0062 0x0323 
 0x0064 0x0323 
 0x0065 0x0323 
 0x0068 0x0323 
 0x0069 0x0323 
 0x006b 0x0323 
 0x006c 0x0323 
 0x006d 0x0323 
 0x006e 0x0323 
 0x006f 0x0323 
 0x0072 0x0323 
 0x0073 0x0323 
 0x0074 0x0323 
 0x0075 0x0323 
 0x0076 0x0323 
 0x0077 0x0323 
 0x0079 0x0323 
 0x007a 0x0323 
 0x0055 0x0324 
 0x0075 0x0324 
 0x0041 0x0325 
 0x0061 0x0325 
 0x0043 0x0327 
 0x0044 0x0327 
 0x0045 0x0327 
 0x0047 0x0327 
 0x0048 0x0327 
 0x004b 0x0327 
 0x004c 0x0327 
 0x004e 0x0327 
 0x0052 0x0327 
 0x0053 0x0327 
 0x0054 0x0327 
 0x0063 0x0327 
 0x0064 0x0327 
 0x0065 0x0327 
 0x0067 0x0327 
 0x0068 0x0327 
 0x006b 0x0327 
 0x006c 0x0327 
 0x006e 0x0327 
 0x0072 0x0327 
 0x0073 0x0327 
 0x0074 0x0327 
 0x0041 0x0328 
 0x0045 0x0328 
 0x0049 0x0328 
 0x004f 0x0328 
 0x0055 0x0328 
 0x0061 0x0328 
 0x0065 0x0328 
 0x0069 0x0328 
 0x006f 0x0328 
 0x0075 0x0328 
 0x0044 0x032d 
 0x0045 0x032d 
 0x004c 0x032d 
 0x004e 0x032d 
 0x0054 0x032d 
 0x0055 0x032d 
 0x0064 0x032d 
 0x0065 0x032d 
 0x006c 0x032d 
 0x006e 0x032d 
 0x0074 0x032d 
 0x0075 0x032d 
 0x0048 0x032e 
 0x0068 0x032e 
 0x0045 0x0330 
 0x0049 0x0330 
 0x0055 0x0330 
 0x0065 0x0330 
 0x0069 0x0330 
 0x0075 0x0330 
 0x0042 0x0331 
 0x0044 0x0331 
 0x004b 0x0331 
 0x004c 0x0331 
 0x004e 0x0331 
 0x0052 0x0331 
 0x0054 0x0331 
 0x005a 0x0331 
 0x0062 0x0331 
 0x0064 0x0331 
 0x0068 0x0331 
 0x006b 0x0331 
 0x006c 0x0331 
 0x006e 0x0331 
 0x0072 0x0331 
 0x0074 0x0331 
 0x007a 0x0331 
 0x00a8 0x0342 
 0x03b1 0x0342 
 0x03b7 0x0342 
 0x03b9 0x0342 
 0x03c5 0x0342 
 0x03c9 0x0342 
 0x1fbf 0x0342 
 0x1ffe 0x0342 
 0x0391 0x0345 
 0x0397 0x0345 
 0x03a9 0x0345 
 0x03b1 0x0345 
 0x03b7 0x0345 
 0x03bf 0x0345 
 0x03c9 0x0345 
 0x05d0 0x05b7 
 0x05f2 0x05b7 
 0x05d0 0x05b8 
 0x05d5 0x05b9 
 0x05d0 0x05bc 
 0x05d1 0x05bc 
 0x05d2 0x05bc 
 0x05d3 0x05bc 
 0x05d4 0x05bc 
 0x05d5 0x05bc 
 0x05d6 0x05bc 
 0x05d8 0x05bc 
 0x05d9 0x05bc 
 0x05da 0x05bc 
 0x05db 0x05bc 
 0x05dc 0x05bc 
 0x05de 0x05bc 
 0x05e0 0x05bc 
 0x05e1 0x05bc 
 0x05e3 0x05bc 
 0x05e4 0x05bc 
 0x05e6 0x05bc 
 0x05e7 0x05bc 
 0x05e8 0x05bc 
 0x05e9 0x05bc 
 0x05ea 0x05bc 
 0x05d1 0x05bf 
 0x05db 0x05bf 
 0x05e4 0x05bf 
 0x05e9 0x05c1 
 0x05e9 0x05c2 
 0x0915 0x093c 
 0x0916 0x093c 
 0x0917 0x093c 
 0x091c 0x093c 
 0x0921 0x093c 
 0x0922 0x093c 
 0x0928 0x093c 
 0x092b 0x093c 
 0x092f 0x093c 
 0x0930 0x093c 
 0x0933 0x093c 
 0x09a1 0x09bc 
 0x09a2 0x09bc 
 0x09ac 0x09bc 
 0x09af 0x09bc 
 0x09c7 0x09be 
 0x09c7 0x09d7 
 0x0a16 0x0a3c 
 0x0a17 0x0a3c 
 0x0a1c 0x0a3c 
 0x0a21 0x0a3c 
 0x0a2b 0x0a3c 
 0x0b21 0x0b3c 
 0x0b22 0x0b3c 
 0x0b2f 0x0b3c 
 0x0b47 0x0b3e 
 0x0b47 0x0b56 
 0x0b47 0x0b57 
 0x0bc6 0x0bbe 
 0x0bc7 0x0bbe 
 0x0b92 0x0bd7 
 0x0bc6 0x0bd7 
 0x0c46 0x0c56 
 0x0cc6 0x0cc2 
 0x0cbf 0x0cd5 
 0x0cc6 0x0cd5 
 0x0cc6 0x0cd6 
 0x0d46 0x0d3e 
 0x0d47 0x0d3e 
 0x0d46 0x0d57 
 0x0e4d 0x0e32 
 0x0ecd 0x0eb2 
 0x0f72 0x0f71 
 0x0f74 0x0f71 
 0x0f80 0x0f71 
 0x0fb2 0x0f80 
 0x0fb3 0x0f80 
 0x0f40 0x0fb5 
 0x0f90 0x0fb5 
 0x0f42 0x0fb7 
 0x0f4c 0x0fb7 
 0x0f51 0x0fb7 
 0x0f56 0x0fb7 
 0x0f5b 0x0fb7 
 0x0f92 0x0fb7 
 0x0f9c 0x0fb7 
 0x0fa1 0x0fb7 
 0x0fa6 0x0fb7 
 0x0fab 0x0fb7 
 0x3046 0x3099 
 0x304b 0x3099 
 0x304d 0x3099 
 0x304f 0x3099 
 0x3051 0x3099 
 0x3053 0x3099 
 0x3055 0x3099 
 0x3057 0x3099 
 0x3059 0x3099 
 0x305b 0x3099 
 0x305d 0x3099 
 0x305f 0x3099 
 0x3061 0x3099 
 0x3064 0x3099 
 0x3066 0x3099 
 0x3068 0x3099 
 0x306f 0x3099 
 0x3072 0x3099 
 0x3075 0x3099 
 0x3078 0x3099 
 0x307b 0x3099 
 0x309d 0x3099 
 0x30a6 0x3099 
 0x30ab 0x3099 
 0x30ad 0x3099 
 0x30af 0x3099 
 0x30b1 0x3099 
 0x30b3 0x3099 
 0x30b5 0x3099 
 0x30b7 0x3099 
 0x30b9 0x3099 
 0x30bb 0x3099 
 0x30bd 0x3099 
 0x30bf 0x3099 
 0x30c1 0x3099 
 0x30c4 0x3099 
 0x30c6 0x3099 
 0x30c8 0x3099 
 0x30cf 0x3099 
 0x30d2 0x3099 
 0x30d5 0x3099 
 0x30d8 0x3099 
 0x30db 0x3099 
 0x30ef 0x3099 
 0x30f0 0x3099 
 0x30f1 0x3099 
 0x30f2 0x3099 
 0x30fd 0x3099 
 0x306f 0x309a 
 0x3072 0x309a 
 0x3075 0x309a 
 0x3078 0x309a 
 0x307b 0x309a 
 0x30cf 0x309a 
 0x30d2 0x309a 
 0x30d5 0x309a 
 0x30d8 0x309a 
 0x30db 0x309a 
 0x0307 0x0053 0x0301 
 0x0307 0x0073 0x0301 
 0x0300 0x0041 0x0302 
 0x0301 0x0041 0x0302 
 0x0303 0x0041 0x0302 
 0x0309 0x0041 0x0302 
 0x0300 0x0045 0x0302 
 0x0301 0x0045 0x0302 
 0x0303 0x0045 0x0302 
 0x0309 0x0045 0x0302 
 0x0300 0x004f 0x0302 
 0x0301 0x004f 0x0302 
 0x0303 0x004f 0x0302 
 0x0309 0x004f 0x0302 
 0x0300 0x0061 0x0302 
 0x0301 0x0061 0x0302 
 0x0303 0x0061 0x0302 
 0x0309 0x0061 0x0302 
 0x0300 0x0065 0x0302 
 0x0301 0x0065 0x0302 
 0x0303 0x0065 0x0302 
 0x0309 0x0065 0x0302 
 0x0300 0x006f 0x0302 
 0x0301 0x006f 0x0302 
 0x0303 0x006f 0x0302 
 0x0309 0x006f 0x0302 
 0x0301 0x004f 0x0303 
 0x0308 0x004f 0x0303 
 0x0301 0x0055 0x0303 
 0x0301 0x006f 0x0303 
 0x0308 0x006f 0x0303 
 0x0301 0x0075 0x0303 
 0x0300 0x0045 0x0304 
 0x0301 0x0045 0x0304 
 0x0300 0x004f 0x0304 
 0x0301 0x004f 0x0304 
 0x0308 0x0055 0x0304 
 0x0300 0x0065 0x0304 
 0x0301 0x0065 0x0304 
 0x0300 0x006f 0x0304 
 0x0301 0x006f 0x0304 
 0x0308 0x0075 0x0304 
 0x0300 0x0041 0x0306 
 0x0301 0x0041 0x0306 
 0x0303 0x0041 0x0306 
 0x0309 0x0041 0x0306 
 0x0300 0x0061 0x0306 
 0x0301 0x0061 0x0306 
 0x0303 0x0061 0x0306 
 0x0309 0x0061 0x0306 
 0x0304 0x0041 0x0307 
 0x0304 0x0061 0x0307 
 0x0304 0x0041 0x0308 
 0x0301 0x0049 0x0308 
 0x0300 0x0055 0x0308 
 0x0301 0x0055 0x0308 
 0x0304 0x0055 0x0308 
 0x030c 0x0055 0x0308 
 0x0304 0x0061 0x0308 
 0x0301 0x0069 0x0308 
 0x0300 0x0075 0x0308 
 0x0301 0x0075 0x0308 
 0x0304 0x0075 0x0308 
 0x030c 0x0075 0x0308 
 0x0300 0x03b9 0x0308 
 0x0301 0x03b9 0x0308 
 0x030d 0x03b9 0x0308 
 0x0342 0x03b9 0x0308 
 0x0300 0x03c5 0x0308 
 0x0301 0x03c5 0x0308 
 0x030d 0x03c5 0x0308 
 0x0342 0x03c5 0x0308 
 0x0301 0x0041 0x030a 
 0x0301 0x0061 0x030a 
 0x0307 0x0053 0x030c 
 0x0307 0x0073 0x030c 
 0x0300 0x0391 0x0313 
 0x0301 0x0391 0x0313 
 0x0342 0x0391 0x0313 
 0x0300 0x0395 0x0313 
 0x0301 0x0395 0x0313 
 0x0300 0x0397 0x0313 
 0x0301 0x0397 0x0313 
 0x0342 0x0397 0x0313 
 0x0300 0x0399 0x0313 
 0x0301 0x0399 0x0313 
 0x0342 0x0399 0x0313 
 0x0300 0x039f 0x0313 
 0x0301 0x039f 0x0313 
 0x0300 0x03a9 0x0313 
 0x0301 0x03a9 0x0313 
 0x0342 0x03a9 0x0313 
 0x0300 0x03b1 0x0313 
 0x0301 0x03b1 0x0313 
 0x0342 0x03b1 0x0313 
 0x0300 0x03b5 0x0313 
 0x0301 0x03b5 0x0313 
 0x0300 0x03b7 0x0313 
 0x0301 0x03b7 0x0313 
 0x0342 0x03b7 0x0313 
 0x0300 0x03b9 0x0313 
 0x0301 0x03b9 0x0313 
 0x0342 0x03b9 0x0313 
 0x0300 0x03bf 0x0313 
 0x0301 0x03bf 0x0313 
 0x0300 0x03c5 0x0313 
 0x0301 0x03c5 0x0313 
 0x0342 0x03c5 0x0313 
 0x0300 0x03c9 0x0313 
 0x0301 0x03c9 0x0313 
 0x0342 0x03c9 0x0313 
 0x0300 0x0391 0x0314 
 0x0301 0x0391 0x0314 
 0x0342 0x0391 0x0314 
 0x0300 0x0395 0x0314 
 0x0301 0x0395 0x0314 
 0x0300 0x0397 0x0314 
 0x0301 0x0397 0x0314 
 0x0342 0x0397 0x0314 
 0x0300 0x0399 0x0314 
 0x0301 0x0399 0x0314 
 0x0342 0x0399 0x0314 
 0x0300 0x039f 0x0314 
 0x0301 0x039f 0x0314 
 0x0300 0x03a5 0x0314 
 0x0301 0x03a5 0x0314 
 0x0342 0x03a5 0x0314 
 0x0300 0x03a9 0x0314 
 0x0301 0x03a9 0x0314 
 0x0342 0x03a9 0x0314 
 0x0300 0x03b1 0x0314 
 0x0301 0x03b1 0x0314 
 0x0342 0x03b1 0x0314 
 0x0300 0x03b5 0x0314 
 0x0301 0x03b5 0x0314 
 0x0300 0x03b7 0x0314 
 0x0301 0x03b7 0x0314 
 0x0342 0x03b7 0x0314 
 0x0300 0x03b9 0x0314 
 0x0301 0x03b9 0x0314 
 0x0342 0x03b9 0x0314 
 0x0300 0x03bf 0x0314 
 0x0301 0x03bf 0x0314 
 0x0300 0x03c5 0x0314 
 0x0301 0x03c5 0x0314 
 0x0342 0x03c5 0x0314 
 0x0300 0x03c9 0x0314 
 0x0301 0x03c9 0x0314 
 0x0342 0x03c9 0x0314 
 0x0300 0x004f 0x031b 
 0x0301 0x004f 0x031b 
 0x0303 0x004f 0x031b 
 0x0309 0x004f 0x031b 
 0x0323 0x004f 0x031b 
 0x0300 0x0055 0x031b 
 0x0301 0x0055 0x031b 
 0x0303 0x0055 0x031b 
 0x0309 0x0055 0x031b 
 0x0323 0x0055 0x031b 
 0x0300 0x006f 0x031b 
 0x0301 0x006f 0x031b 
 0x0303 0x006f 0x031b 
 0x0309 0x006f 0x031b 
 0x0323 0x006f 0x031b 
 0x0300 0x0075 0x031b 
 0x0301 0x0075 0x031b 
 0x0303 0x0075 0x031b 
 0x0309 0x0075 0x031b 
 0x0323 0x0075 0x031b 
 0x0302 0x0041 0x0323 
 0x0306 0x0041 0x0323 
 0x0302 0x0045 0x0323 
 0x0304 0x004c 0x0323 
 0x0302 0x004f 0x0323 
 0x0304 0x0052 0x0323 
 0x0307 0x0053 0x0323 
 0x0302 0x0061 0x0323 
 0x0306 0x0061 0x0323 
 0x0302 0x0065 0x0323 
 0x0304 0x006c 0x0323 
 0x0302 0x006f 0x0323 
 0x0304 0x0072 0x0323 
 0x0307 0x0073 0x0323 
 0x0301 0x0043 0x0327 
 0x0306 0x0045 0x0327 
 0x0301 0x0063 0x0327 
 0x0306 0x0065 0x0327 
 0x0304 0x004f 0x0328 
 0x0304 0x006f 0x0328 
 0x0313 0x0391 0x0345 
 0x0314 0x0391 0x0345 
 0x0313 0x0397 0x0345 
 0x0314 0x0397 0x0345 
 0x0313 0x03a9 0x0345 
 0x0314 0x03a9 0x0345 
 0x0300 0x03b1 0x0345 
 0x0301 0x03b1 0x0345 
 0x0313 0x03b1 0x0345 
 0x0314 0x03b1 0x0345 
 0x0342 0x03b1 0x0345 
 0x0300 0x03b7 0x0345 
 0x0301 0x03b7 0x0345 
 0x0313 0x03b7 0x0345 
 0x0314 0x03b7 0x0345 
 0x0342 0x03b7 0x0345 
 0x0301 0x03bf 0x0345 
 0x0300 0x03c9 0x0345 
 0x0313 0x03c9 0x0345 
 0x0314 0x03c9 0x0345 
 0x0342 0x03c9 0x0345 
 0x05c1 0x05e9 0x05bc 
 0x05c2 0x05e9 0x05bc 
 0x0cd5 0x0cc6 0x0cc2 
 0x0f71 0x0fb2 0x0f80 
 0x0f71 0x0fb3 0x0f80 
 0x0300 0x0313 0x0391 0x0345 
 0x0301 0x0313 0x0391 0x0345 
 0x0342 0x0313 0x0391 0x0345 
 0x0300 0x0314 0x0391 0x0345 
 0x0301 0x0314 0x0391 0x0345 
 0x0342 0x0314 0x0391 0x0345 
 0x0300 0x0313 0x0397 0x0345 
 0x0301 0x0313 0x0397 0x0345 
 0x0342 0x0313 0x0397 0x0345 
 0x0300 0x0314 0x0397 0x0345 
 0x0301 0x0314 0x0397 0x0345 
 0x0342 0x0314 0x0397 0x0345 
 0x0300 0x0313 0x03a9 0x0345 
 0x0301 0x0313 0x03a9 0x0345 
 0x0342 0x0313 0x03a9 0x0345 
 0x0300 0x0314 0x03a9 0x0345 
 0x0301 0x0314 0x03a9 0x0345 
 0x0342 0x0314 0x03a9 0x0345 
 0x0300 0x0313 0x03b1 0x0345 
 0x0301 0x0313 0x03b1 0x0345 
 0x0342 0x0313 0x03b1 0x0345 
 0x0300 0x0314 0x03b1 0x0345 
 0x0301 0x0314 0x03b1 0x0345 
 0x0342 0x0314 0x03b1 0x0345 
 0x0300 0x0313 0x03b7 0x0345 
 0x0301 0x0313 0x03b7 0x0345 
 0x0342 0x0313 0x03b7 0x0345 
 0x0300 0x0314 0x03b7 0x0345 
 0x0301 0x0314 0x03b7 0x0345 
 0x0342 0x0314 0x03b7 0x0345 
 0x0300 0x0313 0x03c9 0x0345 
 0x0301 0x0313 0x03c9 0x0345 
 0x0342 0x0313 0x03c9 0x0345 
 0x0300 0x0314 0x03c9 0x0345 
 0x0301 0x0314 0x03c9 0x0345 
 0x0342 0x0314 0x03c9 0x0345 
 SPDX-License-Identifier: GPL-2.0
   linuxfshfsplusextents.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
  Handling of Extents both in catalog and extents overflow trees
 Compare two extents keys, returns 0 on same, posneg for difference 
 panic? 
 Fail early and avoid ENOSPC during the btree operation 
	
	  We can't just use hfsplus_mark_inode_dirty here, because we
	  also get called from hfsplus_write_inode, which should not
	  redirty the inode.  Instead the callers have to be careful
	  to explicily mark the inode dirty, too.
 Get a block at iblock for inode, possibly allocating if create 
 Convert inode block to disk allocation block 
	
	  hfsplus_ext_read_extent will write out a cached extent into
	  the extents btree.  In that case we may have to mark the inode
	  dirty even for a pure read of an extent here.
 panic? 
 Mapping the allocation file may lock the extent tree 
 panic? 
			
			  Try to free all extents and
			  return only last error
 extend alloc file 
 no extents yet 
 try to append to extents in inode 
 XXX: We lack error handling of hfsplus_file_truncate() 
 SPDX-License-Identifier: GPL-2.0
   linuxfshfsplusinode.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
  Inode handling routines
	
	  In case of error extending write may have instantiated a few
	  blocks outside i_size. Trim these off again.
	
	  Sync inode metadata into the catalog and extent trees.
	
	  And explicitly write out the btrees.
 panic? 
 panic? 
 simple node checks? 
 don't silently ignore unsupported ext2 flags 
 SPDX-License-Identifier: GPL-2.0
  linuxfshfsplusattributes.c
  Vyacheslav Dubeyko <slava@dubeyko.com>
  Handling of records in attributes tree
	 The length of the key, as stored in key_len field, does not include
	  the size of the key_len field itself.
	  So, offsetof(hfsplus_attr_key, key_name) is a trick because
	  it takes into consideration key_len field (__be16) of
	  hfsplus_attr_key structure instead of length field (__be16) of
	  hfsplus_attr_unistr structure.
		
		  Mac OS X supports only inline data attributes.
		  Do nothing
		
		  Mac OS X supports only inline data attributes.
		  Do nothing.
		
		  Align len on two-byte boundary.
		  It needs to add pad byte if we have odd len.
 invalid input 
 Fail early and avoid ENOSPC during the btree operation 
 Mac OS X supports only inline data attributes. 
 All is OK. Do nothing. 
 Avoid btree corruption 
 Fail early and avoid ENOSPC during the btree operation 
 SPDX-License-Identifier: GPL-2.0
   linuxfshfsplusdir.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
  Handling of directories
 Find the entry inside dir named dentry->d_name 
 No such entry 
				
				  We found a link pointing to another link,
				  so ignore it and treat it as regular file.
 This is completely artificial... 
	
	  Can be done after the list insertion; exclusion with
	  hfsplus_delete_cat() is provided by directory lock.
 panic? 
 Operation is not supported. 
 Try to delete anyway without error analysis. 
 Operation is not supported. 
 Try to delete anyway without error analysis. 
 Unlink destination if it already exists 
 SPDX-License-Identifier: GPL-2.0
   linuxfshfspluscatalog.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
  Handling of catalog records
 Generates key for catalog filefolders record. 
 Generates key for catalog thread record. 
 invisible and namelocked 
 Try to get a catalog entry for given catalog id 
		
		  Increment subfolder count. Note, the value is only meaningful
		  for folders with HFSPLUS_HAS_FOLDER_COUNT flag set.
		
		  Decrement subfolder count. Note, the value is only meaningful
		  for folders with HFSPLUS_HAS_FOLDER_COUNT flag set.
		 
		  Check for zero. Some subfolders may have been created
		  by an implementation ignorant of this counter.
	
	  Fail early and avoid ENOSPC during the btree operations. We may
	  have to split the root node at most once.
 panic? 
	
	  Fail early and avoid ENOSPC during the btree operations. We may
	  have to split the root node at most once.
 we only need to take spinlock for exclusion with ->release() 
	
	  Fail early and avoid ENOSPC during the btree operations. We may
	  have to split the root node at most twice.
 find the old dir entry and read the data 
 create new dir entry with the data from the old entry 
 finally remove the old entry 
 remove old thread entry 
 create new thread entry 
 SPDX-License-Identifier: GPL-2.0
   linuxfshfsplusbnode.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
  Handle basic btree node operations
 Copy a specified range of bytes from the raw data of a node 
 TODO: optimize later... 
 TODO: optimize later... 
 TODO: optimize later... 
 move down? 
 Load a particular node out of a tree 
 Dispose of resources used by a node 
  Unused nodes have to be zeroed if this is the catalog tree and
  a corresponding flag in the volume header is set.
 SPDX-License-Identifier: GPL-2.0
   linuxfshfsplusunicode.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
  Handler routines for unicode strings
 Fold the case of a unicode char, given the 16 bit value 
 Returns folded char, or 0 if ignorable 
 Compare unicode strings, return values like normal strcmp 
 Compare names as a sequence of 16-bit unsigned integers 
 search for single decomposed char 
 start of a possibly decomposed Hangul char 
 compose the Hangul char 
 main loop for common case of not composed chars 
  Convert one or more ASCII characters into a single unicode character.
  Returns the number of ASCII characters corresponding to the unicode char.
 Decomposes a non-Hangul unicode character. 
  Try to decompose a unicode character as Hangul. Return 0 if @uc is not
  precomposed Hangul, otherwise return the length of the decomposition.
  This function was adapted from sample code from the Unicode Standard
  Annex #15: Unicode Normalization Forms, version 3.2.0.
  Copyright (C) 1991-2018 Unicode, Inc.  All rights reserved.  Distributed
  under the Terms of Use in http:www.unicode.orgcopyright.html.
 Decomposes a single unicode character. 
 Hangul is handled separately 
  Hash a string to an integer as appropriate for the HFS+ filesystem.
  Composed unicode characters are decomposed and case-folding is performed
  if the appropriate bits are (un)set on the superblock.
  Compare strings with HFS+ filename ordering.
  Composed unicode characters are decomposed and case-folding is performed
  if the appropriate bits are (un)set on the superblock.
  linuxfshfspluspart_tbl.c
  Copyright (C) 1996-1997  Paul H. Hargrove
  This file may be distributed under the terms of
  the GNU General Public License.
  Original code to handle the new style Mac partition table based on
  a patch contributed by Holger Schemel (aeglos@valinor.owl.de).
  In function preconditions the term "valid" applied to a pointer to
  a structure means that the pointer is non-NULL and the structure it
  points to has all fields initialized to consistent values.
 offsets to various blocks 
 Driver Descriptor block 
 First block of partition map 
 Block (wi partition) of MDB 
 magic numbers for various disk blocks 
 "ER": driver descriptor map 
 "TS": old-type partition map 
 "PM": new-type partition map 
 "BD": HFS MDB (super block) 
 MFS MDB (super block) 
  The new style Mac partition map
  For each partition on the media there is a physical block (512-byte
  block) containing one of these structures.  These blocks are
  contiguous starting at block 1.
 signature 
 padding 
 partition blocks count 
 physical block start of partition 
 physical block count of partition 
	u8	pmPartName[32];	 (null terminated?) string
				   giving the name of this
	u8	pmPartType[32];	 (null terminated?) string
				   giving the type of this
 a bunch more stuff we don't need 
  The old style Mac partition map
  The partition map consists for a 2-byte signature followed by an
  array of these structures.  The map is terminated with an all-zero
  one of these.
 Signature bytes 
"TFS1" &&
  Parse the partition map looking for the start and length of a
  HFSHFS+ partition.
 SPDX-License-Identifier: GPL-2.0
   linuxfshfsplusbitmap.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
  Handling of allocation file
 scan the first partial u32 for zero bits 
 scan complete u32s for the first zero bit 
 do any partial u32 at the start 
 do full u32s 
 do any partial u32 at end 
 is there any actual work to be done? 
 are all of the bits in range? 
 do any partial u32 at the start 
 do full u32s 
 do any partial u32 at end 
 SPDX-License-Identifier: GPL-2.0
  linuxfshfsplusxattr_trusted.c
  Vyacheslav Dubeyko <slava@dubeyko.com>
  Handler for trusted extended attributes.
 SPDX-License-Identifier: GPL-2.0
  linuxfshfsplusxattr_trusted.c
  Vyacheslav Dubeyko <slava@dubeyko.com>
  Handler for storing security labels as extended attributes.
 SPDX-License-Identifier: GPL-2.0
   linuxfshfsplusbrec.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
  Handle individual btree records
 Get the length and offset of the given record in the given node 
 Get the length of the key from a keyed record 
 new record idx and complete record size 
 get last offset 
 write new last offset 
 move all following entries 
 move data away 
	
	  update parent key if we inserted a key
	  at the start of the node and it is not the new node
 create index data entry 
 get index key 
 fill hole 
 panic? 
		 new record is in the lower half,
		  so leave some more space there
 update new bnode header 
 update previous bnode header 
 update next bnode header 
 if there is no next node, this might be the new tail 
 size difference between old and new key 
 move previous cnid too 
 create index key and entry 
 restore search_key 
 insert old root idx into new root 
 SPDX-License-Identifier: GPL-2.0
   linuxfshfsplusbfind.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
  Search routines for btrees
 used-uninitialized warning 
 Find the record in bnode that best matches key (not greater than...)
 Traverse a BTree from the root to a leaf finding best fit to key 
 Return allocated copy of node found, set recnum to best record 
 SPDX-License-Identifier: GPL-2.0
   linuxfshfsplusoptions.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
  Option parsing
 Initialize an options object to reasonable defaults 
 convert a "four byte character" to a 32 bit int with error checks 
 Parse options from mount. Returns 0 on failure 
 input is the options passed to mount() as a string 
 try utf8 first, as this is the old default behaviour 
 SPDX-License-Identifier: GPL-2.0
   linuxfshfsplusbtree.c
  Copyright (C) 2001
  Brad Boyer (flar@allandria.com)
  (C) 2003 Ardis Technologies <roman@ardistech.com>
  Handle openingclosing btree
  Initial source code of clump size calculation is gotten
  from http:opensource.apple.comtarballsdiskdev_cmds
 	    Volume	Attributes	 Catalog	 Extents
 	     Size	Clump (MB)	Clump (MB)	Clump (MB)
   1GB 	  4,		  4,		 4,
   2GB 	  6,		  6,		 4,
   4GB 	  8,		  8,		 4,
   8GB 	 11,		 11,		 5,
	
	  For volumes 16GB and larger, we want to make sure that a full OS
	  install won't require fragmentation of the Catalog or Attributes
	  B-trees.  We do this by making the clump sizes sufficiently large,
	  and by leaving a gap after the B-trees for them to grow into.
	 
	  For SnowLeopard 10A298, a FullNetInstall with all packages selected
	  results in:
	  Catalog B-tree Header
	 	nodeSize:          8192
	 	totalNodes:       31616
	 	freeNodes:         1978
	  (used = 231.55 MB)
	  Attributes B-tree Header
	 	nodeSize:          8192
	 	totalNodes:       63232
	 	freeNodes:          958
	  (used = 486.52 MB)
	 
	  We also want Time Machine backup volumes to have a sufficiently
	  large clump size to reduce fragmentation.
	 
	  The series of numbers for Catalog and Attribute form a geometric
	  series. For Catalog (16GB to 512GB), each term is 8(15) times
	  the previous term.  For Attributes (16GB to 512GB), each term is
	  4(15) times the previous term.  For 1TB to 16TB, each term is
	  2(15) times the previous term.
  16GB 	 64,		 32,		 5,
  32GB 	 84,		 49,		 6,
  64GB 	111,		 74,		 7,
 128GB 	147,		111,		 8,
 256GB 	194,		169,		 9,
 512GB 	256,		256,		11,
   1TB 	294,		294,		14,
   2TB 	338,		338,		16,
   4TB 	388,		388,		20,
   8TB 	446,		446,		25,
  16TB 	512,		512,		32
 Figure out which column of the above table to use for this file. 
	
	  The default clump size is 0.8% of the volume size. And
	  it must also be a multiple of the node and block size.
  0.8 %  
 turn exponent into table index... 
 empty body 
	
	  Round the clump size to a multiple of node and block size.
	  NOTE: This rounds down.
	
	  Rounding down could have rounded down to 0 if the block size was
	  greater than the clump size.  If so, just use one block or node.
 Get a reference to a BTree and do some initial checks 
 Load the header 
 Verify the tree and set the correct compare function 
 Release resources used by a btree 
 panic? 
 Load the header 
 Make sure @tree has enough space for the @rsvd_nodes 
 panic ;
 panic ;
 SPDX-License-Identifier: GPL-2.0-or-later
  Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.
  If new entry was created in the parent, it could create the 8.3 alias (the
  shortname of logname).  So, the parent may have the negative-dentry which
  matches the created 8.3 alias.
  If it happened, the negative dentry isn't actually negative anymore.  So,
  drop it.
	
	  This is not negative dentry. Always valid.
	 
	  Note, rename() to existing directory entry will have ->d_inode, and
	  will use existing name which isn't specified name by user.
	 
	  We may be able to drop this positive dentry here. But dropping
	  positive dentry isn't good idea. So it's unsupported like
	  rename("filename", "FILENAME") for now.
	
	  Drop the negative dentry, in order to make sure to use the case
	  sensitive name which is specified by user if this is for creation.
 returns the length of a struct qstr, ignoring trailing dots 
  Compute the hash for the exfat name corresponding to the dentry.  If the name
  is invalid, we leave the hash code unchanged so that the existing dentry can
  be used. The exfat fs routines will return ENOENT or EINVAL as appropriate.
		
		  exfat_toupper() works only for code points up to the U+FFFF.
 used only in search empty_slot() 
 search EMPTY CONTINUOUS "num_entries" entries 
					 unused empty group means
					  an empty group which includes
					  unused dentry
 found and invalidate hint_femp 
		
		  exFAT spec allows a dir to grow up to 8388608(256MB)
		  dentries
 find empty directory entry.
  if there isn't any empty slot, expand cluster chain.
 we trust p_dir->size regardless of FAT type 
		
		  Allocate new cluster to this directory
 allocate a cluster 
 append to the FAT chain 
			 no-fat-chain bit is disabled,
			  so fat-chain should be synced with alloc-bitmap
			 the special case that new dentry
			  should be allocated from the start of new cluster
 update the directory entry 
 directory inode should be updated in here 
  Name Resolution Functions :
  Zero if it was successful; otherwise nonzero.
 strip all trailing periods 
	
	  strip all leading spaces :
	  "MS windows 7" supports leading spaces.
	  So we should skip this preprocessing for compatibility.
	 file name conversion :
	  If lookup case, we allow bad-name for compatibility.
 return error value 
 exfat_find_empty_entry must be called before alloc_cluster() 
 -EIO or -ENOSPC 
 update the directory entry 
	 fill the dos name directory entry information of the created file.
	  the first cluster is not determined yet. (0)
 timestamp is already written, so mark_inode_dirty() is unneeded. 
 lookup a file 
 for optimized dir & entry to prevent long traverse of cluster chain 
 check the validity of directory name in the given pathname 
 check the validation of hint_stat and initialize it if required 
 search the file name for directories 
 -error value 
 adjust cdir to the optimized value 
	
	  Checking "alias->d_parent == dentry->d_parent" to make sure
	  FS is not corrupted (especially double linked dir).
		
		  Unhashed alias is able to exist because of revalidate()
		  called by lookup_fast. You can easily make this status
		  by calling create and lookup concurrently
		  In such case, we reuse an alias instead of new dentry
			
			  This inode has non anonymous-DCACHE_DISCONNECTED
			  dentry. This means, the user did ->lookup() by an
			  another name (longname vs 8.3 alias of it) in past.
			 
			  Switch to new one for reason of locality if possible.
 remove an entry, BUT don't truncate 
 update the directory entry 
 This doesn't modify ei 
 timestamp is already written, so mark_inode_dirty() is unneeded. 
 -EIO or -ENOSPC 
 -EIO or -ENOSPC 
	
	  the problem that struct exfat_inode_info caches wrong parent info.
	 
	  because of flag-mismatch of ei->dir,
	  there is abnormal traversing cluster chain.
 rename or move a old file into a new file 
 check the validity of pointer parameters 
 check whether new dir is existing directory and empty 
 if new_inode exists, update ei 
 check the validity of directory name in the given new pathname 
 delete entries of new_dir 
 Free the clusters if new_inode is a dir(as if exfat_rmdir) 
 new_ei, new_clu_to_free 
 just set IO error only 
		 Update new_inode ei
		  Prevent syncing removed new_inode
		  (new_ei is already initialized above code ("if (new_inode)")
	
	  The VFS already checks for existence, so for local filesystems
	  the RENAME_NOREPLACE implementation is equivalent to plain rename.
	  Don't support any other flags
 skip drop_nlink if new_inode already has been dropped 
 SPDX-License-Identifier: GPL-2.0-or-later
   Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.
 If there are some dirty buffers in the bdev inode 
 clu 0 & 1 
 Unicode utf16 255 characters 
 retain persistent-flags 
 flags are not changed 
	 skip updating volume dirty flag,
	  if this volume has been mounted with read-only
 Show partition info 
 Deprecated options 
		
		  Make the limit 24 just in case someone invents something
		  unusual.
 set block size to read super block 
 read boot sector 
 check the validity of BOOT 
 fs_name may unprintable 
	
	  must_be_zero field must be filled with zero to prevent mounting
	  from FAT volume.
	
	  sect_size_bits could be at least 9 and at most 12.
	
	  sect_per_clus_bits could be at least 0 and at most 25 - sect_size_bits.
 because the cluster index starts with 2 
 check consistencies 
 exFAT file size is limited by a disk volume size 
 check logical sector size 
 read boot sector sub-regions 
 extended boot sector sub-regions 
 boot checksum sub-regions 
 mount the file system volume 
 set up enough so that it can read an inode 
 volume flag will be updated in exfat_sync_fs 
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 SPDX-License-Identifier: GPL-2.0-or-later
  Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.
	
	  If the indode is already unlinked, there is no need for updating it.
 get the directory entry of given file or directory 
 set FILE_INFO structure using the acquired struct exfat_dentry 
 File size should be zero if there is no cluster allocated 
  Input: inode, (logical) clu_offset, target allocation area
  Output: errcode, cluster number
  clu = (~0), if it's unable to allocate a new cluster
 hint information 
 hint_bmap.clu should be valid 
 allocate a cluster 
 Broken FAT (i_sze > allocated FAT) 
 append to the FAT chain 
				 no-fat-chain bit is disabled,
				  so fat-chain should be synced with
				  alloc-bitmap
 get stream entry 
 update directory entry 
 end of if != DIR_DELETED 
		
		  Move clu pointer along FAT chains (hole care) because the
		  caller of this function expect clu to be the last cluster.
		  This only works when num_to_be_allocated >= 2,
		  clu = (the first cluster of the allocated chain) =>
		  (the last cluster of ...)
 hint information 
	
	  Adjust i_size_aligned if i_size_ondisk is bigger than it.
 Is this block already allocated? 
 sector offset in cluster 
 Treat newly added block  cluster 
		
		  FIXME: blockdev_direct_IO() doesn't use ->write_begin(),
		  so we need to update the ->i_size_aligned to block boundary.
		 
		  But we must fill the remaining area or hole by nul for
		  updating ->i_size_aligned
		 
		  Return 0, and fallback to normal buffered write.
	
	  Need to use the DIO_LOCKING for avoiding the race
	  condition of exfat_get_block() and ->truncate().
 exfat_get_cluster() assumes the requested blocknr isn't truncated. 
  exfat_block_truncate_page() zeroes out a mapping from file offset `from'
  up to the end of the block which corresponds to `from'.
  This is required during truncate to physically zeroout the tail end
  of that block so it doesn't yield old data if the file is later grown.
  Also, avoid causing failure from fsx for cases of "data past EOF"
 doesn't deal with root inode 
 directory 
 regular file 
 ondisk and aligned size should be aligned with block size 
 SPDX-License-Identifier: GPL-2.0-or-later
  Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.
	
	  First entry  : file entry
	  Second entry : stream-extension entry
	  Third entry  : first file-name entry
	  So, the index of first file-name dentry should start from 2.
 end of name entry 
 read a directory entry from the opened directory 
 check if the given file ID is opened 
 hint_information 
 skip iterating emit_dots when dir is empty 
 name buffer should be allocated before use 
		
		  At least we tried to read a sector.  Move cpos to next sector
		  position (should be aligned).
	
	  Before calling dir_emit(), sb_lock should be released.
	  Because page fault can occur in dir_emit() when the size
	  of buffer given from user is larger than one page size.
	
	  To improve performance, free namebuf after unlock sb_lock.
	  If namebuf is not allocated, this function do nothing
 1 file entry + 1 stream entry + name entries 
	
	  We cannot use exfat_get_dentry_set here because file ep is not
	  initialized yet.
 byte offset in cluster 
 byte offset in sector    
 sector offset in cluster 
 Read-ahead is not required 
 Not sector aligned with ra_count, resize ra_count to page size 
  Returns a set of dentries for a file or dir.
  Note It provides a direct pointer to bh->data via exfat_get_dentry_cached().
  User should call exfat_get_dentry_set() after setting 'modified' to apply
  changes made in this entry set to the real device.
  in:
    sb+p_dir+entry: indicates a filedir
    type:  specifies how many dentries should be included.
  return:
    pointer of entry set on success,
    NULL on failure.
 byte offset in cluster 
 byte offset in sector 
 sector offset in cluster 
 get the next sector 
 validiate cached dentries 
  @ei:         inode info of parent directory
  @p_dir:      directory structure of parent directory
  @num_entries:entry size of p_uniname
  @hint_opt:   If p_uniname is found, filled with optimized direntry
               for traversing cluster chain.
  @return:
    >= 0:      file directory entry position where the name exists
    -ENOENT:   entry with the name does not exist
    -EIO:      IO error
	
	  We started at not 0 index,so we should try to find target
	  from 0 index to the index we started at.
 reset empty hint 
 initialized hint_stat 
 next dentry we'll find is out of this cluster 
 just initialized hint_stat 
 SPDX-License-Identifier: GPL-2.0-or-later
   Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.
  0 ~  19
 20 ~  39
 40 ~  59
 60 ~  79
 80 ~  99
100 ~ 119
120 ~ 139
140 ~ 159
160 ~ 179
180 ~ 199
200 ~ 219
220 ~ 239
240 ~ 254
  0 ~  19
 20 ~  39
 40 ~  59
 60 ~  79
 80 ~  99
100 ~ 119
120 ~ 139
140 ~ 159
160 ~ 179
180 ~ 199
200 ~ 219
220 ~ 239
240 ~ 255
   Allocation Bitmap Management Functions
		
		  Only allowed when bogus allocation
		  bitmap size is large
 release all buffers and free vol_amap 
  If the value of "clu" is 0, it means cluster 2 which is the first cluster of
  the cluster heap.
 extend trim range for continuous free cluster 
 trim current range if it's larger than trim_minlen 
 set next start point of the free hole 
 try to trim remainder 
 SPDX-License-Identifier: GPL-2.0-or-later
   Written 1992,1993 by Werner Almesberger
   22112000 - Fixed fat_date_unix2dos for dates earlier than 01011980
 		 and date_dos2unix for date==0 by Igor Zhbanov(bsg@uniyar.ac.ru)
  Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.
  exfat_fs_error reports a file system problem that might indicate fa data
  corruptioninconsistency. Depending on 'errors' mount option the
  panic() is called, or error message is printed FAT and nothing is done,
  or filesystem is remounted read-only (default behavior).
  In case the file system is remounted read-only, it can be made writable
  again by remounting it.
  exfat_msg() - print preformated EXFAT specific messages.
  All logs except what uses exfat_fs_error() should be written by exfat_msg()
 level means KERN_ pacility level 
 0x40 <= (tz_off & 0x7F) <=0x7F 
 Convert a EXFAT timedate pair to a UNIX date (seconds since 1 1 70). 
 time_cs field represent 0 ~ 199cs(1990 ms) 
 Adjust timezone to UTC0. 
 Convert from local time to UTC using time_offset. 
 Convert linear UNIX date to a EXFAT timedate pair. 
 time_cs field represent 0 ~ 199cs(1990 ms) 
	
	  Record 00h value for OffsetFromUtc field and 1 value for OffsetValid
	  to indicate that local time and UTC are the same.
  The timestamp for access_time has double seconds granularity.
  (There is no 10msIncrement field for access_time unlike createmodify_time)
  atime also has only a 2-second resolution.
 SPDX-License-Identifier: GPL-2.0-or-later
  Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.
 use a default check 
 Of the r and x bits, all (subject to umask) must be present.
		
		  Of the w bits, either all (subject to umask) or none must
		  be present.
		
		  If exfat_mode_can_hold_ro(inode) is false, can't change
		  w bits.
 resize the file length 
 check if the given file ID is opened 
		
		  Truncate FAT chain num_clusters after the first cluster
		  num_clusters = min(new, phys);
		
		  Follow FAT chain
		  (defensive coding - works fine even with corrupted FAT table
 update the directory entry 
 File size should be zero if there is no cluster allocated 
 Any directory can not be truncated to zero 
 cut off from the FAT chain 
 invalidate cache and free the clusters 
 clear exfat cache 
 hint information 
 hint_stat will be used if this is directory. 
 free the clusters 
		
		  Empty start_clu != ~0 (not allocated)
 Check for setting the inode time. 
	
	  We don't return -EPERM here. Yes, strange, but this is too
	  old behavior.
 SPDX-License-Identifier: GPL-2.0-or-later
   linuxfsfatcache.c
   Written 1992,1993 by Werner Almesberger
   Mar 1999. AV. Changed cache, so that it uses the starting cluster instead
 	of inode number.
   May 1999. AV. Fixed the bogosity with FAT32 (read "FAT28"). Fscking lusers.
   Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.
 number of contiguous clusters 
 cluster number in the file. 
 cluster number on disk. 
 Find the cache of "fclus" or nearest cache. 
 Find the same part as "new" in cluster-chain. 
 dummy cache 
 this cache was invalidated 
  Cache invalidation occurs rarely, thus the LRU chain is not updated. It
  fixes itself after a while.
 Update. The copy of caches before this id is discarded. 
	
	  Don`t use exfat_cache if zero offset or non-cluster allocation
		
		  dummy, always not contiguous
		  This is reinitialized by cache_init(), later.
 prevent the infinite loop of cluster chain 
 SPDX-License-Identifier: GPL-2.0-or-later
  Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.
 remap reserved clusters to simplify code 
 This function must be called with bitmap_lock held 
 invalid cluster number 
 no cluster to truncate 
 check cluster validation 
 flush bitmap only if index would be changed or for last cluster 
 Zeroing the unused blocks on this cluster 
 find new cluster 
 check cluster validation 
 update allocation bitmap 
 update FAT table 
 SPDX-License-Identifier: GPL-2.0-or-later
  Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.
 Upcase table macro 
  Upcase table in compressed format (7.2.5.1 Recommended Up-case Table
  in exfat specification, See:
  https:docs.microsoft.comen-uswindowswin32fileioexfat-specification).
  Allow full-width illegal characters :
  "MS windows 7" supports full-width-invalid-name-characters.
  So we should check half-width-invalid-name-characters(ASCII) only
  for compatibility.
  "   : < > ? \ |
 conversion failed 
 conversion failed 
 always len >= 0 
 Process UTF-16 surrogate pair as one character 
			
			  UTF-16 surrogate pair encodes code points above
			  U+FFFF. Code points above U+FFFF are not supported
			  by kernel NLS framework therefore use replacement
			  character
 len == 1 
 uni != index , uni != 0xFFFF 
 FATAL error: default upcase table has error 
 load successfully 
 load default upcase table 
 SPDX-License-Identifier: GPL-2.0
  fssysfsdir.c - sysfs core and dir operation implementation
  Copyright (c) 2001-3 Patrick Mochel
  Copyright (c) 2007 SUSE Linux Products GmbH
  Copyright (c) 2007 Tejun Heo <teheo@suse.de>
  Please see Documentationfilesystemssysfs.rst for more information.
  sysfs_create_dir_ns - create a directory for an object with a namespace tag
  @kobj: object we're creating directory for
  @ns: the namespace tag to use
 	sysfs_remove_dir - remove an object's directory.
 	@kobj:	object.
 	The only thing special about this is that we remove any files in
 	the directory before we remove the directory, and we've inlined
 	what used to be sysfs_rmdir() below, instead of calling separately.
	
	  In general, kboject owner is responsible for ensuring removal
	  doesn't race with other operations and sysfs doesn't provide any
	  protection; however, when @kobj is used as a symlink target, the
	  symlinking entity usually doesn't own @kobj and thus has no
	  control over removal.  @kobj->sd may be removed anytime
	  and symlink code may end up dereferencing an already freed node.
	 
	  sysfs_symlink_target_lock synchronizes @kobj->sd
	  disassociation against symlink operations so that symlink code
	  can safely dereference @kobj->sd.
  sysfs_create_mount_point - create an always empty directory
  @parent_kobj:  kobject that will contain this always empty directory
  @name: The name of the always empty directory to add
 	sysfs_remove_mount_point - remove an always empty directory.
 	@parent_kobj: kobject that will contain this always empty directory
 	@name: The name of the always empty directory to remove
 SPDX-License-Identifier: GPL-2.0
  fssysfsfile.c - sysfs regular (text) file implementation
  Copyright (c) 2001-3 Patrick Mochel
  Copyright (c) 2007 SUSE Linux Products GmbH
  Copyright (c) 2007 Tejun Heo <teheo@suse.de>
  Please see Documentationfilesystemssysfs.rst for more information.
  Determine ktype->sysfs_ops for the given kernfs_node.  This function
  must be called while holding an active reference.
  Reads on sysfs are handled through seq_file, which takes care of hairy
  details like buffering and seeking.  The following function pipes
  sysfs_ops->show() result through seq_file.
 acquire buffer and ensure that it's >= PAGE_SIZE and clear 
	
	  The code works fine with PAGE_SIZE return but it's likely to
	  indicate truncated result or overflow in normal use cases.
 Try to struggle along 
 kernfs read callback for regular sysfs files with pre-alloc 
	
	  If buf != of->prealloc_buf, we don't know how
	  large it is, so cannot safely pass it to ->show
 kernfs write callback for regular sysfs files 
 kernfs write callback for bin sysfs files 
 every kobject with an attribute needs a ktype assigned 
  sysfs_create_file_ns - create an attribute file for an object with custom ns
  @kobj: object we're creating for
  @attr: attribute descriptor
  @ns: namespace the new file should belong to
  sysfs_add_file_to_group - add an attribute file to a pre-existing group.
  @kobj: object we're acting for.
  @attr: attribute descriptor.
  @group: group name.
  sysfs_chmod_file - update the modified mode value on an object attribute.
  @kobj: object we're acting for.
  @attr: attribute descriptor.
  @mode: file permissions.
  sysfs_break_active_protection - break "active" protection
  @kobj: The kernel object @attr is associated with.
  @attr: The attribute to break the "active" protection for.
  With sysfs, just like kernfs, deletion of an attribute is postponed until
  all active .show() and .store() callbacks have finished unless this function
  is called. Hence this function is useful in methods that implement self
  deletion.
  sysfs_unbreak_active_protection - restore "active" protection
  @kn: Pointer returned by sysfs_break_active_protection().
  Undo the effects of sysfs_break_active_protection(). Since this function
  calls kernfs_put() on the kernfs node that corresponds to the 'attr'
  argument passed to sysfs_break_active_protection() that attribute may have
  been removed between the sysfs_break_active_protection() and
  sysfs_unbreak_active_protection() calls, it is not safe to access @kn after
  this function has returned.
  sysfs_remove_file_ns - remove an object attribute with a custom ns tag
  @kobj: object we're acting for
  @attr: attribute descriptor
  @ns: namespace tag of the file to remove
  Hash the attribute name and namespace tag and kill the victim.
  sysfs_remove_file_self - remove an object attribute from its own method
  @kobj: object we're acting for
  @attr: attribute descriptor
  See kernfs_remove_self() for details.
  sysfs_remove_file_from_group - remove an attribute file from a group.
  @kobj: object we're acting for.
  @attr: attribute descriptor.
  @group: group name.
 	sysfs_create_bin_file - create binary file for object.
 	@kobj:	object.
 	@attr:	attribute descriptor.
 	sysfs_remove_bin_file - remove binary file for object.
 	@kobj:	object.
 	@attr:	attribute descriptor.
 	sysfs_link_change_owner - change owner of a sysfs file.
 	@kobj:	object of the kernfs_node the symlink is located in.
 	@targ:	object of the kernfs_node the symlink points to.
 	@name:	name of the link.
 	@kuid:	new owner's kuid
 	@kgid:	new owner's kgid
  This function looks up the sysfs symlink entry @name under @kobj and changes
  the ownership to @kuid@kgid. The symlink is looked up in the namespace of
  @targ.
  Returns 0 on success or error code on failure.
 	sysfs_file_change_owner - change owner of a sysfs file.
 	@kobj:	object.
 	@name:	name of the file to change.
 	@kuid:	new owner's kuid
 	@kgid:	new owner's kgid
  This function looks up the sysfs entry @name under @kobj and changes the
  ownership to @kuid@kgid.
  Returns 0 on success or error code on failure.
 	sysfs_change_owner - change owner of the given object.
 	@kobj:	object.
 	@kuid:	new owner's kuid
 	@kgid:	new owner's kgid
  Change the owner of the default directory, files, groups, and attributes of
  @kobj to @kuid@kgid. Note that sysfs_change_owner mirrors how the sysfs
  entries for a kobject are added by driver core. In summary,
  sysfs_change_owner() takes care of the default directory entry for @kobj,
  the default attributes associated with the ktype of @kobj and the default
  attributes associated with the ktype of @kobj.
  Additional properties not added by driver core have to be changed by the
  driver or subsystem which created them. This is similar to how
  driversubsystem specific entries are removed.
  Returns 0 on success or error code on failure.
 Change the owner of the kobject itself. 
		
		  Change owner of the default attributes associated with the
		  ktype of @kobj.
		
		  Change owner of the default groups associated with the
		  ktype of @kobj.
 	sysfs_emit - scnprintf equivalent, aware of PAGE_SIZE buffer.
 	@buf:	start of PAGE_SIZE buffer.
 	@fmt:	format
 	@...:	optional arguments to @format
  Returns number of characters written to @buf.
 	sysfs_emit_at - scnprintf equivalent, aware of PAGE_SIZE buffer.
 	@buf:	start of PAGE_SIZE buffer.
 	@at:	offset in @buf to start write in bytes
 		@at must be >= 0 && < PAGE_SIZE
 	@fmt:	format
 	@...:	optional arguments to @fmt
  Returns number of characters written starting at &@buf[@at].
 SPDX-License-Identifier: GPL-2.0
  fssysfssymlink.c - sysfs symlink implementation
  Copyright (c) 2001-3 Patrick Mochel
  Copyright (c) 2007 SUSE Linux Products GmbH
  Copyright (c) 2007 Tejun Heo <teheo@suse.de>
  Please see Documentationfilesystemssysfs.rst for more information.
	
	  We don't own @target_kobj and it may be removed at any time.
	  Synchronize using sysfs_symlink_target_lock.  See
	  sysfs_remove_dir() for details.
 	sysfs_create_link_sd - create symlink to a given object.
 	@kn:		directory we're creating the link in.
 	@target:	object we're pointing to.
 	@name:		name of the symlink.
 	sysfs_create_link - create symlink between two objects.
 	@kobj:	object whose directory we're creating the link in.
 	@target:	object we're pointing to.
 	@name:		name of the symlink.
 	sysfs_create_link_nowarn - create symlink between two objects.
 	@kobj:	object whose directory we're creating the link in.
 	@target:	object we're pointing to.
 	@name:		name of the symlink.
 	This function does the same as sysfs_create_link(), but it
 	doesn't warn if the link already exists.
 	sysfs_delete_link - remove symlink in object's directory.
 	@kobj:	object we're acting for.
 	@targ:	object we're pointing to.
 	@name:	name of the symlink to remove.
 	Unlike sysfs_remove_link sysfs_delete_link has enough information
 	to successfully delete symlinks in tagged directories.
	
	  We don't own @target and it may be removed at any time.
	  Synchronize using sysfs_symlink_target_lock.  See
	  sysfs_remove_dir() for details.
 	sysfs_remove_link - remove symlink in object's directory.
 	@kobj:	object we're acting for.
 	@name:	name of the symlink to remove.
 	sysfs_rename_link_ns - rename symlink in object's directory.
 	@kobj:	object we're acting for.
 	@targ:	object we're pointing to.
 	@old:	previous name of the symlink.
 	@new:	new name of the symlink.
 	@new_ns: new namespace of the symlink.
 	A helper function for the common rename symlink idiom.
 SPDX-License-Identifier: GPL-2.0
  fssysfsgroup.c - Operations for addingremoving multiple files at once.
  Copyright (c) 2003 Patrick Mochel
  Copyright (c) 2003 Open Source Development Lab
  Copyright (c) 2013 Greg Kroah-Hartman
  Copyright (c) 2013 The Linux Foundation
			
			  In update mode, we're changing the permissions or
			  visibility.  Do this by first removing then
			  re-adding (if required) the file.
 Updates may happen before the object has been instantiated 
  sysfs_create_group - given a directory kobject, create an attribute group
  @kobj:	The kobject to create the group on
  @grp:	The attribute group to create
  This function creates a group for the first time.  It will explicitly
  warn and error if any of the attribute files being created already exist.
  Returns 0 on success or error code on failure.
  sysfs_create_groups - given a directory kobject, create a bunch of attribute groups
  @kobj:	The kobject to create the group on
  @groups:	The attribute groups to create, NULL terminated
  This function creates a bunch of attribute groups.  If an error occurs when
  creating a group, all previously created groups will be removed, unwinding
  everything back to the original state when this function was called.
  It will explicitly warn and error if any of the attribute files being
  created already exist.
  Returns 0 on success or error code from sysfs_create_group on failure.
  sysfs_update_groups - given a directory kobject, create a bunch of attribute groups
  @kobj:	The kobject to update the group on
  @groups:	The attribute groups to update, NULL terminated
  This function update a bunch of attribute groups.  If an error occurs when
  updating a group, all previously updated groups will be removed together
  with already existing (not updated) attributes.
  Returns 0 on success or error code from sysfs_update_group on failure.
  sysfs_update_group - given a directory kobject, update an attribute group
  @kobj:	The kobject to update the group on
  @grp:	The attribute group to update
  This function updates an attribute group.  Unlike
  sysfs_create_group(), it will explicitly not warn or error if any
  of the attribute files being created already exist.  Furthermore,
  if the visibility of the files has changed through the is_visible()
  callback, it will update the permissions and add or remove the
  relevant files. Changing a group's name (subdirectory name under
  kobj's directory in sysfs) is not allowed.
  The primary use for this function is to call it after making a change
  that affects group visibility.
  Returns 0 on success or error code on failure.
  sysfs_remove_group: remove a group from a kobject
  @kobj:	kobject to remove the group from
  @grp:	group to remove
  This function removes a group of attributes from a kobject.  The attributes
  previously have to have been created for this group, otherwise it will fail.
  sysfs_remove_groups - remove a list of groups
  @kobj:	The kobject for the groups to be removed from
  @groups:	NULL terminated list of groups to be removed
  If groups is not NULL, remove the specified groups from the kobject.
  sysfs_merge_group - merge files into a pre-existing attribute group.
  @kobj:	The kobject containing the group.
  @grp:	The files to create and the attribute group they belong to.
  This function returns an error if the group doesn't exist or any of the
  files already exist in that group, in which case none of the new files
  are created.
  sysfs_unmerge_group - remove files from a pre-existing attribute group.
  @kobj:	The kobject containing the group.
  @grp:	The files to remove and the attribute group they belong to.
  sysfs_add_link_to_group - add a symlink to an attribute group.
  @kobj:	The kobject containing the group.
  @group_name:	The name of the group.
  @target:	The target kobject of the symlink to create.
  @link_name:	The name of the symlink to create.
  sysfs_remove_link_from_group - remove a symlink from an attribute group.
  @kobj:	The kobject containing the group.
  @group_name:	The name of the group.
  @link_name:	The name of the symlink to remove.
  compat_only_sysfs_link_entry_to_kobj - add a symlink to a kobject pointing
  to a group or an attribute
  @kobj:		The kobject containing the group.
  @target_kobj:	The target kobject.
  @target_name:	The name of the target group or attribute.
  @symlink_name:	The name of the symlink file (target_name will be
 			considered if symlink_name is NULL).
	
	  We don't own @target_kobj and it may be removed at any time.
	  Synchronize using sysfs_symlink_target_lock. See sysfs_remove_dir()
	  for details.
  sysfs_group_change_owner - change owner of an attribute group.
  @kobj:	The kobject containing the group.
  @grp:	The attribute group.
  @kuid:	new owner's kuid
  @kgid:	new owner's kgid
  Returns 0 on success or error code on failure.
  sysfs_groups_change_owner - change owner of a set of attribute groups.
  @kobj:	The kobject containing the groups.
  @groups:	The attribute groups.
  @kuid:	new owner's kuid
  @kgid:	new owner's kgid
  Returns 0 on success or error code on failure.
 SPDX-License-Identifier: GPL-2.0
  fssysfssymlink.c - operations for initializing and mounting sysfs
  Copyright (c) 2001-3 Patrick Mochel
  Copyright (c) 2007 SUSE Linux Products GmbH
  Copyright (c) 2007 Tejun Heo <teheo@suse.de>
  Please see Documentationfilesystemssysfs.rst for more information.
 SPDX-License-Identifier: GPL-2.0
  Super blockfilesystem wide operations
  Copyright (C) 1996 Peter J. Braam <braam@maths.ox.ac.uk> and 
  Michael Callahan <callahan@maths.ox.ac.uk> 
  Rewritten for Linux 2.1.  Peter Braam <braam@cs.cmu.edu>
  Copyright (C) Carnegie Mellon University
 VFS super_block ops 
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 exported operations 
 Ignore errors in data, for backward compatibility 
 XXXXX  what do we put here?? 
 get root fid from Venus: this needs the root inode 
 make root inode 
 cannot set type 
 Venus is responsible for truncating the container-file!!! 
 fake something like AFS does 
 and fill in the rest 
 init_coda: used by filesystems.c to register coda 
 SPDX-License-Identifier: GPL-2.0
  Inode operations for Coda filesystem
  Original version: (C) 1996 P. Braam and M. Callahan
  Rewritten for Linux 2.1. (C) 1997 Carnegie Mellon University
  Carnegie Mellon encourages users to contribute improvements to
  the Coda project. Contact Peter Braam (coda@cs.cmu.edu).
 initialize the debugging variables 
 print a fid 
 recognize special .CONTROL name 
 utility functions below 
	 inode's i_flags, i_ino are set by iget
	  XXX: is this all we need ??
  BSD sets attributes that need not be modified to -1. 
  Linux uses the valid field to indicate what should be
  looked at.  The BSD type field needs to be deduced from linux 
  mode.
  So we have to do some translations here.
 clean out         
 determine the type 
 don't do others 
 set those vattrs that need change 
 SPDX-License-Identifier: GPL-2.0
  Directory operations for Coda filesystem
  Original version: (C) 1996 P. Braam and M. Callahan
  Rewritten for Linux 2.1. (C) 1997 Carnegie Mellon University
  Carnegie Mellon encourages users to contribute improvements to
  the Coda project. Contact Peter Braam (coda@cs.cmu.edu).
 same as fsbad_inode.c 
 inode operations for directories 
 access routines: lookup, readlink, permission 
 control object, create inode on the fly 
	 invalidate the directory cnode's attributes so we refetch the
	 optimistically we can also act as if our nose bleeds. The
	  granularity of the mtime is coarse anyways so we might actually be
 we have to wrap inc_nlinkdrop_nlink because sometimes userspace uses a
  trick to fool GNU find's optimizations. If we can't be sure of the link
  (because of volume mount points) we set i_nlink to 1 which forces find
  to consider every child as a possible directory. We should also never
 creation routines: create, mknod, mkdir, link, symlink 
 invalidate the directory cnode's attributes 
 invalidate the directory cnode's attributes 
 try to make de an entry in dir_inodde linked to source_de  
	
	  This entry is now negative. Since we do not create
	  an inode for the entry we have to drop it.
 mtime is no good anymore 
 destruction routines: unlink, rmdir 
 VFS may delete the child 
 fix the link count of the parent 
 rename 
 support routines 
 read entries from the directory file 
 end of directory file reached 
 catch truncated reads 
 validate whether the directory file actually makes sense 
 Make sure we skip '.' and '..', we already got those 
 skip null entries 
		 we'll always have progress because d_reclen is unsigned and
 file operations for directories 
 Venus: we must read Venus dirents from a file 
 called when a cache lookup succeeds 
 propagate for a flush 
 pretend it's valid, but don't change the flags 
 clear the flags. 
  This is the callback from dput() when d_count is going to 0.
  We use this to unhash dentries with bad inodes.
  This is called when we want to check if the inode has
  changed on the server.  Coda makes this easy since the
  cache manager Venus issues a downcall to the kernel when this 
  happens 
		 this inode may be lost if:
		   - it's ino changed 
		   - type changes must be permitted for repair and
		   missing mount points.
		 the following can happen when a local fid is replaced 
 SPDX-License-Identifier: GPL-2.0
 cnode related routines for the coda kernel code
   (C) 1996 Peter Braam
 cnode.c 
 we still need to set i_ino for things like stat(2) 
 inode is locked and unique, no need to grab cii->c_lock 
 Inode has changed type, mark bad and grab a new one 
 this is effectively coda_iget:
   - get attributes (might be cached)
   - get the inode for the fid using vfs iget
   - link the two up if this is needed
   - fill in the attributes
 We get inode numbers from Venus -- see venus source 
 Although we treat Coda file identifiers as immutable, there is one
  special case for files created during a disconnection where they may
  not be globally unique. When an identifier collision is detected we
  first try to flush the cached inode from the kernel and finally
  resort to renamingrehashing in-place. Userspace remembers both old
  and new values of the identifier to handle any in-flight upcalls.
  The real solution is to use globally unique UUIDs as identifiers, but
 replace fid and rehash inode 
 XXX we probably need to hold some lock here! 
 convert a fid to an inode. 
	 we should never see newly created inodes because we intentionally
 the CONTROL inode is made without asking attributes from Venus 
 SPDX-License-Identifier: GPL-2.0
  Sysctl operations for Coda filesystem
  Original version: (C) 1996 P. Braam and M. Callahan
  Rewritten for Linux 2.1. (C) 1997 Carnegie Mellon University
  Carnegie Mellon encourages users to contribute improvements to
  the Coda project. Contact Peter Braam (coda@cs.cmu.edu).
 SPDX-License-Identifier: GPL-2.0
  File operations for Coda.
  Original version: (C) 1996 Peter Braam 
  Rewritten for Linux 2.1: (C) 1997 Carnegie Mellon University
  Carnegie Mellon encourages users of this code to contribute improvements
  to the Coda project. Contact Peter Braam <coda@cs.cmu.edu>.
	 only allow additional mmaps as long as userspace isn't changing
 keep track of how often the coda_inodehost_file has been mmapped 
		 if call_mmap fails, our caller will put host_file so we
		  should drop the reference to the coda_file that we got.
 here we add redirects for the openclose vm_operations 
 assume access intents are supported unless we hear otherwise 
 did we mmap this file? 
	 VFS fput ignores the return value from file_operations->release, so
 SPDX-License-Identifier: GPL-2.0
  Pioctl operations for Coda.
  Original version: (C) 1996 Peter Braam
  Rewritten for Linux 2.1: (C) 1997 Carnegie Mellon University
  Carnegie Mellon encourages users of this code to contribute improvements
  to the Coda project. Contact Peter Braam <coda@cs.cmu.edu>.
 pioctl ops 
 exported from this file 
 the coda pioctl inode ops 
 get the Pioctl data arguments from user space 
	
	  Look up the pathname. Note that the pathname is in
	  user memory, and namei takes care of this
 return if it is not a Coda inode 
 now proceed to make the upcall 
 SPDX-License-Identifier: GPL-2.0
  Symlink inode operations for Coda filesystem
  Original version: (C) 1996 P. Braam and M. Callahan
  Rewritten for Linux 2.1. (C) 1997 Carnegie Mellon University
  Carnegie Mellon encourages users to contribute improvements to
  the Coda project. Contact Peter Braam (coda@cs.cmu.edu).
 SPDX-License-Identifier: GPL-2.0
  Cache operations for Coda.
  For Linux 2.1: (C) 1997 Carnegie Mellon University
  For Linux 2.3: (C) 2000 Carnegie Mellon University
  Carnegie Mellon encourages users of this code to contribute improvements
  to the Coda project http:www.coda.cs.cmu.edu <coda@cs.cmu.edu>.
 replace or extend an acl cache hit 
 remove cached acl from an inode 
 remove all acl caches 
 check if the mask has been matched against the acl already 
 Purging dentries and children 
 The following routines drop dentries which are not
   in use and flag dentries which are in use to be 
   zapped later.
   The flags are detected by:
   - coda_dentry_revalidate (for lookups) if the flag is C_PURGE
   - coda_dentry_delete: to remove dentry from the cache when d_count
     falls to zero
   - an inode method coda_revalidate (for attributes) if the 
     flag is C_VATTR
 this won't do any harm: just flag all children 
 don't know what to do with negative dentries 
 SPDX-License-Identifier: GPL-2.0-or-later
       	An implementation of a loadable kernel mode driver providing
 		multiple kerneluser space bidirectional communications links.
  		Author: 	Alan Cox <alan@lxorguk.ukuu.org.uk>
               Adapted to become the Linux 2.0 Coda pseudo device
               Peter  Braam  <braam@maths.ox.ac.uk> 
               Michael Callahan <mjc@emmy.smith.edu>           
               Changes for Linux 2.1
               Copyright (c) 1997 Carnegie-Mellon University
 statistics 
 allows signals during upcalls 
 .. secs, then signals will dequeue 
  Device operations
 	Receive a message written by Venus to the psdev
 make sure there is enough to copy out the (opcode, unique) values 
 Peek at the opcode, uniquefier 
 what downcall errors does Venus handle ? 
 Look for the message on the processing queue. 
 move data into response buffer. 
 don't have more space! 
 adjust outsize. is this useful ?? 
 Convert filedescriptor into a file handle 
 	Read a message from the kernel to Venus
 Move the input args into userspace 
 If request was not a signal, enqueue and don't free 
 Wakeup clients so they can return. 
 Async requests need to be freed here 
 SPDX-License-Identifier: GPL-2.0
  Mostly platform independent upcall operations to Venus:
   -- upcalls
   -- upcall routines
  Linux 2.0 version
  Copyright (C) 1996 Peter J. Braam <braam@maths.ox.ac.uk>, 
  Michael Callahan <callahan@maths.ox.ac.uk> 
  Redone for Linux 2.1
  Copyright (C) 1997 Carnegie Mellon University
  Carnegie Mellon University encourages users of this code to contribute
  improvements to the Coda project. Contact Peter Braam <coda@cs.cmu.edu>.
 the upcalls 
 send Venus a null terminated string 
 Venus must get null terminated string 
 Venus must receive an null terminated string 
 round up to word boundary 
 another null terminated string for Venus 
 round up to word boundary 
 Venus must get null terminated string 
 make sure strings are null terminated 
        inp->coda_symlink.attr = tva; XXXXXX  
 Round up to word boundary and null terminate 
 Round up to word boundary and null terminate 
 build packet for Venus 
         the cmd field was mutated by increasing its size field to
          reflect the path and follow args. We need to subtract that
 in->coda_ioctl.rwflag = flag; 
 get the data out of user space 
 Copy out the OUT buffer. 
 Copy out the OUT buffer. 
	
	  we have to free the request buffer for synchronous upcalls
	  or when asynchronous upcalls fail, but not when asynchronous
	  upcalls succeed
 Chunked access is not supported or an old Coda client 
  coda_upcall and coda_downcall routines.
 Don't allow signals to interrupt the following upcalls before venus
  has seen them,
  - CODA_CLOSE or CODA_RELEASE upcall  (to avoid reference count problems)
  - CODA_STORE				(to avoid data loss)
  - CODA_ACCESS_INTENT                 (to avoid reference count problems)
 got a reply 
  coda_upcall will return an error in the case of
  failed communication with Venus _or_ will peek at Venus
  reply and return Venus' error.
  As venus has 2 types of errors, normal errors (positive) and internal
  errors (negative), normal errors are negated, while internal errors
  are all mapped to -EINTR, while showing a nice warning message. (jh)
 Format the request message. 
 Append msg to pending queue and poke Venus. 
 We can return early on asynchronous requests 
	 We can be interrupted while we wait for Venus to process
	  our request.  If the interrupt occurs before Venus has read
	  the request, we dequeue and return. If it occurs after the
	  read but before the reply, we dequeue, send a signal
	  message, and return. If it occurs after the reply we ignore
	  it. In no case do we want to restart the syscall.  If it
	  was interrupted by a venus shutdown (psdev_close), return
 Go to sleep.  Wake up on signals only after the timeout. 
 Op went through, interrupt or not... 
 here we map positive Venus errors to kernel errors 
 Interrupted before venus read it. 
 Venus saw the upcall, make sure we can send interrupt signal 
 insert at head of queue! 
    The statements below are part of the Coda opportunistic
    programming -- taken from the MachBSD kernel code for Coda. 
    You don't get correct semantics by stating what needs to be
    done without guaranteeing the invariants needed for it to happen.
    When will be have time to find out what exactly is going on?  (pjb)
  There are 7 cases where cache invalidations occur.  The semantics
   of each is listed here:
  CODA_FLUSH     -- flush all entries from the name cache and the cnode cache.
  CODA_PURGEUSER -- flush all entries from the name cache for a specific user
                   This call is a result of token expiration.
  The next arise as the result of callbacks on a file or directory.
  CODA_ZAPFILE   -- flush the cached attributes for a file.
  CODA_ZAPDIR    -- flush the attributes for the dir and
                   force a new lookup for all the children
                    of this dir.
  The next is a result of Venus detecting an inconsistent file.
  CODA_PURGEFID  -- flush the attribute for the file
                   purge it and its children from the dcache
  The last  allows Venus to replace local fids with global ones
  during reintegration.
	
	  Make sure we have received enough data from the cache
	  manager to populate the necessary fields in the buffer
 Handle invalidation requests. 
 catch the dentries later if some are still busy 
  Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)
  Licensed under the GPL
  Ported the filesystem routines to 2.5.
  2003-02-10 Petr Baudis <pasky@ucw.cz>
 Changed in hostfs_args before the kernel starts running 
	
	  This function relies on the fact that dentry_path_raw() will place
	  the path name at the end of the provided buffer.
	
	  do_statfs uses struct statfs64 internally, but the linux kernel
	  struct statfs still has 32-bit versions for most of these fields,
	  so we convert them here
 somebody else had handled it first? 
	
	  If err > 0, write_file has added err to pos, so we are comparing
	  i_size against the last byte written.
 Reencode maj and min with the kernel encoding.
 NULL is printed as '(null)' by printf(): avoid that. 
  Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)
  Licensed under the GPL
	
	  Update accessed andor modified time, in two parts: first set
	  times according to the changes to perform, and then call futimes()
	  or utimes() to apply them.
 Note: ctime is not handled 
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008
  Phillip Lougher <phillip@squashfs.org.uk>
  namei.c
  This file implements code to do filename lookup in directories.
  Like inodes, directories are packed into compressed metadata blocks, stored
  in a directory table.  Directories are accessed using the start address of
  the metablock containing the directory and the offset into the
  decompressed block (<block, offset>).
  Directories are organised in a slightly complex way, and are not simply
  a list of file names.  The organisation takes advantage of the
  fact that (in most cases) the inodes of the files will be in the same
  compressed metadata block, and therefore, can share the start block.
  Directories are therefore organised in a two level list, a directory
  header containing the shared start block value, and a sequence of directory
  entries, each of which share the shared start block.  A new directory header
  is written onceif the inode start block changes.  The directory
  headerdirectory entry list is repeated as many times as necessary.
  Directories are sorted, and can contain a directory index to speed up
  file lookup.  Directory indexes store one entry per metablock, each entry
  storing the indexfilename mapping to the first directory header
  in each metadata block.  Directories are sorted in alphabetical order,
  and at lookup the index is scanned linearly looking for the first filename
  alphabetically larger than the filename being looked up.  At this point the
  location of the metadata block the filename is in has been found.
  The general idea of the index is ensure only one metadata block needs to be
  decompressed to do a lookup irrespective of the length of the directory.
  This scheme has the advantage that it doesn't require extra memory overhead
  and doesn't require much extra storage on disk.
  Lookup name in the directory index, returning the location of the metadata
  block containing it, and the directory index this represents.
  If we get an error reading the index then return the part of the index
  (if any) we have managed to read - the index isn't essential, just
  quicker.
	
	  Return index (f_pos) of the looked up metadata block.  Translate
	  from internal f_pos to external f_pos which is offset by 3 because
	  we invent "." and ".." entries which are not actually stored in the
	  directory.
		
		  Read directory header.
			
			  Read directory entry.
 size should never be larger than SQUASHFS_NAME_LEN 
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008
  Phillip Lougher <phillip@squashfs.org.uk>
  super.c
  This file implements code to read the superblock, read and initialise
  in-memory structures at mount time, and all the VFS glue code to register
  the filesystem.
	
	  msblk->bytes_used is checked in squashfs_read_table to ensure reads
	  are not beyond filesystem end.  But as we're using
	  squashfs_read_table here to read the superblock (including the value
	  of bytes_used) we need to set it to an initial sensible dummy value
 Check it is a SQUASHFS superblock 
 Check the MAJOR & MINOR versions and lookup compression type 
	 Check the filesystem does not extend beyond the end of the
 Check block size for sanity 
	
	  Check the system page size is not larger than the filesystem
	  block size (by default 128K).  This is currently not supported.
 Check block log for sanity 
 Check that block_size and block_log match 
 Check the root inode for sanity 
 Allocate read_page block 
 Handle xattrs 
 Allocate and read xattr id lookup table 
 Allocate and read id index table 
 Handle inode lookup table 
 Allocate and read inode lookup table 
 Allocate and read fragment index table 
 Sanity check directory_table 
 Sanity check inode_table 
 allocate root 
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (c) 2013
  Phillip Lougher <phillip@squashfs.org.uk>
 Read separately compressed datablock directly into page cache 
	
	  Create a "page actor" which will kmap and kunmap the
	  page cache pages appropriately within the decompressor
 Try to grab all the pages covered by the Squashfs block 
		
		  Couldn't get one or more pages, this page has either
		  been VM reclaimed, but others are still in the page cache
		  and uptodate, or we're racing with another thread in
		  squashfs_readpage also trying to grab them.  Fall back to
		  using an intermediate buffer.
 Decompress directly into the page cache buffers 
 Last page may have trailing bytes not filled 
 Mark pages as uptodate, unlock and release 
	 Decompression failed, mark pages as errored.  Target_page is
	  dealt with by the caller
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (c) 2013
  Phillip Lougher <phillip@squashfs.org.uk>
 Read separately compressed datablock and memcopy into page cache 
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010
  Phillip Lougher <phillip@squashfs.org.uk>
  xz_wrapper.c
 check compressor options are the expected length 
 the dictionary size should be 2^n or 2^n+2^(n+1) 
 use defaults 
 XZ_STREAM_END must be reached. 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (c) 2013
  Phillip Lougher <phillip@squashfs.org.uk>
  This file contains implementations of page_actor for decompressing into
  an intermediate buffer, and for decompressing directly into the
  page cache.
  Calling code should avoid sleeping between calls to squashfs_first_page()
  and squashfs_finish_page().
 Implementation of page_actor for decompressing into intermediate buffer 
 empty 
 Implementation of page_actor for decompressing directly into page cache. 
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2010
  Phillip Lougher <phillip@squashfs.org.uk>
  xattr.c
 check that the file system has xattrs 
 loop reading each xattr name 
 no handler or insuffficient privileges, so skip 
 skip remaining xattr entry 
 loop reading each xattr name 
 found xattr 
 val is a reference to the real location 
 read xattr value 
 no match, skip remaining xattr entry 
  User namespace support
  Trusted namespace support
  Security namespace support
 ignore unrecognised type 
 ignore unrecognised type 
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008
  Phillip Lougher <phillip@squashfs.org.uk>
  fragment.c
  This file implements code to handle compressed fragments (tail-end packed
  datablocks).
  Regular files contain a fragment index which is mapped to a fragment
  location on disk and compressed size using a fragment lookup table.
  Like everything in Squashfs this fragment lookup table is itself stored
  compressed into metadata blocks.  A second index table is used to locate
  these.  This second index table for speed of access (and because it
  is small) is read at mount time and cached in memory.
  Look-up fragment using the fragment index table.  Return the on disk
  location of the fragment and its compressed size
  Read the uncompressed fragment lookup table indexes off disk into memory
	
	  Sanity check, length bytes should not extend into the next table -
	  this check also traps instances where fragment_table_start is
	  incorrectly larger than the next table start
	
	  table[0] points to the first fragment table metadata block, this
	  should be less than fragment_table_start
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008
  Phillip Lougher <phillip@squashfs.org.uk>
  inode.c
  This file implements code to create and read inodes from disk.
  Inodes in Squashfs are identified by a 48-bit inode which encodes the
  location of the compressed metadata block containing the inode, and the byte
  offset into that block where the inode is placed (<block, offset>).
  To maximise compression there are different inodes for each file type
  (regular file, directory, device, etc.), the inode contents and length
  varying with the type.
  To further maximise compression, two types of regular file inode and
  directory inode are defined: inodes optimised for frequently occurring
  regular files and directories, and extended types where extra
  information has to be stored.
  Initialise VFS inode with the base inode information common to all
  Squashfs inode types.  Sqsh_ino contains the unswapped base inode
  off disk.
  Initialise VFS inode by reading inode from inode table (compressed
  metadata).  The format and amount of data read depends on type.
	
	  Read inode base common to all inode types.
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (c) 2013
  Phillip Lougher <phillip@squashfs.org.uk>
  This file implements multi-threaded decompression using percpu
  variables, one thread per cpu core.
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009
  Phillip Lougher <phillip@squashfs.org.uk>
  zlib_wrapper.c
 Z_STREAM_END must be reached. 
 SPDX-License-Identifier: GPL-2.0-only
   Copyright (c) 2013
   Minchan Kim <minchan@kernel.org>
  This file implements multi-threaded decompression in the
  decompressor framework
  The reason that multiply two is that a CPU can request new IO
  while it is waiting previous request.
	
	  We should have a decompressor at least as default
	  so if we fail to allocate new decompressor dynamically,
	  we could always fall back to default decompressor and
	  file system works.
 There is available decomp_stream 
		
		  If there is no available decomp and already full,
		  let's wait for releasing decomp from other users.
 Let's allocate new decomp 
		
		  If system memory is tough, let's for other's
		  releasing instead of hurting VM because it could
		  make page cache thrashing.
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008
  Phillip Lougher <phillip@squashfs.org.uk>
  dir.c
  This file implements code to read directories from disk.
  See namei.c for a description of directory organisation on disk.
  Lookup offset (f_pos) in the directory index, returning the
  metadata block containing it.
  If we get an error reading the index then return the part of the index
  (if any) we have managed to read - the index isn't essential, just
  quicker.
	
	  Translate from external f_pos to the internal f_pos.  This
	  is offset by 3 because we invent "." and ".." entries which are
	  not actually stored in the directory.
			
			  Found the index we're looking for.
 size should never be larger than SQUASHFS_NAME_LEN 
	
	  Translate back from internal f_pos to external f_pos.
	
	  Return "." and  ".." entries as the first two filenames in the
	  directory.  To maximise compression these two entries are not
	  stored in the directory, and so we invent them here.
	 
	  It also means that the external f_pos is offset by 3 from the
	  on-disk directory f_pos.
		
		  Read directory header
			
			  Read directory entry.
 size should never be larger than SQUASHFS_NAME_LEN 
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008
  Phillip Lougher <phillip@squashfs.org.uk>
  block.c
  This file implements the low-level routines to read and decompress
  datablocks and metadata blocks.
  Returns the amount of bytes copied to the page actor.
  Read and decompress a metadata block or datablock.  Length is non-zero
  if a datablock is being read (the size is stored elsewhere in the
  filesystem), otherwise the length is obtained from the first two bytes of
  the metadata block.  A bit in the length field indicates if the block
  is stored uncompressed in the filesystem (usually because compression
  generated a larger block - this does occasionally happen with compression
  algorithms).
		
		  Datablock.
		
		  Metadata block.
 Extract the length of the metadata block 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (c) 2013
  Phillip Lougher <phillip@squashfs.org.uk>
  This file implements single-threaded decompression in the
  decompressor framework
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008
  Phillip Lougher <phillip@squashfs.org.uk>
  file.c
  This file contains code for handling regular files.  A regular file
  consists of a sequence of contiguous compressed blocks, andor a
  compressed fragment block (tail-end packed block).   The compressed size
  of each datablock is stored in a block list contained within the
  file inode (itself stored in one or more compressed metadata blocks).
  To speed up access to datablocks when reading 'large' files (256 Mbytes or
  larger), the code implements an index cache that caches the mapping from
  block index to datablock location on disk.
  The index cache allows Squashfs to handle large files (up to 1.75 TiB) while
  retaining a simple and space-efficient block list on disk.  The cache
  is split into slots, caching up to eight 224 GiB files (128 KiB blocks).
  Larger files use multiple slots, with 1.75 TiB files using all 8 slots.
  The index cache is designed to be memory efficient, and by default uses
  16 KiB.
  Locate cache slot in range [offset, index] for specified inode.  If
  there's more than one return the slot closest to index.
  Find and initialise an empty cache slot for index offset.
		
		  First time cache index has been used, allocate and
		  initialise.  The cache index could be allocated at
		  mount time but doing it here means it is allocated only
		  if a 'large' file is read.
  Read the next n blocks from the block list, starting from
  metadata block <start_block, offset>.
  Each cache index slot has SQUASHFS_META_ENTRIES, each of which
  can cache one index -> datablockblocklist-block mapping.  We wish
  to distribute these over the length of the file, entry[0] maps index x,
  entry[1] maps index x + skip, entry[2] maps index x + 2  skip, and so on.
  The larger the file, the greater the skip factor.  The skip factor is
  limited to the size of the metadata cache (SQUASHFS_CACHED_BLKS) to ensure
  the number of metadata blocks that need to be read fits into the cache.
  If the skip factor is limited in this way then the file will use multiple
  slots.
  Search and grow the index cache for the specified inode, returning the
  on-disk locations of the datablock and block list metadata block
  <index_block, index_offset> for index (scaled to nearest cache index).
	
	  Scale index to cache index (cache slot entry)
		
		  If necessary grow cache slot by reading block list.  Cache
		  slot is extended up to index or to the end of the slot, in
		  which case further slots will be used.
					
					  Don't leave an empty slot on read
					  error allocated to this inode...
	
	  Scale cache index (cache slot entry) to index
  Get the on-disk location and compressed size of the datablock
  specified by index.  Fill_meta_index() does most of the work.
	
	  res contains the index of the mapping returned by fill_meta_index(),
	  this will likely be less than the desired index (because the
	  meta_index cache works at a higher granularity).  Read any
	  extra block indexes needed.
	
	  Read length of block specified by index.
 Copy data into page cache  
	
	  Loop copying datablock into pages.  As the datablock likely covers
	  many PAGE_SIZE pages (default block size is 128 KiB) explicitly
	  grab the pages from the page cache, except for the page that we've
	  been called to fill.
 Read datablock stored packed inside a fragment (tail-end packed block) 
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008
  Phillip Lougher <phillip@squashfs.org.uk>
  symlink.c
  This file implements code to handle symbolic links.
  The data contents of symbolic links are stored inside the symbolic
  link inode within the inode table.  This allows the normally small symbolic
  link to be compressed as part of the inode table, achieving much greater
  compression than if the symbolic link was compressed individually.
	
	  Skip index bytes into symlink metadata.
	
	  Read length bytes from symlink metadata.  Squashfs_read_metadata
	  is not used here because it can sleep and we want to use
	  kmap_atomic to map the page.  Instead call the underlying
	  squashfs_cache_get routine.  As length bytes may overlap metadata
	  blocks, we may need to call squashfs_cache_get multiple times.
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008
  Phillip Lougher <phillip@squashfs.org.uk>
  cache.c
  Blocks in Squashfs are compressed.  To avoid repeatedly decompressing
  recently accessed data Squashfs uses two small metadata and fragment caches.
  This file implements a generic cache implementation used for both caches,
  plus functions layered ontop of the generic cache implementation to
  access the metadata and fragment caches.
  To avoid out of memory and fragmentation issues with vmalloc the cache
  uses sequences of kmalloced PAGE_SIZE buffers.
  It should be noted that the cache is not used for file datablocks, these
  are decompressed and cached in the page-cache in the normal way.  The
  cache is only used to temporarily cache fragment and metadata blocks
  which have been read as as a result of a metadata (i.e. inode or
  directory) or fragment access.  Because metadata and fragments are packed
  together into blocks (to gain greater compression) the read of a particular
  piece of metadata or fragment will retrieve other metadatafragments which
  have been packed with it, these because of locality-of-reference may be read
  in the near future. Temporarily caching them ensures they are available for
  near future access without requiring an additional read and decompress.
  Look-up block in cache, and increment usage count.  If not in cache, read
  and decompress it from disk.
			
			  Block not in cache, if all cache entries are used
			  go to sleep waiting for one to become available.
			
			  At least one unused cache entry.  A simple
			  round-robin strategy is used to choose the entry to
			  be evicted from the cache.
			
			  Initialise chosen cache entry, and fill it in from
			  disk.
			
			  While filling this entry one or more other processes
			  have looked it up in the cache, and have slept
			  waiting for it to become available.
		
		  Block already in cache.  Increment refcount so it doesn't
		  get reused until we're finished with it, if it was
		  previously unused there's one less cache entry available
		  for reuse.
		
		  If the entry is currently being filled in by another process
		  go to sleep waiting for it to become available.
  Release cache entry, once usage count is zero it can be reused.
		
		  If there's any processes waiting for a block to become
		  available, wake one up.
  Delete cache reclaiming all kmalloced buffers.
  Initialise cache allocating the specified number of entries, each of
  size block_size.  To avoid vmalloc fragmentation issues each entry
  is allocated as a sequence of kmalloced PAGE_SIZE buffers.
  Copy up to length bytes from cache entry to buffer starting at offset bytes
  into the cache entry.  If there's not length bytes then copy the number of
  bytes available.  In all cases return the number of bytes copied.
  Read length bytes from metadata position <block, offset> (block is the
  start of the compressed block on disk, and offset is the offset into
  the block once decompressed).  Data is packed into consecutive blocks,
  and length bytes may require reading more than one block.
  Look-up in the fragmment cache the fragment located at <start_block> in the
  filesystem.  If necessary read and decompress it from disk.
  Read and decompress the datablock located at <start_block> in the
  filesystem.  The cache is used here to avoid duplicating locking and
  readdecompress code.
  Read a filesystem table (uncompressed sequence of bytes) from disk
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (c) 2013, 2014
  Phillip Lougher <phillip@squashfs.org.uk>
 LZ4 compressed filesystems always have compression options 
		 LZ4 format currently used by the kernel is the 'legacy'
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2016-present, Facebook, Inc.
  All rights reserved.
  zstd_wrapper.c
				 Shouldn't run out of pages
				  before stream is done.
 add the additional data produced 
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008
  Phillip Lougher <phillip@squashfs.org.uk>
  export.c
  This file implements code to make Squashfs filesystems exportable (NFS etc.)
  The export code uses an inode lookup table to map inode numbers passed in
  filehandles to an inode location on disk.  This table is stored compressed
  into metadata blocks.  A second index table is used to locate these.  This
  second index table for speed of access (and because it is small) is read at
  mount time and cached in memory.
  The inode lookup table is used only by the export code, inode disk
  locations are directly encoded in directories, enabling direct access
  without an intermediate lookup for all operations except the export ops.
  Look-up inode number (ino) in table, returning the inode location.
  Read uncompressed inode lookup table indexes off disk into memory
 Sanity check values 
 there should always be at least one inode 
	
	  The computed size of the lookup table (length bytes) should exactly
	  match the table start and end points
	
	  table0], table[1], ... table[indexes - 1] store the locations
	  of the compressed inode lookup blocks.  Each entry should be
	  less than the next (i.e. table[0] < table[1]), and the difference
	  between them should be SQUASHFS_METADATA_SIZE or less.
	  table[indexes - 1] should  be less than lookup_table_start, and
	  again the difference should be SQUASHFS_METADATA_SIZE or less
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009
  Phillip Lougher <phillip@squashfs.org.uk>
  decompressor.c
  This file (and decompressor.h) implements a decompressor framework for
  Squashfs, allowing multiple decompressors to be easily supported
	
	  Read decompressor specific options from file system if present
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2010 LG Electronics
  Chan Jeong <chan.jeong@lge.com>
  lzo_wrapper.c
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008
  Phillip Lougher <phillip@squashfs.org.uk>
  id.c
  This file implements code to handle uids and gids.
  For space efficiency regular files store uid and gid indexes, which are
  converted to 32-bit uidsgids using an id look up table.  This table is
  stored compressed into metadata blocks.  A second index table is used to
  locate these.  This second index table for speed of access (and because it
  is small) is read at mount time and cached in memory.
  Map uidgid index into real 32-bit uidgid using the id look up table
  Read uncompressed id lookup table indexes from disk into memory
 Sanity check values 
 there should always be at least one id 
	
	  The computed size of the index table (length bytes) should exactly
	  match the table start and end points
	
	  table[0], table[1], ... table[indexes - 1] store the locations
	  of the compressed id blocks.   Each entry should be less than
	  the next (i.e. table[0] < table[1]), and the difference between them
	  should be SQUASHFS_METADATA_SIZE or less.  table[indexes - 1]
	  should be less than id_table_start, and again the difference
	  should be SQUASHFS_METADATA_SIZE or less
 SPDX-License-Identifier: GPL-2.0-or-later
  Squashfs - a compressed read only filesystem for Linux
  Copyright (c) 2010
  Phillip Lougher <phillip@squashfs.org.uk>
  xattr_id.c
  This file implements code to map the 32-bit xattr id stored in the inode
  into the on disk location of the xattr data.
  Map xattr id using the xattr id look up table
  Read uncompressed xattr id lookup table indexes from disk into memory
 Sanity check values 
 there is always at least one xattr id 
	
	  The computed size of the index table (len bytes) should exactly
	  match the table start and end points
	 table[0], table[1], ... table[indexes - 1] store the locations
	  of the compressed xattr id blocks.  Each entry should be less than
	  the next (i.e. table[0] < table[1]), and the difference between them
	  should be SQUASHFS_METADATA_SIZE or less.  table[indexes - 1]
	  should be less than table_start, and again the difference
	  shouls be SQUASHFS_METADATA_SIZE or less.
	 
	  Finally xattr_table_start should be less than table[0].
 SPDX-License-Identifier: GPL-2.0
  namei.c
  Copyright (c) 1999 Al Smith
  Portions derived from work (c) 1995,1996 Christian Vogelgsang.
 SPDX-License-Identifier: GPL-2.0
  super.c
  Copyright (c) 1999 Al Smith
  Portions derived from work (c) 1995,1996 Christian Vogelgsang.
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
aeschi.ch.eu.orgefs\n");
 shuts up gcc 
		
		  assume that we're dealing with a partition and allow
		  read_super() to try and detect a valid superblock
		  on the next block.
 read the vh (volume header) block 
	
	  if this returns zero then we didn't find any partition table.
	  this isn't (yet) an error - just assume for the moment that
	  the device is valid and go on to search for a superblock.
 efs magic number 
 blocksize 
 total data blocks 
 free data blocks 
 free blocks for non-root 
 total inodes 
 free inodes 
 max filename length 
 SPDX-License-Identifier: GPL-2.0-only
  inode.c
  Copyright (c) 1999 Al Smith
  Portions derived from work (c) 1995,1996 Christian Vogelgsang,
               and from work (c) 1998 Mike Shaver.
	
	  this is slightly evil. it doesn't just copy
	  efs_extent from src to dst, it also mangles
	  the bits so that dst ends up in cpu byte-order.
	
	 EFS layout:
	
	 |   cylinder group    |   cylinder group    |   cylinder group ..etc
	 |inodes|data          |inodes|data          |inodes|data       ..etc
	
	 work out the inode block index, (considering initially that the
	 inodes are stored as consecutive blocks). then work out the block
	 number of that inode given the above layout, and finally the
	 offset of the inode within that block.
 this is the number of blocks in the file 
 get the number of extents for this object 
 copy the extents contained within the inode to memory 
	
	  given an extent and a logical block within a file,
	  can this block be found within this extent ?
 first check the last extent we returned 
 if we only have one extent then nothing can be found 
		
		  check the stored extents in the inode
		  start with next extent and check forwards
		
		  work out which direct extent contains `cur'.
		 
		  also compute ibase: i.e. the number of the first
		  indirect extent contained within direct extent `cur'.
		 
 should never happen 
 work out block number and offset of this indirect extent 
 SPDX-License-Identifier: GPL-2.0
  dir.c
  Copyright (c) 1999 Al Smith
 work out where this entry can be found 
 each block contains at most 256 slots 
 look at all blocks 
 read the dir block 
 found the next entry 
 sanity check 
 copy filename and data in dirslot 
 SPDX-License-Identifier: GPL-2.0
  file.c
  Copyright (c) 1999 Al Smith
  Portions derived from work (c) 1995,1996 Christian Vogelgsang.
		
		  i have no idea why this happens as often as it does
 are we about to read past the end of a file ? 
		
		  i have no idea why this happens as often as it does
 SPDX-License-Identifier: GPL-2.0
  symlink.c
  Copyright (c) 1999 Al Smith
  Portions derived from work (c) 1995,1996 Christian Vogelgsang.
 read first 512 bytes of link target 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2006  NEC Corporation
  Created by KaiGai Kohei <kaigai@ak.jp.nec.com>
  For licensing information, see the file 'LICENCE' in this directory.
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2006  NEC Corporation
  Created by KaiGai Kohei <kaigai@ak.jp.nec.com>
  For licensing information, see the file 'LICENCE' in this directory.
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
	 Later, this will provide for lsattr.jffs2 and chattr.jffs2, which
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 Duplicate. Free one 
				 It may have been a 'placeholder' deletion dirent, 
 We know frag->ofs <= size. That's what lookup does for us 
 Sanity check for truncation to longer than we started with... 
	 If the last fragment starts at the RAM page boundary, it is
 The node has no valid frags left. It's totally obsoleted 
  Allocate and initializes a new fragment.
  Called when there is no overlapping fragment exist. Inserts a hole before the new
  fragment and inserts the new fragment to the fragtree.
 put a hole in before the new fragment 
			 By definition, the 'this' node has no right-hand child,
			   because there are no frags with offset greater than it.
		 By definition, the 'this' node has no right-hand child,
		   because there are no frags with offset greater than it.
 Doesn't set inode->i_size 
 Skip all the nodes which are completed before this one starts 
 See if we ran off the end of the fragtree 
 We did 
		 Check if 'this' node was on the same page as the new node.
		   If so, both 'this' and the new node get marked REF_NORMAL so
		   the GC can take a look.
	 OK. 'this' is pointing at the first frag that newfrag->ofs at least partially obsoletes,
	  - i.e. newfrag->ofs < this->ofs+this->size && newfrag->ofs >= this->ofs
 This node isn't completely obsoleted. The start of it remains valid 
		 Mark the new node and the partially covered node REF_NORMAL -- let
 The new node splits 'this' frag into two 
 New second frag pointing to this's node 
 Adjust size of original 'this' 
			 Now, we know there's no node with offset
			   greater than this->ofs but smaller than
			   newfrag2->ofs or newfrag->ofs, for obvious
			   reasons. So we can do a tree insert from
			   'this' to insert newfrag, and a tree insert
 New node just reduces 'this' frag in size, doesn't split it 
 Again, we know it lives down here in the tree 
		 New frag starts at the same point as 'this' used to. Replace
	 OK, now we have newfrag added in the correct place in the tree, but
	   frag_next(newfrag) may be a fragment which is overlapped by it
 'this' frag is obsoleted completely. 
	 Now we're pointing at the first frag which isn't totally obsoleted by
 Still some overlap but we don't need to move it in the tree 
 And mark them REF_NORMAL so the GC takes a look at them 
  Given an inode, probably with existing tree of fragments, add the new node
  to the fragment tree.
	 If we now share a page with other nodes, mark either previous
 If we don't start at zero there's _always_ a previous 
 During mount, this needs no locking. During normal operation, its
   callers want to do other stuff while still holding the inocache_lock.
   Rather than introducing special case get_ino_cache functions or
	 Free it now unless it's in READING or CLEARING state, which
	   are the transitions upon read_inode() and clear_inode(). The
	   rest of the time we know nobody else is looking at it, and
	   if it's held by read_inode() or clear_inode() they'll free it
	 The common case in lookup is that there will be a node
 Remember the closest smaller match on the way down 
	 Exact match not found. Go back up looking at each parent,
 Pass 'c' argument to indicate that nodes should be marked obsolete as
			 Not a hole, and it's the final remaining frag
 Set (and test) __totlen field... for now 
 No locking, no reservation of 'ref'. Do not use on a live file system 
 REF_EMPTY_NODE is !obsolete, so that works OK 
 Calculate totlen from surrounding nodes or eraseblock 
 Last node in block. Use free_space 
 TEST_TOTLEN 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 Linux 
 __ECOS 
 Erase failed immediately. Refile it on the list 
 Be nice 
 Wake the GC thread to mark them clean 
	 For NAND, if the failure did not occur at the device level for a
		 We had a device-level failure to erase.  Let's see if we've
 We'd like to give this block another try. 
 Hmmm. Maybe we should accept the extra space it takes and make
 Walk the inode's list once, removing any nodes from this eraseblock 
			 We're looking at the jffs2_inode_cache, which is
			   at the end of the linked list. Stash it and continue
 It's in the block we're erasing 
 Not to be deleted. Skip 
 PARANOIA 
 else it was a non-inode node or already removed, so don't bother 
 Don't muck about if it won't let us point to the whole erase sector 
 It's OK. We know it's properly aligned 
 Write the erase complete marker 
 Cleanmarker in oob area or no cleanmarker at all ? 
 Everything else got zeroed before the erase 
 Account for cleanmarker now, if it's in-band 
 Stick it back on the list from whence it came and come back later 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 should never happen; programmer error 
	 We don't care about i_generation. We'll destroy the flash
	   before we start re-using inode numbers anyway. And even
  JFFS2 mount options.
  Opt_source: The source device
  Opt_override_compr: override default compressor
  Opt_rp_size: size of reserved pool in KiB
  fill in the superblock
	 Initialize JFFS2 superblock locks, the further initialization will
	 Paranoia checks for on-medium structures. If we ask GCC
	   to pack them with __attribute__((packed)) then it _also_
	   assumes that they're not aligned -- so it emits crappy
	   code on some architectures. Ideally we want an attribute
	   which means just 'no padding', without the alignment
	   thing. But GCC doesn't have that -- we have to just
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 Actually dual-licensed, but it doesn't matter for
 the sake of this tag. It's Free Software.
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 Will be overwritten shortly for directories 
 jffs2_write_dnode - given a raw_inode, allocate a full_dnode for it,
 check number of valid vecs 
 Mark the space as dirtied 
			 Don't change raw->size to match retlen. We may have
			   written the node header already, and only the data will
			   seem corrupted, in which case the scan would skip over
			   any node we write before the original intended end of
 Try to reallocate space and retry 
 Locking pain 
 Release the full_dnode which is now useless, and return 
 Mark the space used 
	 If node covers at least a whole page, or if it starts at the
	   beginning of a page and runs to the end of the file, or if
	   it's a hole node, mark it REF_PRISTINE, else REF_NORMAL.
 Release the full_dnode which is now useless, and return 
		 This should never happen, but seems to have done on at least one
dev.laptop.orgticket4184 
 Mark the space as dirtied 
 Try to reallocate space and retry 
 Locking pain 
 Release the full_dnode which is now useless, and return 
 Mark the space used 
 Release the full_dirent which is now useless, and return 
 The OS-specific code fills in the metadata in the jffs2_raw_inode for us, so that
   we don't have to go digging in struct inode or its equivalent. It should set:
 Write error to be retried 
 Eep 
	 Try to reserve enough space for both node and dirent.
	  Just the node will do for now, though
 Eeek. Wave bye bye 
	 No data here. Only a metadata node, which will be
	   obsoleted by the first data write
 Eep. 
 Argh. Now we treat it like a normal delete 
		 dirent failed to write. Delete the inode normally
	 Link the fd into the inode's list, obsoleting an old
 We can't mark stuff obsolete on the medium. We need to write a deletion dirent 
 Build a deletion node 
 File it. This will mark the old one obsolete. 
		 We don't actually want to reserve any space, but we do
				 We don't want to remove it from the list immediately,
				   because that screws up getdents()seek() semantics even
				   more than they're screwed already. Turn it into a
 dead_f is NULL if this was a rename not a real unlink 
	 Also catch the !f->inocache case, where there was a dirent
 There can be only deleted ones 
 NB: Caller must set inode nlink if appropriate 
 Build a deletion node 
 File it. This will mark the old one obsolete. 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2006  NEC Corporation
  Created by KaiGai Kohei <kaigai@ak.jp.nec.com>
  For licensing information, see the file 'LICENCE' in this directory.
 -------- xdatum related functions ----------------
  xattr_datum_hashkey(xprefix, xname, xvalue, xsize)
    is used to calcurate xdatum hashkey. The reminder of hashkey into XATTRINDEX_HASHSIZE is
    the index of the xattr namevalue pair cache (c->xattrindex).
  is_xattr_datum_unchecked(c, xd)
    returns 1, if xdatum contains any unchecked raw nodes. if all raw nodes are not
    unchecked, it returns 0.
  unload_xattr_datum(c, xd)
    is used to release xattr namevalue pair and detach from c->xattrindex.
  reclaim_xattr_datum(c)
    is used to reclaim xattr namevalue pairs on the xattr namevalue pair cache when
    memory usage by cache is over c->xdatum_mem_threshold. Currently, this threshold
    is hard coded as 32KiB.
  do_verify_xattr_datum(c, xd)
    is used to load the xdatum informations without namevalue pair from the medium.
    It's necessary once, because those informations are not collected during mounting
    process when EBS is enabled.
    0 will be returned, if success. An negative return value means recoverable error, and
    positive return value means unrecoverable error. Thus, caller must remove this xdatum
    and xref when it returned positive value.
  do_load_xattr_datum(c, xd)
    is used to load namevalue pair from the medium.
    The meanings of return value is same as do_verify_xattr_datum().
  load_xattr_datum(c, xd)
    is used to be as a wrapper of do_verify_xattr_datum() and do_load_xattr_datum().
    If xd need to call do_verify_xattr_datum() at first, it's called before calling
    do_load_xattr_datum(). The meanings of return value is same as do_verify_xattr_datum().
  save_xattr_datum(c, xd)
    is used to write xdatum to medium. xd->version will be incremented.
  create_xattr_datum(c, xprefix, xname, xvalue, xsize)
    is used to create new xdatum and write to medium.
  unrefer_xattr_datum(c, xd)
    is used to delete a xdatum. When nobody refers this xdatum, JFFS2_XFLAGS_DEAD
    is set on xd->flags and chained xattr_dead_list or release it immediately.
    In the first case, the garbage collector release it later.
 must be called under down_write(xattr_sem) 
 must be called under down_write(xattr_sem) 
 20% reduction 
 must be called under down_write(xattr_sem) 
 unchecked xdatum is chained with c->xattr_unchecked 
 must be called under down_write(xattr_sem) 
	 must be called under down_write(xattr_sem);
	  rc < 0 : recoverable error, try again
	  rc = 0 : success
	  rc > 0 : Unrecoverable error, this node should be deleted.
 must be called under down_write(xattr_sem) 
 Setup raw-xattr 
 success 
 must be called under down_write(xattr_sem) 
 Search xattr_datum has same xnamexvalue by index 
 Not found, Create NEW XATTR-Cache 
 Insert Hash Index 
 must be called under down_write(xattr_sem) 
 -------- xref related functions ------------------
  verify_xattr_ref(c, ref)
    is used to load xref information from medium. Because summary data does not
    contain xidino, it's necessary to verify once while mounting process.
  save_xattr_ref(c, ref)
    is used to write xref to medium. If delete marker is marked, it write
    a delete marker of xref into medium.
  create_xattr_ref(c, ic, xd)
    is used to create a new xref and write to medium.
  delete_xattr_ref(c, ref)
    is used to delete jffs2_xattr_ref. It marks xref XREF_DELETE_MARKER,
    and allows GC to reclaim those physical nodes.
  jffs2_xattr_delete_inode(c, ic)
    is called to remove xrefs related to obsolete inode when inode is unlinked.
  jffs2_xattr_free_inode(c, ic)
    is called to release xattr related objects when unmounting. 
  check_xattr_ref_inode(c, ic)
    is used to confirm inode does not have duplicate xattr namevalue pair.
  jffs2_xattr_do_crccheck_inode(c, ic)
    is used to force xattr data integrity check during the initial gc scan.
 obsolete node 
 must be called under down_write(xattr_sem) 
 success 
 must be called under down_write(xattr_sem) 
 Chain to inode 
 success 
 must be called under down_write(xattr_sem) 
	 It's called from jffs2_evict_inode() on inode removing.
 It's called from jffs2_free_ino_caches() until unmounting FS. 
	 success of check_xattr_ref_inode() means that inode (ic) dose not have
	  duplicate namevalue pairs. If duplicate namevalue pair would be found,
	  one will be removed.
 -------- xattr subsystem functions ---------------
  jffs2_init_xattr_subsystem(c)
    is used to initialize semaphore and list_head, and some variables.
  jffs2_find_xattr_datum(c, xid)
    is used to lookup xdatum while scanning process.
  jffs2_clear_xattr_subsystem(c)
    is used to release any xattr related objects.
  jffs2_build_xattr_subsystem(c)
    is used to associate xdatum and xref while super block building process.
  jffs2_setup_xattr_datum(c, xid, version)
    is used to insert xdatum while scanning process.
 Default 32KB 
 It's only used in scanningbuilding process. 
 Phase.1 : Merge same xref 
 Phase.2 : Bind xref with inode_cache and xattr_datum 
			 At this point, ref->xid and ref->ino contain XID and inode number.
 Phase.3 : Link unchecked xdatum to xattr_unchecked list 
 build complete 
 -------- xattr subsystem functions ---------------
  xprefix_to_handler(xprefix)
    is used to translate xprefix into xattr_handler.
  jffs2_listxattr(dentry, buffer, size)
    is an implementation of listxattr handler on jffs2.
  do_jffs2_getxattr(inode, xprefix, xname, buffer, size)
    is an implementation of getxattr handler on jffs2.
  do_jffs2_setxattr(inode, xprefix, xname, buffer, size, flags)
    is an implementation of setxattr handler on jffs2.
 xdatum is unchached 
 xdatum is unchached 
 Find existing xattr 
 not found 
 create xattr_ref 
 -------- garbage collector functions -------------
  jffs2_garbage_collect_xattr_datum(c, xd, raw)
    is used to move xdatum into new node.
  jffs2_garbage_collect_xattr_ref(c, ref, raw)
    is used to move xref into new node.
  jffs2_verify_xattr(c)
    is used to call do_verify_xattr_datum() before garbage collecting.
  jffs2_release_xattr_datum(c, xd)
    is used to release an in-memory object of xdatum.
  jffs2_release_xattr_ref(c, ref)
    is used to release an in-memory object of xref.
 must be called under spin_lock(&c->erase_completion_lock) 
 must be called under spin_lock(&c->erase_completion_lock) 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 Called with erase_completion_lock held 
	 Pick an eraseblock to garbage collect next. This is where we'll
	 We possibly want to favour the dirtier blocks more when the
		 Note that most of them will have gone directly to be erased.
 Most of the time, pick one off the very_dirty list 
 There are blocks are wating for the wbuf sync 
 Eep. All were empty 
 Have we accidentally picked a clean block with wasted space ? 
 jffs2_garbage_collect_pass
  Make a single attempt to progress GC. Move one node, and possibly
  start erasing one eraseblock.
		 We can't start doing GC until we've finished checking
		 Instead of doing the inodes in numeric order, doing a lookup
		  in the hash for each possible number, just walk the hash
		  buckets of existing inodes. This means that we process
		  them out-of-order, but it can be a lot faster if there's
 with inocache_lock held 
 Point c->check_ino past the end of the last bucket. 
		 For next time round the loop, we want c->checked_ino to indicate
		  the next one we want to check. And since we're walking the
			 We need to wait for it to finish, lest we move on
			   and trigger the BUG() above while we haven't yet
			 We need to come back again for the _same_ inode. We've
 If there are any blocks which need erasing, erase them now 
 First, work out which block we're garbage-collecting 
 Couldn't find a free block. But maybe we can just erase one and make 'progress'? 
 Inode-less node. Clean marker, snapshot or something like that 
 It's an unknown node with JFFS2_FEATURE_RWCOMPAT_COPY 
 Just mark it obsolete 
	 When 'ic' refers xattr_datumxattr_ref, this node is GCed as xattr.
	 We need to hold the inocache. Either the erase_completion_lock or
	   the inocache_lock are sufficient; we trade down since the inocache_lock
	 Three possibilities:
	   1. Inode is already in-core. We must iget it and do proper
	      updating to its fragtree, etc.
	   2. Inode is not in-core, node is REF_PRISTINE. We lock the
	      inocache to prevent a read_inode(), copy the node intact.
	   3. Inode is not in-core, node is not pristine. We must iget()
	      and take the slow path.
		 It's been checked, but it's not currently in-core.
		   We can just copy any pristine nodes, but have
		   to prevent anyone else from doing read_inode() while
 It's in-core. GC must iget() it. 
		 Should never happen. We should have finished checking
		   by the time we actually start doing any GC, and since
		   we're holding the alloc_sem, no other garbage collection
		   can happen.
		 Someone's currently trying to read it. We must wait for
		   them to finish and then go through the full iget() route
		   to do the GC. However, sometimes read_inode() needs to get
		   the alloc_sem() (for marking nodes invalid) so we must
		 And because we dropped the alloc_sem we must start again from the
		   beginning. Ponder chance of livelock here -- we're returning success
		   without actually making any progress.
		   Q: What are the chances that the inode is back in INO_STATE_READING
		   again by the time we next enter this function? And that this happens
		   enough times to cause a real delay?
		   A: Small enough that I don't care :)
	 OK. Now if the inode is in state INO_STATE_GC, we are going to copy the
	   node intact, and we don't have to muck about with the fragtree etc.
	   because we know it's not in-core. If it _was_ in-core, we go through
 Fall through if it wanted us to, with inocache_lock held 
	 Prevent the fairly unlikely race where the gcblock is
	   entirely obsoleted by the final close of a file which had
	   the only valid nodes in the block, followed by erasure,
	   followed by freeing of the ic because the erased block(s)
	   held _all_ the nodes of that inode.... never been seen but
 Eep. This really should never happen. GC is broken 
 If we've finished this block, start it erasing 
 We're GC'ing an empty block? 
	 Now we have the lock for this inode. Check that it's still the one at the head
 They'll call again 
 OK. Looks safe. And nobody can get us now because we have the semaphore. Move the block 
 FIXME. Read node and do lookup? 
 We've found them all 
 Urgh. Return it sensibly. 
 We found a datanode. Do the GC 
 It crosses a page boundary. Therefore, it must be a hole. 
 It could still be a hole. But we GC the page this way anyway 
 Wasn't a dnode. Try dirent 
	 Ask for a small amount of space (or the totlen if smaller) because we
	   don't want to force wastage of the end of a block if splitting would
 'rawlen' is not the exact summary size; it is only an upper estimation 
 Doesn't fit untouched. We'll go the old route and split it 
 If it's inode-less, we don't _know_ what it is. Just copy it intact 
 OK, all the CRCs are good; this node can just be copied as-is. 
 Try to reallocate space and retry 
						 this is not the exact summary size of it,
 For these, we don't actually need to read the old node 
		 Fetch the inode length from the fragtree rather then
	 If the times on this inode were set by explicit utime() they can be different,
	 On a medium where we can't actually mark nodes obsolete
	   pernamently, such as NAND flash, we need to work out
	   whether this deletion dirent is still needed to actively
	   delete a 'real' dirent with the same name that's still
		 Prevent the erase code from nicking the obsolete node refs while
		   we're looking at them. I really don't like this extra lock but
 We only care about obsolete ones 
 Any dirent with the same name is going to have the same length... 
			 Doesn't matter if there's one in the same erase block. We're going to
			 This is an obsolete node belonging to the same directory, and it's of the right
 If we can't read it, we don't need to continue to obsolete it. Continue 
 If the name CRC doesn't match, skip 
 If the name length doesn't match, or it's another deletion dirent, skip 
 OK, check the actual name now 
			 OK. The name really does match. There really is still an older node on
			   the flash which our deletion dirent obsoletes. So we have to write out
	 FIXME: If we're deleting a dirent which contains the current mtime and ctime,
 No need for it any more. Just mark it obsolete and remove it from the list 
		 It's partially obsoleted by a later write. So we have to
 FIXME: We could possibly deal with this by writing new holes for each frag 
		 Fetch the inode length from the fragtree rather then
	
	  We should only get here in the case where the node we are
	  replacing had more than one frag, so we kept the same version
	  number as before. (Except in case of error -- see 'goto fill;'
	  above.)
 This is a partially-overlapped hole node. Mark it REF_NORMAL not REF_PRISTINE 
		 Attempt to do some merging. But only expand to cover logically
		   adjacent frags if the block containing them is already considered
		   to be dirty. Otherwise we end up with GC just going round in
		   circles dirtying the nodes it already wrote out, especially
		   on NAND where we have small eraseblocks and hence a much higher
 BUG_ON(!frag) but that'll happen anyway... 
 First grow down... 
			 If the previous frag doesn't even reach the beginning, there's
 OK. This frag holds the first byte of the page. 
				 OK, it's a frag which extends to the beginning of the page. Does it live
				   in a block which is still considered clean? If so, don't obsolete it.
 ... then up 
 Find last frag which is actually part of the node we're to GC. 
			 If the previous frag doesn't even reach the beginning, there's lots
				 OK, it's a frag which extends to the beginning of the page. Does it live
				   in a block which is still considered clean? If so, don't obsolete it.
	 The rules state that we must obtain the page lock before f->sem, so
	  drop f->sem temporarily. Since we also hold c->alloc_sem, nothing's
	  actually going to change so we're safe; we only allow reading.
	 
	  It is important to note that jffs2_write_begin() will ensure that its
	  page is marked Uptodate before allocating space. That means that if we
	  end up here trying to GC the same page that jffs2_write_begin() is
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 These are initialised to NULL in the kernel startup code.
 If jeb->last_node is really a valid node then skip over it 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004 Thomas Gleixner <tglx@linutronix.de>
  Created by David Woodhouse <dwmw2@infradead.org>
  Modified debugged and enhanced by Thomas Gleixner <tglx@linutronix.de>
  For licensing information, see the file 'LICENCE' in this directory.
 For testing write failures 
 max. erase failures before we mark a block bad 
 If a malloc failed, consider _everything_ dirty 
 If ino == 0, _any_ non-GC writes mean 'yes' 
 Look to see if the inode in question is pending in the wbuf 
 Schedule delayed write-buffer write-out 
			 Most of the time, we just erase it immediately. Otherwise we
			 Sometimes, however, we leave it elsewhere so it doesn't get
 File the existing block on the bad_used_list.... 
 Not sure this should ever happen... need more coffee 
 It has to have had some nodes or we couldn't be here 
 convert to wasted 
 Find a frag which refers to the full_dnode we want to modify 
 Recover from failure to write wbuf. Recover the nodes up to the
	 Find the first node to be recovered, by skipping over every
 All nodes were obsolete. Nothing to recover. 
 Count the number of refs which need to be copied 
		 First affected node was already partially written.
 Do the read... 
 ECC recovered ? 
 If this was the only node to be recovered, give up 
 It wasn't. Go on and try to recover nodes complete in the wbuf 
 Read succeeded. Copy the remaining data from the wbuf 
	 OK... we're to rewrite (end-start) bytes of data from first_raw onwards.
 ... and get an allocation of space from a shiny new block instead 
 The summary is not recovered, so it must be disabled for this erase block 
		 Need to do another write immediately, but it's possible
		   that this is just because the wbuf itself is completely
		   full, and there's nothing earlier read back from the
		   flash. Hence 'buf' isn't necessarily what we're writing
 Argh. We tried. Really we did. 
 Don't muck about with c->wbuf_inodes. False positives are harmless. 
 OK, now we're left with the dregs in whichever buffer we're using 
 Now sort out the jffs2_raw_node_refs, moving them from the old to the next block 
 Ick. This XATTR mess should be fixed shortly... 
 Remove the old node from the per-inode list 
				 If it's an in-core inode, then we have to adjust any
				   full_dirent or full_dnode structure to point to the
 Should never happen; it _must_ be present 
				 We don't lock f->sem. There's a number of ways we could
				   end up in here with it already being locked, and nobody's
				   going to modify it on us anyway because we hold the
				   alloc_sem. We're only changing one ->raw pointer too,
 Fix up the original jeb now it's on the bad_list 
 Meaning of pad argument:
   0: Do not pad. Probably pointless - we only ever use this when we can't pad anyway.
   1: Pad, do not adjust nextblock free_size
   2: Pad, adjust nextblock free_size
	 Nothing to do if not write-buffering the flash. In particular, we shouldn't
 already checked c->wbuf above 
	 claim remaining space on the page
	   this happens, if we have a change to a new block,
	   or if fsync forces us to flush the writebuffer.
	   if we have a switch to next page, we will not have
	   enough remaining space for this.
		 Pad with JFFS2_DIRTY_BITMASK initially.  this helps out ECC'd NOR
	 else jffs2_flash_writev has actually filled in the rest of the
 Adjust free size of the block if we padded. 
		 wbuf_pagesize - wbuf_len is the amount of space that's to be
		   padded. If there is less free space in the block than that,
 FIXME: that made it count as dirty. Convert to wasted 
 Stick any now-obsoleted blocks on the erase_pending_list 
 adjust write buffer offset, else we get a non contiguous write bug 
 Trigger garbage collection to flush the write-buffer.
   If ino arg is zero, do it if _any_ real (i.e. not GC) writes are
   outstanding. If ino arg non-zero, do it only if a write for the
 GC won't make any progress for a while 
		 retry flushing wbuf in case jffs2_wbuf_recover
 GC failed. Flush it with padding instead 
			 retry flushing wbuf in case jffs2_wbuf_recover
 Pad write-buffer to end and write it, wasting space. 
 retry - maybe wbuf recover left some data in wbuf. 
 If not writebuffered flash, don't bother 
 If wbuf_ofs is not initialized, set it to target address 
	
	  Sanity checks on target address.  It's permitted to write
	  at PAD(c->wbuf_len+c->wbuf_ofs), and it's permitted to
	  write at the beginning of a new erase block. Anything else,
	  and you die.  New block starts at xxx000c (0-b = block
	  header)
 It's a write to a new block 
 set pointer to new block 
 We're not writing immediately after the writebuffer. Bad. 
 adjust alignment offset 
 take care of alignment to next page 
	
	  If there's a remainder in the wbuf and it's a non-GC write,
	  remember that the wbuf affects this ino
	
	  At this point we have no problem, c->wbuf is empty. However
	  refile nextblock to avoid writing again to same address.
 	This is the entry for flash write.
 	Check, if we work on NAND FLASH, if so build an kvec and write it via vritev
	Handle readback from writebuffer and ECC failure return
 Read flash 
		
		  We have the raw data without ECC correction in the buffer,
		  maybe we are lucky and all data or parts are correct. We
		  check the node.  If data are corrupted node check will sort
		  it out.  We keep this block, it will fail on write or erase
		  and the we mark it bad. Or should we do that now? But we
		  should give him a chance.  Maybe we had a system crash or
		  power loss before the ecc write or a erase was completed.
		  So we return success. :)
 if no writebuffer available or write buffer empty, return 
 if we read in a different block, return 
 offset in write buffer 
 is read beyond write buffer ? 
 number of bytes to copy 
 offset in read buffer 
 is write beyond write buffer ? 
 number of bytes to copy 
 For historical reasons we use only 8 bytes for OOB clean marker 
  Check, if the out of band area is empty. This function knows about the clean
  marker and if it is present in OOB, treats the OOB as empty anyway.
 Yeah, we know about the cleanmarker 
  Check for a valid cleanmarker.
  Returns: 0 if a valid cleanmarker was found
 	    1 if no cleanmarker was found
 	    negative error code if an error occurred
  On NAND we try to mark this block bad. If the block was erased more
  than MAX_ERASE_FAILURES we mark it finally bad.
  Don't care about failures. This block remains on the erase-pending
  or badblock list as long as nobody manipulates the flash with
  a bootloader or something like that.
 if the count is < max, we try to write the counter to the 2nd page oob area 
 Cleanmarker is out-of-band, so inline size zero 
 Initialise write buffer 
 No cleanmarkers needed 
 Initialize write buffer 
	 Find a suitable c->sector_size
	  - Not too much sectors
	  - Sectors have to be at least 4 K + some bytes
	  - All known dataflashes have erase sizes of 528 or 1056
	  - we take at least 8 eraseblocks and want to have at least 8K size
	  - The concatenation should be a power of 2
 It may be necessary to adjust the flash size 
	 Cleanmarker currently occupies whole programming regions,
 Initialize write buffer 
 We do not need write-buffer 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 These helper functions _must_ increase ofs and also do the dirtyused space accounting.
  Returning an error will abort the mount - bad checksums etc. should just mark the space
  as dirty.
	 Turned wasted size into dirty, since we apparently 
 summary info collected by the scan process 
 Don't muck about if it won't let us point to the whole flash 
		 For NAND it's quicker to read a whole eraseblock at a time,
 reset summary info for next eraseblock scan 
 Now decide which list to put it on 
			
			  Empty block.   Since we can't be sure it
			  was entirely erased, we just queue it for erase
			  again.  It will be marked as such when the erase
			  is complete.  Meanwhile we still count it as empty
			  for later checks.
 Only a CLEANMARKER node is valid 
 It's actually free 
 Dirt 
 Full (or almost full) of clean data. Clean list 
 Some data, but not full. Dirty list. 
			 We want to remember the block with most free space
 Better candidate for the next writes to go to 
 deleting summary information of the old nextblock 
 update collected summary information for the current nextblock 
 Nothing valid - not even a clean marker. Needs erasing. 
 For now we just put it on the erasing list. We'll start the erases later 
 Nextblock dirty is always seen as wasted, because we cannot recycle it now 
		 If we're going to start writing into a block which already
		   contains data, and the end of the data isn't page-aligned,
 move blocks with max 4 byte dirty space to cleanlist 
	 BEFORE jffs2_build_xattr_subsystem() called, 
	  and AFTER xattr_ref is marked as a dead xref,
	  ref->xid is used to store 32bit xid, xd is not used
	  ref->ino is used to store 32bit inode-number, ic is not used
	  Thoes variables are declared as union, thus using those
	  are exclusive. In a similar way, ref->next is temporarily
	  used to chain all xattr_ref object. It's re-chained to
	  jffs2_inode_cache in jffs2_build_xattr_subsystem() correctly.
 Called with 'buf_size == 0' if buf is in fact a pointer _directly_ into
		 Even if it's not found, we still scan to see
		   if the block is empty. We use this information
 XIP case. Just look, point at the summary if it's there 
 If NAND flash, read a whole page of it. Else just the end 
 Read as much as we want into the _end_ of the preallocated buffer 
 sm->offset maybe wrong but MAGIC maybe right 
 Now, make sure the summary itself is available 
 Need to kmalloc for this. 
 Need to read more so that the entire summary node is present 
			 If it returns with a real error, bail. 
			   If it returns positive, that's a block classification
			   (i.e. BLK_STATE_xxx) so return that too.
 This is the XIP case -- we're reading _directly_ from the flash chip 
 We temporarily use 'ofs' as a pointer into the bufferjeb 
 Scan only EMPTY_SCAN_SIZE of 0xFF before declaring it's empty 
 scan oob, take care of cleanmarker 
 don't bother with re-erase 
 OK to erase if all blocks are like this 
 Now ofs is a complete physical flash offset as it always was... 
 Make sure there are node refs available for use 
 Ran off end. 
			 If we're only checking the beginning of a block with a cleanmarker,
 XIPpoint case 
 See how much more there is to read in this eraseblock... 
				 No more to read. Break out of main loop without marking
 point never reaches here 
 OK. We're out of possibilities. Whinge and move on 
 We seem to have a node of sorts. Check the CRC 
 Eep. Node goes over the end of the erase block. 
 Wheee. This is an obsoleted node 
 CONFIG_JFFS2_FS_XATTR 
 We can't summarise nodes we don't grok 
 mark_node_obsolete can add to wasted !! 
	 We do very little here now. Just check the ino# to which we should attribute
	   this node; we can do all the CRC checking etc. later. There's a tradeoff here --
	   we used to scan the flash once only, reading everything we want from it into
	   memory, then building all our in-core data structures and freeing the extra
	   information. Now we allow the first part of the mount to complete a lot quicker,
	   but we have to go _back_ to the flash in order to finish the CRC checking, etc.
	   Which means that the _full_ amount of time to get to proper write mode with GC
 Check the node CRC in any case. 
		
		  We believe totlen because the CRC on the node
		  _header_ was OK, just the node itself failed.
 Wheee. It worked 
	 We don't get here unless the node is still valid, so we don't have to
 We believe totlen because the CRC on the node _header_ was OK, just the node itself failed. 
 Should never happen. Did. (OLPC trac #4184)
 FIXME: Why do we believe totlen? 
 We believe totlen because the CRC on the node _header_ was OK, just the name failed. 
 Note: This breaks if list_empty(head). I don't care. You
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 We keep the dirent list sorted in increasing order of name hash,
   and we use the same hash function as the dentries. Makes this
   nice and simple
 The 'nhash' on the fd_list is not the same as the dentry hash 
 NB: The 2.2 backport will need to explicitly check for '.' and '..' here 
 First loop: curofs = 2; pos = 2 
	 jffs2_do_create() will want to lock it, _after_ reserving
	   space and taking c-alloc_sem. If we keep it locked here,
	   lockdep gets unhappy (although it's a false positive;
	   nothing else will be looking at this inode yet so there's
 Don't let people make hard links to bad inodes. 
 XXX: This is ugly 
	 FIXME: If you care. We'd need to use frags for the target
	 Try to reserve enough space for both node and dirent.
	  Just the node will do for now, though
 Eeek. Wave bye bye 
 We use f->target field to store the target path. 
	 No data here. Only a metadata node, which will be
	   obsoleted by the first data write
 Argh. Now we treat it like a normal delete 
		 dirent failed to write. Delete the inode normally
	 Link the fd into the inode's list, obsoleting an old
	 Try to reserve enough space for both node and dirent.
	  Just the node will do for now, though
 Directories get nlink 2 at start 
 but ic->pino_nlink is the parent ino# 
 Eeek. Wave bye bye 
	 No data here. Only a metadata node, which will be
	   obsoleted by the first data write
 Argh. Now we treat it like a normal delete 
		 dirent failed to write. Delete the inode normally
	 Link the fd into the inode's list, obsoleting an old
	 Try to reserve enough space for both node and dirent.
	  Just the node will do for now, though
 Eeek. Wave bye bye 
	 No data here. Only a metadata node, which will be
	   obsoleted by the first data write
 Argh. Now we treat it like a normal delete 
 XXX: This is ugly. 
		 dirent failed to write. Delete the inode normally
	 Link the fd into the inode's list, obsoleting an old
	 The VFS will check for us and prevent trying to rename a
	  file over a directory and vice versa, but if it's a directory,
	  the VFS can't check whether the victim is empty. The filesystem
	  needs to do that for itself.
	 XXX: We probably ought to alloc enough space for
	   both nodes at the same time. Writing the new link,
	   then getting -ENOSPC, is quite bad :)
 Make a hard link 
 XXX: This is ugly 
 There was a victim. Kill it off nicely 
		 Don't oops if the victim was a dirent pointing to an
	 If it was a directory we moved, and there was no victim,
 Unlink the original 
 We don't touch inode->i_nlink 
 Oh shit. We really ought to make a single node which can do both atomically 
		
		  We can't keep the target in dcache after that.
		  For one thing, we can't afford dentry aliases for directories.
		  For another, if there was a victim, we _can't_ set new inode
		  for that sucker and we have to trigger mount eviction - the
		  caller won't do it on its own since we are returning an error.
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2006  NEC Corporation
  Created by KaiGai Kohei <kaigai@ak.jp.nec.com>
  For licensing information, see the file 'LICENCE' in this directory.
 ---- Initial Security Label(s) Attachment callback --- 
 ---- Initial Security Label(s) Attachment ----------- 
 ---- XATTR Handler for "security." ----------------- 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
	 Plan: call deflate() with avail_in == sourcelen,
		avail_out = dstlen - 12 and flush == Z_FINISH.
		If it doesn't manage to finish,	call it again with
		avail_in == 0 and avail_out set to the remaining 12
		bytes for it to clean up.
	   Q: Is 12 bytes sufficient?
 Linux-only 
 __KERNEL__ 
	 If it's deflate, and it's got no preset dictionary, then
 Let this remain D1 for now -- it should never happen 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
	 There was a bug where we wrote hole nodes out with csizedsize
	 Cases:
	   Reading whole node and it's uncompressed - read directly to buffer provided, check CRC.
	   Reading whole node and it's compressed - read into comprbuf, check CRC and decompress to buffer provided
	   Reading partial node and it's uncompressed - read into readbuf, check CRC, and copy
	   Reading partial node and it's compressed - read into readbuf, check checksum, decompress to decomprbuf and copy
	 XXX FIXME: Where a single physical node actually shows up in two
	 Now we're pointing at the first frag which overlaps our page
	  (or perhaps is before it, if we've been asked to read off the
 offset within the frag to start reading 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
	 Special cases - we don't want more than one data node
	   for these types on the medium at any time. So setattr
	   must read the original data associated with the node
	   (i.e. the device numbers or the target name) and write
 For these, we don't actually need to read the old node 
 It's an extension. Make it a hole node 
		 For truncate-to-zero, treat it as deletion because
 It worked. Update the inode 
	 We have to do the truncate_setsize() without f->sem held, since
	   some pages may be locked and waiting for it in readpage().
	   We are protected from a simultaneous write() extending i_size
	   back past iattr->ia_size, because do_truncate() holds the
	 We can forget about this inode for now - drop all
	   the nodelists associated with it, etc.
 parent and '.' 
 Root dir gets i_nlink 3 for some reason 
 Read the device numbers from the media 
 Eep 
	 We stop if it was running, then restart if it needs to.
	   This also catches the case where it was stopped and this
	   is just a remount to restart it.
 jffs2_new_inode: allocate a new inode and inocache, add it to the hash,
 Set OS-specific defaults for new inodes 
	 POSIX ACLs have to be processed now, at least partly.
	
	  Pick a inocache hash size based on the size of the medium.
	  Count how many megabytes we're dealing with, apply a hashsize twice
	  that size, but rounding down to the usual big powers of 2. And keep
	  to sensible bounds.
 Do not support the MLC nand 
	
	  Size alignment check
 NAND (or other bizarre) flash... do setup accordingly 
		 The inode has zero nlink but its nodes weren't yet marked
		   obsolete. This has to be because we're still waiting for
		   the final (close() and) iput() to happen.
		   There's a possibility that the final iput() could have
		   happened while we were contemplating. In order to ensure
		   that we don't cause a new read_inode() (which would fail)
		   for the inode in question, we use ilookup() in this case
		   instead of iget().
		   The nlink can't _become_ zero at this point because we're
		   holding the alloc_sem, and jffs2_do_unlink() would also
		   need that while decrementing nlink on any inode.
 Wait for progress. Don't just loop 
		 Inode has links to it still; they're not going away because
		   jffs2_do_unlink() would need the alloc_sem and we have it.
		   Just iget() it, and if read_inode() is necessary that's OK.
 NB. This will happen again. We need to do something appropriate here. 
 NAND flash... do setup accordingly 
 and Dataflash 
 and Intel "Sibley" flash 
 and an UBI volume 
 and DataFlash 
 and Intel "Sibley" flash 
 and an UBI volume 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 Trigger GC to flush any pending writes for this inode 
 jffs2_file_inode_operations 
 FIXME: Can kmap fail? 
 Make new hole frag from old EOF to new page 
	
	  Read in the page if it wasn't already present. Cannot optimize away
	  the whole page write case until jffs2_write_end can handle the
	  case of a short-copy.
	 Actually commit the write from the page cache page we're looking at.
	  For now, we write the full page out each time. It sucks, but it's simple
	 We need to avoid deadlock with page_cache_read() in
	   jffs2_garbage_collect_pass(). So the page must be
	   up to date to prevent page_cache_read() from trying
		 When writing out the end of a page, write out the
		   _whole_ page. This helps to reduce the number of
		   nodes in files which have many short writes, like
 Set the fields that the generic jffs2_write_inode_range() code can't find 
	 In 2.4, it was already kmapped by generic_file_write(). Doesn't
 There was an error writing. 
 Adjust writtenlen for the padding we did, so we don't confuse our caller 
		 generic_file_write has written more to the page cache than we've
		   actually written to the medium. Mark the page !Uptodate so that
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by Arjan van de Ven <arjanv@redhat.com>
  For licensing information, see the file 'LICENCE' in this directory.
  Very simple lz77-ish encoder.
  Theory of operation: Both encoder and decoder have a list of "last
  occurrences" for every possible source-value; after sending the
  first source-byte, the second byte indicated the "run" length of
  matches
  The algorithm is intended to only send "whole bytes", no bit-messing.
 _compress returns the compressed size, -1 if bigger 
 We failed 
 Tell the caller how much we managed to compress, and how much space it took 
 first the verbatim copied byte 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 This must only ever be called when no GC thread is currently running 
 Wait for it... 
		 Problem - immediately after bootup, the GCD spends a lot
		  of time in places like jffs2_kill_fragtree(); so much so
		  that userspace processes (like gdm and X) are starved
		  despite plenty of cond_resched()s and renicing.  Yield()
		  doesn't help, either (presumably because userspace and GCD
		  are generally competing for a higher latency resource -
		  disk).
		  This forces the GCD to slow the hell down.   Pulling an
		  inode in with read_inode() is much preferable to having
		 Put_super will send a SIGKILL and then wait on the sem.
 We don't want SIGHUP to interrupt us. STOP and KILL are OK though. 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 kvfree() 
 More in this chain? 
 For each child, increase nlink 
 we can get high latency here with huge directories 
 Clear the icraw union so it doesn't cause problems later. 
 From this point, fd->raw is no longer used so we can set fd->ic 
		 If we appear (at this stage) to have hard-linked directories,
 Can't free scan_dents so far. We might need them in pass 2 
 Scan plan:
 - Scan physical nodes. Build map of inodesdirents. Allocate inocaches as we go
 - Scan directory tree from top down, setting nlink in inocaches
 - Scan inocaches for inodes with nlink==0
	 First, scan the medium and build all the inode caches with
 Now scan the directory tree, increasing nlink according to every dirent found. 
	 Next, scan for inodes with nlink == 0 and remove them. If
	   they were directories, then decrement the nlink of their
	   children too, and repeat the scan. As that's going to be
	   a fairly uncommon occurrence, it's not so evil to do it this
		 If we detected directory hardlinks earlier, hopefully
		  they are gone now because some of the links were from
		  dead directories which still had some old dirents lying
		  around and not yet garbage-collected, but which have
		  been discarded above. So clear the pino_nlink field
		  in each directory, so that the final scan below can
 Finally, we can scan again and free the dirent structs 
			 We do use the pino_nlink field to count nlink of
			  directories during fs build, so set it to the
			  parent ino# now. Now that there's hopefully only
					 We'll have complained about it and marked the coresponding
 We have to have set this in jffs2_build_inode_pass1() 
				 We clear ic->pino_nlink  directories' ic only if dir_hardlinks
				  is set. Otherwise, we know this should never trigger anyway, so
				  we don't do the check. And ic->pino_nlink still contains the nlink
 Should we unlink it from its previous parent? 
 For directories, ic->pino_nlink holds that parent inode # 
 Rotate the lists by some number to ensure wear levelling 
 It's a deletion dirent. Ignore it 
			 Reduce nlink of the child. If it's now zero, stick it on the
	
	   We don't delete the inocache from the hash list and free it yet.
	   The erase code will do that, when all the nodes are completely gone.
	 Deletion should almost _always_ be allowed. We're fairly
	   buggered once we stop allowing people to delete stuff
	 Be conservative about how much space we need before we allow writes.
	   On top of that which is required for deletia, require an extra 2%
	   of the medium to be available, for overhead caused by nodes being
 2% of flash size 
 And 100 bytes per eraseblock 
 ... and round up 
 When do we let the GC thread run in the background 
	 When do we allow garbage collection to merge nodes to make
	 When do we allow garbage collection to eat from bad blocks rather
c->resv_blocks_deletion + 2;
	 What number of 'very dirty' eraseblocks do we allow before we
	   trigger the GC thread even if we don't _need_ the space. When we
	   can't mark nodes obsolete on the medium, the old dirty nodes cause
	 If there's less than this amount of dirty space, don't bother
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
  Check the data CRC of the node.
  Returns: 0 if the data CRC is correct;
  	    1 - if incorrect;
 	    error code if an error occurred.
 Calculate how many bytes were already checked 
	 TODO: instead, incapsulate point() stuff to jffs2_flash_read(),
 succefully pointed to device 
		 TODO: this is very frequent pattern, make it a separate
 Continue calculating CRC 
	 If it should be REF_NORMAL, it'll get marked as such when
	   we build the fragtree, shortly. No need to worry about GC
	   moving it while it's marked REF_PRISTINE -- GC won't happen
	
	  Mark the node as having been checked and fix the
	  accounting accordingly.
  Helper function for jffs2_add_older_frag_to_fragtree().
  Checks the node if we are in the checking stage.
 We only check the data CRC of unchecked nodes 
  This function is used when we read an inode. Data nodes arrive in
  arbitrary order -- they may be older or newer than the nodes which
  are already in the tree. Where overlaps occur, the older node can
  be discarded as long as the newer passes the CRC check. We don't
  bother to keep track of holes in this rbtree, and neither do we deal
  with frags -- we can have multiple entries starting at the same
  offset, and the one with the smallest length will come first in the
  ordering.
  Returns 0 if the node was handled (including marking it obsolete)
 	 < 0 an if error occurred
	 If a node has zero dsize, we only have to keep it if it might be the
	   node with highest version -- i.e. the one which will end up as f->metadata.
	   Note that such nodes won't be REF_UNCHECKED since there are no data to
 We had a candidate mdata node already 
 Find the earliest node which _may_ be relevant to this one 
		 If the node is coincident with another at a lower address,
				
				  We killed a node which set the overlapped
				  flags during the scan. Fix it up.
			 Version number collision means REF_PRISTINE GC. Accept either of them
 The one we already had was OK. Keep it and throw away the new one 
 Who cares if the new one is good; keep it for now anyway. 
 Same overlapping from in front and behind 
 New node entirely overlaps 'this' 
 ... and is good. Kill 'this' and any subsequent nodes which are also overlapped 
 New node entirely overlapped by 'this' 
 ... but 'this' was bad. Replace it... 
	 We neither completely obsoleted nor were completely
 If there's anything behind that overlaps us, note it 
				
				  We killed a node which set the overlapped
				  flags during the scan. Fix it up.
 If the new node overlaps anything ahead, note it 
 Trivial function to remove the last node in the tree. Which by definition
   has no right-hand child  so can be removed just by making its left-hand
   child (if any) take its place under its parent. Since this is only done
   when we're consuming the whole tree, there's no need to use rb_erase()
   and let it worry about adjusting colours and balancing the tree. That
 LAST! 
 We put the version tree in reverse order, so we can use the same eat_last()
 Build final, normal fragtree from tn tree. It doesn't matter which order
   we add nodes to the real fragtree, as long as they don't overlap. And
   having thrown away the majority of overlapped nodes as we went, there
   really shouldn't be many sets of nodes which do overlap. If we start at
   the end, we can use the overlap markers -- we can just eat nodes which
   aren't overlapped, and when we encounter nodes which _do_ overlap we
			
			  We killed a node which set the overlapped
			  flags during the scan. Fix it up.
		 Now we have a bunch of nodes in reverse version
		   order, in the tree at ver_root. Most of the time,
		   there'll actually be only one node in the 'tree',
					 Note that this is different from the other
					   highest_version, because this one is only
					   counting _valid_ nodes which could give the
					 Free the nodes in vers_root; let the caller
 Returns first valid node after 'ref'. May return 'ref' 
  Helper function for jffs2_get_inode_nodes().
  It is called every time an directory entry node is found.
  Returns: 0 on success;
  	    negative error code on failure.
 Obsoleted. This cannot happen, surely? dwmw2 20020308 
 If we've never checked the CRCs on this node, check them now 
 Sanity check 
 Pick out the mctime of the latest dirent 
	
	  Copy as much of the name as possible from the raw
	  dirent we've already read from the flash.
 Do we need to copy any more of the name directly from the flash? 
 FIXME: point() 
		
		  we use CONFIG_JFFS2_SUMMARY because without it, we
		  have checked it while mounting
	
	  Wheee. We now have a complete jffs2_full_dirent structure, with
	  the name in it and everything. Link it into the list
  Helper function for jffs2_get_inode_nodes().
  It is called every time an inode node is found.
  Returns: 0 on success (possibly after marking a bad node obsolete);
  	    negative error code on failure.
 Obsoleted. This cannot happen, surely? dwmw2 20020308 
 If we've never checked the CRCs on this node, check them now 
 Sanity checks 
			 At this point we are supposed to check the data CRC
			  of our unchecked node. But thus far, we do not
			  know whether the node is valid or obsolete. To
			  figure this out, we need to walk all the nodes of
			  the inode and build the inode fragtree. We don't
			  want to spend time checking data of nodes which may
			  later be found to be obsolete. So we put off the full
			  data CRC checking until we have read all the inode
			  nodes and have started building the fragtree.
			 
			  The fragtree is being built starting with nodes
			  having the highest version number, so we'll be able
			  to detect whether a node is valid (i.e., it is not
			  overlapped by a node with higher version) or not.
			  And we'll be able to check only those nodes, which
			  are not obsolete.
			 
			  Of course, this optimization only makes sense in case
			  of NAND flashes (or other flashes with
			  !jffs2_can_mark_obsolete()), since on NOR flashes
			  nodes are marked obsolete physically.
			 
			  Since NAND flashes (or other flashes with
			  jffs2_is_writebuffered(c)) are anyway read by
			  fractions of c->wbuf_pagesize, and we have just read
			  the node header, it is likely that the starting part
			  of the node data is also read when we read the
			  header. So we don't mind to check the CRC of the
			  starting part of the data of the node now, and check
			  the second part later (in jffs2_check_node_data()).
			  Of course, we will not need to re-read and re-check
			  the NAND page which we have just read. This is why we
			  read the whole NAND page at jffs2_get_inode_nodes(),
			  while we needed only the node header.
 'buf' will point to the start of data 
 len will be the read data length 
			 If we actually calculated the whole data CRC
			
			  We checked the header CRC. If the node has no data, adjust
			  the space accounting now. For other nodes this will be done
			  later either when the node is marked obsolete or when its
			  data is checked.
	 There was a bug where we wrote hole nodes out with
 normal case...
  Helper function for jffs2_get_inode_nodes().
  It is called every time an unknown node is found.
  Returns: 0 on success;
  	    negative error code on failure.
 We don't mark unknown nodes as REF_UNCHECKED 
 EEP 
  Helper function for jffs2_get_inode_nodes().
  The function detects whether more data should be read and reads it if yes.
  Returns: 0 on success;
  	    negative error code on failure.
 We need to read more data 
 Get tmp_dnode_info and full_dirent for all non-obsolete nodes associated
   with this ino. Perform a preliminary ordering on data nodes, throwing away
   those which are completely obsoleted by newer ones. The nave approach we
   use to take of just returning them _all_ in version order will cause us to
	 FIXME: in case of NOR and available ->point() this
		 We can hold a pointer to a non-obsolete node without the spinlock,
		   but _obsolete_ nodes may disappear at any time, if the block
		   they're in gets erased. So if we mark 'ref' obsolete while we're
		   not holding the lock, it can go away immediately. For that reason,
		   we find the next valid node first, before processing 'ref'.
		
		  At this point we don't know the type of the node we're going
		  to read, so we do not know the size of its header. In order
		  to minimize the amount of flash IO we assume the header is
		  of size = JFFS2_MIN_NODE_HEADER.
			
			  We are about to read JFFS2_MIN_NODE_HEADER bytes,
			  but this flash has some minimal IO unit. It is
			  possible that we'll need to read more soon, so read
			  up to the next min. IO unit, in order not to
			  re-read the same min. IO unit twice.
 FIXME: point() 
 No need to mask in the valid bit; it shouldn't be invalid 
 Not a JFFS2 node, whinge and move on 
 Grab all nodes relevant to this ino 
 FIXME: We could at least crc-check them all 
 No data nodes for this inode. 
 FIXME: If this fails, there seems to be a memory leak. Find it. 
			 The times in the latest_node are actually older than
 If it was a regular file, truncate it to the latest node's isize 
		 Hack to work around broken isize in old symlink code.
		   Remove this when dwmw2 comes to his senses and stops
		   symlinks from being an entirely gratuitous special
			 Symlink's inode data is the target path. Read it and
			  keep in RAM to facilitate quick follow symlink
		 Certain inode types should have only one data node, and it's
 ASSERT: f->fraglist != NULL 
 FIXME: Deal with it - check crc32, check for duplicate node, check times and discard the older one 
 OK. We're happy 
 Scan the list of all nodes present for this ino, build map of versions, etc. 
 Check its state. We may need to wait before we can use it 
			 If it's in either of these states, we need
			   to wait for whoever's got it to finish and
			 Eep. This should never happen. It can
			happen if Linux calls read_inode() again
 Fail. That's probably better than allowing it to succeed 
 Special case - no root inode on medium 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Copyright  2004 Ferenc Havasi <havasi@inf.u-szeged.hu>,
 		    University of Szeged, Hungary
  Created by Arjan van de Ven <arjan@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 Available compressors are on this list 
 Actual compression mode 
 Statistics for blocks stored without compression 
  Return 1 to use this compression
 Shouldn't happen 
  jffs2_selected_compress:
  @compr: Explicit compression type to use (ie, JFFS2_COMPR_ZLIB).
 	If 0, just take the first available compression mode.
  @data_in: Pointer to uncompressed data
  @cpage_out: Pointer to returned pointer to buffer for compressed data
  @datalen: On entry, holds the amount of data available for compression.
 	On exit, expected to hold the amount of data actually compressed.
  @cdatalen: On entry, holds the amount of space available for compressed
 	data. On exit, expected to hold the actual size of the compressed
 	data.
  Returns: the compression type used.  Zero is used to show that the data
  could not be compressed; probably because we couldn't find the requested
  compression mode.
 Skip decompress-only and disabled modules 
 Skip if not the desired compression type 
		
		  Either compression type was unspecified, or we found our
		  compressor; either way, we're good to go.
 Success 
 jffs2_compress:
  @data_in: Pointer to uncompressed data
  @cpage_out: Pointer to returned pointer to buffer for compressed data
  @datalen: On entry, holds the amount of data available for compression.
 	On exit, expected to hold the amount of data actually compressed.
  @cdatalen: On entry, holds the amount of space available for compressed
 	data. On exit, expected to hold the actual size of the compressed
 	data.
  Returns: Lower byte to be stored with data indicating compression type used.
  Zero is used to show that the data could not be compressed - the
  compressed version was actually larger than the original.
  Upper byte will be used later. (soon)
  If the cdata buffer isn't large enough to hold all the uncompressed data,
  jffs2_compress should compress as much as will fit, and should set
  datalen accordingly to show the amount of data which were compressed.
 Skip decompress-only backwards-compatibility and disabled modules 
 Allocating memory for output buffer if necessary 
	 Older code had a bug where it would write non-zero 'usercompr'
 This should be special-cased elsewhere, but we might as well deal with it 
 Registering compressors 
 Setting default compression mode 
 Unregistering compressors 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
  Check whether the user is allowed to write.
 Always allow root 
 	jffs2_reserve_space - request physical space to write nodes to flash
 	@c: superblock info
 	@minsize: Minimum acceptable size of allocation
 	@len: Returned value of allocation length
 	@prio: Allocation type - ALLOC_{NORMAL,DELETION}
 	Requests a block of physical space on the flash. Returns zero for success
 	and puts 'len' into the appropriate place, or returns -ENOSPC or other 
 	error if appropriate. Doesn't return len since that's 
 	If it returns zero, jffs2_reserve_space() also downs the per-filesystem
 	allocation semaphore, to prevent more than one allocation from being
 	active at any time. The semaphore is later released by jffs2_commit_allocation()
 	jffs2_reserve_space() may trigger garbage collection in order to make room
 	for the requested allocation.
 align it 
	
	  Check if the free space is greater then size of the reserved pool.
	  If not, only allow root to proceed with writing.
 this needs a little more thought (true <tglx> :)) 
			 calculate real dirty size
			  dirty_size contains blocks on erase_pending_list
			  those blocks are counted in c->nr_erasing_blocks.
			  If one block is actually erased, it is not longer counted as dirty_space
			  but it is counted in c->nr_erasing_blocks, so we add it and subtract it
			  with c->nr_erasing_blocks  c->sector_size again.
			  Blocks on erasable_list are counted as dirty_size, but not in c->nr_erasing_blocks
			  This helps us to force gc and pick eventually a clean block to spread the load.
			  We add unchecked_size here, as we hopefully will find some space to use.
			  This will affect the sum only once, as gc first finishes checking
			  of nodes.
			 Calc possibly available space. Possibly available means that we
			  don't know, if unchecked size contains obsoleted nodes, which could give us some
			  more usable space. This will affect the sum only once, as gc first finishes checking
			  of nodes.
			 + Return -ENOSPC, if the maximum possibly available space is less or equal than
			  blocksneeded  sector_size.
			  This blocks endless gc looping on a filesystem, which is nearly full, even if
			  the check above passes.
 Classify nextblock (clean, dirty of verydirty) and force to select an other one 
 Check, if we have a dirty block now, or if it was dirty already 
 Select a new jeb for nextblock 
 Take the next block off the 'free' list 
 c->nextblock is NULL, no update to c->nextblock allowed 
 Have another go. It'll be on the erasable_list now 
			 Ouch. We're in GC, or we wouldn't have got here.
 Don't wait for it; just erase one right now 
		 An erase may have failed, decreasing the
		   amount of free space available. So we must
 reset collected summary 
 adjust write buffer offset, else we get a non contiguous write bug 
 Called with alloc sem _and_ erase_completion_lock 
 for summary information at the end of the jeb 
 NOSUM_SIZE means not to generate summary 
		 Is there enough space for writing out the current node, or we have to
 Has summary been disabled for this jeb? 
 Writing out the collected summary information 
				 jffs2_write_sumnode() couldn't write out the summary information
				   diabling summary for this jeb and free the collected information
 keep always valid value in reserved_size 
 Skip the end of this block and file it as having some dirty space 
 If there's a pending write to it, flush now 
			 Just lock it again and continue. Nothing much can change because
			   we hold c->alloc_sem anyway. In fact, it's not entirely clear why
			   we hold c->erase_completion_lock in the majority of this function...
 FIXME: that made it count as dirty. Convert to wasted 
	 OK, jeb (==c->nextblock) is now pointing at a block which definitely has
		 Only node in it beforehand was a CLEANMARKER node (we think).
		   So mark it obsolete now that there's going to be another node
		   in the block. This will reduce used_size to zero but We've
		   already set c->nextblock so that jffs2_mark_node_obsolete()
		   won't try to refile it to the dirty_list.
 	jffs2_add_physical_node_ref - add a physical node reference to the list
 	@c: superblock info
 	@new: new node reference to add
 	@len: length of this physical node
 	Should only be used to report nodes for which space has been allocated
 	by jffs2_reserve_space.
 	Must be called with the alloc_sem held.
	 Allow non-obsolete nodes only to be added at the end of c->nextblock, 
	   if c->nextblock is set. Note that wbuf.c will file obsolete nodes
 If it lives on the dirty_list, jffs2_reserve_space will put it there 
 Flush the last write in the block if it's outstanding 
		 Hm. This may confuse static lock analysis. If any of the above
		   three conditions is false, we're going to return from this
		   function without actually obliterating any nodes or freeing
		   any jffs2_raw_node_refs. So we don't need to stop erases from
		   happening, or protect against people holding an obsolete
 Take care, that wasted size is taken into concern
 Convert wasted space to dirty, if not a bad block 
 To fool the refiling code later 
		 Flash scanning is in progress. Don't muck about with the block
		   lists because they're not ready yet, and don't actually
		   obliterate nodes that look obsolete. If they weren't
		   marked obsolete on the flash at the time they _became_
 We didn't lock the erase_free_sem 
				 Most of the time, we just erase it immediately. Otherwise we
				 Sometimes, however, we leave it elsewhere so it doesn't get
 We didn't lock the erase_free_sem 
	 The erase_free_sem is locked, and has been since before we marked the node obsolete
	   and potentially put its eraseblock onto the erase_pending_list. Thus, we know that
	   the block hasn't _already_ been erased, and that 'ref' itself hasn't been freed yet
 XXX FIXME: This is ugly now 
	 Nodes which have been marked obsolete no longer need to be
	   associated with any inode. Remove them from the per-inode list.
	   Note we can't do this for NAND at the moment because we need
	   obsolete dirent nodes to stay on the lists, because of the
	   horridness in jffs2_garbage_collect_deletion_dirent(). Also
	   because we delete the inocache, and on NAND we need that to
	   stay around until all the nodes are actually erased, in order
	   to stop us from giving the same inode number to another newly
	 dirty_size contains blocks on erase_pending_list
	  those blocks are counted in c->nr_erasing_blocks.
	  If one block is actually erased, it is not longer counted as dirty_space
	  but it is counted in c->nr_erasing_blocks, so we add it and subtract it
	  with c->nr_erasing_blocks  c->sector_size again.
	  Blocks on erasable_list are counted as dirty_size, but not in c->nr_erasing_blocks
	  This helps us to force gc and pick eventually a clean block to spread the load.
 In debug mode, actually go through and count them all 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by Arjan van de Ven <arjanv@redhat.com>
  For licensing information, see the file 'LICENCE' in this directory.
 behalve lower 
	
	  First, work out how many bits we need from the input stream.
	  Note that we have already done the initial check on this
	  loop prior to calling this function.
	
	  Now get the bits.  We really want this to be "get n bits".
 Failed. Restore old state 
 We failed 
	 Tell the caller how much we managed to compress,
 We didn't actually compress 
 _compress returns the compressed size, -1 if bigger 
 Add back the 8 bytes we took for the probabilities 
 We compressed 
&jffs2_rubinmips_compress,
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2007 Nokia Corporation. All rights reserved.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by Richard Purdie <rpurdie@openedhand.com>
  For licensing information, see the file 'LICENCE' in this directory.
 for lzo_mem and lzo_compress_buf 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2006  NEC Corporation
  Created by KaiGai Kohei <kaigai@ak.jp.nec.com>
  For licensing information, see the file 'LICENCE' in this directory.
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2004  Ferenc Havasi <havasi@inf.u-szeged.hu>,
 		     Zoltan Sogor <weth@inf.u-szeged.hu>,
 		     Patrik Kluba <pajko@halom.u-szeged.hu>,
 		     University of Szeged, Hungary
 	       2006  KaiGai Kohei <kaigai@ak.jp.nec.com>
  For licensing information, see the file 'LICENCE' in this directory.
 The following 3 functions are called from scan.c to collect summary info for not closed jeb 
 relative offset from the beginning of the jeb 
 relative from the beginning of the jeb 
 Cleanup every collected summary information 
 Move the collected summary information into sb (called from scan.c) 
 Called from wbuf.c to collect writed node info 
 impossible count value 
			 If you implement a new node type you should also implement
			   summary support for it or disable summary.
 If there was a gap, mark it dirty 
 Ew. Summary doesn't actually tell us explicitly about dirty space 
 Process the stored summary information - helper function for jffs2_sum_scan_sumnode() 
 Make sure there's a spare ref for dirty space 
dev.laptop.orgticket4184 
 node is not the newest one 
 For compatible node types, just fall back to the full scan 
 Process the summary node - called from jffs2_scan_eraseblock() 
 OK, now check for node validity and CRC 
	 -ENOTRECOVERABLE isn't a fatal error -- it means we should do a full
 real error 
 for PARANOIA_CHECK 
 Write summary data to flash - helper function for jffs2_sum_write_sumnode() 
 It won't fit in the buffer. Abort summary for this jeb 
 Non-fatal 
 Is there enough space for summary? 
 don't try to write out summary for this jeb 
 Non-fatal 
 The above call removes the list, nothing more to do 
 unknown node in summary information 
 Waste remaining space 
 Write out summary information - called from jffs2_do_reserve_space 
  JFFS2 -- Journalling Flash File System, Version 2.
  Copyright  2001-2007 Red Hat, Inc.
  Copyright  2004-2010 David Woodhouse <dwmw2@infradead.org>
  Created by David Woodhouse <dwmw2@infradead.org>
  For licensing information, see the file 'LICENCE' in this directory.
 JFFS2_DBG_SANITY_CHECKS 
  Check the fragtree.
			 A hole node which isn't multi-page should be garbage-collected
			   and merged anyway, so we just check for the frag size here,
			   rather than mucking around with actually reading the node
			   and checking the compression type, which is the real way
  Check if the flash contains all 0xFF before we start writing.
  Check the space accounting and node_ref list correctness for the JFFS2 erasable block 'jeb'.
 This should work when we implement ref->__totlen elemination 
 JFFS2_DBG_PARANOIA_CHECKS 
  Dump the node_refs of the 'jeb' JFFS2 eraseblock.
  Dump an eraseblock's space accounting.
  Dump a JFFS2 node.
 JFFS2_DBG_DUMPS || JFFS2_DBG_PARANOIA_CHECKS 
 SPDX-License-Identifier: GPL-2.0
  Data verification functions, i.e. hooks for ->readpages()
  Copyright 2019 Google LLC
  hash_at_level() - compute the location of the block's hash at the given level
  @params:	(in) the Merkle tree parameters
  @dindex:	(in) the index of the data block being verified
  @level:	(in) the level of hash we want (0 is leaf level)
  @hindex:	(out) the index of the hash block containing the wanted hash
  @hoffset:	(out) the byte offset to the wanted hash within the hash block
 Offset of the hash within the level's region, in hashes 
 Index of the hash block in the tree overall 
 Offset of the wanted hash (in bytes) within the hash block 
 Extract a hash from a hash page 
  Verify a single data page against the file's Merkle tree.
  In principle, we need to verify the entire path to the root node.  However,
  for efficiency the filesystem may cache the hash pages.  Therefore we need
  only ascend the tree until an already-verified page is seen, as indicated by
  the PageChecked bit being set; then verify the path to that page.
  This code currently only supports the case where the verity block size is
  equal to PAGE_SIZE.  Doing otherwise would be possible but tricky, since we
  wouldn't be able to use the PageChecked bit.
  Note that multiple processes may race to verify a hash page and mark it
  Checked, but it doesn't matter; the result will be the same either way.
  Return: true if the page is valid, else false.
	
	  Starting at the leaf level, ascend the tree saving hash pages along
	  the way until we find a verified hash page, indicated by PageChecked;
	  or until we reach the root.
 Descend the tree verifying hash pages 
 Finally, verify the data page 
  fsverity_verify_page() - verify a data page
  @page: the page to verity
  Verify a page that has just been read from a verity file.  The page must be a
  pagecache page that is still locked and not yet uptodate.
  Return: true if the page is valid, else false.
 This allocation never fails, since it's mempool-backed. 
  fsverity_verify_bio() - verify a 'read' bio that has just completed
  @bio: the bio to verify
  Verify a set of pages that have just been read from a verity file.  The pages
  must be pagecache pages that are still locked and not yet uptodate.  Pages
  that fail verification are set to the Error state.  Verification is skipped
  for pages already in the Error state, e.g. due to fscrypt decryption failure.
  This is a helper function for use by the ->readpages() method of filesystems
  that issue bios to read data directly into the page cache.  Filesystems that
  populate the page cache without issuing bios (e.g. non block-based
  filesystems) must instead call fsverity_verify_page() directly on each page.
  All filesystems must also call fsverity_verify_page() on holes.
 This allocation never fails, since it's mempool-backed. 
		
		  If this bio is for data readahead, then we also do readahead
		  of the first (largest) level of the Merkle tree.  Namely,
		  when a Merkle tree page is read, we also try to piggy-back on
		  some additional pages -- up to 14 the number of data pages.
		 
		  This improves sequential read performance, as it greatly
		  reduces the number of IO requests made to the Merkle tree.
 CONFIG_BLOCK 
  fsverity_enqueue_verify_work() - enqueue work on the fs-verity workqueue
  @work: the work to enqueue
  Enqueue verification work for asynchronous processing.
	
	  Use an unbound workqueue to allow bios to be verified in parallel
	  even when they happen to complete on the same CPU.  This sacrifices
	  locality, but it's worthwhile since hashing is CPU-intensive.
	 
	  Also use a high-priority workqueue to prioritize verification work,
	  which blocks reads from completing, over regular application tasks.
 SPDX-License-Identifier: GPL-2.0
  fs-verity module initialization and logging
  Copyright 2019 Google LLC
 SPDX-License-Identifier: GPL-2.0
  fs-verity hash algorithms
  Copyright 2019 Google LLC
 The hash algorithms supported by fs-verity 
  fsverity_get_hash_alg() - validate and prepare a hash algorithm
  @inode: optional inode for logging purposes
  @num: the hash algorithm number
  Get the struct fsverity_hash_alg for the given hash algorithm number, and
  ensure it has a hash transform ready to go.  The hash transforms are
  allocated on-demand so that we don't waste resources unnecessarily, and
  because the crypto modules may be initialized later than fsverity.
  Return: pointer to the hash alg on success, else an ERR_PTR()
 pairs with smp_store_release() below 
	
	  Using the shash API would make things a bit simpler, but the ahash
	  API is preferable as it allows the use of crypto accelerators.
 pairs with smp_load_acquire() above 
  fsverity_alloc_hash_request() - allocate a hash request object
  @alg: the hash algorithm for which to allocate the request
  @gfp_flags: memory allocation flags
  This is mempool-backed, so this never fails if __GFP_DIRECT_RECLAIM is set in
  @gfp_flags.  However, in that case this might need to wait for all
  previously-allocated requests to be freed.  So to avoid deadlocks, callers
  must never need multiple requests at a time to make forward progress.
  Return: the request object on success; NULL on failure (but see above)
  fsverity_free_hash_request() - free a hash request object
  @alg: the hash algorithm
  @req: the hash request object to free
  fsverity_prepare_hash_state() - precompute the initial hash state
  @alg: hash algorithm
  @salt: a salt which is to be prepended to all data to be hashed
  @salt_size: salt size in bytes, possibly 0
  Return: NULL if the salt is empty, otherwise the kmalloc()'ed precomputed
 	   initial hash state on success or an ERR_PTR() on failure.
 This allocation never fails, since it's mempool-backed. 
	
	  Zero-pad the salt to the next multiple of the input size of the hash
	  algorithm's compression function, e.g. 64 bytes for SHA-256 or 128
	  bytes for SHA-512.  This ensures that the hash algorithm won't have
	  any bytes buffered internally after processing the salt, thus making
	  salted hashing just as fast as unsalted hashing.
  fsverity_hash_page() - hash a single data or hash page
  @params: the Merkle tree's parameters
  @inode: inode for which the hashing is being done
  @req: preallocated hash request
  @page: the page to hash
  @out: output digest, size 'params->digest_size' bytes
  Hash a single data or hash block, assuming block_size == PAGE_SIZE.
  The hash is salted if a salt is specified in the Merkle tree parameters.
  Return: 0 on success, -errno on failure
  fsverity_hash_buffer() - hash some data
  @alg: the hash algorithm to use
  @data: the data to hash
  @size: size of data to hash, in bytes
  @out: output digest, size 'alg->digest_size' bytes
  Hash some data which is located in physically contiguous memory (i.e. memory
  allocated by kmalloc(), not by vmalloc()).  No salt is used.
  Return: 0 on success, -errno on failure
 This allocation never fails, since it's mempool-backed. 
	
	  Sanity check the hash algorithms (could be a build-time check, but
	  they're in an array)
		
		  For efficiency, the implementation currently assumes the
		  digest and block sizes are powers of 2.  This limitation can
		  be lifted if the code is updated to handle other values.
 SPDX-License-Identifier: GPL-2.0
  Opening fs-verity files
  Copyright 2019 Google LLC
  fsverity_init_merkle_tree_params() - initialize Merkle tree parameters
  @params: the parameters struct to initialize
  @inode: the inode for which the Merkle tree is being built
  @hash_algorithm: number of hash algorithm to use
  @log_blocksize: log base 2 of block size to use
  @salt: pointer to salt (optional)
  @salt_size: size of salt, possibly 0
  Validate the hash algorithm and block size, then compute the tree topology
  (num levels, num blocks in each level, etc.) and initialize @params.
  Return: 0 on success, -errno on failure
	
	  Compute the number of levels in the Merkle tree and create a map from
	  level to the starting block of that level.  Level 'num_levels - 1' is
	  the root and is stored first.  Level 0 is the level directly "above"
	  the data blocks and is stored last.
 Compute number of levels and the number of blocks in each level 
 temporarily using level_start[] to store blocks in level 
 Compute the starting block of each level 
  Compute the file digest by hashing the fsverity_descriptor excluding the
  signature and with the sig_size field set to 0.
  Create a new fsverity_info from the given fsverity_descriptor (with optional
  appended signature), and check the signature if present.  The
  fsverity_descriptor must have already undergone basic validation.
	
	  Multiple tasks may race to set ->i_verity_info, so use
	  cmpxchg_release().  This pairs with the smp_load_acquire() in
	  fsverity_get_info().  I.e., here we publish ->i_verity_info with a
	  RELEASE barrier so that other tasks can ACQUIRE it.
 Lost the race, so free the fsverity_info we allocated. 
		
		  Afterwards, the caller may access ->i_verity_info directly,
		  so make sure to ACQUIRE the winning fsverity_info.
  Read the inode's fsverity_descriptor (with optional appended signature) from
  the filesystem, and do basic validation of it.
 Ensure the inode has an ->i_verity_info 
  fsverity_file_open() - prepare to open a verity file
  @inode: the inode being opened
  @filp: the struct file being set up
  When opening a verity file, deny the open if it is for writing.  Otherwise,
  set up the inode's ->i_verity_info if not already done.
  When combined with fscrypt, this must be called after fscrypt_file_open().
  Otherwise, we won't have the key set up to decrypt the verity metadata.
  Return: 0 on success, -errno on failure
  fsverity_prepare_setattr() - prepare to change a verity inode's attributes
  @dentry: dentry through which the inode is being changed
  @attr: attributes to change
  Verity files are immutable, so deny truncates.  This isn't covered by the
  open-time check because sys_truncate() takes a path, not a file descriptor.
  Return: 0 on success, -errno on failure
  fsverity_cleanup_inode() - free the inode's verity info, if present
  @inode: an inode being evicted
  Filesystems must call this on inode eviction to free ->i_verity_info.
 SPDX-License-Identifier: GPL-2.0
  Ioctl to get a verity file's digest
  Copyright 2019 Google LLC
  fsverity_ioctl_measure() - get a verity file's digest
  @filp: file to get digest of
  @_uarg: user pointer to fsverity_digest
  Retrieve the file digest that the kernel is enforcing for reads from a verity
  file.  See the "FS_IOC_MEASURE_VERITY" section of
  Documentationfilesystemsfsverity.rst for the documentation.
  Return: 0 on success, -errno on failure
 not a verity file 
	
	  The user specifies the digest_size their buffer has space for; we can
	  return the digest if it fits in the available space.  We write back
	  the actual size, which may be shorter than the user-specified size.
 SPDX-License-Identifier: GPL-2.0-only
  Ioctl to read verity metadata
  Copyright 2021 Google LLC
	
	  Iterate through each Merkle tree page in the requested range and copy
	  the requested portion to userspace.  Note that the Merkle tree block
	  size isn't important here, as we are returning a byte stream; i.e.,
	  we can just work with pages even if the tree block size != PAGE_SIZE.
 Copy the requested portion of the buffer to userspace. 
 don't include the signature 
	
	  Include only the signature.  Note that fsverity_get_descriptor()
	  already verified that sig_size is in-bounds.
  fsverity_ioctl_read_metadata() - read verity metadata from a file
  @filp: file to read the metadata from
  @uarg: user pointer to fsverity_read_metadata_arg
  Return: length read on success, 0 on EOF, -errno on failure
 not a verity file 
	
	  Note that we don't have to explicitly check that the file is open for
	  reading, since verity files can only be opened for reading.
 offset + length must not overflow. 
 Ensure that the return value will fit in INT_MAX. 
 SPDX-License-Identifier: GPL-2.0
  Ioctl to enable verity on a file
  Copyright 2019 Google LLC
  Read a file data page for Merkle tree construction.  Do aggressive readahead,
  since we're sequentially reading the entire file.
 checked earlier too 
 unused 
 Leaf: hashing a data block 
 Non-leaf: hashing hash block from level below 
 Root hash? 
 Flush the pending hash block 
  Build the Merkle tree for the given file using the given parameters, and
  return the root hash in @root_hash.
  The tree is written to a filesystem-specific location as determined by the
  ->write_merkle_tree_block() method.  However, the blocks that comprise the
  tree are the same for all filesystems.
 Empty file is a special case; root hash is all 0's 
 This allocation never fails, since it's mempool-backed. 
	
	  Build each level of the Merkle tree, starting at the leaf level
	  (level 0) and ascending to the root node (level 'num_levels - 1').
	  Then at the end (level 'num_levels'), calculate the root hash.
 Start initializing the fsverity_descriptor 
 Get the salt if the user provided one 
 Get the signature if the user provided one 
 Prepare the Merkle tree parameters 
	
	  Start enabling verity on this file, serialized by the inode lock.
	  Fail if verity is already enabled or is already being enabled.
	
	  Build the Merkle tree.  Don't hold the inode lock during this, since
	  on huge files this may take a very long time and we don't want to
	  force unrelated syscalls like chown() to block forever.  We don't
	  need the inode lock here because deny_write_access() already prevents
	  the file from being written to or truncated, and we still serialize
	  ->begin_enable_verity() and ->end_enable_verity() using the inode
	  lock and only allow one process to be here at a time on a given file.
	
	  Create the fsverity_info.  Don't bother trying to save work by
	  reusing the merkle_tree_params from above.  Instead, just create the
	  fsverity_info from the fsverity_descriptor as if it were just loaded
	  from disk.  This is simpler, and it serves as an extra check that the
	  metadata we're writing is valid before actually enabling verity.
	
	  Tell the filesystem to finish enabling verity on the file.
	  Serialized with ->begin_enable_verity() by the inode lock.
 Successfully enabled verity 
		
		  Readers can start using ->i_verity_info immediately, so it
		  can't be rolled back once set.  So don't set it until just
		  after the filesystem has successfully enabled verity.
  fsverity_ioctl_enable() - enable verity on a file
  @filp: file to enable verity on
  @uarg: user pointer to fsverity_enable_arg
  Enable fs-verity on a file.  See the "FS_IOC_ENABLE_VERITY" section of
  Documentationfilesystemsfsverity.rst for the documentation.
  Return: 0 on success, -errno on failure
	
	  Require a regular file with write access.  But the actual fd must
	  still be readonly so that we can lock out all writers.  This is
	  needed to guarantee that no writable fds exist to the file once it
	  has verity enabled, and to stabilize the data being hashed.
 -EROFS 
 -ETXTBSY 
	
	  Some pages of the file may have been evicted from pagecache after
	  being used in the Merkle tree construction, then read into pagecache
	  again by another process reading from the file concurrently.  Since
	  these pages didn't undergo verification against the file digest which
	  fs-verity now claims to be enforcing, we have to wipe the pagecache
	  to ensure that all future reads are verified.
	
	  allow_write_access() is needed to pair with deny_write_access().
	  Regardless, the filesystem won't allow writing to verity files.
 SPDX-License-Identifier: GPL-2.0
  Verification of builtin signatures
  Copyright 2019 Google LLC
  procsysfsverityrequire_signatures
  If 1, all verity files must have a valid builtin signature.
  Keyring that contains the trusted X.509 certificates.
  Only root (kuid=0) can modify this.  Also, root may use
  keyctl_restrict_keyring() to prevent any more additions.
  fsverity_verify_signature() - check a verity file's signature
  @vi: the file's fsverity_info
  @signature: the file's built-in signature
  @sig_size: size of signature in bytes, or 0 if no signature
  If the file includes a signature of its fs-verity file digest, verify it
  against the certificates in the fs-verity keyring.
  Return: 0 on success (signature valid or not required); -errno on failure
 !CONFIG_SYSCTL 
 !CONFIG_SYSCTL 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
  Copyright (C) 2004-2021 Red Hat, Inc.  All rights reserved.
  midcomms.c
  This is the appallingly named "mid-level" comms layer. It takes care about
  deliver an on application layer "reliable" communication above the used
  lowcomms transport layer.
  How it works:
  Each nodes keeps track of all send DLM messages in send_queue with a sequence
  number. The receive will send an DLM_ACK message back for every DLM message
  received at the other side. If a reconnect happens in lowcomms we will send
  all unacknowledged dlm messages again. The receiving side might drop any already
  received message by comparing sequence numbers.
  How version detection works:
  Due the fact that dlm has pre-configured node addresses on every side
  it is in it's nature that every side connects at starts to transmit
  dlm messages which ends in a race. However DLM_RCOM_NAMES, DLM_RCOM_STATUS
  and their replies are the first messages which are exchanges. Due backwards
  compatibility these messages are not covered by the midcomms re-transmission
  layer. These messages have their own re-transmission handling in the dlm
  application layer. The version field of every node will be set on these RCOM
  messages as soon as they arrived and the node isn't yet part of the nodes
  hash. There exists also logic to detect version mismatched if something weird
  going on or the first messages isn't an expected one.
  Termination:
  The midcomms layer does a 4 way handshake for termination on DLM protocol
  like TCP supports it with half-closed socket support. SCTP doesn't support
  half-closed socket, so we do it on DLM layer. Also socket shutdown() can be
  interrupted by .e.g. tcp reset itself. Additional there exists the othercon
  paradigm in lowcomms which cannot be easily without breaking backwards
  compatibility. A node cannot send anything to another node when a DLM_FIN
  message was send. There exists additional logic to print a warning if
  DLM wants to do it. There exists a state handling like RFC 793 but reduced
  to termination only. The event "member removal event" describes the cluster
  manager removed the node from internal lists, at this point DLM does not
  send any message to the other node. There exists two cases:
  1. The cluster member was removed and we received a FIN
  OR
  2. We received a FIN but the member was not removed yet
  One of these cases will do the CLOSE_WAIT to LAST_ACK change.
                               +---------+
                               | CLOSED  |
                               +---------+
                                    | add memberreceive RCOM version
                                    |            detection msg
                                    V
                               +---------+
                               |  ESTAB  |
                               +---------+
                        CLOSE    |     |    rcv FIN
                       -------   |     |    -------
  +---------+          snd FIN         \   snd ACK          +---------+
  |  FIN    |<-----------------           ------------------>|  CLOSE  |
  | WAIT-1  |------------------                              |   WAIT  |
  +---------+          rcv FIN  \                            +---------+
  | rcv ACK of FIN   -------   |                            CLOSE  | member
  | --------------   snd ACK   |                           ------- | removal
  V        x                   V                           snd FIN V event
  +---------+                  +---------+                   +---------+
  |FINWAIT-2|                  | CLOSING |                   | LAST-ACK|
  +---------+                  +---------+                   +---------+
  |                rcv ACK of FIN |                 rcv ACK of FIN |
  |  rcv FIN       -------------- |                 -------------- |
  |  -------              x       V                        x       V
   \ snd ACK                 +---------+                   +---------+
    ------------------------>| CLOSED  |                   | CLOSED  |
                             +---------+                   +---------+
  NOTE: any state can interrupted by midcomms_close() and state will be
  switched to CLOSED in case of fencing. There exists also some timeout
  handling when we receive the version detection RCOM messages which is
  made by observation.
  Future improvements:
  There exists some known issuesimprovements of the dlm handling. Some
  of them should be done in a next major dlm version bump which makes
  it incompatible with previous versions.
  Unaligned memory access:
  There exists cases when the dlm message buffer length is not aligned
  to 8 byte. However seems nobody detected any problem with it. This
  can be fixed in the next major version bump of dlm.
  Version detection:
  The version detection and how it's done is related to backwards
  compatibility. There exists better ways to make a better handling.
  However this should be changed in the next major version bump of dlm.
  Tail Size checking:
  There exists a message tail payload in e.g. DLM_MSG however we don't
  check it against the message length yet regarding to the receive buffer
  length. That need to be validated.
  Fencing bad nodes:
  At timeout places or weird sequence number behaviours we should send
  a fencing request to the cluster manager.
 Debug switch to enable a 5 seconds sleep waiting of a termination.
  This can be useful to test fencing while termination is running.
  This requires a setup with only gfs2 as dlm user, so that the
  last umount will terminate the connection.
  However it became useful to test, while the 5 seconds block in umount
  just press the reset button. In a lot of dropping the termination
  process can could take several seconds.
 init value for sequence numbers for testing purpose only e.g. overflows 
 3 minutes wait to sync ending of dlm 
	 These queues are unbound because we cannot drop any message in dlm.
	  We could send a fence signal for a specific node to the cluster
	  manager if queues hits some maximum value, however this handling
	  not supported yet.
 dlm tcp termination state 
	 counts how many lockspaces are using this node
	  this refcount is necessary to determine if the
	  node wants to disconnect.
 not protected by srcu, node_hash lifetime 
 get_mhandlecommit srcu idx exchange 
 This mutex prevents that midcomms_close() is running while
  stop() or remove(). As I experienced invalid memory access
  behaviours when DLM_DEBUG_FENCE_TERMINATION is enabled and
  resetting machines. I will end in some double deletion in nodes
  datastructure.
	 check again if there was somebody else
	  earlier here to add the node
 send queue should be ordered 
 send queue should be ordered 
 DLM_CLOSED 
 not valid but somehow we got what we want 
 send ack before fin 
				 passive shutdown DLM_LAST_ACK case 1
				  additional we check if the node is used by
				  cluster manager events at all.
 probably remove_member caught it, do nothing 
		 retry to ack message which we already have by sending back
		  current node->seq_next number as ack.
					 some invalid state passive shutdown
					  was failed, we try to reset and
					  hope it will go on.
	 we only trust outer header msglen because
	  it's checked against receive buffer length.
		 these rcom message we use to determine version.
		  they have their own retransmission handling and
		  are the first messages of dlm.
		 
		  length already checked.
 recheck inner msglen just if it's not garbage 
 length already checked 
  Called from the low-level comms layer to process a buffer of
  commands.
		 no message should be more than DLM_MAX_SOCKET_BUFSIZE or
		  less than dlm_header size.
		 
		  Some messages does not have a 8 byte length boundary yet
		  which can occur in a unaligned memory access of some dlm
		  messages. However this problem need to be fixed at the
		  sending side, for now it seems nobody run into architecture
		  related issues yet but it slows down some processing.
		  Fixing this issue should be scheduled in future by doing
		  the next major version bump.
		 caller will take care that leftover
		  will be parsed next call with more data
 old protocol, we do nothing 
 do nothing if we didn't delivered stateful to ulp 
 we only ack if state is ESTABLISHED 
 do nothing FIN has it's own ack send 
 old protocol, we don't support to retransmit on failure 
 add possible options here 
 this is a bug, however we going on and hope it will be resolved 
	 keep in mind that is a must to call
	  dlm_midcomms_commit_msg() which releases
	  nodes_srcu using mh->idx which is assumed
	  here that the application will call it.
 nexthdr chain for fast lookup 
 mh is not part of rcu list in this case 
 not valid but somehow we got what we want 
			 some invalid state passive shutdown
			  was failed, we try to reset and
			  hope it will go on.
	 hitting users count to zero means the
	  other side is running dlm_midcomms_stop()
	  we meet us to have a clean disconnect.
 passive shutdown DLM_LAST_ACK case 2 
 probably receive fin caught it, do nothing 
 already gone, do nothing 
 old protocol, we don't wait for pending operations 
 we have what we want 
		 busy to enter DLM_FIN_WAIT1, wait until passive
		  done in shutdown_wait to enter DLM_CLOSED.
 wait for other side dlm + fin 
 Abort pending closeremove operation 
 let shutdown waiters leave 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
  Copyright (C) 2004-2011 Red Hat, Inc.  All rights reserved.
	 dlm_controld will see the uevent, do the necessary group management
 Thread for sendingreceiving messages for all lockspace's 
	 ls_exflags are forced to match among nodes, and we don't
	 Due backwards compatibility with 3.1 we need to use maximum
	  possible dlm message size to be sure the message will fit and
	  not having out of bounds issues. However on sending side 3.2
	  might send less.
	
	  Once started, dlm_recoverd first looks for ls in lslist, then
	  initializes ls_in_recovery as locked in "down" mode.  We need
	  to wait for the wakeup from dlm_recoverd because in_recovery
	  has to start out in down mode.
 let kobject handle freeing of ls if there's an error 
	 This uevent triggers dlm_controld in userspace to add us to the
	   group of nodes that are members of this lockspace (managed by the
	   cluster infrastructure.)  Once it's done that, it tells us who the
	   current lockspace members are (via configfs) and then tells the
 NOTE: We check the lkbidr here rather than the resource table.
   This is because there may be LKBs queued as ASTs that have been unlinked
 remove_lockspace takes ls off lslist 
	
	  Free all lkb's in idr
	
	  Free all rsb's on rsbtbl[] lists
	
	  Free structures on any other lists
 The ls structure will be freed when the kobject is done with 
  Called when a system has released all its locks and is not going to use the
  lockspace any longer.  We free everything we're managing for this lockspace.
  Remaining nodes will go through the recovery process as if we'd died.  The
  lockspace must continue to function as usual, participating in recoveries,
  until this returns.
  Force has 4 possible values:
  0 - don't destroy locksapce if it has any LKBs
  1 - destroy lockspace if it has remote LKBs but not if it has local LKBs
  2 - destroy lockspace regardless of LKBs
  3 - destroy lockspace as part of a forced shutdown
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2005-2009 Red Hat, Inc.  All rights reserved.
 Print the LVB: 
 Print the locks attached to this resource 
 microseconds since lkb was added to current queue 
	 id nodeid remid pid xid exflags flags sts grmode rqmode time_us
  If the buffer is full, seq_printf can be called again, but it
  does nothing.  So, the these printing routines periodically check
  seq_has_overflowed to avoid wasting too much time trying to print to
  a full buffer.
	
	  move to the first rsb in the next non-empty bucket
 zero the entry 
	
	  move to the next rsb in the same bucket
	
	  move to the first rsb in the next non-empty bucket
 zero the entry 
 the dlm_ls 
 the dlm_ls 
 the dlm_ls 
 the dlm_ls 
  dump lkb's on the ls_waiters list
 format 1 
 format 2 
 format 3 
 format 4 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
  Copyright (C) 2005-2008 Red Hat, Inc.  All rights reserved.
 When replying to a status request, a node also sends back its
   configuration values.  The requesting node then checks that the remote
  low nodeid gathers one slot value at a time from each node.
  it sets need_slots=0, and saves rf_our_slot returned from each
  rcom_config.
  other nodes gather all slot values at once from the low nodeid.
  they set need_slots=1, and ignore the rf_our_slot returned from each
  rcom_config.  they use the rf_num_slots returned from the low
  node's rcom_config.
 we pretend the remote lockspace exists with 0 status 
 the caller looks at rc_result for the remote recovery status 
 Old code would send this special id to trigger a debug dump. 
	 FIXME: might we have an lvb without DLM_LKF_VALBLK set ?
 needs at least dlm_rcom + rcom_lock 
	 We send back the same rcom_lock struct we received, but
 If the lockspace doesn't exist then still send a status message
  Ignore messages for stage Y before we set
  recover_status bit for stage X:
  recover_status = 0
  dlm_recover_members()
  - send nothing
  - recv nothing
  - ignore NAMES, NAMES_REPLY
  - ignore LOOKUP, LOOKUP_REPLY
  - ignore LOCK, LOCK_REPLY
  recover_status |= NODES
  dlm_recover_members_wait()
  dlm_recover_directory()
  - send NAMES
  - recv NAMES_REPLY
  - ignore LOOKUP, LOOKUP_REPLY
  - ignore LOCK, LOCK_REPLY
  recover_status |= DIR
  dlm_recover_directory_wait()
  dlm_recover_masters()
  - send LOOKUP
  - recv LOOKUP_REPLY
  dlm_recover_locks()
  - send LOCKS
  - recv LOCKS_REPLY
  recover_status |= LOCKS
  dlm_recover_locks_wait()
  recover_status |= DONE
 Called by dlm_recv; corresponds to dlm_receive_message() but special
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
  Copyright (C) 2004-2005 Red Hat, Inc.  All rights reserved.
  We use the upper 16 bits of the hash value to select the directory node.
  Low bits are used for distribution of rsb's among hash buckets on each node.
  To give the exact range wanted (0 to num_nodes-1), we apply a modulus of
  num_nodes to the hash value.  This value in the desired range is used as an
  offset into the sorted list of nodeid's to give the particular nodeid.
			
			  pick namelenname pairs out of received buffer
				 namelen of 0xFFFFF marks end of names for
				   this node; namelen of 0 marks end of the
				 The name was found in rsbtbl, but the
				  master nodeid is different from
				  memb->nodeid which says it is the master.
				 The name was found in rsbtbl, and the
				 The name was not found in rsbtbl and was
 Find the rsb where we left off (or start again), then send rsb names
   for rsb's we're master of and whose directory node matches the requesting
		
		  The block ends when we can't fit the following in the
		  remaining buffer space:
		  namelen (uint16_t) +
		  name (r->res_length) +
		  end-of-block record 0x0000 (uint16_t)
 Write end-of-block record 
	
	  If we've reached the end of the list (and there's room) write a
	  terminating record.
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
  Copyright (C) 2004-2005 Red Hat, Inc.  All rights reserved.
  Recovery waiting routines: these functions wait for a particular reply from
  a remote node, or for the remote node to report a certain status.  They need
  to abort if the lockspace is stopped indicating a node has failed (perhaps
  the one being waited for).
  Wait until given function returns non-zero or lockspace is stopped
  (LS_RECOVERY_STOP set due to failure of a node in ls_nodes).  When another
  function thinks it could have completed the waited-on task, they should wake
  up ls_wait_general to get an immediate response rather than waiting for the
  timeout.  This uses a timeout so it can check periodically if the wait
  should abort due to node failure (which doesn't cause a wake_up).
  This should only be called by the dlm_recoverd thread.
  An efficient way for all nodes to wait for all others to have a certain
  status.  The node with the lowest nodeid polls all the others for their
  status (wait_status_all) and all the others poll the node with the low id
  for its accumulated result (wait_status_low).  When all nodes have set
  status flag X, then status flag X_ALL will be set on the low nodeid.
 slots array is sparse, slots_size may be > num_slots 
  The recover_list contains all the rsb's for which we've requested the new
  master nodeid.  As replies are returned from the resource directories the
  rsb's are removed from the list.  When the list is empty we're done.
  The recover_list is later similarly used for all rsb's for which we've sent
  new lkb's and need to receive new corresponding lkid's.
  We use the address of the rsb struct as a simple local identifier for the
  rsb so we can match an rcom reply with the rsb it was sent for.
 Master recovery: find new master node for rsb's that were
   mastered on nodes that have been removed.
   dlm_recover_masters
   recover_master
   dlm_send_rcom_lookup            ->  receive_rcom_lookup
                                       dlm_dir_lookup
   receive_rcom_lookup_reply       <-
   dlm_recover_master_reply
   set_new_master
   set_master_lkbs
   set_lock_master
  Set the lock master for all LKBs in a lock queue
  If we are the new master of the rsb, we may have received new
  MSTCPY locks from other nodes already which we need to ignore
  when setting the new nodeid.
  Propagate the new master nodeid to locks
  The NEW_MASTER flag tells dlm_recover_locks() which rsb's to consider.
  The NEW_MASTER2 flag tells recover_lvb() and recover_grant() which
  rsb's to consider.
  We do async lookups on rsb's that need new masters.  The rsb's
  waiting for a lookup reply are kept on the recover_list.
  Another node recovering the master may have sent us a rcom lookup,
  and our dlm_master_lookup() set it as the new master, along with
  NEW_MASTER so that we'll recover it here (this implies dir_nodeid
  equals our_nodeid below).
		 set master of lkbs to ourself when is_removed, or to
		   another new master which we set along with NEW_MASTER
  All MSTCPY locks are purged and rebuilt, even if the master stayed the same.
  This is necessary because recovery can be started, aborted and restarted,
  causing the master nodeid to briefly change during the aborted recovery, and
  change back to the original value in the second recovery.  The MSTCPY locks
  may or may not have been purged during the aborted recovery.  Another node
  with an outstanding request in waiters list and a request reply saved in the
  requestqueue, cannot know whether it should ignore the reply and resend the
  request, or accept the reply and complete the request.  It must do the
  former if the remote node purged MSTCPY locks, and it must do the later if
  the remote node did not.  This is solved by always purging MSTCPY locks, in
  which case, the request reply would always be ignored and the request
  resent.
  Go through local root resources and for each rsb which has a master which
  has departed, get the new master nodeid from the directory.  The dir will
  assign mastery to the first node to look up the new master.  That means
  we'll discover in this lookup if we're the new master of any rsb's.
  We fire off all the dir lookup requests individually and asynchronously to
  the correct dir node.
 Lock recovery: rebuild the process-copy locks we hold on a
   remastered rsb on the new rsb master.
   dlm_recover_locks
   recover_locks
   recover_locks_queue
   dlm_send_rcom_lock              ->  receive_rcom_lock
                                       dlm_recover_master_copy
   receive_rcom_lock_reply         <-
   dlm_recover_process_copy
  keep a count of the number of lkb's we send to the new master; when we get
  an equal number of replies then recovery for the rsb is done
  The lvb needs to be recovered on all master rsb's.  This includes setting
  the VALNOTVALID flag if necessary, and determining the correct lvb contents
  based on the lvb's of the locks held on the rsb.
  RSB_VALNOTVALID is set in two cases:
  1. we are master, but not new, and we purged an EXPW lock held by a
  failed node (in dlm_recover_purge which set RSB_RECOVER_LVB_INVAL)
  2. we are a new master, and there are only NLCR locks left.
  (We could probably improve this by only invaliding in this way when
  the previous master left uncleanly.  VMS docs mention that.)
  The LVB contents are only considered for changing when this is a new master
  of the rsb (NEW_MASTER2).  Then, the rsb's lvb is taken from any lkb with
  mode > CR.  If no lkb's exist with mode above CR, the lvb contents are taken
  from the lkb with the largest lvb sequence number.
 case 1 above 
	 we are the new master, so figure out if VALNOTVALID should
 lvb is invalidated if only NLCR locks remain 
 All master rsb's flagged RECOVER_CONVERT need to be looked at.  The locks
 We've become the new master for this rsb and waitingconverting locks may
   need to be granted in dlm_recover_grant() due to locks that may have
			 recover lvb before granting locks so the updated
 Create a single list of all root rsb's to be used during recovery 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2005-2011 Red Hat, Inc.  All rights reserved.
 ls_slots array is sparse, but not rcom_slots 
 for any nodes that do not support slots, we will not have set memb->slot
   in wait_status_all(), so memb->slot will remain -1, and we will not
 our own memb struct will have slot -1 gen 0 
 node doesn't support slots 
 node needs a slot assigned 
 node has a slot assigned 
 sanity check, once slot is assigned it shouldn't change 
 fill in slots (offsets) that are used 
 assign new slots from unused offsets 
 FIXME: can use list macro here 
 all nodes revert to weight of 1 if all have weight 0 
 send a status request to all members just to establish comms connections 
	 if there is no comms connection with this node
	   or the present comms connection is newer
	   than the one when this member was added, then
	   we consider the node to have failed (versus
	 previously removed members that we've not finished removing need to
 move departed members from ls_nodes to ls_nodes_gone 
 removed and re-added 
 add new members to ls_nodes 
	 error -EINTR means that a new recovery action is triggered.
	  We ignore this recovery action and let run the new one which might
	  have new member configuration.
	 new_lockspace() may be waiting to know if the config
	  is good or bad
 Userspace guarantees that dlm_ls_stop() has completed on all nodes before
	
	  Prevent dlm_recv from being in the middle of something when we do
	  the stop.  This includes ensuring dlm_recv isn't processing a
	  recovery message (rcom), while dlm_recoverd is aborting and
	  resetting things from an in-progress recovery.  i.e. we want
	  dlm_recoverd to abort its recovery without worrying about dlm_recv
	  processing an rcom at the same time.  Stopping dlm_recv also makes
	  it easy for dlm_receive_message() to check locking stopped and add a
	  message to the requestqueue without races.
	
	  Abort any recovery that's in progress (see RECOVER_STOP,
	  dlm_recovery_stopped()) and tell any other threads running in the
	  dlm to quit any processing (see RUNNING, dlm_locking_stopped()).
	
	  Let dlm_recv run again, now any normal messages will be saved on the
	  requestqueue for later.
	
	  This in_recovery lock does two things:
	  1) Keeps this function from returning until all threads are out
	     of locking routines and locking is truly stopped.
	  2) Keeps any new requests from being processed until it's unlocked
	     when recovery is complete.
	
	  The recoverd suspendresume makes sure that dlm_recoverd (if
	  running) has noticed RECOVER_STOP above and quit processing the
	  previous recovery.
 the lockspace needs to be stopped before it can be started 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
  Copyright (C) 2004-2010 Red Hat, Inc.  All rights reserved.
		
		  Suppress some redundant basts here, do more on removal.
		  Don't even add a bast if the callback just before it
		  is a bast for the same mode or a more restrictive mode.
		  (the addional > PR check is needed for PRCW inversion)
 oldest undelivered cb is callbacks[0] 
 shift others down 
	 if cb is a bast, it should be skipped if the blocking mode is
 no callback work exists, shouldn't happen 
 cbs remain, loop should have removed all, shouldn't happen 
 undo kref_get from dlm_add_callback, may cause lkb to be freed 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2005-2008 Red Hat, Inc.  All rights reserved.
 does it for others u32 in union as well 
 does it for others u32 in union as well 
 higher errno values are inconsistent across architectures, so select
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
  Copyright (C) 2004-2011 Red Hat, Inc.  All rights reserved.
  configdlm<cluster>spaces<space>nodes<node>nodeid
  configdlm<cluster>spaces<space>nodes<node>weight
  configdlm<cluster>comms<comm>nodeid
  configdlm<cluster>comms<comm>local
  configdlm<cluster>comms<comm>addr      (write only)
  configdlm<cluster>comms<comm>addr_list (read only)
  The <cluster> level is useless, but I haven't figured out how to avoid it.
 TCP 
 SCTP 
 space->members 
 copy of cm->seq when nd->nodeid is set 
 assert list_empty(&sp->members) 
 default weight of 1 if none is set 
 set to 0 once it's been read by dlm_nodeid_list() 
  Functions for user space to readwrite attributes
 Taken from ip6_addr_string() defined in libvsprintf.c 
 Derived from SIMPLE_ATTR_SIZE of fsconfigfsfile.c 
  Functions for the dlm to get the info that's been configured
 caller must free mem 
 num 0 is first addr, num 1 is second addr 
 Config file defaults 
 5 sec = 500 centiseconds 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2005-2010 Red Hat, Inc.  All rights reserved.
 Central locking logic has four stages:
   dlm_lock()
   dlm_unlock()
   request_lock(ls, lkb)
   convert_lock(ls, lkb)
   unlock_lock(ls, lkb)
   cancel_lock(ls, lkb)
   _request_lock(r, lkb)
   _convert_lock(r, lkb)
   _unlock_lock(r, lkb)
   _cancel_lock(r, lkb)
   do_request(r, lkb)
   do_convert(r, lkb)
   do_unlock(r, lkb)
   do_cancel(r, lkb)
   Stage 1 (lock, unlock) is mainly about checking input args and
   splitting into one of the four main operations:
       dlm_lock          = request_lock
       dlm_lock+CONVERT  = convert_lock
       dlm_unlock        = unlock_lock
       dlm_unlock+CANCEL = cancel_lock
   Stage 2, xxxx_lock(), just finds and locks the relevant rsb which is
   provided to the next stage.
   Stage 3, _xxxx_lock(), determines if the operation is local or remote.
   When remote, it calls send_xxxx(), when local it calls do_xxxx().
   Stage 4, do_xxxx(), is the guts of the operation.  It manipulates the
   given rsb and lkb and queues callbacks.
   For remote operations, send_xxxx() results in the corresponding do_xxxx()
   function being executed on the remote node.  The connecting sendreceive
   calls on local (L) and remote (R) nodes:
   L: send_xxxx()              ->  R: receive_xxxx()
                                   R: do_xxxx()
   L: receive_xxxx_reply()     <-  R: send_xxxx_reply()
  Lock compatibilty matrix - thanks Steve
  UN = Unlocked state. Not really a state, used as a flag
  PD = Padding. Used to make the matrix a nice power of two in size
  Other states are the same as the VMS DLM.
  Usage: matrix[grmode+1][rqmode+1]  (although m[rq+1][gr+1] is the same)
 UN NL CR CW PR PW EX PD 
 UN 
 NL 
 CR 
 CW 
 PR 
 PW 
 EX 
 PD 
  This defines the direction of transfer of LVB data.
  Granted mode is the row; requested mode is the column.
  Usage: matrix[grmode+1][rqmode+1]
  1 = LVB is returned to the caller
  0 = LVB is written to the resource
  -1 = nothing happens to the LVB
 UN   NL  CR  CW  PR  PW  EX  PD
 UN 
 NL 
 CR 
 CW 
 PR 
 PW 
 EX 
 PD 
  Compatibility matrix for conversions with QUECVT set.
  Granted mode is the row; requested mode is the column.
  Usage: matrix[grmode+1][rqmode+1]
 UN NL CR CW PR PW EX PD 
 UN 
 NL 
 CR 
 CW 
 PR 
 PW 
 EX 
 PD 
 Threads cannot use the lockspace while it's being recovered 
	 if the operation was a cancel, then return -DLM_ECANCEL, if a
  Basic operations on rsb's and lkb's
 This is only called to add a reference when the code already holds
 When all references to the rsb are gone it's transferred to
 If ls->ls_new_rsb is empty, return -EAGAIN, so the caller can
   unlock any spinlocks, go back and call pre_rsb_struct again.
 Convert the empty list_head to a NULL rb_node for tree usage: 
  Find rsb in rsbtbl and potentially createadd one
  Delaying the release of rsb's has a similar benefit to applications keeping
  NL locks on an rsb, but without the guarantee that the cached master value
  will still be valid when the rsb is reused.  Apps aren't always smart enough
  to keep NL locks on an rsb that they may lock again shortly; this can lead
  to excessive master lookups and removals if we don't delay the release.
  Searching for an rsb means looking through both the normal list and toss
  list.  When found on the toss list the rsb is moved to the normal list with
  ref count of 1; when found on normal list the ref count is incremented.
  rsb's on the keep list are being used locally and refcounted.
  rsb's on the toss list are not being used locally, and are not refcounted.
  The toss list rsb's were either
  - previously used locally but not any more (were on keep list, then
    moved to toss list when last refcount dropped)
  - created and put on toss list as a directory record for a lookup
    (we are the dir node for the res, but are not using the res right now,
    but some other node is)
  The purpose of find_rsb() is to return a refcounted rsb for local use.
  So, if the given rsb is on the toss list, it is moved to the keep list
  before being returned.
  toss_rsb() happens when all local usage of the rsb is done, i.e. no
  more refcounts exist, so the rsb is moved from the keep list to the
  toss list.
  rsb's on both keep and toss lists are used for doing a name to master
  lookups.  rsb's that are in use locally (and being refcounted) are on
  the keep list, rsb's that are not in use locally (not refcounted) and
  only exist for namemaster lookups are on the toss list.
  rsb's on the toss list who's dir_nodeid is not local can have stale
  namemaster mappings.  So, remote requests on such rsb's can potentially
  return with an error, which means the mapping is stale and needs to
  be updated with a new lookup.  (The idea behind MASTER UNCERTAIN and
  first_lkid is to keep only a single outstanding request on an rsb
  while that rsb has a potentially stale master.)
	
	  flags & R_RECEIVE_RECOVER is from dlm_recover_master_copy, so
	  from_nodeid has sent us a lock in dlm_recover_locks, believing
	  we're the new master.  Our local recovery may not have set
	  res_master_nodeid to our_nodeid yet, so allow either.  Don't
	  create the rsb; dlm_recover_process_copy() will handle EBADR
	  by resending.
	 
	  If someone sends us a request, we are the dir node, and we do
	  not find the rsb anywhere, then recreate it.  This happens if
	  someone sends us a request after we have removedfreed an rsb
	  from our toss list.  (They sent a request instead of lookup
	  because they are using an rsb from their toss list.)
	
	  rsb is active, so we can't check master_nodeid without lock_rsb.
	
	  rsb found inactive (master_nodeid may be out of date unless
	  we are the dir_nodeid or were the master)  No other thread
	  is using this rsb because it's on the toss list, so we can
	  look at or update res_master_nodeid without lock_rsb.
		 our rsb was not master, and another node (not the dir node)
 don't think this should ever happen 
 fix it and go on 
		 Because we have held no locks on this rsb,
	
	  rsb not found
 want to see how often this happens 
 should never happen 
		 When we are the dir nodeid, we can set the master
 set_master will send_lookup to dir_nodeid 
 During recovery, other nodes can send us new MSTCPY locks (from
   dlm_recover_locks) before we've made ourself master (in
	
	  rsb is active, so we can't check master_nodeid without lock_rsb.
	
	  rsb found inactive. No other thread is using this rsb because
	  it's on the toss list, so we can look at or update
	  res_master_nodeid without lock_rsb.
		 our rsb is not master, and another node has sent us a
		 our rsb is not master, and we are dir; may as well fix it;
	
	  rsb not found
 we have received a request and found that res_master_nodeid != our_nodeid,
		 our rsb is not master, and another node (not the dir node)
	   	   has sent us a request.  this is much more common when our
		 our rsb is not master, but the dir nodeid has sent us a
  We're the dir node for this res and another node wants to know the
  master nodeid.  During normal operation (non recovery) this is only
  called from receive_lookup(); master lookups when the local node is
  the dir node are done by find_rsb().
  normal operation, we are the dir node for a resource
  . _request_lock
  . set_master
  . send_lookup
  . receive_lookup
  . dlm_master_lookup flags 0
  recover directory, we are rebuilding dir for all resources
  . dlm_recover_directory
  . dlm_rcom_names
    remote node sends back the rsb names it is master of and we are dir of
  . dlm_master_lookup RECOVER_DIR (fix_master 0, from_master 1)
    we either create new rsb setting remote node as master, or find existing
    rsb and set master to be the remote node.
  recover masters, we are finding the new master for resources
  . dlm_recover_masters
  . recover_master
  . dlm_send_rcom_lookup
  . receive_rcom_lookup
  . dlm_master_lookup RECOVER_MASTER (fix_master 1, from_master 0)
		 because the rsb is active, we need to lock_rsb before
	 because the rsb is inactive (on toss list), it's not refcounted
 should not happen, but may as well fix it and carry on 
		 Recovery uses this function to set a new master when
		   the previous master failed.  Setting NEW_MASTER will
		   force dlm_recover_masters to call recover_master on this
 I don't think we should ever find it on toss list. 
		 this will happen if from_nodeid became master during
		   a previous recovery cycle, and we aborted the previous
		 this will happen if recovery happens while we're looking
		 this can happen when the master sends remove, the dir node
		   finds the rsb on the keep list and ignores the remove,
 the rsb was inactive (on toss list) 
 the rsb was active 
 should never happen 
 See comment for unhold_lkb 
	 All work is done after the return from kref_put() so we
 Attachingdetaching lkb's from rsb's is for rsb reference counting.
	 All work is done after the return from kref_put() so we
 __put_lkb() is used when an lkb may not have an rsb attached to
 for localprocess lkbs, lvbptr points to caller's lksb 
 This is only called to add a reference when the code already holds
 This is called when we need to remove a reference and are certain
   it's not the last ref.  e.g. del_lkb is always called between a
   find_lkbput_lkb and is always the inverse of a previous add_lkb.
 addremove lkb to rsb's grantconvertwait queue 
 convention says granted locks kept in order of grmode 
 addremove lkb from global waiters list of lkb's waiting for
 for debugging 
 We clear the RESEND flag because we might be taking an lkb off the waiters
   list as part of process_requestqueue (e.g. a lookup that has an optimized
   request reply on the requestqueue) between dlm_recover_waiters_pre() which
	 Cancel state was preemptively cleared by a successful convert,
	 Remove for the convert reply, and premptively remove for the
	   cancel reply.  A convert has been granted while there's still
	   an outstanding cancel on it (the cancel is moot and the result
	   in the cancel reply should be 0).  We preempt the cancel reply
	   because the app gets the convert result and then can follow up
	   with another op, like convert.  This subsequent op would see the
	 N.B. type of reply may not always correspond to type of original
	 the force-unlockcancel has completed and we haven't recvd a reply
	   to the op that was in progress prior to the unlockcancel; we
	   give up on any reply to the earlier op.  FIXME: not sure whenhow
 Handles situations where we might be processing a "fake" or "stub" reply in
 If there's an rsb for the same resource being removed, ensure
   that the remove message is sent before the new lookup message.
   It should be rare to need a delay here, but if not, then it may
  ls_remove_spin protects ls_remove_name and ls_remove_len which are
  read by other threads in wait_pending_remove.  ls_remove_names
  and ls_remove_lens are only used by the scan thread, so they do
  not need protection.
		 If we're the directory record for this rsb, and
		   we're not the master of it, then we need to wait
		   for the master node to send us a dir remove for
			 We're the master of this rsb but we're not
			   the directory record, so we need to tell the
	
	  While searching for rsb's to free, we found some that require
	  remote removal.  We leave them in place and find them again here
	  so there is a very small gap between removing them from the toss
	  list and sending the removal.  Keeping this gap small is
	  important to keep us (the master node) from being out of sync
	  with the remote dir node for very long.
	 
	  From the time the rsb is removed from toss until just after
	  send_remove, the rsb name is saved in ls_remove_name.  A new
	  lookup checks this to ensure that a new lookup message for the
	  same resource name is not sent just before the remove message.
 should never happen 
 block lookup of same name until we've sent remove 
 allow lookup of name again 
 FIXME: is it safe to look at lkb_exflags, lkb_flags, lkb_timestamp, and
   lkb_lksb_timeout without lock_rsb?  Note: we can't lock timeout_mutex
   and then lock rsb because of lock ordering in add_timeout.  We may need
   to specify some special timeout-related bits in the lkb that are just to
 clear flag so we only warn once 
 This is only called by dlm_recoverd, and we rely on dlm_ls_stop() stopping
 lkb is master or local copy 
	 b=1 lvb returned to caller
	   b=0 lvb written to rsb or invalidated
 lkb is process copy (pc) 
 Manipulate lkb's on rsb's convertgrantedwaiting queues
   remove_lock -- used for unlock, removes lkb from granted
   revert_lock -- used for cancel, moves lkb from convert to granted
   grant_lock  -- used for request and convert, adds lkb to granted or
                  moves lkb from convert or waiting to granted
   Each of these is used for master or local copy lkb's.  There is
   also a _pc() variation used to make the corresponding change on
	 this unhold undoes the original ref from create_lkb()
 returns: 0 did nothing
	    1 moved lock to granted
		 this unhold undoes the original ref from create_lkb()
 called by grant_pending_locks() which means an async grant message must
   be sent to the requesting node in addition to granting the lock if the
 The special CONVDEADLK, ALTPR and ALTCW flags allow the master to
   change the grantedrequested modes.  We're munging things accordingly in
   the process copy.
   CONVDEADLK: our grmode may have been forced down to NL to resolve a
   conversion deadlock
   ALTPRALTCW: our rqmode may have been changed to PR or CW to become
 Check if the given lkb conflicts with another lkb on the queue. 
  "A conversion deadlock arises with a pair of lock requests in the converting
  queue for one resource.  The granted mode of each lock blocks the requested
  mode of the other lock."
  Part 2: if the granted mode of lkb is preventing an earlier lkb in the
  convert queue from being granted, then deadlkdemote lkb.
  Example:
  Granted Queue: empty
  Convert Queue: NL->EX (first lock)
                 PR->EX (second lock)
  The first lock can't be granted because of the granted mode of the second
  lock and the second lock can't be granted because it's not first in the
  list.  We either cancel lkb's conversion (PR->EX) and return EDEADLK, or we
  demote the granted mode of lkb (from PR to NL) if it has the CONVDEADLK
  flag set and return DEMOTED in the lksb flags.
  Originally, this function detected conv-deadlk in a more limited scope:
  - if !modes_compat(lkb1, lkb2) && !modes_compat(lkb2, lkb1), or
  - if lkb1 was the first entry in the queue (not just earlier), and was
    blocked by the granted mode of lkb2, and there was nothing on the
    granted queue preventing lkb1 from being granted immediately, i.e.
    lkb2 was the only thing preventing lkb1 from being granted.
  That second condition meant we'd only say there was conv-deadlk if
  resolving it (by demotion) would lead to the first lock on the convert
  queue being granted right away.  It allowed conversion deadlocks to exist
  between locks on the convert queue while they couldn't be granted anyway.
  Now, we detect and take action on conversion deadlocks immediately when
  they're created, even if they may not be immediately consequential.  If
  lkb1 exists anywhere in the convert queue and lkb2 comes in with a granted
  mode that would prevent lkb1's conversion from being granted, we do a
  deadlkdemote on lkb2 right away and don't let it onto the convert queue.
  I think this means that the lkb_is_ahead condition below should always
  be zero, i.e. there will never be conv-deadlk between two locks that are
  both already on the convert queue.
  Return 1 if the lock can be granted, 0 otherwise.
  Also detect and resolve conversion deadlocks.
  lkb is the lock to be granted
  now is 1 if the function is being called in the context of the
  immediate request, it is 0 if called later, after the lock has been
  queued.
  recover is 1 if dlm_recover_grant() is trying to grant conversions
  after recovery.
  References are from chapter 6 of "VAXcluster Principles" by Roy Davis
	
	  6-10: Version 5.4 introduced an option to address the phenomenon of
	  a new request for a NL mode lock being blocked.
	 
	  6-11: If the optional EXPEDITE flag is used with the new NL mode
	  request, then it would be granted.  In essence, the use of this flag
	  tells the Lock Manager to expedite theis request by not considering
	  what may be in the CONVERTING or WAITING queues...  As of this
	  writing, the EXPEDITE flag can be used only with new requests for NL
	  mode locks.  This flag is not valid for conversion requests.
	 
	  A shortcut.  Earlier checks return an error if EXPEDITE is used in a
	  conversion or used with a non-NL requested mode.  We also know an
	  EXPEDITE request is always granted immediately, so now must always
	  be 1.  The full condition to grant an expedite request: (now &&
	  !conv && lkb->rqmode == DLM_LOCK_NL && (flags & EXPEDITE)) can
	  therefore be shortened to just checking the flag.
	
	  A shortcut. Without this, !queue_conflict(grantqueue, lkb) would be
	  added to the remaining conditions.
	
	  6-3: By default, a conversion request is immediately granted if the
	  requested mode is compatible with the modes of all other granted
	  locks
	
	  The RECOVER_GRANT flag means dlm_recover_grant() is granting
	  locks for a recovered rsb, on which lkb's have been rebuilt.
	  The lkb's may have been rebuilt on the queues in a different
	  order than they were in on the previous master.  So, granting
	  queued conversions in order after recovery doesn't make sense
	  since the order hasn't been preserved anyway.  The new order
	  could also have created a new "in place" conversion deadlock.
	  (e.g. old, failed master held granted EX, with PR->EX, NL->EX.
	  After recovery, there would be no granted locks, and possibly
	  NL->EX, PR->EX, an in-place conversion deadlock.)  So, after
	  recovery, grant conversions without considering order.
	
	  6-5: But the default algorithm for deciding whether to grant or
	  queue conversion requests does not by itself guarantee that such
	  requests are serviced on a "first come first serve" basis.  This, in
	  turn, can lead to a phenomenon known as "indefinate postponement".
	 
	  6-7: This issue is dealt with by using the optional QUECVT flag with
	  the system service employed to request a lock conversion.  This flag
	  forces certain conversion requests to be queued, even if they are
	  compatible with the granted modes of other locks on the same
	  resource.  Thus, the use of this flag results in conversion requests
	  being ordered on a "first come first servce" basis.
	 
	  DCT: This condition is all about new conversions being able to occur
	  "in place" while the lock remains on the granted queue (assuming
	  nothing else conflicts.)  IOW if QUECVT isn't set, a conversion
	  doesn't _have_ to go onto the convert queue where it's processed in
	  order.  The "now" variable is necessary to distinguish converts
	  being received and processed for the first time now, because once a
	  convert is moved to the conversion queue the condition below applies
	  requiring fifo granting.
	
	  Even if the convert is compat with all granted locks,
	  QUECVT forces it behind other locks on the convert queue.
	
	  The NOORDER flag is set to avoid the standard vms rules on grant
	  order.
	
	  6-3: Once in that queue [CONVERTING], a conversion request cannot be
	  granted until all other conversion requests ahead of it are granted
	  andor canceled.
	
	  6-4: By default, a new request is immediately granted only if all
	  three of the following conditions are satisfied when the request is
	  issued:
	  - The queue of ungranted conversion requests for the resource is
	    empty.
	  - The queue of ungranted new requests for the resource is empty.
	  - The mode of the new request is compatible with the most
	    restrictive mode of all granted locks on the resource.
	
	  6-4: Once a lock request is in the queue of ungranted new requests,
	  it cannot be granted until the queue of ungranted conversion
	  requests is empty, all ungranted new requests ahead of it are
	  granted andor canceled, and it is compatible with the granted mode
	  of the most restrictive lock granted on the resource.
	
	  The CONVDEADLK flag is non-standard and tells the dlm to resolve
	  conversion deadlocks by demoting grmode to NL, otherwise the dlm
	  cancels one of the locks.
	
	  The ALTPR and ALTCW flags are non-standard and tell the dlm to try
	  to grant a request in a mode other than the normal rqmode.  It's a
	  simple way to provide a big optimization to applications that can
	  use them.
 Returns the highest requested mode of all blocked conversions; sets
			
			  If DLM_LKB_NODLKWT flag is set and conversion
			  deadlock is detected, we request blocking AST and
			  down (or cancel) conversion.
 cw of 1 means there's a lock with a rqmode of DLM_LOCK_CW that's blocked
   on either the convert or waiting queue.
   high is the largest rqmode of all locks blocked on the convert or
	
	  If there are locks left on the waitconvert queue then send blocking
	  ASTs to granted locks based on the largest requested mode (high)
	  found above.
 skip self when sending basts to convertqueue 
 set_master(r, lkb) -- set the master nodeid of a resource
   The purpose of this function is to set the nodeid field in the given
   lkb using the nodeid field in the given rsb.  If the rsb's nodeid is
   known, it can just be copied to the lkb and the function will return
   0.  If the rsb's nodeid is _not_ known, it needs to be looked up
   before it can be copied to the lkb.
   When the rsb nodeid is being looked up remotely, the initial lkb
   causing the lookup is kept on the ls_waiters list waiting for the
   lookup reply.  Other lkb's waiting for the same rsb lookup are kept
   on the rsb's res_lookup list until the master is verified.
   Return values:
   0: nodeid is set in rsblkb and the caller should go ahead and use it
   1: the rsb master is not available and the lkb has been placed on
      a wait queue
		 This is a somewhat unusual case; find_rsb will usually
		   have set res_master_nodeid when dir nodeid is local, but
		   there are cases where we become the dir node after we've
		   past find_rsb and go through _request_lock again.
		   confirm_master() or process_lookup_list() needs to be
 confirm_master -- confirm (or deny) an rsb's master nodeid 
		 the remote request failed and won't be retried (it was
		   a NOQUEUE, or has been canceledunlocked); make a waiting
 check for invalid arg usage 
	 these args will be copied to the lkb in validate_lock_args,
	   it cannot be done now because when converting locks, fields in
 when dlm_unlock() sees -EBUSY with CANCELFORCEUNLOCK it returns 0
 note: it's valid for lkb_nodeidres_nodeid to be -1 when we get here
   because there may be a lookup in progress and it's valid to do
	 an lkb may still exist even though the lock is EOL'ed due to a
	   cancel, unlock or failed noqueue request; an app can't use these
	 an lkb may be waiting for an rsb lookup to complete where the
 undoes create_lkb() 
 caller changes -EBUSY to 0 for CANCEL and FORCEUNLOCK 
 cancel not allowed with another cancelunlock in progress 
 don't let scand try to do a cancel 
 there's nothing to cancel 
 add_to_waiters() will set OVERLAP_CANCEL 
	 do we need to allow a force-unlock if there's a normal unlock
	   already in progress?  in what conditions could the normal unlock
 don't let scand try to do a cancel 
 add_to_waiters() will set OVERLAP_UNLOCK 
 normal unlock not allowed if there's any op in progress 
 an overlapping op shouldn't blow away exflags from other op 
  Four stage 4 varieties:
  do_request(), do_convert(), do_unlock(), do_cancel()
  These are called on the master node for the given lock and
  from the central locking logic.
 changing an existing lock may allow others to be granted 
	 can_be_granted() detected that this lock would block in a conversion
	   deadlock, so we leave it on the granted queue and return EDEADLK in
 it's left on the granted queue 
	 is_demoted() means the can_be_granted() above set the grmode
	   to NL, and left us on the granted queue.  This auto-demotion
	   (due to CONVDEADLK) might mean other locks, andor this lock, are
	   now grantable.  We have to try to grant other converting locks
 else fall through and move to convert queue 
 grant_pending_locks also sends basts 
 returns: 0 did nothing, -DLM_ECANCEL canceled lock 
  Four stage 3 varieties:
  _request_lock(), _convert_lock(), _unlock_lock(), _cancel_lock()
 add a new lkb to a possibly new rsb, called by requesting process 
 set_master: sets lkb nodeid from r 
 receive_request() calls do_request() on remote node 
		 for remote locks the request_reply is sent
 change some property of an existing lkb, e.g. mode 
 receive_convert() calls do_convert() on remote node 
		 for remote locks the convert_reply is sent
 remove an existing lkb from the granted queue 
 receive_unlock() calls do_unlock() on remote node 
		 for remote locks the unlock_reply is sent
 remove an existing lkb from the convert or wait queue 
 receive_cancel() calls do_cancel() on remote node 
		 for remote locks the cancel_reply is sent
  Four stage 2 varieties:
  request_lock(), convert_lock(), unlock_lock(), cancel_lock()
  Two stage 1 varieties:  dlm_lock() and dlm_unlock()
  sendreceive routines for remote operations and replies
  send_args
  send_common
  send_request			receive_request
  send_convert			receive_convert
  send_unlock			receive_unlock
  send_cancel			receive_cancel
  send_grant			receive_grant
  send_bast			receive_bast
  send_lookup			receive_lookup
  send_remove			receive_remove
  				send_common_reply
  receive_request_reply	send_request_reply
  receive_convert_reply	send_convert_reply
  receive_unlock_reply		send_unlock_reply
  receive_cancel_reply		send_cancel_reply
  receive_lookup_reply		send_lookup_reply
	 get_buffer gives us a message handle (mh) that we need to
	   pass into midcomms_commit and a message buffer (mb) that we
 further lowcomms enhancements or alternate implementations may make
	 m_result and m_bastmode are set from function args,
	 compare with switch in create_message; send_remove() doesn't
 down conversions go without a reply from the master 
 FIXME: if this lkb is the only lock we hold on the rsb, then set
   MASTER_UNCERTAIN to force the next request on the rsb to confirm
 which args we save from a received message depends heavily on the type
   of message, unlike the send side where we can safely send everything about
 lkb was just created so there won't be an lvb yet 
 We fill in the stub-lkb fields with the info that send_xxxx_reply()
 This is called after the rsb is locked so that we can safely inspect
 use ls->remove_name2 to avoid conflict with shrink? 
	 The dir node is the authority on whether we are the master
	   for this rsb or not, so if the master sends us a request, we should
	   recreate the rsb if we've destroyed it.   This race happens when we
	   send a remove message to the dir node at the same time that the dir
	 TODO: instead of returning ENOTBLK, add the lkb to res_lookup
	   and do this receive_request again from process_lookup_list once
	   we get the lookup reply.  This would avoid a many repeated
	   ENOTBLK request failures when the lookup reply designating us
	 We could repeatedly return -EBADR here if our send_remove() is
	   delayed in being sentarrivingbeing processed on the dir node.
	   Another node would repeatedly lookup up the master, and the dir
	   node would continue returning our nodeid until our send_remove
	   took effect.
	   We send another remove message in case our previous send_remove
 Optimization: we're master so treat lookup as a request 
	 Look for name on rsbtbl.toss, if it's there, kill it.
	   If it's on rsbtbl.keep, it's being used, and we should ignore this
	   message.  This is an expected race between the dir node sending a
	   request to the master node at the same time as the master node sends
	   a remove to the dir node.  The resolution to that race is for the
	   dir node to ignore the remove message, and the master node to
	   recreate the master rsb when it gets a request from the dir node for
 verify the rsb is on keep list per comment above 
 should not happen 
 should not happen 
	 Optimization: the dir node was also the master, so it took our
 this is the value returned from do_request() on the master 
 request would block (be queued) on remote master 
 undoes create_lkb() 
 request was queued or granted on remote master 
 find_rsb failed to find rsb or rsb wasn't master 
 cause _request_lock->set_master->send_lookup 
 we'll ignore error in cancelunlock reply 
 undoes create_lkb() 
 this is the value returned from do_convert() on the master 
 convert would block (be queued) on remote master 
 convert was queued on remote master 
 convert was granted on remote master 
 stub reply can happen with waiters_mutex held 
 stub reply can happen with waiters_mutex held 
 this is the value returned from do_unlock() on the master 
 stub reply can happen with waiters_mutex held 
 this is the value returned from do_cancel() on the master 
	 ms->m_result is the value returned by dlm_master_lookup on dir node
	 We sometimes receive a request from the dir node for this
	   rsb before we've received the dir node's loookup_reply for it.
	   The request from the dir node implies we're the master, so we set
	   ourself as master in receive_request_reply, and verify here that
 This should never happen 
 the remote node doesn't believe it's the dir node 
 set_master() will set lkb_nodeid from r 
 undoes create_lkb() 
 messages sent to a master node 
 messages sent from a master node (replies to above) 
 messages sent from a master node (only two types of async msg) 
 messages sent to a dir node 
 messages sent from a dir node (remove has no reply) 
 other messages 
	
	  When checking for ENOENT, we're checking the result of
	  find_lkb(m_remid):
	 
	  The lock id referenced in the message wasn't found.  This may
	  happen in normal usage for the async messages and cancel, so
	  only use log_debug for them.
	 
	  Some errors are expected and normal.
 If the lockspace is in recovery mode (locking stopped), then normal
   messages are saved on the requestqueue for processing after recovery is
   done.  When not in recovery mode, we wait for dlm_recoverd to drain saved
   messages off the requestqueue before we process new ones. This occurs right
   after recovery completes when we transition from saving all messages on
   requestqueue, to processing all the saved messages, to processing new
		 If we were a member of this lockspace, left, and rejoined,
		   other nodes may still be sending us messages from the
 This is called by dlm_recoverd to process messages that were saved on
 This is called by the midcomms layer when something is received for
   the lockspace.  It could be either a MSG (normal message sent as part of
   standard locking activity) or an RCOM (recovery message sent as part of
	 this rwsem allows dlm_ls_stop() to wait for all dlm_recv threads to
 Same special case as in receive_rcom_lock_args() 
	 lkb->lkb_rqmode < lkb->lkb_grmode shouldn't happen since down
 A waiting lkb needs recovery if the master node has failed, or
 Recovery for locks that are waiting for replies from nodes that are now
   gone.  We can just complete unlocks and cancels by faking a reply from the
   dead node.  Requests and up-conversions we flag to be resent after
   recovery.  Down-conversions can just be completed with a fake reply like
		 exclude debug messages about unlocks because there can be so
		 all outstanding lookups, regardless of destination  will be
		 Main reply may have been received leaving a zero wait_type,
		   but a reply for the overlapping op may not have been
		   received.  In that case we need to fake the appropriate
 Deal with lookups and lkb's marked RESEND from _pre.  We may now be the
   master or dir-node for r.  Processing the lkb may result in it being placed
 We do this after normal locking has been enabled and any saved messages
   (in requestqueue) have been processed.  We should be confident that at
   this point we won't get or process a reply to any of these waiting
   operations.  But, new ops may be coming in on the rsbslocks here from
 there may have been an overlap unlockcancel prior to recovery or after
   recovery.  if before, the lkb may still have a pos wait_count; if after, the
   overlap flag would just have been set and nothing new sent.  we can be
   confident here than any replies to either the initial op or overlap ops
		 At this point we assume that we won't get a reply to any
		   previous op or overlap op on this lock.  First, do a big
 for waiters list 
 do an unlock or cancel instead of resending 
 undoes create_lkb() 
		 don't purge lkbs we've added in recover_master_copy for
 this put should free the lkb 
			 tell recover_lvb to invalidate the lvb
 this put should free the lkb 
 Get rid of locks held by nodes that are gone. 
	 cache one removed nodeid to optimize the common
  Attempt to grant locks on resources that we are the master of.
  Locks may have become grantable during recovery because locks
  from departed nodes have been purged (or not rebuilt), allowing
  previously blocked locks to now be granted.  The subset of rsb's
  we are interested in are those with lkb's on either the convert or
  waiting queues.
  Simplest would be to go through each master rsb and check for non-empty
  convert or waiting queues, and attempt to grant on those rsbs.
  Checking the queues requires lock_rsb, though, for which we'd need
  to release the rsbtbl lock.  This would make iterating through all
  rsb's very inefficient.  So, we rely on earlier recovery routines
  to set RECOVER_GRANT on any rsb's that we should attempt to grant
  locks for.
 the RECOVER_GRANT flag is checked in the grant path 
 needs at least dlm_rcom + rcom_lock 
 don't set lkb_status because add_lkb wants to itself 
	 Conversions between PR and CW (middle modes) need special handling.
	   The real granted mode of these converting locks cannot be determined
 This lkb may have been recovered in a previous aborted recovery so we need
   to check if the rsb already has an lkb with the given remote nodeidlkid.
   If so we just send back a standard reply.  If not, we create a new lkb with
   the given values and send back our lkid.  We send back our lkid by sending
 needs at least dlm_rcom + rcom_lock 
	 In general we expect the rsb returned to be R_MASTER, but we don't
	   have to require it.  Recovery of masters on one node can overlap
	   recovery of locks on another node, so one node can send us MSTCPY
	   locks before we've made ourselves master of this rsb.  We can still
	   add new MSTCPY locks that we receive here without any harm; when
	   we make ourselves master, dlm_recover_masters() won't touch the
	 this is the new value returned to the lock holder for
 needs at least dlm_rcom + rcom_lock 
		 There's a chance the new master received our lock before
		   dlm_recover_master_reply(), this wouldn't happen if we did
	 an ack for dlm_recover_locks() which waits for replies from
	 After ua is attached to lkb it will be freed by dlm_free_lkb().
	   When DLM_IFL_USER is set, the dlm knows that this is a userspace
 add this new lkb to the per-process list of locks 
	 user can change the params on its lock when it converts it, or
  The caller asks for an orphan lock on a given resource with a given mode.
  If a matching lock exists, it's moved to the owner's list of locks and
  the lkid is returned.
	
	  The lkb reference from the ls_orphans list was not
	  removed above, and is now considered the reference
	  for the proc locks list.
 from validate_unlock_args() 
 dlm_user_add_cb() may have already taken lkb off the proc list 
 from validate_unlock_args() 
 same as cancel_lock(), but set DEADLOCK_CANCEL after lock_rsb 
 from validate_unlock_args() 
 lkb's that are removed from the waiters list by revert are just left on the
 reference for the ls_orphans list 
 The FORCEUNLOCK flag allows the unlock to go ahead even if the lkb isn't
   granted.  Regardless of what rsb queue the lock is on, it's removed and
   freed.  The IVVALBLK flag causes the lvb on the resource to be invalidated
 We have to release clear_proc_locks mutex before calling unlock_proc_lock()
   (which does lock_rsb) due to deadlock with receiving a message that does
 The ls_clear_proc_locks mutex protects against dlm_user_add_cb() which
   1) references lkb->ua which we free here and 2) adds lkbs to proc->asts,
 proc CLOSING flag is set so no more device_reads should look at proc->asts
   list, and no more device_writes should add lkb's to proc->locks list; so we
   shouldn't need to take asts_spin or locks_spin here.  this assumes that
   device readswritescloses are serialized -- FIXME: we may need to serialize
		 this removes the reference for the proc->locks list
		   added by dlm_user_request, it may result in the lkb
 in-progress unlocks 
 ref from proc->locks list 
 pid of 0 means purge all orphans 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2007 Red Hat, Inc.  All rights reserved.
 add the message headers 
 why this? 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2006-2010 Red Hat, Inc.  All rights reserved.
 Offsets may be zero if no data is present 
 Figure out if this lock is at the end of its life and no longer
   available for the application to use.  The lkb still exists until
   the final ast is read.  A lock becomes EOL in three situations:
     1. a noqueue request fails with EAGAIN
     2. an unlock completes with EUNLOCK
     3. a cancel of a waiting request completes with ECANCELEDEADLK
   An EOL lock needs to be removed from the process's list of locks.
   And we can't allow any new operation on an EOL lock.  This is
   not related to the lifetime of the lkb struct which is managed
 we could possibly check if the cancel of an orphan has resulted in the lkb
	 If ORPHANDEAD flag is set, it means the process is dead so an ast
	   can't be delivered.  For ORPHAN's, dlm_clear_proc_locks() freed
	   lkb->ua so we can't try to use it.  This second check is necessary
	   for cases where a completion ast is received for an operation that
 N.B. spin_lock locks_spin, not asts_spin 
	 The device is already registered.  This happens when the
		 this has to be set to NULL
		  to avoid a double-free in dlm_device_deregister
	 The device is not registered.  This happens when the lockspace
	   was never used from userspace, or when device_create_lockspace()
	 The final dlm_release_lockspace waits for references to go to
	   zero, so all processes will need to close their device for the
	   ls before the release will proceed.  release also calls the
	   device_deregister above.  Converting a positive return value
	   from release to zero means that userspace won't know when its
 Check the user's version matches ours 
  device_write
    device_user_lock
      dlm_user_request -> request_lock
      dlm_user_convert -> convert_lock
    device_user_unlock
      dlm_user_unlock -> unlock_lock
      dlm_user_cancel -> cancel_lock
    device_create_lockspace
      dlm_new_lockspace
    device_remove_lockspace
      dlm_release_lockspace
 a write to a lockspace device is a lock or unlock request, a write
	
	  can't compare against COMPATdlm_write_request32 because
	  we don't yet know if is64bit is zero
 add 1 after namelen so that the name string is terminated 
 do we really need this? can a write happen after a close? 
 Every process that opens the lockspace device has its own "proc" structure
   hanging off the open file that's used to keep track of locks owned by the
	 at this point no more lkb's should exist for this lockspace,
	   so there's no chance of dlm_user_add_ast() being called and
 for the find in device_open() 
	 FIXME: AUTOFREE: if this ls is no longer used do
	 FIXME: dlm1 provides for the user's bastparamaddr to not be updated
	   in a conversion unless the conversion is successful.  See code
	   in dlm_user_convert() for updating ua from ua_tmp.  OpenVMS, though,
	   notes that a new blocking AST address and parameter are set even if
	 copy lvb to userspace if there is one, it's been updated, and
 a read returns a single ast described in a struct dlm_lock_result 
 do we really need this? can a read happen after a close? 
	 if we empty lkb_callbacks, we don't want to unlock the spinlock
	   without removing lkb_cb_list; so empty lkb_cb_list is always
 rem_lkb_callback sets a new lkb_last_cast 
		 this shouldn't happen; lkb should have been removed from
 removes ref for proc->asts, may cause lkb to be freed 
 removes ref for proc->asts, may cause lkb to be freed 
 removes ref for proc->asts, may cause lkb to be freed 
	 dlm_controld hasn't started (or, has started, but not
	 This is to deal with versions of dlm_controld that don't
	   know about the monitor device.  We assume that if the
	   dlm_controld was started (above), but the monitor device
	   was never opened, that it's an old version.  dlm_controld
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2005-2007 Red Hat, Inc.  All rights reserved.
  Requests received while the lockspace is in recovery get added to the
  request queue and processed when recovery is complete.  This happens when
  the lockspace is suspended on some nodes before it is on others, or the
  lockspace is enabled on some while still suspended on others.
  Called by dlm_recoverd to process normal messages saved while recovery was
  happening.  Normal locking has been enabled before this is called.  dlm_recv
  upon receiving a message, will wait for all saved messages to be drained
  here before processing the message it got.  If a new dlm_ls_stop() arrives
  while we're processing these saved messages, it may block trying to suspend
  dlm_recv if dlm_recv is waiting for us in dlm_wait_requestqueue.  In that
  case, we don't abort since locking_stopped is still 0.  If dlm_recv is not
  waiting for us, then this processing may be aborted due to locking_stopped.
  After recovery is done, locking is resumed and dlm_recoverd takes all the
  saved requests and processes them as they would have been by dlm_recv.  At
  the same time, dlm_recv will start receiving new requests from remote nodes.
  We want to delay dlm_recv processing new requests until dlm_recoverd has
  finished processing the old saved requests.  We don't check for locking
  stopped here because dlm_ls_stop won't stop locking until it's suspended us
  (dlm_recv).
 the ls is being cleaned up and freed by release_lockspace 
	 directory operations are always purged because the directory is
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2005-2008 Red Hat, Inc.  All rights reserved.
 If a process was killed while waiting for the only plock on a file,
   locks_remove_posix will not see any lock on the file so it won't
   send an unlock-close to us to pass on to userspace to clean up the
   abandoned waiter.  So, we have to insert the unlock-close when the
		 fl_owner is lockd which doesn't distinguish
 Returns failure iff a successful lock operation should be canceled 
 check if the following 2 are still valid or make a copy 
 got fs lock; bookkeep locally as well: 
		
		  This can only happen in the case of kmalloc() failure.
		  The filesystem's own lock is the authoritative lock,
		  so a failure to get the lock locally is not a disaster.
		  As long as the fs cannot reliably cancel locks (especially
		  in a low-memory situation), we're better off ignoring
		  this failure than trying to recover.
 XXX: We need to cancel the fs lock here: 
 cause the vfs unlock to return ENOENT if lock is not found 
	 info.rv from userspace is 1 for conflict, 0 for no-conflict,
 a read copies out one plock request from the send list 
	 there is no need to get a reply from userspace for unlocks
	   that were generated by the vfs cleaning up for a close
 a write copies in one plock result that should match a plock_op
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
  Copyright (C) 2004-2009 Red Hat, Inc.  All rights reserved.
  lowcomms.c
  This is the "low-level" comms layer.
  It is responsible for sendingreceiving messages
  from other nodes in the cluster.
  Cluster nodes are referred to by their nodeids. nodeids are
  simply 32 bit numbers to the locking module - if they need to
  be expanded for the cluster infrastructure then that is its
  responsibility. It is this layer's
  responsibility to resolve these into IP address or
  whatever it needs for inter-node communication.
  The comms level is two kernel threads that deal mainly with
  the receiving of messages from other nodes and passing them
  up to the mid-level comms layer (which understands the
  message format) for execution by the locking core, and
  a send thread which does all the setting up of connections
  to remote nodes and the sending of data. Threads are not allowed
  to send their own data because it may cause them to wait in times
  of high load. Also, this way, the sending thread can collect together
  messages bound for one node and send them in one block.
  lowcomms will choose to use either TCP or SCTP as its transport layer
  depending on the configuration variable 'protocol'. This should be set
  to 0 (default) for TCP or 1 for SCTP. It should be configured using a
  cluster-wide mechanism as it must be the same on all nodes of the cluster
  for the DLM to function.
 Number of messages to send before rescheduling 
 NULL if not connected 
 So we know who we are in the list 
 List of outgoing writequeue_entries 
 Receive workqueue 
 Send workqueue 
 wait for graceful shutdown 
 An entry waiting to be sent 
 new()commit() idx exchange 
 What to do to shutdown 
 What to do to eof check 
 Work queues 
 need to held writequeue_lock 
  If 'allocation' is zero then we don't attempt to create a new
  connection structure for this node.
	 Because multiple workqueuesthreads calls this function it can
	  race on multiple cpu's. Instead of locking hot path __find_con()
	  we just check in rare cases of recently added nodes again
	  under protection of connections_lock. If this is the case we
	  abort our connection creation and return the existing connection.
 Loop round all connections 
 caller need to held dlm_node_addrs_spin lock 
 Data available on socket or listen socket received a connect 
	 SCTP layer is not calling sk_data_ready when the connection
	  is done, so we catch the signal through here. Also, it
	  doesn't switch socket state when entering shutdown, so we
	  skip the write in that case.
 below sendcon only handling 
 Note: sk_callback_lock must be locked before calling this function. 
 Install a data_ready callback 
 Make a socket active 
 Install a data_ready callback 
 Add the port number to an IPv6 or 4 sockaddr and return the address
 Close a remote connection and tidy up 
 Will only re-enter once. 
	 if we send a writequeue entry only a half way, we drop the
	  whole entry because reconnection and that we not start of the
	  middle of a msg which will confuse the other end.
	 
	  we can always drop messages because retransmits, but what we
	  cannot allow is to transmit half messages which may be processed
	  at the other side.
	 
	  our policy is to start on a clean state when disconnects, we don't
	  know what's sendreceived on transport layer in this case.
 nothing to shutdown 
 copy any leftover from last receive 
 swap to new buffer space 
 Data received from remote end 
 realloc if we get new buffer size to read out 
		 calculate new buffer parameter regarding last receive and
		  possible leftover bytes
 new buflen according readed bytes and leftover from last receive 
		 calculate leftover bytes from process and put it into begin of
		  the receive buffer, so next receive we have the full message
		  at the start address of the receive buffer.
 handling for tcp shutdown 
 signal to breaking receive worker 
 Listening socket is busy, accept a connection 
 Get the connected socket's peer 
 Get the new node's NODEID 
	  Check to see if we already have a connection to this node. This
	   could happen if the two nodes initiate a connection at roughly
	   the same time and the connections cross on the wire.
	   In this case we store the incoming one in "othercon"
 close other sock con if we have something new 
		 accept copies the sk after we've saved the callbacks, so we
		   don't want to save them a second time or comm errors will
	
	  Add it to the active queue in case we got data
	  between processing the accept adding the socket
	  to the read_sockets list
  writequeue_entry_complete - try to delete and free write queue entry
  @e: write queue entry to try to delete
  @completed: bytes completed
  writequeue_lock must be held.
 signal that page was half way transmitted 
  sctp_bind_addrs - bind a SCTP socket to all our addresses
 Get local addresses 
	 this mutex is being used as a wait to avoid multiple "fast"
	  new writequeue page list entry allocs in new_wq_entry in
	  normal operation which is sleepable context. Without it
	  we could end in multiple writequeue entries with one
	  dlm message because multiple callers were waiting at
	  the writequeue_lock in new_wq_entry().
 we assume if successful commit must called 
 does not held connections_srcu, usage workqueue only 
 Send a message 
				 Notify TCP that we're limited by the
				  application window size.
 Don't starve people filling buffers 
 close if we got EOF 
 handling for tcp shutdown 
 Called from recovery when it knows that a node has
 Receive workqueue function 
 Some odd races can cause double-connects, ignore them 
 Create a socket to communicate with 
	
	  Some errors are fatal and this list might need adjusting. For other
	  errors we try again until the max number of retries is reached.
 Send workqueue function 
	 Set all the flags to prevent any
	  socket activity.
	 Bind to our cluster-known address connecting to avoid
	  routing problems.
 This may not indicate a critical error 
 We don't support multi-homed hosts 
 Turn off Nagle's algorithm 
 Bind to our port 
	
	  Make sock->ops->connect() function return in specified time,
	  since O_NONBLOCK argument in connect() function does not work here,
	  then, we should restore the default value of this attribute.
 Turn off Nagle's algorithm 
 Start listening 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
  Copyright (C) 2004-2011 Red Hat, Inc.  All rights reserved.
 If the start for which we're re-enabling locking (seq) has been superseded
   by a newer stop (ls_recover_seq), we need to leave locking disabled.
   We suspend dlm_recv threads here to avoid the race where dlm_recv a) sees
   locking stopped and b) adds a message to the requestqueue, but dlm_recoverd
 unblocks processes waiting to enter the dlm 
	
	  This list of root rsb's will be the basis of most of the recovery
	  routines.
	
	  Add or remove nodes from the lockspace's ls_nodes list.
	
	  Rebuild our own share of the directory by collecting from all other
	  nodes their master rsb names that hash to us.
	
	  We may have outstanding operations that are waiting for a reply from
	  a failed node.  Mark these to be resent after recovery.  Unlock and
	  cancel ops can just be completed.
		
		  Clear lkb's for departed nodes.
		
		  Get new master nodeid's for rsb's that were mastered on
		  departed nodes.
		
		  Send our locks on remastered rsb's to the new masters.
		
		  Finalize state in master rsb's now that all locks can be
		  checked.  This includes conversion resolution and lvb
		  settings.
		
		  Other lockspace members may be going through the "neg" steps
		  while also adding us to the lockspace, in which case they'll
		  be doing the recover_locks (RS_LOCKS) barrier.
	
	  Purge directory-related requests that are saved in requestqueue.
	  All dir requests from before recovery are invalid now due to the dir
	  rebuild and will be resent by the requesting nodes.
 The dlm_ls_start() that created the rv we take here may already have been
   stopped via dlm_ls_stop(); in that case we need to leave the RECOVERY_STOP
		
		  We call kthread_should_stop() after set_current_state().
		  This is because it works correctly if kthread_stop() is
		  called just before set_current_state().
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
  Copyright (C) 2004-2007 Red Hat, Inc.  All rights reserved.
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
  Copyright (C) 2004-2007 Red Hat, Inc.  All rights reserved.
 SPDX-License-Identifier: MIT
  VirtualBox Guest Shared Folders support: Virtual File System.
  Module initializationfinalization
  File system registrationderegistration
  Superblock reading
  Few utility functions
  Copyright (C) 2006-2018 Oracle Corporation
 'VBox' little endian 
 forward declaration 
 Load nls if not utf8 
 Turn source into a shfl_string and map the folder 
	
	  vboxsf_free_inode uses the idr, make sure all delayed rcu free
	  inodes are flushed.
	
	  Don't return 0 here since the guest may then think that it is not
	  possible to create any more files.
 Apply changed options to the root inode 
 Module initializationfinalization handlers 
		
		  Make sure all delayed rcu free inodes are flushed
		  before we destroy the cache.
 SPDX-License-Identifier: MIT
  VirtualBox Guest Shared Folders support: Directory inode and file operations
  Copyright (C) 2006-2018 Oracle Corporation
		
		  Note the vboxsf_dir_info objects we are iterating over here
		  are variable sized, so the info pointer may end up being
		  unaligned. This is how we get the data from the host.
		  Since vboxsf is only supported on x86 machines this is not
		  a problem.
 Only happens if the host gives us corrupt data 
 Info now points to the right entry, emit it. 
		
		  On 32-bit systems pos is 64-bit signed, while ino is 32-bit
		  unsigned so fake_ino may overflow, check for this.
 skip erroneous entry and proceed 
  This is called during name resolutionlookup to check if the @dentry in
  the cache is still valid. the job is handled by vboxsf_inode_revalidate.
 iops 
 The host may have given us different attr then requested 
 parent directory accesschange time changed 
 Only creates 
 This also closes the handle passed to vboxsf_create_sf_handle() 
 parent directory accesschange time changed 
 parent directories accesschange time changed 
 -EROFS means symlinks are note support -> -EPERM 
 parent directory accesschange time changed 
 SPDX-License-Identifier: MIT
  VirtualBox Guest Shared Folders support: Utility functions.
  Mainly conversion fromto VirtualBoxLinux data structures.
  Copyright (C) 2006-2018 Oracle Corporation
 set [inode] attributes based on [info], uidgid based on [sbi] 
 We use the host-side values for these 
			
			  XXX: this probably should be set to the number of entries
			  in the directory plus two (. ..)
 i_blocks always in units of 512 bytes! 
	
	  If the file was changed on the host side we need to invalidate the
	  page-cache for it.  Note this also gets triggered by our own writes,
	  this is unavoidable.
 this is at least required for Posix hosts 
	
	  Setting the file size and setting the other attributes has to
	  be handled separately.
		
		  Ignore ctime (inode change time) as it can't be set
		  from userland anyway.
 the host may have given us different attr then requested 
 the host may have given us different attr then requested 
 Update the inode with what the host has actually given us. 
  [dentry] contains string encoded in coding system that corresponds
  to [sbi]->nls, we must convert it to UTF8 here.
  Returns a shfl_string allocated through __getname (must be freed using
  __putname), or an ERR_PTR on error.
		
		  dentry_path stores the name at the end of buf, but the
		  shfl_string string we return must be properly aligned.
 Reserve space for terminating 0 
 vboxsf_dirinfo returns 1 on end of dir 
 -EILSEQ means the host could not translate a filename, ignore 
 SPDX-License-Identifier: MIT
  VirtualBox Guest Shared Folders support: Regular file inode and file ops.
  Copyright (C) 2006-2018 Oracle Corporation
 the host may have given us different attr then requested 
 init our handle struct and add it to the inode's handles list 
	
	  We check the value of params.handle afterwards to find out if
	  the call succeeded or failed, as the API does not seem to cleanly
	  distinguish error and informational messages.
	 
	  Furthermore, we must set params.handle to SHFL_HANDLE_NIL to
	  make the shared folders host service use our mode parameter.
		
		  We ignore O_EXCL, as the Linux kernel seems to call create
		  beforehand itself, so O_EXCL should always fail.
	
	  When a file is closed on our (the guest) side, we want any subsequent
	  accesses done on the host side to see all changes done from our side.
  Write back dirty pages now, because there may not be any suitable
  open files later
  Note that since we are accessing files on the host's filesystem, files
  may always be changed underneath us by the host!
  The vboxsf API between the guest and the host does not offer any functions
  to deal with this. There is no inode-generation to check for changes, no
  events  callback on changes and no way to lock files.
  To avoid returning stale data when a file gets opened on our (the guest)
  side, we do a "stat" on the host side, then compare the mtime with the
  last known mtime and invalidate the page-cache if they differ.
  This is done from vboxsf_inode_revalidate().
  When reads are done through the read_iter fop, it is possible to do
  further cache revalidation then, there are 3 options to deal with this:
  1)  Rely solely on the revalidation done at open time
  2)  Do another "stat" and compare mtime again. Unfortunately the vboxsf
      host API does not allow stat on handles, so we would need to use
      file->f_path.dentry and the stat will then fail if the file was unlinked
      or renamed (and there is no thing like NFS' silly-rename). So we get:
  2a) "stat" and compare mtime, on stat failure invalidate the cache
  2b) "stat" and compare mtime, on stat failure do nothing
  3)  Simply always call invalidate_inode_pages2_range on the range of the read
  Currently we are keeping things KISS and using option 1. this allows
  directly using generic_file_read_iter without wrapping it.
  This means that only data written on the host side before open() on
  the guest side is guaranteed to be seen by the guest. If necessary
  we may provide other read-cache strategies in the future and make this
  configurable through a mount option.
 mtime changed 
 zero the stale part of the page if we did a short copy 
 mtime changed 
  Note simple_write_begin does not read the page from disk on partial writes
  this is ok since vboxsf_write_end only writes the written parts of the
  page and it does not call SetPageUptodate for partial writes.
 SPDX-License-Identifier: MIT
  Wrapper functions for the shfl host calls.
  Copyright (C) 2006-2018 Oracle Corporation
 No guest-device 
 guest-device is gone, already disconnected 
 guest-dev removed underneath us 
  vboxsf_create - Create a new file or folder
  @root:         Root of the shared folder in which to create the file
  @parsed_path:  The path of the file or folder relative to the shared folder
  @param:        create_parms Parameters for filefolder creation.
  Create a new file or folder or open an existing one in a shared folder.
  Note this function always returns 0  success unless an exceptional condition
  occurs - out of memory, invalid arguments, etc. If the file or folder could
  not be opened or created, create_parms->handle will be set to
  SHFL_HANDLE_NIL on return.  In this case the value in create_parms->result
  provides information as to why (e.g. SHFL_FILE_EXISTS), create_parms->result
  is also set on success as additional information.
  Returns:
  0 or negative errno value.
 Returns 0 on success, 1 on end-of-dir, negative errno otherwise 
 out parameter only 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2016 Namjae Jeon <linkinjeon@kernel.org>
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
  Fixed format data defining GSS header and fixed string
  "not_defined_in_RFC4178@please_ignore".
  So sec blob data in neg phase could be generated statically.
  ksmbd_gen_sess_key() - function to generate session key
  @sess:	session of connection
  @hash:	source hash value to be used for find session key
  @hmac:	source hmac value to be used for finding session key
 convert user_name to unicode 
 Convert domain name or conn name to unicode and uppercase 
  ksmbd_auth_ntlmv2() - NTLMv2 authentication handler
  @sess:	session of connection
  @ntlmv2:		NTLMv2 challenge response
  @blen:		NTLMv2 blob length
  @domain_name:	domain name
  Return:	0 on success, error number on error
  ksmbd_decode_ntlmssp_auth_blob() - helper function to construct
  authenticate blob
  @authblob:	authenticate blob source pointer
  @usr:	user details
  @sess:	session of connection
  Return:	0 on success, error number on error
 TODO : use domain name that imported from configuration file 
 process NTLMv2 authentication 
  ksmbd_decode_ntlmssp_neg_blob() - helper function to construct
  negotiate blob
  @negblob: negotiate blob source pointer
  @rsp:     response header pointer to be updated
  @sess:    session of connection
  ksmbd_build_ntlmssp_challenge_blob() - helper function to construct
  challenge blob
  @chgblob: challenge blob source pointer to initialize
  @rsp:     response header pointer to be updated
  @sess:    session of connection
 Initialize random conn challenge 
 Add Target Information to security buffer 
 Add target info list for NetBIOSDNS settings 
 Add terminator subblock 
  ksmbd_sign_smb2_pdu() - function to generate packet signing
  @conn:	connection
  @key:	signing key
  @iov:        buffer iov array
  @n_vec:	number of iovecs
  @sig:	signature value generated for client request packet
  ksmbd_sign_smb3_pdu() - function to generate packet signing
  @conn:	connection
  @key:	signing key
  @iov:        buffer iov array
  @n_vec:	number of iovecs
  @sig:	signature value generated for client request packet
 Add two entries for transform header and signature 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2016 Namjae Jeon <linkinjeon@kernel.org>
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
 for vfs_path_lookup 
  ksmbd_vfs_lock_parent() - lock parent dentry if it is stable
  the parent dentry got by dget_parent or @parent could be
  unstable, we try to lock a parent inode and lookup the
  child dentry again.
  the reference count of @parent isn't incremented.
  ksmbd_vfs_create() - vfs helper for smb create file
  @work:	work
  @name:	file name that is relative to share
  @mode:	file create mode
  Return:	0 on success, otherwise error
  ksmbd_vfs_mkdir() - vfs helper for smb create directory
  @work:	work
  @name:	directory name that is relative to share
  @mode:	directory create mode
  Return:	0 on success, otherwise error
  check_lock_range() - vfs helper for smb byte range file locking
  @filp:	the file to apply the lock to
  @start:	lock start byte offset
  @end:	lock end byte offset
  @type:	byte range type readwrite
  Return:	0 on success, otherwise error
 check conflict locks 
 check owner in lock 
  ksmbd_vfs_read() - vfs helper for smb file read
  @work:	smb work
  @fid:	file id of open file
  @count:	read byte count
  @pos:	file pos
  Return:	number of read bytes on success, otherwise error
  ksmbd_vfs_write() - vfs helper for smb file write
  @work:	work
  @fid:	file id of open file
  @buf:	buf containing data for writing
  @count:	read byte count
  @pos:	file pos
  @sync:	fsync after write
  @written:	number of bytes written
  Return:	0 on success, otherwise error
 Do we need to break any of a levelII oplock? 
  ksmbd_vfs_getattr() - vfs helper for smb getattr
  @work:	work
  @fid:	file id of open file
  @attrs:	inode attributes
  Return:	0 on success, otherwise error
  ksmbd_vfs_fsync() - vfs helper for smb fsync
  @work:	work
  @fid:	file id of open file
  Return:	0 on success, otherwise error
  ksmbd_vfs_remove_file() - vfs helper for smb rmdir or unlink
  @name:	directory or file name that is relative to share
  Return:	0 on success, otherwise error
  ksmbd_vfs_link() - vfs helper for creating smb hardlink
  @oldname:	source file name
  @newname:	hardlink name that is relative to share
  Return:	0 on success, otherwise error
  ksmbd_vfs_truncate() - vfs helper for smb file truncate
  @work:	work
  @fid:	file id of old file
  @size:	truncate to given size
  Return:	0 on success, otherwise error
 Do we need to break any of a levelII oplock? 
  ksmbd_vfs_listxattr() - vfs helper for smb list extended attributes
  @dentry:	dentry of file for listing xattrs
  @list:	destination buffer
  @size:	destination buffer length
  Return:	xattr list length on success, otherwise error
  ksmbd_vfs_getxattr() - vfs helper for smb get extended attributes value
  @user_ns:	user namespace
  @dentry:	dentry of file for getting xattrs
  @xattr_name:	name of xattr name to query
  @xattr_buf:	destination buffer xattr value
  Return:	read xattr value length on success, otherwise error
  ksmbd_vfs_setxattr() - vfs helper for smb set extended attributes value
  @user_ns:	user namespace
  @dentry:	dentry to set XATTR at
  @name:	xattr name for setxattr
  @value:	xattr value to set
  @size:	size of xattr value
  @flags:	destination buffer length
  Return:	0 on success, otherwise error
  ksmbd_vfs_set_fadvise() - convert smb IO caching options to linux options
  @filp:	file pointer for IO
  @options:	smb IO options
	
	  Shrink request scope to what the fs can actually handle.
  ksmbd_vfs_empty_dir() - check for empty directory
  @fp:	ksmbd file pointer
  Return:	true if directory empty, otherwise false
  ksmbd_vfs_lookup_in_dir() - lookup a file in a directory
  @dir:	path info
  @name:	filename to lookup
  @namelen:	filename length
  Return:	0 on success, otherwise error
  ksmbd_vfs_kern_path() - lookup a file and get path info
  @name:	file path that is relative to share
  @flags:	lookup flags
  @path:	if lookup succeed, return path info
  @caseless:	caseless filename lookup
  Return:	0 on success, otherwise error
  ksmbd_vfs_init_kstat() - convert unix stat information to smb stat format
  @p:          destination buffer
  @ksmbd_kstat:      ksmbd kstat wrapper
	
	  set default value for the case that store dos attributes is not yes
	  or that acl is disable in server's filesystem and the config is yes.
 Set default owner group 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2019 Samsung Electronics Co., Ltd.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2019 Samsung Electronics Co., Ltd.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2016 Namjae Jeon <linkinjeon@kernel.org>
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
  alloc_opinfo() - allocate a new opinfo object for oplock info
  @work:	smb work
  @id:		fid of open file
  @Tid:	tree id of connection
  Return:      allocated opinfo object on success, otherwise NULL
  opinfo_write_to_read() - convert a write oplock to read oplock
  @opinfo:		current oplock info
  Return:      0 on success, otherwise -EINVAL
  opinfo_read_handle_to_read() - convert a readhandle oplock to read oplock
  @opinfo:		current oplock info
  Return:      0 on success, otherwise -EINVAL
  opinfo_write_to_none() - convert a write oplock to none
  @opinfo:	current oplock info
  Return:      0 on success, otherwise -EINVAL
  opinfo_read_to_none() - convert a write read to none
  @opinfo:	current oplock info
  Return:      0 on success, otherwise -EINVAL
  lease_read_to_write() - upgrade lease state from read to write
  @opinfo:	current lease info
  Return:      0 on success, otherwise -EINVAL
  lease_none_upgrade() - upgrade lease state from none
  @opinfo:	current lease info
  @new_state:	new lease state
  Return:	0 on success, otherwise -EINVAL
  close_id_del_oplock() - release oplock object at file close time
  @fp:		ksmbd file pointer
  grant_write_oplock() - grant exclusivebatch oplock or write lease
  @opinfo_new:	new oplock info object
  @req_oplock: request oplock
  @lctx:	lease context information
  Return:      0
  grant_read_oplock() - grant level2 oplock or read lease
  @opinfo_new:	new oplock info object
  @lctx:	lease context information
  Return:      0
  grant_none_oplock() - grant none oplock or none lease
  @opinfo_new:	new oplock info object
  @lctx:	lease context information
  Return:      0
  same_client_has_lease() - check whether current lease request is
 		from lease owner of file
  @ci:		master file pointer
  @client_guid:	Client GUID
  @lctx:		lease context information
  Return:      oplock(lease) object on success, otherwise NULL
	
	  Compare lease key and client_guid to know request from same owner
	  of same client
 skip upgrading lease about breaking lease 
 upgrading lease 
 is this a timeout ? 
 memory barrier is needed for wake_up_bit() 
 Not immediately break to none. 
  __smb2_oplock_break_noti() - send smb2 oplock break cmd from conn
  to client
  @wk:     smb work object
  There are two ways this function can be called. 1- while file open we break
  from exclusivebatch lock to levelII oplock and 2- while file writetruncate
  we break from levelII oplock no oplock.
  work->request_buf contains oplock_info.
  smb2_oplock_break_noti() - send smb2 exclusivebatch to level2 oplock
 		break command from server to client
  @opinfo:		oplock info object
  Return:      0 on success, otherwise error
  __smb2_lease_break_noti() - send lease break command from server
  to client
  @wk:     smb work object
  smb2_lease_break_noti() - break lease when a new client request
 			write lease
  @opinfo:		conains lease state information
  Return:	0 on success, otherwise error
 Need to break exclusivebatch oplock, write lease or overwrite_if 
			
			  Create overwrite break trigger the lease break to
			  none.
  smb_grant_oplock() - handle oplocklease request on file open
  @work:		smb work
  @req_op_level:	oplock level
  @pid:		id of open file
  @fp:			ksmbd file pointer
  @tid:		Tree id of connection
  @lctx:		lease context information on file open
  @share_ret:		share mode
  Return:      0 on success, otherwise error
 not support directory lease 
 ci does not have any oplock 
 grant none-oplock if second open is trunc 
 is lease already granted ? 
 Check all oplock was freed by close 
 grant fixed oplock on stacked locking between lease and oplock 
  smb_break_all_write_oplock() - break batchexclusive oplock to level2
  @work:	smb work
  @fp:		ksmbd file pointer
  @is_trunc:	truncate on open
  smb_break_all_levII_oplock() - send level2 oplock or read lease break command
 	from server to client
  @work:	smb work
  @fp:		ksmbd file pointer
  @is_trunc:	truncate on open
 Skip oplock being break to none 
  smb_break_all_oplock() - break both batchexclusive and level2 oplock
  @work:	smb work
  @fp:		ksmbd file pointer
  smb2_map_lease_to_oplock() - map lease state to corresponding oplock type
  @lease_state:     lease type
  Return:      0 if no mapping, otherwise corresponding oplock type
  create_lease_buf() - create lease context for open cmd response
  @rbuf:	buffer to create lease context response
  @lease:	buffer to stored parsed lease state information
  parse_lease_state() - parse lease context containted in file open request
  @open_req:	buffer containing smb2 file open(create) request
  Return:  oplock state, -ENOENT if create lease context not found
  smb2_find_context_vals() - find a particular context info in open request
  @open_req:	buffer containing smb2 file open(create) request
  @tag:	context name to search for
  Return:	pointer to requested context, NULL if @str context not found
 		or error pointer if name length is invalid.
	
	  CreateContextsOffset and CreateContextsLength are guaranteed to
	  be valid because of ksmbd_smb2_check_message().
  create_durable_rsp_buf() - create durable handle context
  @cc:	buffer to create durable context response
 SMB2_CREATE_DURABLE_HANDLE_RESPONSE is "DHnQ" 
  create_durable_v2_rsp_buf() - create durable handle v2 context
  @cc:	buffer to create durable context response
  @fp: ksmbd file pointer
 SMB2_CREATE_DURABLE_HANDLE_RESPONSE_V2 is "DH2Q" 
  create_mxac_rsp_buf() - create query maximal access context
  @cc:			buffer to create maximal access context response
  @maximal_access:	maximal access
 SMB2_CREATE_QUERY_MAXIMAL_ACCESS_RESPONSE is "MxAc" 
 SMB2_CREATE_QUERY_ON_DISK_ID_RESPONSE is "QFid" 
  create_posix_rsp_buf() - create posix extension context
  @cc:	buffer to create posix on posix response
  @fp: ksmbd file pointer
 SMB2_CREATE_TAG_POSIX is "0x93AD25509CB411E7B42383DE968BCD7C" 
  Find lease object(opinfo) for given lease keyfid from lease
  breakfile close path.
  lookup_lease_in_table() - find a matching lease info object
  @conn:	connection instance
  @lease_key:	lease key to be searched for
  Return:      opinfo if found matching opinfo, otherwise NULL
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2016 Namjae Jeon <linkinjeon@kernel.org>
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
	
	  Make sure that this really is an SMB, that it is a response.
   The following table defines the expected "StructureSize" of SMB2 requests
   in order by SMB2 command.  This is similar to "wct" in SMBCIFS requests.
   Note that commands are defined in smb2pdu.h in le16 but the array below is
   indexed by command in host byte order
 SMB2_NEGOTIATE  cpu_to_le16(36),
 SMB2_SESSION_SETUP  cpu_to_le16(25),
 SMB2_LOGOFF  cpu_to_le16(4),
 SMB2_TREE_CONNECT  cpu_to_le16(9),
 SMB2_TREE_DISCONNECT  cpu_to_le16(4),
 SMB2_CREATE  cpu_to_le16(57),
 SMB2_CLOSE  cpu_to_le16(24),
 SMB2_FLUSH  cpu_to_le16(24),
 SMB2_READ  cpu_to_le16(49),
 SMB2_WRITE  cpu_to_le16(49),
 SMB2_LOCK  cpu_to_le16(48),
 SMB2_IOCTL  cpu_to_le16(57),
 SMB2_CANCEL  cpu_to_le16(4),
 SMB2_ECHO  cpu_to_le16(4),
 SMB2_QUERY_DIRECTORY  cpu_to_le16(33),
 SMB2_CHANGE_NOTIFY  cpu_to_le16(32),
 SMB2_QUERY_INFO  cpu_to_le16(41),
 SMB2_SET_INFO  cpu_to_le16(33),
 use 44 for lease break 
 SMB2_OPLOCK_BREAK  cpu_to_le16(36)
  The size of the variable area depends on the offset and length fields
  located in different fields for various SMB2 requests. SMB2 requests
  with no variable length info, show an offset of zero for the offset field.
 SMB2_NEGOTIATE  true,
 SMB2_SESSION_SETUP  true,
 SMB2_LOGOFF  false,
 SMB2_TREE_CONNECT 	true,
 SMB2_TREE_DISCONNECT  false,
 SMB2_CREATE  true,
 SMB2_CLOSE  false,
 SMB2_FLUSH  false,
 SMB2_READ 	true,
 SMB2_WRITE  true,
 SMB2_LOCK 	true,
 SMB2_IOCTL  true,
 SMB2_CANCEL  false, 
 SMB2_ECHO  false,
 SMB2_QUERY_DIRECTORY  true,
 SMB2_CHANGE_NOTIFY  false,
 SMB2_QUERY_INFO  true,
 SMB2_SET_INFO  true,
 SMB2_OPLOCK_BREAK  false
  Set length of the data area and the offset to arguments.
  if they are invalid, return error.
 error reqeusts do not have data area 
	
	  Following commands have data areas so we have to get the location
	  of the data buffer offset and data buffer length for the particular
	  command.
		
		  smb2_lock request size is 48 included single
		  smb2_lock_element structure size.
  Calculate the size of the SMB message based on the fixed header
  portion, the number of word parameters and the data portion of the message.
 the offset from the beginning of SMB to data area 
 the length of the variable length data area 
 Structure Size has already been checked to make sure it is 64 
	
	  StructureSize2, ie length of fixed parameter area has already
	  been checked to make sure it is the correct length.
	
	  StructureSize2 of smb2_lock pdu is set to 48, indicating
	  the size of smb2 lock request with single smb2_lock_element
	  regardless of number of locks. Subtract single
	  smb2_lock_element for correct buffer size check.
		
		  Check to make sure that data area begins after fixed area,
		  Note that last byte of the fixed area is part of data area
		  for some commands, typically those with odd StructureSize,
		  so we must add one to the calculation.
 calculated length 
 error packets have 9 byte structure size 
 special case for SMB2.1 lease break message 
 client can return one byte more due to implied bcc[0] 
		
		  Some windows servers (win2016) will pad also the final
		  PDU in a compound to 8 bytes.
		
		  windows client also pad up to 8 bytes when compounding.
		  If pad is longer than eight bytes, log the server behavior
		  (once), since may indicate a problem but allow it and
		  continue since the frame is parseable.
 SPDX-License-Identifier: LGPL-2.1+
    Copyright (C) International Business Machines  Corp., 2007,2008
    Author(s): Steve French (sfrench@us.ibm.com)
    Copyright (C) 2020 Samsung Electronics Co., Ltd.
    Author(s): Namjae Jeon <linkinjeon@kernel.org>
 security id for everyoneworld system group 
 security id for everyoneworld system group 
 security id for everyoneworld system group 
 security id for Authenticated Users system group 
 S-1-22-1 Unmapped Unix users 
 S-1-22-2 Unmapped Unix groups 
  See http:technet.microsoft.comen-uslibraryhh509017(v=ws.10).aspx
 S-1-5-88 MS NFS and Apple style UIDGIDmode 
 S-1-5-88-1 Unix uid 
 S-1-5-88-2 Unix gid 
 S-1-5-88-3 Unix mode 
  if the two SIDs (roughly equivalent to a UUID for a user or group) are
  the same returns zero, if they do not match returns non-zero.
 compare the revision 
 compare all of the six auth values 
 compare all of the subauth values if any 
 sids comparematch 
  change posix mode to reflect permissions
  pmode is the existing mode (we only want to overwrite part of this
  bits to set can be: S_IRWXU, S_IRWXG or S_IRWXO ie 00700 or 00070 or 00007
  Generate access flags to reflect permissions mode is the existing mode.
  This function is called for every ACE in the DACL whose SID matches
  with either owner or group or everyone.
 reset access mask 
 bits to use are either S_IRWXU or S_IRWXG or S_IRWXO 
	
	  check for RWX UGO since we do not know whose flags
	  is this but we have cleared all the bits sans RWX for
	  either user or group or other as per bits_to_use
 RID 
	
	  If we have too many subauthorities, then something is really wrong.
	  Just return an error.
		
		  Translate raw sid into kuid in the server's user
		  namespace.
 If this is an idmapped mount, apply the idmapping. 
		
		  Translate raw sid into kgid in the server's user
		  namespace.
 If this is an idmapped mount, apply the idmapping. 
	
	  In the worst case, each individual acl could be for a distinct
	  named user or group, but we don't know which, so we allocate
	  enough space for either:
 validate that we do not go past end of acl 
	
	  reset rwx permissions for usergroupother.
	  Also, if num_aces is 0 i.e. DACL has no ACEs,
	  usergroupother have no permissions
 The owner must be set to at least read-only. 
 owner RID 
 Group RID 
 creator owner 
 creator group 
 other 
	
	  validate that we do not go past end of ACL - sid must be at least 8
	  bytes long (assuming no sub-auths - e.g. the null SID
 Convert CIFS ACL to POSIX form 
 no need for SACL ptr 
 Convert permission bits from mode to equivalent CIFS ACL 
 no need for SACL ptr 
 Update posix acls 
 Check it only calling from SD BUFFER context 
 Update WinACL in xattr 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2016 Namjae Jeon <linkinjeon@kernel.org>
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
  match_pattern() - compare a string with a pattern which might include
  wildcard '' and '?'
  TODO : implement consideration about DOS_DOT, DOS_QM and DOS_STAR
  @string:	string to compare with a pattern
  @len:	string length
  @pattern:	pattern string which might include wildcard '' and '?'
  Return:	0 if pattern matched with the string, otherwise non zero value
  is_char_allowed() - check for valid character
  @ch:		input character to be checked
  Return:	1 if char is allowed, otherwise 0
 check for control chars, wildcards etc. 
  convert_to_nt_pathname() - extract and return windows path string
       whose share directory prefix was removed from file path
  @filename : unix filename
  @sharepath: share path string
  Return : windows path string or error
  ksmbd_extract_sharename() - get share name from tree connect request
  @treename:	buffer containing tree name and share name
  Return:      share name on success, otherwise error
 caller has to free the memory 
  convert_to_unix_name() - convert windows name to unix format
  @path:	name to be converted
  @tid:	tree id of mathing share
  Return:	converted name on success, otherwise NULL
 XXX 
 We allocate buffer twice bigger than needed. 
  Convert the NT UTC (based 1601-01-01, in hundred nanosecond units)
  into Unix UTC (based 1970-01-01, in seconds).
 Subtract the NTFS time offset, then convert to 1s intervals. 
	
	  Unfortunately can not use normal 64 bit division on 32 bit arch, but
	  the alternative, do_div, does not work with negative numbers so have
	  to special case them
 Convert the Unix UTC into NT UTC. 
 Convert to 100ns intervals and then add the NTFS time offset. 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
    Copyright (C) 2018 Namjae Jeon <linkinjeon@kernel.org>
for shortname implementation 
  ksmbd_verify_smb_message() - check for valid smb2 request header
  @work:	smb work
  check for valid smb signature and packet direction(requestresponse)
  Return:      0 on success, otherwise -EINVAL
  ksmbd_smb_request() - check for valid smb request type
  @conn:	connection instance
  Return:      true on success, otherwise false
 fill dot entry info 
  ksmbd_extract_shortname() - get shortname from long filename
  @conn:	connection instance
  @longname:	source long filename
  @shortname:	destination short filename
  Return:	shortname length or 0 when source long name is '.' or '..'
  TODO: Though this function comforms the restriction of 8.3 Filename spec,
  but the result is different with Windows 7's one. need to check.
no mangling required 
name starts with a dot
	
	  Lookup fp in master fp list, and check desired access and
	  shared mode between previous open and current open.
		
		  Only check FILE_SHARE_DELETE if stream opened and
		  normal file opened.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2016 Namjae Jeon <linkinjeon@kernel.org>
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
  check_conn_state() - check state of server thread connection
  @work:     smb work containing server thread information
  Return:	0 on valid connection, otherwise 1 to reconnect
 AndX commands - chained request can return positive values 
 either uid or tid is not correct 
		
		  Call smb2_set_rsp_credits() function to set number of credits
		  granted in hdr of smb2 response.
  handle_ksmbd_work() - process pending smb work requests
  @wk:	smb work containing request command buffer
  called by kworker threads to processing remaining smb work requests
  queue_ksmbd_work() - queue a smb request to worker thread queue
 		for proccessing smb command and sending response
  @conn:	connection instance
  read remaining data from socket create and submit work.
 update activity on connection 
	
	  Inc this each time you change stats output format,
	  so user space will know what to do.
  ksmbd_server_exit() - shutdown forker thread and free memory at module exit
 SPDX-License-Identifier: GPL-2.0-or-later
    Some of the source code in this file came from fscifscifs_unicode.c
    Copyright (c) International Business Machines  Corp., 2000,2009
    Modified by Steve French (sfrench@us.ibm.com)
    Modified by Namjae Jeon (linkinjeon@kernel.org)
  smb_utf16_bytes() - how long will a string be after conversion?
  @from:	pointer to input string
  @maxbytes:	don't go past this many bytes of input string
  @codepage:	destination codepage
  Walk a utf16le string and return the number of bytes that the string will
  be after being converted to the given charset, not including any null
  termination required. Don't walk past maxbytes in the source buffer.
  Return:	string length after conversion
  cifs_mapchar() - convert a host-endian char to proper char in codepage
  @target:	where converted character should be copied
  @src_char:	2 byte host-endian source character
  @cp:		codepage to which character should be converted
  @mapchar:	should character be mapped according to mapchars mount option?
  This function handles the conversion of a single character. It is the
  responsibility of the caller to ensure that the target buffer is large
  enough to hold the result of the conversion (at least NLS_MAX_CHARSET_SIZE).
  Return:	string length after conversion
	
	  BB: Cannot handle remapping UNI_SLASH until all the calls to
	      build_path_from_dentry are modified, as they use slash as
	      separator.
  is_char_allowed() - check for valid character
  @ch:		input character to be checked
  Return:	1 if char is allowed, otherwise 0
 check for control chars, wildcards etc. 
  smb_from_utf16() - convert utf16le string to local charset
  @to:		destination buffer
  @from:	source buffer
  @tolen:	destination buffer size (in bytes)
  @fromlen:	source buffer size (in bytes)
  @codepage:	codepage to which characters should be converted
  @mapchar:	should characters be remapped according to the mapchars option?
  Convert a little-endian utf16le string (as sent by the server) to a string
  in the provided codepage. The tolen and fromlen parameters are to ensure
  that the code doesn't walk off of the end of the buffer (which is always
  a danger if the alignment of the source buffer is off). The destination
  string is always properly null terminated and fits in the destination
  buffer. Returns the length of the destination string in bytes (including
  null terminator).
  Note that some windows versions actually send multiword UTF-16 characters
  instead of straight UTF16-2. The linux nls routines however aren't able to
  deal with those characters properly. In the event that we get some of
  those characters, they won't be translated properly.
  Return:	string length after conversion
	
	  because the chars can be of varying widths, we need to take care
	  not to overflow the destination buffer when we get close to the
	  end of it. Until we get to this offset, we don't need to check
	  for overflow however.
		
		  check to see if converting this character might make the
		  conversion bleed into the null terminator
 put converted char into 'to' buffer 
 properly null-terminate string 
  smb_strtoUTF16() - Convert character string to unicode string
  @to:		destination buffer
  @from:	source buffer
  @len:	destination buffer size (in bytes)
  @codepage:	codepage to which characters should be converted
  Return:	string length after conversion
 needed to quiet sparse 
 special case for utf8 to handle no plane0 chars 
		
		  convert utf8 -> utf16, we assume we have enough space
		  as caller should have assumed conversion does not overflow
		  in destination len is length in wchar_t units (16bits)
 if success terminate and exit 
		
		  if fails fall back to UCS encoding as this
		  function should not return negative values
		  currently can fail only if source contains
		  invalid encoded characters
 A question mark 
  smb_strndup_from_utf16() - copy a string from wire format to the local
 		codepage
  @src:	source string
  @maxlen:	don't walk past this many bytes in the source string
  @is_unicode:	is this a unicode string?
  @codepage:	destination codepage
  Take a string given by the server, convert it to the local codepage and
  put it in a new buffer. Returns a pointer to the new string or NULL on
  error.
  Return:	destination string buffer or error ptr
  Convert 16 bit Unicode pathname to wire format from string in current code
  page. Conversion may involve remapping up the six characters that are
  only legal in POSIX-like OS (if they are present in the string). Path
  names are little endian 16 bit Unicode on the wire
  smbConvertToUTF16() - convert string from local charset to utf16
  @target:	destination buffer
  @source:	source buffer
  @srclen:	source buffer size (in bytes)
  @cp:		codepage to which characters should be converted
  @mapchar:	should characters be remapped according to the mapchars option?
  Convert 16 bit Unicode pathname to wire format from string in current code
  page. Conversion may involve remapping up the six characters that are
  only legal in POSIX-like OS (if they are present in the string). Path
  names are little endian 16 bit Unicode on the wire
  Return:	char length after conversion
		
		  FIXME: We can not handle remapping backslash (UNI_SLASH)
		  until all the calls to build_path_from_dentry are modified,
		  as they use backslash as separator.
			
			  if no match, use question mark, which at least in
			  some cases serves as wild card
		
		  character may take more than one byte in the source string,
		  but will take exactly two bytes in the target string
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2016 Namjae Jeon <linkinjeon@kernel.org>
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
  init_smb2_1_server() - initialize a smb server connection with smb2.1
 			command dispatcher
  @conn:	connection instance
  init_smb3_0_server() - initialize a smb server connection with smb3.0
 			command dispatcher
  @conn:	connection instance
  init_smb3_02_server() - initialize a smb server connection with smb3.02
 			command dispatcher
  @conn:	connection instance
  init_smb3_11_server() - initialize a smb server connection with smb3.11
 			command dispatcher
  @conn:	connection instance
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
		
		  Response message type value should be equal to
		  request message type + 1.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2021 Samsung Electronics Co., Ltd.
    Author(s): Namjae Jeon <linkinjeon@kernel.org>
 push permission 
 ACL ACCESS 
 DEFAULT ACL ACCESS 
 push hash type and hash 64bytes 
 push ndr for security descriptor 
 Read Level 
 Read Ref Id 
 Read Time 
 Read Posix ACL hash 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2016 Namjae Jeon <linkinjeon@kernel.org>
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
  kvec_array_init() - initialize a IO vector segment
  @new:	IO vector to be initialized
  @iov:	base IO vector
  @nr_segs:	number of segments in base iov
  @bytes:	total iovec length so far for read
  Return:	Number of IO segments
  get_conn_iovec() - get connection iovec for reading from socket
  @t:		TCP transport instance
  @nr_segs:	number of segments in iov
  Return:	return existing or newly allocate iovec
 not big enough -- allocate a new one and release the old 
  ksmbd_tcp_new_connection() - create a new tcp session on mount
  @client_sk:	socket associated with new connection
  whenever a new connection is requested, create a conn thread
  (session thread) to handle new incoming smb requests from the connection
  Return:	0 on success, otherwise error
  ksmbd_kthread_fn() - listen to new SMB connections and callback server
  @p:		arguments to forker thread
  Return:	0 on success, error number otherwise
 check for new connections every 100 msecs 
  ksmbd_tcp_run_kthread() - start forker thread
  @iface: pointer to struct interface
  start forker thread(ksmbd0) at module init time to listen
  on port 445 for new SMB connection requests. It creates per connection
  server threads(ksmbdx)
  Return:	0 on success or error number
  ksmbd_tcp_readv() - read data from socket in given iovec
  @t:		TCP transport instance
  @iov_orig:	base IO vector
  @nr_segs:	number of segments in base iov
  @to_read:	number of bytes to read from socket
  Return:	on success return number of bytes read from socket,
 		otherwise return error number
  ksmbd_tcp_read() - read data from socket in given buffer
  @t:		TCP transport instance
  @buf:	buffer to store read data from socket
  @to_read:	number of bytes to read from socket
  Return:	on success return number of bytes read from socket,
 		otherwise return error number
 set zero to timeout 
  create_socket - create socket for ksmbd0
  Return:	0 on success, error number otherwise
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2016 Namjae Jeon <namjae.jeon@protocolfreedom.org>
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
  ksmbd_conn_free() - free resources of the connection instance
  @conn:	connection instance to be cleand up
  During the thread termination, the corresponding conn instance
  resources(sockmemory) are released and finally the conn object is freed.
  ksmbd_conn_alloc() - initialize a new connection instance
  Return:	ksmbd_conn struct on success, otherwise NULL
	
	  Stop current session if the time that get last request from client
	  is bigger than deadtime user configured and opening file count is
	  zero.
  ksmbd_conn_handler_loop() - session thread to listen on new smb requests
  @p:		connection instance
  One thread each per connection
  Return:	0 on success
		
		  Check if pdu size is valid (min : smb header size,
		  max : 0x00FFFFFF).
 4 for rfc1002 length field 
		
		  We already read 4 bytes to find out PDU size, now
		  read in PDU
 Wait till all reference dropped to the Server object
 100ms 
 SPDX-License-Identifier: GPL-2.0-or-later
  Copyright (C) 2016 Namjae Jeon <linkinjeon@kernel.org>
  Copyright (C) 2019 Samsung Electronics Co., Ltd.
  INODE hash
 init master fp hash table 
	 because the reference count of fp is 0, it is guaranteed that
	  there are not accesses to fp->lock_list.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2016 Namjae Jeon <linkinjeon@kernel.org>
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
  check_session_id() - check for valid session id in smb header
  @conn:	connection instance
  @id:		session id from smb header
  Return:      1 if valid session id, otherwise 0
  smb2_get_ksmbd_tcon() - get tree connection information using a tree id.
  @work:	smb work
  Return:	0 if there is a tree connection matched or these are
 		skipable commands, otherwise error
  smb2_set_err_rsp() - set error response code on smb response
  @work:	smb work containing response buffer
  is_smb2_neg_cmd() - is it smb2 negotiation command
  @work:	smb work containing smb header
  Return:      true if smb2 negotiation command, otherwise false
 is it SMB2 header ? 
 make sure it is request not response message 
  is_smb2_rsp() - is it smb2 response
  @work:	smb work containing smb response buffer
  Return:      true if smb2 response, otherwise false
 is it SMB2 header ? 
 make sure it is response not request message 
  get_smb2_cmd_val() - get smb command code from smb header
  @work:	smb work containing smb request buffer
  Return:      smb2 request command value
  set_smb2_rsp_status() - set error response code on smb2 header
  @work:	smb work containing response buffer
  @err:	error response code
  init_smb2_neg_rsp() - initialize smb2 response for negotiate command
  @work:	smb work containing smb request buffer
  smb2 negotiate response is sent in reply of smb1 negotiate command for
  dialect auto-negotiation.
	 Not setting conn guid rsp->ServerGUID, as it
	  not used by client for identifying connection
 Default Max Message Size till SMB2.0, 64K
  smb2_set_rsp_credits() - set number of credits in response buffer
  @work:	smb work containing smb response buffer
	 according to smb2.credits smbtorture, Windows server
	  2016 or later grant up to 8192 credits at once.
	 
	  TODO: Need to adjuct CreditRequest value according to
	  current cpu load
 Update CreditRequest in last request 
  init_chained_smb2_rsp() - initialize smb2 chained response
  @work:	smb work containing smb response buffer
	 Len of this response = updated RFC len - offset of previous cmd
	  in the compound rsp
	 Storing the current local FID which may be needed by subsequent
	  command in the compound request
	
	  Message is response. We don't grant oplock yet.
  is_chained_smb2_message() - check for chained command
  @work:	smb work containing smb request buffer
  Return:      true if chained request, otherwise false
		
		  This is last request in chained command,
		  align response to 8 byte
  init_smb2_rsp_hdr() - initialize smb2 response
  @work:	smb work containing smb request buffer
  Return:      0
	
	  Message is response. We don't grant oplock yet.
  smb2_allocate_rsp_buf() - allocate smb2 response buffer
  @work:	smb work containing smb request buffer
  Return:      0 on success, otherwise -ENOMEM
 allocate large response buf for chained commands 
  smb2_check_user_session() - check for valid session for a user
  @work:	smb work containing smb request buffer
  Return:      0 on success, otherwise error
	
	  SMB2_ECHO, SMB2_NEGOTIATE, SMB2_SESSION_SETUP command do not
	  require a session id, so no need to validate user session's for
	  these commands.
 Check for validity of user session 
  smb2_get_name() - get filename string from on the wire smb format
  @share:	ksmbd_share_config pointer
  @src:	source buffer
  @maxlen:	maxlen of source string
  @nls_table:	nls_table pointer
  Return:      matching converted filename on success, otherwise error ptr
  smb2_get_dos_mode() - get file mode in dos format from unix mode
  @stat:	kstat containing file mode
  @attribute:	attribute flags
  Return:      converted dos mode
 SMB2_CREATE_TAG_POSIX is "0x93AD25509CB411E7B42383DE968BCD7C" 
 Round to 8 byte boundary 
 Round to 8 byte boundary 
 Temporarily set to SMB3_COMPRESS_NONE 
 Round to 8 byte boundary 
 Round to 8 byte boundary 
 +4 is to account for the RFC1001 len field 
 check that offset is not beyond end of SMB 
 offsets must be 8 byte aligned 
  smb2_handle_negotiate() - handler for smb2 negotiate command
  @work:	smb work containing smb request buffer
  Return:      0
 For stats 
	 Not setting conn guid rsp->ServerGUID, as it
	  not used by client for identifying server
 Check for previous session 
		
		  Reuse session if anonymous try to connect
		  on reauthetication.
		
		  If session state is SMB2_SESSION_VALID, We can assume
		  that it is reauthentication. And the userpassword
		  has been verified, so return it here.
			
			  signing is disable if encryption is enable
			  on this session
 Check previous session 
				
				  Note: here total size -1 is done as an
				  adjustment for 0 size blob
 TODO: need one more negotiation 
		
		  SecurityBufferOffset should be set to zero
		  in session setup error response.
			
			  To avoid dictionary attacks (repeated session setups rapidly sent) to
			  connect to server, ksmbd make a delay of a 5 seconds on session setup
			  failure to make it harder to send enough random connection requests
			  to break into a server.
  smb2_tree_connect() - handler for smb2 tree connect command
  @work:	smb work containing smb request buffer
  Return:      0 on success, otherwise error
 default manual caching 
  smb2_create_open_flags() - convert smb open flags to unix open flags
  @file_present:	is file already present
  @access:		file access flags
  @disposition:	file disposition flags
  @may_flags:		set with MAY_ flags
  Return:      file open flags
  smb2_tree_disconnect() - handler for smb tree connect request
  @work:	smb work containing request buffer
  Return:      0
  smb2_session_logoff() - handler for session log off request
  @work:	smb work containing request buffer
  Return:      0
 Got a valid session, set connection state 
 setting CifsExiting here may race with start_tcp_sess 
 let start_tcp_sess free connection info now 
  create_smb2_pipe() - create IPC pipe
  @work:	smb work containing request buffer
  Return:      0 on success, otherwise error
 StructureSize - 1
  smb2_set_ea() - handler for setting extended attributes using set
 		info command
  @eabuf:	set info command buffer
  @buf_len:	set info command buffer length
  @path:	dentry path for get ea
  Return:	0 on success, otherwise error
 delete the EA only when it exits 
 if the EA doesn't exist, just do nothing. 
 Check if there is stream prefix in xattr space 
 get FileAttributes from XATTR_NAME_DOS_ATTRIBUTE 
 Parse SD BUFFER create contexts 
  smb2_open() - handler for smb file open request
  @work:	smb work containing request buffer
  Return:      0 on success, otherwise error
 Parse non-durable handle create contexts 
			
			  If file exists with under flags, return access
			  denied error.
create file if not present 
		 FILE_READ_ATTRIBUTE is allowed without inode_permission,
		  because execute(search) permission on a parent directory,
		  is already granted.
 Obtain Volatile-ID 
 Get Persistent-ID 
 Set default windows and posix acls if creating new file 
	 fp should be searchable through ksmbd_inode.m_fp_list
	  after daccess, saccess, attrib_only, and stream are
	  initialized.
 Check delete pending among previous fp before oplock break 
 StructureSize - 1
 If lease is request send lease context response 
  smb2_populate_readdir_entry() - encode directory entry in smb2 response
  buffer
  @conn:	connection instance
  @info_level:	smb information level
  @d_info:	structure included variables for query dir
  @user_ns:	user namespace
  @ksmbd_kstat:	ksmbd wrapper of dirent stat information
  if directory has many entries, find first can't read it fully.
  find next might be called multiple times to read remaining dir entries
  Return:	0 on success, otherwise error
 Somehow the name has only terminating NULL bytes 
 switch (info_level) 
 switch (info_level) 
 dot and dotdot entries are already reserved 
	
	  reserve dot and dotdot entries in head of buffer
	  in first response
  buffer_check_err() - helper function to check buffer errors
  @reqOutputBufferLength:	max buffer length expected in command response
  @rsp:		query info response buffer contains output buffer length
  @infoclass_size:	query info class response buffer size
  Return:	0 on success, otherwise error
 any unique number 
	
	  Windows can sometime send query file info request on
	  pipe without opening it, checking error condition here
  smb2_get_ea() - handler for smb2 get extended attribute command
  @work:	smb work containing query info command buffer
  @fp:		ksmbd_file pointer
  @req:	get extended attribute request
  @rsp:	response buffer pointer
  @rsp_org:	base response buffer pointer in case of chained response
  Return:	0 on success, otherwise error
 single EA entry is requested with given user. name 
 need to send all EAs, if no specific EA is requested
 there is no EA in the file 
		
		  CIFS does not support EA other than user. namespace,
		  still keep the framework generic, to list other attrs
		  in future.
 bailout if xattr can't fit in buf_free_len 
 align next xattr entry at 4 byte bundary 
 no more ea entries 
 plus : size 
 last entry offset should be 0 
 smb2 info file called for pipe 
 Taking dummy value of serial number
		
		  TODO : The current implementation is based on
		  test result with win7(NTFS) server. It's need to
		  modify this to get valid Quota values
		  from Linux kernel
  smb2_query_info() - handler for smb2 query info command
  @work:	smb work containing query info request buffer
  Return:	0 on success, otherwise error
  smb2_close_pipe() - handler for closing IPC pipe
  @work:	smb work containing close request buffer
  Return:	0
  smb2_close() - handler for smb2 close file command
  @work:	smb work containing close request buffer
  Return:	0
 file already closed, return FILE_CLOSED 
 file closed, stored id is not valid anymore 
  smb2_echo() - handler for smb2 echo(ping) command
  @work:	smb work containing echo request buffer
  Return:	0
	
	  TODO : It's working fine only when store dos attributes
	  is not yes. need to implement a logic which works
	  properly with any smb.conf option
		
		  Allocation size could be smaller than original one
		  which means allocated blocks in file should be
		  deallocated. use truncate to cut out it, but inode
		  size is also updated with truncate offset.
		  inode size is retained by backup inode size.
	
	  If FILE_END_OF_FILE_INFORMATION of set_info_file is called
	  on FAT32 shared device, truncate execution time is too long
	  and network error could cause from windows client. because
	  truncate of some filesystem like FAT32 fill zero data in
	  truncated range.
	
	  TODO : need to implement consideration for
	  FILE_SYNCHRONOUS_IO_ALERT and FILE_SYNCHRONOUS_IO_NONALERT
  smb2_set_info_file() - handler for smb2 set info command
  @work:	smb work containing set info command buffer
  @fp:		ksmbd_file pointer
  @info_class:	smb2 set info class
  @share:	ksmbd_share_config pointer
  Return:	0 on success, otherwise error
  TODO: need to implement an error handling for STATUS_INFO_LENGTH_MISMATCH
  smb2_set_info() - handler for smb2 set info command handler
  @work:	smb work containing set info request buffer
  Return:	0 on success, otherwise error
  smb2_read_pipe() - handler for smb2 read from IPC pipe
  @work:	smb work containing read IPC pipe command buffer
  Return:	0 on success, otherwise error
  smb2_read() - handler for smb2 read from file
  @work:	smb work containing read command buffer
  Return:	0 on success, otherwise error
 write data to the client using rdma channel 
  smb2_write_pipe() - handler for smb2 write on IPC pipe
  @work:	smb work containing write IPC pipe command buffer
  Return:	0 on success, otherwise error
  smb2_write() - handler for smb2 write from file
  @work:	smb work containing write command buffer
  Return:	0 on success, otherwise error
		 read data from the client using rdma channel, and
		  write the data.
  smb2_flush() - handler for smb2 flush file - fsync
  @work:	smb work containing flush command buffer
  Return:	0 on success, otherwise error
  smb2_cancel() - handler for smb2 cancel command
  @work:	smb work containing cancel command buffer
  Return:	0 on success, otherwise error
 For SMB2_CANCEL command itself send no response
 Checking for wrong flag combination during lock request
 check pending lock waiters 
  smb2_lock() - handler for smb2 file lock command
  @work:	smb work containing lock command buffer
  Return:	0 on success, otherwise error
 Check conflict locks in one request 
 check locks in connection list 
 check zero byte lock range 
 verify the SRV_COPYCHUNK_COPY packet 
	
	  FILE_READ_DATA should only be included in
	  the FSCTL_COPYCHUNK case
 zero if this is last one 
			
			  set STATUS_SOME_NOT_MAPPED response
			  for unknown domain sid.
  smb2_ioctl() - handler for smb2 ioctl command
  @work:	smb work containing ioctl command buffer
  Return:	0 on success, otherwise error
 Not support DFS yet 
		
		  TODO: This is dummy implementation to pass smbtorture
		  Need to check correct response later
  smb20_oplock_break_ack() - handler for smb2.0 oplock break command
  @work:	smb work containing oplock break command buffer
  Return:	0
  smb21_lease_break_ack() - handler for smb2.1 lease break command
  @work:	smb work containing lease break command buffer
  Return:	0
 check for bad lease state 
 valid lease state changes 
  smb2_oplock_break() - dispatcher for smb2.0 and 2.1 oplocklease break
  @work:	smb work containing oplocklease break command buffer
  Return:	0
  smb2_notify() - handler for smb2 notify request
  @work:   smb work containing notify command buffer
  Return:      0
  smb2_is_sign_req() - handler for checking packet signing status
  @work:	smb work containing notify command buffer
  @command:	SMB2 command id
  Return:	true if packed is signed, false otherwise
  smb2_check_sign_req() - handler for req packet sign processing
  @work:   smb work containing notify command buffer
  Return:	1 on success, 0 otherwise
  smb2_set_sign_rsp() - handler for rsp packet sign processing
  @work:   smb work containing notify command buffer
  smb3_check_sign_req() - handler for req packet sign processing
  @work:   smb work containing notify command buffer
  Return:	1 on success, 0 otherwise
  smb3_set_sign_rsp() - handler for rsp packet sign processing
  @work:   smb work containing notify command buffer
  smb3_preauth_hash_rsp() - handler for computing preauth hash on response
  @work:   smb work containing response buffer
 fill transform header 
 SPDX-License-Identifier: GPL-2.0-or-later
  The ASB.1BER parsing code is derived from ip_nat_snmp_basic.c which was in
  turn derived from the gxsnmp package by Gregory McLean & Jochen Friedrich
  Copyright (c) 2000 RP Internet (www.rpi.net.au).
 insert tag 
 insert seq 
 insert main gss header 
 insert neg result 
 insert oid 
 insert response token - ntlmssp blob 
 insert main gss header 
 insert neg result 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2017, Microsoft Corporation.
    Copyright (C) 2018, LG Electronics.
    Author(s): Long Li <longli@microsoft.com>,
 		Hyunchul Lee <hyc.lee@gmail.com>
    This program is free software;  you can redistribute it andor modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 2 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY;  without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
    the GNU General Public License for more details.
 SMB_DIRECT negotiation timeout in seconds 
  Default maximum number of RDMA readwrite outstanding on this connection
  This value is possibly decreased during QP creation on hardware limit
 Maximum number of retries on data transfer operations 
 No need to retry on Receiver Not Ready since SMB_DIRECT manages credits 
  User configurable initial values per SMB_DIRECT transport connection
  as defined in [MS-SMBD] 3.1.1.1
  Those may change after a SMB_DIRECT negotiation
 The local peer's maximum number of credits to grant to the peer 
 The remote peer's credit request of local peer 
 The maximum single message size can be sent to remote peer 
  The maximum fragmented upper-layer payload receive size supported 
  The maximum single-message size which can be received 
	
	  Make sure reassembly_data_length is updated after list and
	  reassembly_queue_length are updated. On the dequeue side
	  reassembly_data_length is checked without a lock to determine
	  if reassembly_queue_length and list is up to date
	
	  No need to hold the reassembly queue lock all the time as we are
	  the only one reading from the front of the queue. The transport
	  may add more entries to the back of the queue at the same time
		
		  Need to make sure reassembly_data_length is read before
		  reading reassembly_queue_length and calling
		  get_first_reassembly. This call is lock free
		  as we never read at the end of the queue which are being
		  updated in SOFTIRQ as more data is received
			
			  The upper layer expects RFC1002 length at the
			  beginning of the payload. Return it to indicate
			  the total length of the packet. This minimize the
			  change to upper layer packet processing logic. This
			  will be eventually remove when an intermediate
			  transport layer is added
 move on to the next buffer? 
				
				  No need to lock if we are not at the
				  end of the queue
	 iterate and free the list of messages in reverse. the list's head
	  is invalid.
 Fill in the packet header 
 Map the packet to DMA 
 If this is a packet without payload, don't send padding 
FIXME: skip RFC1002 header..
 iov[start] is too big, break it 
 send out all remaining vecs 
	
	  As an optimization, we don't wait for individual IO to finish
	  before sending the next one.
	  Send them all and wait for pending send count to get to 0
	  that means all the IOs have been out and we are good to return
 TODO: mempool 
	 need 2 more sge. because a SMB_DIRECT header will be mapped,
	  and maybe a send buffer could be not page aligned.
	
	  allow smb_direct_max_outstanding_rw_ops of in-flight RDMA
	  readwrites. HCA guarantees at least max_send_sge of sges for
	  a RDMA readwrite work request, and if memory registration is used,
	  we need reg_mr, local_inv wrs for each readwrite.
	 When a client is running out of send credits, the credits are
	  granted by the server's sending a packet using this queue.
	  This avoids the situation that a clients cannot send packets
	  for lack of credits
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
 Avoid put_path() 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) 2018 Samsung Electronics Co., Ltd.
 SPDX-License-Identifier: GPL-2.0-only
   inode.c - part of tracefs, a pseudo file system for activating tracing
  Based on debugfs by: Greg Kroah-Hartman <greg@kroah.com>
   Copyright (C) 2014 Red Hat Inc, author: Steven Rostedt <srostedt@redhat.com>
  tracefs is the file system that is used by the tracing infrastructure.
	
	  The mkdir call can call the generic functions that create
	  the files within the tracefs system. It is up to the individual
	  mkdir routine to handle races.
	
	  The rmdir call can call the generic functions that create
	  the files within the tracefs system. It is up to the individual
	  rmdir routine to handle races.
	  This time we need to unlock not only the parent (inode) but
	  also the directory that is being deleted.
		
		  We might like to report bad mount options here;
		  but traditionally tracefs has ignored all mount options
	 If the parent is not specified, we create it in the root.
	  We need the root dentry to do this, which is in the super
	  block. A pointer to that is in the struct vfsmount that we
	  have around.
  tracefs_create_file - create a file in the tracefs filesystem
  @name: a pointer to a string containing the name of the file to create.
  @mode: the permission that the file should have.
  @parent: a pointer to the parent dentry for this file.  This should be a
           directory dentry if set.  If this parameter is NULL, then the
           file will be created in the root of the tracefs filesystem.
  @data: a pointer to something that the caller will want to get to later
         on.  The inode.i_private pointer will point to this value on
         the open() call.
  @fops: a pointer to a struct file_operations that should be used for
         this file.
  This is the basic "create a file" function for tracefs.  It allows for a
  wide range of flexibility in creating a file, or a directory (if you want
  to create a directory, the tracefs_create_dir() function is
  recommended to be used instead.)
  This function will return a pointer to a dentry if it succeeds.  This
  pointer must be passed to the tracefs_remove() function when the file is
  to be removed (no automatic cleanup happens if your module is unloaded,
  you are responsible here.)  If an error occurs, %NULL will be returned.
  If tracefs is not enabled in the kernel, the value -%ENODEV will be
  returned.
 Do not set bits for OTH 
 directory inodes start off with i_nlink == 2 (for "." entry) 
  tracefs_create_dir - create a directory in the tracefs filesystem
  @name: a pointer to a string containing the name of the directory to
         create.
  @parent: a pointer to the parent dentry for this file.  This should be a
           directory dentry if set.  If this parameter is NULL, then the
           directory will be created in the root of the tracefs filesystem.
  This function creates a directory in tracefs with the given name.
  This function will return a pointer to a dentry if it succeeds.  This
  pointer must be passed to the tracefs_remove() function when the file is
  to be removed. If an error occurs, %NULL will be returned.
  If tracing is not enabled in the kernel, the value -%ENODEV will be
  returned.
  tracefs_create_instance_dir - create the tracing instances directory
  @name: The name of the instances directory to create
  @parent: The parent directory that the instances directory will exist
  @mkdir: The function to call when a mkdir is performed.
  @rmdir: The function to call when a rmdir is performed.
  Only one instances directory is allowed.
  The instances directory is special as it allows for mkdir and rmdir to
  to be done by userspace. When a mkdir or rmdir is performed, the inode
  locks are released and the methods passed in (@mkdir and @rmdir) are
  called without locks and with the name of the directory being created
  within the instances directory.
  Returns the dentry of the instances directory.
 Only allow one instance of the instances directory. 
  tracefs_remove - recursively removes a directory
  @dentry: a pointer to a the dentry of the directory to be removed.
  This function recursively removes a directory tree in tracefs that
  was previously created with a call to another tracefs function
  (like tracefs_create_file() or variants thereof.)
  tracefs_initialized - Tells whether tracefs has been registered
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2012 Red Hat, Inc.
  Copyright (C) 2012 Jeremy Kerr <jeremy.kerr@canonical.com>
  Compare two efivarfs file names.
  An efivarfs filename is composed of two parts,
 	1. A case-sensitive variable name
 	2. A case-insensitive GUID
  So we need to perform a case-sensitive match on part 1 and a
  case-insensitive match on part 2.
 Case-sensitive compare for the variable name 
 Case-insensitive compare for the GUID 
 GUID is case-insensitive. 
 name, plus '-', plus GUID, plus NUL
 replace invalid slashes like kobject_set_name_vargs does for sysfirmwareefivars. 
 copied by the above to local storage in the dentry. 
 Remove all entries and destroy 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2012 Red Hat, Inc.
  Copyright (C) 2012 Jeremy Kerr <jeremy.kerr@canonical.com>
  Return true if 'str' is a valid efivarfs filename of the form,
 	VariableName-12345678-1234-1234-1234-1234567891bc
	
	  We need a GUID, plus at least one letter for the variable name,
	  plus the '-' separator
 GUID must be preceded by a '-' 
	
	  Validate that 's' is of the correct format, e.g.
	 
	 	12345678-1234-1234-1234-123456789abc
 length of the variable name itself: remove GUID and separator 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2012 Red Hat, Inc.
  Copyright (C) 2012 Jeremy Kerr <jeremy.kerr@canonical.com>
	
	  efivarfs represents uncommitted variables with
	  zero-length files. Reading them should return EOF.
 SPDX-License-Identifier: GPL-2.0
   linuxfsminixnamei.c
   Copyright (C) 1991, 1992  Linus Torvalds
  directories can handle most operations...
 SPDX-License-Identifier: GPL-2.0-only
   linuxfsminixinode.c
   Copyright (C) 1991, 1992  Linus Torvalds
   Copyright (C) 1996  Gertjan van Wingerde
 	Minix V2 fs support.
   Modified for 680x0 by Andreas Schwab
   Updated to filesystem version 3 by Daniel Aragones
 s_state is now out from V3 sb 
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 Mounting a rw partition read-only. 
 Mount a partition which is read-only, read-write. 
	
	  s_max_size must not exceed the block mapping limitation.  This check
	  is only needed for V1 filesystems, since V2V3 support an extra level
	  of indirect blocks which places the limit well above U32_MAX.
	
	  Allocate the buffer map to keep the superblock small.
	 Apparently minix can create filesystems that allocate more blocks for
	  the bitmaps than needed.  We simply ignore that, but verify it didn't
	  create one with not enough blocks and bail out if so.
 set up enough so that it can read an inode 
 s_state is now out from V3 sb 
  The minix V1 function to read an inode.
  The minix V2 function to read an inode.
  The global function to read an inode.
  The minix V1 function to synchronize an inode.
  The minix V2 function to synchronize an inode.
  The function that is called for file truncation.
 SPDX-License-Identifier: GPL-2.0
 Generic part 
 i_data is not going away, no lock needed 
 Allocate the next block 
 Allocation failed, free what we already allocated 
 Verify that place we are splicing to is still there and vacant 
 We are done with atomic stuff, now do the rest of housekeeping 
 had we spliced it onto indirect block? 
 Simplest case - block found, no allocation needed 
 Clean up and exit 
 the whole chain 
 Next simple case - plain lookup or failed read of indirect block 
	
	  Indirect block might be removed by truncate while we were
	  reading it. Handling of that case (forget what we've got and
	  reread) is taken out of the main path.
 Clear the ends of indirect blocks on the shared branch 
 Kill the remaining (whole) subtrees 
 SPDX-License-Identifier: GPL-2.0
   linuxfsminixdir.c
   Copyright (C) 1991, 1992 Linus Torvalds
   minix directory handling functions
   Updated to filesystem version 3 by Daniel Aragones
  Return the offset into page `page_nr' of the last valid
  byte in that page, plus one.
 	minix_find_entry()
  finds an entry in the specified directory with the wanted name. It
  returns the cache buffer in which the entry was found, and the entry
  itself (as a parameter - res_dir). It does NOT read the inode of the
  entry - you'll have to do that yourself if you want to.
	
	  We take care of directory expansion in the same loop
	  This code plays outside i_size, so it locks the page
	  to protect that region.
 We hit i_size 
  routine to check that the specified directory is empty (for rmdir)
 check for . and .. 
 Releases the page 
 SPDX-License-Identifier: GPL-2.0
 Have triple indirect 
 32 bit, host order 
 SPDX-License-Identifier: GPL-2.0
   linuxfsminixfile.c
   Copyright (C) 1991, 1992 Linus Torvalds
   minix regular file handling primitives
  We have mostly NULLs here: the current defaults are OK for
  the minix filesystem.
 SPDX-License-Identifier: GPL-2.0
   linuxfsminixbitmap.c
   Copyright (C) 1991, 1992  Linus Torvalds
  Modified for 680x0 by Hamish Macdonald
  Fixed for 680x0 by Andreas Schwab
 bitmap.c contains the code that handles the inode and block bitmaps 
  bitmap consists of blocks filled with 16bit words
  bit set == busy, bit clear == free
  endianness is a mess, but for counting zero bits it really doesn't matter...
 Clear the link count and mode of a deleted inode on disk. 
 clear on-disk copy 
 shouldn't happen 
 SPDX-License-Identifier: GPL-2.0
 Only double indirect 
 16 bit, host order 
 SPDX-License-Identifier: GPL-2.0+
  linuxfsjbd2commit.c
  Written by Stephen C. Tweedie <sct@redhat.com>, 1998
  Copyright 1998 Red Hat corp --- All Rights Reserved
  Journal commit routines for the generic filesystem journaling code;
  part of the ext2fs journaling system.
  IO end handler for temporary buffer_heads handling writes to the journal.
  When an ext4 file is truncated, it is possible that some pages are not
  successfully freed, because they are attached to a committing transaction.
  After the transaction commits, these pages are left on the LRU, with no
  ->mapping, and with attached buffers.  These pages are trivially reclaimable
  by the VM, but their apparent absence upsets the VM accounting, and it makes
  the numbers in procmeminfo look odd.
  So here, we have a buffer which has just come off the forget list.  Look to
  see if we can strip all buffers from the backing page.
  Called under lock_journal(), and possibly under journal_datalist_lock.  The
  caller provided us with a ref against the buffer, and we drop that here.
 OK, it's a truncated page 
  Done it all: now submit the commit record.  We should have
  cleaned up our previous buffers by now, so if we are in abort
  mode we can now just skip the rest of the journal write
  entirely.
  Returns 1 if the journal needs to be aborted or 0 on success
  This function along with journal_submit_commit_record
  allows to write the commit record asynchronously.
 One for getblk() 
  write the filemap data using writepage() address_space_operations.
  We don't do block allocation here even for delalloc. We don't
  use writepages() because with delayed allocation we may be doing
  block allocation in writepages().
	
	  submit the inode data buffers. We use writepage
	  instead of writepages. Because writepages can do
	  block allocation with delalloc. We need to write
	  only allocated blocks here.
 Send all the data buffers related to an inode 
  Submit all the data buffers of inode associated with the transaction to
  disk.
  We are in a committing transaction. Therefore no new inode can be added to
  our inode list. We use JI_COMMIT_RUNNING flag to protect inode we currently
  operate on from being released while we write out pages.
 submit the inode data buffers. 
  Wait for data submitted for writeout, refile inodes to proper
  transaction if needed.
 For locking, see the comment in journal_submit_data_buffers() 
 wait for the inode data buffers writeout. 
 Now refile inode to proper lists 
  jbd2_journal_commit_transaction
  The primary function for committing a transaction to the log.  This
  function is called by the journal thread to begin a complete commit.
 For transactional checksums 
 Tail of the journal 
	
	  First job: lock down the current transaction and wait for
	  all outstanding updates to complete.
 Do we need to erase the effects of a prior jbd2_journal_flush? 
		
		  We hold j_checkpoint_mutex so tail cannot change under us.
		  We don't need any special data guarantees for writing sb
		  since journal is empty and it is ok for write to be
		  flushed only with transaction commit.
		
		  TODO: by blocking fast commits here, we are increasing
		  fsync() latency slightly. Strictly speaking, we don't need
		  to block fast commits until the transaction enters T_FLUSH
		  state. So an optimization is possible where we block new fast
		  commits here and wait for existing ones to complete
		  just before we enter T_FLUSH. That way, the existing fast
		  commits and this full commit can proceed parallely.
	
	  First thing we are allowed to do is to discard any remaining
	  BJ_Reserved buffers.  Note, it is _not_ permissible to assume
	  that there are no such buffers: if a large filesystem
	  operation like a truncate needs to split itself over multiple
	  transactions, then it may try to do a jbd2_journal_restart() while
	  there are still BJ_Reserved buffers outstanding.  These must
	  be released cleanly from the current transaction.
	 
	  In this case, the filesystem must still reserve write access
	  again before modifying the buffer in the new transaction, but
	  we do not require it to remember exactly which old buffers it
	  has reserved.  This is consistent with the existing behaviour
	  that multiple jbd2_journal_get_write_access() calls to the same
	  buffer are perfectly permissible.
		
		  A jbd2_journal_get_undo_access()+jbd2_journal_release_buffer() may
		  leave undo-committed data.
	
	  Now try to drop any written-back buffers from the journal's
	  checkpoint lists.  We do this before commit because it potentially
	  frees some memory
	
	  Clear revoked flag to reflect there is no revoked buffers
	  in the next transaction which is going to be started.
	
	  Switch to a new revoke table.
	
	  Reserved credits cannot be claimed anymore, free them
	
	  Now start flushing things to disk, in the order they appear
	  on the transaction lists.  Data blocks go first.
	
	  Way to go: we have now written out all of the data for a
	  transaction!  Now comes the tricky part: we need to write out
	  metadata.  Loop over the transaction's entire buffer list:
 Find the next buffer to be journaled... 
		 If we're in abort mode, we just un-journal the buffer and
			 If that was the last one, we need to clean up
			  any descriptor buffers which may have been
			  already allocated, even if we are now
		 Make sure we have a descriptor block in which to
			 Record it so that we can wait for IO
 Where is the buffer to be written? 
		 If the block mapping failed, just abandon the buffer
		   and repeat this loop: we'll fall into the
		
		  start_this_handle() uses t_outstanding_credits to determine
		  the free space in the log.
		 Bump b_count to prevent truncate from stumbling over
                   the shadowed buffer!  @@@ This can go if we ever get
		
		  Make a temporary IO buffer with which to write it out
		  (this will requeue the metadata buffer to BJ_Shadow).
		 Record the new block's tag in the current descriptor
		 If there's no more to do, or if the descriptor is full,
			 Write an end-of-descriptor marker before
                           submitting the IOs.  "tag" still points to
				
				  Compute checksum.
			 Force a new descriptor to be generated next
	
	  Get current oldest transaction in the log before we issue flush
	  to the filesystem device. After the flush we can be sure that
	  blocks of all older transactions are checkpointed to persistent
	  storage and we will be safe to update journal start in the
	  superblock with the numbers we get here.
 Update tail only if we free significant amount of space 
	 
	  If the journal is not located on the file system device,
	  then we must flush the file system device before we issue
	  the commit record
 Done it all: now write the commit record asynchronously. 
	 Lo and behold: we have just managed to send a transaction to
           the log.  Before we can commit it, wait for the IO so far to
           complete.  Control buffers being written are on the
           transaction's t_log_list queue, and metadata buffers are on
           the io_bufs list.
	   Wait for the buffers in reverse order.  That way we are
	   less likely to be woken up until all IOs have completed, and
	   so we incur less scheduling load.
		
		  The list contains temporary buffer heads created by
		  jbd2_journal_write_metadata_buffer().
 We also have to refile the corresponding shadowed buffer 
		 The metadata is now released for reuse, but we need
                   to remember it against this transaction so that when
                   we finally commit, we can do any checkpointing
 Here we wait for the revoke record and descriptor record buffers 
 One for getblk 
 AKPM: bforget here 
	
	  Now disk caches for filesystem device are flushed so we are safe to
	  erase checkpointed transactions from the log by updating journal
	  superblock.
	 End of a transaction!  Finally, we can do checkpoint
           processing: any buffers committed as a result of this
           transaction can be removed from any checkpoint list it was on
	
	  As there are other places (journal_unmap_buffer()) adding buffers
	  to this list we have to be careful and hold the j_list_lock.
		
		  Get a reference so that bh cannot be freed before we are
		  done with it.
		
		  If there is undo-protected committed data against
		  this buffer, then we can remove it now.  If it is a
		  buffer needing such protection, the old frozen_data
		  field now points to a committed version of the
		  buffer, so rotate that field to the new committed
		  data.
		 
		  Otherwise, we can just throw away the frozen data now.
		 
		  We also know that the frozen data has already fired
		  its triggers if they exist, so we can clear that too.
		 Only re-checkpoint the buffer_head if it is marked
		  dirty.  If the buffer was added to the BJ_Forget list
		  by jbd2_journal_forget, it may no longer be dirty and
		  there's no point in keeping a checkpoint record for
		
		  A buffer which has been freed while still being journaled
		  by a previous transaction, refile the buffer to BJ_Forget of
		  the running transaction. If the just committed transaction
		  contains "add to orphan" operation, we can completely
		  invalidate the buffer now. We are rather through in that
		  since the buffer may be still accessible when blocksize <
		  pagesize and it is attached to the last partial page.
			
			  Block device buffers need to stay mapped all the
			  time, so it is enough to clear buffer_jbddirty and
			  buffer_freed bits. For the file mapping buffers (i.e.
			  journalled data) we need to unmap buffer and clear
			  more bits. We also need to be careful about the check
			  because the data page mapping can get cleared under
			  our hands. Note that if mapping == NULL, we don't
			  need to make buffer unmapped because the page is
			  already detached from the mapping and buffers cannot
			  get reused.
			
			  The buffer on BJ_Forget list and not jbddirty means
			  it has been freed by this transaction and hence it
			  could not have been reallocated until this
			  transaction has committed. BUT it could be
			  reallocated once we have written all the data to
			  disk and before we process the buffer on BJ_Forget
			  list.
 Drops bh reference 
	
	  This is a bit sleazy.  We use j_list_lock to protect transition
	  of a transaction into T_FINISHED state and calling
	  __jbd2_journal_drop_transaction(). Otherwise we could race with
	  other checkpointing code processing the transaction...
	
	  Now recheck if some buffers did not get attached to the transaction
	  while the lock was dropped...
	 Add the transaction to the checkpoint list
	  __journal_remove_checkpoint() can not destroy transaction
 Done with this transaction! 
	
	  File the transaction statistics
	
	  weight the commit time higher than the average time so we don't
	  react too strongly to vast changes in the commit time
 Check if the transaction can be dropped now that we are finished 
	
	  Calculate overall stats
 SPDX-License-Identifier: GPL-2.0+
  linuxfsjbd2journal.c
  Written by Stephen C. Tweedie <sct@redhat.com>, 1998
  Copyright 1998 Red Hat corp --- All Rights Reserved
  Generic filesystem journal-writing code; part of the ext2fs
  journaling system.
  This file manages journals: areas of disk reserved for logging
  transactional updates.  This includes the kernel journaling thread
  which is responsible for scheduling updates to the log.
  We do not actually manage the physical storage of the journal in this
  file: that is left to a per-journal policy function, which allows us
  to store the journal within a filesystem-specified area for ext2
  journaling (ext2 can use a reserved inode for storing the log).
 Checksumming functions 
  Helper function used to manage commit timeouts
  kjournald2: The main thread function used to manage a logging device
  journal.
  This kernel thread is responsible for two things:
  1) COMMIT:  Every so often we need to commit the current state of the
     filesystem to disk.  The journal thread is responsible for writing
     all of the metadata buffers to disk. If a fast commit is ongoing
     journal thread waits until it's done and then continues from
     there on.
  2) CHECKPOINT: We cannot reuse a used section of the log file until all
     of the data in that part of the log has been rewritten elsewhere on
     the disk.  Flushing these old buffers to reclaim space in the log is
     known as checkpointing, and this thread is responsible for that job.
	
	  Set up an interval timer which can be used to trigger a commit wakeup
	  after the commit interval expires
 Record that the journal thread is running 
	
	  Make sure that no allocations from this kernel thread will ever
	  recurse to the fs layer because we are responsible for the
	  transaction commit and any fs involvement might get stuck waiting for
	  the trasn. commit.
	
	  And now, wait forever for commit wakeup events.
		
		  The simpler the better. Flushing journal isn't a
		  good idea, because that depends on threads that may
		  be already stopped.
		
		  We assume on resume that commits are already there,
		  so we don't sleep
	
	  Were we woken up by a commit wakeup event?
  jbd2_journal_write_metadata_buffer: write a metadata buffer to the journal.
  Writes a metadata buffer to a given disk block.  The actual IO is not
  performed but a new buffer_head is constructed which labels the data
  to be written with the correct destination disk block.
  Any magic-number escaping which needs to be done will cause a
  copy-out here.  If the buffer happens to start with the
  JBD2_MAGIC_NUMBER, then we can't write it to the log directly: the
  magic number is only written to the log for descripter blocks.  In
  this case, we copy the data and replace the first word with 0, and we
  return a result code which indicates that this buffer needs to be
  marked as an escaped buffer in the corresponding log descriptor
  block.  The missing word can then be restored when the block is read
  during recovery.
  If the source buffer has already been modified by a new transaction
  since we took the last commit snapshot, we use the frozen copy of
  that data for IO. If we end up using the existing buffer_head's data
  for the write, then we have to make sure nobody modifies it while the
  IO is in progress. do_get_write_access() handles this.
  The function returns a pointer to the buffer_head to be used for IO.
  Return value:
   <0: Error
  >=0: Finished OK
  On success:
  Bit 0 set == escape performed on the data
  Bit 1 set == buffer copy-out performed (kfree the data after IO)
	
	  The buffer really shouldn't be locked: only the current committing
	  transaction is allowed to write it, so nobody else is allowed
	  to do any IO.
	 
	  akpm: except if we're journalling data, and write() output is
	  also part of a shared mapping, and another thread has
	  decided to launch a writepage() against this buffer.
 keep subsequent assertions sane 
	
	  If a new transaction has already done a buffer copy-out, then
	  we use that version of the data for the commit.
	
	  Fire data frozen trigger if data already wasn't frozen.  Do this
	  before checking for escaping, as the trigger may modify the magic
	  offset.  If a copy-out happens afterwards, it will have the correct
	  data in the buffer.
	
	  Check for escaping
	
	  Do we need to do a data copy?
		
		  This isn't strictly necessary, as we're using frozen
		  data for the escaping, but it keeps consistency with
		  b_frozen_data usage.
	
	  Did we need to do an escaping?  Now we've done all the
	  copying, we can finally do so.
	
	  The to-be-written buffer needs to get moved to the io queue,
	  and the original buffer whose contents we are shadowing or
	  copying is moved to the transaction's shadow queue.
  Allocation code for the journal file.  Manage the space left in the
  journal, so that we can begin checkpointing when appropriate.
  Called with j_state_lock locked for writing.
  Returns true if a transaction commit was started.
 Return if the txn has already requested to be committed 
	
	  The only transaction we can possibly wait upon is the
	  currently running transaction (if it exists).  Otherwise,
	  the target tid must be an old one.
		
		  We want a new commit: OK, mark the request and wakeup the
		  commit thread.  We do _not_ do the commit ourselves.
		 This should never happen, but if it does, preserve
		   the evidence before kjournald goes into a loop and
  Force and wait any uncommitted transactions.  We can only force the running
  transaction if we don't have an active handle, otherwise, we will deadlock.
  Returns: <0 in case of error,
            0 if nothing to commit,
            1 if transaction was successfully committed.
 Nothing to commit 
  jbd2_journal_force_commit_nested - Force and wait upon a commit if the
  calling process is not within transaction.
  @journal: journal to force
  Returns true if progress was made.
  This is used for forcing out undo-protected data which contains
  bitmaps, when the fs is running out of space.
  jbd2_journal_force_commit() - force any uncommitted transactions
  @journal: journal to force
  Caller want unconditional commit. We can only force the running transaction
  if we don't have an active handle, otherwise, we will deadlock.
  Start a commit of the current running transaction (if any).  Returns true
  if a transaction is going to be committed (or is currently already
  committing), and fills its tid in at ptid
		 There's a running transaction and we've just made sure
		
		  If commit has been started, then we have to wait for
		  completion of that transaction.
  Return 1 if a given transaction has not yet sent barrier request
  connected with a transaction commit. If 0 is returned, transaction
  may or may not have sent the barrier. Used to avoid sending barrier
  twice in common cases.
 Transaction already committed? 
	
	  Transaction is being committed and we already proceeded to
	  submitting a flush to fs partition?
  Wait for a specified commit to complete.
  The caller may not hold the journal lock.
	
	  Some callers make sure transaction is already committing and in that
	  case we cannot block on open handles anymore. So don't warn in that
	  case.
  Start a fast commit. If there's an ongoing fast or full commit wait for
  it to complete. Returns 0 if a new fast commit was started. Returns -EALREADY
  if a fast commit is not needed, either because there's an already a commit
  going on or this tid has already been committed. Returns -EINVAL if no jbd2
  commit has yet been performed.
	
	  Fast commits only allowed if at least one full commit has
	  been processed.
  Stop a fast commit. If fallback is set, this function starts commit of
  TID tid before any other fast commit can start.
 Return 1 when transaction with given tid has already committed. 
  When this function returns the transaction corresponding to tid
  will be completed.  If the transaction has currently running, start
  committing that transaction before waiting for it to complete.  If
  the transaction id is stale, it is by definition already completed,
  so just return SUCCESS.
 transaction not yet started, so request it 
  Log buffer allocation routines:
 Map one fast commit buffer for use by the file system 
  Wait on fast commit buffers that were allocated by jbd2_fc_get_buf
  for completion.
	
	  Wait in reverse order to minimize chances of us being woken up before
	  all IOs have completed
  Conversion of logical to physical block numbers for the journal
  On external journals the journal blocks are identity-mapped, so
  this is a no-op.  If needed, we can use j_blk_offset - everything is
  ready.
 +journal->j_blk_offset 
  We play buffer_head aliasing tricks to write datametadata blocks to
  the journal without copying their contents, but for journal
  descriptor blocks we do need to generate bona fide buffers.
  After the caller of jbd2_journal_get_descriptor_buffer() has finished modifying
  the buffer's contents they really should run flush_dcache_page(bh->b_page).
  But we don't bother doing that, so there will be coherency problems with
  mmaps of blockdevs which hold live JBD-controlled filesystems.
  Return tid of the oldest transaction in the journal and block in the journal
  where the transaction starts.
  If the journal is now empty, return which will be the next transaction ID
  we will write and where will that transaction start.
  The return value is 0 if journal tail cannot be pushed any further, 1 if
  it can.
  Update information in journal structure and in on disk journal superblock
  about log tail. This function does not check whether information passed in
  really pushes log tail further. It's responsibility of the caller to make
  sure provided log tail information is valid (e.g. by holding
  j_checkpoint_mutex all the time between computing log tail and calling this
  function as is the case with jbd2_cleanup_journal_tail()).
  Requires j_checkpoint_mutex
	
	  We cannot afford for write to remain in drive's caches since as
	  soon as we update j_tail, next transaction can start reusing journal
	  space and if we lose sb update during power failure we'd replay
	  old transaction with possibly newly overwritten data.
  This is a variation of __jbd2_update_log_tail which checks for validity of
  provided log tail and locks j_checkpoint_mutex. So it is safe against races
  with other threads updating log tail.
 Minimum size of descriptor tag 
	
	  Tag with 32-bit block numbers does not use last four bytes of the
	  structure
  jbd2_journal_shrink_scan()
  Scan the checkpointed buffer on the checkpoint list and release the
  journal_head.
  jbd2_journal_shrink_count()
  Count the number of checkpoint buffers on the checkpoint list.
  Management for journal control blocks: functions to create and
  destroy journal_t structures, and to initialise and read existing
 First: create and setup a journal_t object in memory.  We initialise
  very few fields yet: that has to wait until we have created the
 15ms 
 The journal is marked for error until we succeed with recovery! 
 Set up a default-sized revoke table for the new mount. 
 journal descriptor can store up to n blocks -bzzz 
 We need enough buffers to write out full descriptor block. 
 jbd2_journal_init_dev and jbd2_journal_init_inode:
  Create a journal structure assigned some fixed set of disk blocks to
  the journal.  We don't actually touch those disk blocks yet, but we
  need to set up all of the mapping information to tell the journaling
  system where the journal blocks are.
   journal_t  jbd2_journal_init_dev() - creates and initialises a journal structure
   @bdev: Block device on which to create the journal
   @fs_dev: Device which hold journalled filesystem for this journal.
   @start: Block nr Start of journal.
   @len:  Length of the journal in blocks.
   @blocksize: blocksize of journalling device
   Returns: a newly created journal_t 
   jbd2_journal_init_dev creates a journal which maps a fixed contiguous
   range of blocks on an arbitrary block device.
   journal_t  jbd2_journal_init_inode () - creates a journal which maps to a inode.
   @inode: An inode to create the journal in
  jbd2_journal_init_inode creates a journal which maps an on-disk inode as
  the journal.  The inode must exist already, must support bmap() and
  must have all data blocks preallocated.
  If the journal init or create aborts, we need to mark the journal
  superblock as being NULL to prevent the journal destroy from writing
  back a bogus superblock.
  Given a journal_t structure, initialise the various fields for
  startup of a new journaling session.  We use this both when creating
  a journal, and after recovering an old journal to reset it for
  subsequent use.
	
	  Now that journal recovery is done, turn fast commits off here. This
	  way, if fast commit was enabled before the crash but if now FS has
	  disabled it, we don't enable fast commits.
	
	  As a special case, if the on-disk copy is already marked as needing
	  no recovery (s_start == 0), then we can safely defer the superblock
	  update until the next commit by setting JBD2_FLUSHED.  This avoids
	  attempting a write to a potential-readonly device.
 Lock here to make assertions happy... 
		
		  Update log tail information. We use REQ_FUA since new
		  transaction will start reusing journal space and so we
		  must make sure information about current log tail is on
		  disk before that.
  This function expects that the caller will have locked the journal
  buffer head, and will return with it unlocked
 Buffer got discarded which means block device got invalidated 
		
		  Oh, dear.  A previous attempt to write the journal
		  superblock failed.  This could happen because the
		  USB device was yanked out.  Or it could happen to
		  be a transient write error and maybe the block will
		  be remapped.  Nothing we can do but to retry the
		  write and hope for the best.
  jbd2_journal_update_sb_log_tail() - Update log tail in journal sb on disk.
  @journal: The journal to update.
  @tail_tid: TID of the new transaction at the tail of the log
  @tail_block: The first block of the transaction at the tail of the log
  @write_op: With which operation should we write the journal sb
  Update a journal's superblock information about log tail and write it to
  disk, waiting for the IO to complete.
 Log is no longer empty 
  jbd2_mark_journal_empty() - Mark on disk journal as empty.
  @journal: The journal to update.
  @write_op: With which operation should we write the journal sb
  Update a journal's dynamic superblock fields to show that journal is empty.
  Write updated superblock to disk waiting for IO to complete.
 Is it already empty? 
		
		  When journal is clean, no need to commit fast commit flag and
		  make file system incompatible with older kernels.
 Log is no longer empty 
  __jbd2_journal_erase() - Discard or zeroout journal blocks (excluding superblock)
  @journal: The journal to erase.
  @flags: A discardzeroout request is sent for each physically contigous
 	region of the journal. Either JBD2_JOURNAL_FLUSH_DISCARD or
 	JBD2_JOURNAL_FLUSH_ZEROOUT must be set to determine which operation
 	to perform.
  Note: JBD2_JOURNAL_FLUSH_ZEROOUT attempts to use hardware offload. Zeroes
  will be explicitly written if no hardware offload is available, see
  blkdev_issue_zeroout for more details.
 logical 
 physical 
 flags must be set to either discard or zeroout 
	
	  lookup block mapping and issue discardzeroout for each
	  contiguous region
		
		  last block not contiguous with current block,
		  process last contiguous region and return to this block on
		  next loop
			
			  if this isn't the last block of journal,
			  no need to process now because next block may also
			  be part of this contiguous region
		
		  end of contiguous region or this is last block of journal,
		  take care of the region
 reset start and stop after processing a region 
  jbd2_journal_update_sb_errno() - Update error in the journal.
  @journal: The journal to update.
  Update a journal's errno.  Write updated superblock to disk waiting for IO
  to complete.
  Read the superblock for a given journal, performing initial
  validation of the format.
 Can't have checksum v2 and v3 at the same time! 
 Can't have checksum v1 and v2 on at the same time! 
 Load the checksum driver 
 Check superblock checksum 
 Precompute checksum seed for all metadata 
  Load the on-disk journal superblock and read the key fields into the
  journal_t.
  jbd2_journal_load() - Read journal from disk.
  @journal: Journal to act on.
  Given a journal_t structure which tells us which disk blocks contain
  a journal, read the journal from disk to initialise the in-memory
  structures.
	 If this is a V2 superblock, then we have to check the
	
	  Create a slab for this blocksize
	 Let the recovery code check whether it needs to recover any
	
	  clear JBD2_ABORT flag initialized in journal_init_common
	  here to update log tail information with the newest seq.
	 OK, we've finished with the dynamic journal bits:
	  reinitialise the dynamic contents of the superblock in memory
  jbd2_journal_destroy() - Release a journal_t structure.
  @journal: Journal to act on.
  Release a journal_t structure once it is no longer in use by the
  journaled object.
  Return <0 if we couldn't clean up the journal.
 Wait for the commit thread to wake up and die. 
 Force a final log commit 
 Force any old transactions to disk 
 Totally anal locking here... 
		
		  If checkpointing failed, just free the buffers to avoid
		  looping forever
	
	  OK, all checkpoint transactions have been checked, now check the
	  write out io error flag and abort the journal if some buffer failed
	  to write back to the original location, otherwise the filesystem
	  may become inconsistent.
  jbd2_journal_check_used_features() - Check if features specified are used.
  @journal: Journal to check.
  @compat: bitmask of compatible features
  @ro: bitmask of features that force read-only mount
  @incompat: bitmask of incompatible features
  Check whether the journal uses all of a given set of
  features.  Return true (non-zero) if it does.
 Load journal superblock if it is not loaded yet. 
  jbd2_journal_check_available_features() - Check feature set in journalling layer
  @journal: Journal to check.
  @compat: bitmask of compatible features
  @ro: bitmask of features that force read-only mount
  @incompat: bitmask of incompatible features
  Check whether the journaling code supports the use of
  all of a given set of features on this journal.  Return true
	 We can support any known requested features iff the
	  superblock is in version 2.  Otherwise we fail to support any
 Are we called twice? 
  jbd2_journal_set_features() - Mark a given journal feature in the superblock
  @journal: Journal to act on.
  @compat: bitmask of compatible features
  @ro: bitmask of features that force read-only mount
  @incompat: bitmask of incompatible features
  Mark a given journal feature as present on the
  superblock.  Returns true if the requested features could be set.
 If enabling v2 checksums, turn on v3 instead 
 Asking for checksumming v3 and v1?  Only give them v3. 
 Load the checksum driver if necessary 
 Precompute checksum seed for all metadata 
 If enabling v3 checksums, update superblock 
 If enabling v1 checksums, downgrade superblock 
  jbd2_journal_clear_features() - Clear a given journal feature in the
  				    superblock
  @journal: Journal to act on.
  @compat: bitmask of compatible features
  @ro: bitmask of features that force read-only mount
  @incompat: bitmask of incompatible features
  Clear a given journal feature as present on the
  superblock.
  jbd2_journal_flush() - Flush journal
  @journal: Journal to act on.
  @flags: optional operation on the journal blocks after the flush (see below)
  Flush all data for a given journal to disk and empty the journal.
  Filesystems can use this when remounting readonly to ensure that
  recovery does not need to happen on remount. Optionally, a discard or zeroout
  can be issued on the journal blocks after flushing.
  flags:
 	JBD2_JOURNAL_FLUSH_DISCARD: issues discards for the journal blocks
 	JBD2_JOURNAL_FLUSH_ZEROOUT: issues zeroouts for the journal blocks
 Force everything buffered to the log... 
 Wait for the log commit to complete... 
 ...and flush everything in the log out to disk. 
	 Finally, mark the journal as really needing no recovery.
	  This sets s_start==0 in the underlying superblock, which is
	  the magic code for a fully-recovered superblock.  Any future
	  commits of data to the journal will restore the current
  jbd2_journal_wipe() - Wipe journal contents
  @journal: Journal to act on.
  @write: flag (see below)
  Wipe out all of the contents of a journal, safely.  This will produce
  a warning if the journal contains any valid recovery information.
  Must be called between journal_init_() and jbd2_journal_load().
  If 'write' is non-zero, then we wipe out the journal on disk; otherwise
  we merely suppress recovery.
 Lock to make assertions happy... 
  jbd2_journal_abort () - Shutdown the journal immediately.
  @journal: the journal to shutdown.
  @errno:   an error number to record in the journal indicating
            the reason for the shutdown.
  Perform a complete, immediate shutdown of the ENTIRE
  journal (not of a single transaction).  This operation cannot be
  undone without closing and reopening the journal.
  The jbd2_journal_abort function is intended to support higher level error
  recovery mechanisms such as the ext2ext3 remount-readonly error
  mode.
  Journal abort has very specific semantics.  Any existing dirty,
  unjournaled buffers in the main filesystem will still be written to
  disk by bdflush, but the journaling mechanism will be suspended
  immediately and no further transaction commits will be honoured.
  Any dirty, journaled buffers will be written back to disk without
  hitting the journal.  Atomicity cannot be guaranteed on an aborted
  filesystem, but we _do_ attempt to leave as much data as possible
  behind for fsck to use for cleanup.
  Any attempt to get a new transaction handle on a journal which is in
  ABORT state will just result in an -EROFS error return.  A
  jbd2_journal_stop on an existing handle will return -EIO if we have
  entered abort state during the update.
  Recursive transactions are not disturbed by journal abort until the
  final jbd2_journal_stop, which will receive the -EIO error.
  Finally, the jbd2_journal_abort call allows the caller to supply an errno
  which will be recorded (if possible) in the journal superblock.  This
  allows a client to record failure conditions in the middle of a
  transaction without having to complete the transaction to record the
  failure to disk.  ext3_error, for example, now uses this
  functionality.
	
	  Lock the aborting procedure until everything is done, this avoid
	  races between filesystem's error handling flow (e.g. ext4_abort()),
	  ensure panic after the error info is written into journal's
	  superblock.
	
	  ESHUTDOWN always takes precedence because a file system check
	  caused by any other journal abort error is not required after
	  a shutdown triggered.
	
	  Mark the abort as occurred and start current running transaction
	  to release all journaled buffer.
	
	  Record errno to the journal super block, so that fsck and jbd2
	  layer could realise that a filesystem check is needed.
  jbd2_journal_errno() - returns the journal's error state.
  @journal: journal to examine.
  This is the errno number set with jbd2_journal_abort(), the last
  time the journal was mounted - if the journal was stopped
  without calling abort this will be 0.
  If the journal has been aborted on this mount time -EROFS will
  be returned.
  jbd2_journal_clear_err() - clears the journal's error state
  @journal: journal to act on.
  An error must be cleared or acked to take a FS out of readonly
  mode.
  jbd2_journal_ack_err() - Ack journal err.
  @journal: journal to act on.
  An error must be cleared or acked to take a FS out of readonly
  mode.
  helper functions to deal with 32 or 64bit block numbers.
  JBD memory management
  These functions are used to allocate block-sized chunks of memory
  used for making copies of buffer_head data.  Very often it will be
  page-sized chunks of data, but sometimes it will be in
  sub-page-size chunks.  (For example, 16k pages on Power systems
  with a 4k block file system.)  For blocks smaller than a page, we
  use a SLAB allocator.  There are slab caches for each block size,
  which are allocated at mount time, if necessary, and we only free
  (all of) the slab caches whenif the jbd2 module is unloaded.  For
  this reason we don't need to a mutex to protect access to
  jbd2_slab[] allocating or releasing memory; only in
  jbd2_journal_create_slab().
 Already created 
 Must be a power of 2 
	 Check alignment; SLUB has gotten this wrong in the past,
  Journal_head storage management
 offset 
 ctor 
  journal_head splicing and dicing
  A journal_head is attached to a buffer_head whenever JBD has an
  interest in the buffer.
  Whenever a buffer has an attached journal_head, its ->b_state:BH_JBD bit
  is set.  This bit is tested in core kernel code where we need to take
  JBD-specific actions.  Testing the zeroness of ->b_private is not reliable
  there.
  When a buffer has its BH_JBD bit set, its ->b_count is elevated by one.
  When a buffer has its BH_JBD bit set it is immune from being released by
  core kernel code, mainly via ->b_count.
  A journal_head is detached from its buffer_head when the journal_head's
  b_jcount reaches zero. Running transaction (b_transaction) and checkpoint
  transaction (b_cp_transaction) hold their references to b_jcount.
  Various places in the kernel want to attach a journal_head to a buffer_head
  _before_ attaching the journal_head to a transaction.  To protect the
  journal_head in this situation, jbd2_journal_add_journal_head elevates the
  journal_head's b_jcount refcount by one.  The caller must call
  jbd2_journal_put_journal_head() to undo this.
  So the typical usage would be:
 	(Attach a journal_head if needed.  Increments b_jcount)
 	struct journal_head jh = jbd2_journal_add_journal_head(bh);
 	...
       (Get another reference for transaction)
 	jbd2_journal_grab_journal_head(bh);
 	jh->b_transaction = xxx;
 	(Put original reference)
 	jbd2_journal_put_journal_head(jh);
  Give a buffer_head a journal_head.
  May sleep.
 We consumed it 
  Grab a ref against this buffer_head's journal_head.  If it ended up not
  having a journal_head, return NULL
 Unlink before dropping the lock 
 debug, really 
  Drop a reference on the passed journal_head.  If it fell to zero then
  release the journal_head from the buffer_head.
  Initialize jbd inode head
  Function to be called before we start removing inode from memory (i.e.,
  clear_inode() is a fine place to be called from). It removes inode from
  transaction's lists.
 Is commit writing out inode - we have to wait 
  Module startup and shutdown
 SPDX-License-Identifier: GPL-2.0+
  linuxfsjbd2revoke.c
  Written by Stephen C. Tweedie <sct@redhat.com>, 2000
  Copyright 2000 Red Hat corp --- All Rights Reserved
  Journal revoke routines for the generic filesystem journaling code;
  part of the ext2fs journaling system.
  Revoke is the mechanism used to prevent old log records for deleted
  metadata from being replayed on top of newer data using the same
  blocks.  The revoke mechanism is used in two separate places:
  + Commit: during commit we write the entire list of the current
    transaction's revoked blocks to the journal
  + Recovery: during recovery we record the transaction ID of all
    revoked blocks.  If there are multiple revoke records in the log
    for a single block, only the last one counts, and if there is a log
    entry for a block beyond the last revoke, then that log entry still
    gets replayed.
  We can get interactions between revokes and new log data within a
  single transaction:
  Block is revoked and then journaled:
    The desired end result is the journaling of the new block, so we
    cancel the revoke before the transaction commits.
  Block is journaled and then revoked:
    The revoke must take precedence over the write of the block, so we
    need either to cancel the journal entry or to write the revoke
    later in the log than the log block.  In this case, we choose the
    latter: journaling a block cancels any revoke record for that block
    in the current transaction, so any revoke for that block in the
    transaction must have happened after the block was journaled and so
    the revoke must take precedence.
  Block is revoked and then written as data:
    The data write is allowed to succeed, but the revoke is _not_
    cancelled.  We still need to prevent old log records from
    overwriting the new data.  We don't even need to clear the revoke
    bit here.
  We cache revoke status of a buffer in the current transaction in b_states
  bits.  As the name says, revokevalid flag indicates that the cached revoke
  status of a buffer is valid and we can rely on the cached status.
  Revoke information on buffers is a tri-state value:
  RevokeValid clear:	no cached revoke status, need to look it up
  RevokeValid set, Revoked clear:
 			buffer has not been revoked, and cancel_revoke
 			need do nothing.
  RevokeValid set, Revoked set:
 			buffer has been revoked.
  Locking rules:
  We keep two hash tables of revoke records. One hashtable belongs to the
  running transaction (is pointed to by journal->j_revoke), the other one
  belongs to the committing transaction. Accesses to the second hash table
  happen only from the kjournald and no other thread touches this table.  Also
  journal_switch_revoke_table() which switches which hashtable belongs to the
  running and which to the committing transaction is called only from
  kjournald. Therefore we need no locks when accessing the hashtable belonging
  to the committing transaction.
  All users operating on the hash table belonging to the running transaction
  have a handle to the transaction. Therefore they are safe from kjournald
  switching hash tables under them. For operations on the lists of entries in
  the hash table j_revoke_lock is used.
  Finally, also replay code uses the hash tables but at this moment no one else
  can touch them (filesystem isn't mounted yet) and hence no locking is
  needed.
 Each revoke record represents one single revoked block.  During
   journal replay, this involves recording the transaction ID of the
 Used for recovery only 
 The revoke table is just a simple hash table of revoke records. 
	 It is conceivable that we might want a larger hash table
 Utility functions to maintain the revoke table 
 Find a revoke record in the journal's hash table. 
 Initialise the revoke table for a given journal to a given size. 
 Destroy a journal's revoke table.  The table must already be empty! 
  jbd2_journal_revoke: revoke a given buffer_head from the journal.  This
  prevents the block from being replayed during recovery if we take a
  crash after this current transaction commits.  Any subsequent
  metadata writes of the buffer in this transaction cancel the
  revoke.
  Note that this call may block --- it is up to the caller to make
  sure that there are no further calls to journal_write_metadata
  before the revoke is complete.  In ext3, this implies calling the
  revoke before clearing the block bitmap when we are deleting
  metadata.
  Revoke performs a jbd2_journal_forget on any buffer_head passed in as a
  parameter, but does _not_ forget the buffer_head if the bh was only
  found implicitly.
  bh_in may not be a journalled buffer - it may have come off
  the hash tables without an attached journal_head.
  If bh_in is non-zero, jbd2_journal_revoke() will decrement its b_count
  by one.
		 If there is a different buffer_head lying around in
 ... and it has RevokeValid status... 
				 ...then it better be revoked too,
				  since it's illegal to create a revoke
				  record against a buffer_head which is
				  not marked revoked --- that would
				  risk missing a subsequent revoke
	 We really ought not ever to revoke twice in a row without
           first having the revoke cancelled: it's illegal to free a
  Cancel an outstanding revoke.  For use only internally by the
  journaling code (called from jbd2_journal_get_write_access).
  We trust buffer_revoked() on the buffer if the buffer is already
  being journaled: if there is no revoke pending on the buffer, then we
  don't do anything here.
  This would break if it were possible for a buffer to be revoked and
  discarded, and then reallocated within the same transaction.  In such
  a case we would have lost the revoked bit, but when we arrived here
  the second time we would still have a pending revoke to cancel.  So,
  do not trust the Revoked bit on buffers unless RevokeValid is also
  set.
 akpm: debug 
	 Is the existing Revoke bit valid?  If so, we trust it, and
	  only perform the full cancel if the revoke bit is set.  If
	  not, we can't trust the revoke bit, and we need to do the
 There better not be one left behind by now! 
	 Finally, have we just cleared revoke on an unhashed
	  buffer_head?  If so, we'd better make sure we clear the
	  revoked status on any hashed alias too, otherwise the revoke
  journal_clear_revoked_flag clears revoked flag of buffers in
  revoke table to reflect there is no revoked buffers in the next
  transaction which is going to be started.
 journal_switch_revoke table select j_revoke for next transaction
  we do not want to suspend any processing until all revokes are
  written -bzzz
  Write revoke records to the journal for all entries in the current
  revoke hash, deleting the entries as we go.
 select revoke table for committing transaction 
  Write out one revoke record.  We need to create a new descriptor
  block if the old one is full or if we have not already created one.
	 If we are already aborting, this all becomes a noop.  We
           still need to go round the loop in
           jbd2_journal_write_revoke_records in order to free all of the
 Do we need to leave space at the end for a checksum? 
 Make sure we have a descriptor with space left for the record 
 Record it so that we can wait for IO completion later 
  Flush a revoke descriptor out to the journal.  If we are aborting,
  this is a noop; otherwise we are generating a buffer which needs to
  be waited for during commit, so it has to go onto the appropriate
  journal buffer list.
  Revoke support for recovery.
  Recovery needs to be able to:
   record all revoke records, including the tid of the latest instance
   of each revoke in the journal
   check whether a given block in a given transaction should be replayed
   (ie. has not been revoked by a revoke record in that or a subsequent
   transaction)
   empty the revoke table after recovery.
  First, setting revoke records.  We create a new revoke record for
  every block ever revoked in the log as we scan it for recovery, and
  we update the existing records if we find multiple revokes for a
  single block.
		 If we have multiple occurrences, only record the
  Test revoke records.  For a given block referenced in the log, has
  that block been revoked?  A revoke record with a given transaction
  sequence number revokes all blocks in that transaction and earlier
  ones, but later transactions still need replayed.
  Finally, once recovery is over, we need to clear the revoke table so
  that it can be reused by the running filesystem.
 SPDX-License-Identifier: GPL-2.0+
  linuxfsjbd2checkpoint.c
  Written by Stephen C. Tweedie <sct@redhat.com>, 1999
  Copyright 1999 Red Hat Software --- All Rights Reserved
  Checkpoint routines for the generic filesystem journaling code.
  Part of the ext2fs journaling system.
  Checkpointing is the process of ensuring that a section of the log is
  committed fully to disk, so that that portion of the log can be
  reused.
  Unlink a buffer from a transaction checkpoint list.
  Called with j_list_lock held.
  Unlink a buffer from a transaction checkpoint(io) list.
  Called with j_list_lock held.
  Move a buffer from the checkpoint list to the checkpoint io list
  Called with j_list_lock held
  Check a checkpoint buffer could be release or not.
  Requires j_list_lock
  __jbd2_log_wait_for_space: wait until there is space in the journal.
  Called under j-state_lock only.  It will be unlocked if we have to wait
  for a checkpoint to free up some space in the log.
 assert_spin_locked(&journal->j_state_lock); 
		
		  Test again, another process may have checkpointed while we
		  were waiting for the checkpoint lock. If there are no
		  transactions ready to be checkpointed, try to recover
		  journal space by calling cleanup_journal_tail(), and if
		  that doesn't work, by waiting for the currently committing
		  transaction to complete.  If there is absolutely no way
		  to make progress, this is either a BUG or corrupted
		  filesystem, so abort the journal and leave a stack
		  trace for forensic evidence.
 We were able to recover space; yay! 
				
				  jbd2_journal_commit_transaction() may want
				  to take the checkpoint_mutex if JBD2_FLUSHED
				  is set.  So we need to temporarily drop it.
  Perform an actual checkpoint. We take the first transaction on the
  list of transactions to be checkpointed and send all its buffers
  to disk. We submit larger chunks of data at once.
  The journal should be locked before calling this function.
  Called with j_checkpoint_mutex held.
	
	  First thing: if there are any transactions in the log which
	  don't need checkpointing, just eliminate them from the
	  journal straight away.
	
	  OK, we need to start writing disk blocks.  Take one transaction
	  and write it.
	
	  If someone cleaned up this transaction while we slept, we're
	  done (maybe it's a new transaction, but it fell at the same
	  address).
 checkpoint all of the transaction's buffers 
 the journal_head may have gone by now 
				
				  The journal thread is dead; so
				  starting and waiting for a commit
				  to finish will cause us to wait for
				  a _very_ long time.
			
			  jbd2_journal_commit_transaction() may want
			  to take the checkpoint_mutex if JBD2_FLUSHED
			  is set, jbd2_update_log_tail() called by
			  jbd2_journal_commit_transaction() may also take
			  checkpoint_mutex.  So we need to temporarily
			  drop it.
 The transaction was released; we're done 
		
		  Important: we are about to write the buffer, and
		  possibly block, while still holding the journal
		  lock.  We cannot afford to let the transaction
		  logic start messing around with this buffer before
		  we write it to disk, as that would break
		  recoverability.
	
	  Now we issued all of the transaction's buffers, let's deal
	  with the buffers that are out for IO.
 Did somebody clean up the transaction in the meanwhile? 
 the journal_head may have gone by now 
		
		  Now in whatever state the buffer currently is, we
		  know that it has been written out and so we can
		  drop it from the list
  Check the list of checkpoint transactions for the journal to see if
  we have already got rid of any since the last update of the log tail
  in the journal superblock.  If so, we can instantly roll the
  superblock forward to remove those transactions from the log.
  Return <0 on error, 0 on success, 1 if there was nothing to clean up.
  Called with the journal lock held.
  This is the only part of the journaling code which really needs to be
  aware of transaction aborts.  Checkpointing involves writing to the
  main filesystem area rather than to the journal, so it can proceed
  even in abort state, but we must not update the super block if
  checkpointing may have failed.  Otherwise, we would lose some metadata
  buffers which should be written-back to the filesystem.
	
	  We need to make sure that any blocks that were recently written out
	  --- perhaps by jbd2_log_do_checkpoint() --- are flushed out before
	  we drop the transactions from the journal. It's unlikely this will
	  be necessary, especially with an appropriately sized journal, but we
	  need this to guarantee correctness.  Fortunately
	  jbd2_cleanup_journal_tail() doesn't get called all that often.
 Checkpoint list management 
  journal_clean_one_cp_list
  Find all the written-back checkpoint buffers in the given list and
  release them. If 'destroy' is set, clean all buffers unconditionally.
  Called with j_list_lock held.
  Returns 1 if we freed the transaction, 0 otherwise.
		
		  This function only frees up some memory
		  if possible so we dont have an obligation
		  to finish processing. Bail out if preemption
		  requested:
  journal_shrink_one_cp_list
  Find 'nr_to_scan' written-back checkpoint buffers in the given list
  and try to release them. If the whole transaction is released, set
  the 'released' parameter. Return the number of released checkpointed
  buffers.
  Called with j_list_lock held.
  jbd2_journal_shrink_checkpoint_list
  Find 'nr_to_scan' written-back checkpoint buffers in the journal
  and try to release them. Return the number of released checkpointed
  buffers.
  Called with j_list_lock held.
	
	  Get next shrink transaction, resume previous scan or start
	  over again. If some others do checkpoint and drop transaction
	  from the checkpoint list, we ignore saved j_shrink_transaction
	  and start over unconditionally.
  journal_clean_checkpoint_list
  Find all the written-back checkpoint buffers in the journal and release them.
  If 'destroy' is set, release all buffers unconditionally.
  Called with j_list_lock held.
		
		  This function only frees up some memory if possible so we
		  dont have an obligation to finish processing. Bail out if
		  preemption requested:
		
		  It is essential that we are as careful as in the case of
		  t_checkpoint_list with removing the buffer from the list as
		  we can possibly see not yet submitted buffers on io_list
		
		  Stop scanning if we couldn't free the transaction. This
		  avoids pointless scanning of transactions which still
		  weren't checkpointed.
  Remove buffers from all checkpoint lists as journal is aborted and we just
  need to free memory
	
	  We loop because __jbd2_journal_clean_checkpoint_list() may abort
	  early due to a need of rescheduling.
  journal_remove_checkpoint: called after a buffer has been committed
  to disk (either by being write-back flushed to disk, or being
  committed to the log).
  We cannot safely clean a transaction out of the log until all of the
  buffer updates committed in that transaction have safely been stored
  elsewhere on disk.  To achieve this, all of the buffers in a
  transaction need to be maintained on the transaction's checkpoint
  lists until they have been rewritten, at which point this function is
  called to remove the buffer from the existing transaction's
  checkpoint lists.
  The function returns 1 if it frees the transaction, 0 otherwise.
  The function can free jh and bh.
  This function is called with j_list_lock held.
	
	  If we have failed to write the buffer out to disk, the filesystem
	  may become inconsistent. We cannot abort the journal here since
	  we hold j_list_lock and we have to be careful about races with
	  jbd2_journal_destroy(). So mark the writeback IO error in the
	  journal here and we abort the journal later from a better context.
 Is this transaction empty? 
	
	  There is one special case to worry about: if we have just pulled the
	  buffer off a running or committing transaction's checkpoing list,
	  then even if the checkpoint list is empty, the transaction obviously
	  cannot be dropped!
	 
	  The locking here around t_state is a bit sleazy.
	  See the comment at the end of jbd2_journal_commit_transaction().
	
	  OK, that was the last buffer for the transaction, we can now
	  safely remove this transaction from the log.
  journal_insert_checkpoint: put a committed buffer onto a checkpoint
  list so that we know when it is safe to clean the transaction out of
  the log.
  Called with the journal locked.
  Called with j_list_lock held.
 Get reference for checkpointing transaction 
  We've finished with this transaction structure: adios...
  The transaction must have no links except for the checkpoint by this
  point.
  Called with the journal locked.
  Called with j_list_lock held.
 SPDX-License-Identifier: GPL-2.0+
  linuxfsjbd2transaction.c
  Written by Stephen C. Tweedie <sct@redhat.com>, 1998
  Copyright 1998 Red Hat corp --- All Rights Reserved
  Generic filesystem transaction handling code; part of the ext2fs
  journaling system.
  This file manages transactions (compound commits managed by the
  journaling code) and handles (individual atomic operations by the
  filesystem).
  Base amount of descriptor blocks we reserve for each transaction.
 Subtract UUID 
 Commit code leaves a slack space of 16 bytes at the end of block 
	
	  Revoke descriptors are accounted separately so we need to reserve
	  space for commit block and normal transaction descriptor blocks.
  jbd2_get_transaction: obtain a new transaction_t object.
  Simply initialise a new transaction. Initialize it in
  RUNNING state and add it to the current journal (which should not
  have an existing running transaction: we only make a new transaction
  once we have started to commit the old one).
  Preconditions:
 	The journal MUST be locked.  We don't perform atomic mallocs on the
 	new transaction	and we can't block without protecting against other
 	processes trying to touch the journal while it is in transition.
 Set up the commit timer for the new transaction. 
  Handle management.
  A handle_t is an object which represents a single atomic update to a
  filesystem, and which tracks all of the modifications which form part
  of that one update.
  Update transaction's maximum wait time, if debugging is enabled.
  In order for t_max_wait to be reliable, it must be protected by a
  lock.  But doing so will mean that start_this_handle() can not be
  run in parallel on SMP systems, which limits our scalability.  So
  unless debugging is enabled, we no longer update t_max_wait, which
  means that maximum wait time reported by the jbd2_run_stats
  tracepoint will always be zero.
  Wait until running transaction passes to T_FLUSH state and new transaction
  can thus be started. Also starts the commit if needed. The function expects
  running transaction to exist and releases j_state_lock.
  Wait until running transaction transitions from T_SWITCH to T_FLUSH
  state and new transaction can thus be started. The function releases
  j_state_lock.
	
	  We don't call jbd2_might_wait_for_commit() here as there's no
	  waiting for outstanding handles happening anymore in T_SWITCH state
	  and handling of reserved handles actually relies on that for
	  correctness.
  Wait until we can add credits for handle to the running transaction.  Called
  with j_state_lock held for reading. Returns 0 if handle joined the running
  transaction. Returns 1 if we had to wait, j_state_lock is dropped, and
  caller must retry.
  Note: because j_state_lock may be dropped depending on the return
  value, we need to fake out sparse so ti doesn't complain about a
  locking imbalance.  Callers of add_transaction_credits will need to
  make a similar accomodation.
	
	  If the current transaction is locked down for commit, wait
	  for the lock to be released.
 fake out sparse 
	
	  If there is not enough space left in the log to write all
	  potential buffers requested by this operation, we need to
	  stall pending a log checkpoint to free some more log space.
		
		  If the current transaction is already too large,
		  then start to commit it: we can then go back and
		  attach this handle to a new transaction.
		
		  Is the number of reserved credits in the current transaction too
		  big to fit this handle? Wait until reserved credits are freed.
 fake out sparse 
 fake out sparse 
	
	  The commit code assumes that it can get enough log space
	  without forcing a checkpoint.  This is critical for
	  correctness: a checkpoint of a buffer which is also
	  associated with a committing transaction creates a deadlock,
	  so commit simply cannot force through checkpoints.
	 
	  We must therefore ensure the necessary space in the journal
	  before starting to dirty potentially checkpointed buffers
	  in the new transaction.
 fake out sparse 
 No reservation? We are done... 
 We allow at most half of a transaction to be reserved 
 fake out sparse 
  start_this_handle: Given a handle, deal with any locking or stalling
  needed to make sure that there is enough journal space for the handle
  to begin.  Attach the handle to a transaction and set up the
  transaction's buffer credits.
	
	  Limit the number of reserved credits to 12 of maximum transaction
	  size and limit the number of total credits to not exceed maximum
	  transaction size per operation.
	
	  This check is racy but it is just an optimization of allocating new
	  transaction early if there are high chances we'll need it. If we
	  guess wrong, we'll retry or free unused transaction.
		
		  If __GFP_FS is not present, then we may be being called from
		  inside the fs writeback layer, so we MUST NOT fail.
	
	  We need to hold j_state_lock until t_updates has been incremented,
	  for proper journal barrier handling
	
	  Wait on the journal's transaction barrier if necessary. Specifically
	  we allow reserved handles to proceed because otherwise commit could
	  deadlock on page writeback not being able to complete.
 We may have dropped j_state_lock - restart in that case 
			
			  add_transaction_credits releases
			  j_state_lock on a non-zero return
		
		  We have handle reserved so we are allowed to join T_LOCKED
		  transaction and we don't have to check for transaction size
		  and journal space. But we still have to wait while running
		  transaction is being switched to a committing one as it
		  won't wait for any handles anymore.
	 OK, account for the buffers that this operation expects to
	  use and add the handle to the running transaction. 
	
	  Ensure that no allocations done while the transaction is open are
	  going to recurse back to the fs layer.
 Allocate a new handle.  This should probably be in a slab... 
  jbd2_journal_start() - Obtain a new handle.
  @journal: Journal to start transaction on.
  @nblocks: number of block buffer we might modify
  We make sure that the transaction can guarantee at least nblocks of
  modified buffers in the log.  We block until the log can guarantee
  that much space. Additionally, if rsv_blocks > 0, we also create another
  handle with rsv_blocks reserved blocks in the journal. This handle is
  stored in h_rsv_handle. It is not attached to any particular transaction
  and thus doesn't block transaction commit. If the caller uses this reserved
  handle, it has to set h_rsv_handle to NULL as otherwise jbd2_journal_stop()
  on the parent handle will dispose the reserved one. Reserved handle has to
  be converted to a normal handle using jbd2_journal_start_reserved() before
  it can be used.
  Return a pointer to a newly allocated handle, or an ERR_PTR() value
  on failure.
 Get j_state_lock to pin running transaction if it exists 
  jbd2_journal_start_reserved() - start reserved handle
  @handle: handle to start
  @type: for handle statistics
  @line_no: for handle statistics
  Start handle that has been previously reserved with jbd2_journal_reserve().
  This attaches @handle to the running transaction (or creates one if there's
  not transaction running). Unlike jbd2_journal_start() this function cannot
  block on journal commit, checkpointing, or similar stuff. It can block on
  memory allocation or frozen journal though.
  Return 0 on success, non-zero on error - handle is freed in that case.
 Someone passed in normal handle? Just stop it. 
	
	  Usefulness of mixing of reserved and unreserved handles is
	  questionable. So far nobody seems to need it so just error out.
	
	  GFP_NOFS is here because callers are likely from writeback or
	  similarly constrained call sites
  jbd2_journal_extend() - extend buffer credits.
  @handle:  handle to 'extend'
  @nblocks: nr blocks to try to extend by.
  @revoke_records: number of revoke records to try to extend by.
  Some transactions, such as large extends and truncates, can be done
  atomically all at once or in several stages.  The operation requests
  a credit for a number of buffer modifications in advance, but can
  extend its credit if it needs more.
  jbd2_journal_extend tries to give the running handle more buffer credits.
  It does not guarantee that allocation - this is a best-effort only.
  The calling process MUST be able to deal cleanly with a failure to
  extend here.
  Return 0 on success, non-zero on failure.
  return code < 0 implies an error
  return code > 0 implies normal transaction-full status.
 Don't extend a locked-down transaction! 
	
	  Subtract necessary revoke descriptor blocks from handle credits. We
	  take care to account only for revoke descriptor blocks the
	  transaction will really need as large sequences of transactions with
	  small numbers of revokes are relatively common.
	
	  Scope of the GFP_NOFS context is over here and so we can restore the
	  original alloc context.
  jbd2__journal_restart() - restart a handle .
  @handle:  handle to restart
  @nblocks: nr credits requested
  @revoke_records: number of revoke record credits requested
  @gfp_mask: memory allocation flags (for start_this_handle)
  Restart a handle for a multi-transaction filesystem
  operation.
  If the jbd2_journal_extend() call above fails to grant new buffer credits
  to a running handle, a call to jbd2_journal_restart will commit the
  handle's transaction so far and reattach the handle to a new
  transaction capable of guaranteeing the requested number of
  credits. We preserve reserved handle if there's any attached to the
  passed in handle.
	 If we've had an abort of any type, don't even think about
	
	  First unlink the handle from its current transaction, and start the
	  commit on that.
	
	  TODO: If we use READ_ONCE  WRITE_ONCE for j_commit_request we can
 	  get rid of pointless j_state_lock traffic like this.
  jbd2_journal_lock_updates () - establish a transaction barrier.
  @journal:  Journal to establish a barrier on.
  This locks out any further updates from being started, and blocks
  until all existing updates have completed, returning only once the
  journal is in a quiescent state with no updates running.
  The journal lock should not be held on entry.
 Wait until there are no reserved handles 
 Wait until there are no running updates 
	
	  We have now established a barrier against other normal updates, but
	  we also need to barrier against other jbd2_journal_lock_updates() calls
	  to make sure that we serialise special journal-locked operations
	  too.
  jbd2_journal_unlock_updates () - release barrier
  @journal:  Journal to release the barrier on.
  Release a transaction barrier obtained with jbd2_journal_lock_updates().
  Should be called without the journal lock held.
 Call t_frozen trigger and copy buffer data into jh->b_frozen_data. 
 Fire data frozen trigger just before we copy the data 
	
	  Now that the frozen data is saved off, we need to store any matching
	  triggers.
  If the buffer is already part of the current transaction, then there
  is nothing we need to do.  If it is already part of a prior
  transaction which we are still committing to disk, then we need to
  make sure that we do not overwrite the old copy: we do copy-out to
  preserve the copy going to disk.  We also account the buffer against
  the handle's metadata buffer credits (unless the buffer is already
  part of the transaction, that is).
 @@@ Need to check for errors here at some point. 
 If it takes too long to lock the buffer, trace it 
	 We now hold the buffer lock so it is safe to query the buffer
	  state.  Is the buffer dirty?
	 
	  If so, there are two possibilities.  The buffer may be
	  non-journaled, and undergoing a quite legitimate writeback.
	  Otherwise, it is journaled, and we don't expect dirty buffers
	  in that state (the buffers should be marked JBD_Dirty
	  instead.)  So either the IO is being done under our own
	  control and this is a bug, or it's a third party IO such as
	  dump(8) (which may leave the buffer scheduled for read ---
	  ie. locked but not dirty) or tune2fs (which may actually have
		
		  First question: is this buffer already part of the current
		  transaction or the existing committing transaction?
		
		  In any case we need to clean the dirty flag and we must
		  do it under the buffer lock to be sure we don't race
		  with running write-out.
	
	  The buffer is already part of this transaction if b_transaction or
	  b_next_transaction points to it
	
	  this is the first time this transaction is touching this buffer,
	  reset the modified flag
	
	  If the buffer is not journaled right now, we need to make sure it
	  doesn't get written to disk before the caller actually commits the
	  new data
		
		  Make sure all stores to jh (b_modified, b_frozen_data) are
		  visible before attaching it to the running transaction.
		  Paired with barrier in jbd2_write_access_granted()
	
	  If there is already a copy-out version of this buffer, then we don't
	  need to make another one
	
	  There is one case we have to be very careful about.  If the
	  committing transaction is currently writing this buffer out to disk
	  and has NOT made a copy-out, then we cannot modify the buffer
	  contents at all right now.  The essence of copy-out is that it is
	  the extra copy, not the primary copy, which gets journaled.  If the
	  primary copy is already going to disk then we cannot do copy-out
	  here.
	
	  Only do the copy if the currently-owning transaction still needs it.
	  If buffer isn't on BJ_Metadata list, the committing transaction is
	  past that stage (here we use the fact that BH_Shadow is set under
	  bh_state lock together with refiling to BJ_Shadow list and at this
	  point we know the buffer doesn't have BH_Shadow set).
	 
	  Subtle point, though: if this is a get_undo_access, then we will be
	  relying on the frozen_data to contain the new value of the
	  committed_data record after the transaction, so we HAVE to force the
	  frozen_data copy in that case.
	
	  Make sure all stores to jh (b_modified, b_frozen_data) are visible
	  before attaching it to the running transaction. Paired with barrier
	  in jbd2_write_access_granted()
	
	  If we are about to journal a buffer, then any revoke pending on it is
	  no longer valid
 It's usually NULL 
 Fast check whether buffer is already attached to the required transaction 
 Dirty buffers require special handling... 
	
	  RCU protects us from dereferencing freed pages. So the checks we do
	  are guaranteed not to oops. However the jh slab object can get freed
	  & reallocated while we work with it. So we have to be careful. When
	  we see jh attached to the running transaction, we know it must stay
	  so until the transaction is committed. Thus jh won't be freed and
	  will be attached to the same bh while we run.  However it can
	  happen jh gets freed, reallocated, and attached to the transaction
	  just after we get pointer to it from bh. So we have to be careful
	  and recheck jh still belongs to our bh before we return success.
 This should be bh2jh() but that doesn't work with inline functions 
 For undo access buffer must have data copied 
	
	  There are two reasons for the barrier here:
	  1) Make sure to fetch b_bh after we did previous checks so that we
	  detect when jh went through free, realloc, attach to transaction
	  while we were checking. Paired with implicit barrier in that path.
	  2) So that access to bh done after jbd2_write_access_granted()
	  doesn't get reordered and see inconsistent state of concurrent
	  do_get_write_access().
  jbd2_journal_get_write_access() - notify intent to modify a buffer
 				     for metadata (not data) update.
  @handle: transaction to add buffer modifications to
  @bh:     bh to be used for metadata writes
  Returns: error code or 0 on success.
  In full data journalling mode the buffer may be of type BJ_AsyncData,
  because we're ``write()ing`` a buffer which is also part of a shared mapping.
	 We do not want to get caught playing with fields which the
	  log thread also manipulates.  Make sure that the buffer
  When the user wants to journal a newly created buffer_head
  (ie. getblk() returned a new buffer and we are going to populate it
  manually rather than reading off disk), then we need to keep the
  buffer_head locked until it has been completely filled with new
  data.  In this case, we should be able to make the assertion that
  the bh is not already part of an existing transaction.
  The buffer should already be locked by the caller by this point.
  There is no lock ranking violation: it was a newly created,
  jbd2_journal_get_create_access () - notify intent to use newly created bh
  @handle: transaction to new buffer to
  @bh: new buffer.
  Call this if you create a new bh.
	
	  The buffer may already belong to this transaction due to pre-zeroing
	  in the filesystem's new_block code.  It may also be on the previous,
	  committing transaction's lists, but it HAS to be in Forget state in
	  that case: the transaction must have deleted the buffer for it to be
	  reused here.
		
		  Previous jbd2_journal_forget() could have left the buffer
		  with jbddirty bit set because it was being committed. When
		  the commit finished, we've filed the buffer for
		  checkpointing and marked it dirty. Now we are reallocating
		  the buffer so the transaction freeing it must have
		  committed and so it's safe to clear the dirty bit.
 first access by this transaction 
 first access by this transaction 
	
	  akpm: I added this.  ext3_alloc_branch can pick up new indirect
	  blocks which contain freed but then revoked metadata.  We need
	  to cancel the revoke in case we end up freeing it yet again
	  and the reallocating as data - this would cause a second revoke,
	  which hits an assertion error.
  jbd2_journal_get_undo_access() -  Notify intent to modify metadata with
      non-rewindable consequences
  @handle: transaction
  @bh: buffer to undo
  Sometimes there is a need to distinguish between metadata which has
  been committed to disk and that which has not.  The ext3fs code uses
  this for freeing and allocating space, we have to make sure that we
  do not reuse freed space until the deallocation has been committed,
  since if we overwrote that space we would make the delete
  un-rewindable in case of a crash.
  To deal with that, jbd2_journal_get_undo_access requests write access to a
  buffer for parts of non-rewindable operations such as delete
  operations on the bitmaps.  The journaling code must keep a copy of
  the buffer's contents prior to the undo_access call until such time
  as we know that the buffer has definitely been committed to disk.
  We never need to know which transaction the committed data is part
  of, buffers touched here are guaranteed to be dirtied later and so
  will be committed to a new transaction in due course, at which point
  we can discard the old committed data pointer.
  Returns error number or 0 on success.
	
	  Do this first --- it can drop the journal lock, so we want to
	  make sure that obtaining the committed_data is done
	  atomically wrt. completion of any outstanding commits.
		 Copy out the current buffer contents into the
  jbd2_journal_set_triggers() - Add triggers for commit writeout
  @bh: buffer to trigger on
  @type: struct jbd2_buffer_trigger_type containing the trigger(s).
  Set any triggers on this journal_head.  This is always safe, because
  triggers for a committing buffer will be saved off, and triggers for
  a running transaction will match the buffer in that transaction.
  Call with NULL to clear the triggers.
  jbd2_journal_dirty_metadata() -  mark a buffer as containing dirty metadata
  @handle: transaction to add buffer to.
  @bh: buffer to mark
  mark dirty metadata which needs to be journaled as part of the current
  transaction.
  The buffer must have previously had jbd2_journal_get_write_access()
  called so that it has a valid journal_head attached to the buffer
  head.
  The buffer is placed on the transaction's metadata list and is marked
  as belonging to the transaction.
  Returns error number or 0 on success.
  Special care needs to be taken if the buffer already belongs to the
  current committing transaction (in which case we should have frozen
  data present for that commit).  In that case, we don't relink the
  buffer: that only gets done when the old transaction finally
  completes its commit.
	
	  We don't grab jh reference here since the buffer must be part
	  of the running transaction.
	
	  This and the following assertions are unreliable since we may see jh
	  in inconsistent state unless we grab bh_state lock. But this is
	  crucial to catch bugs so let's do a reliable check until the
	  lockless handling is fully proven.
 If it's in our transaction it must be in BJ_Metadata list. 
		
		  This buffer's got modified and becoming part
		  of the transaction. This needs to be done
		  once a transaction -bzzz
	
	  fastpath, to avoid expensive locking.  If this buffer is already
	  on the running transaction's metadata list there is nothing to do.
	  Nobody can take it off again because there is a handle open.
	  I _think_ we're OK here with SMP barriers - a mistaken decision will
	  result in this test being false, so we go in and take the locks.
	
	  Metadata already on the current transaction list doesn't
	  need to be filed.  Metadata on another transaction's list must
	  be committing, and will be refiled once the commit completes:
	  leave it alone for now.
		 And this case is illegal: we can't reuse another
 That test should have eliminated the following case: 
  jbd2_journal_forget() - bforget() for potentially-journaled buffers.
  @handle: transaction handle
  @bh:     bh to 'forget'
  We can only do the bforget if there are no commits pending against the
  buffer.  If the buffer is dirty in the current running transaction we
  can safely unlink it.
  bh may not be a journalled buffer at all - it may be a non-JBD
  buffer which came off the hashtable.  Check for this.
  Decrements bh->b_count by one.
  Allow this call even if the handle has aborted --- it may be part of
  the caller's cleanup after an abort.
	 Critical error: attempting to delete a bitmap buffer, maybe?
 keep track of whether or not this transaction modified us 
	
	  The buffer's going from the transaction, we must drop
	  all references -bzzz
		 If we are forgetting a buffer which is already part
		  of this transaction, then we can just drop it from
		
		  we only want to drop a reference if this transaction
		  modified the buffer
		
		  We are no longer going to journal this buffer.
		  However, the commit of this transaction is still
		  important to the buffer: the delete that we are now
		  processing might obsolete an old log entry, so by
		  committing, we can satisfy the buffer's checkpoint.
		 
		  So, if we have a checkpoint on the buffer, we should
		  now refile the buffer on our BJ_Forget list so that
		  we know to remove the checkpoint after we commit.
		 However, if the buffer is still owned by a prior
		 ... but we CAN drop it from the new transaction through
		  marking the buffer as freed and set j_next_transaction to
		  the new transaction, so that not only the commit code
		  knows it should clear dirty bits when it is done with the
		  buffer, but also the buffer can be checkpointed only
			
			  only drop a reference if this transaction modified
			  the buffer
		
		  Finally, if the buffer is not belongs to any
		  transaction, we can just drop it now if it has no
		  checkpoint.
		
		  Otherwise, if the buffer has been written to disk,
		  it is safe to remove the checkpoint and drop it.
		
		  The buffer is still not written to disk, we should
		  attach this buffer to current transaction so that the
		  buffer can be checkpointed only after the current
		  transaction commits.
 no need to reserve log space for this block -bzzz 
  jbd2_journal_stop() - complete a transaction
  @handle: transaction to complete.
  All done for a particular handle.
  There is not much action needed here.  We just return any remaining
  buffer credits to the transaction and remove the handle.  The only
  complication is that we need to start a commit operation if the
  filesystem is marked for synchronous update.
  jbd2_journal_stop itself will not usually return an error, but it may
  do so in unusual circumstances.  In particular, expect it to
  return -EIO if a jbd2_journal_abort has been executed since the
  transaction began.
		
		  Handle is already detached from the transaction so there is
		  nothing to do other than free the handle.
	
	  Implement synchronous transaction batching.  If the handle
	  was synchronous, don't force a commit immediately.  Let's
	  yield and let another thread piggyback onto this
	  transaction.  Keep doing that while new threads continue to
	  arrive.  It doesn't cost much - we're about to run a commit
	  and sleep on IO anyway.  Speeds up many-threaded, many-dir
	  operations by 30x or more...
	 
	  We try and optimize the sleep time against what the
	  underlying disk can do, instead of having a static sleep
	  time.  This is useful for the case where our storage is so
	  fast that it is more optimal to go ahead and force a flush
	  and wait for the transaction to be committed than it is to
	  wait for an arbitrary amount of time for new writers to
	  join the transaction.  We achieve this by measuring how
	  long it takes to commit a transaction, and compare it with
	  how long this transaction has been running, and if run time
	  < commit time then we sleep for the delta and commit.  This
	  greatly helps super fast disks that would see slowdowns as
	  more threads started doing fsyncs.
	 
	  But don't do this if this process was the most recent one
	  to perform a synchronous write.  We do this to detect the
	  case where a single process is doing a stream of sync
	  writes.  No point in waiting for joiners in that case.
	 
	  Setting max_batch_time to 0 disables this completely.
	
	  If the handle is marked SYNC, we need to set another commit
	  going!  We also want to force a commit if the transaction is too
	  old now.
		 Do this even for aborted journals: an abort still
		  completes the commit thread, it just doesn't write
 This is non-blocking 
		
		  Special case: JBD2_SYNC synchronous updates require us
		  to wait for the commit to complete.
	
	  Once stop_this_handle() drops t_updates, the transaction could start
	  committing on us and eventually disappear.  So we must not
	  dereference transaction pointer again after calling
	  stop_this_handle().
  List management code snippets: various functions for manipulating the
  transaction buffer lists.
  Append a buffer to a transaction list, given the transaction's list head
  pointer.
  j_list_lock is held.
  jh->b_state_lock is held.
 Insert at the tail of the list to preserve order 
  Remove a buffer from a transaction list, given the transaction's list
  head pointer.
  Called with j_list_lock held, and the journal may not be locked.
  jh->b_state_lock is held.
  Remove a buffer from the appropriate transaction list.
  Note that this function can change the value of
  bh->b_transaction->t_buffers, t_forget, t_shadow_list, t_log_list or
  t_reserved_list.  If the caller is holding onto a copy of one of these
  pointers, it could go bad.  Generally the caller needs to re-read the
  pointer from the transaction_t.
  Called under j_list_lock.
 Expose it to the VM 
  Remove buffer from all transactions. The caller is responsible for dropping
  the jh reference that belonged to the transaction.
  Called with bh_state lock and j_list_lock
 Get reference so that buffer cannot be freed before we unlock it 
  Called from jbd2_journal_try_to_free_buffers().
  Called under jh->b_state_lock
 written-back checkpointed metadata buffer 
  jbd2_journal_try_to_free_buffers() - try to free page buffers.
  @journal: journal for operation
  @page: to try and free
  For all the buffers on this page,
  if they are fully written out ordered data, move them onto BUF_CLEAN
  so try_to_free_buffers() can reap them.
  This function returns non-zero if we wish try_to_free_buffers()
  to be called. We do this if the page is releasable by try_to_free_buffers().
  We also do it if the page has locked or dirty buffers and the caller wants
  us to perform sync or async writeout.
  This complicates JBD locking somewhat.  We aren't protected by the
  BKL here.  We wish to remove the buffer from its committing or
  running transaction's ->t_datalist via __jbd2_journal_unfile_buffer.
  This may change the value of transaction_t->t_datalist, so anyone
  who looks at t_datalist needs to lock against this function.
  Even worse, someone may be doing a jbd2_journal_dirty_data on this
  buffer.  So we need to lock against that.  jbd2_journal_dirty_data()
  will come out of the lock with the buffer dirty, which makes it
  ineligible for release here.
  Who else is affected by this?  hmm...  Really the only contender
  is do_get_write_access() - it could be looking at the buffer while
  journal_try_to_free_buffer() is changing its state.  But that
  cannot happen because we never reallocate freed data as metadata
  while the data is part of a transaction.  Yes?
  Return 0 on failure, 1 on success
		
		  We take our own ref against the journal_head here to avoid
		  having to add tons of locking around each instance of
		  jbd2_journal_put_journal_head().
  This buffer is no longer needed.  If it is on an older transaction's
  checkpoint list we need to record it on this transaction's forget list
  to pin this buffer (and hence its checkpointing transaction) down until
  this transaction commits.  If the buffer isn't on a checkpoint list, we
  release it.
  Returns non-zero if JBD no longer has an interest in the buffer.
  Called under j_list_lock.
  Called under jh->b_state_lock.
		
		  We don't want to write the buffer anymore, clear the
		  bit so that we don't confuse checks in
		  __journal_file_buffer
  jbd2_journal_invalidatepage
  This code is tricky.  It has a number of cases to deal with.
  There are two invariants which this code relies on:
  i_size must be updated on disk before we start calling invalidatepage on the
  data.
   This is done in ext3 by defining an ext3_setattr method which
   updates i_size before truncate gets going.  By maintaining this
   invariant, we can be sure that it is safe to throw away any buffers
   attached to the current transaction: once the transaction commits,
   we know that the data will not be needed.
   Note however that we can not throw away data belonging to the
   previous, committing transaction!
  Any disk blocks which are part of the previous, committing
  transaction (and which therefore cannot be discarded immediately) are
  not going to be reused in the new running transaction
   The bitmap committed_data images guarantee this: any block which is
   allocated in one transaction and removed in the next will be marked
   as in-use in the committed_data bitmap, so cannot be reused until
   the next transaction to delete the block commits.  This means that
   leaving committing buffers dirty is quite safe: the disk blocks
   cannot be reallocated to a different file and so buffer aliasing is
   not possible.
  The above applies mainly to ordered data mode.  In writeback mode we
  don't make guarantees about the order in which data hits disk --- in
  particular we don't guarantee that new dirty data is flushed before
  transaction commit --- so it is always safe just to discard data
  immediately in that mode.  --sct
  The journal_unmap_buffer helper function returns zero if the buffer
  concerned remains pinned as an anonymous buffer belonging to an older
  transaction.
  We're outside-transaction here.  Either or both of j_running_transaction
  and j_committing_transaction may be NULL.
	
	  It is safe to proceed here without the j_list_lock because the
	  buffers cannot be stolen by try_to_free_buffers as long as we are
	  holding the page lock. --sct
 OK, we have data buffer in journaled mode 
	
	  We cannot remove the buffer from checkpoint lists until the
	  transaction adding inode to orphan list (let's call it T)
	  is committed.  Otherwise if the transaction changing the
	  buffer would be cleaned from the journal before T is
	  committed, a crash will cause that the correct contents of
	  the buffer will be lost.  On the other hand we have to
	  clear the buffer dirty bit at latest at the moment when the
	  transaction marking the buffer as freed in the filesystem
	  structures is committed because from that moment on the
	  block can be reallocated and used by a different page.
	  Since the block hasn't been freed yet but the inode has
	  already been added to orphan list, it is safe for us to add
	  the buffer to BJ_Forget list of the newest transaction.
	 
	  Also we have to clear buffer_mapped flag of a truncated buffer
	  because the buffer_head may be attached to the page straddling
	  i_size (can happen only when blocksize < pagesize) and thus the
	  buffer_head can be reused when the file is extended again. So we end
	  up keeping around invalidated buffers attached to transactions'
	  BJ_Forget list just to stop checkpointing code from cleaning up
	  the transaction this buffer was modified in.
		 First case: not on any transaction.  If it
		  has no checkpoint link, then we can zap it:
		  it's a writeback-mode buffer so we don't care
 bdflush has written it.  We can drop it now 
		 OK, it must be in the journal but still not
		  written fully to disk: it's metadata or
			 ... and once the current transaction has
			  committed, the buffer won't be needed any
			 There is no currently-running transaction. So the
			  orphan record which we wrote for this file must have
			  passed into commit.  We must attach this buffer to
				 The orphan record's transaction has
		
		  The buffer is committing, we simply cannot touch
		  it. If the page is straddling i_size we have to wait
		  for commit and try again.
		
		  OK, buffer won't be reachable after truncate. We just clear
		  b_modified to not confuse transaction credit accounting, and
		  set j_next_transaction to the running transaction (if there
		  is one) and mark buffer as freed so that commit code knows
		  it should clear dirty bits when it is done with the buffer.
		 Good, the buffer belongs to the running transaction.
		  We are writing our own transaction's data, not any
		  previous one's, so it is safe to throw it away
		  (remember that we expect the filesystem to have set
		  i_size already for this truncate so recovery will not
	
	  This is tricky. Although the buffer is truncated, it may be reused
	  if blocksize < pagesize and it is attached to the page straddling
	  EOF. Since the buffer might have been added to BJ_Forget list of the
	  running transaction, journal_get_write_access() won't clear
	  b_modified and credit accounting gets confused. So clear b_modified
	  here.
  jbd2_journal_invalidatepage()
  @journal: journal to use for flush...
  @page:    page to flush
  @offset:  start of the range to invalidate
  @length:  length of the range to invalidate
  Reap page buffers containing data after in the specified range in page.
  Can return -EBUSY if buffers are part of the committing transaction and
  the page is straddling i_size. Caller then has to wait for current commit
  and try again.
	 We will potentially be playing with lists other than just the
	  data lists (especially for journaled data mode), so be
 This block is wholly outside the truncation point 
  File a buffer on the given transaction list.
		
		  For metadata buffers, we track dirty bit in buffer_jbddirty
		  instead of buffer_dirty. We should not see a dirty bit set
		  here because we clear it in do_get_write_access but e.g.
		  tune2fs can modify the sb and set the dirty bit at any time
		  so we try to gracefully handle that.
  Remove a buffer from its current buffer list in preparation for
  dropping it from its current transaction entirely.  If the buffer has
  already started to be used by a subsequent transaction, refile the
  buffer on that transaction's metadata list.
  Called under j_list_lock
  Called under jh->b_state_lock
  When this function returns true, there's no next transaction to refile to
  and the caller has to drop jh reference through
  jbd2_journal_put_journal_head().
 If the buffer is now unused, just drop it. 
	
	  It has been modified by a later transaction: add it to the new
	  transaction's metadata list.
	
	  b_transaction must be set, otherwise the new b_transaction won't
	  be holding jh reference
	
	  We set b_transaction here because b_next_transaction will inherit
	  our jh reference and thus __jbd2_journal_file_buffer() must not
	  take a new one.
  __jbd2_journal_refile_buffer() with necessary locking added. We take our
  bh reference so that we can safely unlock bh.
  The jh and bh may be freed by this call.
  File inode in the inode list of the handle's transaction
 Is inode already attached where we need it? 
	
	  We only ever set this variable to 1 so the test is safe. Since
	  t_need_data_flush is likely to be set, we do the test to save some
	  cacheline bouncing
	 On some different transaction's list - should be
 Not on any transaction list... 
  File truncate and transaction commit interact with each other in a
  non-trivial way.  If a transaction writing data block A is
  committing, we cannot discard the data by truncate until we have
  written them.  Otherwise if we crashed after the transaction with
  write has committed but before the transaction with truncate has
  committed, we could see stale data in block A.  This function is a
  helper to solve this problem.  It starts writeout of the truncated
  part in case it is in the committing transaction.
  Filesystem code must call this function when inode is journaled in
  ordered mode before truncation happens and after the inode has been
  placed on orphan list with the new inode size. The second condition
  avoids the race that someone writes new data and we start
  committing the transaction after this function has been called but
  before a transaction for truncate is started (and furthermore it
  allows us to optimize the case where the addition to orphan list
  happens in the same transaction as write --- we don't have to write
  any data in such case).
 This is a quick check to avoid locking if not necessary 
	 Locks are here just to force reading of recent values, it is
	  enough that the transaction was not committing before we started
 SPDX-License-Identifier: GPL-2.0+
  linuxfsjbd2recovery.c
  Written by Stephen C. Tweedie <sct@redhat.com>, 1999
  Copyright 1999-2000 Red Hat Software --- All Rights Reserved
  Journal recovery routines for the generic filesystem journaling code;
  part of the ext2fs journaling system.
  Maintain information about the progress of the recovery job, so that
  the different passes can carry information between them.
 Release readahead buffers after use 
  When reading from the journal, we are going through the block device
  layer directly and so there is no readahead being done for us.  We
  need to implement any readahead ourselves if we want it to happen at
  all.  Recovery is basically one long sequential read, so make sure we
  do the IO in reasonably large chunks.
  This is not so critical that we need to be enormously clever about
  the readahead size, though.  128K is a purely arbitrary, good-enough
  fixed value.
 Do up to 128K of readahead 
	 Do the readahead itself.  We'll submit MAXBUF buffer_heads at
 __KERNEL__ 
  Read a block from the journal
		 If this is a brand new buffer, start readahead.
  Count the number of in-use tags in a journal descriptor block.
 Make sure we wrap around the log correctly! 
  jbd2_journal_recover - recovers a on-disk journal
  @journal: the journal to recover
  The primary function for recovering the log contents when mounting a
  journaled device.
  Recovery is done in three passes.  In the first pass, we look for the
  end of the log.  In the second, we assemble the list of revoke
  blocks.  In the third and final pass, we replay any un-revoked blocks
  in the log.
	
	  The journal superblock's s_start field (the current log head)
	  is always zero if, and only if, the journal was cleanly
	  unmounted.
	 Restart the log at the next transaction ID, thus invalidating
 Make sure all replayed data is on permanent storage 
  jbd2_journal_skip_recovery - Start journal and wipe exiting records
  @journal: journal to startup
  Locate any valid recovery information from the journal and set up the
  journal structures in memory to ignore it (presumably because the
  caller has evidence that it is out of date).
  This function doesn't appear to be exported..
  We perform one pass over the journal to allow us to tell the user how
  much recovery information is being erased, and to let us initialise
  the journal transaction sequence numbers to the next unused ID.
  calc_chksums calculates the checksums for the blocks described in the
  descriptor block.
 Calculate checksum of the descriptor block. 
 Transactional Checksums 
	
	  First thing is to establish what we expect to find in the log
	  (in terms of transaction IDs), and where (in terms of log
	  block offsets): query the superblock.
	
	  Now we walk through the log, transaction by transaction,
	  making sure that each transaction has a commit block in the
	  expected place.  Each complete transaction gets replayed back
	  into the main filesystem.
		 If we already know where to stop the log traversal,
		  check right now that we haven't gone past the end of
		 Skip over each chunk of the transaction looking
		  either the next descriptor block or the final commit
		 What kind of buffer is it?
		 
		  If it is a descriptor block, check that it has the
		  expected sequence number.  Otherwise, we're all done
		 OK, we have a valid descriptor block which matches
		  all of the sequence number checks.  What are we going
 Verify checksum first 
				
				  PASS_SCAN can see stale blocks due to lazy
				  journal init. Don't error out on those yet.
			 If it is a valid descriptor block, replay it
			  in pass REPLAY; if journal_checksums enabled, then
			  calculate checksums in PASS_SCAN, otherwise,
			 A descriptor block: we can now write all of
			  the data blocks.  Yay, useful work is finally
					 Recover what we can, but
					 If the block has been
					  revoked, then we're all done
 Look for block corruption 
					 Find a buffer for the new
 ll_rw_block(WRITE, 1, &nbh); 
			     How to differentiate between interrupted commit
			                and journal corruption ?
			 
			  {nth transaction}
			         Checksum Verification Failed
			 			 |
			 		 ____________________
			 		|		     |
			  	async_commit             sync_commit
			      		|                    |
			 		| GO TO NEXT    "Journal Corruption"
			 		| TRANSACTION
			 		|
			  {(n+1)th transanction}
			 		|
			  	 _______|______________
			  	|	 	      |
			  Commit block found	Commit block not found
			       |		      |
			  "Journal Corruption"       |
			 		 _____________|_________
			      		|	           	|
			 	nth trans corrupt	OR   nth trans
			 	and (n+1)th interrupted     interrupted
			 	before commit block
			       could reach the disk.
			 	(Cannot find the difference in above
			 	 mentioned conditions. Hence assume
			 	 "Interrupted Commit".)
			
			  If need_check_commit_time is set, it means we are in
			  PASS_SCAN and csum verify failed before. If
			  commit_time is increasing, it's the same journal,
			  otherwise it is stale journal block, just end this
			  recovery.
				
				  It likely does not belong to same journal,
				  just end this recovery with success.
			
			  Found an expected commit block: if checksums
			  are present, verify them in PASS_SCAN; else not
			  much to do other than move on to the next sequence
			  number.
 Neither checksum match nor unused? 
			
			  Check revoke block crc in pass_scan, if csum verify
			  failed, check commit block time later.
			 If we aren't in the REVOKE pass, then we can
	
	  We broke out of the log scan loop: either we came to the
	  known end of the log or we found an unexpected block in the
	  log.  If the latter happened, then we know that the "current"
	  transaction marks the end of the valid log.
		 It's really bad news if different passes end up at
 Scan a revoke record, marking all blocks mentioned as revoked. 
 Block- or MTD-based romfs
  Copyright  2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Derived from: ROMFS file system, Linux implementation
  Copyright  1997-1999  Janos Farkas <chexum@shadow.banki.hu>
  Using parts of the minix filesystem
  Copyright  1991, 1992  Linus Torvalds
  and parts of the affs filesystem additionally
  Copyright  1993  Ray Burr
  Copyright  1996  Hans-Joachim Widmaier
  Changes
 					Changed for 2.1.19 modules
 	Jan 1997			Initial release
 	Jun 1997			2.1.43+ changes
 					Proper page locking in readpage
 					Changed to work with 2.1.45+ fs
 	Jul 1997			Fixed follow_link
 			2.1.47
 					lookup shouldn't return -ENOENT
 					from Horst von Brand:
 					  fail on wrong checksum
 					  double unlock_super was possible
 					  correct namelen for statfs
 					spotted by Bill Hawes:
 					  readlink shouldn't iput()
 	Jun 1998	2.1.106		from Avery Pennarun: glibc scandir()
 					  exposed a problem in readdir
 			2.1.107		code-freeze spellchecker run
 	Aug 1998			2.1.118+ VFS changes
 	Sep 1998	2.1.122		another VFS change (follow_link)
 	Apr 1999	2.2.7		no more EBADF checking in
 					  lookupreaddir, use ERR_PTR
 	Jun 1999	2.3.6		d_alloc_root use changed
 			2.3.9		clean up usage of ENOENTnegative
 					  dentries in lookup
 					clean up page flags setting
 					  (error, uptodate, locking) in
 					  in readpage
 					use init_special_inode for
 					  fifossockets (and streamline) in
 					  read_inode, fix _ops table order
 	Aug 1999	2.3.16		__initfunc() => __init change
 	Oct 1999	2.3.24		page->owner hack obsoleted
 	Nov 1999	2.3.27		2.3.25+ page->offset => index change
  This program is free software; you can redistribute it andor
  modify it under the terms of the GNU General Public Licence
  as published by the Free Software Foundation; either version
  2 of the Licence, or (at your option) any later version.
 hard link 
 directory 
 regular file 
 symlink 
 blockdev 
 chardev 
 socket 
 FIFO 
  read a page worth of data from the image
 32 bit warning -- but not for us :) 
  read the entries from a directory
 XXX dynamic? 
 Not really failsafe, but we are read-only... 
 Fetch inode info 
  look up an entry in a directory
 got from dentry 
	 search all the file entries in the list starting from the one
 try to match the first 16 bytes of name 
 Hard link handling 
 next entry 
  get a romfs inode based on its position in the image (which doubles as the
  inode number)
	 we might have to traverse a chain of "hard link" file entries to get
 XXX: do romfs_checksum here too (with name) 
 determine the length of the filename 
 get an inode for this image position 
 precalculate the data offset 
 Hard to decide.. 
 set up mode and ops 
 depending on MBZ for sockfifos 
  allocate a new inode
  return a spent inode to the slab cache
  get filesystem statistics
	 When calling huge_encode_dev(),
	  use sb->s_bdev->bd_dev when,
	    - CONFIG_ROMFS_ON_BLOCK defined
	  use sb->s_dev when,
	    - CONFIG_ROMFS_ON_BLOCK undefined and
	    - CONFIG_ROMFS_ON_MTD defined
	  leave id as 0 when,
	    - CONFIG_ROMFS_ON_BLOCK undefined and
	    - CONFIG_ROMFS_ON_MTD undefined
  remounting must involve read-only
  checksum check on part of a romfs filesystem
  fill in the superblock
 Use same dev ID from the underlying mtdblock device 
 read the image superblock and check it 
 find the root directory 
  get a superblock for mounting
  Set up the filesystem mount context.
  destroy a romfs superblock in the appropriate manner
  inode storage initialiser
  romfs module initialisation
  romfs module removal
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 Actually dual-licensed, but it doesn't matter for 
 SPDX-License-Identifier: GPL-2.0-or-later
 RomFS storage access routines
  Copyright  2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  read data from an romfs image on an MTD device
  determine the length of a string in a romfs image on an MTD device
 scan the string up to 16 bytes at a time 
  compare a string to one in a romfs image on MTD
  - return 1 if matched, 0 if differ, -ve if error
	 scan the string up to 16 bytes at a time, and attempt to grab the
 check the trailing NUL was 
 CONFIG_ROMFS_ON_MTD 
  read data from an romfs image on a block device
 copy the string up to blocksize bytes at a time 
  determine the length of a string in romfs on a block device
 scan the string up to blocksize bytes at a time 
  compare a string to one in a romfs image on a block device
  - return 1 if matched, 0 if differ, -ve if error
 compare string up to a block at a time 
		 the terminating NUL must be on the first byte of the next
 CONFIG_ROMFS_ON_BLOCK 
  read data from the romfs image
  determine the length of a string in romfs
  compare a string to one in romfs
  - the string to be compared to, str, may not be NUL-terminated; instead the
    string is of the specified size
  - return 1 if matched, 0 if differ, -ve if error
 SPDX-License-Identifier: GPL-2.0-or-later
 NOMMU mmap support for RomFS on MTD devices
  Copyright  2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  try to determine where a shared mapping can be made
  - only supported for NOMMU at the moment (MMU can't doesn't copy private
    mappings)
  - attempts to map through to the underlying MTD device
 the mapping mustn't extend beyond the EOF 
 the mapping mustn't extend beyond the EOF 
  permit a RO mapping to be made directly through onto an MTD device if
  possible
 SPDX-License-Identifier: GPL-2.0-or-later
 -- linux-c -- --------------------------------------------------------- 
  linuxfsdevptsinode.c
   Copyright 1998-2004 H. Peter Anvin -- All Rights Reserved
  ptmx is a new node in devpts and will be unused in legacy (single-
  instance) mode. To prevent surprises in user space, set permissions of
  ptmx to 0. Use 'chmod' or remount with '-o ptmxmode' to set meaningful
  permissions.
  sysctl support for setting limits on the number of Unix98 ptys allocated.
  Otherwise one can eat up all kernel memory by opening devptmx repeatedly.
 Is a devpts filesystem at "pts" in the same directory? 
 Is the path the root of a devpts filesystem? 
  Try to find a suitable devpts filesystem. We support the following
  scenarios:
  - The ptmx device node is located in the same directory as the devpts
    mount where the pts device nodes are located.
    This is e.g. the case when calling open on the devptsptmx device
    node when the devpts filesystem is mounted at devpts.
  - The ptmx device node is located outside the devpts filesystem mount
    where the pts device nodes are located. For example, the ptmx device
    is a symlink, separate device node, or bind-mount.
    A supported scenario is bind-mounting devptsptmx to devptmx and
    then calling open on devptmx. In this case a suitable pts
    subdirectory can be found in the common parent directory dev of the
    devpts mount and the ptmx bind-mount, after resolving the devptmx
    bind-mount.
    If no suitable pts subdirectory can be found this function will fail.
    This is e.g. the case when bind-mounting devptsptmx to ptmx.
	 Walk upward while the start point is a bind mount of
	  a single file.
 devpts_ptmx_path() finds a devpts fs or returns an error. 
 Has the devpts filesystem already been found? 
	
	  pty code needs to hold extra references in case of last devtty close
  parse_mount_options():
 	Set @opts to mount options specified in @data. If an option is not
 	specified in @data, set it to its default value.
  Note: @data may be NULL (in which case all options are set to default).
	 Only allow instances mounted from the initial mount
	  namespace to tap the reserve pool of ptys.
 If we have already created ptmx node, return 
	
	  Create a new 'ptmx' node in this mount of devpts.
	
	  parse_mount_options() restores options to default values
	  before parsing and may have changed ptmxmode. So, update the
	  mode in the inode too. Bogus options don't fail the remount,
	  so do this even on error return.
  devpts_mount()
      Mount a new (private) instance of devpts.  PTYs created in this
      instance are independent of the PTYs in other devpts instances.
  The normal naming convention is simply devpts<number>; this conforms
  to the System V naming convention
  devpts_pty_new -- create a new inode in devpts
  @ptmx_inode: inode of the master
  @device: major+minor of the node to be created
  @index: used as a name of the node
  @priv: what's given back by devpts_get_priv
  The created inode is returned. Remove it from devpts by devpts_pty_kill.
  devpts_get_priv -- get private data for a slave
  @pts_inode: inode of the slave
  Returns whatever was passed as priv in devpts_pty_new for a given inode.
  devpts_pty_kill -- remove inode form devpts
  @inode: inode of the slave to be removed
  This is an inverse operation of devpts_pty_new.
 d_alloc_name() in devpts_pty_new() 
 SPDX-License-Identifier: GPL-2.0
  Filesystem-level keyring for fscrypt
  Copyright 2019 Google LLC
  This file implements management of fscrypt master keys in the
  filesystem-level keyring, including the ioctls:
  - FS_IOC_ADD_ENCRYPTION_KEY
  - FS_IOC_REMOVE_ENCRYPTION_KEY
  - FS_IOC_REMOVE_ENCRYPTION_KEY_ALL_USERS
  - FS_IOC_GET_ENCRYPTION_KEY_STATUS
  See the "User API" section of Documentationfilesystemsfscrypt.rst for more
  information about these ioctls.
  Type of key in ->s_master_keys.  Each key of this type represents a master
  key which has been added to the filesystem.  Its payload is a
  'struct fscrypt_master_key'.  The "." prefix in the key type name prevents
  users from adding keys of this type via the keyrings syscalls rather than via
  the intended method of FS_IOC_ADD_ENCRYPTION_KEY.
	
	  We just charge FSCRYPT_MAX_KEY_SIZE bytes to the user's key quota for
	  each key, regardless of the exact key size.  The amount of memory
	  actually used is greater than the size of the raw key anyway.
  Type of key in ->mk_users.  Each key of this type represents a particular
  user who has added a particular master key.
  Note that the name of this key type really should be something like
  ".fscrypt-user" instead of simply ".fscrypt".  But the shorter name is chosen
  mainly for simplicity of presentation in prockeys when read by a non-root
  user.  And it is expected to be rare that a key is actually added by multiple
  users, since users should keep their encryption keys confidential.
 Search ->s_master_keys or ->mk_users 
	
	  We need to mark the keyring reference as "possessed" so that we
	  acquire permission to search it, via the KEY_POS_SEARCH permission.
 possessed );
 not found 
 recently invalidated 
 Create ->s_master_keys if needed.  Synchronized by fscrypt_add_key_mutex. 
	
	  Pairs with the smp_load_acquire() in fscrypt_find_master_key().
	  I.e., here we publish ->s_master_keys with a RELEASE barrier so that
	  concurrent tasks can ACQUIRE it.
  Find the specified master key in ->s_master_keys.
  Returns ERR_PTR(-ENOKEY) if not found.
	
	  Pairs with the smp_store_release() in allocate_filesystem_keyring().
	  I.e., another task can publish ->s_master_keys concurrently,
	  executing a RELEASE barrier.  We need to use smp_load_acquire() here
	  to safely ACQUIRE the memory the other task published.
 No keyring yet, so no keys yet. 
  Find the current user's "key" in the master key's ->mk_users.
  Returns ERR_PTR(-ENOKEY) if not found.
  Give the current user a "key" in ->mk_users.  This charges the user's quota
  and marks the master key as added by the current user, so that it cannot be
  removed by another user with the key.  Either the master key's key->sem must
  be held for write, or the master key must be still undergoing initialization.
  Remove the current user's "key" from ->mk_users.
  The master key's key->sem must be held for write.
  Returns 0 if removed, -ENOKEY if not found, or another -errno code.
  Allocate a new fscrypt_master_key which contains the given secret, set it as
  the payload of a new 'struct key' of type fscrypt, and link the 'struct key'
  into the given keyring.  Synchronized by fscrypt_add_key_mutex.
 secret is present 
	
	  Note that we don't charge this key to anyone's quota, since when
	  ->mk_users is in use those keys are charged instead, and otherwise
	  (when ->mk_users isn't in use) only root can add these keys.
	
	  If the current user is already in ->mk_users, then there's nothing to
	  do.  (Not applicable for v1 policy keys, which have NULL ->mk_users.)
 If we'll be re-adding ->mk_secret, try to take the reference. 
 Add the current user to ->mk_users, if applicable. 
 Re-add the secret if needed. 
 serialize find + link 
 Didn't find the key in ->s_master_keys.  Add it. 
		
		  Found the key in ->s_master_keys.  Re-add the secret if
		  needed, and add the user to ->mk_users if needed.
 Key being removed or needs to be removed 
		
		  Now that the HKDF context is initialized, the raw key is no
		  longer needed.
 Calculate the key identifier 
  Retrieve the raw key from the Linux keyring key specified by 'key_id', and
  store it into 'secret'.
  The key must be of type "fscrypt-provisioning" and must have the field
  fscrypt_provisioning_key_payload::type set to 'type', indicating that it's
  only usable with fscrypt with the particular KDF version identified by
  'type'.  We don't use the "logon" key type because there's no way to
  completely restrict the use of such keys; they can be used by any kernel API
  that accepts "logon" keys and doesn't require a specific service prefix.
  The ability to specify the key via Linux keyring key is intended for cases
  where userspace needs to re-add keys after the filesystem is unmounted and
  re-mounted.  Most users should just provide the raw key directly instead.
 Don't allow fscrypt v1 keys to be used as v2 keys and vice versa. 
  Add a master encryption key to the filesystem, causing all files which were
  encrypted with it to appear "unlocked" (decrypted) when accessed.
  When adding a key for use by v1 encryption policies, this ioctl is
  privileged, and userspace must provide the 'key_descriptor'.
  When adding a key for use by v2+ encryption policies, this ioctl is
  unprivileged.  This is needed, in general, to allow non-root users to use
  encryption without encountering the visibility problems of process-subscribed
  keyrings and the inability to properly remove keys.  This works by having
  each key identified by its cryptographically secure hash --- the
  'key_identifier'.  The cryptographic hash ensures that a malicious user
  cannot add the wrong key for a given identifier.  Furthermore, each added key
  is charged to the appropriate user's quota for the keyrings service, which
  prevents a malicious user from adding too many keys.  Finally, we forbid a
  user from removing a key while other users have added it too, which prevents
  a user who knows another user's key from causing a denial-of-service by
  removing it at an inopportune time.  (We tolerate that a user who knows a key
  can prevent other users from removing it.)
  For more details, see the "FS_IOC_ADD_ENCRYPTION_KEY" section of
  Documentationfilesystemsfscrypt.rst.
	
	  Only root can add keys that are identified by an arbitrary descriptor
	  rather than by a cryptographic hash --- since otherwise a malicious
	  user could add the wrong key.
 Return the key identifier to userspace, if applicable 
  Add the key for '-o test_dummy_encryption' to the filesystem keyring.
  Use a per-boot random key to prevent people from misusing this option.
  Verify that the current user has added a master key with the given identifier
  (returns -ENOKEY if not).  This is needed to prevent a user from encrypting
  their files using some other user's key which they don't actually know.
  Cryptographically this isn't much of a problem, but the semantics of this
  would be a bit weird, so it's best to just forbid it.
  The system administrator (CAP_FOWNER) can override this, which should be
  enough for any use cases where encryption policies are being set using keys
  that were chosen ahead of time but aren't available at the moment.
  Note that the key may have already removed by the time this returns, but
  that's okay; we just care whether the key was there at some point.
  Return: 0 if the key is added, -ENOKEY if it isn't, or another -errno code
  Try to evict the inode's dentries from the dentry cache.  If the inode is a
  directory, then it can have at most one dentry; however, that dentry may be
  pinned by child dentries, so first try to evict the children too.
 select an example file to show for debugging purposes 
 If the inode is currently being created, ino may still be 0. 
	
	  An inode can't be evicted while it is dirty or has dirty pages.
	  Thus, we first have to clean the inodes in ->mk_decrypted_inodes.
	 
	  Just do it the easy way: call sync_filesystem().  It's overkill, but
	  it works, and it's more important to minimize the amount of caches we
	  drop than the amount of data we sync.  Also, unprivileged users can
	  already call sync_filesystem() via sys_syncfs() or sys_sync().
 If a sync error occurs, still try to evict as much as possible. 
	
	  Inodes are pinned by their dentries, so we have to evict their
	  dentries.  shrink_dcache_sb() would suffice, but would be overkill
	  and inappropriate for use by unprivileged users.  So instead go
	  through the inodes' alias lists and try to evict each dentry.
	
	  evict_dentries_for_decrypted_inodes() already iput() each inode in
	  the list; any inodes for which that dropped the last reference will
	  have been evicted due to fscrypt_drop_inode() detecting the key
	  removal and telling the VFS to evict the inode.  So to finish, we
	  just need to check whether any inodes couldn't be evicted.
  Try to remove an fscrypt master encryption key.
  FS_IOC_REMOVE_ENCRYPTION_KEY (all_users=false) removes the current user's
  claim to the key, then removes the key itself if no other users have claims.
  FS_IOC_REMOVE_ENCRYPTION_KEY_ALL_USERS (all_users=true) always removes the
  key itself.
  To "remove the key itself", first we wipe the actual master key secret, so
  that no more inodes can be unlocked with it.  Then we try to evict all cached
  inodes that had been unlocked with the key.
  If all inodes were evicted, then we unlink the fscrypt_master_key from the
  keyring.  Otherwise it remains in the keyring in the "incompletely removed"
  state (without the actual secret key) where it tracks the list of remaining
  inodes.  Userspace can execute the ioctl again later to retry eviction, or
  alternatively can re-add the secret key again.
  For more details, see the "Removing keys" section of
  Documentationfilesystemsfscrypt.rst.
	
	  Only root can add and remove keys that are identified by an arbitrary
	  descriptor rather than by a cryptographic hash.
 Find the key being removed. 
 If relevant, remove current user's (or all users) claim to the key 
			
			  Other users have still added the key too.  We removed
			  the current user's claim to the key, but we still
			  can't remove the key itself.
 No user claims remaining.  Go ahead and wipe the secret. 
		
		  No inodes reference the key, and we wiped the secret, so the
		  key object is free to be removed from the keyring.
 Some inodes still reference this key; try to evict them. 
	
	  We return 0 if we successfully did something: removed a claim to the
	  key, wiped the secret, or tried locking the files again.  Users need
	  to check the informational status flags if they care whether the key
	  has been fully removed including all files locked.
  Retrieve the status of an fscrypt master encryption key.
  We set ->status to indicate whether the key is absent, present, or
  incompletely removed.  "Incompletely removed" means that the master key
  secret has been removed, but some files which had been unlocked with it are
  still in use.  This field allows applications to easily determine the state
  of an encrypted directory without using a hack such as trying to open a
  regular file in it (which can confuse the "incompletely removed" state with
  absent or present).
  In addition, for v2 policy keys we allow applications to determine, via
  ->status_flags and ->user_count, whether the key has been added by the
  current user, by other users, or by both.  Most applications should not need
  this, since ordinarily only one user should know a given key.  However, if a
  secret key is shared by multiple users, applications may wish to add an
  already-present key to prevent other users from removing it.  This ioctl can
  be used to check whether that really is the case before the work is done to
  add the key --- which might e.g. require prompting the user for a passphrase.
  For more details, see the "FS_IOC_GET_ENCRYPTION_KEY_STATUS" section of
  Documentationfilesystemsfscrypt.rst.
 SPDX-License-Identifier: GPL-2.0
  This contains functions for filename crypto management
  Copyright (C) 2015, Google, Inc.
  Copyright (C) 2015, Motorola Mobility
  Written by Uday Savagaonkar, 2014.
  Modified by Jaegeuk Kim, 2015.
  This has not yet undergone a rigorous security audit.
  struct fscrypt_nokey_name - identifier for directory entry when key is absent
  When userspace lists an encrypted directory without access to the key, the
  filesystem must present a unique "no-key name" for each filename that allows
  it to find the directory entry again if requested.  Naively, that would just
  mean using the ciphertext filenames.  However, since the ciphertext filenames
  can contain illegal characters ('\0' and ''), they must be encoded in some
  way.  We use base64url.  But that can cause names to exceed NAME_MAX (255
  bytes), so we also need to use a strong hash to abbreviate long names.
  The filesystem may also need another kind of hash, the "dirhash", to quickly
  find the directory entry.  Since filesystems normally compute the dirhash
  over the on-disk filename (i.e. the ciphertext), it's not computable from
  no-key names that abbreviate the ciphertext using the strong hash to fit in
  NAME_MAX.  It's also not computable if it's a keyed hash taken over the
  plaintext (but it may still be available in the on-disk directory entry);
  casefolded directories use this type of dirhash.  At least in these cases,
  each no-key name must include the name's dirhash too.
  To meet all these requirements, we base64url-encode the following
  variable-length structure.  It contains the dirhash, or 0's if the filesystem
  didn't provide one; up to 149 bytes of the ciphertext name; and for
  ciphertexts longer than 149 bytes, also the SHA-256 of the remaining bytes.
  This ensures that each no-key name contains everything needed to find the
  directory entry again, contains only legal characters, doesn't exceed
  NAME_MAX, is unambiguous unless there's a SHA-256 collision, and that we only
  take the performance hit of SHA-256 on very long filenames (which are rare).
 189 bytes => 252 bytes base64url-encoded, which is <= NAME_MAX (255) 
  Decoded size of max-size no-key name, i.e. a name that was abbreviated using
  the strong hash and thus includes the 'sha256' field.  This isn't simply
  sizeof(struct fscrypt_nokey_name), as the padding at the end isn't included.
 Encoded size of max-size no-key name 
  fscrypt_fname_encrypt() - encrypt a filename
  @inode: inode of the parent directory (for regular filenames)
 	   or of the symlink (for symlink targets)
  @iname: the filename to encrypt
  @out: (output) the encrypted filename
  @olen: size of the encrypted filename.  It must be at least @iname->len.
 	  Any extra space is filled with NUL padding before encryption.
  Return: 0 on success, -errno on failure
	
	  Copy the filename to the output buffer for encrypting in-place and
	  pad it with the needed number of NUL bytes.
 Initialize the IV 
 Set up the encryption request 
 Do the encryption 
  fname_decrypt() - decrypt a filename
  @inode: inode of the parent directory (for regular filenames)
 	   or of the symlink (for symlink targets)
  @iname: the encrypted filename to decrypt
  @oname: (output) the decrypted filename.  The caller must have allocated
 	   enough space for this, e.g. using fscrypt_fname_alloc_buffer().
  Return: 0 on success, -errno on failure
 Allocate request 
 Initialize IV 
 Create decryption request 
  fscrypt_base64url_encode() - base64url-encode some binary data
  @src: the binary data to encode
  @srclen: the length of @src in bytes
  @dst: (output) the base64url-encoded string.  Not NUL-terminated.
  Encodes data using base64url encoding, i.e. the "Base 64 Encoding with URL
  and Filename Safe Alphabet" specified by RFC 4648.  '='-padding isn't used,
  as it's unneeded and not required by the RFC.  base64url is used instead of
  base64 to avoid the '' character, which isn't allowed in filenames.
  Return: the length of the resulting base64url-encoded string in bytes.
 	   This will be equal to FSCRYPT_BASE64URL_CHARS(srclen).
  fscrypt_base64url_decode() - base64url-decode a string
  @src: the string to decode.  Doesn't need to be NUL-terminated.
  @srclen: the length of @src in bytes
  @dst: (output) the decoded binary data
  Decodes a string using base64url encoding, i.e. the "Base 64 Encoding with
  URL and Filename Safe Alphabet" specified by RFC 4648.  '='-padding isn't
  accepted, nor are non-encoding characters such as whitespace.
  This implementation hasn't been optimized for performance.
  Return: the length of the resulting decoded binary data in bytes,
 	   or -1 if the string isn't a valid base64url string.
  fscrypt_fname_alloc_buffer() - allocate a buffer for presented filenames
  @max_encrypted_len: maximum length of encrypted filenames the buffer will be
 		       used to present
  @crypto_str: (output) buffer to allocate
  Allocate a buffer that is large enough to hold any decrypted or encoded
  filename (null-terminated), for the given maximum encrypted filename length.
  Return: 0 on success, -errno on failure
  fscrypt_fname_free_buffer() - free a buffer for presented filenames
  @crypto_str: the buffer to free
  Free a buffer that was allocated by fscrypt_fname_alloc_buffer().
  fscrypt_fname_disk_to_usr() - convert an encrypted filename to
 				 user-presentable form
  @inode: inode of the parent directory (for regular filenames)
 	   or of the symlink (for symlink targets)
  @hash: first part of the name's dirhash, if applicable.  This only needs to
 	  be provided if the filename is located in an indexed directory whose
 	  encryption key may be unavailable.  Not needed for symlink targets.
  @minor_hash: second part of the name's dirhash, if applicable
  @iname: encrypted filename to convert.  May also be "." or "..", which
 	   aren't actually encrypted.
  @oname: output buffer for the user-presentable filename.  The caller must
 	   have allocated enough space for this, e.g. using
 	   fscrypt_fname_alloc_buffer().
  If the key is available, we'll decrypt the disk name.  Otherwise, we'll
  encode it for presentation in fscrypt_nokey_name format.
  See struct fscrypt_nokey_name for details.
  Return: 0 on success, -errno on failure
 size of the unencoded no-key name 
	
	  Sanity check that struct fscrypt_nokey_name doesn't have padding
	  between fields and that its encoded size never exceeds NAME_MAX.
 Compute strong hash of remaining part of name. 
  fscrypt_setup_filename() - prepare to search a possibly encrypted directory
  @dir: the directory that will be searched
  @iname: the user-provided filename being searched for
  @lookup: 1 if we're allowed to proceed without the key because it's
 	->lookup() or we're finding the dir_entry for deletion; 0 if we cannot
 	proceed without the key because we're going to create the dir_entry.
  @fname: the filename information to be filled in
  Given a user-provided filename @iname, this function sets @fname->disk_name
  to the name that would be stored in the on-disk directory entry, if possible.
  If the directory is unencrypted this is simply @iname.  Else, if we have the
  directory's encryption key, then @iname is the plaintext, so we encrypt it to
  get the disk_name.
  Else, for keyless @lookup operations, @iname should be a no-key name, so we
  decode it to get the struct fscrypt_nokey_name.  Non-@lookup operations will
  be impossible in this case, so we fail them with ENOKEY.
  If successful, fscrypt_free_filename() must be called later to clean up.
  Return: 0 on success, -errno on failure
	
	  We don't have the key and we are doing a lookup; decode the
	  user-supplied name
 The full ciphertext filename is available. 
  fscrypt_match_name() - test whether the given name matches a directory entry
  @fname: the name being searched for
  @de_name: the name from the directory entry
  @de_name_len: the length of @de_name in bytes
  Normally @fname->disk_name will be set, and in that case we simply compare
  that to the name stored in the directory entry.  The only exception is that
  if we don't have the key for an encrypted directory and the name we're
  looking for is very long, then we won't have the full disk_name and instead
  we'll need to match against a fscrypt_nokey_name that includes a strong hash.
  Return: %true if the name matches, otherwise %false.
  fscrypt_fname_siphash() - calculate the SipHash of a filename
  @dir: the parent directory
  @name: the filename to calculate the SipHash of
  Given a plaintext filename @name and a directory @dir which uses SipHash as
  its dirhash method and has had its fscrypt key set up, this function
  calculates the SipHash of that name using the directory's secret dirhash key.
  Return: the SipHash of @name using the hash key of @dir
  Validate dentries in encrypted directories to make sure we aren't potentially
  caching stale dentries after a key has been added.
	
	  Plaintext names are always valid, since fscrypt doesn't support
	  reverting to no-key names without evicting the directory's inode
	  -- which implies eviction of the dentries in the directory.
	
	  No-key name; valid if the directory's key is still unavailable.
	 
	  Although fscrypt forbids rename() on no-key names, we still must use
	  dget_parent() here rather than use ->d_parent directly.  That's
	  because a corrupted fs image may contain directory hard links, which
	  the VFS handles by moving the directory's dentry tree in the dcache
	  each time ->lookup() finds the directory and it already has a dentry
	  elsewhere.  Thus ->d_parent can be changing, and we must safely grab
	  a reference to some ->d_parent to prevent it from being freed.
	
	  Pass allow_unsupported=true, so that files with an unsupported
	  encryption policy can be deleted.
 SPDX-License-Identifier: GPL-2.0-only
  fscryptohooks.c
  Encryption hooks for higher-level filesystem operations.
  fscrypt_file_open() - prepare to open a possibly-encrypted regular file
  @inode: the inode being opened
  @filp: the struct file being set up
  Currently, an encrypted regular file can only be opened if its encryption key
  is available; access to the raw encrypted contents is not supported.
  Therefore, we first set up the inode's encryption key (if not already done)
  and return an error if it's unavailable.
  We also verify that if the parent directory (from the path via which the file
  is being opened) is encrypted, then the inode being opened uses the same
  encryption policy.  This is needed as part of the enforcement that all files
  in an encrypted directory tree use the same encryption policy, as a
  protection against certain types of offline attacks.  Note that this check is
  needed even when opening an unencrypted file, since it's forbidden to have
  an unencrypted file in an encrypted directory.
  Return: 0 on success, -ENOKEY if the key is missing, or another -errno code
	
	  We don't need to separately check that the directory inode's key is
	  available, as it's implied by the dentry not being a no-key name.
	
	  We don't need to separately check that the directory inodes' keys are
	  available, as it's implied by the dentries not being no-key names.
  fscrypt_prepare_setflags() - prepare to change flags with FS_IOC_SETFLAGS
  @inode: the inode on which flags are being changed
  @oldflags: the old flags
  @flags: the new flags
  The caller should be holding i_rwsem for write.
  Return: 0 on success; -errno if the flags change isn't allowed or if
 	   another error occurs.
	
	  When the CASEFOLD flag is set on an encrypted directory, we must
	  derive the secret key needed for the dirhash.  This is only possible
	  if the directory uses a v2 encryption policy.
  fscrypt_prepare_symlink() - prepare to create a possibly-encrypted symlink
  @dir: directory in which the symlink is being created
  @target: plaintext symlink target
  @len: length of @target excluding null terminator
  @max_len: space the filesystem has available to store the symlink target
  @disk_link: (out) the on-disk symlink target being prepared
  This function computes the size the symlink target will require on-disk,
  stores it in @disk_link->len, and validates it against @max_len.  An
  encrypted symlink may be longer than the original.
  Additionally, @disk_link->name is set to @target if the symlink will be
  unencrypted, but left NULL if the symlink will be encrypted.  For encrypted
  symlinks, the filesystem must call fscrypt_encrypt_symlink() to create the
  on-disk target later.  (The reason for the two-step process is that some
  filesystems need to know the size of the symlink target before creating the
  inode, e.g. to determine whether it will be a "fast" or "slow" symlink.)
  Return: 0 on success, -ENAMETOOLONG if the symlink target is too long,
  -ENOKEY if the encryption key is missing, or another -errno code if a problem
  occurred while setting up the encryption key.
	
	  To calculate the size of the encrypted symlink target we need to know
	  the amount of NUL padding, which is determined by the flags set in
	  the encryption policy which will be inherited from the directory.
 Not encrypted 
	
	  Calculate the size of the encrypted symlink and verify it won't
	  exceed max_len.  Note that for historical reasons, encrypted symlink
	  targets are prefixed with the ciphertext length, despite this
	  actually being redundant with i_size.  This decreases by 2 bytes the
	  longest symlink target we can accept.
	 
	  We could recover 1 byte by not counting a null terminator, but
	  counting it (even though it is meaningless for ciphertext) is simpler
	  for now since filesystems will assume it is there and subtract it.
	
	  fscrypt_prepare_new_inode() should have already set up the new
	  symlink inode's encryption key.  We don't wait until now to do it,
	  since we may be in a filesystem transaction now.
 filesystem-provided buffer 
	
	  Null-terminating the ciphertext doesn't make sense, but we still
	  count the null terminator in the length, so we might as well
	  initialize it just in case the filesystem writes it out.
 Cache the plaintext symlink target for later use by get_link() 
  fscrypt_get_symlink() - get the target of an encrypted symlink
  @inode: the symlink inode
  @caddr: the on-disk contents of the symlink
  @max_size: size of @caddr buffer
  @done: if successful, will be set up to free the returned target if needed
  If the symlink's encryption key is available, we decrypt its target.
  Otherwise, we encode its target for presentation.
  This may sleep, so the filesystem must have dropped out of RCU mode already.
  Return: the presentable symlink target or an ERR_PTR()
 This is for encrypted symlinks only 
 If the decrypted target is already cached, just return it. 
	
	  Try to set up the symlink's encryption key, but we can continue
	  regardless of whether the key is available or not.
	
	  For historical reasons, encrypted symlink targets are prefixed with
	  the ciphertext length, even though this is redundant with i_size.
	
	  Cache decrypted symlink targets in i_link for later use.  Don't cache
	  symlink targets encoded without the key, since those become outdated
	  once the key is added.  This pairs with the READ_ONCE() above and in
	  the VFS path lookup code.
  fscrypt_symlink_getattr() - set the correct st_size for encrypted symlinks
  @path: the path for the encrypted symlink being queried
  @stat: the struct being filled with the symlink's attributes
  Override st_size of encrypted symlinks to be the length of the decrypted
  symlink target (or the no-key encoded symlink target, if the key is
  unavailable) rather than the length of the encrypted symlink target.  This is
  necessary for st_size to match the symlink target that userspace actually
  sees.  POSIX requires this, and some userspace programs depend on it.
  This requires reading the symlink target from disk if needed, setting up the
  inode's encryption key if possible, and then decrypting or encoding the
  symlink target.  This makes lstat() more heavyweight than is normally the
  case.  However, decrypted symlink targets will be cached in ->i_link, so
  usually the symlink won't have to be read and decrypted again later ifwhen
  it is actually followed, readlink() is called, or lstat() is called again.
  Return: 0 on success, -errno on failure
	
	  To get the symlink target that userspace will see (whether it's the
	  decrypted target or the no-key encoded target), we can just get it in
	  the same way the VFS does during path resolution and readlink().
 SPDX-License-Identifier: GPL-2.0-only
  This contains encryption functions for per-file encryption.
  Copyright (C) 2015, Google, Inc.
  Copyright (C) 2015, Motorola Mobility
  Written by Michael Halcrow, 2014.
  Filename encryption additions
 	Uday Savagaonkar, 2014
  Encryption policy handling additions
 	Ildar Muslukhov, 2014
  Add fscrypt_pullback_bio_page()
 	Jaegeuk Kim, 2015.
  This has not yet undergone a rigorous security audit.
  The usage of AES-XTS should conform to recommendations in NIST
  Special Publication 800-38E and IEEE P1619D16.
  fscrypt_free_bounce_page() - free a ciphertext bounce page
  @bounce_page: the bounce page to free, or NULL
  Free a bounce page that was allocated by fscrypt_encrypt_pagecache_blocks(),
  or by fscrypt_alloc_bounce_page() directly.
 Encrypt or decrypt a single filesystem block of file contents 
  fscrypt_encrypt_pagecache_blocks() - Encrypt filesystem blocks from a
 					pagecache page
  @page:      The locked pagecache page containing the block(s) to encrypt
  @len:       Total size of the block(s) to encrypt.  Must be a nonzero
 		multiple of the filesystem's block size.
  @offs:      Byte offset within @page of the first block to encrypt.  Must be
 		a multiple of the filesystem's block size.
  @gfp_flags: Memory allocation flags.  See details below.
  A new bounce page is allocated, and the specified block(s) are encrypted into
  it.  In the bounce page, the ciphertext block(s) will be located at the same
  offsets at which the plaintext block(s) were located in the source page; any
  other parts of the bounce page will be left uninitialized.  However, normally
  blocksize == PAGE_SIZE and the whole page is encrypted at once.
  This is for use by the filesystem's ->writepages() method.
  The bounce page allocation is mempool-backed, so it will always succeed when
  @gfp_flags includes __GFP_DIRECT_RECLAIM, e.g. when it's GFP_NOFS.  However,
  only the first page of each bio can be allocated this way.  To prevent
  deadlocks, for any additional pages a mask like GFP_NOWAIT must be used.
  Return: the new encrypted bounce page on success; an ERR_PTR() on failure
  fscrypt_encrypt_block_inplace() - Encrypt a filesystem block in-place
  @inode:     The inode to which this block belongs
  @page:      The page containing the block to encrypt
  @len:       Size of block to encrypt.  Doesn't need to be a multiple of the
 		fs block size, but must be a multiple of FS_CRYPTO_BLOCK_SIZE.
  @offs:      Byte offset within @page at which the block to encrypt begins
  @lblk_num:  Filesystem logical block number of the block, i.e. the 0-based
 		number of the block within the file
  @gfp_flags: Memory allocation flags
  Encrypt a possibly-compressed filesystem block that is located in an
  arbitrary page, not necessarily in the original pagecache page.  The @inode
  and @lblk_num must be specified, as they can't be determined from @page.
  Return: 0 on success; -errno on failure
  fscrypt_decrypt_pagecache_blocks() - Decrypt filesystem blocks in a
 					pagecache page
  @page:      The locked pagecache page containing the block(s) to decrypt
  @len:       Total size of the block(s) to decrypt.  Must be a nonzero
 		multiple of the filesystem's block size.
  @offs:      Byte offset within @page of the first block to decrypt.  Must be
 		a multiple of the filesystem's block size.
  The specified block(s) are decrypted in-place within the pagecache page,
  which must still be locked and not uptodate.  Normally, blocksize ==
  PAGE_SIZE and the whole page is decrypted at once.
  This is for use by the filesystem's ->readpages() method.
  Return: 0 on success; -errno on failure
  fscrypt_decrypt_block_inplace() - Decrypt a filesystem block in-place
  @inode:     The inode to which this block belongs
  @page:      The page containing the block to decrypt
  @len:       Size of block to decrypt.  Doesn't need to be a multiple of the
 		fs block size, but must be a multiple of FS_CRYPTO_BLOCK_SIZE.
  @offs:      Byte offset within @page at which the block to decrypt begins
  @lblk_num:  Filesystem logical block number of the block, i.e. the 0-based
 		number of the block within the file
  Decrypt a possibly-compressed filesystem block that is located in an
  arbitrary page, not necessarily in the original pagecache page.  The @inode
  and @lblk_num must be specified, as they can't be determined from @page.
  Return: 0 on success; -errno on failure
  fscrypt_initialize() - allocate major buffers for fs encryption.
  @cop_flags:  fscrypt operations flags
  We only call this when we start accessing encrypted files, since it
  results in memory getting allocated that wouldn't otherwise be used.
  Return: 0 on success; -errno on failure
 No need to allocate a bounce page pool if this FS won't use it. 
  fscrypt_init() - Set up for fs encryption.
  Return: 0 on success; -errno on failure
	
	  Use an unbound workqueue to allow bios to be decrypted in parallel
	  even when they happen to complete on the same CPU.  This sacrifices
	  locality, but it's worthwhile since decryption is CPU-intensive.
	 
	  Also use a high-priority workqueue to prioritize decryption work,
	  which blocks reads from completing, over regular application tasks.
 SPDX-License-Identifier: GPL-2.0
  Implementation of HKDF ("HMAC-based Extract-and-Expand Key Derivation
  Function"), aka RFC 5869.  See also the original paper (Krawczyk 2010):
  "Cryptographic Extraction and Key Derivation: The HKDF Scheme".
  This is used to derive keys from the fscrypt master keys.
  Copyright 2019 Google LLC
  HKDF supports any unkeyed cryptographic hash algorithm, but fscrypt uses
  SHA-512 because it is well-established, secure, and reasonably efficient.
  HKDF-SHA256 was also considered, as its 256-bit security strength would be
  sufficient here.  A 512-bit security strength is "nice to have", though.
  Also, on 64-bit CPUs, SHA-512 is usually just as fast as SHA-256.  In the
  common case of deriving an AES-256-XTS key (512 bits), that can result in
  HKDF-SHA512 being much faster than HKDF-SHA256, as the longer digest size of
  SHA-512 causes HKDF-Expand to only need to do one iteration rather than two.
  HKDF consists of two steps:
  1. HKDF-Extract: extract a pseudorandom key of length HKDF_HASHLEN bytes from
     the input keying material and optional salt.
  2. HKDF-Expand: expand the pseudorandom key into output keying material of
     any length, parameterized by an application-specific info string.
  HKDF-Extract can be skipped if the input is already a pseudorandom key of
  length HKDF_HASHLEN bytes.  However, cipher modes other than AES-256-XTS take
  shorter keys, and we don't want to force users of those modes to provide
  unnecessarily long master keys.  Thus fscrypt still does HKDF-Extract.  No
  salt is used, since fscrypt master keys should already be pseudorandom and
  there's no way to persist a random salt per master key from kernel mode.
 HKDF-Extract (RFC 5869 section 2.2), unsalted 
  Compute HKDF-Extract using the given master key as the input keying material,
  and prepare an HMAC transform object keyed by the resulting pseudorandom key.
  Afterwards, the keyed HMAC transform object can be used for HKDF-Expand many
  times without having to recompute HKDF-Extract each time.
  HKDF-Expand (RFC 5869 section 2.3).  This expands the pseudorandom key, which
  was already keyed into 'hkdf->hmac_tfm' by fscrypt_init_hkdf(), into 'okmlen'
  bytes of output keying material parameterized by the application-specific
  'info' of length 'infolen' bytes, prefixed by "fscrypt\0" and the 'context'
  byte.  This is thread-safe and may be called by multiple threads in parallel.
  ('context' isn't part of the HKDF specification; it's just a prefix fscrypt
  adds to its application-specific info strings to guarantee that it doesn't
  accidentally repeat an info string when using HKDF for different purposes.)
 so caller doesn't need to 
 SPDX-License-Identifier: GPL-2.0
  Inline encryption support for fscrypt
  Copyright 2019 Google LLC
  With "inline encryption", the block layer handles the decryptionencryption
  as part of the bio, instead of the filesystem doing the crypto itself via
  crypto API.  See Documentationblockinline-encryption.rst.  fscrypt still
  provides the key and IV to use.
 Default case: IVs are just the file logical block number 
 Enable inline encryption for this file if supported. 
 The file must need contents encryption, not filenames encryption 
 The crypto mode must have a blk-crypto counterpart 
 The filesystem must be mounted with -o inlinecrypt 
	
	  When a page contains multiple logically contiguous filesystem blocks,
	  some filesystem code only calls fscrypt_mergeable_bio() for the first
	  block in the page. This is fine for most of fscrypt's IV generation
	  strategies, where contiguous blocks imply contiguous IVs. But it
	  doesn't work with IV_INO_LBLK_32. For now, simply exclude
	  IV_INO_LBLK_32 with blocksize != PAGE_SIZE from inline encryption.
	
	  On all the filesystem's devices, blk-crypto must support the crypto
	  configuration that the file would use.
	
	  We have to start using blk-crypto on all the filesystem's devices.
	  We also have to save all the request_queue's for later so that the
	  key can be evicted from them.  This is needed because some keys
	  aren't destroyed until after the filesystem was already unmounted
	  (namely, the per-mode keys in struct fscrypt_master_key).
	
	  Pairs with the smp_load_acquire() in fscrypt_is_key_prepared().
	  I.e., here we publish ->blk_key with a RELEASE barrier so that
	  concurrent tasks can ACQUIRE it.  Note that this concurrency is only
	  possible for per-mode keys, not for per-file keys.
  fscrypt_set_bio_crypt_ctx() - prepare a file contents bio for inline crypto
  @bio: a bio which will eventually be submitted to the file
  @inode: the file's inode
  @first_lblk: the first file logical block number in the IO
  @gfp_mask: memory allocation flags - these must be a waiting mask so that
 					bio_crypt_set_ctx can't fail.
  If the contents of the file should be encrypted (or decrypted) with inline
  encryption, then assign the appropriate encryption context to the bio.
  Normally the bio should be newly allocated (i.e. no pages added yet), as
  otherwise fscrypt_mergeable_bio() won't work as intended.
  The encryption context will be freed automatically when the bio is freed.
 Extract the inode and logical block number from a buffer_head. 
	
	  The ext4 journal (jbd2) can submit a buffer_head it directly created
	  for a non-pagecache page.  fscrypt doesn't care about these.
  fscrypt_set_bio_crypt_ctx_bh() - prepare a file contents bio for inline
 				    crypto
  @bio: a bio which will eventually be submitted to the file
  @first_bh: the first buffer_head for which IO will be submitted
  @gfp_mask: memory allocation flags
  Same as fscrypt_set_bio_crypt_ctx(), except this takes a buffer_head instead
  of an inode and block number directly.
  fscrypt_mergeable_bio() - test whether data can be added to a bio
  @bio: the bio being built up
  @inode: the inode for the next part of the IO
  @next_lblk: the next file logical block number in the IO
  When building a bio which may contain data which should undergo inline
  encryption (or decryption) via fscrypt, filesystems should call this function
  to ensure that the resulting bio contains only contiguous data unit numbers.
  This will return false if the next part of the IO cannot be merged with the
  bio because either the encryption key would be different or the encryption
  data unit numbers would be discontiguous.
  fscrypt_set_bio_crypt_ctx() must have already been called on the bio.
  Return: true iff the IO is mergeable
	
	  Comparing the key pointers is good enough, as all IO for each key
	  uses the same pointer.  I.e., there's currently no need to support
	  merging requests where the keys are the same but the pointers differ.
  fscrypt_mergeable_bio_bh() - test whether data can be added to a bio
  @bio: the bio being built up
  @next_bh: the next buffer_head for which IO will be submitted
  Same as fscrypt_mergeable_bio(), except this takes a buffer_head instead of
  an inode and block number directly.
  Return: true iff the IO is mergeable
 SPDX-License-Identifier: GPL-2.0
  Encryption policy functions for per-file encryption support.
  Copyright (C) 2015, Google, Inc.
  Copyright (C) 2015, Motorola Mobility.
  Originally written by Michael Halcrow, 2015.
  Modified by Jaegeuk Kim, 2015.
  Modified by Eric Biggers, 2019 for v2 policy support.
  fscrypt_policies_equal() - check whether two encryption policies are the same
  @policy1: the first policy
  @policy2: the second policy
  Return: %true if equal, else %false
	
	  IV_INO_LBLK_ exist only because of hardware limitations, and
	  currently the only known use case for them involves AES-256-XTS.
	  That's also all we test currently.  For these reasons, for now only
	  allow AES-256-XTS here.  This can be relaxed later if a use case for
	  IV_INO_LBLK_ with other encryption modes arises.
	
	  It's unsafe to include inode numbers in the IVs if the filesystem can
	  potentially renumber inodes, e.g. via filesystem shrinking.
 With v1, there's no way to derive dirhash keys. 
	
	  IV_INO_LBLK_32 hashes the inode number, so in principle it can
	  support any ino_bits.  However, currently the inode number is gotten
	  from inode::i_ino which is 'unsigned long'.  So for now the
	  implementation limit is 32 bits.
  fscrypt_supported_policy() - check whether an encryption policy is supported
  @policy_u: the encryption policy
  @inode: the inode on which the policy will be used
  Given an encryption policy, check whether all its encryption modes and other
  settings are supported by this kernel on the given inode.  (But we don't
  currently don't check for crypto API support here, so attempting to use an
  algorithm not configured into the crypto API will still fail later.)
  Return: %true if supported, else %false
  fscrypt_new_context() - create a new fscrypt_context
  @ctx_u: output context
  @policy_u: input policy
  @nonce: nonce to use
  Create an fscrypt_context for an inode that is being assigned the given
  encryption policy.  @nonce must be a new random nonce.
  Return: the size of the new context in bytes.
  fscrypt_policy_from_context() - convert an fscrypt_context to
 				   an fscrypt_policy
  @policy_u: output policy
  @ctx_u: input context
  @ctx_size: size of input context in bytes
  Given an fscrypt_context, build the corresponding fscrypt_policy.
  Return: 0 on success, or -EINVAL if the fscrypt_context has an unrecognized
  version number or size.
  This does not validate the settings within the policy itself, e.g. the
  modes, flags, and reserved bits.  Use fscrypt_supported_policy() for that.
 unreachable 
 Retrieve an inode's encryption policy 
 key available, use the cached policy 
		
		  The original encryption policy version provided no way of
		  verifying that the correct master key was supplied, which was
		  insecure in scenarios where multiple users have access to the
		  same encrypted files (even just read-only access).  The new
		  encryption policy version fixes this and also implies use of
		  an improved key derivation function and allows non-root users
		  to securely remove keys.  So as long as compatibility with
		  old kernels isn't required, it is recommended to use the new
		  policy version for all new encrypted directories.
	
	  We should just copy the remaining 'size - 1' bytes here, but a
	  bizarre bug in gcc 7 and earlier (fixed by gcc r255731) causes gcc to
	  think that size can be 0 here (despite the check above!) and that
	  it's a compile-time constant.  Thus it would think copy_from_user()
	  is passed compile-time constant ULONG_MAX, causing the compile-time
	  buffer overflow check to fail, breaking the build. This only occurred
	  when building an i386 kernel with -Os and branch profiling enabled.
	 
	  Work around it by just copying the first byte again...
 The file already uses a different encryption policy. 
 Original ioctl version; can only get the original policy version 
 Extended ioctl version; can get policies of any version 
 arg is policy_size, then policy 
 FS_IOC_GET_ENCRYPTION_NONCE: retrieve file's encryption nonce for testing 
  fscrypt_has_permitted_context() - is a file's encryption policy permitted
 				     within its directory?
  @parent: inode for parent directory
  @child: inode for file being looked up, opened, or linked into @parent
  Filesystems must call this before permitting access to an inode in a
  situation where the parent directory is encrypted (either before allowing
  ->lookup() to succeed, or for a regular file before allowing it to be opened)
  and before any operation that involves linking an inode into an encrypted
  directory, including link, rename, and cross rename.  It enforces the
  constraint that within a given encrypted directory tree, all files use the
  same encryption policy.  The pre-access check is needed to detect potentially
  malicious offline violations of this constraint, while the link and rename
  checks are needed to prevent online violations of this constraint.
  Return: 1 if permitted, 0 if forbidden.
 No restrictions on file types which are never encrypted 
 No restrictions if the parent directory is unencrypted 
 Encrypted directories must not contain unencrypted files 
	
	  Both parent and child are encrypted, so verify they use the same
	  encryption policy.  Compare the fscrypt_info structs if the keys are
	  available, otherwise retrieve and compare the fscrypt_contexts.
	 
	  Note that the fscrypt_context retrieval will be required frequently
	  when accessing an encrypted directory tree without the key.
	  Performance-wise this is not a big deal because we already don't
	  really optimize for file access without the key (to the extent that
	  such access is even possible), given that any attempted access
	  already causes a fscrypt_context retrieval and keyring search.
	 
	  In any case, if an unexpected error occurs, fall back to "forbidden".
	
	  Allow the case where the parent and child both have an unrecognized
	  encryption policy, so that files with an unrecognized encryption
	  policy can be deleted.
  Return the encryption policy that new files in the directory will inherit, or
  NULL if none, or an ERR_PTR() on error.  If the directory is encrypted, also
  ensure that its key is set up, so that the new filename can be encrypted.
  fscrypt_set_context() - Set the fscrypt context of a new inode
  @inode: a new inode
  @fs_data: private data given by FS and passed to ->set_context()
  This should be called after fscrypt_prepare_new_inode(), generally during a
  filesystem transaction.  Everything here must be %GFP_NOFS-safe.
  Return: 0 on success, -errno on failure
 fscrypt_prepare_new_inode() should have set up the key already. 
	
	  This may be the first time the inode number is available, so do any
	  delayed key setup that requires the inode number.
  fscrypt_set_test_dummy_encryption() - handle '-o test_dummy_encryption'
  @sb: the filesystem on which test_dummy_encryption is being specified
  @arg: the argument to the test_dummy_encryption option.  May be NULL.
  @dummy_policy: the filesystem's current dummy policy (inputoutput, see
 		  below)
  Handle the test_dummy_encryption mount option by creating a dummy encryption
  policy, saving it in @dummy_policy, and adding the corresponding dummy
  encryption key to the filesystem.  If the @dummy_policy is already set, then
  instead validate that it matches @arg.  Don't support changing it via
  remount, as that is difficult to do safely.
  Return: 0 on success (dummy policy set, or the same policy is already set);
          -EEXIST if a different dummy policy is already set;
          or another -errno value.
 key_spec.u.identifier gets filled in when adding the key 
  fscrypt_show_test_dummy_encryption() - show '-o test_dummy_encryption'
  @seq: the seq_file to print the option to
  @sep: the separator character to use
  @sb: the filesystem whose options are being shown
  Show the test_dummy_encryption mount option, if it was specified.
  This is mainly used for procmounts.
 Handle numbering quirk 
 SPDX-License-Identifier: GPL-2.0
  Key setup facility for FS encryption support.
  Copyright (C) 2015, Google, Inc.
  Originally written by Michael Halcrow, Ildar Muslukhov, and Uday Savagaonkar.
  Heavily modified since then.
 Create a symmetric cipher object for the given encryption mode and key 
		
		  fscrypt performance can vary greatly depending on which
		  crypto algorithm implementation is used.  Help people debug
		  performance problems by logging the ->cra_driver_name the
		  first time a mode is used.
  Prepare the crypto transform object or blk-crypto key in @prep_key, given the
  raw key, encryption mode (@ci->ci_mode), flag indicating which encryption
  implementation (fs-layer or blk-crypto) will be used (@ci->ci_inlinecrypt),
  and IV generation method (@ci->ci_policy.flags).
	
	  Pairs with the smp_load_acquire() in fscrypt_is_key_prepared().
	  I.e., here we publish ->tfm with a RELEASE barrier so that
	  concurrent tasks can ACQUIRE it.  Note that this concurrency is only
	  possible for per-mode keys, not for per-file keys.
 Destroy a crypto transform object andor blk-crypto key. 
 Given a per-file encryption key, set up the file's crypto transform object 
  Derive a SipHash key from the given fscrypt master key and the given
  application-specific information string.
  Note that the KDF produces a byte array, but the SipHash APIs expect the key
  as a pair of 64-bit words.  Therefore, on big endian CPUs we have to do an
  endianness swap in order to get the same results as on little endian CPUs.
 pairs with smp_store_release() below 
 pairs with smp_load_acquire() above 
	
	  New inodes may not have an inode number assigned yet.
	  Hashing their inode number is delayed until later.
		
		  DIRECT_KEY: instead of deriving per-file encryption keys, the
		  per-file nonce will be included in all the IVs.  But unlike
		  v1 policies, for v2 policies in this case we don't encrypt
		  with the master key directly but rather derive a per-mode
		  encryption key.  This ensures that the master key is
		  consistently used only for HKDF, avoiding key reuse issues.
		
		  IV_INO_LBLK_64: encryption keys are derived from (master_key,
		  mode_num, filesystem_uuid), and inode number is included in
		  the IVs.  This format is optimized for use with inline
		  encryption hardware compliant with the UFS standard.
 Derive a secret dirhash key for directories that need it. 
  Check whether the size of the given master key (@mk) is appropriate for the
  encryption settings which a particular file will use (@ci).
  If the file uses a v1 encryption policy, then the master key must be at least
  as long as the derived key, as this is a requirement of the v1 KDF.
  Otherwise, the KDF can accept any size key, so we enforce a slightly looser
  requirement: we require that the size of the master key be at least the
  maximum security strength of any algorithm whose key will be derived from it
  (but in practice we only need to consider @ci->ci_mode, since any other
  possible subkeys such as DIRHASH and INODE_HASH will never increase the
  required key size over @ci->ci_mode).  This allows AES-256-XTS keys to be
  derived from a 256-bit master key, which is cryptographically sufficient,
  rather than requiring a 512-bit master key which is unnecessarily long.  (We
  still allow 512-bit master keys if the user chooses to use them, though.)
  Find the master key, then set up the inode's actual encryption key.
  If the master key is found in the filesystem-level keyring, then the
  corresponding 'struct key' is returned in master_key_ret with its semaphore
  read-locked.  This is needed to ensure that only one task links the
  fscrypt_info into ->mk_decrypted_inodes (as multiple tasks may race to create
  an fscrypt_info for the same inode), and to synchronize the master key being
  removed with a new inode starting to use it.
		
		  As a legacy fallback for v1 policies, search for the key in
		  the current task's subscribed keyrings too.  Don't move this
		  to before the search of ->s_master_keys, since users
		  shouldn't be able to override filesystem-level keys.
 Has the secret been removed (via FS_IOC_REMOVE_ENCRYPTION_KEY)? 
		
		  Remove this inode from the list of inodes that were unlocked
		  with the master key.
		 
		  In addition, if we're removing the last inode from a key that
		  already had its secret removed, invalidate the key so that it
		  gets removed from ->s_master_keys.
	
	  For existing inodes, multiple tasks may race to set ->i_crypt_info.
	  So use cmpxchg_release().  This pairs with the smp_load_acquire() in
	  fscrypt_get_info().  I.e., here we publish ->i_crypt_info with a
	  RELEASE barrier so that other tasks can ACQUIRE it.
		
		  We won the race and set ->i_crypt_info to our crypt_info.
		  Now link it into the master key's inode list.
  fscrypt_get_encryption_info() - set up an inode's encryption key
  @inode: the inode to set up the key for.  Must be encrypted.
  @allow_unsupported: if %true, treat an unsupported encryption policy (or
 		       unrecognized encryption context) the same way as the key
 		       being unavailable, instead of returning an error.  Use
 		       %false unless the operation being performed is needed in
 		       order for files (or directories) to be deleted.
  Set up ->i_crypt_info, if it hasn't already been done.
  Note: unless ->i_crypt_info is already set, this isn't %GFP_NOFS-safe.  So
  generally this shouldn't be called from within a filesystem transaction.
  Return: 0 if ->i_crypt_info was set or was already set, or if the
 	   encryption key is unavailable.  (Use fscrypt_has_encryption_key() to
 	   distinguish these cases.)  Also can return another -errno code.
 Algorithm unavailable? 
  fscrypt_prepare_new_inode() - prepare to create a new inode in a directory
  @dir: a possibly-encrypted directory
  @inode: the new inode.  ->i_mode must be set already.
 	   ->i_ino doesn't need to be set yet.
  @encrypt_ret: (output) set to %true if the new inode will be encrypted
  If the directory is encrypted, set up its ->i_crypt_info in preparation for
  encrypting the name of the new file.  Also, if the new inode will be
  encrypted, set up its ->i_crypt_info and set encrypt_ret=true.
  This isn't %GFP_NOFS-safe, and therefore it should be called before starting
  any filesystem transaction to create the inode.  For this reason, ->i_ino
  isn't required to be set yet, as the filesystem may not have set it yet.
  This doesn't persist the new inode's encryption context.  That still needs to
  be done later by calling fscrypt_set_context().
  Return: 0 on success, -ENOKEY if the encryption key is missing, or another
 	   -errno code
	
	  Only regular files, directories, and symlinks are encrypted.
	  Special files like device nodes and named pipes aren't.
  fscrypt_put_encryption_info() - free most of an inode's fscrypt data
  @inode: an inode being evicted
  Free the inode's fscrypt_info.  Filesystems must call this when the inode is
  being evicted.  An RCU grace period need not have elapsed yet.
  fscrypt_free_inode() - free an inode's fscrypt data requiring RCU delay
  @inode: an inode being freed
  Free the inode's cached decrypted symlink target, if any.  Filesystems must
  call this after an RCU grace period, just before they free the inode.
  fscrypt_drop_inode() - check whether the inode's master key has been removed
  @inode: an inode being considered for eviction
  Filesystems supporting fscrypt must call this from their ->drop_inode()
  method so that encrypted inodes are evicted as soon as they're no longer in
  use and their master key has been removed.
  Return: 1 if fscrypt wants the inode to be evicted now, otherwise 0
	
	  If ci is NULL, then the inode doesn't have an encryption key set up
	  so it's irrelevant.  If ci_master_key is NULL, then the master key
	  was provided via the legacy mechanism of the process-subscribed
	  keyrings, so we don't know whether it's been removed or not.
	
	  With proper, non-racy use of FS_IOC_REMOVE_ENCRYPTION_KEY, all inodes
	  protected by the key were cleaned by sync_filesystem().  But if
	  userspace is still using the files, inodes can be dirtied between
	  then and now.  We mustn't lose any writes, so skip dirty inodes here.
	
	  Note: since we aren't holding the key semaphore, the result here can
	  immediately become outdated.  But there's no correctness problem with
	  unnecessarily evicting.  Nor is there a correctness problem with not
	  evicting while iput() is racing with the key being removed, since
	  then the thread removing the key will either evict the inode itself
	  or will correctly detect that it wasn't evicted due to the race.
 SPDX-License-Identifier: GPL-2.0
  Utility functions for file contents encryptiondecryption on
  block device-based filesystems.
  Copyright (C) 2015, Google, Inc.
  Copyright (C) 2015, Motorola Mobility
  fscrypt_decrypt_bio() - decrypt the contents of a bio
  @bio: the bio to decrypt
  Decrypt the contents of a "read" bio following successful completion of the
  underlying disk read.  The bio must be reading a whole number of blocks of an
  encrypted file directly into the page cache.  If the bio is reading the
  ciphertext into bounce pages instead of the page cache (for example, because
  the file is also compressed, so decompression is required after decryption),
  then this function isn't applicable.  This function may sleep, so it must be
  called from a workqueue rather than from the bio's bi_end_io callback.
  This function sets PG_error on any pages that contain any blocks that failed
  to be decrypted.  The filesystem must not mark such pages uptodate.
 This always succeeds since __GFP_DIRECT_RECLAIM is set. 
  fscrypt_zeroout_range() - zero out a range of blocks in an encrypted file
  @inode: the file's inode
  @lblk: the first file logical block to zero out
  @pblk: the first filesystem physical block to zero out
  @len: number of blocks to zero out
  Zero out filesystem blocks in an encrypted regular file on-disk, i.e. write
  ciphertext blocks which decrypt to the all-zeroes block.  The blocks must be
  both logically and physically contiguous.  It's also assumed that the
  filesystem only uses a single block device, ->s_bdev.
  Note that since each block uses a different IV, this involves writing a
  different ciphertext to each block; we can't simply reuse the same one.
  Return: 0 on success; -errno on failure.
 write up to 16 pages at a time 
	
	  We need at least one page for ciphertext.  Allocate the first one
	  from a mempool, with __GFP_DIRECT_RECLAIM set so that it can't fail.
	 
	  Any additional page allocations are allowed to fail, as they only
	  help performance, and waiting on the mempool for them could deadlock.
 This always succeeds since __GFP_DIRECT_RECLAIM is set. 
 SPDX-License-Identifier: GPL-2.0
  Key setup for v1 encryption policies
  Copyright 2015, 2019 Google LLC
  This file implements compatibility functions for the original encryption
  policy version ("v1"), including:
  - Deriving per-file encryption keys using the AES-128-ECB based KDF
    (rather than the new method of using HKDF-SHA512)
  - Retrieving fscrypt master keys from process-subscribed keyrings
    (rather than the new method of using a filesystem-level keyring)
  - Handling policies with the DIRECT_KEY flag set using a master key table
    (rather than the new method of implementing DIRECT_KEY with per-mode keys
     managed alongside the master keys in the filesystem-level keyring)
 Table of keys referenced by DIRECT_KEY policies 
 6 bits = 64 buckets 
  v1 key derivation function.  This generates the derived key by encrypting the
  master key with AES-128-ECB using the nonce as the AES key.  This provides a
  unique derived key with sufficient entropy for each inode.  However, it's
  nonstandard, non-extensible, doesn't evenly distribute the entropy from the
  master key, and is trivially reversible: an attacker who compromises a
  derived key can "decrypt" it to get back to the master key, then derive any
  other key.  For all new code, use HKDF instead.
  The master key must be at least as long as the derived key.  If the master
  key is longer, then only the first 'derived_keysize' bytes are used.
  Search the current task's subscribed keyrings for a "logon" key with
  description prefix:descriptor, and if found acquire a read lock on it and
  return a pointer to its validated payload in payload_ret.
 was the key revoked before we acquired its semaphore? 
 Master key referenced by DIRECT_KEY policy 
  Findinsert the given key into the fscrypt_direct_keys table.  If found, it
  is returned with elevated refcount, and 'to_insert' is freed if non-NULL.  If
  not found, 'to_insert' is inserted and returned if it's non-NULL; otherwise
  NULL is returned.
	
	  Careful: to avoid potentially leaking secret key bytes via timing
	  information, we must key the hash table by descriptor rather than by
	  raw key, and use crypto_memneq() when comparing raw keys.
 using existing tfm with same (descriptor, mode, raw_key) 
 Prepare to encrypt directly using the master key in the given mode 
 Is there already a tfm for this key? 
 Nope, allocate one. 
 v1 policy, DIRECT_KEY: use the master key directly 
 v1 policy, !DIRECT_KEY: derive the file's encryption key 
	
	  This cannot be a stack buffer because it will be passed to the
	  scatterlist crypto API during derive_key_aes().
 SPDX-License-Identifier: GPL-2.0
  fsf2fsacl.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
  Portions of this code from linuxfsext2acl.c
  Copyright (C) 2001-2003 Andreas Gruenbacher, <agruen@suse.de>
  Most part of f2fs_acl_clone, f2fs_acl_create_masq, f2fs_acl_create
  are copied from posix_acl.c
 assert(atomic_read(acl->a_refcount) == 1); 
 SPDX-License-Identifier: GPL-2.0
  fsf2fsnamei.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
 Otherwise, will be 0 
 Inherit the compression flag in directory 
	
	  filename format of multimedia file should be defined as:
	  "filename + '.' + extension + (optional: '.' + temp extension)".
 file has no temp extension 
  Set file's temperature for hotcold data separation
		 Eventually we want to call d_add_ci(dentry, NULL)
		  for negative dentries in the encoding case as
		  well.  For now, prevent the negative dentry
		  from being cached.
	 VFS negative dentries are incompatible with Encoding and
	  Case-insensitiveness. Eventually we'll want avoid
	  invalidating the dentries here, alongside with returning the
	  negative dentries at f2fs_lookup(), when it is better
	  supported by the VFS for the CI case.
 this is broken symlink case 
	
	  Let's flush symlink data in order to avoid broken symlink as much as
	  possible. Nevertheless, fsyncing is the best way, but there is no
	  way to get a file descriptor in order to flush that.
	 
	  Note that, it needs to do dir->fsync to make this recoverable.
	  If the symlink path is stored into inline_data, there is no
	  performance regression.
	
	  add this non-linked tmpfile to orphan list, in this way we could
	  remove all unused data of tmpfile after abnormal power-off.
 link_count was changed by d_tmpfile as well. 
	
	  If new_inode is null, the below renaming flow will
	  add a link in old_dir which can conver inline_dir.
	  After then, if we failed to get the entry due to other
	  reasons like ENOMEM, we had to remove the new entry.
	  Instead of adding such the error handling routine, let's
	  simply convert first here.
 adjust dir's i_pino to pass fsck check 
 prepare for updating ".." directory entry info later 
	
	  If cross rename between file and directory those are not
	  in the same directory, we will inc nlink of file's parent
	  later, so we should check upper boundary of its nlink.
 update ".." directory entry info of old dentry 
 update ".." directory entry info of new dentry 
 update directory entry info of old dir inode 
 adjust dir's i_pino to pass fsck check 
 update directory entry info of new dir inode 
 adjust dir's i_pino to pass fsck check 
	
	  VFS has already handled the new dentry existence case,
	  here, we just deal with "RENAME_NOREPLACE" as regular rename.
 SPDX-License-Identifier: GPL-2.0
  f2fs extent cache support
  Copyright (c) 2015 Motorola Mobility
  Copyright (c) 2015 Samsung Electronics
  Authors: Jaegeuk Kim <jaegeuk@kernel.org>
           Chao Yu <chao2.yu@samsung.com>
  lookup rb entry in position of @ofs in rb-tree,
  if hit, return the entry, otherwise, return NULL
  @prev_ex: extent before ofs
  @next_ex: extent after ofs
  @insert_p: insert point for new extent at ofs
  in order to simpfy the insertion after.
  tree must stay unchanged between lookup and insertion.
 lookup prev node for merging backward later 
 lookup next node for merging frontward later 
  Flow to release an extent_node:
  1. list_del_init
  2. __detach_extent_node
  3. kmem_cache_free.
 never died until evict_inode 
 return true, if inode page is changed 
 drop largest extent 
 update in global extent list 
	
	  drop largest extent before lookup, in case it's already
	  been shrunk from extent tree
 1. lookup first extent node in range [fofs, fofs + len - 1] 
 2. invlidate all extent nodes in range [fofs, fofs + len - 1] 
 # of parts current extent split into 
		
		  if original extent is split into zero or two parts, extent
		  tree has been altered by deletion or insertion, therefore
		  invalidate pointers regard to tree.
 3. update extent in extent cache 
 give up extent_cache, if split and small updates happen 
 it is safe here to check FI_NO_EXTENT wo et->lock in ro image 
 1. remove unreferenced extent tree 
 2. remove LRU extent entries 
 refresh this extent node's position in extent list 
 free all extent info belong to this extent tree 
 delete extent tree entry in radix tree 
 SPDX-License-Identifier: GPL-2.0
  fsf2fsdata.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
 postprocessing steps for read bios 
 compile out the decryption-related code 
 compile out the decompression-related code 
 compile out the verity-related code 
	
	  Update and unlock the bio's pagecache pages, and put the
	  decompression context for any compressed pages.
 PG_error was set if decryption or verity failed. 
 will re-read again later 
	
	  fsverity_verify_bio() may call readpages() again, and while verity
	  will be disabled for this, decryption andor decompression may still
	  be needed, resulting in another bio_post_read_ctx being allocated.
	  So to prevent deadlocks we need to release the current ctx to the
	  mempool first.  This assumes that verity is the last post-read step.
	
	  Verify the bio's pages with fs-verity.  Exclude compressed pages,
	  as those were handled separately by f2fs_end_read_compressed_page().
  If the bio's data needs to be verified with fs-verity, then enqueue the
  verity work for the bio.  Otherwise finish the bio now.
  Note that to avoid deadlocks, the verity work can't be done on the
  decryptiondecompression workqueue.  This is because verifying the data pages
  can involve reading verity metadata pages from the file, and these verity
  metadata pages may be encrypted andor compressed.
  Handle STEP_DECOMPRESS by decompressing any compressed clusters whose last
  remaining page was read by @ctx->bio.
  Note that a bio may span clusters (even a mix of compressed and uncompressed
  clusters) or be for just part of a cluster.  STEP_DECOMPRESS just indicates
  that the bio includes at least one compressed page.  The actual decompression
  is done on a per-cluster basis, not a per-bio basis.
 PG_error was set if decryption failed. 
	
	  Optimization: if all the bio's pages are compressed, then scheduling
	  the per-bio verity work is unnecessary, as verity will be fully
	  handled at the compression cluster level.
	
	  The f2fs garbage collector sets ->encrypted_page when it wants to
	  readwrite raw data without encryption.
	
	  The f2fs garbage collector sets ->encrypted_page when it wants to
	  readwrite raw data without encryption.
 fill dummy pages 
		
		  In the NODE case, we lose next block address chain. So, we
		  need to do checkpoint in f2fs_sync_file.
	
	  datanode io flag bits per temp:
	       REQ_META     |      REQ_FUA      |
	     5 |    4 |   3 |    2 |    1 |   0 |
	  Cold | Warm | Hot | Cold | Warm | Hot |
 change META to META_FLUSH in the checkpoint procedure 
 TODO: use HOT temp only for meta pages now. 
  Fill the locked page with data located in the block address.
  A caller needs to unlock the page on failure.
 Allocate a new bio 
 IOs in bio is aligned and left space of vectors is not enough 
 page can't be merged into bio; submit the bio 
 set submitted = true as a return value 
	
	  STEP_DECOMPRESS is handled specially, since a compressed file might
	  contain both compressed and uncompressed clusters.  We'll allocate a
	  bio_post_read_ctx if the file is compressed, but the caller is
	  responsible for enabling STEP_DECOMPRESS if it's actually needed.
 Due to the mempool, this never fails. 
 This can handle encryption stuffs 
 wait for GCed page writeback via META_MAPPING 
 Get physical address of data block 
  Lock ordering for the change of data block address:
  ->data_page
   ->node_page
     update block addresses in the node page
 dn->ofs_in_node will be returned with up-to-date last block pointer 
 Should keep dn->ofs_in_node unchanged 
	
	  A new dentry page is allocated but not able to be written, since its
	  new inode page couldn't be allocated due to -ENOSPC.
	  In such the case, its blkaddr can be remained as NEW_ADDR.
	  see, f2fs_add_link -> f2fs_get_new_data_page ->
	  f2fs_init_inode_metadata.
  If it tries to access a hole, return an error.
  Because, the callers, functions in dir.c and GC, should be able to know
  whether this page exists or not.
 wait for read completion 
  Caller ensures that this data page is never allocated.
  A new zero-filled data page is allocated in the page cache.
  Also, caller should grab and release a rwsem by calling f2fs_lock_op() and
  f2fs_unlock_op().
  Note that, ipage is set only by make_empty_dir, and if any error occur,
  ipage should be released by this function.
		
		  before exiting, we should make sure ipage will be released
		  if any error occur.
 if ipage exists, blkaddr should be NEW_ADDR 
	
	  i_size will be updated by direct_IO. Otherwise, we'll get stale
	  data from unwritten block via dio_read.
  f2fs_map_blocks() tries to find or build mapping relationship which
  maps continuous logical blocks to physical blocks, and return such
  info via f2fs_map_blocks structure.
 it only supports block size == page size 
 for hardware encryption, but to avoid potential issue in future 
 When reading holes, we need its node page 
			
			  There is one exceptional case that read_node_page()
			  may return -ENOENT due to filesystem has been
			  shutdown or cp_error, so force to convert error
			  number to EIO for such case.
 use out-place-update for driect IO under LFS mode 
 for defragment case 
 preallocated unwritten block should be mapped for fiemap. 
 preallocate blocks in batch for one dnode page 
		
		  for hardware encryption, but to avoid potential issue
		  in future
 two direct node blocks 
 two indirect node blocks 
 one double indirect node block 
 HOLE 
 In a case of compressed cluster, append this to the last extent 
 just zeroing out page which is beyond EOF 
	
	  Map blocks using the previous result first.
	
	  Then do more f2fs_map_blocks() calls until we are
	  done with this page.
	
	  This page will go to BIO.  Do we need to send this
	  BIO off first?
	
	  If the page is under writeback, we need to wait for
	  its completion to see the correct decrypted data.
 get rid of pages beyond EOF 
 we are done since all pages are beyond EOF 
 nothing to decompress 
  This function was originally taken from fsmpage.c, and customized for f2fs.
  Major change was from block_size == page_size in f2fs by default.
 there are remained comressed pages, submit them 
 last page 
 If the file has inline data, try to read it directly 
 If the file has inline data, skip readpages 
 wait for GCed page writeback via META_MAPPING 
 flush pending IOs and wait for a while in the ENOMEM case 
	
	  IPU for rewrite async pages
 this is only set during fdatasync 
 swap file is migrating in aligned write mode 
 if this is cold file, we should overwrite to avoid fragmentation 
 swap file is migrating in aligned write mode 
 Deadlock due to between page->lock and f2fs_lock_op 
 This page is already truncated 
	
	  If current allocation needs SSR,
	  it had better in-place writes for updated data.
 LFS mode write path 
 we should bypass data pages to proceed the kworkder jobs 
		
		  don't drop any dirty dentry pages for keeping lastest
		  directory structure.
	
	  If the offset is out-of-range of file size,
	  this page does not have to be written to disk.
 we should not write 0'th page having journal header 
 Dentryquota blocks are controlled by checkpoint 
		
		  We need to wait for node_write to avoid block allocation during
		  checkpoint. This can only happen to quota writes which can cause
		  the below discard race condition.
	
	  pageout() in MM traslates EAGAIN, so calls handle_write_error()
	  -> mapping_set_error() -> set_bit(AS_EIO, ...).
	  file_write_and_wait_range() will see EIO error, which is critical
	  to return value of fsync() followed by atomic_write failure to user.
  This function was copied from write_cche_pages from mmpage-writeback.c.
  The major change is making write step of cold data page separately from
  warmhot data page.
 Inclusive 
 prev offset 
 give a priority to WB_SYNC threads 
 someone wrote it for us 
				
				  keep nr_to_write, since vfs uses this to
				  get # of written pages.
 flush remained pages in compress cluster 
 submit cached bio of IPU write 
 to avoid deadlock in path of data flush 
 deal with chardevs and other special file 
 skip writing if there is no dirty page in this inode 
 during POR, we don't need to trigger writepage at all. 
 skip writing during file defragment 
 to avoid spliting IOs due to mixed WB_SYNC_ALL and WB_SYNC_NONE 
	
	  if some pages were truncated, we cannot guarantee its mapping->host
	  to detect pending bios.
 In the fs-verity case, f2fs_end_enable_verity() does the truncate 
	
	  we already allocated all the blocks, so we don't need to get
	  the block addresses when there is no need to fill the page.
 f2fs_lock_op avoids race between write CP and convert_inline_page 
 check inline_data 
 hole case 
 convert_inline_page can make node_changed 
	
	  We should check this at this moment to avoid deadlock on inode page
	  and #0 page. The locking rule for inline_data conversion should be:
	  lock_page(page #0) -> lock_page(inode_page)
	
	  Do not use grab_cache_page_write_begin() to avoid deadlock due to
	  wait_for_stable_page. Will wait that below with our IO control.
 TODO: cluster can be compressed due to race with .writepage 
 The page got truncated from under us 
	
	  This should be come from len == PAGE_SIZE, and we expect copied
	  should be PAGE_SIZE. Otherwise, we treat it with zero copied and
	  let generic_perform_write() try to copy data again through copied=0.
 overwrite compressed file 
 If this is dirty page, keep PagePrivate 
 This is atomic written page, keep Private 
		
		  Previously, this page has been registered, we just
		  return here.
 make sure allocating whole blocks 
 Block number less than F2FS MAX BLOCKS 
 migrating an atomic written page is safe with the inmem_lock hold 
 one extra reference was held for atomic_write page 
 guarantee to start from no stale private field 
	
	  Map all the blocks into the extent list.  This code doesn't try
	  to be very smart.
 hole 
 this extent is last one 
 exclude the header page 
		
		  We found a PAGE_SIZE-length, PAGE_SIZE-aligned run of blocks
 force Empty message 
 SPDX-License-Identifier: GPL-2.0
  fsf2fssuper.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
 f2fs-wide shrinker description 
 limit is 0.2% 
	
	  We do the test below only for project quotas. 'usrquota' and
	  'grpquota' mount options are allowed even without quota feature
	  to support legacy quotas in quota files.
	
	  This mount option is just for testing, and it's not worthwhile to
	  implement the extra complexity (e.g. RCU protection) that would be
	  needed to allow it to be set or changed during remount.  We do allow
	  it to be specified during remount, but only if there is no change.
  1. The same extension name cannot not appear in both compress and non-compress extension
  at the same time.
  2. If the compress extension specifies all files, the types specified by the non-compress
  extension will be treated as special cases and will not be compressed.
  3. Don't allow the non-compress extension specifies all files.
		
		  Initialize args struct so we know whether arg was
		  found; some options take optional arguments.
 this option mounts f2fs with ro 
	
	  The BLKZONED feature indicates that the drive was formatted with
	  zone alignment optimization. This is optional for host-aware
	  devices, but mandatory for host-managed zoned block devices.
	 Not pass down write hints if the number of active logs is lesser
	  than NR_CURSEG_PERSIST_TYPE.
 Initialize f2fs-specific inode info 
 Will be used by directory only 
	
	  during filesystem shutdown, if checkpoint is disabled,
	  drop useless metanode dirty pages.
	
	  This is to avoid a deadlock condition like below.
	  writeback_single_inode(inode)
	   - f2fs_write_data_page
	     - f2fs_gc -> iput -> evict
	        - inode_wait_for_writeback(inode)
 to avoid evict_inode call simultaneously 
 some remained atomic pages should discarded 
 should remain fi->extent_tree for writepage 
  f2fs_dirty_inode() is called from __mark_inode_dirty()
  We should call set_dirty_inode to write the dirty inode through write_inode.
 unregister procfssysfs entries in advance to avoid race case 
 prevent remaining shrinker jobs 
	
	  flush all issued checkpoints and stop checkpoint issue thread.
	  after then, all checkpoints should be done by each process context.
	
	  We don't need to do checkpoint when superblock is clean.
	  But, the previous checkpoint was not done by umount, it needs to do
	  clean checkpoint again.
 be sure to wait for any on-going discard commands 
	
	  normally superblock is clean, so we need to release this.
	  In addition, EIO will skip do checkpoint, we need this as well.
 our cp_error case, we can wait for any writeback page 
	
	  iput() can update stat information, if f2fs_write_checkpoint()
	  above failed with error.
 destroy f2fs internal modules 
 IO error happened before 
 must be clean, since sync_filesystem() was already called 
 ensure no checkpoint required 
 init some FS parameters 
 Restore SB_RDONLY status 
 we should flush all the data to keep data consistency 
	
	  Save the old mount options in case we
	  need to restore them.
 recover superblocks we couldn't write due to previous RO mount 
 parse mount options 
	
	  Previous and new state of filesystem is RO,
	  so skip checking GC and FLUSH_MERGE conditions.
 dquot_resume needs RW 
 disallow enable atgc dynamically 
 disallow enabledisable extent_cache dynamically 
	
	  We stop the GC thread if FS is mounted as RO
	  or if background_gc = off is passed in mount
	  option. Also sync the filesystem.
	
	  We stop issue flush thread if FS is mounted as RO
	  or if flush_merge is not passed in mount option.
 Release old quota file names 
 Update the POSIXACL Flag 
 Read data from quotafile 
 Write to quotafile 
 Don't account quota for quota files to avoid recursion 
 if we are using journalled quota 
	
	  Now when everything is written we can discard the pagecache so
	  that userspace sees the changes.
		
		  do_quotactl
		   f2fs_quota_sync
		   down_read(quota_sem)
		   dquot_writeback_dquots()
		   f2fs_dquot_commit
		 			      block_operation
		 			      down_read(quota_sem)
 if quota sysfile exists, deny enabling quota with specific file 
	
	  quotactl can shutdown journalled quota, result in inconsistence
	  between quota record and fs data by following updates, tag the
	  flag to let fsck be aware of it.
	
	  In case of checkpoint=disable, we must flush quota blocks.
	  This can cause NULL exception for node_inode in end_io, since
	  put_super already dropped it.
 if we are using journalled quota 
	
	  Encrypting the root directory is not allowed because fsck
	  expects lost+found directory to exist and remain unencrypted
	  if LOST_FOUND feature is enabled.
	 
	
	  f2fs_iget isn't quite right if the inode is currently unallocated!
	  However f2fs_iget currently does appropriate checks to handle stale
	  inodes so everything is OK.
 we didn't find the right inode.. 
	
	  note: previously, result is equal to (DEF_ADDRS_PER_INODE -
	  DEFAULT_INLINE_XATTR_ADDRS), but now f2fs try to reserve more
	  space in inode.i_addr, it will be more safe to reassign
	  result as zero.
 two direct node blocks 
 two indirect node blocks 
 one double indirect node block 
 it's rare case, we can do fua all the time 
 fix in-memory information all the time 
 Check checksum_offset and crc in superblock 
 Currently, support only 4KB block size 
 check log blocks per segment 
 Currently, support 512102420484096 bytes sector size 
 blocks_per_seg should be 512, given the above check 
 check reserved ino info 
 check CPSITNATSSAMAIN_AREA area boundary 
 Get block zones type and zone-capacity 
  Read f2fs raw super block.
  Because we have two copies of super block, so read both of them
  to get the first valid one. If any one of them is broken, we pass
  them recovery flag back to the caller.
 sanity checking of raw super 
 No valid superblock 
 we should update superblock crc here 
 write back-up superblock first 
 if we are in recovery path, skip writing valid superblock 
 write current valid superblock 
 Initialize single device information 
	
	  Initialize multiple devices information, or single
	  zoned block device information.
 Single zoned block device mount 
 Multi-device mount 
 to release errored devices 
 adjust parameters according to the volume size 
 allocate memory for f2fs-specific super block info 
 Load the checksum driver 
 set a block size 
 precompute checksum seed for metadata 
 parse mount options 
 init f2fs-specific super block info 
 disallow all the datanodemeta page writes 
 init per sbi slab cache 
 get an inode for meta space 
 Initialize device list 
 setup checkpoint request control and start checkpoint issue thread 
 setup f2fs internal modules 
 For write statistics 
 Read accumulated write IO statistics if exists 
 get an inode for node space 
 read root inode and dentry 
 allocate root dentry 
 Enable quota usage during mount 
 if there are any orphan inodes, free them 
 recover fsynced data 
		
		  mount should be failed, when device has readonly mode, and
		  previous checkpoint was not done by clean system shutdown.
	
	  If the f2fs is not readonly and fsync data recovery succeeds,
	  check zoned block devices' write pointer consistency.
 f2fs_recover_fsync_data() cleared this already 
	
	  If filesystem is not mounted as read-only then
	  do start the gc_thread.
 After POR, we can run background GC thread.
 recover broken superblock 
 safe to flush all the data 
	
	  Some dirty meta pages can be produced by f2fs_recover_orphan_inodes()
	  failed by EIO. Then, iput(node_inode) can trigger balance_fs_bg()
	  followed by f2fs_write_checkpoint() through f2fs_write_node_pages(), which
	  falls into an infinite loop in f2fs_sync_meta_pages().
 evict some inodes being cached by GC 
 stop discard thread before destroying node manager 
 give only one another chance 
		
		  latter evict_inode() can bypass checking and invalidating
		  compress inode cache.
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 SPDX-License-Identifier: GPL-2.0
  fsf2fsxattr.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
  Portions of this code from linuxfsext2xattr.c
  Copyright (C) 2001-2003 Andreas Gruenbacher <agruen@suse.de>
  Fix by Harrison Xing <harrison@mountainviewdata.com>.
  Extended attributes for symlinks and special files added per
   suggestion of Luka Renko <luka.renko@hermes.si>.
  xattr consolidation Copyright (c) 2004 James Morris <jmorris@redhat.com>,
   Red Hat Inc.
 inline xattr header or entry across max inline xattr size 
 The inode already has an extended attribute block. 
 read from inline xattr 
 read from xattr node block 
 read from inline xattr 
 read from xattr node block 
 never been allocated xattrs 
 write to inline xattr 
 no need to use xattr node block 
 write to xattr node block 
 find entry with wanted name. 
 1. Check space 
		
		  If value is NULL, it is remove operation.
		  In case of update operation, we calculate free.
 2. Remove old entry 
		
		  If entry is found, remove old entry.
		  If not found, remove operation is not needed.
 3. Write new entry 
		
		  Before we come here, old entry is removed.
		  We just write new entry.
 this case is only from f2fs_init_inode_metadata 
 SPDX-License-Identifier: GPL-2.0
  fsf2fsgc.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
 give it a try one time 
		
		  [GC triggering condition]
		  0. GC is not conducted currently.
		  1. There are enough dirty segments.
		  2. IO subsystem is idle by checking the # of writeback pages.
		  3. IO subsystem is idle by checking the # of requests in
		     bdev's request list.
		 
		  Note) We have to avoid triggering GCs frequently.
		  Because it is possible that some segments can be
		  invalidated soon after by user update or deletion.
		  So, I'd like to wait some time to collect dirty segments.
 foreground GC was been triggered via f2fs_balance_fs() 
 if return value is not zero, no victim was selected 
 balancing f2fs's metadata periodically 
	
	  adjust candidates range, should select all dirty segments for
	  foreground GC and urgent GC cases.
 let's select beginning hotsmall space first in no_heap mode
 SSR allocates in a segment unit 
 LFS 
 No other gc_mode 
	
	  If the gc_type is FG_GC, we can select victim segments
	  selected by background GC before.
	  Those segments guarantee they have small valid blocks.
 Handle if the system time has changed by the user 
 alloc_mode == LFS 
 Handle if the system time has changed by the user 
 don't choose young section as candidate 
 age = 10000  x%  60 
 u = 10000  x%  40 
  select candidates around source section in range of
  [target - dirty_threshold, target + dirty_threshold]
 rare case 
  This function is called from two paths.
  One is garbage collection and the other is SSR segment selection.
  When it is called during GC, it just gets a victim segment
  and it does not remove it from dirty seglist.
  When it is called from SSR segment selection, it finds a segment
  which has minimum valid blocks and removes it from dirty seglist.
		
		  skip selecting the invalid segno (that is failed due to block
		  validity check failure during GC) to avoid endless GC loop in
		  such cases.
 Don't touch checkpointed data 
				
				  LFS is set to find source section during GC.
				  The victim should have no checkpointed data.
				
				  SSR | AT_SSR are set to find target segment
				  for writes which can be full by checkpointed
				  and newly written blocks.
 get victim for GC_ATAT_SSR 
  This function compares node address got in summary with that in NAT.
  On validity, copy that node with cold status, otherwise (invalid node)
  ignore that.
 stop BG_GC if there is not enough free sections. 
 phase == 2 
 block may become invalid during f2fs_get_node_page 
  Calculate start block index indicating the given node offset.
  Be careful, caller should give this node offset only indicating direct node
  blocks. If any node offsets, which point the other types of node blocks such
  as indirect or double indirect node blocks, are given, it must be a caller's
  bug.
 read page 
	
	  don't cache encrypted data into meta inode until previous dirty
	  data were writebacked to avoid racing between GC and flush.
  Move data block via META_MAPPING while keeping locked data page.
  This can be used to move blocks, aka LBAs, directly on disk.
 do not read out 
	
	  don't cache encrypted data into meta inode until previous dirty
	  data were writebacked to avoid racing between GC and flush.
 read page 
 read source block in mpage 
 allocate block address 
 write target block 
  This function tries to get parent node of victim data block, and identifies
  data block validity. If the block is valid, copy that with cold status and
  modify parent node.
  If the parent node is not valid or the data block address is different,
  the victim data block is ignored.
 dnode info for the data 
		
		  stop BG_GC if there is not enough free sections.
		  Or, stop GC if the segment becomes fully valid caused by
		  race condition along with SSR block allocation.
 Get an inode by ino with checking validity 
 phase 4 
 wait for all inflight aio data 
	
	  zone-capacity can be less than zone-size in zoned devices,
	  resulting in less than expected usable segments in the zone,
	  calculate the end segno in the zone which can be garbage collected
 readahead multi ssa blocks those have contiguous address 
 reference all summary page 
 find segment summary of victim 
		
		  this is to avoid deadlock:
		  - lock_page(sum_page)         - f2fs_replace_block
		   - check_valid_map()            - down_write(sentry_lock)
		    - down_read(sentry_lock)     - change_curseg()
		                                   - lock_page(sum_page)
		
		  For example, if there are many prefree_segments below given
		  threshold, we can make them free by checkpoint. Then, we
		  secure free segments which doesn't need fggc any more.
 f2fs_balance_fs doesn't need to do BG_GC in critical path. 
 give warmcold data area from slower device 
 Force block allocation for GC 
 Move out cursegs from the target range 
 do GC to move out valid blocks in the range 
 new fs size should align to section size 
 stop other GC 
 stop CP to protect MAIN_SEC in free_segment_range 
 SPDX-License-Identifier: GPL-2.0
  f2fs compress support
  Copyright (c) 2019 Chao Yu <chao@kernel.org>
 i_crypto_info and iv index 
	
	  we do not change cc->clen to LZ4_compressBound(inputsize) to
	  adapt worst compress case, because lz4 compressor can handle
	  output budget properly.
	
	  there is compressed data remained in intermediate buffer due to
	  no more space in cbuf.cdata
 Now we're going to cut unnecessary tail pages 
 zero out any unused part of the last page 
  This is called when a page of a compressed cluster has been read from disk
  (or failed to be read from disk).  It checks whether this page was the last
  page being waited on in the cluster, and if so, it decompresses the cluster
  (or in the case of a failure, cleans up without actually decompressing).
 beyond EOF 
 [..., COMPR_ADDR, ...] 
 [COMPR_ADDR, ..., COMPR_ADDR] 
 [COMPR_ADDR, NULL_ADDR or NEW_ADDR, valid_blkaddr] 
 return # of compressed blocks in compressed cluster 
 return # of valid blocks in compressed cluster 
 keep page reference to avoid page reclaim 
 page can be truncated 
 truncate normal cluster 
 truncate compressed cluster 
 should not be a normal cluster 
 we should bypass data pages to proceed the kworkder jobs 
		
		  We need to wait for node_write to avoid block allocation during
		  checkpoint. This can only happen to quota writes which can cause
		  the below discard race condition.
 wait for GCed page writeback via META_MAPPING 
 cluster header 
				
				  for quota file, just redirty left pages to
				  avoid deadlock caused by cluster update race
				  from foreground operation.
  Update and unlock the cluster's pagecache pages, and release the reference to
  the decompress_io_ctx that was being held for IO completion.
 PG_error was set if verity failed. 
 will re-read again later 
 Verify the cluster's decompressed pages with fs-verity. 
  This is called when a compressed cluster has been decompressed
  (or failed to be read andor decompressed).
		
		  Note that to avoid deadlocks, the verity work can't be done
		  on the decompression workqueue.  This is because verifying
		  the data pages can involve reading metadata pages from the
		  file, and these metadata pages may be compressed.
  Put a reference to a compressed page's decompress_io_ctx.
  This is called when the page is no longer needed and can be freed.
  check whether cluster blocks are contiguous, and add extent cache entry
  only if cluster blocks are logically and physically contiguous.
 SPDX-License-Identifier: GPL-2.0
  fsf2fsinode.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
 Check if ino is within scope 
		
		  Previous inline data or directory always reserved 200 bytes
		  in inode layout, even if inline_xattr is disabled. In order
		  to keep inline_dentry's structure for backward compatibility,
		  we get the space back only from inline_data.
 check data exist 
 try to recover cold bit for non-dir inode 
 get rdev by using inline_info 
 deleted inode 
	
	  atime could be updated without dirtying f2fs inode in lazytime mode
	
	  We need to balance fs here to prevent from producing dirty node pages
	  during the urgent cleaning time when running out of free sections.
  Called at the last iput() if i_nlink is zero
 some remained atomic pages should discarded 
 give more chances, if ENOMEM case 
 for the case f2fs_new_inode() was failed, .i_ino is zero, skip it 
		
		  If xattr nid is corrupted, we can reach out error condition,
		  err & !f2fs_exist_written_data(sbi, inode->i_ino, ORPHAN_INO)).
		  In that case, f2fs_check_nid_range() is enough to give a clue.
 caller should call f2fs_lock_op() 
	
	  clear nlink of inode in order to release resource of inode
	  immediately.
	
	  we must call this to avoid inode being remained as dirty, resulting
	  in a panic when flushing dirty inodes in gdirty_list.
 don't make bad inode, since it becomes a regular file. 
	
	  Note: we should add inode to orphan list before f2fs_unlock_op()
	  so we can prevent losing this orphan when encoutering checkpoint
	  and following suddenly power-off.
 iput will drop the inode object 
 SPDX-License-Identifier: GPL-2.0
  fsf2fssegment.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
  __reverse_ffs is copied from includeasm-genericbitops__ffs.h since
  MSB and LSB are reversed in a byte by f2fs_set_bit.
  __find_rev_next(_zero)_bit is copied from libfind_next_bit.c because
  f2fs_set_bit makes MSB and LSB reversed in a byte.
  @size must be integral times of unsigned long.
  Example:
                              MSB <--> LSB
    f2fs_set_bit(0, bitmap) => 1000 0000
    f2fs_set_bit(7, bitmap) => 0000 0001
 add atomic page indices to the list 
 increase reference count with clean state 
			
			  to avoid deadlock in between page lock and
			  inmem_lock.
 we don't need to invalidate this in the sccessful status 
 record old blkaddr for revoking 
		
		  try to revoke all committed pages, but still we could fail
		  due to no memory or other reason, if that happened, EAGAIN
		  will be returned, which means in such case, transaction is
		  already not integrity, caller should use journal to do the
		  recovery or rewrite & commit last transaction. For other
		  error number, revoking was done by filesystem itself.
 drop all uncommitted pages 
  This function balances dirty node and dentry pages.
  In addition, it controls garbage collection.
 balance_fs_bg is able to be pending 
	
	  We should do GC or end up with checkpoint, if there are so many dirty
	  dirnode pages without enough free segments.
 try to shrink extent cache when there is no enough memory 
 check the # of cached NAT entries 
 there is background inflight IO or foreground operation recently 
 exceed periodical checkpoint timeout threshold 
 checkpoint is the only way to shrink partial cached entries 
	
	  update issue_list before we wake up issue_flush thread, this
	  smp_mb() pairs with another barrier in ___wait_event(), see
	  more details in comments of waitqueue_active().
 need not be added 
  Should not occur error such as -ENOMEM.
  Adding dirty entry into seglist is not critical operation.
  If a given segment is one of current working segments, it won't be added.
 Recovery routine with SSR needs this 
 This moves currently empty dirty blocks to prefree. Must hold seglist_lock 
 DATA and NODE 
 This is only used by SBI_CP_DISABLED 
 common policy 
 we need to issue all to keep CP_TRIMMED_FLAG 
 this function is copied from blkdev_issue_discard from blockblk-lib.c 
		
		  should keep before submission to avoid D_DONE
		  right away
 sanity check on discard range 
 wait all 
 This should be covered by global mutex, &sit_i->sentry_lock 
 This comes from f2fs_put_super 
 just to make sure there is no pending discard commands 
 clean up pending candidates before going to sleep 
 For sequential zones, reset the zone write pointer 
 For conventional zones, use regular discard if supported 
 SIT_VBLOCK_MAP_SIZE should be multiple of sizeof(unsigned long) 
 drop caches 
  Should call f2fs_clear_prefree_segments after checkpoint is done.
 send small discards 
	
	  Recovery can cache discard commands, so in error path of
	  fill_super(), it needs to give a chance to handle them.
 Update valid block bitmap 
		
		  SSR should never reuse block which is checkpointed
		  or newly invalidated.
			
			  If checkpoints are off, we must not reuse data that
			  was used in the previous checkpoint. If it was used
			  before, we must track that to know how much space we
			  really have.
 update total number of valid blocks to be written in ckpt area 
 add it into sit main buffer 
 add it into dirty seglist 
  This function should be resided under the curseg_mutex lock
  Calculate the number of current summary pages for writing
  Caller should put this summary page
  Find a new segment from the free segments bitmap to right order
  This function should be returned with success, otherwise BUG
 give up on finding another zone 
 zone is in user, try another 
 set it as dirty segment in free segmap 
 if segs_per_sec is large than 1, we need to keep original policy. 
 inmem log may not locate on any segment after mount 
 find segments from 0 to reuse freed segments 
  Allocate a current working segment.
  This function always allocates a free segment in LFS manner.
  If a segment is written by LFS manner, next block offset is just obtained
  by increasing the current block offset. However, if a segment is written by
  SSR manner, next block offset obtained by calling __next_free_blkoff
 To allocate block chunks in different sizes, use random number 
  This function always allocates a used segment(from dirty seglist) by SSR
  manner, so it should recover the existing segment information of valid blocks
 GC won't be able to use stale summary pages by cp_error 
 allocate cold segment by default 
 f2fs_need_SSR() already forces to do this 
 For node segments, let's do SSR more intensively 
 find valid_blocks=0 in dirty list 
  flush out current segment and replace it with new segment
  This function should be returned with success, otherwise BUG
 startend segment number in main_area 
	
	  We filed discard candidates, but actually we don't need to wait for
	  all of them, since they'll be issued in idle time along with runtime
	  discard option. User configuration looks like using runtime discard
	  or periodic fstrim instead of it.
 This returns write hints for each segment type. This hints will be
  passed down to block layer. There are mapping tables which depend on
  the mount option 'whint_mode'.
  1) whint_mode=off. F2FS only passes down WRITE_LIFE_NOT_SET.
  2) whint_mode=user-based. F2FS tries to pass down hints given by users.
  User                  F2FS                     Block
  ----                  ----                     -----
                        META                     WRITE_LIFE_NOT_SET
                        HOT_NODE                 "
                        WARM_NODE                "
                        COLD_NODE                "
  ioctl(COLD)           COLD_DATA                WRITE_LIFE_EXTREME
  extension list        "                        "
  -- buffered io
  WRITE_LIFE_EXTREME    COLD_DATA                WRITE_LIFE_EXTREME
  WRITE_LIFE_SHORT      HOT_DATA                 WRITE_LIFE_SHORT
  WRITE_LIFE_NOT_SET    WARM_DATA                WRITE_LIFE_NOT_SET
  WRITE_LIFE_NONE       "                        "
  WRITE_LIFE_MEDIUM     "                        "
  WRITE_LIFE_LONG       "                        "
  -- direct io
  WRITE_LIFE_EXTREME    COLD_DATA                WRITE_LIFE_EXTREME
  WRITE_LIFE_SHORT      HOT_DATA                 WRITE_LIFE_SHORT
  WRITE_LIFE_NOT_SET    WARM_DATA                WRITE_LIFE_NOT_SET
  WRITE_LIFE_NONE       "                        WRITE_LIFE_NONE
  WRITE_LIFE_MEDIUM     "                        WRITE_LIFE_MEDIUM
  WRITE_LIFE_LONG       "                        WRITE_LIFE_LONG
  3) whint_mode=fs-based. F2FS passes down hints with its policy.
  User                  F2FS                     Block
  ----                  ----                     -----
                        META                     WRITE_LIFE_MEDIUM;
                        HOT_NODE                 WRITE_LIFE_NOT_SET
                        WARM_NODE                "
                        COLD_NODE                WRITE_LIFE_NONE
  ioctl(COLD)           COLD_DATA                WRITE_LIFE_EXTREME
  extension list        "                        "
  -- buffered io
  WRITE_LIFE_EXTREME    COLD_DATA                WRITE_LIFE_EXTREME
  WRITE_LIFE_SHORT      HOT_DATA                 WRITE_LIFE_SHORT
  WRITE_LIFE_NOT_SET    WARM_DATA                WRITE_LIFE_LONG
  WRITE_LIFE_NONE       "                        "
  WRITE_LIFE_MEDIUM     "                        "
  WRITE_LIFE_LONG       "                        "
  -- direct io
  WRITE_LIFE_EXTREME    COLD_DATA                WRITE_LIFE_EXTREME
  WRITE_LIFE_SHORT      HOT_DATA                 WRITE_LIFE_SHORT
  WRITE_LIFE_NOT_SET    WARM_DATA                WRITE_LIFE_NOT_SET
  WRITE_LIFE_NONE       "                        WRITE_LIFE_NONE
  WRITE_LIFE_MEDIUM     "                        WRITE_LIFE_MEDIUM
  WRITE_LIFE_LONG       "                        WRITE_LIFE_LONG
	
	  __add_sum_entry should be resided under the curseg_mutex
	  because, this function updates a summary entry in the
	  current summary block.
	
	  SIT information should be updated before segment allocation,
	  since SSR needs latest valid block information.
	
	  segment dirty status should be updated after segment allocation,
	  so we just need to update status only one time after previous
	  segment being closed.
 update device state for fsync 
 update device state for checkpoint 
 writeout dirty page into bdev 
 io temperature is needed for passing down write hints 
 for recovery flow 
 se->type is volatile as SSR allocation 
 change the current segment 
 submit cached LFS IO 
 sbumit cached IPU IO 
 Step 1: restore nat cache 
 Step 2: restore sit cache 
 Step 3: restore summary entries 
 get segment number and block addr 
 set uncompleted segment to curseg 
 update journal info 
 restore for compacted data summary 
 sanity check for summary blocks 
 Step 1: write nat cache 
 Step 2: write sit cache 
 Step 3: write summary entries 
  CP calls this function, which flushes SIT entries including sit_journal,
  and moves prefree segs to free segs.
	
	  add and account sit entries of dirty bitmap in sit entry
	  set temporarily
	
	  if there are no enough space in journal to store dirty sit
	  entries, remove all entries from journal and add and account
	  them in sit entry set.
	
	  there are two steps to flush sit entries:
	  #1, flush sit entries to journal in current cold data summary block.
	  #2, flush sit entries to sit page.
 flush dirty sit entries in region of current sit set 
 add discard candidates 
 allocate memory for SIT information 
 get information related with SIT 
 setup SIT bitmap from ckeckpoint pack 
 init SIT information 
 allocate memory for free segmap information 
 set all segments as dirty temporarily 
 init free segmap information 
 build discard map only one time 
 set use the current segments 
 find dirty segment based on free segmap 
 allocate memory for dirty segments list information 
	
	  In LFSSSR curseg, .next_blkoff should point to an unused blkaddr;
	  In LFS curseg, all blkaddr after .next_blkoff should be unused.
	
	  Skip check of zones cursegs point to, since
	  fix_curseg_write_pointer() checks them.
	
	  Get last valid block of the zone.
	
	  If last valid block is beyond the write pointer, report the
	  inconsistency. This inconsistency does not cause write error
	  because the zone will not be selected for write operation until
	  it get discarded. Just report it.
	
	  If there is no valid block in the zone and if write pointer is
	  not at zone start, reset the write pointer.
 report zone for the sector the curseg points to 
 check consistency of the zone curseg pointed to 
 check newly assigned zone 
 Return the zone index in the given device 
  Return the usable segments in a section based on the zone's
  corresponding zone capacity. Zone is equal to a section.
 Conventional zone's capacity is always equal to zone size 
	
	  If the zone_capacity_blocks array is NULL, then zone capacity
	  is equal to the zone size for all zones
 Get the segment count beyond zone capacity block 
  Return the number of usable blocks in a segment. The number of blocks
  returned is always equal to the number of blocks in a segment for
  segments fully contained within a sequential zone capacity or a
  conventional zone. For segments partially contained in a sequential
  zone capacity, the number of usable blocks up to the zone capacity
  is returned. 0 is returned in all other cases.
	
	  Conventional zone's capacity is always equal to zone size,
	  so, blocks per segment is unchanged.
	
	  If segment starts before zone capacity and spans beyond
	  zone capacity, then usable blocks are from seg start to
	  zone capacity. If the segment starts after the zone capacity,
	  then there are no usable blocks.
  Update min, max modified time for cost-benefit GC algorithm
 init sm info 
 reinit free segmap based on SIT 
 discard pre-freedirty segments list 
 SPDX-License-Identifier: GPL-2.0
  fsf2fsdir.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
 If @dir is casefolded, initialize @fname->cf_name from @fname->usr_fname. 
 fall back to treating name as opaque byte sequence 
 hash was decoded from the no-key name 
  Prepare to search for @iname in @dir.  This is similar to
  fscrypt_setup_filename(), but this also handles computing the casefolded name
  and the f2fs dirhash if needed, then packing all the information about this
  filename up into a 'struct f2fs_filename'.
  Prepare to look up @dentry in @dir.  This is similar to
  fscrypt_prepare_lookup(), but this also handles computing the casefolded name
  and the f2fs dirhash if needed, then packing all the information about this
  filename up into a 'struct f2fs_filename'.
  Test whether a case-insensitive directory entry matches the filename
  being searched for.
  Returns 1 for a match, 0 for no match, and -errno on an error.
	
	  In strict mode, ignore invalid names.  In non-strict mode,
	  fall back to treating them as opaque byte sequences.
 utf8_strncasecmp_folded returns 0 on match 
 CONFIG_UNICODE 
 no need to allocate new dentry pages to all the indices 
 This is to increase the speed of f2fs_create 
  Find an entry in the specified directory with the wanted name.
  It returns the page where the entry was found (as a parameter - res_page),
  and the entry itself. Page is returned mapped and unlocked.
  Entry is guaranteed to be valid.
 tmpfile case? 
 copy name info. to this inode page 
		
		  Roll-forward recovery doesn't have encryption keys available,
		  so it can't compute the dirhash for encrypted+casefolded
		  filenames.  Append it to i_name if possible.  Else, disable
		  roll-forward recovery of the dentry (i.e., make fsync'ing the
		  file force a checkpoint) by setting LOST_PINO.
 update dirent of "." 
 update dirent of ".." 
 in order to handle error case 
	
	  This file should be checkpointed during fsync.
	  We lost i_pino from now on.
		
		  If link the tmpfile to alias through linkat path,
		  we should remove this inode from orphan list.
 avoid wrong garbage data for readdir 
 Increase the depth, if required 
 Move to next level to find the empty slot for new dentry 
 synchronize inode page's data from inode cache 
  Caller should grab and release a rwsem by calling f2fs_lock_op() and
  f2fs_unlock_op().
	
	  An immature stackable filesystem shows a race condition between lookup
	  and create. If we have same task when doing lookup and create, it's
	  definitely fine as expected by VFS normally. Otherwise, let's just
	  verify on-disk dentry one more time, which guarantees filesystem
	  consistency more.
  It only removes the dentry from the dentry page, corresponding name
  entry in name page does not need to be touched during deletion.
 Let's check and deallocate this dentry page 
 check memory boundary before moving forward 
 allow readdir() to be interrupted 
 readahead for multi pages of dir 
 SPDX-License-Identifier: GPL-2.0
  fsf2fscheckpoint.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
  We guarantee no failure on the returned page.
 for POR only 
  Readahead CPNATSITSSAPOR pages
 get nat block addr 
 get sit block addr 
 collect a number of dirty meta pages and write together 
 if locked failed, cp will flush dirty pages instead 
 someone wrote it for us 
 add new dirty ino entry into list 
 remove dirty ino entry from list 
 mode should be APPEND_INO, UPDATE_INO or TRANS_DIR_INO 
 add new orphan ino entry into list 
 remove orphan entry from orphan list 
		
		  there should be a bug that we can't find the entry
		  to orphan inode.
 truncate all the data during iput 
 ENOMEM was fully retried in f2fs_evict_inode. 
	
	  Turn on quotas which were not enabled for read-only mounts if
	  filesystem has quota feature, so that they are updated correctly.
 clear Orphan Flag 
 Turn quotas off 
 Restore SB_RDONLY status 
	
	  we don't need to do spin_lock(&im->ino_lock) here, since all the
	  orphan inode operations are covered under f2fs_lock_op().
	  And, spin_lock should be avoided due to page operations below.
 loop for each orphan inode entry and write them in Jornal block 
			
			  an orphan block is full of 1020 entries,
			  then we need to flush current orphan blocks
			  and bring another one in memory
	
	  Finding out valid cp block involves read both
	  sets( cp pack 1 and cp pack 2)
 The second checkpoint pack should start at the next segment 
 Sanity checking of checkpoint 
 We need to give cpu to another writers. 
		
		  We should submit bio, since it exists several
		  wribacking dentry pages in the freeing inode.
 it's on eviction 
  Freeze all the FS-operations for checkpoint.
	
	  Let's flush inline_data in dirty node pages.
 only failed during mountumountfreezequotactl 
 write all the dirty dentry pages 
	
	  POR: we should ensure that there are no dirty node pages
	  until finishing natsit flush. inode->i_blocks can be updated.
	
	  sbi->node_change is used only for AIO write_begin path which produces
	  dirty node blocks and some checkpoint values by block allocation.
 set this flag to activate crc|cp_ver for recovery 
	
	  pagevec_lookup_tag and lock_page again will take
	  some extra time. Therefore, f2fs_update_meta_pages and
	  f2fs_sync_meta_pages are combined in this function.
 writeout cp pack 2 page 
 submit checkpoint (with barrier if NOBARRIER is not set) 
 Flush all the NATSIT pages 
 start to update checkpoint, cp ver is already updated previously 
 2 cp + n data seg summary + orphan inode blocks 
 update ckpt flag for checkpoint 
 update SITNAT bitmap 
 write nat bits 
 write out checkpoint buffer at block 0 
 Record write statistics in the hot node summary 
 update user_block_counts 
 Here, we have one bio having CP pack except cp pack 2 page 
 Wait for all dirty meta pages to be submitted for IO 
 wait for previous submitted meta pages writeback 
 flush all device cache 
 barrier and flush checkpoint cp pack 2 page if it can 
	
	  invalidate intermediate page cache borrowed from meta inode which are
	  used for migration of encrypted, verity or compressed inode's blocks.
	
	  redirty superblock if metadata like node page or inode cache is
	  updated during writing checkpoint.
 this is the case of multiple fstrims without any changes 
	
	  update checkpoint pack index
	  Increase the version number so that
	  SIT entries and seg summaries are written at correct place
 write cached NATSIT entries to NATSIT area 
 save inmem log status 
 update CP_TIME to trigger checkpoint periodically 
 already dispatched by issue_checkpoint_thread 
	
	  update issue_list before we wake up issue_checkpoint thread,
	  this smp_mb() pairs with another barrier in ___wait_event(),
	  see more details in comments of waitqueue_active().
 SPDX-License-Identifier: GPL-2.0
  fsf2fsfile.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
 should do out of any locked page 
 block allocation 
 wait for GCed page writeback via META_MAPPING 
	
	  check to see if the page is mapped already (no holes)
 page is wholly or partially inside EOF 
	
	  Make sure to get the non-deleted alias.  The alias associated with
	  the open file descriptor being fsync()'ed may be deleted already.
 But we need to avoid that there are some inode updates 
 if fdatasync is triggered, let's do in-place-update 
 if the inode is dirty, let's recover all the time 
	
	  if there is no written data, don't waste time to write recovery info.
 it may call write_inode just prior to fsync 
		
		  for OPU case, during fsync(), node can be persisted before
		  data when lower device doesn't support write barrier, result
		  in data corruption after SPO.
		  So for strict fsync mode, force to use atomic write sematics
		  to keep write order in between datanode and last node to
		  avoid potential data corruption.
	
	  Both of fdatasync() and fsync() are able to be recovered from
	  sudden-power-off.
 all the dirty node pages should be flushed for POR 
		
		  We've secured consistency through sync_fs. Following pino
		  will be used only for fsynced inodes after checkpoint.
 if cp_error was enabled, we should avoid infinite loop 
	
	  If it's atomic_write, it's just fine to keep write ordering. So
	  here we don't need to wait for node write completion, since we use
	  node chain which serializes node blocks. If one of node writes are
	  reordered, we can see simply broken chain, resulting in stopping
	  roll-forward recovery. It means we'll recover all or none node blocks
	  given fsync mark.
 once recovery info is written, don't need to tack this 
 handle inline data case 
 direct node does not exists 
 find datahole in dnode block 
 Assumption: truncateion starts with cluster 
		
		  once we invalidate valid blkaddr in range [ofs, ofs + count],
		  we will invalidate all blkaddr in the whole range.
 An encrypted inode should have a key and truncate the last page. 
 lastly zero out the first data page 
	
	  for compressed file, only support cluster size
	  aligned truncation.
	
	  For compressed file, after release compress blocks, don't allow write
	  direct, but we should allow write direct after truncate to zero.
 we should check inline_data size 
 we need to show initial sectors used for inline_datadentries 
		
		  update uidgid under lock_op(), so that dquot and inode can
		  be updated atomically.
			
			  should convert inline inode before i_size_write to
			  keep smaller than inline_data size with inline flag.
		
		  do not trim all blocks after i_size if target size is
		  larger than i_size.
 file size may changed here 
 inode change will produce dirty node pages flushed by checkpoint 
 do not invalidate this block address 
 avoid gc operation during block exchange 
 collapse range should be aligned to block size of f2fs. 
 write out all dirty pages from offset 
 write out all moved pages, if possible 
		
		  f2fs_reserve_new_blocks will not guarantee entire block
		  allocation.
 insert range should be aligned to block size of f2fs. 
 write out all dirty pages from offset 
 avoid gc operation during block exchange 
 write out all moved pages, if possible 
 update new size to the failed position 
 f2fs only support ->fallocate for regular file 
	
	  f2fs_relase_file is called at every close calls. So we should
	  not drop any inmemory pages by close called by other process.
 some remained atomic pages should discarded 
	
	  If the process doing a transaction is crashed, we should do
	  roll-back. Otherwise, other readerwrite can see corrupted database
	  until all the writers close its file. Since this should be done
	  before dropping file lock, it needs to do in ->flush.
 mask can be shrunk by flags_valid selector 
 Is it quota file? Do not allow user to mess with it 
 FS_IOC_[GS]ETFLAGS and FS_IOC_FS[GS]ETXATTR support 
  To make a new on-disk f2fs i_flag gettable via FS_IOC_GETFLAGS, add an entry
  for it to f2fs_fsflags_map[], and add its FS__FL equivalent to
  F2FS_GETTABLE_FS_FL.  To also make it settable via FS_IOC_SETFLAGS, also add
  its FS__FL equivalent to F2FS_SETTABLE_FS_FL.
  Translating flags to fsx_flags value used by FS_IOC_FSGETXATTR and
  FS_IOC_FSSETXATTR is done by the VFS.
 Convert f2fs on-disk i_flags to FS_IOC_{GET,SET}FLAGS flags 
 Convert FS_IOC_{GET,SET}FLAGS flags to f2fs on-disk i_flags 
	
	  Should wait end_io to count F2FS_WB_CP_DATA correctly by
	  f2fs_is_atomic_file.
 add inode in inmem_list first and set atomic_file 
 do checkpoint only 
 do checkpoint only 
 update superblock with uuid 
 undo new data 
 if in-place-update policy is enabled, don't waste time here 
 writeback all dirty pages in the range 
	
	  lookup mapping info in extent cache, skip defragmenting if physical
	  block addresses are continuous.
	
	  lookup mapping info in dnode page cache, skip defragmenting if all
	  physical block addresses are continuous even if there are hole(s)
	  in logical blocks.
 record total count of block that we're going to move 
	
	  make sure there are enough free section for LFS allocation, this can
	  avoid defragment running in SSR mode when free section are allocated
	  intensively
 verify alignment of offset & size 
 verify the end result is block aligned 
 write out all dirty pages from offset 
 Must validate to set it with SQLite behavior in Android. 
 Is it quota file? Do not allow user to mess with it 
 Use i_gc_failures for normal file as a risk signal. 
			
			  Convert inline data for Direct IO before entering
			  f2fs_direct_IO().
			
			  If force_buffere_io() is true, we have to allocate
			  blocks all the time, since f2fs_direct_IO will fall
			  back to buffered IO.
 if we couldn't write data, we should deallocate blocks. 
 SPDX-License-Identifier: GPL-2.0
  f2fs iostat support
  Copyright 2021 Google LLC
  Author: Daeho Jeong <daehojeong@google.com>
 print app write IOs 
 print fs write IOs 
 print app read IOs 
 print fs read IOs 
 print other IOs 
 Need double check under the lock 
 Due to the mempool, this never fails. 
 init iostat info 
 SPDX-License-Identifier: GPL-2.0
  f2fs shrinker support
    the basic infra was copied from fsubifsshrinker.c
  Copyright (c) 2015 Motorola Mobility
  Copyright (c) 2015 Jaegeuk Kim <jaegeuk@kernel.org>
 stop f2fs_put_super 
 count extent cache entries 
 count clean nat cache entries 
 count free nids cache entries 
 stop f2fs_put_super 
 shrink extent cache entries 
 shrink clean nat cache entries 
 shrink free nids cache entries 
 SPDX-License-Identifier: GPL-2.0
  fsf2fsrecovery.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
  Roll forward recovery scenarios.
  [Term] F: fsync_mark, D: dentry_mark
  1. inode(x) | CP | inode(x) | dnode(F)
  -> Update the latest inode(x).
  2. inode(x) | CP | inode(F) | dnode(F)
  -> No problem.
  3. inode(x) | CP | dnode(F) | inode(x)
  -> Recover to the latest dnode(F), and drop the last inode(x)
  4. inode(x) | CP | dnode(F) | inode(F)
  -> No problem.
  5. CP | inode(x) | dnode(F)
  -> The inode(DF) was missing. Should drop this dnode(F).
  6. CP | inode(DF) | dnode(F)
  -> No problem.
  7. CP | dnode(F) | inode(DF)
  -> If f2fs_iget fails, then goto next to find inode(DF).
  8. CP | dnode(F) | inode(x)
  -> If f2fs_iget fails, then goto next to find inode(DF).
     But it will fail due to no inode(DF).
 inode should not be recovered, drop it 
 Compute the hash of the filename 
		
		  In this case the hash isn't computable without the key, so it
		  was saved on-disk.
 Case-sensitive match is fine for recovery 
 get node pages in the current segment 
			
			  CP | dnode(F) | inode(DF)
			  For this case, we should not give up now.
 sanity check in order to detect looped node chain 
 check next segment 
 Get the previous summary 
 Use the locked dnode page and inode 
 Get the node page 
 Deallocate previous index in the node page 
	
	  if inode page is locked, unlock temporarily, but its reference
	  count keeps alive.
 step 1: recover xattr 
 step 2: recover inline data 
 step 3: recover data indices 
 skip recovering if dest is the same as src 
 dest is invalid, just invalidate src block 
		
		  dest is reserved block, invalidate src block
		  and then reserve one new block in dnode page.
 dest is valid block, try to recover from src to dest 
 We should not get -ENOSPC 
 Check the previous node page having this index 
 write dummy data page 
 get node pages in the current segment 
		
		  inode(x) | CP | inode(x) | dnode(F)
		  In this case, we can lose the latest inode(x).
		  So, call recover_inode for the inode update.
 check next segment 
 Turn on quotas so that they are updated correctly 
 prevent checkpoint 
 step #1: find fsynced inode numbers 
 step #2: recover data 
 truncate meta pages to be used by the recovery 
	
	  If fsync data succeeds or there is no fsync data to recover,
	  and the f2fs is not read only, check and fix zoned block devices'
	  write pointer consistency.
 let's drop all the directory inodes for clean checkpoint 
 Turn quotas off 
 Restore SB_RDONLY status 
 SPDX-License-Identifier: GPL-2.0
  f2fs sysfs interface
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
  Copyright (c) 2017 Chao Yu <chao@kernel.org>
 Sysfs support for f2fs 
 struct f2fs_gc_thread 
 struct f2fs_sm_info 
 struct discard_cmd_control 
 struct f2fs_nm_info 
 struct f2fs_sb_info 
 struct f2fs_stat_info 
 struct f2fs_fault_info 
 struct f2fs_fault_info 
 struct f2fs_sb_info 
 struct ckpt_req_control 
 struct atgc_management 
  Note that there are three feature list entries:
  1) sysfsf2fsfeatures
    : shows runtime features supported by in-kernel f2fs along with Kconfig.
      - ref. F2FS_FEATURE_RO_ATTR()
  2) sysfsf2fs$s_idfeatures <deprecated>
    : shows on-disk features enabled by mkfs.f2fs, used for old kernels. This
      won't add new feature anymore, and thus, users should check entries in 3)
      instead of this 2).
  3) sysfsf2fs$s_idfeature_list
    : shows on-disk features enabled by mkfs.f2fs per instance, which follows
      sysfs entry rule where each entry should expose single value.
      This list covers old feature list provided by 2) and beyond. Therefore,
      please add new on-disk feature in this list only.
      - ref. F2FS_SB_FEATURE_RO_ATTR()
 CONFIG_FS_ENCRYPTION 
 For ATGC 
 For ATGC 
 CONFIG_FS_ENCRYPTION 
 SPDX-License-Identifier: GPL-2.0
  f2fs debugging statistics
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
  Copyright (c) 2012 Linux Foundation
  Copyright (c) 2012 Greg Kroah-Hartman <gregkh@linuxfoundation.org>
  This function calculates BDF of every segments
 these will be changed if online resize is done 
 validation check of the segment numbers 
  This function calculates memory footprint.
 build stat 
 build superblock 
 build sm 
 build sit 
 build free segmap 
 build curseg 
 build dirty segmap 
 build nm 
 build gc 
 build merge flush thread 
 free nids 
 segment usage info 
 memory footprint 
 SPDX-License-Identifier: GPL-2.0
  fsf2fsinline.c
  Copyright (c) 2013, Intel Corporation
  Authors: Huajun Li <huajun.li@intel.com>
           Haicheng Li <haicheng.li@intel.com>
 Copy the whole inline data block 
 clear dirty state 
 write data page to try to make data consistent 
 this converted inline_data should be recovered. 
 clear inline data and flag after data writeback 
	
	  The inline_data recovery policy is as follows.
	  [prev.] [next] of inline_data flag
	     o       o  -> recover inline_data
	     o       x  -> remove inline_data, and then recover data blocks
	     x       o  -> remove data blocks, and then recover inline_data
	     x       x  -> recover data blocks
 update i_size to MAX_INLINE_DATA 
  NOTE: ipage is grabbed by caller, but if any error occurs, we should
  release ipage in this function.
 copy data from inline dentry block to new dentry block 
	
	  we do not need to zero out remainder part of dentry and filename
	  field, since we have used bitmap for marking the usage status of
	  them, besides, we can also ignore copyingzeroing reserved space
	  of dentry block, because them haven't been used so far.
 clear inline dir and flag after data writeback 
	
	  should retrieve reserved space which was used to keep
	  inline_dentry's structure for backward compatibility.
		
		  We only need the disk_name and hash to move the dentry.
		  We don't need the original or casefolded filenames.
	
	  should retrieve reserved space which was used to keep
	  inline_dentry's structure for backward compatibility.
 we don't need to mark_inode_dirty now 
 synchronize inode page's data from inode cache 
	
	  f2fs_readdir was protected by inode.i_rwsem, it is safe to access
	  ipage without page's lock held.
 SPDX-License-Identifier: GPL-2.0
  fsf2fsverity.c: fs-verity support for f2fs
  Copyright 2019 Google LLC
  Implementation of fsverity_operations for f2fs.
  Like ext4, f2fs stores the verity metadata (Merkle tree and
  fsverity_descriptor) past the end of the file, starting at the first 64K
  boundary beyond i_size.  This approach works because (a) verity files are
  readonly, and (b) pages fully beyond i_size aren't visible to userspace but
  can be readwritten internally by f2fs with only some relatively small
  changes to f2fs.  Extended attributes cannot be used because (a) f2fs limits
  the total size of an inode's xattr entries to 4096 bytes, which wouldn't be
  enough for even a single Merkle tree block, and (b) f2fs encryption doesn't
  encrypt xattrs, yet the verity metadata must be encrypted when the file is
  because it contains hashes of the plaintext data.
  Using a 64K boundary rather than a 4K one keeps things ready for
  architectures with 64K pages, and it doesn't necessarily waste space on-disk
  since there can be a hole between i_size and the start of the Merkle tree.
  Read some verity metadata from the inode.  __vfs_read() can't be used because
  we need to read beyond i_size.
  Write some verity metadata to the inode for FS_IOC_ENABLE_VERITY.
  kernel_write() can't be used because the file descriptor is readonly.
  Format of f2fs verity xattr.  This points to the location of the verity
  descriptor within the file data rather than containing it directly because
  the verity descriptor must be encrypted when f2fs encryption is used.  But,
  f2fs encryption does not encrypt xattrs.
	
	  Since the file was opened readonly, we have to initialize the quotas
	  here and not rely on ->open() doing it.  This must be done before
	  evicting the inline data.
	
	  If an error already occurred (which fsverity signals by passing
	  desc == NULL), then only clean-up is needed.
 Append the verity descriptor. 
	
	  Write all pages (both data and verity metadata).  Note that this must
	  happen before clearing FI_VERITY_IN_PROGRESS; otherwise pages beyond
	  i_size won't be written properly.  For crash consistency, this also
	  must happen before the verity inode flag gets persisted.
 Set the verity xattr. 
 Finally, set the verity inode flag. 
	
	  Verity failed to be enabled, so clean up by truncating any verity
	  metadata that was written beyond i_size (both from cache and from
	  disk) and clearing FI_VERITY_IN_PROGRESS.
	 
	  Taking i_gc_rwsem[WRITE] is needed to stop f2fs garbage collection
	  from re-instantiating cached pages we are truncating (since unlike
	  normal file accesses, garbage collection isn't limited by i_size).
 Get the descriptor location 
 Get the descriptor 
 SPDX-License-Identifier: GPL-2.0
  fsf2fshash.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
  Portions of this code from linuxfsext3hash.c
  Copyright (C) 2002 by Theodore Ts'o
  Hashing code copied from ext3
 Initialize the default seed for the hash checksum functions 
  Compute @fname->hash.  For all directories, @fname->disk_name must be set.
  For casefolded directories, @fname->usr_fname must be set, and also
  @fname->cf_name if the filename is valid Unicode.
		
		  If the casefolded name is provided, hash it instead of the
		  on-disk name.  If the casefolded name is not provided, that
		  should only be because the name wasn't valid Unicode, so fall
		  back to treating the name as an opaque byte sequence.  Note
		  that to handle encrypted directories, the fallback must use
		  usr_fname (plaintext) rather than disk_name (ciphertext).
 SPDX-License-Identifier: GPL-2.0
  fsf2fsnode.c
  Copyright (c) 2012 Samsung Electronics Co., Ltd.
              http:www.samsung.com
  Check whether the given nid is within node id range.
 only uses low memory 
	
	  give 25%, 25%, 50%, 50%, 50% memory for each components respectively
 it allows 20%  total_ram for inmemory pages 
		
		  free memory is lower than watermark or cached page count
		  exceed threshold, deny caching compress page.
 get current nat block page with lock 
 must be locked by nat_tree_lock 
 for recent accessed nat entry, move it to tail of lru list 
	
	  update entry_cnt in below condition:
	  1. update NEW_ADDR to valid block address;
	  2. update old block address to new one;
 must be locked by nat_tree_lock 
		
		  when nid is reallocated,
		  previous nat entry can be remained in nat cache.
		  So, reinitialize it with new information.
 let's free early to reduce memory consumption 
 sanity check 
 increment version no as node is removed 
 change address 
 update fsync_mark if its inode nat entry is still alive 
 Check nat cache 
	
	  Check current segment summary by trying to grab journal_rwsem first.
	  This sem is on the critical path on the checkpoint requiring the above
	  nat_tree_lock. Therefore, we should retry, if we failed to grab here
	  while not bothering checkpoint.
 Fill node_info from nat page 
 cache nat entry 
  readahead MAX_RA_NODE number of node pages.
 Then, try readahead for siblings of the desired node 
  The maximum depth is four.
  Offset[0] will have raw inode offset.
  Caller should call f2fs_put_dnode(dn).
  Also, it should grab and release a rwsem by calling f2fs_lock_op() and
  f2fs_unlock_op() only if mode is set with ALLOC_NODE.
 if inline_data is set, should not report any block indices 
 get indirect or direct nodes 
 alloc new node 
 Deallocate node address 
 get direct node 
 Make dnode_of_data for parameter 
 remove current indirect node 
 get indirect nodes in the path 
 reference count'll be increased 
 free direct nodes linked to a partial indirect node 
  All the block addresses of data and nodes should be nullified.
 caller must lock inode page 
  Caller should grab and release a rwsem by calling f2fs_lock_op() and
  f2fs_unlock_op().
 remove potential inline_data blocks 
 0 is possible, after f2fs_new_inode() has failed 
 will put inode & node pages 
 allocate inode page for new inode 
 caller should f2fs_put_page(page, 1); 
  Caller should do after getting the following values.
  0: f2fs_put_page(page, 0)
  LOCKED_PAGE or error: f2fs_put_page(page, 1)
 NEW_ADDR can be seen, after cp_error drops some dirty node pages 
  Readahead a node page
 should flush inline_data before evict_inode 
 someone wrote it for us 
 get old block addr of this node page 
 This page is already truncated 
 should add to global list before clearing PAGECACHE status 
 set page dirty and write it 
 someone wrote it for us 
 may be written by other thread 
 someone wrote it for us 
 flush inline_data, if it's async context. 
 give a priority to WB_SYNC threads 
			
			  flushing sequence with step:
			  0. indirect nodes
			  1. dentry dnodes
			  2. file dnodes
 someone wrote it for us 
 flush inline_datainode, if it's async context. 
 flush inline_data 
 flush dirty inode 
 balancing f2fs's metadata in background 
 collect a number of dirty node pages and write together 
  Structure of the f2fs node operations
 return if the nid is recognized as free 
 0 nid should not be used 
		
		    Thread A             Thread B
		   - f2fs_create
		    - f2fs_new_inode
		     - f2fs_alloc_nid
		      - __insert_nid_to_list(PREALLOC_NID)
		                      - f2fs_balance_fs_bg
		                       - f2fs_build_free_nids
		                        - __f2fs_build_free_nids
		                         - scan_nat_page
		                          - add_free_nid
		                           - __lookup_nat_cache
		   - f2fs_add_link
		    - f2fs_init_inode_metadata
		     - f2fs_new_inode_page
		      - f2fs_new_node_page
		       - set_node_addr
		   - f2fs_alloc_nid_done
		    - __remove_nid_from_list(PREALLOC_NID)
		                          - __insert_nid_to_list(FREE_NID)
 Enough entries 
 try to find free nids in free_nid_bitmap 
 readahead nat pages to be scanned 
 go to the next free nat pages to find free nids abundantly 
 find free nids from current sum_pages 
  If this function returns success, caller can obtain a new nid
  from second parameter of this function.
  The returned nid could be used ino as well as nid when inode is created.
 We should not use stale free nids created by f2fs_build_free_nids 
 Let's scan nat pages and its caches to get free nids 
  f2fs_alloc_nid() should be called prior to this function.
  f2fs_alloc_nid() should be called prior to this function.
 1: invalidate the previous xattr nid 
 2: update xattr nid in inode 
 3: update and set xattr node page dirty 
 Should not use this inode from free nid list 
 scan the node segment 
 readahead node pages 
		
		  if a free nat in journal has not been used after last
		  checkpoint, we should remove it from available nids,
		  since later we will add it again.
 handle nid zero due to it should never be used 
	
	  there are two steps to flush nat entries:
	  #1, flush nat entries to journal in current hot data summary block.
	  #2, flush nat entries to nat page.
 flush dirty nats in nat entry set 
 Allow dirty nats by node block allocation in write_begin 
  This function is called during the checkpointing process.
	
	  during unmount, let's flush nat_bits before checking
	  nat_cnt[DIRTY_NAT].
	
	  if there are no enough space in journal to store dirty nat
	  entries, remove all entries from journal and merge them
	  into nat entry set.
 flush dirty nats in nat entry set 
 Allow dirty nats by node block allocation in write_begin 
 segment_count_nat includes pair segment so divide to 2. 
 not used nids: 0, node, meta, (and root counted as valid node) 
 load free nid status from nat_bits table 
 destroy free nid list 
 destroy nat cache 
 destroy nat set cache 
 entry_cnt is not zero, when cp_error was occurred 
 SPDX-License-Identifier: GPL-2.0-or-later
 Handle fileserver selection and rotation.
  Copyright (C) 2017 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Begin iteration through a server list, starting with the vnode's last used
  server if possible, or the last recorded good server if not.
 See if the vnode's preferred record is still available 
		 If we have a lock outstanding on a server that's no longer
		  serving this vnode, then we can't switch to another server
		  and have to return an error.
 Note that the callback promise is effectively broken 
  Post volume busy note.
  Sleep and retry the operation to the same fileserver.
  Select the fileserver to use.  May be called multiple times to rotate
  through the fileservers.
 Evaluate the result of the previous operation, if there was one. 
 Success or local failure.  Stop. 
		 The far side rejected the operation on some grounds.  This
		  might involve the server being busy or the volume having been moved.
			 This fileserver doesn't know about the volume.
			  - May indicate that the VL is wrong - retry once and compare
			    the results.
			  - May indicate that the fileserver couldn't attach to the vol.
			 If the server list didn't change, then assume that
			  it's the fileserver having trouble.
 Try again 
 TODO: Should this return an error or iterate? 
			 Retry after going round all the servers unless we
			  have a file lock we need to maintain.
 Retry with same server & address 
			 The volume migrated to another server.  We consider
			  consider all locks and callbacks broken and request
			  an update from the VLDB.
			 
			  We also limit the number of VMOVED hops we will
			  honour, just in case someone sets up a loop.
			 If the server list didn't change, then the VLDB is
			  out of sync with the fileservers.  This is hopefully
			  a temporary condition, however, so we don't want to
			  permanently block access to the file.
			 
			  TODO: Try other fileservers if we can.
			 
			  TODO: Retry a few times with sleeps.
	 See if we need to do an update of the volume record.  Note that the
	  volume may have moved or even have been deleted.
	 Pick the untried server with the lowest RTT.  If we have outstanding
	  callbacks, we stick with the server we're already using if we can.
	 We're starting on a different fileserver from the list.  We need to
	  check it, create a callback intercept, find its address list and
	  probe its capabilities before we use it.
	 Iterate over the current server's address list to try and find an
	  address on which it will respond to us.
	 We've now had a failure to respond on all of a server's addresses -
	  immediately probe them again and consider retrying the server.
	 That's all the servers poked to no good effect.  Try again if some
	  of them were busy.
  Dump cursor state in the case of the error being EDESTADDRREQ.
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS silly rename handling
  Copyright (C) 2019 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  - Derived from NFS's sillyrename.
  Actually perform the silly rename step.
  Perform silly-rename of a dentry.
  AFS is stateless and the server doesn't know when the client is holding a
  file open.  To prevent application problems when a file is unlinked while
  it's still open, the client performs a "silly-rename".  That is, it renames
  the file to a hidden file in the same directory, and only performs the
  unlink once the last reference to it is put.
  The final cleanup is done during dentry_iput.
 We don't allow a dentry to be silly-renamed twice. 
		 Create a silly name.  Note that the ".__afs" prefix is
		  understood by the salvager and must not be changed.
		 N.B. Better to return EBUSY here ... it could be dangerous
		  to delete the file while it's in use.
 The rename succeeded. 
		 The result of the rename is unknown. Play it safe by forcing
		  a new lookup.
  Tell the server to remove a sillyrename file.
	 If there was a conflict with a third party, check the status of the
	  unlinked vnode.
  Remove sillyrename file on iput.
		 We raced with lookup...  See if we need to transfer the
		  sillyrename information to the aliased dentry.
 Stop lock-release from complaining. 
 AFS superblock handling
  Copyright (c) 2002, 2007, 2018 Red Hat, Inc. All rights reserved.
  This software may be freely redistributed under the terms of the
  GNU General Public License.
  You should have received a copy of the GNU General Public License
  along with this program; if not, write to the Free Software
  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
  Authors: David Howells <dhowells@redhat.com>
           David Woodhouse <dwmw2@infradead.org>
  initialise the filesystem
 create ourselves an inode cache 
 now export our filesystem to lesser mortals 
  clean up the filesystem
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
  Display the mount device name in procmounts.
  Display the mount options in procmounts.
  Parse the source name to get cell name, volume name, volume type and RW
  selector.
  This can be one of the following:
 	"%[cell:]volume[.]"		RW volume
 	"#[cell:]volume[.]"		RO or RW volume (RO parent),
 					 or RW (RW parent) volume
 	"%[cell:]volume.readonly"	RO volume
 	"#[cell:]volume.readonly"	RO volume
 	"%[cell:]volume.backup"		Backup volume
 	"#[cell:]volume.backup"		Backup volume
 To use dynroot, we don't want to have to provide a source 
 determine the type of volume we're looking for 
 split the cell name out if there is one 
 the volume type is further affected by a possible suffix 
 lookup the cell record 
  Parse a single mount parameter.
  Validate the options, get the cell key and look up the volume.
 We try to do the mount securely. 
  check a superblock to see if it's the one we're looking for
  fill in the superblock
 fill in the superblock 
 allocate the root inode and dentry 
	 Clear the callback interests (which will do ilookup5) before
	  deactivating the superblock.
  Get an AFS superblock and root directory.
 allocate a superblock info record 
 allocate a deviceless superblock 
 initial superblockroot creation 
  Set up the filesystem mount context.
 Default to the workstation cell. 
  Initialise an inode cache slab element prior to any use.  Note that
  afs_alloc_inode() must reset anything that could incorrectly leak from one
  inode to another.
  allocate an AFS inode struct from our slab cache
 Reset anything that shouldn't leak from one inode to the next. 
  destroy an AFS inode struct
  return information about an AFS volume
 SPDX-License-Identifier: GPL-2.0-or-later
 handling of writes to regular files and writing back to the server
  Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  mark a page as having been made dirty and thus needing writeback
  prepare to perform part of a write to a page
	 Prefetch area to be written into the cache if we're caching this
	  file.  We need to do this before we get a lock on the page in case
	  there's more than one writer competing for the same cache block.
	 See if this page is already partially written in a way that we can
	  merge the new write with.
		 If the file is being filled locally, allow inter-write
		  spaces to be merged into writes.  If it's not, only write
		  back what the user gives us.
	 The previous write and this write aren't adjacent or overlapping, so
	  flush the page out.
  finalise part of a write to a page
  kill all the pages in the given range
  Redirty all the pages in a given range.
  completion of write to server
  Find a key to use for the writeback.  We cached the keys used to author the
  writes on the vnode.  _wbk will contain the last writeback key used or NULL
  and we need to start from there if it's set.
  write to a file
  Extend the region to be written back to include subsequent contiguously
  dirty pages if possible, but don't sleep while doing so.
  If this page holds new content, then we can include filler zeros in the
  writeback.
		 Firstly, we gather up a batch of contiguous dirty pages
		  under the RCU read lock - but we can't clear the dirty flags
		  there if any of those pages are mapped.
 Has the page moved or been split? 
		 Now, if we obtained any pages, we can shift them to being
		  writable and mark them for caching.
  Synchronously write back the locked page and any subsequent non-locked dirty
  pages.
	 Find all consecutive lockable dirty pages that have contiguous
	  written regions, stopping when we find a page that is not
	  immediately lockable, is not dirty or is missing, or we reach the
	  end of the range.
		 Trim the write to the EOF; the extra data is ignored.  Also
		  put an upper limit on the size of a single storedata op.
	 We now have a contiguous set of dirty pages, each with writeback
	  set; the first page is still locked at this point, but all the rest
	  have been unlocked.
 The dirty region was entirely beyond the EOF. 
  write a page back to the server
  - the caller locked the page for us
  write a region of pages back to the server
 May regress with THPs 
		 At this point we hold neither the i_pages lock nor the
		  page lock: the page may be truncated or invalidated
		  (changing page->mapping to NULL), or even swizzled
		  back from swapper_space to tmpfs file mapping
  write some of the pending data back to the server
	 We have to be careful as we can end up racing with setattr()
	  truncating the pagecache since the caller doesn't take a lock here
	  to prevent it.
  write to an AFS file
  flush any dirty pages for this process, and check for write errors.
  - the return status from this call provides a reliable indication of
    whether any write errors occurred for this process.
  notification that a previously read-only page is about to become writable
  - if it returns an error, the caller will deliver a bus error signal
	 Wait for the page to be written to the cache before we allow it to
	  be modified.  We then assume the entire page will need writing back.
	 We mustn't change folio->private until writeback is complete as that
	  details the portion of the page we need to write back and we might
	  need to redirty the page if there's a problem.
  Prune the keys cached for writeback.  The caller must hold vnode->wb_lock.
 Discard unused keys 
  Clean up a page during invalidation.
 SPDX-License-Identifier: GPL-2.0-or-later
 Extended attribute handling for AFS.  We use xattrs to get and set metadata
  instead of providing pioctl().
  Copyright (C) 2017 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Deal with the result of a successful fetch ACL operation.
  Get a file's ACL.
  Set a file's AFS3 ACL.
 Don't free op->yacl in .put here 
  Get a file's YFS ACL.
  Set a file's YFS ACL.
  Get the name of the cell on which a file resides.
  Get the volume ID, vnode ID and vnode uniquifier of a file as a sequence of
  hex numbers separated by colons.
	 The volume ID is 64-bit, the vnode ID is 96-bit and the
	  uniquifier is 32-bit.
  Get the name of the volume on which a file resides.
 afs.yfs. prefix 
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS filesystem directory editing
  Copyright (C) 2018 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Find a number of contiguous clear bits in a directory block bitmask.
  There are 64 slots, which means we can load the entire bitmap into a
  variable.  The first bit doesn't count as it corresponds to the block header
  slot.  nr_slots is between 1 and 9.
 The first entry is metadata 
  Set a number of contiguous bits in the directory block bitmap.
  Clear a number of contiguous bits in the directory block bitmap.
  Get a new directory folio.
  Scan a directory block looking for a dirent of the right name.
 The block was NUL-terminated by afs_dir_check_page(). 
  Initialise a new directory block.  Note that block 0 is special and contains
  some extra metadata.
  Edit a directory's file data to add a new directory entry.  Doing this after
  create, mkdir, symlink, link or rename if the data version number is
  incremented by exactly one avoids the need to re-download the entire
  directory contents.
  The caller must hold the inode locked.
 Work out how many slots we're going to need. 
	 Find a block that has sufficient slots available.  Each folio
	  contains two or more directory blocks.
		 If the directory extended into a new folio, then we need to
		  tack a new folio on the end.
 Abandon the edit if we got a callback break. 
 Initialise the block if necessary. 
 Only lower dir blocks have a counter in the header. 
			 We need to try and find one or more consecutive
			  slots to hold the entry.
	 There are no spare slots of sufficient size, yet the operation
	  succeeded.  Download the directory again.
 Set the dirent slot. 
 TODO: Really need to maintain this
 Adjust the bitmap. 
 Adjust the allocation counter. 
  Edit a directory's file data to remove a new directory entry.  Doing this
  after unlink, rmdir or rename if the data version number is incremented by
  exactly one avoids the need to re-download the entire directory contents.
  The caller must hold the inode locked.
 Work out how many slots we're going to discard. 
	 Find a block that has sufficient slots available.  Each folio
	  contains two or more directory blocks.
 Abandon the edit if we got a callback break. 
 Didn't find the dirent to clobber.  Download the directory again. 
 Adjust the bitmap. 
 Adjust the allocation counter. 
  Copyright (c) 2002 Red Hat, Inc. All rights reserved.
  This software may be freely redistributed under the terms of the
  GNU General Public License.
  You should have received a copy of the GNU General Public License
  along with this program; if not, write to the Free Software
  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
  Authors: David Woodhouse <dwmw2@infradead.org>
           David Howells <dhowells@redhat.com>
  Initialise an inode from the vnode status.
 Symlinks with a mode of 0644 are actually mountpoints. 
		 it's a symlink we just created (the fileserver
  Update the core inode struct from a returned status record.
		 Expected directory change is handled elsewhere so
		  that we can locally edit the directory and save on a
		  download.
		 Only update the size if the data version jumped.  If the
		  file is being modified locally, then we might have our own
		  idea of what the size should be that's not the same as
		  what's on the server.
  Apply a callback to a vnode.
  Apply the received status and callback to an inode all in the same critical
  section to avoid races with afs_validate().
		 A YFS server will return this from RemoveFile2 and AFS and
		  YFS will return this from InlineBulkStatus.
			 Ignore the result of a speculative bulk status fetch
			  if it splits around a modification op, thereby
			  appearing to regress the data version.
  Fetch file status from the volume.
  ilookup() comparator
  iget5() comparator
struct afs_vnode vnode = AFS_FS_I(inode);
  iget5() inode initialiser
	 YFS supports 96-bit vnode IDs, but Linux only supports
	  64-bit inode numbers.
  Get a cache cookie for an inode.
 Allow for a 96-bit key 
  inode retrieval
 deal with an existing inode 
 success 
 failure 
  Set up the root inode for a volume.  This is always vnode 1, unique 1 within
  the volume.
  mark the data attached to an inode as obsolete due to a write on the server
  - might also want to ditch all the outstanding writes and dirty pages
	 nuke all the non-dirty pages that aren't locked, mapped or being
	  written back in a regular file and completely discard the pages in a
  Check to see if we have a server currently serving this volume and that it
  hasn't been reinitialised or dropped from the list.
  Check the validity of a vnodeinode.
  validate a vnodeinode
  - there are several things we need to check
    - parent dir data changes (rm, rmdir, rename, mkdir, create, link,
      symlink)
    - parent dir metadata changed (security changes)
    - dentry data changed (write, truncate)
    - dentry metadata changed (security changes)
	 if the promise has expired, we need to check the server again to get
	  a new promise - note that if the (parent) directory's metadata was
	  changed then the security may be different and we may no longer have
	 if the vnode's data version number changed then its contents are
  read the attributes of an inode
  discard an AFS inode
  clear an AFS inode
 inode->i_size has now been changed. 
  set the attributes of an inode
 flush any dirty data outstanding on a regular file 
 Prevent any new writebacks from starting whilst we do this. 
  Copyright (c) 2002, 2007 Red Hat, Inc. All rights reserved.
  This software may be freely redistributed under the terms of the
  GNU General Public License.
  You should have received a copy of the GNU General Public License
  along with this program; if not, write to the Free Software
  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
  Authors: David Woodhouse <dwmw2@infradead.org>
           David Howells <dhowells@redhat.com>
  Handle invalidation of an mmap'd file.  We invalidate all the PTEs referring
  to the pages in this file's pagecache, forcing the kernel to go through
  ->fault() or ->page_mkwrite() - at which point we can handle invalidation
  more fully.
  Allow the fileserver to request callback state (re-)initialisation.
  Unfortunately, UUIDs are not guaranteed unique.
  actually break a callback
  Look up a volume by volume ID under RCU conditions.
		 Unfortunately, rbtree walking doesn't give reliable results
		  under just the RCU read lock, so we have to check for
		  changes.
  allow the fileserver to explicitly break one callback
  - happens when
    - the backing file is changed
    - a lock is released
 The callback break applies to an entire volume. 
	 See if we can find a matching inode - even an I_NEW inode needs to
	  be marked as it can have its callback broken before we finish
	  setting up the local inode.
	 TODO: Find all matching volumes if we couldn't match the server and
	  break them anyway.
  allow the fileserver to break callback promises
 SPDX-License-Identifier: GPL-2.0-or-later
 Fileserver-directed operation handling.
  Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Create an operation against a volume.
  Lock the vnode(s) being operated upon.
  Begin an operation on the fileserver.
  Fileserver operations are serialised on the server by vnode, so we serialise
  them here also using the io_lock.
  Tidy up a filesystem cursor and unlock the vnode.
  Wait for an in-progress operation to complete.
  Dispose of an operation.
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS cell alias detection
  Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Sample a volume.
 Explicitly leave it to the VLDB 
 This might need to be something 
  Compare two addresses.
  Compare the address lists of a pair of fileservers.
  Compare the fileserver lists of two volumes.  The server lists are sorted in
  order of ascending UUID.
  Compare root.cell volumes.
 Ignore cells that don't have a root.cell volume. 
  Query the new cell for a volume from a cell we're already using.
 Arbitrarily pick a volume from the list. 
 And see if it's in the new cell. 
 That volume is not in the new cell, so not an alias 
	 The new cell has a like-named volume also - compare volume ID,
	  server and address lists.
  Query the new cell for volumes we know exist in cells we're already using.
 Ignore cells that have a root.cell volume. 
 Transfer our ref 
  Look up a VLDB record for a volume.
 Transfer our ref 
 Try and get the root.cell volume for comparison with other cells 
	 Okay, this cell doesn't have an root.cell volume.  We need to
	  locate some other random volume and use that to check.
  Check to see if a new cell is an alias of a cell we already have.  At this
  point we have the cell's volume server list.
  Returns 0 if we didn't detect an alias, 1 if we found an alias and an error
  if we had problems gathering the data required.  In the case the we did
  detect an alias, cell->alias_of is set to point to the assumed master.
 SPDX-License-Identifier: GPL-2.0-or-later
 dir.c: AFS filesystem directory handling
  Copyright (C) 2002, 2018 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
 This should never happen. 
  Drop the refs that we're holding on the folios we were reading into.  We've
  got refs on the first nr_pages pages.
  check that a directory folio is valid
	 Determine how many magic numbers there should be in this folio, but
	  we must take care because the directory may change size under us.
		 Make sure each block is NUL terminated so we can reasonably
		  use string functions on it.  The filenames in the folio
		  should be NUL-terminated anyway.
  Dump the contents of a directory.
  Check all the blocks in a directory.  All the folios are held pinned.
  open an AFS directory file
  Read the directory into the pagecache in one go, scrubbing the previous
  contents.  The list of folios is returned, pinning them so that they don't
  get reclaimed during the iteration.
 May change 
 We can ask for more than there is 
 May change 
	 Fill in any gaps that we might find where the memory reclaimer has
	  been at work and pin all the folios.  If there are any gaps, we will
	  need to reread the entire directory contents.
	 If we're going to reload, we need to lock all the pages to prevent
	  races.
			 The content has grown, so we need to expand the
			  buffer.
 Validate the data we just read. 
 TODO: Trim excess pages
  deal with one block in an AFS directory
 walk through the block, an entry at a time 
 skip entries marked unused in the bitmap 
 got a valid entry 
 Check that the name-extension dirents are all allocated 
 skip if starts before the current position 
 found the next entry 
  iterate through the data blob that lists the contents of an AFS directory
 round the file position up to the next entry boundary 
 walk through the blocks in sequence 
		 Fetch the appropriate folio from the directory and re-add it
		  to the LRU.  We have all the pages pinned with an extra ref.
  read an AFS directory
  Search the directory for a single name
  - if afs_dir_iterate_block() spots this function, it'll pass the FID
    uniquifier through dtype
 insanity checks first 
  Do a lookup of a single name in a directory
  - just returns the FID the dentry name maps to if found
 search the directory 
  search the directory for a name
  - if afs_dir_iterate_block() spots this function, it'll pass the FID
    uniquifier through dtype
 insanity checks first 
  Deal with the result of a successful lookup operation.  Turn all the files
  into inodes and save the first one - which is the one we actually want.
 Assume vnode->cb_break is 0  +
  See if we know that the server we expect to use doesn't support
  FS.InlineBulkStatus.
  Do a lookup in a directory.  We make use of bulk lookup to query a slew of
  files in one go and create inodes for them.  The inode of the file we were
  asked for is returned.
	cookie->nr_fids = 2;  slot 0 is saved for the fid we actually want
 search the directory 
 Check to see if we already have an inode for the primary fid. 
 We do 
	 Okay, we didn't find it.  We need to query the server - and whilst
	  we're doing that, we're going to attempt to look up a bunch of other
	  vnodes also.
 Need space for examining all the selected files 
			 Find any inodes that already exist and get their
			  callback counters.
 vnode not locked 
	 Try FS.InlineBulkStatus first.  Abort codes for the individual
	  lookups contained therein are stored in the reply without aborting
	  the whole operation.
		 We could try FS.BulkStatus next, but this aborts the entire
		  op if any of the lookups fails - so, for the moment, revert
		  to FS.FetchStatus for op->file[1].
  Look up an entry in a directory with @sys substitution.
 There is an ordered list of substitutes that we have to try. 
	 We don't want to d_add() the @sys dentry here as we don't want to
	  the cached dentry to hide changes to the sysnames list.
  look up an entry in a directory
  Check the validity of a dentry under RCU conditions.
 Check the parent directory is still valid first. 
	 We only need to invalidate a dentry if the server's copy changed
	  behind our back.  If we made the change, it's no problem.  Note that
	  on a 32-bit system, we only have 32 bits in the dentry to store the
	  version.
 Still valid 
  check that a dentry lookup hit has found a valid entry
  - NOTE! the hit can be a negative hit too, so we can't assume we have an
    inode
 Hold the parent dentry so we can peer at it 
 validate the parent directory 
	 We only need to invalidate a dentry if the server's copy changed
	  behind our back.  If we made the change, it's no problem.  Note that
	  on a 32-bit system, we only have 32 bits in the dentry to store the
	  version.
 search the directory for this vnode 
 the filename maps to something 
		 if the vnode ID has changed, then the dirent points to a
		 if the vnode ID uniqifier has changed, then the file has
		  been deleted and replaced, and the original vnode ID has
 the filename is unknown 
  allow the VFS to enquire as to whether a dentry should be unhashed (mustn't
  sleep)
  - called from dput() when d_count is going to 0.
  - return 1 to request dentry be unhashed, 0 otherwise
  Clean up sillyrename files on dentry removal.
  handle dentry release
  Create a new inode for createmkdirsymlink
		 ENOMEM or EINTR at a really inconvenient time - just abandon
		  the new directory on the server.
  create a directory on an AFS filesystem
  Remove a subdir from a directory.
  remove a directory from an AFS filesystem
 Try to make sure we have a callback promise on the victim. 
  Remove a link to a file or symlink from a directory.
  If the file was not deleted due to excess hard links, the fileserver will
  break the callback promise on the file - if it had one - before it returns
  to us, and if it was deleted, it won't
  However, if we didn't have a callback promise outstanding, or it was
  outstanding on a different server, then it won't break it either...
 Already done 
  Remove a file or symlink from an AFS filesystem.
 Try to make sure we have a callback promise on the victim. 
 Start asynchronous writeout of the inode 
 Prevent a race with RCU lookup. 
	 If there was a conflict with a third party, check the status of the
	  unlinked vnode.
  create a regular file on an AFS filesystem
  create a hard link between files in an AFS filesystem
  create a symlink in an AFS filesystem
	 Now we can update d_fsdata on the dentries to reflect their
	  new parent's data_version.
	 
	  Note that if we ever implement RENAME_EXCHANGE, we'll have
	  to update both dentries with opposing dir versions.
  rename a file in an AFS filesystem andor move it between directories
 Don't allow silly-rename files be moved around. 
 May be same as orig_dvnode 
	 For non-directories, check whether the target is busy and if so,
	  make a copy of the dentry and then do a silly-rename.  If the
	  silly-rename succeeds, the copied dentry is hashed and becomes the
	  new target.
		 To prevent any new references to the target during the
		  rename, we unhash the dentry in advance.
 copy the target dentry's name 
	 This bit is potentially nasty as there's a potential race with
	  afs_d_revalidate{,_rcu}().  We have to change d_fsdata on the dentry
	  to reflect it's new parent's new data_version after the op, but
	  d_revalidate may see old_dentry between the op having taken place
	  and the version being updated.
	 
	  So drop the old_dentry for now to make other threads go through
	  lookup instead - which we hold a lock against.
  Release a directory folio and clean up its private state if it's not busy
  - return true if the folio can now be released, false if not
 The directory will need reloading. 
  Invalidate part or all of a folio.
 The directory will need reloading. 
 we clean up only if the entire folio is being invalidated 
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS fileserver probing
  Copyright (C) 2018, 2020 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Start the probe polling timer.  We have to supply it with an inc on the
  outstanding server count.
  Handle the completion of a set of probes.
  Handle the completion of a probe.
  Handle inability to send a probe due to ENOMEM when trying to allocate a
  call struct.
  Process the result of probing a fileserver.  This is called after successful
  or failed delivery of an FS.GetCapabilities operation.
 Responded, but call expired. 
 Set rtt before responded. 
  Probe one or all of a fileserver's addresses to find out the best route and
  to query its capabilities.
  Wait for the first as-yet untried fileserver to respond.
 Only wait for servers that have a probe outstanding. 
  Probe timer.  We have an increment on fs_outstanding that we need to pass
  along to the work item.
  Dispatch a probe to a server.
	 We remove it from the queues here - it will be added back to
	  one of the queues on the completion of the probe.
  Probe a server immediately without waiting for its due time to come
  round.  This is used when all of the addresses have been tried.
  Probe dispatcher to regularly dispatch probes to keep NAT alive.
  Wait for a probe on a particular fileserver to complete for 2s.
  Clean up the probing when the namespace is killed off.
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS security handling
  Copyright (C) 2007, 2017 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  get a key
 act as anonymous user 
 act as authorised user 
  Get a key when pathwalk is in rcuwalk mode.
 act as anonymous user 
 act as authorised user 
  Dispose of a list of permits.
  Discard a permission cache.
  Clear a permit cache on callback break.
  Hash a list of permits.  Use simple addition to make it easy to add an extra
  one at an as-yet indeterminate position in the list.
  Cache the CallerAccess result obtained from doing a fileserver operation
  that returned a vnode status for a particular key.  If a callback break
  occurs whilst the operation was in progress then we have to ditch the cache
  as the ACL may have changed.
	 Check for the common case first: We got back the same access as last
	  time we tried and already have it recorded.
 The cache is still good. 
		 If this set of permits is now wrong, clear the permits
		  pointer so that no one tries to use the stale information.
	 We need a ref on any permits list we want to copy as we'll have to
	  drop the lock to do memory allocation.
	 Speculatively create a new list with the revised permission set.  We
	  discard this if we find an extant match already in the hash, but
	  it's easier to compare with memcmp this way.
	 
	  We fill in the key pointers at this time, but we don't get the refs
	  yet.
 Now see if the permit list we want is actually already available 
	 Someone else changed the cache under us - don't recheck at this
	  time.
 check the permits to see if we've got one yet 
  check with the fileserver to see if the directory or parent directory is
  permitted to be accessed with this authorisation, and if so, what access it
  is granted
 check the permits to see if we've got one yet 
		 Check the status on the file we're actually interested in
		  (the post-processing will cache the result).
  check the permissions on an AFS file
  - AFS ACLs are attached to directories only, and a file is controlled by its
    parent directory's ACL
 check the permits to see if we've got one yet 
 interpret the access mask 
 rmdir, unlink, rename from 
 create, mkdir, symlink, rename to 
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS Volume Location Service client
  Copyright (C) 2002 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Deliver reply data to a VL.GetEntryByNameU call.
 unmarshall the reply once we've received all of it 
	 If there is a new replication site that we can use, ignore all the
	  sites that aren't marked as new.
  VL.GetEntryByNameU operation type.
  Dispatch a get volume entry by name or ID operation (uuid variant).  If the
  volname is a decimal number then it's a volume ID not a volume name.
 Marshall the parameters 
  Deliver reply data to a VL.GetAddrsU call.
 	GetAddrsU(IN ListAddrByAttributes inaddr,
 		  OUT afsUUID uuidp1,
 		  OUT uint32_t uniquifier,
 		  OUT uint32_t nentries,
 		  OUT bulkaddrs blkaddrs);
		 Extract the returned uuid, uniquifier, nentries and
 and extract entries 
  VL.GetAddrsU operation type.
  Dispatch an operation to get the addresses for a server, where the server is
  nominated by UUID.
 Marshall the parameters 
  Deliver reply data to an VL.GetCapabilities operation.
 and extract the capabilities word count 
 and extract capabilities words 
 TODO: Examine capabilities 
  VL.GetCapabilities operation type
  Probe a volume server for the capabilities that it supports.  This can
  return up to 196 words.
  We use this to probe for service upgrade to determine what the server at the
  other end supports.
 marshall the parameters 
 Can't take a ref on server 
  Deliver reply data to a YFSVL.GetEndpoints call.
 	GetEndpoints(IN yfsServerAttributes attr,
 		     OUT opr_uuid uuid,
 		     OUT afs_int32 uniquifier,
 		     OUT endpoints fsEndpoints,
 		     OUT endpoints volEndpoints)
		 Extract the returned uuid, uniquifier, fsEndpoints count and
		  either the first fsEndpoint type or the volEndpoints
 Type or next count 
 and extract fsEndpoints[] entries 
		 Got either the type of the next entry or the count of
		  volEndpoints if no more fsEndpoints.
 Extract the list of volEndpoints. 
		 Extract the type of volEndpoints[0].  Normally we would
		  extract the type of the next endpoint when we extract the
		  data of the current one, but this is the first...
 Get next type too 
 and extract volEndpoints[] entries 
		 Got either the type of the next entry or the count of
		  volEndpoints if no more fsEndpoints.
 Done 
  YFSVL.GetEndpoints operation type.
  Dispatch an operation to get the addresses for a server, where the server is
  nominated by UUID.
 Marshall the parameters 
 Type opr_uuid 
  Deliver reply data to a YFSVL.GetCellName operation.
 and extract the cell name length 
 and extract cell name 
 and extract padding 
  VL.GetCapabilities operation type
  Probe a volume server for the capabilities that it supports.  This can
  return up to 196 words.
  We use this to probe for service upgrade to determine what the server at the
  other end supports.
 marshall the parameters 
 Can't take a ref on server 
 SPDX-License-Identifier: GPL-2.0-or-later
 miscellaneous bits
  Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  convert an AFS abort code to a Linux error number
 Low errno codes inserted into abort namespace 
 VICE "special error" codes; 101 - 111 
 Volume Location server errors 
 Unified AFS error table 
 RXKAD abort codes; from includerxrpcpacket.h.  ET "RXK" == 0x1260B00 
  Select the error to report from a set of errors.
 Responded, but call expired. 
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS Cache Manager Service
  Copyright (C) 2002 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  CB.CallBack operation type
  CB.InitCallBackState operation type
  CB.InitCallBackState3 operation type
  CB.Probe operation type
  CB.ProbeUuid operation type
  CB.TellMeAboutYourself operation type
  YFS CB.CallBack operation type
  route an incoming cache manager call
  - return T if supported, F if not
  Find the server record by peer address and record a probe to the cache
  manager from a server.
  Find the server record by server UUID and record a probe to the cache
  manager from a server.
  Clean up a cache manager call.
  Abort a service call from within an action function.
  The server supplied a list of callbacks that it wanted to break.
	 We need to break the callbacks before sending the reply as the
	  server holds up change visibility till it receives our reply so as
	  to maintain cache coherency.
  deliver request data to a CB.CallBack call
 extract the FID array and its count in two steps 
 extract the callback array and its count in two steps 
	 we'll need the file server record as that tells us which set of
  allow the fileserver to request callback state (re-)initialisation
  deliver request data to a CB.InitCallBackState call
	 we'll need the file server record as that tells us which set of
  deliver request data to a CB.InitCallBackState3 call
	 we'll need the file server record as that tells us which set of
  allow the fileserver to see if the cache manager is still alive
  deliver request data to a CB.Probe call
  Allow the fileserver to quickly find out if the cache manager has been
  rebooted.
  deliver request data to a CB.ProbeUuid call
  allow the fileserver to ask about the cache manager's capabilities
 InterfaceAddr  {
 Capabilities  {
  deliver request data to a CB.TellMeAboutYourself call
  deliver request data to a YFS CB.CallBack call
 extract the FID array and its count in two steps 
	 We'll need the file server record as that tells us which set of
	  vnodes to operate upon.
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS vlserver probing
  Copyright (C) 2018 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Handle the completion of a set of probes.
  Handle the completion of a probe RPC call.
  Process the result of probing a vlserver.  This is called after successful
  or failed delivery of an VL.GetCapabilities operation.
 Responded, but call expired. 
 Set rtt before responded. 
  Probe all of a vlserver's addresses to find out the best route and to
  query its capabilities.
  Send off probes to all unprobed servers.
  Wait for the first as-yet untried server to respond.
 Only wait for servers that have a probe outstanding. 
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS filesystem file handling
  Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Discard a pin on a writeback key.
  Cache key for writeback.
  open an AFS file or directory and attach a key to it
  release an AFS file or directory and discard its key
  Allocate a new read record.
  Dispose of a ref to a read record.
  Fetch file data from the volume.
  Adjust the dirty region of the page on truncation or full invalidation,
  getting rid of the markers altogether if the region is entirely invalidated.
 we clean up only if the entire page is being invalidated 
	  If the page was dirtied by page_mkwrite(), the PTE stays writable
	   and we don't get another notification to tell us to expand it
	   again.
 We may need to shorten the dirty region 
 Doesn't overlap 
 Splits the dirty region - just absorb it 
  invalidate part or all of a page
  - release a page and clean up its private data if offset is 0 (indicating
    the entire page)
  release a page and clean up its private state if it's not busy
  - return true if the page can now be released, false if not
	 deny if page is being written to the cache and the caller hasn't
 Indicate that the folio can be released 
  Handle setting up a memory mapping on an AFS file.
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS server record management
  Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
 Server record timeout in seconds 
  Find a server by one of its addresses.
  Look up a server by its UUID and mark it active.
		 Unfortunately, rbtree walking doesn't give reliable results
		  under just the RCU read lock, so we have to check for
		  changes.
  Install a server record in the namespace tree.  If there's a clash, we stick
  it into a list anchored on whichever afs_server struct is actually in the
  tree.
 Firstly install the server in the UUID lookup tree 
			 We have the same UUID representing servers in
			  different cells.  Append the new server to the list.
	 Secondly, if the server has any IPv4 andor IPv6 addresses, install
	  it in the IPv4 andor IPv6 reverse-map lists.
	 
	  TODO: For speed we want to use something other than a flat list
	  here; even sorting the list in terms of lowest address would help a
	  bit, but anything we might want to do gets messy and memory
	  intensive.
  Allocate a new server record and mark it active.
  Look up an address record for a server
  Get or create a fileserver record.
		 Immediately dispatch an asynchronous probe to each interface
		  on the fileserver.  This will make sure the repeat-probing
		  service is started.
  Set the server timer to fire after a given delay, assuming it's not already
  set for an earlier time.
  Server management timer.  We have an increment on fs_outstanding that we
  need to pass along to the work item.
  Get a reference on a server object.
  Try to get a reference on a server object.
  Get an active count on a server object.
  Release a reference on a server record.
  Drop an active count on a server object without updating the last-unused
  time.
  Drop an active count on a server object.
  destroy a dead server
  Garbage collect any expired servers.
 The one at the front is in the tree 
 This server is not at the front 
  Manage the records of servers known to be within a network namespace.  This
  includes garbage collecting unused servers.
  Note also that we were given an increment on net->servers_outstanding by
  whoever queued us that we need to deal with before returning.
	 Trawl the server list looking for servers that have expired from
	  lack of use.
	 Update the timer on the way out.  We have to pass an increment on
	  servers_outstanding in the namespace that we are in to the timer or
	  the work scheduler.
  Purge list of servers.
  Get an update for a server's address list.
  See if a server's address list needs updating.
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS cell and server record management
  Copyright (C) 2002, 2017 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Set the cell timer to fire after a given delay, assuming it's not already
  set for an earlier time.
  Look up and get an activation reference on a cell record.  The caller must
  hold net->cells_lock at least read-locked.
  Look up and get an activation reference on a cell record.
  Set up a cell record and fill in its name, VL server address list and
  allocate an anonymous key
	 Prohibit cell names that contain unprintable chars, '' and '@' or
	  that begin with a dot.  This also precludes "@cell".
	 Provide a VL server list, filling it in if we were given a list of
	  addresses to use.
 vs sourcestatus 
  afs_lookup_cell - Look up or create a cell record.
  @net:	The network namespace
  @name:	The name of the cell.
  @namesz:	The strlen of the cell name.
  @vllist:	A coloncomma separated list of numeric IP addresses or NULL.
  @excl:	T if an error should be given if the cell name already exists.
  Look up a cell record by name and query the DNS for VL server addresses if
  needed.  Note that that actual DNS query is punted off to the manager thread
  so that this function can return immediately if interrupted whilst allowing
  cell records to be shared even if not yet fully constructed.
	 Assume we're probably going to create a cell and preallocate and
	  mostly set up a candidate record.  We can then use this to stash the
	  name, the net namespace and VL server addresses.
	 
	  We also want to do this before we hold any locks as it may involve
	  upcalling to userspace to make DNS queries.
	 Find the insertion point and check to see if someone else added a
	  cell whilst we were allocating.
 vs error 
 Check the state obtained from the wait check. 
  set the root cell information
  - can be called with a module parameter string
  - can be called from a write to procfsafsrootcell
		 module is loaded with no parameters, or built statically.
		  - in the future we might initialize cell DB here.
 allocate a cell record for the root cell 
 install the new cell 
  Update a cell's VL server address list from the DNS.
			 The DNS said that the cell does not exist or there
			  weren't any addresses to be had.
	 Replace the VL server list if the new record has servers or the old
	  record doesn't.
 vs sourcestatus 
  Destroy a cell record
  Queue the cell manager.
  Cell management timer.  We have an increment on cells_outstanding that we
  need to pass along to the work item.
  Get a reference on a cell record.
  Drop a reference on a cell record.
  Note a cell becoming more active.
  Record a cell becoming less active.  When the active counter reaches 1, it
  is scheduled for destruction, but may get reactivated.
 'cell' may now be garbage collected. 
  Note that a cell has been seen.
  Queue a cell for management, giving the workqueue a ref to hold.
  Allocate a key to use as a placeholder for anonymous user security.
 Create a key to represent an anonymous user. 
  Activate a cell.
  Deactivate a cell.
  Manage a cell record, initialising and destroying it, maintaining its DNS
  records.
 Unhandled state 
 vs error 
 The root volume is pinning the cell 
  Manage the records of cells known to a network namespace.  This includes
  updating the DNS records and garbage collecting unused cells that were
  automatically added.
  Note that constructed cell records may only be removed from net->cells by
  this work item, so it is safe for this work item to stash a cursor pointing
  into the tree and then return to caller (provided it skips cells that are
  still under construction).
  Note also that we were given an increment on net->cells_outstanding by
  whoever queued us that we need to deal with before returning.
	 Trawl the cell database looking for cells that have expired from
	  lack of use and cells whose DNS results have expired and dispatch
	  their managers.
	 Update the timer on the way out.  We have to pass an increment on
	  cells_outstanding in the namespace that we are in to the timer or
	  the work scheduler.
  Purge in-memory cell database.
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS caching stuff
  Copyright (C) 2008 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  check that the auxiliary data indicates that the entry is still valid
 check the size of the data is what we're expecting 
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS vlserver list management.
  Copyright (C) 2018 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
_debug("VL PUT %p{%u}", vlserver, u);
_debug("VLLS PUT %p{%u}", vllist, u);
  Build a VL server address list from a DNS queried server list.
 Start with IPv6 if available. 
  Build a VL server list from a DNS queried server list.
 Check that it's a server list, v1 
 See if we can update an old server record 
		 Extract the addresses - note that we can't skip this as we
		  have to advance the payload pointer.
 TODO: Might want to check for duplicates 
 Insertion-sort by priority and weight 
 Lower preferable 
 Higher preferable 
 SPDX-License-Identifier: GPL-2.0-or-later
 Handle vlserver selection and rotation.
  Copyright (C) 2018 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Begin an operation on a volume location server.
  Begin iteration through a server list, starting with the last used server if
  possible, or the last recorded good server if not.
 Status load is ordered after lookup counter load 
  Select the vlserver to use.  May be called multiple times to rotate
  through the vlservers.
 Evaluate the result of the previous operation, if there was one. 
 Success or local failure.  Stop. 
		 The far side rejected the operation on some grounds.  This
		  might involve the server being busy or the volume having been moved.
 The server went weird. 
write_lock(&vc->cell->vl_servers_lock);
vc->server_list->weird_mask |= 1 << vc->index;
write_unlock(&vc->cell->vl_servers_lock);
 Pick the untried server with the lowest RTT. 
	 We're starting on a different vlserver from the list.  We need to
	  check it, find its address list and probe its capabilities before we
	  use it.
	 Iterate over the current server's address list to try and find an
	  address on which it will respond to us.
	 That's all the servers poked to no good effect.  Try again if some
	  of them were busy.
  Dump cursor state in the case of the error being EDESTADDRREQ.
  Tidy up a volume location server cursor and unlock the vnode.
 SPDX-License-Identifier: GPL-2.0-or-later
 proc interface for AFS
  Copyright (C) 2002 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
 Must be first 
  Display the list of cells known to the namespace.
 display header on line 1 
 display one cell per line on subsequent lines 
  handle writes to procfsafscells
  - to add cells: echo "add <cellname> <IP>[:<IP>][:<IP>]"
 trim to first NL 
 split into command, name and argslist 
 determine command to perform 
  Display the name of the current workstation cell.
  Set the current workstation cell and optionally supply its list of volume
  location servers.
 	echo "cell.name:192.168.231.14" >procfsafsrootcell
 trim to first NL 
 determine command to perform 
  Display the list of volumes known to a cell.
 Display header on line 1 
  Display the list of Volume Location servers we're using for a cell.
  Display the list of fileservers we're using within a namespace.
  Display the list of strings that may be substituted for the @sys pathname
  macro.
  Allow the @sys substitution to be configured.
 Protect against recursion 
  Display general per-net namespace statistics
  initialise procfsafs<cell>
  remove procfsafs<cell>
  initialise the procfsafs directory
  clean up the procfsafs directory
 SPDX-License-Identifier: GPL-2.0-or-later
 mountpoint management
  Copyright (C) 2002 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  no valid lookup procedure on this sort of dir
  no valid open procedure on this sort of dir
  Set the parameters for the proposed superblock.
 if the directory is a pseudo directory, use the d_name 
 read the contents of the AFS special symlink 
  create a vfsmount to be automounted
  handle an automount point
 prevent immediate expiration 
  handle mountpoint expiry timer going off
  kill the AFS mountpoint timer if it's still running
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS volume management
  Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Insert a volume into a cell.  If there's an existing volume record, that is
  returned instead with a ref held.
  Allocate a volume record and load it up from a vldb record.
  Look up or allocate a volume record.
  Look up a VLDB record for a volume.
  Look up a volume in the VL server and create a candidate volume record for
  it.
  The volume name can be one of the following:
 	"%[cell:]volume[.]"		RW volume
 	"#[cell:]volume[.]"		RO or RW volume (rwparent=0),
 					 or RW (rwparent=1) volume
 	"%[cell:]volume.readonly"	RO volume
 	"#[cell:]volume.readonly"	RO volume
 	"%[cell:]volume.backup"		Backup volume
 	"#[cell:]volume.backup"		Backup volume
  The cell name is optional, and defaults to the current cell.
  See "The Rules of Mount Point Traversal" in Chapter 5 of the AFS SysAdmin
  Guide
  - Rule 1: Explicit type suffix forces access of that type or nothing
            (no suffix, then use Rule 2 & 3)
  - Rule 2: If parent volume is RO, then mount RO volume by preference, RW
            if not available
  - Rule 3: If parent volume is RW, then only mount RW volume unless
            explicitly told otherwise
 Make the final decision on the type we want 
  Destroy a volume record
  Get a reference on a volume record.
  Drop a reference on a volume record.
  Activate a volume.
  Deactivate a volume.
  Query the VL service to update the volume status.
	 We look up an ID by passing it as a decimal string in the
	  operation's name parameter.
 See if the volume got renamed. 
 TODO: Use RCU'd string. 
 See if the volume's server list got updated. 
  Make sure the volume record is up to date.
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS file locking support
  Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  if the callback is broken on this vnode, then the lock may now be available
  the lock will time out in 5 minutes unless we extend it, so schedule
  extension in a bit less than that time
  In the case of successful completion of a lock operation, record the time
  the reply appeared and start the lock extension timer.
  grant one or more locks (readlocks are allowed to jump the queue if the
  first lock in the queue is itself a readlock)
  - the caller must hold the vnode lock
  If an error is specified, reject every pending lock that matches the
  authentication and type of the lock we failed to get.  If there are any
  remaining lockers, try to wake up one of them to have a go.
 Select the next locker to hand off to. 
  Kill off all waiters in the the pending lock queue due to the vnode being
  deleted.
  Get a lock on a file
  Extend a lock on a file
  Release a lock on a file
  do work for a lock, including:
  - probing for a lock we're waiting on but didn't get immediately
  - extending a lock that's close to timing out
		 attempt to release the server lock; if it fails, we just
	 If we've already got a lock, then it must be time to extend that
	  lock as AFS locks time out after 5 minutes.
 RPC 
	 If we're waiting for a callback to indicate lock release, we can't
	  actually rely on this, so need to recheck at regular intervals.  The
	  problem is that the server might not notify us if the lock just
	  expires (say because a client died) rather than being explicitly
	  released.
 Looks like a lock request was withdrawn. 
  pass responsibility for the unlocking of a vnode on the server to the
  manager thread, lest a pending signal in the calling thread interrupt
  AF_RXRPC
  - the caller must hold the vnode lock
  Check that our view of the file metadata is up to date and check to see
  whether we think that we have a locking permit.
	 Make sure we've got a callback on this file and that our view of the
	  data version is up to date.
	 Check the permission set to see if we're actually going to be
	  allowed to get a lock on this file.
	 At a rough estimation, you need LOCK, WRITE or INSERT perm to
	  read-lock a file and WRITE or INSERT perm to write-lock a file.
	 
	  We can't rely on the server to do this for us since if we want to
	  share a read lock that we already have, we won't go the server.
  request a lock on a file on the server
	 AFS3 protocol only supports full-file locks and doesn't provide any
	  method of upgradedowngrade, so we need to emulate for partial-file
	  locks.
	 
	  The OpenAFS client only gets a server lock for a full-file lock and
	  keeps partial-file locks local.  Allow this behaviour to be emulated
	  (as the default).
	 If we've already got a lock on the server then try to move to having
	  the VFS grant the requested lock.  Note that this means that other
	  clients may get starved out.
 Write locked 
 Locked 
	 We don't have a lock on this vnode and we aren't currently waiting
	  for one either, so ask the server for a lock.
	 
	  Note that we need to be careful if we get interrupted by a signal
	  after dispatching the request as we may still get the lock, even
	  though we don't wait for the reply (it's not too bad a problem - the
	  lock will expire in 5 mins anyway).
 RPC 
		 The server doesn't have a lock-waiting queue, so the client
		  will have to retry.  The server will break the outstanding
		  callbacks on a file when a lock is released.
 the lock has been granted by the server... 
 ... but the VFS still needs to distribute access on this client. 
	 Again, make sure we've got a callback on this file and, again, make
	  sure that our view of the data version is up to date (we ignore
	  errors incurred here and deal with the consequences elsewhere).
	 We're going to have to wait.  Either this client doesn't have a lock
	  on the server yet and we need to wait for a callback to occur, or
	  the client does have a lock on the server, but it's shared and we
	  need an exclusive lock.
				 We need to retry the lock.  We may not be
				  notified by the server if it just expired
				  rather than being released.
	 The VFS rejected the lock we just obtained, so we have to discard
	  what we just got.  We defer this to the lock manager work item to
	  deal with.
  unlock on a file on the server
 Flush all pending writes before doing anything with locks. 
  return information about a lock we currently hold, if indeed we hold one
 check local lock records first 
 no local locks; consult the server 
  manage POSIX locks on a file
  manage FLOCK locks on a file
	
	  No BSD flocks over NFS allowed.
	  Note: we could try to fake a POSIX lock request here by
	  using ((u32) filp | 0x80000000) or some such as the pid.
	  Not sure whether that would be unique, though, or whether
	  that would break in other places.
 we're simulating flock() locks using posix locks on the server 
  the POSIX lock management core VFS code copies the lock record and adds the
  copy into its own list, so we need to add that copy to the vnode's lock
  queue in the same place as the original (which will be deleted shortly
  after)
  need to remove this lock from the vnode queue when it's removed from the
  VFS's list
 SPDX-License-Identifier: GPL-2.0-or-later
 YFS File Server client stubs
  Copyright (C) 2018 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
 Convert to 100ns intervals. 
  Convert a signed 100ns-resolution 64-bit time into a timespec.
	
	  Unfortunately can not use normal 64 bit division on 32 bit arch, but
	  the alternative, do_div, does not work with negative numbers so have
	  to special case them
  Dump a bad file status record.
  Decode a YFSFetchStatus block
 Ticket dependent 
  Decode a YFSCallBack block
  Decode a YFSVolSync block
  Encode the requested attributes into a YFSStoreStatus block
  Decode a YFSFetchVolumeStatus block.
  Deliver reply data to operations that just return a file status and a volume
  sync record.
  Deliver reply data to an YFS.FetchData64.
		 Extract the returned data length into ->actual_len.  This
		  may indicate more or less data than was requested will be
		  returned.
 extract the returned data 
 Discard any excess data the server gave us 
 extract the metadata 
  YFS.FetchData64 operation type
  Fetch data from a file.
 marshall the parameters 
 RPC flags 
  Deliver reply data for YFS.CreateFile or YFS.MakeDir.
 unmarshall the reply once we've received all of it 
  FS.CreateFile and FS.MakeDir operation type
  Create a file.
 marshall the parameters 
 RPC flags 
 ViceLockType 
  Make a directory.
 marshall the parameters 
 RPC flags 
  Deliver reply data to a YFS.RemoveFile2 operation.
 Was deleted if vnode->status.abort_code == VNOVNODE. 
  YFS.RemoveFile2 operation type.
  Remove a file and retrieve new file status.
 marshall the parameters 
 RPC flags 
  Deliver reply data to a YFS.RemoveFile or YFS.RemoveDir operation.
  FS.RemoveDir and FS.RemoveFile operation types.
  Remove a file.
 marshall the parameters 
 RPC flags 
  Remove a directory.
 marshall the parameters 
 RPC flags 
  Deliver reply data to a YFS.Link operation.
  YFS.Link operation type.
  Make a hard link.
 marshall the parameters 
 RPC flags 
  Deliver reply data to a YFS.Symlink operation.
 unmarshall the reply once we've received all of it 
  YFS.Symlink operation type
  Create a symbolic link.
 marshall the parameters 
 RPC flags 
  Deliver reply data to a YFS.Rename operation.
	 If the two dirs are the same, we have two copies of the same status
	  report, so we just decode it twice.
  YFS.Rename operation type
  Rename a file or directory.
 marshall the parameters 
 RPC flags 
  YFS.StoreData64 operation type.
  Store a set of pages to a large file.
 marshall the parameters 
 RPC flags 
  YFS.StoreStatus operation type
  Set the attributes on a file, using YFS.StoreData64 rather than
  YFS.StoreStatus so as to alter the file size also.
 marshall the parameters 
 RPC flags 
 position of start of write 
 size of write 
 new file length 
  Set the attributes on a file, using YFS.StoreData64 if there's a change in
  file size, and YFS.StoreStatus otherwise.
 marshall the parameters 
 RPC flags 
  Deliver reply data to a YFS.GetVolumeStatus operation.
 extract the returned status record 
 extract the volume name length 
 It's padded 
 extract the volume name 
 extract the offline message length 
 It's padded 
 extract the offline message 
 extract the message of the day length 
 It's padded 
 extract the message of the day 
  YFS.GetVolumeStatus operation type
  fetch the status of a volume
 marshall the parameters 
 RPC flags 
  YFS.SetLock operation type
  YFS.ExtendLock operation type
  YFS.ReleaseLock operation type
  Set a lock on a file
 marshall the parameters 
 RPC flags 
  extend a lock on a file
 marshall the parameters 
 RPC flags 
  release a lock on a file
 marshall the parameters 
 RPC flags 
  Deliver a reply to YFS.FetchStatus
 unmarshall the reply once we've received all of it 
  YFS.FetchStatus operation type
  Fetch the status information for a fid without needing a vnode handle.
 marshall the parameters 
 RPC flags 
  Deliver reply data to an YFS.InlineBulkStatus call
 Extract the file status count and array in two steps 
 Extract the callback count and array in two steps 
  FS.InlineBulkStatus operation type
  Fetch the status information for up to 1024 files
 marshall the parameters 
 RPCFlags 
  Deliver reply data to an YFS.FetchOpaqueACL.
 Extract the file ACL length 
 Extract the file ACL 
 Extract the volume ACL length 
 Extract the volume ACL 
 extract the metadata 
  YFS.FetchOpaqueACL operation type
  Fetch the YFS advanced ACLs for a file.
 marshall the parameters 
 RPC flags 
  YFS.StoreOpaqueACL2 operation type
  Fetch the YFS ACL for a file.
 marshall the parameters 
 RPC flags 
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS fileserver list management.
  Copyright (C) 2017 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Build a server list from a VLDB record.
 Make sure a records exists for each server in the list. 
 Insertion-sort by UUID 
  Copy the annotations from an old server list to its potential replacement.
 Maintain the same preferred server as before if possible. 
 SPDX-License-Identifier: GPL-2.0-or-later
 Server address list management
  Copyright (C) 2017 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Release an address list.
  Allocate an address list.
  Parse a text string consisting of delimited addresses.
 Count the addresses 
 Extract the addresses 
 Port number specification "+1234" 
  Compare old and new address lists to see if there's been any change.
  - How to do this in better than O(Nlog(N)) time?
    - We don't really want to sort the address list, but would rather take the
      list as we got it so as not to undo record rotation by the DNS server.
  Perform a DNS query for VL servers and build a up an address list.
  Merge an IPv4 entry into a fileserver address list.
  Merge an IPv6 entry into a fileserver address list.
  Get an address to try.
  Release an address list cursor.
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS dynamic root handling
  Copyright (C) 2018 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  iget5() comparator for inode created by autocell operations
  These pseudo inodes don't match anything.
  iget5() inode initialiser
  Create an inode for a dynamic root directory or an autocell dynamic
  automount dir.
 there shouldn't be an existing inode 
  Probe to see if a cell may exist.  This prevents positive dentries from
  being created unnecessarily.
 Names prefixed with a dot are RW mounts. 
  Try to auto mount the mountpoint with pseudo directory, if the autocell
  operation is setted.
  Look up @cell in a dynroot directory.  This is a substitution for the
  local cell name for the net namespace.
	 We don't want to d_add() the @cell dentry here as we don't want to
	  the cached dentry to hide changes to the local cell name.
  Look up an entry in a dynroot directory.
  Dirs in the dynamic root don't need revalidation.
  Allow the VFS to enquire as to whether a dentry should be unhashed (mustn't
  sleep)
  - called from dput() when d_count is going to 0.
  - return 1 to request dentry be unhashed, 0 otherwise
  Create a manually added cell mount directory.
  - The caller must hold net->proc_cells_lock
 Let the ->lookup op do the creation 
 Note that we're retaining an extra ref on the dentry 
  Remove a manually added cell mount directory.
  - The caller must hold net->proc_cells_lock
 Don't want to trigger a lookup call, which will re-add the cell 
  Populate a newly created dynamic root with cell names.
  When a dynamic root that's in the process of being destroyed, depopulate it
  of pinned directories.
 Prevent more subdirs from being created 
 Remove all the pins for dirs created for manually added cells 
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS File Server client stubs
  Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  decode an AFSFid block
  Dump a bad file status record.
  decode an AFSFetchStatus block
			 The OpenAFS fileserver has a bug in FS.InlineBulkStatus
			  whereby it doesn't set the interface version in the error
			  case.
 Ticket dependent 
 version 
 type 
  decode an AFSVolSync block
 spare2 
 spare3 
 spare4 
 spare5 
 spare6 
  encode the requested attributes into an AFSStoreStatus block
 segment size 
  decode an AFSFetchVolumeStatus block
  deliver reply data to an FS.FetchStatus
 unmarshall the reply once we've received all of it 
  FS.FetchStatus operation type
  fetch the status information for a file
 marshall the parameters 
  deliver reply data to an FS.FetchData
		 Extract the returned data length into
		  ->actual_len.  This may indicate more or less data than was
		  requested will be returned.
 extract the returned data 
 Discard any excess data the server gave us 
 extract the metadata 
  FS.FetchData operation type
  fetch data from a very large file
 marshall the parameters 
  fetch data from a file
 marshall the parameters 
  deliver reply data to an FS.CreateFile or an FS.MakeDir
 unmarshall the reply once we've received all of it 
  FS.CreateFile and FS.MakeDir operation type
  Create a file.
 marshall the parameters 
 mtime 
 owner 
 group 
 unix mode 
 segment size 
  Create a new directory
 marshall the parameters 
 mtime 
 owner 
 group 
 unix mode 
 segment size 
  Deliver reply data to any operation that returns status and volume sync.
 unmarshall the reply once we've received all of it 
  FS.RemoveFile operation type
  Remove a file.
 marshall the parameters 
  Remove a directory.
 marshall the parameters 
  deliver reply data to an FS.Link
 unmarshall the reply once we've received all of it 
  FS.Link operation type
  make a hard link
 marshall the parameters 
  deliver reply data to an FS.Symlink
 unmarshall the reply once we've received all of it 
  FS.Symlink operation type
  create a symbolic link
 marshall the parameters 
 mtime 
 owner 
 group 
 unix mode 
 segment size 
  deliver reply data to an FS.Rename
	 If the two dirs are the same, we have two copies of the same status
	  report, so we just decode it twice.
  FS.Rename operation type
  Renamemove a file or directory.
 marshall the parameters 
  Deliver reply data to FS.StoreData or FS.StoreStatus
 unmarshall the reply once we've received all of it 
  FS.StoreData operation type
  store a set of pages to a very large file
 marshall the parameters 
 mask 
 mtime 
 owner 
 group 
 unix mode 
 segment size 
  Write data to a file on the server.
 marshall the parameters 
 mask 
 mtime 
 owner 
 group 
 unix mode 
 segment size 
  FS.StoreStatus operation type
  set the attributes on a very large file, using FS.StoreData rather than
  FS.StoreStatus so as to alter the file size also
 marshall the parameters 
 position of start of write 
 size of write 
 new file length 
  set the attributes on a file, using FS.StoreData rather than FS.StoreStatus
  so as to alter the file size also
 marshall the parameters 
 position of start of write 
 size of write 
 new file length 
  set the attributes on a file, using FS.StoreData if there's a change in file
  size, and FS.StoreStatus otherwise
 marshall the parameters 
  deliver reply data to an FS.GetVolumeStatus
 extract the returned status record 
 extract the volume name length 
 It's padded 
 extract the volume name 
 extract the offline message length 
 It's padded 
 extract the offline message 
 extract the message of the day length 
 It's padded 
 extract the message of the day 
  FS.GetVolumeStatus operation type
  fetch the status of a volume
 marshall the parameters 
  deliver reply data to an FS.SetLock, FS.ExtendLock or FS.ReleaseLock
 unmarshall the reply once we've received all of it 
  FS.SetLock operation type
  FS.ExtendLock operation type
  FS.ReleaseLock operation type
  Set a lock on a file
 marshall the parameters 
  extend a lock on a file
 marshall the parameters 
  release a lock on a file
 marshall the parameters 
  Deliver reply data to an FS.GiveUpAllCallBacks operation.
  FS.GiveUpAllCallBacks operation type
  Flush all the callbacks we have on a server.
 marshall the parameters 
  Deliver reply data to an FS.GetCapabilities operation.
 Extract the capabilities word count 
 Extract the first word of the capabilities to call->tmp 
 Extract remaining capabilities words 
  FS.GetCapabilities operation type
  Probe a fileserver for the capabilities that it supports.  This RPC can
  reply with up to 196 words.  The operation is asynchronous and if we managed
  to allocate a call, true is returned the result is delivered through the
  ->done() - otherwise we return false to indicate we didn't even try.
 marshall the parameters 
  Deliver reply data to an FS.InlineBulkStatus call
 Extract the file status count and array in two steps 
 Extract the callback count and array in two steps 
  FS.InlineBulkStatus operation type
  Fetch the status information for up to 50 files
 marshall the parameters 
  deliver reply data to an FS.FetchACL
 extract the returned data length 
 extract the returned data 
 extract the metadata 
  FS.FetchACL operation type
  Fetch the ACL for a file.
 marshall the parameters 
  FS.StoreACL operation type
  Fetch the ACL for a file.
 marshall the parameters 
 SPDX-License-Identifier: GPL-2.0-or-later
 Maintain an RxRPC server socket to do AFS communications through
  Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
 asynchronous incoming call initial processing 
  open an RxRPC socket and bind it to be a server for callback notifications
  - the socket is left in blocking mode and non-blocking ops use MSG_DONTWAIT
 bind the callback manager's address to make this a server socket 
	 Ideally, we'd turn on service upgrade here, but we can't because
	  OpenAFS is buggy and leaks the userStatus field from packet to
	  packet and between FS packets and CB packets - so if we try to do an
	  upgrade on an FS packet, OpenAFS will leak that into the CB packet
	  it sends back to us.
  close the RxRPC socket AFS was using
  Allocate a call.
  Dispose of a reference on a call.
  Queue the call for actual work.
  allocate a call with flat request and reply buffers
  clean up a call with flat buffer
  Advance the AFS call state when the RxRPC call ends the transmit phase.
  Initiate a call and synchronously queue up the parameters for dispatch.  Any
  error is stored into the call struct, which the caller must check for.
	 Work out the length we're going to transmit.  This is awkward for
	  calls such as FS.StoreData where there's an extra injection of data
	  after the initial fixed part.
	 If the call is going to be asynchronous, we need an extra ref for
	  the call to hold itself so the caller need not hang on to its ref.
 create a call 
 send the request 
	 Note that at this point, we may have received the reply or an abort
	  - and an asynchronous call may already have completed.
	 
	  afs_wait_for_call_to_complete(call, ac)
	  must be called to synchronously clean up.
	 We need to dispose of the extra ref we grabbed for an async call.
	  The call, however, might be queued on afs_async_calls and we need to
	  make sure we don't get any more notifications that might requeue it.
  Log remote abort codes that indicate that we have a protocol disagreement
  with the server.
  deliver messages to a call
  Wait synchronously for a call to complete and clean up the call struct.
 deliver any messages that are in the queue 
 rxrpc terminated the call. 
 Kill off the call if it's still live. 
  wake up a waiting call
  wake up an asynchronous call
  Perform IO processing on an asynchronous call.  The work item carries a ref
  to the call struct that we either need to release or to pass on.
  Charge the incoming call preallocation.
  Discard a preallocated call when a socket is shut down.
  Notification of an incoming call.
  Grab the operation ID from an incoming cache manager call.  The socket
  buffer is discarded on error or if we don't yet have sufficient data.
 the operation ID forms the first four bytes of the request data 
	 ask the cache manager to route the call (it'll change the call type
	 pass responsibility for the remainer of this message off to the
  Advance the AFS call state when an RxRPC service call ends the transmit
  phase.
  send an empty reply
  send a simple reply
 Success 
  Extract a piece of data from the received data socket buffers.
  Log protocol error production.
 SPDX-License-Identifier: GPL-2.0-or-later
 AFS client file system
  Copyright (C) 2002,5 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Initialise an AFS network namespace record.
 Register the proc stuff 
 Initialise the cell DB 
 Create the RxRPC transport 
  Clean up and destroy an AFS network namespace record.
  initialise the AFS client FS module
 we want to be able to cache 
 register the filesystems 
 XXX late_initcall is kludgy, but the only alternative seems to create
  a transport upon the first mount, which is worse. Or is it?
 must be called after net to create socket 
  clean up on module removal
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Copyright (C) 2006, 2007 University of Szeged, Hungary
  Authors: Zoltan Sogor
           Artem Bityutskiy ( )
           Adrian Hunter
 This file implements EXT2-compatible extended attribute ioctl() calls 
 Need to be kept consistent with checked flags in ioctl2ubifs() 
 Need to be kept consistent with checked flags in ubifs2ioctl() 
  ubifs_set_inode_flags - set VFS inode flags.
  @inode: VFS inode to set flags for
  This function propagates flags from UBIFS inode object to VFS inode object.
  ioctl2ubifs - convert ioctl inode flags to UBIFS inode flags.
  @ioctl_flags: flags to convert
  This function converts ioctl flags (@FS_COMPR_FL, etc) to UBIFS inode flags
  (@UBIFS_COMPR_FL, etc).
  ubifs2ioctl - convert UBIFS inode flags to ioctl inode flags.
  @ubifs_flags: flags to convert
  This function converts UBIFS inode flags (@UBIFS_COMPR_FL, etc) to ioctl
  flags (@FS_COMPR_FL, etc).
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
  This file implements functions that manage the running of the commit process.
  Each affected module has its own functions to accomplish their part in the
  commit and those functions are called here.
  The commit is the process whereby all updates to the index and LEB properties
  are written out together and the journal becomes empty. This keeps the
  file system consistent - at all times the state can be recreated by reading
  the index and LEB properties and then replaying the journal.
  The commit is split into two parts named "commit start" and "commit end".
  During commit start, the commit process has exclusive access to the journal
  by holding the commit semaphore down for writing. As few IO operations as
  possible are performed during commit start, instead the nodes that are to be
  written are merely identified. During commit end, the commit semaphore is no
  longer held and the journal is again in operation, allowing users to continue
  to use the file system while the bulk of the commit IO is performed. The
  purpose of this two-step approach is to prevent the commit from causing any
  latency blips. Note that in any case, the commit does not prevent lookups
  (as permitted by the TNC mutex), or access to VFS data structures e.g. page
  cache.
  nothing_to_commit - check if there is nothing to commit.
  @c: UBIFS file-system description object
  This is a helper function which checks if there is anything to commit. It is
  used as an optimization to avoid starting the commit if it is not really
  necessary. Indeed, the commit operation always assumes flash IO (e.g.,
  writing the commit start node to the log), and it is better to avoid doing
  this unnecessarily. E.g., 'ubifs_sync_fs()' runs the commit, but if there is
  nothing to commit, it is more optimal to avoid any flash IO.
  This function has to be called with @c->commit_sem locked for writing -
  this function does not take LPTTNC locks because the @c->commit_sem
  guarantees that we have exclusive access to the TNC and LPT data structures.
  This function returns %1 if there is nothing to commit and %0 otherwise.
	
	  During mounting or remounting from RO mode to RW mode we may
	  commit for various recovery-related reasons.
	
	  If the root TNC node is dirty, we definitely have something to
	  commit.
	
	  Even though the TNC is clean, the LPT tree may have dirty nodes. For
	  example, this may happen if the budgeting subsystem invoked GC to
	  make some free space, and the GC found an LEB with only dirty and
	  free space. In this case GC would just change the lprops of this
	  LEB (by turning all space into free space) and unmap it.
  do_commit - commit the journal.
  @c: UBIFS file-system description object
  This function implements UBIFS commit. It has to be called with commit lock
  locked. Returns zero in case of success and a negative error code in case of
  failure.
 Sync all write buffers (necessary for recovery) 
  run_bg_commit - run background commit if it is needed.
  @c: UBIFS file-system description object
  This function runs background commit if it is needed. Returns zero in case
  of success and a negative error code in case of failure.
	
	  Run background commit only if background commit was requested or if
	  commit is required.
  ubifs_bg_thread - UBIFS background thread function.
  @info: points to the file-system description object
  This function implements various file-system background activities:
  o when a write-buffer timer expires it synchronizes the appropriate
    write-buffer;
  o when the journal is about to be full, it starts in-advance commit.
  Note, other stuff like background garbage collection may be added here in
  future.
 Check if there is something to do 
			
			  Nothing prevents us from going sleep now and
			  be never woken up and block the task which
			  could wait in 'kthread_stop()' forever.
  ubifs_commit_required - set commit state to "required".
  @c: UBIFS file-system description object
  This function is called if a commit is required but cannot be done from the
  calling function, so it is just flagged instead.
  ubifs_request_bg_commit - notify the background thread to do a commit.
  @c: UBIFS file-system description object
  This function is called if the journal is full enough to make a commit
  worthwhile, so background thread is kicked to start it.
  wait_for_commit - wait for commit.
  @c: UBIFS file-system description object
  This function sleeps until the commit operation is no longer running.
	
	  The following sleeps if the condition is false, and will be woken
	  when the commit ends. It is possible, although very unlikely, that we
	  will wake up and see the subsequent commit running, rather than the
	  one we were waiting for, and go back to sleep.  However, we will be
	  woken again, so there is no danger of sleeping forever.
  ubifs_run_commit - run or wait for commit.
  @c: UBIFS file-system description object
  This function runs commit and returns zero in case of success and a negative
  error code in case of failure.
		
		  We set the commit state to 'running required' to indicate
		  that we want it to complete as quickly as possible.
 Ok, the commit is indeed needed 
	
	  Since we unlocked 'c->cs_lock', the state may have changed, so
	  re-check it.
  ubifs_gc_should_commit - determine if it is time for GC to run commit.
  @c: UBIFS file-system description object
  This function is called by garbage collection to determine if commit should
  be run. If commit state is @COMMIT_BACKGROUND, which means that the journal
  is full enough to start commit, this function returns true. It is not
  absolutely necessary to commit yet, but it feels like this should be better
  then to keep doing GC. This function returns %1 if GC has to initiate commit
  and %0 if not.
  Everything below is related to debugging.
  struct idx_node - hold index nodes during index tree traversal.
  @list: list
  @iip: index in parent (slot number of this indexing node in the parent
        indexing node)
  @upper_key: all keys in this indexing node have to be less or equivalent to
              this key
  @idx: index node (8-byte aligned because all node structures must be 8-byte
        aligned)
  dbg_old_index_check_init - get information for the next old index check.
  @c: UBIFS file-system description object
  @zroot: root of the index
  This function records information about the index that will be needed for the
  next old index check i.e. 'dbg_check_old_index()'.
  This function returns %0 on success and a negative error code on failure.
  dbg_check_old_index - check the old copy of the index.
  @c: UBIFS file-system description object
  @zroot: root of the new index
  In order to be able to recover from an unclean unmount, a complete copy of
  the index must exist on flash. This is the "old" index. The commit process
  must write the "new" index to flash without overwriting or destroying any
  part of the old index. This function is run at commit end in order to check
  that the old index does indeed exist completely intact.
  This function returns %0 on success and a negative error code on failure.
 Start at the old zroot 
	
	  Traverse the index tree preorder depth-first i.e. do a node and then
	  its subtrees from left to right.
 Get the next index node 
 Keep the index nodes on our path in a linked list 
 Read the index node 
 Validate index node 
 Check root level and sqnum 
 Set last values as though root had a parent 
		
		  The index is always written bottom up hence a child's sqnum
		  is always less than the parents.
 Check key range 
 Go to next index node 
 At the bottom, so go up until can go right 
 Drop the bottom of the list 
 No more list means we are done 
 Look at the new bottom 
 Can we go right 
 Nope, so go up again 
 Go down left 
		
		  We have the parent in 'idx' and now we set up for reading the
		  child pointed to by slot 'iip'.
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Author: Adrian Hunter
  An orphan is an inode number whose inode node has been committed to the index
  with a link count of zero. That happens when an open file is deleted
  (unlinked) and then a commit is run. In the normal course of events the inode
  would be deleted when the file is closed. However in the case of an unclean
  unmount, orphans need to be accounted for. After an unclean unmount, the
  orphans' inodes must be deleted which means either scanning the entire index
  looking for them, or keeping a list on flash somewhere. This unit implements
  the latter approach.
  The orphan area is a fixed number of LEBs situated between the LPT area and
  the main area. The number of orphan area LEBs is specified when the file
  system is created. The minimum number is 1. The size of the orphan area
  should be so that it can hold the maximum number of orphans that are expected
  to ever exist at one time.
  The number of orphans that can fit in a LEB is:
          (c->leb_size - UBIFS_ORPH_NODE_SZ)  sizeof(__le64)
  For example: a 15872 byte LEB can fit 1980 orphans so 1 LEB may be enough.
  Orphans are accumulated in a rb-tree. When an inode's link count drops to
  zero, the inode number is added to the rb-tree. It is removed from the tree
  when the inode is deleted.  Any new orphans that are in the orphan tree when
  the commit is run, are written to the orphan area in 1 or more orphan nodes.
  If the orphan area is full, it is consolidated to make space.  There is
  always enough space because validation prevents the user from creating more
  than the maximum number of orphans allowed.
  ubifs_add_orphan - add an orphan.
  @c: UBIFS file-system description object
  @inum: orphan inode number
  Add an orphan. This function is called when an inodes link count drops to
  zero.
  ubifs_delete_orphan - delete an orphan.
  @c: UBIFS file-system description object
  @inum: orphan inode number
  Delete an orphan. This function is called when an inode is deleted.
  ubifs_orphan_start_commit - start commit of orphans.
  @c: UBIFS file-system description object
  Start commit of orphans.
  avail_orphs - calculate available space.
  @c: UBIFS file-system description object
  This function returns the number of orphans that can be written in the
  available space.
  tot_avail_orphs - calculate total space.
  @c: UBIFS file-system description object
  This function returns the number of orphans that can be written in half
  the total space. That leaves half the space for adding new orphans.
  do_write_orph_node - write a node to the orphan head.
  @c: UBIFS file-system description object
  @len: length of node
  @atomic: write atomically
  This function writes a node to the orphan head from the orphan buffer. If
  %atomic is not zero, then the write is done atomically. On success, %0 is
  returned, otherwise a negative error code is returned.
 Ensure LEB has been unmapped 
  write_orph_node - write an orphan node.
  @c: UBIFS file-system description object
  @atomic: write atomically
  This function builds an orphan node from the cnext list and writes it to the
  orphan head. On success, %0 is returned, otherwise a negative error code
  is returned.
			
			  We limit the number of orphans so that this should
			  never happen.
 Mark the last node of the commit 
  write_orph_nodes - write orphan nodes until there are no more to commit.
  @c: UBIFS file-system description object
  @atomic: write atomically
  This function writes orphan nodes for all the orphans to commit. On success,
  %0 is returned, otherwise a negative error code is returned.
 Unmap any unused LEBs after consolidation 
  consolidate - consolidate the orphan area.
  @c: UBIFS file-system description object
  This function enables consolidation by putting all the orphans into the list
  to commit. The list is in the order that the orphans were added, and the
  LEBs are written atomically in order, so at no time can orphans be lost by
  an unclean unmount.
  This function returns %0 on success and a negative error code on failure.
 Change the cnext list to include all non-new orphans 
		
		  We limit the number of orphans so that this should
		  never happen.
  commit_orphans - commit orphans.
  @c: UBIFS file-system description object
  This function commits orphans to flash. On success, %0 is returned,
  otherwise a negative error code is returned.
 Not enough space to write new orphans, so consolidate 
  erase_deleted - erase the orphans marked for deletion.
  @c: UBIFS file-system description object
  During commit, the orphans being committed cannot be deleted, so they are
  marked for deletion and deleted by this function. Also, the recovery
  adds killed orphans to the deletion list, and therefore they are deleted
  here too.
  ubifs_orphan_end_commit - end commit of orphans.
  @c: UBIFS file-system description object
  End commit of orphans.
  ubifs_clear_orphans - erase all LEBs used for orphans.
  @c: UBIFS file-system description object
  If recovery is not required, then the orphans from the previous session
  are not needed. This function locates the LEBs used to record
  orphans, and un-maps them.
  insert_dead_orphan - insert an orphan.
  @c: UBIFS file-system description object
  @inum: orphan inode number
  This function is a helper to the 'do_kill_orphans()' function. The orphan
  must be kept until the next commit, so it is added to the rb-tree and the
  deletion list.
 Already added - no problem 
  do_kill_orphans - remove orphan inodes from the index.
  @c: UBIFS file-system description object
  @sleb: scanned LEB
  @last_cmt_no: cmt_no of last orphan node read is passed and returned here
  @outofdate: whether the LEB is out of date is returned here
  @last_flagged: whether the end orphan node is encountered
  This function is a helper to the 'kill_orphans()' function. It goes through
  every orphan node in a LEB and for every inode number recorded, removes
  all keys for that inode from the TNC.
 Check commit number 
		
		  The commit number on the master node may be less, because
		  of a failed commit. If there are several failed commits in a
		  row, the commit number written on orphan nodes will continue
		  to increase (because the commit number is adjusted here) even
		  though the commit number on the master node stays the same
		  because the master node has not been re-written.
			
			  The last orphan node had a higher commit number and
			  was flagged as the last written for that commit
			  number. That makes this orphan node, out of date.
			
			  Check whether an inode can really get deleted.
			  linkat() with O_TMPFILE allows rebirth of an inode.
  kill_orphans - remove all orphan inodes from the index.
  @c: UBIFS file-system description object
  If recovery is required, then orphan inodes recorded during the previous
  session (which ended with an unclean unmount) must be deleted from the index.
  This is done by updating the TNC, but since the index is not updated until
  the next commit, the LEBs where the orphan information is recorded are not
  erased until the next commit.
 Check no-orphans flag and skip this if no orphans 
	
	  Orph nodes always start at c->orph_first and are written to each
	  successive LEB in turn. Generally unused LEBs will have been unmapped
	  but may contain out of date orphan nodes if the unmap didn't go
	  through. In addition, the last orphan node written for each commit is
	  marked (top bit of orph->cmt_no is set to 1). It is possible that
	  there are orphan nodes from the next commit (i.e. the commit did not
	  complete successfully). In that case, no orphans will have been lost
	  due to the way that orphans are written, and any orphans added will
	  be valid orphans anyway and so can be deleted.
  ubifs_mount_orphans - delete orphan inodes and erase LEBs that recorded them.
  @c: UBIFS file-system description object
  @unclean: indicates recovery from unclean unmount
  @read_only: indicates read only mount
  This function is called when mounting to erase orphans from the previous
  session. If UBIFS was not unmounted cleanly, then the inodes recorded as
  orphans are deleted.
  Everything below is related to debugging.
 Lowest node type is the inode node, so it comes first 
 Must be recorded as an orphan 
 Check no-orphans flag and skip this if no orphans 
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Artem Bityutskiy ( )
           Adrian Hunter
  This file contains functions for finding LEBs for various purposes e.g.
  garbage collection. In general, lprops category heaps and lists are used
  for fast access, falling back on scanning the LPT as a last resort.
  struct scan_data - data provided to scan callback functions
  @min_space: minimum number of bytes for which to scan
  @pick_free: whether it is OK to scan for empty LEBs
  @lnum: LEB number found is returned here
  @exclude_index: whether to exclude index LEBs
  valuable - determine whether LEB properties are valuable.
  @c: the UBIFS file-system description object
  @lprops: LEB properties
  This function return %1 if the LEB properties should be added to the LEB
  properties tree in memory. Otherwise %0 is returned.
  scan_for_dirty_cb - dirty space scan callback.
  @c: the UBIFS file-system description object
  @lprops: LEB properties to scan
  @in_tree: whether the LEB properties are in main memory
  @data: information passed to and from the caller of the scan
  This function returns a code that indicates whether the scan should continue
  (%LPT_SCAN_CONTINUE), whether the LEB properties should be added to the tree
  in main memory (%LPT_SCAN_ADD), or whether the scan should stop
  (%LPT_SCAN_STOP).
 Exclude LEBs that are currently in use 
 Determine whether to add these LEB properties to the tree 
 Exclude LEBs with too little space 
 If specified, exclude index LEBs 
 If specified, exclude empty or freeable LEBs 
 Exclude LEBs with too little dirty space (unless it is empty) 
 Finally we found space 
  scan_for_dirty - find a data LEB with free space.
  @c: the UBIFS file-system description object
  @min_space: minimum amount free plus dirty space the returned LEB has to
              have
  @pick_free: if it is OK to return a free or freeable LEB
  @exclude_index: whether to exclude index LEBs
  This function returns a pointer to the LEB properties found or a negative
  error code.
 There may be an LEB with enough dirty space on the free heap 
	
	  A LEB may have fallen off of the bottom of the dirty heap, and ended
	  up as uncategorized even though it has enough dirty space for us now,
	  so check the uncategorized list. N.B. neither empty nor freeable LEBs
	  can end up as uncategorized because they are kept on lists not
	  finite-sized heaps.
 We have looked everywhere in main memory, now scan the flash 
 All pnodes are in memory, so skip scan 
  ubifs_find_dirty_leb - find a dirty LEB for the Garbage Collector.
  @c: the UBIFS file-system description object
  @ret_lp: LEB properties are returned here on exit
  @min_space: minimum amount free plus dirty space the returned LEB has to
              have
  @pick_free: controls whether it is OK to pick empty or index LEBs
  This function tries to find a dirty logical eraseblock which has at least
  @min_space free and dirty space. It prefers to take an LEB from the dirty or
  dirty index heap, and it falls-back to LPT scanning if the heaps are empty
  or do not have an LEB which satisfies the @min_space criteria.
  Note, LEBs which have less than dead watermark of free + dirty space are
  never picked by this function.
  The additional @pick_free argument controls if this function has to return a
  free or freeable LEB if one is present. For example, GC must to set it to %1,
  when called from the journal space reservation function, because the
  appearance of free space may coincide with the loss of enough dirty space
  for GC to succeed anyway.
  In contrast, if the Garbage Collector is called from budgeting, it should
  just make free space, not return LEBs which are already free or freeable.
  In addition @pick_free is set to %2 by the recovery process in order to
  recover gc_lnum in which case an index LEB must not be returned.
  This function returns zero and the LEB properties of found dirty LEB in case
  of success, %-ENOSPC if no dirty LEB was found and a negative error code in
  case of other failures. The returned LEB is marked as "taken".
		
		  Note, the index may consume more LEBs than have been reserved
		  for it. It is OK because it might be consolidated by GC.
		  But if the index takes fewer LEBs than it is reserved for it,
		  this function must avoid picking those reserved LEBs.
 Check if there are enough free LEBs for the index 
 OK, try to find an empty LEB 
 Or a freeable LEB 
			
			  We cannot pick freefreeable LEBs in the below code.
 Look on the dirty and dirty index heaps 
		
		  Since we reserve thrice as much space for the index than it
		  actually takes, it does not make sense to pick indexing LEBs
		  with less than, say, half LEB of dirty space. May be half is
		  not the optimal boundary - this should be tested and
		  checked. This boundary should determine how much we use
		  in-the-gaps to consolidate the index comparing to how much
		  we use garbage collector to consolidate it. The "half"
		  criteria just feels to be fine.
 Pick the LEB with most space 
 Did not find a dirty LEB on the dirty heaps, have to scan 
  scan_for_free_cb - free space scan callback.
  @c: the UBIFS file-system description object
  @lprops: LEB properties to scan
  @in_tree: whether the LEB properties are in main memory
  @data: information passed to and from the caller of the scan
  This function returns a code that indicates whether the scan should continue
  (%LPT_SCAN_CONTINUE), whether the LEB properties should be added to the tree
  in main memory (%LPT_SCAN_ADD), or whether the scan should stop
  (%LPT_SCAN_STOP).
 Exclude LEBs that are currently in use 
 Determine whether to add these LEB properties to the tree 
 Exclude index LEBs 
 Exclude LEBs with too little space 
 If specified, exclude empty LEBs 
	
	  LEBs that have only free and dirty space must not be allocated
	  because they may have been unmapped already or they may have data
	  that is obsolete only because of nodes that are still sitting in a
	  wbuf.
 Finally we found space 
  do_find_free_space - find a data LEB with free space.
  @c: the UBIFS file-system description object
  @min_space: minimum amount of free space required
  @pick_free: whether it is OK to scan for empty LEBs
  @squeeze: whether to try to find space in a non-empty LEB first
  This function returns a pointer to the LEB properties found or a negative
  error code.
 There may be an LEB with enough free space on the dirty heap 
	
	  A LEB may have fallen off of the bottom of the free heap, and ended
	  up as uncategorized even though it has enough free space for us now,
	  so check the uncategorized list. N.B. neither empty nor freeable LEBs
	  can end up as uncategorized because they are kept on lists not
	  finite-sized heaps.
 We have looked everywhere in main memory, now scan the flash 
 All pnodes are in memory, so skip scan 
  ubifs_find_free_space - find a data LEB with free space.
  @c: the UBIFS file-system description object
  @min_space: minimum amount of required free space
  @offs: contains offset of where free space starts on exit
  @squeeze: whether to try to find space in a non-empty LEB first
  This function looks for an LEB with at least @min_space bytes of free space.
  It tries to find an empty LEB if possible. If no empty LEBs are available,
  this function searches for a non-empty data LEB. The returned LEB is marked
  as "taken".
  This function returns found LEB number in case of success, %-ENOSPC if it
  failed to find a LEB with @min_space bytes of free space and other a negative
  error codes in case of failure.
 Check if there are enough empty LEBs for commit 
		
		  OK to allocate an empty LEB, but we still don't want to go
		  looking for one if there aren't any.
			
			  Because we release the space lock, we must account
			  for this allocation here. After the LEB properties
			  flags have been updated, we subtract one. Note, the
			  result of this is that lprops also decreases
			  @taken_empty_lebs in 'ubifs_change_lp()', so it is
			  off by one for a short period of time which may
			  introduce a small disturbance to budgeting
			  calculations, but this is harmless because at the
			  worst case this would make the budgeting subsystem
			  be more pessimistic than needed.
			 
			  Fundamentally, this is about serialization of the
			  budgeting and lprops subsystems. We could make the
			  @space_lock a mutex and avoid dropping it before
			  calling 'ubifs_change_lp()', but mutex is more
			  heavy-weight, and we want budgeting to be as fast as
			  possible.
		
		  Ensure that empty LEBs have been unmapped. They may not have
		  been, for example, because of an unclean unmount.  Also
		  LEBs that were freeable LEBs (free + dirty == leb_size) will
		  not have been unmapped.
  scan_for_idx_cb - callback used by the scan for a free LEB for the index.
  @c: the UBIFS file-system description object
  @lprops: LEB properties to scan
  @in_tree: whether the LEB properties are in main memory
  @data: information passed to and from the caller of the scan
  This function returns a code that indicates whether the scan should continue
  (%LPT_SCAN_CONTINUE), whether the LEB properties should be added to the tree
  in main memory (%LPT_SCAN_ADD), or whether the scan should stop
  (%LPT_SCAN_STOP).
 Exclude LEBs that are currently in use 
 Determine whether to add these LEB properties to the tree 
 Exclude index LEBS 
 Exclude LEBs that cannot be made empty 
	
	  We are allocating for the index so it is safe to allocate LEBs with
	  only free and dirty space, because write buffers are sync'd at commit
	  start.
  scan_for_leb_for_idx - scan for a free LEB for the index.
  @c: the UBIFS file-system description object
  ubifs_find_free_leb_for_idx - find a free LEB for the index.
  @c: the UBIFS file-system description object
  This function looks for a free LEB and returns that LEB number. The returned
  LEB is marked as "taken", "index".
  Only empty LEBs are allocated. This is for two reasons. First, the commit
  calculates the number of LEBs to allocate based on the assumption that they
  will be empty. Secondly, free space at the end of an index LEB is not
  guaranteed to be empty because it may have been used by the in-the-gaps
  method prior to an unclean unmount.
  If no LEB is found %-ENOSPC is returned. For other failures another negative
  error code is returned.
			
			  The first condition means the following: go scan the
			  LPT if there are uncategorized lprops, which means
			  there may be freeable LEBs there (UBIFS does not
			  store the information about freeable LEBs in the
			  master node).
	
	  Ensure that empty LEBs have been unmapped. They may not have been,
	  for example, because of an unclean unmount. Also LEBs that were
	  freeable LEBs (free + dirty == leb_size) will not have been unmapped.
  ubifs_save_dirty_idx_lnums - save an array of the most dirty index LEB nos.
  @c: the UBIFS file-system description object
  This function is called each commit to create an array of LEB numbers of
  dirty index LEBs sorted in order of dirty and free space.  This is used by
  the in-the-gaps method of TNC commit.
 Copy the LPROPS_DIRTY_IDX heap 
 Sort it so that the dirtiest is now at the end 
 Replace the lprops pointers with LEB numbers 
  scan_dirty_idx_cb - callback used by the scan for a dirty index LEB.
  @c: the UBIFS file-system description object
  @lprops: LEB properties to scan
  @in_tree: whether the LEB properties are in main memory
  @data: information passed to and from the caller of the scan
  This function returns a code that indicates whether the scan should continue
  (%LPT_SCAN_CONTINUE), whether the LEB properties should be added to the tree
  in main memory (%LPT_SCAN_ADD), or whether the scan should stop
  (%LPT_SCAN_STOP).
 Exclude LEBs that are currently in use 
 Determine whether to add these LEB properties to the tree 
 Exclude non-index LEBs 
 Exclude LEBs with too little space 
 Finally we found space 
  find_dirty_idx_leb - find a dirty index LEB.
  @c: the UBIFS file-system description object
  This function returns LEB number upon success and a negative error code upon
  failure.  In particular, -ENOSPC is returned if a dirty index LEB is not
  found.
  Note that this function scans the entire LPT but it is called very rarely.
 Check all structures in memory first 
 All pnodes are in memory, so skip scan 
  get_idx_gc_leb - try to get a LEB number from trivial GC.
  @c: the UBIFS file-system description object
	
	  The LEB was due to be unmapped after the commit but
	  it is needed now for this commit.
  find_dirtiest_idx_leb - find dirtiest index LEB from dirtiest array.
  @c: the UBIFS file-system description object
 The lprops pointers were replaced by LEB numbers 
  ubifs_find_dirty_idx_leb - try to find dirtiest index LEB as at last commit.
  @c: the UBIFS file-system description object
  This function attempts to find an untaken index LEB with the most free and
  dirty space that can be used without overwriting index nodes that were in the
  last index committed.
	
	  We made an array of the dirtiest index LEB numbers as at the start of
	  last commit.  Try that array first.
 Next try scanning the entire LPT 
 Finally take any index LEBs awaiting trivial GC 
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Copyright (C) 2006, 2007 University of Szeged, Hungary
  Authors: Artem Bityutskiy ( )
           Adrian Hunter
           Zoltan Sogor
  This file implements UBIFS IO subsystem which provides various IO-related
  helper functions (readingwritingcheckingvalidating nodes) and implements
  write-buffering support. Write buffers help to save space which otherwise
  would have been wasted for padding to the nearest minimal IO unit boundary.
  Instead, data first goes to the write-buffer and is flushed when the
  buffer is full or when it is not used for some time (by timer). This is
  similar to the mechanism is used by JFFS2.
  UBIFS distinguishes between minimum write size (@c->min_io_size) and maximum
  write size (@c->max_write_size). The latter is the maximum amount of bytes
  the underlying flash is able to program at a time, and writing in
  @c->max_write_size units should presumably be faster. Obviously,
  @c->min_io_size <= @c->max_write_size. Write-buffers are of
  @c->max_write_size bytes in size for maximum performance. However, when a
  write-buffer is flushed, only the portion of it (aligned to @c->min_io_size
  boundary) which contains data is written, not the whole write-buffer,
  because this is more space-efficient.
  This optimization adds few complications to the code. Indeed, on the one
  hand, we want to write in optimal @c->max_write_size bytes chunks, which
  also means aligning writes at the @c->max_write_size bytes offsets. On the
  other hand, we do not want to waste space when synchronizing the write
  buffer, so during synchronization we writes in smaller chunks. And this makes
  the next write offset to be not aligned to @c->max_write_size bytes. So the
  have to make sure that the write-buffer offset (@wbuf->offs) becomes aligned
  to @c->max_write_size bytes again. We do this by temporarily shrinking
  write-buffer size (@wbuf->size).
  Write-buffers are defined by 'struct ubifs_wbuf' objects and protected by
  mutexes defined inside these objects. Since sometimes upper-level code
  has to lock the write-buffer (e.g. journal space reservation code), many
  functions related to write-buffers have "nolock" suffix which means that the
  caller has to lock the write-buffer before calling this function.
  UBIFS stores nodes at 64 bit-aligned addresses. If the node length is not
  aligned, UBIFS starts the next node from the aligned address, and the padded
  bytes may contain any rubbish. In other words, UBIFS does not put padding
  bytes in those small gaps. Common headers of nodes store real node lengths,
  not aligned lengths. Indexing nodes also store real lengths in branches.
  UBIFS uses padding when it pads to the next min. IO unit. In this case it
  uses padding nodes or padding bytes, if the padding node does not fit.
  All UBIFS nodes are protected by CRC checksums and UBIFS checks CRC when
  they are read from the flash media.
  ubifs_ro_mode - switch UBIFS to read read-only mode.
  @c: UBIFS file-system description object
  @err: error code which is the reason of switching to RO mode
  Below are simple wrappers over UBI IO functions which include some
  additional checks and UBIFS debugging stuff. See corresponding UBI function
  for more information.
	
	  In case of %-EBADMSG print the error message only if the
	  @even_ebadmsg is true.
  ubifs_check_node - check node.
  @c: UBIFS file-system description object
  @buf: node to check
  @len: node length
  @lnum: logical eraseblock number
  @offs: offset within the logical eraseblock
  @quiet: print no messages
  @must_chk_crc: indicates whether to always check the CRC
  This function checks node magic number and CRC checksum. This function also
  validates node length to prevent UBIFS from becoming crazy when an attacker
  feeds it a file-system image with incorrect nodes. For example, too large
  node length in the common header could cause UBIFS to read memory outside of
  allocated buffer when checking the CRC checksum.
  This function may skip data nodes CRC checking if @c->no_chk_data_crc is
  true, which is controlled by corresponding UBIFS mount option. However, if
  @must_chk_crc is true, then @c->no_chk_data_crc is ignored and CRC is
  checked. Similarly, if @c->mounting or @c->remounting_rw is true (we are
  mounting or re-mounting to RW mode), @c->no_chk_data_crc is ignored and CRC
  is checked. This is because during mounting or re-mounting from RO mode to
  RW mode we may read journal nodes (when replying the journal or doing the
  recovery) and the journal nodes may potentially be corrupted, so checking is
  required.
  This function returns zero in case of success and %-EUCLEAN in case of bad
  CRC or magic.
  ubifs_pad - pad flash space.
  @c: UBIFS file-system description object
  @buf: buffer to put padding to
  @pad: how many bytes to pad
  The flash media obliges us to write only in chunks of %c->min_io_size and
  when we have to write less data we add padding node to the write-buffer and
  pad it to the next minimal IO unit's boundary. Padding nodes help when the
  media is being scanned. If the amount of wasted space is not enough to fit a
  padding node which takes %UBIFS_PAD_NODE_SZ bytes, we write padding bytes
  pattern (%UBIFS_PADDING_BYTE).
  Padding nodes are also used to fill gaps when the "commit-in-gaps" method is
  used.
 Too little space, padding node won't fit 
  next_sqnum - get next sequence number.
  @c: UBIFS file-system description object
  ubifs_prepare_node_hmac - prepare node to be written to flash.
  @c: UBIFS file-system description object
  @node: the node to pad
  @len: node length
  @hmac_offs: offset of the HMAC in the node
  @pad: if the buffer has to be padded
  This function prepares node at @node to be written to the media - it
  calculates node CRC, fills the common header, and adds proper padding up to
  the next minimum IO unit if @pad is not zero. if @hmac_offs is positive then
  a HMAC is inserted into the node at the given offset.
  This function returns 0 for success or a negative error code otherwise.
  ubifs_prepare_node - prepare node to be written to flash.
  @c: UBIFS file-system description object
  @node: the node to pad
  @len: node length
  @pad: if the buffer has to be padded
  This function prepares node at @node to be written to the media - it
  calculates node CRC, fills the common header, and adds proper padding up to
  the next minimum IO unit if @pad is not zero.
	
	  Deliberately ignore return value since this function can only fail
	  when a hmac offset is given.
  ubifs_prep_grp_node - prepare node of a group to be written to flash.
  @c: UBIFS file-system description object
  @node: the node to pad
  @len: node length
  @last: indicates the last node of the group
  This function prepares node at @node to be written to the media - it
  calculates node CRC and fills the common header.
  wbuf_timer_callback - write-buffer timer callback function.
  @timer: timer data (write-buffer descriptor)
  This function is called when the write-buffer timer expires.
  new_wbuf_timer - start new write-buffer timer.
  @c: UBIFS file-system description object
  @wbuf: write-buffer descriptor
 centi to milli, milli to nano, then 10% 
  cancel_wbuf_timer - cancel write-buffer timer.
  @wbuf: write-buffer descriptor
  ubifs_wbuf_sync_nolock - synchronize write-buffer.
  @wbuf: write-buffer to synchronize
  This function synchronizes write-buffer @buf and returns zero in case of
  success or a negative error code in case of failure.
  Note, although write-buffers are of @c->max_write_size, this function does
  not necessarily writes all @c->max_write_size bytes to the flash. Instead,
  if the write-buffer is only partially filled with data, only the used part
  of the write-buffer (aligned on @c->min_io_size boundary) is synchronized.
  This way we waste less space.
 Write-buffer is empty or not seeked 
	
	  Do not write whole write buffer but write only the minimum necessary
	  amount of min. IO units.
	
	  Now @wbuf->offs is not necessarily aligned to @c->max_write_size.
	  But our goal is to optimize writes and make sure we write in
	  @c->max_write_size chunks and to @c->max_write_size-aligned offset.
	  Thus, if @wbuf->offs is not aligned to @c->max_write_size now, make
	  sure that @wbuf->offs + @wbuf->size is aligned to
	  @c->max_write_size. This way we make sure that after next
	  write-buffer flush we are again at the optimal offset (aligned to
	  @c->max_write_size).
  ubifs_wbuf_seek_nolock - seek write-buffer.
  @wbuf: write-buffer
  @lnum: logical eraseblock number to seek to
  @offs: logical eraseblock offset to seek to
  This function targets the write-buffer to logical eraseblock @lnum:@offs.
  The write-buffer has to be empty. Returns zero in case of success and a
  negative error code in case of failure.
  ubifs_bg_wbufs_sync - synchronize write-buffers.
  @c: UBIFS file-system description object
  This function is called by background thread to synchronize write-buffers.
  Returns zero in case of success and a negative error code in case of
  failure.
		
		  If the mutex is locked then wbuf is being changed, so
		  synchronization is not necessary.
 Cancel all timers to prevent repeated errors 
  ubifs_wbuf_write_nolock - write data to flash via write-buffer.
  @wbuf: write-buffer
  @buf: node to write
  @len: node length
  This function writes data to flash via write-buffer @wbuf. This means that
  the last piece of the node won't reach the flash media immediately if it
  does not take whole max. write unit (@c->max_write_size). Instead, the node
  will sit in RAM until the write-buffer is synchronized (e.g., by timer, or
  because more data are appended to the write-buffer).
  This function returns zero in case of success and a negative error code in
  case of failure. If the node cannot be written because there is no more
  space in this logical eraseblock, %-ENOSPC is returned.
		
		  The node is not very large and fits entirely within
		  write-buffer.
		
		  The node is large enough and does not fit entirely within
		  current available space. We have to fill and flush
		  write-buffer and switch to the next max. write unit.
		
		  The write-buffer offset is not aligned to
		  @c->max_write_size and @wbuf->size is less than
		  @c->max_write_size. Write @wbuf->size bytes to make sure the
		  following writes are done in optimal @c->max_write_size
		  chunks.
	
	  The remaining data may take more whole max. write units, so write the
	  remains multiple to max. write unit size directly to the flash media.
	  We align node length to 8-byte boundary because we anyway flash wbuf
	  if the remaining space is less than 8 bytes.
		
		  And now we have what's left and what does not take whole
		  max. write unit, so write it to the write-buffer and we are
		  done.
  ubifs_write_node_hmac - write node to the media.
  @c: UBIFS file-system description object
  @buf: the node to write
  @len: node length
  @lnum: logical eraseblock number
  @offs: offset within the logical eraseblock
  @hmac_offs: offset of the HMAC within the node
  This function automatically fills node magic number, assigns sequence
  number, and calculates node CRC checksum. The length of the @buf buffer has
  to be aligned to the minimal IO unit size. This function automatically
  appends padding node and padding bytes if needed. Returns zero in case of
  success and a negative error code in case of failure.
  ubifs_write_node - write node to the media.
  @c: UBIFS file-system description object
  @buf: the node to write
  @len: node length
  @lnum: logical eraseblock number
  @offs: offset within the logical eraseblock
  This function automatically fills node magic number, assigns sequence
  number, and calculates node CRC checksum. The length of the @buf buffer has
  to be aligned to the minimal IO unit size. This function automatically
  appends padding node and padding bytes if needed. Returns zero in case of
  success and a negative error code in case of failure.
  ubifs_read_node_wbuf - read node from the media or write-buffer.
  @wbuf: wbuf to check for un-written data
  @buf: buffer to read to
  @type: node type
  @len: node length
  @lnum: logical eraseblock number
  @offs: offset within the logical eraseblock
  This function reads a node of known type and length, checks it and stores
  in @buf. If the node partially or fully sits in the write-buffer, this
  function takes data from the buffer, otherwise it reads the flash media.
  Returns zero in case of success, %-EUCLEAN if CRC mismatched and a negative
  error code in case of failure.
 We may safely unlock the write-buffer and read the data 
 Don't read under wbuf 
 Copy the rest from the write-buffer 
 Read everything that goes before write-buffer 
  ubifs_read_node - read node.
  @c: UBIFS file-system description object
  @buf: buffer to read to
  @type: node type
  @len: node length (not aligned)
  @lnum: logical eraseblock number
  @offs: offset within the logical eraseblock
  This function reads a node of known type and length, checks it and
  stores in @buf. Returns zero in case of success, %-EUCLEAN if CRC mismatched
  and a negative error code in case of failure.
  ubifs_wbuf_init - initialize write-buffer.
  @c: UBIFS file-system description object
  @wbuf: write-buffer to initialize
  This function initializes write-buffer. Returns zero in case of success
  %-ENOMEM in case of failure.
	
	  If the LEB starts at the max. write size aligned address, then
	  write-buffer size has to be set to @c->max_write_size. Otherwise,
	  set it to something smaller so that it ends at the closest max.
	  write size boundary.
  ubifs_wbuf_add_ino_nolock - add an inode number into the wbuf inode array.
  @wbuf: the write-buffer where to add
  @inum: the inode number
  This function adds an inode number to the inode array of the write-buffer.
 NOR flash or something similar 
  wbuf_has_ino - returns if the wbuf contains data from the inode.
  @wbuf: the write-buffer
  @inum: the inode number
  This function returns with %1 if the write-buffer contains some data from the
  given inode otherwise it returns with %0.
  ubifs_sync_wbufs_by_inode - synchronize write-buffers for an inode.
  @c: UBIFS file-system description object
  @inode: inode to synchronize
  This function synchronizes write-buffers which contain nodes belonging to
  @inode. Returns zero in case of success and a negative error code in case of
  failure.
			
			  GC head is special, do not look at it. Even if the
			  head contains something related to this inode, it is
			  a _copy_ of corresponding on-flash node which sits
			  somewhere else.
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Artem Bityutskiy ( )
           Adrian Hunter
  This file implements UBIFS initialization and VFS superblock operations. Some
  initialization stuff which is rather large and complex is placed at
  corresponding subsystems, but most of it is here.
  Maximum amount of memory we may 'kmalloc()' without worrying that we are
  allocating too much.
 Slab cache for UBIFS inodes 
 UBIFS TNC shrinker description 
  validate_inode - validate inode.
  @c: UBIFS file-system description object
  @inode: the inode to validate
  This is a helper function for 'ubifs_iget()' which validates various fields
  of a newly built inode to make sure they contain sane values and prevent
  possible vulnerabilities. Returns zero if the inode is all right and
  a non-zero error code if not.
  Note, Linux write-back code calls this without 'i_mutex'.
	
	  Due to races between write-back forced by budgeting
	  (see 'sync_some_inodes()') and background write-back, the inode may
	  have already been synchronized, do not do this again. This might
	  also happen if it was synchronized in an VFS operation, e.g.
	  'ubifs_link()'.
	
	  As an optimization, do not write orphan inodes to the media just
	  because this is not needed.
		
		  Extended attribute inode deletions are fully handled in
		  'ubifs_removexattr()'. These inodes are special and have
		  limited usage, so there is nothing to do here.
		
		  Worst case we have a lost orphan inode wasting space, so a
		  simple error message is OK here.
 We've deleted something - clean the "no space" flags 
	
	  Zero @wait is just an advisory thing to help the file system shove
	  lots of data into the queues, and there will be the second
	  '->sync_fs()' call, with non-zero @wait.
	
	  Synchronize write buffers, because 'ubifs_run_commit()' does not
	  do this if it waits for an already running commit.
	
	  Strictly speaking, it is not necessary to commit the journal here,
	  synchronizing write-buffers would be enough. But committing makes
	  UBIFS free space predictions much more accurate, so we want to let
	  the user be able to get more accurate results of 'statfs()' after
	  they synchronize the file system.
  init_constants_early - initialize UBIFS constants.
  @c: UBIFS file-system description object
  This function initialize UBIFS constants which do not need the superblock to
  be read. It also checks that the UBI volume satisfies basic UBIFS
  requirements. Returns zero in case of success and a negative error code in
  case of failure.
	
	  Maximum write size has to be greater or equivalent to min. IO
	  size, and be multiple of min. IO size.
	
	  UBIFS aligns all node to 8-byte boundary, so to make function in
	  io.c simpler, assume minimum IO unit size to be 8 bytes if it is
	  less than 8.
	
	  Initialize node length ranges which are mostly needed for node
	  length validation.
	
	  Minimum indexing node size is amended later when superblock is
	  read and the key length is known.
	
	  Maximum indexing node size is amended later when superblock is
	  read and the fanout is known.
	
	  Initialize dead and dark LEB space watermarks. See gc.c for comments
	  about these values.
	
	  Calculate how many bytes would be wasted at the end of LEB if it was
	  fully filled with data nodes of maximum size. This is used in
	  calculations when reporting free space.
 Buffer size for bulk-reads 
 Log is ready, preserve one LEB for commits. 
  bud_wbuf_callback - bud LEB write-buffer synchronization call-back.
  @c: UBIFS file-system description object
  @lnum: LEB the write-buffer was synchronized to
  @free: how many free bytes left in this LEB
  @pad: how many bytes were padded
  This is a callback function which is called by the IO unit when the
  write-buffer is synchronized. We need this to correctly maintain space
  accounting in bud logical eraseblocks. This function returns zero in case of
  success and a negative error code in case of failure.
  This function actually belongs to the journal, but we keep it here because
  we want to keep it static.
  init_constants_sb - initialize UBIFS constants.
  @c: UBIFS file-system description object
  This is a helper function which initializes various UBIFS constants after
  the superblock has been read. It also checks various UBIFS parameters and
  makes sure they are all right. Returns zero in case of success and a
  negative error code in case of failure.
 Make sure LEB size is large enough to fit full commit 
	
	  Make sure that the log is large enough to fit reference nodes for
	  all buds plus one reserved LEB.
	
	  When budgeting we assume worst-case scenarios when the pages are not
	  be compressed and direntries are of the maximum size.
	 
	  Note, data, which may be stored in inodes is budgeted separately, so
	  it is not included into 'c->bi.inode_budget'.
	
	  When the amount of flash space used by buds becomes
	  'c->max_bud_bytes', UBIFS just blocks all writers and starts commit.
	  The writers are unblocked when the commit is finished. To avoid
	  writers to be blocked UBIFS initiates background commit in advance,
	  when number of bud bytes becomes above the limit defined below.
	
	  Ensure minimum journal size. All the bytes in the journal heads are
	  considered to be used, when calculating the current journal usage.
	  Consequently, if the journal is too small, UBIFS will treat it as
	  always full.
 Initialize effective LEB size used in budgeting calculations 
  init_constants_master - initialize UBIFS constants.
  @c: UBIFS file-system description object
  This is a helper function which initializes various UBIFS constants after
  the master node has been read. It also checks various UBIFS parameters and
  makes sure they are all right.
	
	  Calculate total amount of FS blocks. This number is not used
	  internally because it does not make much sense for UBIFS, but it is
	  necessary to report something for the 'statfs()' call.
	 
	  Subtract the LEB reserved for GC, the LEB which is reserved for
	  deletions, minimum LEBs for the index, and assume only one journal
	  head is available.
  take_gc_lnum - reserve GC LEB.
  @c: UBIFS file-system description object
  This function ensures that the LEB reserved for garbage collection is marked
  as "taken" in lprops. We also have to set free space to LEB size and dirty
  space to zero, because lprops may contain out-of-date information if the
  file-system was un-mounted before it has been committed. This function
  returns zero in case of success and a negative error code in case of
  failure.
 And we have to tell lprops that this LEB is taken 
  alloc_wbufs - allocate write-buffers.
  @c: UBIFS file-system description object
  This helper function allocates and initializes UBIFS write-buffers. Returns
  zero in case of success and %-ENOMEM in case of failure.
 Initialize journal heads 
	
	  Garbage Collector head does not need to be synchronized by timer.
	  Also GC head nodes are not grouped.
  free_wbufs - free write-buffers.
  @c: UBIFS file-system description object
  free_orphans - free orphans.
  @c: UBIFS file-system description object
  free_buds - free per-bud objects.
  @c: UBIFS file-system description object
  check_volume_empty - check if the UBI volume is empty.
  @c: UBIFS file-system description object
  This function checks if the UBIFS volume is empty by looking if its LEBs are
  mapped or not. The result of checking is stored in the @c->empty variable.
  Returns zero in case of success and a negative error code in case of
  failure.
  UBIFS mount options.
  Opt_fast_unmount: do not run a journal commit before un-mounting
  Opt_norm_unmount: run a journal commit before un-mounting
  Opt_bulk_read: enable bulk-reads
  Opt_no_bulk_read: disable bulk-reads
  Opt_chk_data_crc: check CRCs when reading data nodes
  Opt_no_chk_data_crc: do not check CRCs when reading data nodes
  Opt_override_compr: override default compressor
  Opt_assert: set ubifs_assert() action
  Opt_auth_key: The key name used for authentication
  Opt_auth_hash_name: The hash type used for authentication
  Opt_err: just end of array marker
  parse_standard_option - parse a standard mount option.
  @option: the option to parse
  Normally, standard mount options like "sync" are passed to file-systems as
  flags. However, when a "rootflags=" kernel boot parameter is used, they may
  be present in the options string. This function tries to deal with this
  situation and parse standard options. Returns 0 if the option was not
  recognized, and the corresponding integer flag if it was.
  UBIFS is only interested in the "sync" option, so do not check for anything
  else.
  ubifs_parse_options - parse mount parameters.
  @c: UBIFS file-system description object
  @options: parameters to parse
  @is_remount: non-zero if this is FS re-mount
  This function parses UBIFS mount options and returns zero in case success
  and a negative error code in case of failure.
		
		  %Opt_fast_unmount and %Opt_norm_unmount options are ignored.
		  We accept them in order to be backward-compatible. But this
		  should be removed at some point.
FIXME: is c ready?
  ubifs_release_options - release mount parameters which have been dumped.
  @c: UBIFS file-system description object
  destroy_journal - destroy journal data structures.
  @c: UBIFS file-system description object
  This function destroys journal data structures including those that may have
  been created by recovery functions.
  bu_init - initialize bulk-read information.
  @c: UBIFS file-system description object
 Already initialized 
 Just disable bulk-read 
  check_free_space - check if there is enough free space to mount.
  @c: UBIFS file-system description object
  This function makes sure UBIFS has enough free space to be mounted in
  readwrite mode. UBIFS must always have some free space to allow deletions.
  mount_ubifs - mount UBIFS file-system.
  @c: UBIFS file-system description object
  This function mounts UBIFS file system. Returns zero in case of success and
  a negative error code in case of failure.
 Suppress error messages while probing if SB_SILENT is set 
		
		  This UBI volume is empty, and read-only, or the file system
		  is mounted read-only - we cannot format it.
	
	  The requirement for the buffer is that it should fit indexing B-tree
	  height amount of integers. We assume the height if the TNC tree will
	  never exceed 64.
	
	  Make sure the compressor which is set as default in the superblock
	  or overridden by mount options is actually compiled in.
 Create background thread 
		
		  Set the "dirty" flag so that if we reboot uncleanly we
		  will notice this immediately on the next mount.
	
	  Handle offline signed images: Now that the master node is
	  written and its validation no longer depends on the hash
	  in the superblock, we can update the offline signed
	  superblock with a HMAC version,
 Calculate 'min_idx_lebs' after journal replay 
 Check for enough log space 
			
			  GC LEB may contain garbage if there was an unclean
			  reboot, and it should be un-mapped.
		
		  Even if we mount read-only, we have to set space in GC LEB
		  to proper value because this affects UBIFS free space
		  reporting. We do not want to have a situation when
		  re-mounting from RO to RW changes amount of free space.
			
			  GC LEB has to be empty and taken at this point. But
			  the journal head LEBs may also be accounted as
			  "empty taken" if they are empty.
  ubifs_umount - un-mount UBIFS file-system.
  @c: UBIFS file-system description object
  Note, this function is called to free allocated resourced when un-mounting,
  as well as free resources when an error occurred while we were half way
  through mounting (error path cleanup function). So it has to make sure the
  resource was actually allocated before freeing it.
  ubifs_remount_rw - re-mount in read-write mode.
  @c: UBIFS file-system description object
  UBIFS avoids allocating many unnecessary resources when mounted in read-only
  mode. This function allocates the needed resources and re-mounts UBIFS in
  read-write mode.
 A readonly mount is not allowed to have orphans 
 Create background thread 
 Check for enough log space 
		
		  Do not run the debugging space check if the were doing
		  recovery, because when we saved the information we had the
		  file-system in a state where the TNC and lprops has been
		  modified in memory, but all the IO operations (including a
		  commit) were deferred. So the file-system was in
		  "non-committed" state. Now the file-system is in committed
		  state, and of course the amount of free space will change
		  because, for example, the old index size was imprecise.
  ubifs_remount_ro - re-mount in read-only mode.
  @c: UBIFS file-system description object
  We assume VFS has stopped writing. Possibly the background thread could be
  running a commit, however kthread_stop will wait in that case.
	
	  The following asserts are only valid if there has not been a failure
	  of the media. For example, there will be dirty inodes if we failed
	  to write them back because of IO errors.
	
	  The 'c->umount_lock' prevents races between UBIFS memory shrinker
	  and file system un-mount. Namely, it prevents the shrinker from
	  picking this superblock for shrinking - it will be just skipped if
	  the mutex is locked.
		
		  First of all kill the background thread to make sure it does
		  not interfere with un-mounting and freeing resources.
		
		  On fatal errors c->ro_error is set to 1, in which case we do
		  not write the master node.
 Synchronize write-buffers 
			
			  We are being cleanly unmounted which means the
			  orphans were killed - indicate this in the master
			  node. Also save the reserved GC LEB number.
				
				  Recovery will attempt to fix the master area
				  next mount, so we just print a message and
				  continue to unmount normally.
 Make sure write-buffer timers are canceled 
  open_ubi - parse UBI device name string and open the UBI device.
  @name: UBI volume name
  @mode: UBI volume open mode
  The primary method of mounting UBIFS is by specifying the UBI volume
  character device node path. However, UBIFS may also be mounted without any
  character device node using one of the following methods:
  o ubiX_Y    - mount UBI device number X, volume Y;
  o ubiY      - mount UBI device number 0, volume Y;
  o ubiX:NAME - mount UBI device X, volume with name NAME;
  o ubi:NAME  - mount UBI device 0, volume with name NAME.
  Alternative '!' separator may be used instead of ':' (because some shells
  like busybox may interpret ':' as an NFS host name separator). This function
  returns UBI volume description object in case of success and a negative
  error code in case of failure.
 First, try to open using the device node path method 
 Try the "nodev" method 
 ubi:NAME method 
 ubiY method 
 ubiX_Y method 
 ubiX:NAME method 
 Re-open the UBI device in read-write mode 
	
	  UBIFS provides 'backing_dev_info' in order to disable read-ahead. For
	  UBIFS, IO is not deferred, it is done immediately in readpage,
	  which means the user would have to wait not just for their own IO
	  but the read-ahead IO as well i.e. completely pointless.
	 
	  Read-ahead will be disabled because @sb->s_bdi->ra_pages is 0. Also
	  @sb->s_bdi->capabilities are initialized to 0 so there won't be any
	  writeback happening.
 Read the root inode 
	
	  Get UBI device number and volume ID. Mount it read-only so far
	  because this might be a new mount point, and UBI allows only one
	  read-write user at a time.
 A new mount point for already mounted UBIFS 
 We do not support atime 
 'fill_super()' opens ubi again so we must close it here 
  Inode slab cache constructor.
 Make sure node sizes are 8-byte aligned 
 Check min. node size 
 Defined node sizes 
	
	  We use 2 bit wide bit-fields to store compression type, which should
	  be amended if more compressors are added. The bit-fields are:
	  @compr_type in 'struct ubifs_inode', @default_compr in
	  'struct ubifs_info' and @compr_type in 'struct ubifs_mount_opts'.
	
	  We require that PAGE_SIZE is greater-than-or-equal-to
	  UBIFS_BLOCK_SIZE. It is assumed that both are powers of 2.
 late_initcall to let compressors initialize first 
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 SPDX-License-Identifier: GPL-2.0
  This file is part of UBIFS.
  Copyright (C) 2018 Pengutronix, Sascha Hauer <s.hauer@pengutronix.de>
  This file implements various helper functions for UBIFS authentication support
  ubifs_node_calc_hash - calculate the hash of a UBIFS node
  @c: UBIFS file-system description object
  @node: the node to calculate a hash for
  @hash: the returned hash
  Returns 0 for success or a negative error code otherwise.
  ubifs_hash_calc_hmac - calculate a HMAC from a hash
  @c: UBIFS file-system description object
  @hash: the node to calculate a HMAC for
  @hmac: the returned HMAC
  Returns 0 for success or a negative error code otherwise.
  ubifs_prepare_auth_node - Prepare an authentication node
  @c: UBIFS file-system description object
  @node: the node to calculate a hash for
  @inhash: input hash of previous nodes
  This function prepares an authentication node for writing onto flash.
  It creates a HMAC from the given input hash and writes it to the node.
  Returns 0 for success or a negative error code otherwise.
  __ubifs_hash_get_desc - get a descriptor suitable for hashing a node
  @c: UBIFS file-system description object
  This function returns a descriptor suitable for hashing a node. Free after use
  with kfree.
  ubifs_bad_hash - Report hash mismatches
  @c: UBIFS file-system description object
  @node: the node
  @hash: the expected hash
  @lnum: the LEB @node was read from
  @offs: offset in LEB @node was read from
  This function reports a hash mismatch when a node has a different hash than
  expected.
  __ubifs_node_check_hash - check the hash of a node against given hash
  @c: UBIFS file-system description object
  @node: the node
  @expected: the expected hash
  This function calculates a hash over a node and compares it to the given hash.
  Returns 0 if both hashes are equal or authentication is disabled, otherwise a
  negative error code is returned.
  ubifs_sb_verify_signature - verify the signature of a superblock
  @c: UBIFS file-system description object
  @sup: The superblock node
  To support offline signed images the superblock can be signed with a
  PKCS#7 signature. The signature is placed directly behind the superblock
  node in an ubifs_sig_node.
  Returns 0 when the signature can be successfully verified or a negative
  error code if not.
  ubifs_init_authentication - initialize UBIFS authentication support
  @c: UBIFS file-system description object
  This function returns 0 for success or a negative error code otherwise.
 key was revoked before we acquired its semaphore 
  __ubifs_exit_authentication - release resource
  @c: UBIFS file-system description object
  This function releases the authentication related resources.
  ubifs_node_calc_hmac - calculate the HMAC of a UBIFS node
  @c: UBIFS file-system description object
  @node: the node to insert a HMAC into.
  @len: the length of the node
  @ofs_hmac: the offset in the node where the HMAC is inserted
  @hmac: returned HMAC
  This function calculates a HMAC of a UBIFS node. The HMAC is expected to be
  embedded into the node, so this area is not covered by the HMAC. Also not
  covered is the UBIFS_NODE_MAGIC and the CRC of the node.
 behind common node header CRC up to HMAC begin 
 behind HMAC, if any 
  __ubifs_node_insert_hmac - insert a HMAC into a UBIFS node
  @c: UBIFS file-system description object
  @node: the node to insert a HMAC into.
  @len: the length of the node
  @ofs_hmac: the offset in the node where the HMAC is inserted
  This function inserts a HMAC at offset @ofs_hmac into the node given in
  @node.
  This function returns 0 for success or a negative error code otherwise.
  __ubifs_node_verify_hmac - verify the HMAC of UBIFS node
  @c: UBIFS file-system description object
  @node: the node to insert a HMAC into.
  @len: the length of the node
  @ofs_hmac: the offset in the node where the HMAC is inserted
  This function verifies the HMAC at offset @ofs_hmac of the node given in
  @node. Returns 0 if successful or a negative error code otherwise.
  ubifs_hmac_wkm - Create a HMAC of the well known message
  @c: UBIFS file-system description object
  @hmac: The HMAC of the well known message
  This function creates a HMAC of a well known message. This is used
  to check if the provided key is suitable to authenticate a UBIFS
  image. This is only a convenience to the user to provide a better
  error message when the wrong key is provided.
  This function returns 0 for success or a negative error code otherwise.
  ubifs_hmac_zero - test if a HMAC is zero
  @c: UBIFS file-system description object
  @hmac: the HMAC to test
  This function tests if a HMAC is zero and returns true if it is
  and false otherwise.
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Artem Bityutskiy ( )
           Adrian Hunter
  This file implements UBIFS extended attributes support.
  Extended attributes are implemented as regular inodes with attached data,
  which limits extended attribute size to UBIFS block size (4KiB). Names of
  extended attributes are described by extended attribute entries (xentries),
  which are almost identical to directory entries, but have different key type.
  In other words, the situation with extended attributes is very similar to
  directories. Indeed, any inode (but of course not xattr inodes) may have a
  number of associated xentries, just like directory inodes have associated
  directory entries. Extended attribute entries store the name of the extended
  attribute, the host inode number, and the extended attribute inode number.
  Similarly, direntries store the name, the parent and the target inode
  numbers. Thus, most of the common UBIFS mechanisms may be re-used for
  extended attributes.
  The number of extended attributes is not limited, but there is Linux
  limitation on the maximum possible size of the list of all extended
  attributes associated with an inode (%XATTR_LIST_MAX), so UBIFS makes sure
  the sum of all extended attribute names of the inode does not exceed that
  limit.
  Extended attributes are synchronous, which means they are written to the
  flash media synchronously and there is no write-back for extended attribute
  inodes. The extended attribute values are not stored in compressed form on
  the media.
  Since extended attributes are represented by regular inodes, they are cached
  in the VFS inode cache. The xentries are cached in the LNC cache (see
  tnc.c).
  ACL support is not implemented.
  Extended attribute type constants.
  USER_XATTR: user extended attribute ("user.")
  TRUSTED_XATTR: trusted extended attribute ("trusted.)
  SECURITY_XATTR: security extended attribute ("security.")
  create_xattr - create an extended attribute.
  @c: UBIFS file-system description object
  @host: host inode
  @nm: extended attribute name
  @value: extended attribute value
  @size: size of extended attribute value
  This is a helper function which creates an extended attribute of name @nm
  and value @value for inode @host. The host inode is also updated on flash
  because the ctime and extended attribute accounting data changes. This
  function returns zero in case of success and a negative error code in case
  of failure.
	
	  Linux limits the maximum size of the extended attribute names list
	  to %XATTR_LIST_MAX. This means we should not allow creating more
	  extended attributes if the name list becomes larger. This limitation
	  is artificial for UBIFS, though.
 Re-define all operations to be "nothing" 
	
	  We handle UBIFS_XATTR_NAME_ENCRYPTION_CONTEXT here because we
	  have to set the UBIFS_CRYPT_FL flag on the host inode.
	  To avoid multiple updates of the same inode in the same operation,
	  let's do it here.
  change_xattr - change an extended attribute.
  @c: UBIFS file-system description object
  @host: host inode
  @inode: extended attribute inode
  @value: extended attribute value
  @size: size of extended attribute value
  This helper function changes the value of extended attribute @inode with new
  data from @value. Returns zero in case of success and a negative error code
  in case of failure.
	
	  It is important to write the host inode after the xattr inode
	  because if the host inode gets synchronized (via 'fsync()'), then
	  the extended attribute inode gets synchronized, because it goes
	  before the host inode in the write-buffer.
	
	  The extended attribute entries are stored in LNC, so multiple
	  look-ups do not involve reading the flash.
 We are asked not to create the xattr 
 We are asked not to replace the xattr 
 If @buf is %NULL we are supposed to return the length 
 File encryption related xattrs are for internal use only 
 Show trusted namespace only for "power" users 
		
		  We should return the minimum buffer size which will fit a
		  null-terminated list of all the extended attribute names.
  ubifs_evict_xattr_inode - Evict an xattr inode.
  @c: UBIFS file-system description object
  @xattr_inum: xattr inode number
  When an inode that hosts xattrs is being removed we have to make sure
  that cached inodes of the xattrs also get removed from the inode cache
  otherwise we'd waste memory. This function looks up an inode from the
  inode cache and clears the link counter such that iput() will evict
  the inode.
 If @i_nlink is 0, 'iput()' will delete the inode 
		
		  creating a new inode without holding the inode rwsem,
		  no need to check whether inode is locked.
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
  This file implements garbage collection. The procedure for garbage collection
  is different depending on whether a LEB as an index LEB (contains index
  nodes) or not. For non-index LEBs, garbage collection finds a LEB which
  contains a lot of dirty space (obsolete nodes), and copies the non-obsolete
  nodes to the journal, at which point the garbage-collected LEB is free to be
  reused. For index LEBs, garbage collection marks the non-obsolete index nodes
  dirty in the TNC, and after the next commit, the garbage-collected LEB is
  to be reused. Garbage collection will cause the number of dirty index nodes
  to grow, however sufficient space is reserved for the index to ensure the
  commit will never run out of space.
  Notes about dead watermark. At current UBIFS implementation we assume that
  LEBs which have less than @c->dead_wm bytes of free + dirty space are full
  and not worth garbage-collecting. The dead watermark is one min. IO unit
  size, or min. UBIFS node size, depending on what is greater. Indeed, UBIFS
  Garbage Collector has to synchronize the GC head's write buffer before
  returning, so this is about wasting one min. IO unit. However, UBIFS GC can
  actually reclaim even very small pieces of dirty space by garbage collecting
  enough dirty LEBs, but we do not bother doing this at this implementation.
  Notes about dark watermark. The results of GC work depends on how big are
  the UBIFS nodes GC deals with. Large nodes make GC waste more space. Indeed,
  if GC move data from LEB A to LEB B and nodes in LEB A are large, GC would
  have to waste large pieces of free space at the end of LEB B, because nodes
  from LEB A would not fit. And the worst situation is when all nodes are of
  maximum size. So dark watermark is the amount of free + dirty space in LEB
  which are guaranteed to be reclaimable. If LEB has less space, the GC might
  be unable to reclaim it. So, LEBs with free + dirty greater than dark
  watermark are "good" LEBs from GC's point of view. The other LEBs are not so
  good, and GC takes extra care when moving them.
  GC may need to move more than one LEB to make progress. The below constants
  define "soft" and "hard" limits on the number of LEBs the garbage collector
  may move.
  switch_gc_head - switch the garbage collection journal head.
  @c: UBIFS file-system description object
  This function switch the GC head to the next LEB which is reserved in
  @c->gc_lnum. Returns %0 in case of success, %-EAGAIN if commit is required,
  and other negative error code in case of failures.
	
	  The GC write-buffer was synchronized, we may safely unmap
	  'c->gc_lnum'.
  data_nodes_cmp - compare 2 data nodes.
  @priv: UBIFS file-system description object
  @a: first data node
  @b: second data node
  This function compares data nodes @a and @b. Returns %1 if @a has greater
  inode or block number, and %-1 otherwise.
  nondata_nodes_cmp - compare 2 non-data nodes.
  @priv: UBIFS file-system description object
  @a: first node
  @a: second node
  This function compares nodes @a and @b. It makes sure that inode nodes go
  first and sorted by length in descending order. Directory entry nodes go
  after inode nodes and are sorted in ascending hash valuer order.
 Inodes go before directory entries 
  sort_nodes - sort nodes for GC.
  @c: UBIFS file-system description object
  @sleb: describes nodes to sort and contains the result on exit
  @nondata: contains non-data nodes on exit
  @min: minimum node size is returned here
  This function sorts the list of inodes to garbage collect. First of all, it
  kills obsolete nodes and separates data and non-data nodes to the
  @sleb->nodes and @nondata lists correspondingly.
  Data nodes are then sorted in block number order - this is important for
  bulk-read; data nodes with lower inode number go before data nodes with
  higher inode number, and data nodes with lower block number go before data
  nodes with higher block number;
  Non-data nodes are sorted as follows.
    o First go inode nodes - they are sorted in descending length order.
    o Then go directory entry nodes - they are sorted in hash order, which
      should supposedly optimize 'readdir()'. Direntry nodes with lower parent
      inode number go before direntry nodes with higher parent inode number,
      and direntry nodes with lower name hash values go before direntry nodes
      with higher name hash values.
  This function returns zero in case of success and a negative error code in
  case of failure.
 Separate data nodes and non-data nodes 
 Probably truncation node, zap it 
 The node is obsolete, remove it from the list 
 Sort data and non-data nodes 
  move_node - move a node.
  @c: UBIFS file-system description object
  @sleb: describes the LEB to move nodes from
  @snod: the mode to move
  @wbuf: write-buffer to move node to
  This function moves node @snod to @wbuf, changes TNC correspondingly, and
  destroys @snod. Returns zero in case of success and a negative error code in
  case of failure.
  move_nodes - move nodes.
  @c: UBIFS file-system description object
  @sleb: describes the LEB to move nodes from
  This function moves valid nodes from data LEB described by @sleb to the GC
  journal head. This function returns zero in case of success, %-EAGAIN if
  commit is required, and other negative error codes in case of other
  failures.
		
		  The GC journal head is not set, because it is the first GC
		  invocation since mount.
 Write nodes to their new location. Use the first-fit strategy 
 Move data nodes 
				
				  Do not skip data nodes in order to optimize
				  bulk-read.
 Move non-data nodes 
				
				  Keep going only if this is an inode with
				  some data. Otherwise stop and switch the GC
				  head. IOW, we assume that data-less inode
				  nodes and direntry nodes are roughly of the
				  same size.
		
		  Waste the rest of the space in the LEB and switch to the
		  next LEB.
  gc_sync_wbufs - sync write-buffers for GC.
  @c: UBIFS file-system description object
  We must guarantee that obsoleting nodes are on flash. Unfortunately they may
  be in a write-buffer instead. That is, a node could be written to a
  write-buffer, obsoleting another node in a LEB that is GC'd. If that LEB is
  erased before the write-buffer is sync'd and then there is an unclean
  unmount, then an existing node is lost. To avoid this, we sync all
  write-buffers.
  This function returns %0 on success or a negative error code on failure.
  ubifs_garbage_collect_leb - garbage-collect a logical eraseblock.
  @c: UBIFS file-system description object
  @lp: describes the LEB to garbage collect
  This function garbage-collects an LEB and returns one of the @LEB_FREED,
  @LEB_RETAINED, etc positive codes in case of success, %-EAGAIN if commit is
  required, and other negative error codes in case of failures.
 Special case - a free LEB  
			
			  Write buffers must be sync'd before unmapping
			  freeable LEBs, because one of them may contain data
			  which obsoletes something in 'lp->lnum'.
	
	  We scan the entire LEB even though we only really need to scan up to
	  (c->leb_size - lp->free).
		
		  Don't release the LEB until after the next commit, because
		  it may contain data which is needed for recovery. So
		  although we freed this LEB, it will become usable only after
		  the commit.
 Allow for races with TNC 
 We may have moved at least some nodes so allow for races with TNC 
  ubifs_garbage_collect - UBIFS garbage collector.
  @c: UBIFS file-system description object
  @anyway: do GC even if there are free LEBs
  This function does out-of-place garbage collection. The return codes are:
    o positive LEB number if the LEB has been freed and may be used;
    o %-EAGAIN if the caller has to run commit;
    o %-ENOSPC if GC failed to make any progress;
    o other negative error codes in case of other errors.
  Garbage collector writes data to the journal when GC'ing data LEBs, and just
  marking indexing nodes dirty when GC'ing indexing LEBs. Thus, at some point
  commit may be required. But commit cannot be run from inside GC, because the
  caller might be holding the commit lock, so %-EAGAIN is returned instead;
  And this error code means that the caller has to run commit, and re-run GC
  if there is still no free space.
  There are many reasons why this function may return %-EAGAIN:
  o the log is full and there is no space to write an LEB reference for
    @c->gc_lnum;
  o the journal is too large and exceeds size limitations;
  o GC moved indexing LEBs, but they can be used only after the commit;
  o the shrinker fails to find clean znodes to free and requests the commit;
  o etc.
  Note, if the file-system is close to be full, this function may return
  %-EAGAIN infinitely, so the caller has to limit amount of re-invocations of
  the function. E.g., this happens if the limits on the journal size are too
  tough and GC writes too much to the journal before an LEB is freed. This
  might also mean that the journal is too large, and the TNC becomes to big,
  so that the shrinker is constantly called, finds not clean znodes to free,
  and requests commit. Well, this may also happen if the journal is all right,
  but another kernel process consumes too much memory. Anyway, infinite
  %-EAGAIN may happen, but in some extrememisconfiguration cases.
 We expect the write-buffer to be empty on entry 
 Give the commit an opportunity to run 
			
			  We've done enough iterations. Indexing LEBs were
			  moved and will be available after the commit.
			
			  We've moved too many LEBs and have not made
			  progress, give up.
		
		  Empty and freeable LEBs can turn up while we waited for
		  the wbuf lock, or while we have been running GC. In that
		  case, we should just return one of those instead of
		  continuing to GC dirty LEBs. Hence we request
		  'ubifs_find_dirty_leb()' to return an empty LEB if it can.
				
				  This is not error, so we have to return the
				  LEB to lprops. But if 'ubifs_return_leb()'
				  fails, its failure code is propagated to the
				  caller instead of the original '-EAGAIN'.
 An LEB has been freed and is ready for use 
			
			  This was an indexing LEB and it cannot be
			  immediately used. And instead of requesting the
			  commit straight away, we try to garbage collect some
			  more.
 GC makes progress, keep working 
		
		  GC moved an LEB bud have not done any progress. This means
		  that the previous GC head LEB contained too few free space
		  and the LEB which was GC'ed contained only large nodes which
		  did not fit that space.
		 
		  We can do 2 things:
		  1. pick another LEB in a hope it'll contain a small node
		     which will fit the space we have at the end of current GC
		     head LEB, but there is no guarantee, so we try this out
		     unless we have already been working for too long;
		  2. request an LEB with more dirty space, which will force
		     'ubifs_find_dirty_leb()' to start scanning the lprops
		     table, instead of just picking one from the heap
		     (previously it already picked the dirtiest LEB).
  ubifs_gc_start_commit - garbage collection at start of commit.
  @c: UBIFS file-system description object
  If a LEB has only dirty and free space, then we may safely unmap it and make
  it free.  Note, we cannot do this with indexing LEBs because dirty space may
  correspond index nodes that are required for recovery.  In that case, the
  LEB cannot be unmapped until after the next commit.
  This function returns %0 upon success and a negative error code upon failure.
	
	  Unmap (non-index) freeable LEBs. Note that recovery requires that all
	  wbufs are sync'd before this, which is done in 'do_commit()'.
 Mark GC'd index LEBs OK to unmap after this commit finishes 
 Record index freeable LEBs for unmapping after commit 
 Don't release the LEB until after the next commit 
  ubifs_gc_end_commit - garbage collection at end of commit.
  @c: UBIFS file-system description object
  This function completes out-of-place garbage collection of index LEBs.
  ubifs_destroy_idx_gc - destroy idx_gc list.
  @c: UBIFS file-system description object
  This function destroys the @c->idx_gc list. It is called when unmounting
  so locks are not needed. Returns zero in case of success and a negative
  error code in case of failure.
  ubifs_get_idx_gc_leb - get a LEB from GC'd index LEB list.
  @c: UBIFS file-system description object
  Called during start commit so locks are not needed.
 c->idx_gc_cnt is updated by the caller when lprops are updated 
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
  This file contains journal replay code. It runs when the file-system is being
  mounted and requires no locking.
  The larger is the journal, the longer it takes to scan it, so the longer it
  takes to mount UBIFS. This is why the journal has limited size which may be
  changed depending on the system requirements. But a larger journal gives
  faster IO speed because it writes the index less frequently. So this is a
  trade-off. Also, the journal is indexed by the in-memory index (TNC), so the
  larger is the journal, the more memory its index may consume.
  struct replay_entry - replay list entry.
  @lnum: logical eraseblock number of the node
  @offs: node offset
  @len: node length
  @deletion: non-zero if this entry corresponds to a node deletion
  @sqnum: node sequence number
  @list: links the replay list
  @key: node key
  @nm: directory entry name
  @old_size: truncation old size
  @new_size: truncation new size
  The replay process first scans all buds and builds the replay list, then
  sorts the replay list in nodes sequence number order, and then inserts all
  the replay entries to the TNC.
  struct bud_entry - entry in the list of buds to replay.
  @list: next bud in the list
  @bud: bud description object
  @sqnum: reference node sequence number
  @free: free bytes in the bud
  @dirty: dirty bytes in the bud
  set_bud_lprops - set free and dirty space used by a bud.
  @c: UBIFS file-system description object
  @b: bud entry which describes the bud
  This function makes sure the LEB properties of bud @b are set correctly
  after the replay. Returns zero in case of success and a negative error code
  in case of failure.
		
		  The LEB was added to the journal with a starting offset of
		  zero which means the LEB must have been empty. The LEB
		  property values should be @lp->free == @c->leb_size and
		  @lp->dirty == 0, but that is not the case. The reason is that
		  the LEB had been garbage collected before it became the bud,
		  and there was not commit inbetween. The garbage collector
		  resets the free and dirty space without recording it
		  anywhere except lprops, so if there was no commit then
		  lprops does not have that information.
		 
		  We do not need to adjust free space because the scan has told
		  us the exact value which is recorded in the replay entry as
		  @b->free.
		 
		  However we do need to subtract from the dirty space the
		  amount of space that the garbage collector reclaimed, which
		  is the whole LEB minus the amount of space that was free.
		
		  If the replay order was perfect the dirty space would now be
		  zero. The order is not perfect because the journal heads
		  race with each other. This is not a problem but is does mean
		  that the dirty space may temporarily exceed c->leb_size
		  during the replay.
 Make sure the journal head points to the latest bud 
  set_buds_lprops - set free and dirty space for all replayed buds.
  @c: UBIFS file-system description object
  This function sets LEB properties for all replayed buds. Returns zero in
  case of success and a negative error code in case of failure.
  trun_remove_range - apply a replay entry for a truncation to the TNC.
  @c: UBIFS file-system description object
  @r: replay entry of truncation
  inode_still_linked - check whether inode in question will be re-linked.
  @c: UBIFS file-system description object
  @rino: replay entry to test
  O_TMPFILE files can be re-linked, this means link count goes from 0 to 1.
  This case needs special care, otherwise all references to the inode will
  be removed upon the first replay entry of an inode with link count 0
  is found.
	
	  Find the most recent entry for the inode behind @rino and check
	  whether it is a deletion.
  apply_replay_entry - apply a replay entry to the TNC.
  @c: UBIFS file-system description object
  @r: replay entry to apply
  Apply a replay entry to the TNC.
  replay_entries_cmp - compare 2 replay entries.
  @priv: UBIFS file-system description object
  @a: first replay entry
  @b: second replay entry
  This is a comparios function for 'list_sort()' which compares 2 replay
  entries @a and @b by comparing their sequence number.  Returns %1 if @a has
  greater sequence number and %-1 otherwise.
  apply_replay_list - apply the replay list to the TNC.
  @c: UBIFS file-system description object
  Apply all entries in the replay list to the TNC. Returns zero in case of
  success and a negative error code in case of failure.
  destroy_replay_list - destroy the replay.
  @c: UBIFS file-system description object
  Destroy the replay list.
  insert_node - insert a node to the replay list
  @c: UBIFS file-system description object
  @lnum: node logical eraseblock number
  @offs: node offset
  @len: node length
  @key: node key
  @sqnum: sequence number
  @deletion: non-zero if this is a deletion
  @used: number of bytes in use in a LEB
  @old_size: truncation old size
  @new_size: truncation new size
  This function inserts a scanned non-direntry node to the replay list. The
  replay list contains @struct replay_entry elements, and we sort this list in
  sequence number order before applying it. The replay list is applied at the
  very end of the replay process. Since the list is sorted in sequence number
  order, the older modifications are applied first. This function returns zero
  in case of success and a negative error code in case of failure.
  insert_dent - insert a directory entry node into the replay list.
  @c: UBIFS file-system description object
  @lnum: node logical eraseblock number
  @offs: node offset
  @len: node length
  @key: node key
  @name: directory entry name
  @nlen: directory entry name length
  @sqnum: sequence number
  @deletion: non-zero if this is a deletion
  @used: number of bytes in use in a LEB
  This function inserts a scanned directory entry node or an extended
  attribute entry to the replay list. Returns zero in case of success and a
  negative error code in case of failure.
  ubifs_validate_entry - validate directory or extended attribute entry node.
  @c: UBIFS file-system description object
  @dent: the node to validate
  This function validates directory or extended attribute entry node @dent.
  Returns zero if the node is all right and a %-EINVAL if not.
  is_last_bud - check if the bud is the last in the journal head.
  @c: UBIFS file-system description object
  @bud: bud description object
  This function checks if bud @bud is the last bud in its journal head. This
  information is then used by 'replay_bud()' to decide whether the bud can
  have corruptions or not. Indeed, only last buds can be corrupted by power
  cuts. Returns %1 if this is the last bud, and %0 if not.
	
	  The following is a quirk to make sure we work correctly with UBIFS
	  images used with older UBIFS.
	 
	  Normally, the last bud will be the last in the journal head's list
	  of bud. However, there is one exception if the UBIFS image belongs
	  to older UBIFS. This is fairly unlikely: one would need to use old
	  UBIFS, then have a power cut exactly at the right point, and then
	  try to mount this image with new UBIFS.
	 
	  The exception is: it is possible to have 2 buds A and B, A goes
	  before B, and B is the last, bud B is contains no data, and bud A is
	  corrupted at the end. The reason is that in older versions when the
	  journal code switched the next bud (from A to B), it first added a
	  log reference node for the new bud (B), and only after this it
	  synchronized the write-buffer of current bud (A). But later this was
	  changed and UBIFS started to always synchronize the write-buffer of
	  the bud (A) before writing the log reference for the new bud (B).
	 
	  But because older UBIFS always synchronized A's write-buffer before
	  writing to B, we can recognize this exceptional situation but
	  checking the contents of bud B - if it is empty, then A can be
	  treated as the last and we can recover it.
	 
	  TODO: remove this piece of code in a couple of years (today it is
	  16.05.2011).
 authenticate_sleb_hash is split out for stack usage 
  authenticate_sleb - authenticate one scan LEB
  @c: UBIFS file-system description object
  @sleb: the scan LEB to authenticate
  @log_hash:
  @is_last: if true, this is the last LEB
  This function iterates over the buds of a single LEB authenticating all buds
  with the authentication nodes on this LEB. Authentication nodes are written
  after some buds and contain a HMAC covering the authentication node itself
  and the buds between the last authentication node and the current
  authentication node. It can happen that the last buds cannot be authenticated
  because a powercut happened when some nodes were written but not the
  corresponding authentication node. This function returns the number of nodes
  that could be authenticated or a negative error code.
	
	  A powercut can happen when some nodes were written, but not yet
	  the corresponding authentication node. This may only happen on
	  the last bud though.
  replay_bud - replay a bud logical eraseblock.
  @c: UBIFS file-system description object
  @b: bud entry which describes the bud
  This function replays bud @bud, recovers it if needed, and adds all nodes
  from this bud to the replay list. Returns zero in case of success and a
  negative error code in case of failure.
		
		  Recover only last LEBs in the journal heads, because power
		  cuts may cause corruptions only in these LEBs, because only
		  these LEBs could possibly be written to at the power cut
		  time.
	
	  The bud does not have to start from offset zero - the beginning of
	  the 'lnum' LEB may contain previously committed data. One of the
	  things we have to do in replay is to correctly update lprops with
	  newer information about this LEB.
	 
	  At this point lprops thinks that this LEB has 'c->leb_size - offs'
	  bytes of free space because it only contain information about
	  committed data.
	 
	  But we know that real amount of free space is 'c->leb_size -
	  sleb->endpt', and the space in the 'lnum' LEB between 'offs' and
	  'sleb->endpt' is used by bud data. We have to correctly calculate
	  how much of these data are dirty and update lprops with this
	  information.
	 
	  The dirt in that LEB region is comprised of padding nodes, deletion
	  nodes, truncation nodes and nodes which are obsoleted by subsequent
	  nodes in this LEB. So instead of calculating clean space, we
	  calculate used space ('used' variable).
 Validate truncation node 
			
			  Create a fake truncation key just to use the same
			  functions which expect nodes to have keys.
  replay_buds - replay all buds.
  @c: UBIFS file-system description object
  This function returns zero in case of success and a negative error code in
  case of failure.
  destroy_bud_list - destroy the list of buds to replay.
  @c: UBIFS file-system description object
  add_replay_bud - add a bud to the list of buds to replay.
  @c: UBIFS file-system description object
  @lnum: bud logical eraseblock number to replay
  @offs: bud start offset
  @jhead: journal head to which this bud belongs
  @sqnum: reference node sequence number
  This function returns zero in case of success and a negative error code in
  case of failure.
  validate_ref - validate a reference node.
  @c: UBIFS file-system description object
  @ref: the reference node to validate
  This function returns %1 if a bud reference already exists for the LEB. %0 is
  returned if the reference node is new, otherwise %-EINVAL is returned if
  validation failed.
	
	  ref->offs may point to the end of LEB when the journal head points
	  to the end of LEB and we write reference node for it during commit.
	  So this is why we require 'offs > c->leb_size'.
 Make sure we have not already looked at this bud 
  replay_log_leb - replay a log logical eraseblock.
  @c: UBIFS file-system description object
  @lnum: log logical eraseblock to replay
  @offs: offset to start replaying from
  @sbuf: scan buffer
  This function replays a log LEB and returns zero in case of success, %1 if
  this is the last LEB in the log, and a negative error code in case of
  failure.
		
		  Note, the below function will recover this log LEB only if
		  it is the last, because unclean reboots can possibly corrupt
		  only the tail of the log.
		
		  This is the first log LEB we are looking at, make sure that
		  the first node is a commit start node. Also record its
		  sequence number so that UBIFS can determine where the log
		  ends, because all nodes which were have higher sequence
		  numbers.
		
		  This means that we reached end of log and now
		  look to the older log data, which was already
		  committed but the eraseblock was not erased (UBIFS
		  only un-maps it). So this basically means we have to
		  exit with "end of log" code.
 Make sure the first node sits at offset zero of the LEB 
 Already have this bud 
 Make sure it sits at the beginning of LEB 
  take_ihead - update the status of the index head in lprops to 'taken'.
  @c: UBIFS file-system description object
  This function returns the amount of free space in the index head LEB or a
  negative error code.
  ubifs_replay_journal - replay journal.
  @c: UBIFS file-system description object
  This function scans the journal, replays and cleans it up. It makes sure all
  memory data structures related to uncommitted journal are built (dirty TNC
  tree, tree of buds, modified lprops, etc).
 Update the status of the index head in lprops to 'taken' 
 Error code 
 We hit the end of the log 
			
			  The head of the log must always start with the
			  "commit start" node on a properly formatted UBIFS.
			  But we found no nodes at all, which means that
			  something went wrong and we cannot proceed mounting
			  the file-system.
	
	  UBIFS budgeting calculations use @c->bi.uncommitted_idx variable
	  to roughly estimate index growth. Things like @c->bi.min_idx_lebs
	  depend on it. This means we have to initialize it to make sure
	  budgeting works properly.
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Artem Bityutskiy ( )
           Adrian Hunter
  This file implements UBIFS journal.
  The journal consists of 2 parts - the log and bud LEBs. The log has fixed
  length and position, while a bud logical eraseblock is any LEB in the main
  area. Buds contain file system data - data nodes, inode nodes, etc. The log
  contains only references to buds and some other stuff like commit
  start node. The idea is that when we commit the journal, we do
  not copy the data, the buds just become indexed. Since after the commit the
  nodes in bud eraseblocks become leaf nodes of the file system index tree, we
  use term "bud". Analogy is obvious, bud eraseblocks contain nodes which will
  become leafs in the future.
  The journal is multi-headed because we want to write data to the journal as
  optimally as possible. It is nice to have nodes belonging to the same inode
  in one LEB, so we may write data owned by different inodes to different
  journal heads, although at present only one data head is used.
  For recovery reasons, the base head contains all inode nodes, all directory
  entry nodes and all truncate nodes. This means that the other heads contain
  only data nodes.
  Bud LEBs may be half-indexed. For example, if the bud was not full at the
  time of commit, the bud is retained to continue to be used in the journal,
  even though the "front" of the LEB is now indexed. In that case, the log
  reference contains the offset where the bud starts for the purposes of the
  journal.
  The journal size has to be limited, because the larger is the journal, the
  longer it takes to mount UBIFS (scanning the journal) and the more memory it
  takes (indexing in the TNC).
  All the journal write operations like 'ubifs_jnl_update()' here, which write
  multiple UBIFS nodes to the journal at one go, are atomic with respect to
  unclean reboots. Should the unclean reboot happen, the recovery code drops
  all the nodes.
  zero_ino_node_unused - zero out unused fields of an on-flash inode node.
  @ino: the inode to zero out
  zero_dent_node_unused - zero out unused fields of an on-flash directory
                          entry node.
  @dent: the directory entry to zero out
  zero_trun_node_unused - zero out unused fields of an on-flash truncation
                          node.
  @trun: the truncation node to zero out
  reserve_space - reserve space in the journal.
  @c: UBIFS file-system description object
  @jhead: journal head number
  @len: node length
  This function reserves space in journal head @head. If the reservation
  succeeded, the journal head stays locked and later has to be unlocked using
  'release_head()'. Returns zero in case of success, %-EAGAIN if commit has to
  be done, and other negative error codes in case of other failures.
	
	  Typically, the base head has smaller nodes written to it, so it is
	  better to try to allocate space at the ends of eraseblocks. This is
	  what the squeeze parameter does.
	
	  Write buffer wasn't seek'ed or there is no enough space - look for an
	  LEB with some empty space.
	
	  No free space, we have to run garbage collector to make
	  some. But the write-buffer mutex has to be unlocked because
	  GC also takes it.
		
		  GC could not make a free LEB. But someone else may
		  have allocated new bud for this journal head,
		  because we dropped @wbuf->io_mutex, so try once
		  again.
		
		  Someone else has switched the journal head and we have
		  enough space now. This happens when more than one process is
		  trying to write to the same journal head at the same time.
	
	  Make sure we synchronize the write-buffer before we add the new bud
	  to the log. Otherwise we may have a power cut after the log
	  reference node for the last bud (@lnum) is written but before the
	  write-buffer data are written to the next-to-last bud
	  (@wbuf->lnum). And the effect would be that the recovery would see
	  that there is corruption in the next-to-last bud.
 An error occurred and the LEB has to be returned to lprops 
		
		  Return original error code only if it is not %-EAGAIN,
		  which is not really an error. Otherwise, return the error
		  code of 'ubifs_return_leb()'.
  write_head - write data to a journal head.
  @c: UBIFS file-system description object
  @jhead: journal head
  @buf: buffer to write
  @len: length to write
  @lnum: LEB number written is returned here
  @offs: offset written is returned here
  @sync: non-zero if the write-buffer has to by synchronized
  This function writes data to the reserved space of journal head @jhead.
  Returns zero in case of success and a negative error code in case of
  failure.
  make_reservation - reserve journal space.
  @c: UBIFS file-system description object
  @jhead: journal head
  @len: how many bytes to reserve
  This function makes space reservation in journal head @jhead. The function
  takes the commit lock and locks the journal head, and the caller has to
  unlock the head and finish the reservation with 'finish_reservation()'.
  Returns zero in case of success and a negative error code in case of
  failure.
  Note, the journal head may be unlocked as soon as the data is written, while
  the commit lock has to be released after the data has been added to the
  TNC.
 c->commit_sem will get released via finish_reservation(). 
		
		  GC could not make any progress. We should try to commit
		  once because it could make some dirty space and GC would
		  make progress, so make the error -EAGAIN so that the below
		  will commit and re-try.
		
		  This means that the budgeting is incorrect. We always have
		  to be able to write to the media, because all operations are
		  budgeted. Deletions are not budgeted, though, but we reserve
		  an extra LEB for them.
	
	  -EAGAIN means that the journal is full or too large, or the above
	  code wants to do one commit. Do this and re-try.
		
		  This should not happen unless the journal size limitations
		  are too tough.
 This are some budgeting problems, print useful information 
  release_head - release a journal head.
  @c: UBIFS file-system description object
  @jhead: journal head
  This function releases journal head @jhead which was locked by
  the 'make_reservation()' function. It has to be called after each successful
  'make_reservation()' invocation.
  finish_reservation - finish a reservation.
  @c: UBIFS file-system description object
  This function finishes journal space reservation. It must be called after
  'make_reservation()'.
  get_dent_type - translate VFS inode mode to UBIFS directory entry type.
  @mode: inode mode
  pack_inode - pack an inode node.
  @c: UBIFS file-system description object
  @ino: buffer in which to pack inode node
  @inode: inode to pack
  @last: indicates the last node of the group
	
	  Drop the attached data if this is a deletion inode, the data is not
	  needed anymore.
  mark_inode_clean - mark UBIFS inode as clean.
  @c: UBIFS file-system description object
  @ui: UBIFS inode to mark as clean
  This helper function marks UBIFS inode @ui as clean by cleaning the
  @ui->dirty flag and releasing its budget. Note, VFS may still treat the
  inode as dirty and try to write it back, but 'ubifs_write_inode()' would
  just do nothing.
  ubifs_jnl_update - update inode.
  @c: UBIFS file-system description object
  @dir: parent inode or host inode in case of extended attributes
  @nm: directory entry name
  @inode: inode to update
  @deletion: indicates a directory entry deletion i.e unlink or rmdir
  @xent: non-zero if the directory entry is an extended attribute entry
  This function updates an inode by writing a directory entry (or extended
  attribute entry), the inode itself, and the parent directory inode (or the
  host inode) to the journal.
  The function writes the host inode @dir last, which is important in case of
  extended attributes. Indeed, then we guarantee that if the host inode gets
  synchronized (with 'fsync()'), and the write-buffer it sits in gets flushed,
  the extended attribute inode gets flushed too. And this is exactly what the
  user expects - synchronizing the host inode synchronizes its extended
  attributes. Similarly, this guarantees that if @dir is synchronized, its
  directory entry corresponding to @nm gets synchronized too.
  If the inode (@inode) or the parent directory (@dir) are synchronous, this
  function synchronizes the write-buffer.
  This function marks the @dir and @inode inodes as clean and returns zero on
  success. In case of failure, a negative error code is returned.
	
	  If the last reference to the inode is being deleted, then there is
	  no need to attach and write inode data, it is being deleted anyway.
	  And if the inode is being deleted, no need to synchronize
	  write-buffer even if the inode is synchronous.
 Make sure to also account for extended attributes 
 Make reservation before allocating sequence numbers 
	
	  Note, we do not remove the inode from TNC even if the last reference
	  to it has just been deleted, because the inode may still be opened.
	  Instead, the inode has been added to orphan lists and the orphan
	  subsystem will take further care about it.
  ubifs_jnl_write_data - write a data node to the journal.
  @c: UBIFS file-system description object
  @inode: inode the data node belongs to
  @key: node key
  @buf: buffer to write
  @len: data length (must not exceed %UBIFS_BLOCK_SIZE)
  This function writes a data node to the journal. Returns %0 if the data node
  was successfully written, and a negative error code in case of failure.
		
		  Fall-back to the write reserve buffer. Note, we might be
		  currently on the memory reclaim path, when the kernel is
		  trying to free some memory by writing out dirty pages. The
		  write reserve buffer helps us to guarantee that we are
		  always able to write the data.
 Compression is disabled for this inode 
 Make reservation before allocating sequence numbers 
  ubifs_jnl_write_inode - flush inode to the journal.
  @c: UBIFS file-system description object
  @inode: inode to flush
  This function writes inode @inode to the journal. If the inode is
  synchronous, it also synchronizes the write-buffer. Returns zero in case of
  success and a negative error code in case of failure.
	
	  If the inode is being deleted, do not write the attached data. No
	  need to synchronize the write-buffer either.
 Make reservation before allocating sequence numbers 
  ubifs_jnl_delete_inode - delete an inode.
  @c: UBIFS file-system description object
  @inode: inode to delete
  This function deletes inode @inode which includes removing it from orphans,
  deleting it from TNC and, in some cases, writing a deletion inode to the
  journal.
  When regular file inodes are unlinked or a directory inode is removed, the
  'ubifs_jnl_update()' function writes a corresponding deletion inode and
  direntry to the media, and adds the inode to orphans. After this, when the
  last reference to this inode has been dropped, this function is called. In
  general, it has to write one more deletion inode to the media, because if
  a commit happened between 'ubifs_jnl_update()' and
  'ubifs_jnl_delete_inode()', the deletion inode is not in the journal
  anymore, and in fact it might not be on the flash anymore, because it might
  have been garbage-collected already. And for optimization reasons UBIFS does
  not read the orphan area if it has been unmounted cleanly, so it would have
  no indication in the journal that there is a deleted inode which has to be
  removed from TNC.
  However, if there was no commit between 'ubifs_jnl_update()' and
  'ubifs_jnl_delete_inode()', then there is no need to write the deletion
  inode to the media for the second time. And this is quite a typical case.
  This function returns zero in case of success and a negative error code in
  case of failure.
 A commit happened for sure or inode hosts xattrs 
	
	  Check commit number again, because the first test has been done
	  without @c->commit_sem, so a commit might have happened.
  ubifs_jnl_xrename - cross rename two directory entries.
  @c: UBIFS file-system description object
  @fst_dir: parent inode of 1st directory entry to exchange
  @fst_inode: 1st inode to exchange
  @fst_nm: name of 1st inode to exchange
  @snd_dir: parent inode of 2nd directory entry to exchange
  @snd_inode: 2nd inode to exchange
  @snd_nm: name of 2nd inode to exchange
  @sync: non-zero if the write-buffer has to be synchronized
  This function implements the cross rename operation which may involve
  writing 2 inodes and 2 directory entries. It marks the written inodes as clean
  and returns zero on success. In case of failure, a negative error code is
  returned.
 Make reservation before allocating sequence numbers 
 Make new dent for 1st entry 
 Make new dent for 2nd entry 
  ubifs_jnl_rename - rename a directory entry.
  @c: UBIFS file-system description object
  @old_dir: parent inode of directory entry to rename
  @old_dentry: directory entry to rename
  @new_dir: parent inode of directory entry to rename
  @new_dentry: new directory entry (or directory entry to replace)
  @sync: non-zero if the write-buffer has to be synchronized
  This function implements the re-name operation which may involve writing up
  to 4 inodes and 2 directory entries. It marks the written inodes as clean
  and returns zero on success. In case of failure, a negative error code is
  returned.
 Make reservation before allocating sequence numbers 
 Make new dent 
 Make deletion dent 
  truncate_data_node - re-compressencrypt a truncated data node.
  @c: UBIFS file-system description object
  @inode: inode which refers to the data node
  @block: data block number
  @dn: data node to re-compress
  @new_len: new length
  This function is used when an inode is truncated and the last data node of
  the inode has to be re-compressedencrypted and re-written.
  ubifs_jnl_truncate - update the journal for a truncation.
  @c: UBIFS file-system description object
  @inode: inode to truncate
  @old_size: old size
  @new_size: new size
  When the size of a file decreases due to truncation, a truncation node is
  written, the journal tree is updated, and the last data block is re-written
  if it has been affected. The inode is also updated in order to synchronize
  the new inode size.
  This function marks the inode as clean and returns zero on success. In case
  of failure, a negative error code is returned.
 Get last data block so it can be truncated 
 Not found (so it is a hole) 
 Nothing to do 
 Must make reservation before allocating sequence numbers 
  ubifs_jnl_delete_xattr - delete an extended attribute.
  @c: UBIFS file-system description object
  @host: host inode
  @inode: extended attribute inode
  @nm: extended attribute entry name
  This function delete an extended attribute which is very similar to
  un-linking regular files - it writes a deletion xentry, a deletion inode and
  updates the target inode. Returns zero in case of success and a negative
  error code in case of failure.
	
	  Since we are deleting the inode, we do not bother to attach any data
	  to it and assume its length is %UBIFS_INO_NODE_SZ.
 Make reservation before allocating sequence numbers 
 Remove the extended attribute entry from TNC 
	
	  Remove all nodes belonging to the extended attribute inode from TNC.
	  Well, there actually must be only one node - the inode itself.
 And update TNC with the new host inode position 
  ubifs_jnl_change_xattr - change an extended attribute.
  @c: UBIFS file-system description object
  @inode: extended attribute inode
  @host: host inode
  This function writes the updated version of an extended attribute inode and
  the host inode to the journal (to the base head). The host inode is written
  after the extended attribute inode in order to guarantee that the extended
  attribute will be flushed when the inode is synchronized by 'fsync()' and
  consequently, the write-buffer is synchronized. This function returns zero
  in case of success and a negative error code in case of failure.
 Make reservation before allocating sequence numbers 
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Copyright (C) 2006, 2007 University of Szeged, Hungary
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
           Zoltan Sogor
  This file provides a single place to access to compression and
  decompression.
 Fake description object for the "none" compressor 
 All UBIFS compressors 
  ubifs_compress - compress data.
  @in_buf: data to compress
  @in_len: length of the data to compress
  @out_buf: output buffer where compressed data should be stored
  @out_len: output buffer length is returned here
  @compr_type: type of compression to use on enter, actually used compression
               type on exit
  This function compresses input buffer @in_buf of length @in_len and stores
  the result in the output buffer @out_buf and the resulting length in
  @out_len. If the input buffer does not compress, it is just copied to the
  @out_buf. The same happens if @compr_type is %UBIFS_COMPR_NONE or if
  compression error occurred.
  Note, if the input buffer was not compressed, it is copied to the output
  buffer and %UBIFS_COMPR_NONE is returned in @compr_type.
 If the input data is small, do not even try to compress it 
	
	  If the data compressed only slightly, it is better to leave it
	  uncompressed to improve read speed.
  ubifs_decompress - decompress data.
  @in_buf: data to decompress
  @in_len: length of the data to decompress
  @out_buf: output buffer where decompressed data should
  @out_len: output length is returned here
  @compr_type: type of compression
  This function decompresses data from buffer @in_buf into buffer @out_buf.
  The length of the uncompressed data is returned in @out_len. This functions
  returns %0 on success or a negative error code on failure.
  compr_init - initialize a compressor.
  @compr: compressor description object
  This function initializes the requested compressor and returns zero in case
  of success or a negative error code in case of failure.
  compr_exit - de-initialize a compressor.
  @compr: compressor description object
  ubifs_compressors_init - initialize UBIFS compressors.
  This function initializes the compressor which were compiled in. Returns
  zero in case of success and a negative error code in case of failure.
  ubifs_compressors_exit - de-initialize UBIFS compressors.
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
  This file implements TNC (Tree Node Cache) which caches indexing nodes of
  the UBIFS B-tree.
  At the moment the locking rules of the TNC tree are quite simple and
  straightforward. We just have a mutex and lock it when we traverse the
  tree. If a znode is not in memory, we read it from flash while still having
  the mutex locked.
  Returned codes of 'matches_name()' and 'fallible_matches_name()' functions.
  @NAME_LESS: name corresponding to the first argument is less than second
  @NAME_MATCHES: names match
  @NAME_GREATER: name corresponding to the second argument is greater than
                 first
  @NOT_ON_MEDIA: node referred by zbranch does not exist on the media
  These constants were introduce to improve readability.
  insert_old_idx - record an index node obsoleted since the last commit start.
  @c: UBIFS file-system description object
  @lnum: LEB number of obsoleted index node
  @offs: offset of obsoleted index node
  Returns %0 on success, and a negative error code on failure.
  For recovery, there must always be a complete intact version of the index on
  flash at all times. That is called the "old index". It is the index as at the
  time of the last successful commit. Many of the index nodes in the old index
  may be dirty, but they must not be erased until the next successful commit
  (at which point that index becomes the old index).
  That means that the garbage collection and the in-the-gaps method of
  committing must be able to determine if an index node is in the old index.
  Most of the old index nodes can be found by looking up the TNC using the
  'lookup_znode()' function. However, some of the old index nodes may have
  been deleted from the current index or may have been changed so much that
  they cannot be easily found. In those cases, an entry is added to an RB-tree.
  That is what this function does. The RB-tree is ordered by LEB number and
  offset because they uniquely identify the old index node.
  insert_old_idx_znode - record a znode obsoleted since last commit start.
  @c: UBIFS file-system description object
  @znode: znode of obsoleted index node
  Returns %0 on success, and a negative error code on failure.
  ins_clr_old_idx_znode - record a znode obsoleted since last commit start.
  @c: UBIFS file-system description object
  @znode: znode of obsoleted index node
  Returns %0 on success, and a negative error code on failure.
  destroy_old_idx - destroy the old_idx RB-tree.
  @c: UBIFS file-system description object
  During start commit, the old_idx RB-tree is used to avoid overwriting index
  nodes that were in the index last commit but have since been deleted.  This
  is necessary for recovery i.e. the old index must be kept intact until the
  new index is successfully written.  The old-idx RB-tree is used for the
  in-the-gaps method of writing index nodes and is destroyed every commit.
  copy_znode - copy a dirty znode.
  @c: UBIFS file-system description object
  @znode: znode to copy
  A dirty znode being committed may not be changed, so it is copied.
 The children now have new parent 
  add_idx_dirt - add dirt due to a dirty znode.
  @c: UBIFS file-system description object
  @lnum: LEB number of index node
  @dirt: size of index node
  This function updates lprops dirty space and the new size of the index.
  dirty_cow_znode - ensure a znode is not being committed.
  @c: UBIFS file-system description object
  @zbr: branch of znode to check
  Returns dirtied znode on success or negative error code on failure.
 znode is not being committed 
  lnc_add - add a leaf node to the leaf node cache.
  @c: UBIFS file-system description object
  @zbr: zbranch of leaf node
  @node: leaf node
  Leaf nodes are non-index nodes directory entry nodes or data nodes. The
  purpose of the leaf node cache is to save re-reading the same leaf node over
  and over again. Most things are cached by VFS, however the file system must
  cache directory entries for readdir and for resolving hash collisions. The
  present implementation of the leaf node cache is extremely simple, and
  allows for error returns that are not used but that may be needed if a more
  complex implementation is created.
  Note, this function does not add the @node object to LNC directly, but
  allocates a copy of the object and adds the copy to LNC. The reason for this
  is that @node has been allocated outside of the TNC subsystem and will be
  used with @c->tnc_mutex unlock upon return from the TNC subsystem. But LNC
  may be changed at any time, e.g. freed by the shrinker.
 We don't have to have the cache, so no error 
  lnc_add_directly - add a leaf node to the leaf-node-cache.
  @c: UBIFS file-system description object
  @zbr: zbranch of leaf node
  @node: leaf node
  This function is similar to 'lnc_add()', but it does not create a copy of
  @node but inserts @node to TNC directly.
  lnc_free - remove a leaf node from the leaf node cache.
  @zbr: zbranch of leaf node
  tnc_read_hashed_node - read a "hashed" leaf node.
  @c: UBIFS file-system description object
  @zbr: key and position of the node
  @node: node is returned here
  This function reads a "hashed" node defined by @zbr from the leaf node cache
  (in it is there) or from the hash media, in which case the node is also
  added to LNC. Returns zero in case of success or a negative error
  code in case of failure.
 Read from the leaf node cache 
		
		  When the node was not found, return -ENOENT, 0 otherwise.
		  Negative return codes stay as-is.
 Add the node to the leaf node cache 
  try_read_node - read a node if it is a node.
  @c: UBIFS file-system description object
  @buf: buffer to read to
  @type: node type
  @zbr: the zbranch describing the node to read
  This function tries to read a node of known type and length, checks it and
  stores it in @buf. This function returns %1 if a node is present and %0 if
  a node is not present. A negative error code is returned for IO errors.
  This function performs that same function as ubifs_read_node except that
  it does not require that there is actually a node present and instead
  the return code indicates if a node was read.
  Note, this function does not check CRC of data nodes if @c->no_chk_data_crc
  is true (it is controlled by corresponding mount option). However, if
  @c->mounting or @c->remounting_rw is true (we are mounting or re-mounting to
  RW mode), @c->no_chk_data_crc is ignored and CRC is checked. This is
  because during mounting or re-mounting from RO mode to RW mode we may read
  journal nodes (when replying the journal or doing the recovery) and the
  journal nodes may potentially be corrupted, so checking is required.
  fallible_read_node - try to read a leaf node.
  @c: UBIFS file-system description object
  @key:  key of node to read
  @zbr:  position of node
  @node: node returned
  This function tries to read a node and returns %1 if the node is read, %0
  if the node is not present, and a negative error code in the case of error.
 All nodes have key in the same place 
  matches_name - determine if a direntry or xattr entry matches a given name.
  @c: UBIFS file-system description object
  @zbr: zbranch of dent
  @nm: name to match
  This function checks if xentrydirentry referred by zbranch @zbr matches name
  @nm. Returns %NAME_MATCHES if it does, %NAME_LESS if the name referred by
  @zbr is less than @nm, and %NAME_GREATER if it is greater than @nm. In case
  of failure, a negative error code is returned.
 If possible, match against the dent in the leaf node cache 
 Add the node to the leaf node cache 
  get_znode - get a TNC znode that may not be loaded yet.
  @c: UBIFS file-system description object
  @znode: parent znode
  @n: znode branch slot number
  This function returns the znode or a negative error code.
  tnc_next - find next TNC entry.
  @c: UBIFS file-system description object
  @zn: znode is passed and returned here
  @n: znode branch slot number is passed and returned here
  This function returns %0 if the next TNC entry is found, %-ENOENT if there is
  no next entry, or a negative error code otherwise.
  tnc_prev - find previous TNC entry.
  @c: UBIFS file-system description object
  @zn: znode is returned here
  @n: znode branch slot number is passed and returned here
  This function returns %0 if the previous TNC entry is found, %-ENOENT if
  there is no next entry, or a negative error code otherwise.
  resolve_collision - resolve a collision.
  @c: UBIFS file-system description object
  @key: key of a directory or extended attribute entry
  @zn: znode is returned here
  @n: zbranch number is passed and returned here
  @nm: name of the entry
  This function is called for "hashed" keys to make sure that the found key
  really corresponds to the looked up node (directory or extended attribute
  entry). It returns %1 and sets @zn and @n if the collision is resolved.
  %0 is returned if @nm is not found and @zn and @n are set to the previous
  entry, i.e. to the entry after which @nm could follow if it were in TNC.
  This means that @n may be set to %-1 if the leftmost key in @zn is the
  previous one. A negative error code is returned on failures.
 Look left 
				
				  We have found the branch after which we would
				  like to insert, but inserting in this znode
				  may still be wrong. Consider the following 3
				  znodes, in the case where we are resolving a
				  collision with Key2.
				 
				                   znode zp
				             ----------------------
				  level 1     |  Key0  |  Key1  |
				             -----------------------
				                  |            |
				        znode za  |            |  znode zb
				           ------------      ------------
				  level 0  |  Key0  |        |  Key2  |
				           ------------      ------------
				 
				  The lookup finds Key2 in znode zb. Lets say
				  there is no match and the name is greater so
				  we look left. When we find Key0, we end up
				  here. If we return now, we will insert into
				  znode za at slot n = 1.  But that is invalid
				  according to the parent's keys.  Key2 must
				  be inserted into znode zb.
				 
				  Note, this problem is not relevant for the
				  case when we go right, because
				  'tnc_insert()' would correct the parent key.
 Should be impossible 
 Look right 
  fallible_matches_name - determine if a dent matches a given name.
  @c: UBIFS file-system description object
  @zbr: zbranch of dent
  @nm: name to match
  This is a "fallible" version of 'matches_name()' function which does not
  panic if the direntryxentry referred by @zbr does not exist on the media.
  This function checks if xentrydirentry referred by zbranch @zbr matches name
  @nm. Returns %NAME_MATCHES it does, %NAME_LESS if the name referred by @zbr
  is less than @nm, %NAME_GREATER if it is greater than @nm, and @NOT_ON_MEDIA
  if xentrydirentry referred by @zbr does not exist on the media. A negative
  error code is returned in case of failure.
 If possible, match against the dent in the leaf node cache 
 The node was not present 
  fallible_resolve_collision - resolve a collision even if nodes are missing.
  @c: UBIFS file-system description object
  @key: key
  @zn: znode is returned here
  @n: branch number is passed and returned here
  @nm: name of directory entry
  @adding: indicates caller is adding a key to the TNC
  This is a "fallible" version of the 'resolve_collision()' function which
  does not panic if one of the nodes referred to by TNC does not exist on the
  media. This may happen when replaying the journal if a deleted node was
  Garbage-collected and the commit was not done. A branch that refers to a node
  that is not present is called a dangling branch. The following are the return
  codes for this function:
   o if @nm was found, %1 is returned and @zn and @n are set to the found
     branch;
   o if we are @adding and @nm was not found, %0 is returned;
   o if we are not @adding and @nm was not found, but a dangling branch was
     found, then %1 is returned and @zn and @n are set to the dangling branch;
   o a negative error code is returned in case of failure.
		
		  We are unlucky and hit a dangling branch straight away.
		  Now we do not really know where to go to find the needed
		  branch - to the left or to the right. Well, let's try left.
 Remove a dangling branch wherever it is 
 Look left 
 See comments in 'resolve_collision()' 
 Should be impossible 
 Look right 
 Never match a dangling branch when adding 
  matches_position - determine if a zbranch matches a given position.
  @zbr: zbranch of dent
  @lnum: LEB number of dent to match
  @offs: offset of dent to match
  This function returns %1 if @lnum:@offs matches, and %0 otherwise.
  resolve_collision_directly - resolve a collision directly.
  @c: UBIFS file-system description object
  @key: key of directory entry
  @zn: znode is passed and returned here
  @n: zbranch number is passed and returned here
  @lnum: LEB number of dent node to match
  @offs: offset of dent node to match
  This function is used for "hashed" keys to make sure the found directory or
  extended attribute entry node is what was looked for. It is used when the
  flash address of the right node is known (@lnum:@offs) which makes it much
  easier to resolve collisions (no need to read entries and match full
  names). This function returns %1 and sets @zn and @n if the collision is
  resolved, %0 if @lnum:@offs is not found and @zn and @n are set to the
  previous directory entry. Otherwise a negative error code is returned.
 Look left 
 Look right 
  dirty_cow_bottom_up - dirty a znode and its ancestors.
  @c: UBIFS file-system description object
  @znode: znode to dirty
  If we do not have a unique key that resides in a znode, then we cannot
  dirty that znode from the top down (i.e. by using lookup_level0_dirty)
  This function records the path back to the last dirty ancestor, and then
  dirties the znodes on that path.
 Go up until parent is dirty 
 Come back down, dirtying as we go 
  ubifs_lookup_level0 - search for zero-level znode.
  @c: UBIFS file-system description object
  @key:  key to lookup
  @zn: znode is returned here
  @n: znode branch slot number is returned here
  This function looks up the TNC tree and search for zero-level znode which
  refers key @key. The found zero-level znode is returned in @zn. There are 3
  cases:
    o exact match, i.e. the found zero-level znode contains key @key, then %1
      is returned and slot number of the matched branch is stored in @n;
    o not exact match, which means that zero-level znode does not contain
      @key, then %0 is returned and slot number of the closest branch or %-1
      is stored in @n; In this case calling tnc_next() is mandatory.
    o @key is so small that it is even less than the lowest key of the
      leftmost zero-level node, then %0 is returned and %0 is stored in @n.
  Note, when the TNC tree is traversed, some znodes may be absent, then this
  function reads corresponding indexing nodes and inserts them to TNC. In
  case of failure, a negative error code is returned.
 znode is not in TNC cache, load it from the media 
	
	  Here is a tricky place. We have not found the key and this is a
	  "hashed" key, which may collide. The rest of the code deals with
	  situations like this:
	 
	                   | 3 | 5 |
	                          \
	           | 3 | 5 |      | 6 | 7 | (x)
	 
	  Or more a complex example:
	 
	                 | 1 | 5 |
	                        \
	        | 1 | 3 |         | 5 | 8 |
	               \           
	           | 5 | 5 |   | 6 | 7 | (x)
	 
	  In the examples, if we are looking for key "5", we may reach nodes
	  marked with "(x)". In this case what we have do is to look at the
	  left and see if there is "5" key there. If there is, we have to
	  return it.
	 
	  Note, this whole situation is possible because we allow to have
	  elements which are equivalent to the next key in the parent in the
	  children of current znode. For example, this happens if we split a
	  znode like this: | 3 | 5 | 5 | 6 | 7 |, which results in something
	  like this:
	                       | 3 | 5 |
	                             \
	                 | 3 | 5 |   | 5 | 6 | 7 |
	                               ^
	  And this becomes what is at the first "picture" after key "5" marked
	  with "^" is removed. What could be done is we could prohibit
	  splitting in the middle of the colliding sequence. Also, when
	  removing the leftmost key, we would have to correct the key of the
	  parent node, which would introduce additional complications. Namely,
	  if we changed the leftmost key of the parent znode, the garbage
	  collector would be unable to find it (GC is doing this when GC'ing
	  indexing LEBs). Although we already have an additional RB-tree where
	  we save such changed znodes (see 'ins_clr_old_idx_znode()') until
	  after the commit. But anyway, this does not look easy to implement
	  so we did not try this.
  lookup_level0_dirty - search for zero-level znode dirtying.
  @c: UBIFS file-system description object
  @key:  key to lookup
  @zn: znode is returned here
  @n: znode branch slot number is returned here
  This function looks up the TNC tree and search for zero-level znode which
  refers key @key. The found zero-level znode is returned in @zn. There are 3
  cases:
    o exact match, i.e. the found zero-level znode contains key @key, then %1
      is returned and slot number of the matched branch is stored in @n;
    o not exact match, which means that zero-level znode does not contain @key
      then %0 is returned and slot number of the closed branch is stored in
      @n;
    o @key is so small that it is even less than the lowest key of the
      leftmost zero-level node, then %0 is returned and %-1 is stored in @n.
  Additionally all znodes in the path from the root to the located zero-level
  znode are marked as dirty.
  Note, when the TNC tree is traversed, some znodes may be absent, then this
  function reads corresponding indexing nodes and inserts them to TNC. In
  case of failure, a negative error code is returned.
 znode is not in TNC cache, load it from the media 
	
	  See huge comment at 'lookup_level0_dirty()' what is the rest of the
	  code.
  maybe_leb_gced - determine if a LEB may have been garbage collected.
  @c: UBIFS file-system description object
  @lnum: LEB number
  @gc_seq1: garbage collection sequence number
  This function determines if @lnum may have been garbage collected since
  sequence number @gc_seq1. If it may have been then %1 is returned, otherwise
  %0 is returned.
 Same seq means no GC 
 Different by more than 1 means we don't know 
	
	  We have seen the sequence number has increased by 1. Now we need to
	  be sure we read the right LEB number, so read it again.
 Finally we can check lnum 
  ubifs_tnc_locate - look up a file-system node and return it and its location.
  @c: UBIFS file-system description object
  @key: node key to lookup
  @node: the node is returned here
  @lnum: LEB number is returned here
  @offs: offset is returned here
  This function looks up and reads node with key @key. The caller has to make
  sure the @node buffer is large enough to fit the node. Returns zero in case
  of success, %-ENOENT if the node was not found, and a negative error code in
  case of failure. The node location can be returned in @lnum and @offs.
		
		  In this case the leaf node cache gets used, so we pass the
		  address of the zbranch and keep the mutex locked
 Drop the TNC mutex prematurely and race with garbage collection 
 We do not GC journal heads 
		
		  The node may have been GC'ed out from under us so try again
		  while keeping the TNC mutex locked.
  ubifs_tnc_get_bu_keys - lookup keys for bulk-read.
  @c: UBIFS file-system description object
  @bu: bulk-read parameters and results
  Lookup consecutive data node keys for the same inode that reside
  consecutively in the same LEB. This function returns zero in case of success
  and a negative error code in case of failure.
  Note, if the bulk-read buffer length (@bu->buf_len) is known, this function
  makes sure bulk-read nodes fit the buffer. Otherwise, this function prepares
  maximum possible amount of nodes for bulk-read.
 Find first key 
 Key found 
 The buffer must be big enough for at least 1 node 
 Add this key 
 Find next key 
 See if there is another data key for this file 
 First key found 
			
			  The data nodes must be in consecutive positions in
			  the same LEB.
 Must not exceed buffer length 
 Allow for holes 
 Add this key 
 See if we have room for more 
	
	  An enormous hole could cause bulk-read to encompass too many
	  page cache pages, so limit the number here.
	
	  Ensure that bulk-read covers a whole number of page cache
	  pages.
 At the end of file we can round up 
 Exclude data nodes that do not make up a whole page cache page 
  read_wbuf - bulk-read from a LEB with a wbuf.
  @wbuf: wbuf that may overlap the read
  @buf: buffer into which to read
  @len: read length
  @lnum: LEB number from which to read
  @offs: offset from which to read
  This functions returns %0 on success or a negative error code on failure.
 We may safely unlock the write-buffer and read the data 
 Don't read under wbuf 
 Copy the rest from the write-buffer 
 Read everything that goes before write-buffer 
  validate_data_node - validate data nodes for bulk-read.
  @c: UBIFS file-system description object
  @buf: buffer containing data node to validate
  @zbr: zbranch of data node to validate
  This functions returns %0 on success or a negative error code on failure.
 Make sure the key of the read node is correct 
  ubifs_tnc_bulk_read - read a number of data nodes in one go.
  @c: UBIFS file-system description object
  @bu: bulk-read parameters and results
  This functions reads and validates the data nodes that were identified by the
  'ubifs_tnc_get_bu_keys()' function. This functions returns %0 on success,
  -EAGAIN to indicate a race with GC, or another negative error code on
  failure.
 Do the read 
 Check for a race with GC 
 Validate the nodes read 
  do_lookup_nm- look up a "hashed" node.
  @c: UBIFS file-system description object
  @key: node key to lookup
  @node: the node is returned here
  @nm: node name
  This function looks up and reads a node which contains name hash in the key.
  Since the hash may have collisions, there may be many nodes with the same
  key, so we have to sequentially look to all of them until the needed one is
  found. This function returns zero in case of success, %-ENOENT if the node
  was not found, and a negative error code in case of failure.
  ubifs_tnc_lookup_nm - look up a "hashed" node.
  @c: UBIFS file-system description object
  @key: node key to lookup
  @node: the node is returned here
  @nm: node name
  This function looks up and reads a node which contains name hash in the key.
  Since the hash may have collisions, there may be many nodes with the same
  key, so we have to sequentially look to all of them until the needed one is
  found. This function returns zero in case of success, %-ENOENT if the node
  was not found, and a negative error code in case of failure.
	
	  We assume that in most of the cases there are no name collisions and
	  'ubifs_tnc_lookup()' returns us the right direntry.
	
	  Unluckily, there are hash collisions and we have to iterate over
	  them look at each direntry with colliding name hash sequentially.
  ubifs_tnc_lookup_dh - look up a "double hashed" node.
  @c: UBIFS file-system description object
  @key: node key to lookup
  @node: the node is returned here
  @cookie: node cookie for collision resolution
  This function looks up and reads a node which contains name hash in the key.
  Since the hash may have collisions, there may be many nodes with the same
  key, so we have to sequentially look to all of them until the needed one
  with the same cookie value is found.
  This function returns zero in case of success, %-ENOENT if the node
  was not found, and a negative error code in case of failure.
	
	  We assume that in most of the cases there are no name collisions and
	  'ubifs_tnc_lookup()' returns us the right direntry.
	
	  Unluckily, there are hash collisions and we have to iterate over
	  them look at each direntry with colliding name hash sequentially.
  correct_parent_keys - correct parent znodes' keys.
  @c: UBIFS file-system description object
  @znode: znode to correct parent znodes for
  This is a helper function for 'tnc_insert()'. When the key of the leftmost
  zbranch changes, keys of parent znodes have to be corrected. This helper
  function is called in such situations and corrects the keys if needed.
  insert_zbranch - insert a zbranch into a znode.
  @c: UBIFS file-system description object
  @znode: znode into which to insert
  @zbr: zbranch to insert
  @n: slot number to insert to
  This is a helper function for 'tnc_insert()'. UBIFS does not allow "gaps" in
  znode's array of zbranches and keeps zbranches consolidated, so when a new
  zbranch has to be inserted to the @znode->zbranches[]' array at the @n-th
  slot, zbranches starting from @n have to be moved right.
	
	  After inserting at slot zero, the lower bound of the key range of
	  this znode may have changed. If this znode is subsequently split
	  then the upper bound of the key range may change, and furthermore
	  it could change to be lower than the original lower bound. If that
	  happens, then it will no longer be possible to find this znode in the
	  TNC using the key from the index node on flash. That is bad because
	  if it is not found, we will assume it is obsolete and may overwrite
	  it. Then if there is an unclean unmount, we will start using the
	  old index which will be broken.
	 
	  So we first mark znodes that have insertions at slot zero, and then
	  if they are split we add their lnumoffs to the old_idx tree.
  tnc_insert - insert a node into TNC.
  @c: UBIFS file-system description object
  @znode: znode to insert into
  @zbr: branch to insert
  @n: slot number to insert new zbranch to
  This function inserts a new node described by @zbr into znode @znode. If
  znode does not have a free slot for new zbranch, it is split. Parent znodes
  are splat as well if needed. Returns zero in case of success or a negative
  error code in case of failure.
 Implement naive insert for now 
 Ensure parent's key is correct 
	
	  Unfortunately, @znode does not have more empty slots and we have to
	  split it.
		
		  We can no longer be sure of finding this znode by key, so we
		  record it in the old_idx tree.
 Decide where to split 
 Try not to split consecutive data keys 
 Try not to split consecutive data keys 
	
	  Although we don't at present, we could look at the neighbors and see
	  if we can move some zbranches there.
 Insert into existing znode 
 Insert into new znode 
 Re-parent 
 Move zbranch 
 Re-parent 
 Insert new key and branch 
 Insert new znode (produced by spitting) into the parent 
 Locate insertion point 
 Tail recursion 
 We have to split root znode 
  ubifs_tnc_add - add a node to TNC.
  @c: UBIFS file-system description object
  @key: key to add
  @lnum: LEB number of node
  @offs: node offset
  @len: node length
  @hash: The hash over the node
  This function adds a node with key @key to TNC. The node may be new or it may
  obsolete some existing one. Returns %0 on success or negative error code on
  failure.
  ubifs_tnc_replace - replace a node in the TNC only if the old node is found.
  @c: UBIFS file-system description object
  @key: key to add
  @old_lnum: LEB number of old node
  @old_offs: old node offset
  @lnum: LEB number of node
  @offs: node offset
  @len: node length
  This function replaces a node with key @key in the TNC only if the old node
  is found.  This function is called by garbage collection when node are moved.
  Returns %0 on success or negative error code on failure.
 Ensure the znode is dirtied 
  ubifs_tnc_add_nm - add a "hashed" node to TNC.
  @c: UBIFS file-system description object
  @key: key to add
  @lnum: LEB number of node
  @offs: node offset
  @len: node length
  @hash: The hash over the node
  @nm: node name
  This is the same as 'ubifs_tnc_add()' but it should be used with keys which
  may have collisions, like directory entry keys.
 Ensure the znode is dirtied 
			
			  We did not find it in the index so there may be a
			  dangling branch still in the index. So we remove it
			  by passing 'ubifs_tnc_remove_nm()' the same key but
			  an unmatchable name.
  tnc_delete - delete a znode form TNC.
  @c: UBIFS file-system description object
  @znode: znode to delete from
  @n: zbranch slot number to delete
  This function deletes a leaf node from @n-th slot of @znode. Returns zero in
  case of success and a negative error code in case of failure.
 Delete without merge for now 
 We do not "gap" zbranch slots 
	
	  This was the last zbranch, we have to delete this znode from the
	  parent.
 while removing last child 
 Remove from znode, entry n - 1 
	
	  If this is the root and it has only 1 child then
	  collapse the tree.
  ubifs_tnc_remove - remove an index entry of a node.
  @c: UBIFS file-system description object
  @key: key of node
  Returns %0 on success or negative error code on failure.
  ubifs_tnc_remove_nm - remove an index entry for a "hashed" node.
  @c: UBIFS file-system description object
  @key: key of node
  @nm: directory entry name
  Returns %0 on success or negative error code on failure.
 Ensure the znode is dirtied 
  ubifs_tnc_remove_dh - remove an index entry for a "double hashed" node.
  @c: UBIFS file-system description object
  @key: key of node
  @cookie: node cookie for collision resolution
  Returns %0 on success or negative error code on failure.
 If the cookie does not match, we're facing a hash collision. 
  key_in_range - determine if a key falls within a range of keys.
  @c: UBIFS file-system description object
  @key: key to check
  @from_key: lowest key in range
  @to_key: highest key in range
  This function returns %1 if the key is in range and %0 otherwise.
  ubifs_tnc_remove_range - remove index entries in range.
  @c: UBIFS file-system description object
  @from_key: lowest key to remove
  @to_key: highest key to remove
  This function removes index entries starting at @from_key and ending at
  @to_key.  This function returns zero in case of success and a negative error
  code in case of failure.
 Find first level 0 znode that contains keys to remove 
 Ensure the znode is dirtied 
 Remove all keys in range except the first 
 Now delete the first 
  ubifs_tnc_remove_ino - remove an inode from TNC.
  @c: UBIFS file-system description object
  @inum: inode number to remove
  This function remove inode @inum and all the extended attributes associated
  with the anode from TNC and returns zero in case of success or a negative
  error code in case of failure.
	
	  Walk all extended attribute entries and remove them together with
	  corresponding extended attribute inodes.
  ubifs_tnc_next_ent - walk directory or extended attribute entries.
  @c: UBIFS file-system description object
  @key: key of last entry
  @nm: name of last entry found or %NULL
  This function finds and reads the next directory or extended attribute entry
  after the given key (@key) if there is one. @nm is used to resolve
  collisions.
  If the name of the current entry is not known and only the key is known,
  @nm->name has to be %NULL. In this case the semantics of this function is a
  little bit different and it returns the entry corresponding to this key, not
  the next one. If the key was not found, the closest "right" entry is
  returned.
  If the fist entry has to be found, @key has to contain the lowest possible
  key value for this inode and @name has to be %NULL.
  This function returns the found directory or extended attribute entry node
  in case of success, %-ENOENT is returned if no entry was found, and a
  negative error code is returned in case of failure.
 Handle collisions 
 Now find next entry 
		
		  The full name of the entry was not given, in which case the
		  behavior of this function is a little different and it
		  returns current entry, not the next one.
			
			  However, the given key does not exist in the TNC
			  tree and @znode@n variables contain the closest
			  "preceding" element. Switch to the next one.
	
	  The above 'tnc_next()' call could lead us to the next inode, check
	  this.
  tnc_destroy_cnext - destroy left-over obsolete znodes from a failed commit.
  @c: UBIFS file-system description object
  Destroy left-over obsolete znodes from a failed commit.
  ubifs_tnc_close - close TNC subsystem and free all related resources.
  @c: UBIFS file-system description object
  left_znode - get the znode to the left.
  @c: UBIFS file-system description object
  @znode: znode
  This function returns a pointer to the znode to the left of @znode or NULL if
  there is not one. A negative error code is returned on failure.
 Go up until we can go left 
 Now go down the rightmost branch to 'level' 
  right_znode - get the znode to the right.
  @c: UBIFS file-system description object
  @znode: znode
  This function returns a pointer to the znode to the right of @znode or NULL
  if there is not one. A negative error code is returned on failure.
 Go up until we can go right 
 Now go down the leftmost branch to 'level' 
  lookup_znode - find a particular indexing node from TNC.
  @c: UBIFS file-system description object
  @key: index node key to lookup
  @level: index node level
  @lnum: index node LEB number
  @offs: index node offset
  This function searches an indexing node by its first key @key and its
  address @lnum:@offs. It looks up the indexing tree by pulling all indexing
  nodes it traverses to TNC. This function is called for indexing nodes which
  were found on the media by scanning, for example when garbage-collecting or
  when doing in-the-gaps commit. This means that the indexing node which is
  looked for does not have to have exactly the same leftmost key @key, because
  the leftmost key may have been changed, in which case TNC will contain a
  dirty znode which still refers the same @lnum:@offs. This function is clever
  enough to recognize such indexing nodes.
  Note, if a znode was deleted or changed too much, then this function will
  not find it. For situations like this UBIFS has the old index RB-tree
  (indexed by @lnum:@offs).
  This function returns a pointer to the znode found or %NULL if it is not
  found. A negative error code is returned on failure.
	
	  The arguments have probably been read off flash, so don't assume
	  they are valid.
 Get the root znode 
 Check if it is the one we are looking for 
 Descend to the parent level i.e. (level + 1) 
			
			  We reached a znode where the leftmost key is greater
			  than the key we are searching for. This is the same
			  situation as the one described in a huge comment at
			  the end of the 'ubifs_lookup_level0()' function. And
			  for exactly the same reasons we have to try to look
			  left before giving up.
 Check if the child is the one we are looking for 
 If the key is unique, there is nowhere else to look 
	
	  The key is not unique and so may be also in the znodes to either
	  side.
 Look left 
 Move one branch to the left 
 Check it 
 Stop if the key is less than the one we are looking for 
 Back to the middle 
 Look right 
 Move one branch to the right 
 Check it 
 Stop if the key is greater than the one we are looking for 
  is_idx_node_in_tnc - determine if an index node is in the TNC.
  @c: UBIFS file-system description object
  @key: key of index node
  @level: index node level
  @lnum: LEB number of index node
  @offs: offset of index node
  This function returns %0 if the index node is not referred to in the TNC, %1
  if the index node is referred to in the TNC and the corresponding znode is
  dirty, %2 if an index node is referred to in the TNC and the corresponding
  znode is clean, and a negative error code in case of failure.
  Note, the @key argument has to be the key of the first child. Also note,
  this function relies on the fact that 0:0 is never a valid LEB number and
  offset for a main-area node.
  is_leaf_node_in_tnc - determine if a non-indexing not is in the TNC.
  @c: UBIFS file-system description object
  @key: node key
  @lnum: node LEB number
  @offs: node offset
  This function returns %1 if the node is referred to in the TNC, %0 if it is
  not, and a negative error code in case of failure.
  Note, this function relies on the fact that 0:0 is never a valid LEB number
  and offset for a main-area node.
 Error code 
 Found it 
	
	  Because the key is not unique, we have to look left
	  and right as well
 Look left 
 Found it 
 Look right 
 Found it 
  ubifs_tnc_has_node - determine whether a node is in the TNC.
  @c: UBIFS file-system description object
  @key: node key
  @level: index node level (if it is an index node)
  @lnum: node LEB number
  @offs: node offset
  @is_idx: non-zero if the node is an index node
  This function returns %1 if the node is in the TNC, %0 if it is not, and a
  negative error code in case of failure. For index nodes, @key has to be the
  key of the first child. An index node is considered to be in the TNC only if
  the corresponding znode is clean or has not been loaded.
 The index node was found but it was dirty 
 The index node was found and it was clean 
  ubifs_dirty_idx_node - dirty an index node.
  @c: UBIFS file-system description object
  @key: index node key
  @level: index node level
  @lnum: index node LEB number
  @offs: index node offset
  This function loads and dirties an index node so that it can be garbage
  collected. The @key argument has to be the key of the first child. This
  function relies on the fact that 0:0 is never a valid LEB number and offset
  for a main-area node. Returns %0 on success and a negative error code on
  failure.
  dbg_check_inode_size - check if inode size is correct.
  @c: UBIFS file-system description object
  @inode: inode to check
  @size: inode size
  This function makes sure that the inode size (@size) is correct and it does
  not have any pages beyond @size. Returns zero if the inode is OK, %-EINVAL
  if it has a data page beyond @size, and other negative error code in case of
  other errors.
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
  This file implements the scan which is a general-purpose function for
  determining what nodes are in an eraseblock. The scan is used to replay the
  journal, to do garbage collection. for the TNC in-the-gaps method, and by
  debugging functions.
  scan_padding_bytes - scan for padding bytes.
  @buf: buffer to scan
  @len: length of buffer
  This function returns the number of padding bytes on success and
  %SCANNED_GARBAGE on failure.
  ubifs_scan_a_node - scan for a node or padding.
  @c: UBIFS file-system description object
  @buf: buffer to scan
  @len: length of buffer
  @lnum: logical eraseblock number
  @offs: offset within the logical eraseblock
  @quiet: print no messages
  This function returns a scanning code to indicate what was scanned.
 Validate the padding node 
 Make the node pads to 8-byte boundary 
  ubifs_start_scan - create LEB scanning information at start of scan.
  @c: UBIFS file-system description object
  @lnum: logical eraseblock number
  @offs: offset to start at (usually zero)
  @sbuf: scan buffer (must be c->leb_size)
  This function returns the scanned information on success and a negative error
  code on failure.
	
	  Note, we ignore integrity errors (EBASMSG) because all the nodes are
	  protected by CRC checksums.
  ubifs_end_scan - update LEB scanning information at end of scan.
  @c: UBIFS file-system description object
  @sleb: scanning information
  @lnum: logical eraseblock number
  @offs: offset to start at (usually zero)
  ubifs_add_snod - add a scanned node to LEB scanning information.
  @c: UBIFS file-system description object
  @sleb: scanning information
  @buf: buffer containing node
  @offs: offset of node on flash
  This function returns %0 on success and a negative error code on failure.
		
		  The key is in the same place in all keyed
		  nodes.
  ubifs_scanned_corruption - print information after UBIFS scanned corruption.
  @c: UBIFS file-system description object
  @lnum: LEB number of corruption
  @offs: offset of corruption
  @buf: buffer containing corruption
  ubifs_scan - scan a logical eraseblock.
  @c: UBIFS file-system description object
  @lnum: logical eraseblock number
  @offs: offset to start at (usually zero)
  @sbuf: scan buffer (must be of @c->leb_size bytes in size)
  @quiet: print no messages
  This function scans LEB number @lnum and returns complete information about
  its contents. Returns the scanned information in case of success and,
  %-EUCLEAN if the LEB neads recovery, and other negative error codes in case
  of failure.
  If @quiet is non-zero, this function does not print large and scary
  error messages and flash dumps in case of errors.
 Padding bytes or a valid padding node 
 Empty space is checked later 
  ubifs_scan_destroy - destroy LEB scanning information.
  @sleb: scanning information to free
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
  This file contains miscelanious TNC-related functions shared betweend
  different files. This file does not form any logically separate TNC
  sub-system. The file was created because there is a lot of TNC code and
  putting it all in one file would make that file too big and unreadable.
  ubifs_tnc_levelorder_next - next TNC tree element in levelorder traversal.
  @c: UBIFS file-system description object
  @zr: root of the subtree to traverse
  @znode: previous znode
  This function implements levelorder TNC traversal. The LNC is ignored.
  Returns the next element or %NULL if @znode is already the last one.
		
		  First walk up until there is a znode with next branch to
		  look at.
 This level is done, switch to the lower one 
				
				  We were already looking for znode at lower
				  level ('level_search'). As we are here
				  again, it just does not exist. Or all levels
				  were finished ('level < 0').
 Switch to the next index 
 No more children to look at, we have walk up 
 Walk back down to the level we came from ('level') 
				
				  This path is not too deep so it does not
				  reach 'level'. Try next path.
  ubifs_search_zbranch - search znode branch.
  @c: UBIFS file-system description object
  @znode: znode to search in
  @key: key to search for
  @n: znode branch slot number is returned here
  This is a helper function which search branch with key @key in @znode using
  binary search. The result of the search may be:
    o exact match, then %1 is returned, and the slot number of the branch is
      stored in @n;
    o no exact match, then %0 is returned and the slot number of the left
      closest branch is returned in @n; the slot if all keys in this znode are
      greater than @key, then %-1 is returned in @n.
 The insert point is after n 
  ubifs_tnc_postorder_first - find first znode to do postorder tree traversal.
  @znode: znode to start at (root of the sub-tree to traverse)
  Find the lowest leftmost znode in a subtree of the TNC tree. The LNC is
  ignored.
  ubifs_tnc_postorder_next - next TNC tree element in postorder traversal.
  @c: UBIFS file-system description object
  @znode: previous znode
  This function implements postorder TNC traversal. The LNC is ignored.
  Returns the next element or %NULL if @znode is already the last one.
 Switch to the next index in the parent 
 This is in fact the last child, return parent 
 Go to the first znode in this new subtree 
  ubifs_destroy_tnc_subtree - destroy all znodes connected to a subtree.
  @c: UBIFS file-system description object
  @znode: znode defining subtree to destroy
  This function destroys subtree of the TNC tree. Returns number of clean
  znodes in the subtree.
  read_znode - read an indexing node from flash and fill znode.
  @c: UBIFS file-system description object
  @zzbr: the zbranch describing the node to read
  @znode: znode to read to
  This function reads an indexing node from the flash media and fills znode
  with the read data. Returns zero in case of success and a negative error
  code in case of failure. The read indexing node is validated and if anything
  is wrong with it, this function prints complaint messages and returns
  %-EINVAL.
 Validate branch 
	
	  Ensure that the next key is greater or equivalent to the
	  previous one.
 These can only be keys with colliding hash 
  ubifs_load_znode - load znode to TNC cache.
  @c: UBIFS file-system description object
  @zbr: znode branch
  @parent: znode's parent
  @iip: index in parent
  This function loads znode pointed to by @zbr into the TNC cache and
  returns pointer to it in case of success and a negative error code in case
  of failure.
	
	  A slab cache is not presently used for znodes because the znode size
	  depends on the fanout which is stored in the superblock.
	
	  Increment the global clean znode counter as well. It is OK that
	  global and per-FS clean znode counters may be inconsistent for some
	  short time (because we might be preempted at this point), the global
	  one is only used in shrinker.
  ubifs_tnc_read_node - read a leaf node from the flash media.
  @c: UBIFS file-system description object
  @zbr: key and position of the node
  @node: node is returned here
  This function reads a node defined by @zbr from the flash media. Returns
  zero in case of success or a negative error code in case of failure.
	
	  'zbr' has to point to on-flash node. The node may sit in a bud and
	  may even be in a write buffer, so we have to take care about this.
 Make sure the key of the read node is correct 
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Copyright (C) 2006, 2007 University of Szeged, Hungary
  Authors: Artem Bityutskiy ( )
           Adrian Hunter
           Zoltan Sogor
  This file implements directory operations.
  All FS operations in this file allocate budget before writing anything to the
  media. If they fail to allocate it, the error is returned. The only
  exceptions are 'ubifs_unlink()' and 'ubifs_rmdir()' which keep working even
  if they unable to allocate the budget, because deletion %-ENOSPC failure is
  not what users are usually ready to get. UBIFS budgeting subsystem has some
  space reserved for these purposes.
  All operations in this file write all inodes which they change straight
  away, instead of marking them dirty. For example, 'ubifs_link()' changes
  @i_size of the parent inode and writes the parent inode together with the
  target inode. This was done to simplify file-system recovery which would
  otherwise be very difficult to do. The only exception is rename which marks
  the re-named inode dirty (because its @i_ctime is updated) but does not
  write it, but just marks it as dirty.
  inherit_flags - inherit flags of the parent inode.
  @dir: parent inode
  @mode: new inode mode flags
  This is a helper function for 'ubifs_new_inode()' which inherits flag of the
  parent directory inode @dir. UBIFS inodes inherit the following flags:
  o %UBIFS_COMPR_FL, which is useful to switch compression onof on
    sub-directory basis;
  o %UBIFS_SYNC_FL - useful for the same reasons;
  o %UBIFS_DIRSYNC_FL - similar, but relevant only to directories.
  This function returns the inherited flags.
		
		  The parent is not a directory, which means that an extended
		  attribute inode is being created. No flags.
 The "DIRSYNC" flag only applies to directories 
  ubifs_new_inode - allocate new UBIFS inode object.
  @c: UBIFS file-system description object
  @dir: parent directory inode
  @mode: inode mode flags
  This function finds an unused inode number, allocates new inode and
  initializes it. Returns new inode in case of success and an error code in
  case of failure.
	
	  Set 'S_NOCMTIME' to prevent VFS form updating [mc]time of inodes and
	  marking them dirty in file write path (see 'file_update_time()').
	  UBIFS has to fully control "clean <-> dirty" transitions of inodes
	  to make budgeting work.
 Inode number overflow is currently not supported 
	
	  The creation sequence number remains with this inode for its
	  lifetime. All nodes for this inode have a greater sequence number,
	  and so it is possible to distinguish obsolete nodes belonging to a
	  previous incarnation of the same inode number - for example, for the
	  purpose of rebuilding the index.
 ENOENT 
		
		  This should not happen. Probably the file-system needs
		  checking.
	
	  Budget request settings: new inode, new direntry, changing the
	  parent directory inode.
	
	  Budget request settings: new dirty inode, new direntry,
	  budget for dirtied inode will be released via writeback.
  vfs_dent_type - get VFS directory entry type.
  @type: UBIFS directory entry type
  This function converts UBIFS directory entry type into VFS directory entry
  type.
  The classical Unix view for directory is that it is a linear array of
  (name, inode number) entries. LinuxVFS assumes this model as well.
  Particularly, 'readdir()' call wants us to return a directory entry offset
  which later may be used to continue 'readdir()'ing the directory or to
  'seek()' to that specific direntry. Obviously UBIFS does not really fit this
  model because directory entries are identified by keys, which may collide.
  UBIFS uses directory entry hash value for directory offsets, so
  'seekdir()''telldir()' may not always work because of possible key
  collisions. But UBIFS guarantees that consecutive 'readdir()' calls work
  properly by means of saving full directory entry name in the private field
  of the file description object.
  This means that UBIFS cannot support NFS which requires full
  'seekdir()''telldir()' support.
		
		  The directory was seek'ed to a senseless position or there
		  are no more entries.
		
		  The file was seek'ed, which means that @file->private_data
		  is now invalid. This may also be just the first
		  'ubifs_readdir()' invocation, in which case
		  @file->private_data is NULL, and the below code is
		  basically a no-op.
	
	  'generic_file_llseek()' unconditionally sets @file->f_version to
	  zero, and we use this for detecting whether the file was seek'ed.
 File positions 0 and 1 correspond to "." and ".." 
 Find the first entry in TNC and save it 
		
		  The directory was seek'ed to and is now readdir'ed.
		  Find the entry corresponding to @ctx->pos or the closest one.
 Switch to the next entry 
		
		  -ENOENT is a non-fatal error in this context, the TNC uses
		  it to indicate that the cursor moved past the current directory
		  and readdir() has to stop.
 2 is a special value indicating that there are no more direntries 
 Free saved readdir() state when the directory is closed 
  lock_2_inodes - a wrapper for locking two UBIFS inodes.
  @inode1: first inode
  @inode2: second inode
  We do not implement any tricks to guarantee strict lock ordering, because
  VFS has already done it for us on the @i_mutex. So this is just a simple
  wrapper function.
  unlock_2_inodes - a wrapper for unlocking two UBIFS inodes.
  @inode1: first inode
  @inode2: second inode
	
	  Budget request settings: new direntry, changing the target inode,
	  changing the parent inode.
 Handle O_TMPFILE corner case, it is allowed to link a O_TMPFILE. 
	
	  Budget request settings: deletion direntry, deletion inode (+1 for
	  @dirtied_ino), changing the parent directory inode. If budgeting
	  fails, go ahead anyway because we have extra space reserved for
	  deletions.
 We've deleted something - clean the "no space" flags 
  check_dir_empty - check if a directory is empty or not.
  @dir: VFS inode object of the directory to check
  This function checks if directory @dir is empty. Returns zero if the
  directory is empty, %-ENOTEMPTY if it is not, and other negative error codes
  in case of errors.
	
	  Budget request settings: deletion direntry, deletion inode and
	  changing the parent inode. If budgeting fails, go ahead anyway
	  because we have extra space reserved for deletions.
 We've deleted something - clean the "no space" flags 
	
	  Budget request settings: new inode, new direntry and changing parent
	  directory inode.
	
	  Budget request settings: new inode, new direntry and changing parent
	  directory inode.
	
	  Budget request settings: new inode, new direntry and changing parent
	  directory inode.
 encrypt directly into ui->data 
	
	  The terminating zero byte is not written to the flash media and it
	  is put just to make later in-memory string processing simpler. Thus,
	  data length is @disk_link.len - 1, not @disk_link.len.
  lock_4_inodes - a wrapper for locking three UBIFS inodes.
  @inode1: first inode
  @inode2: second inode
  @inode3: third inode
  @inode4: fouth inode
  This function is used for 'ubifs_rename()' and @inode1 may be the same as
  @inode2 whereas @inode3 and @inode4 may be %NULL.
  We do not implement any tricks to guarantee strict lock ordering, because
  VFS has already done it for us on the @i_mutex. So this is just a simple
  wrapper function.
  unlock_4_inodes - a wrapper for unlocking three UBIFS inodes for rename.
  @inode1: first inode
  @inode2: second inode
  @inode3: third inode
  @inode4: fouth inode
	
	  Budget request settings: deletion direntry, new direntry, removing
	  the old inode, and changing old and new parent directory inodes.
	 
	  However, this operation also marks the target inode as dirty and
	  does not write it, so we allocate budget for the target inode
	  separately.
	
	  Like most other Unix systems, set the @i_ctime for inodes on a
	  rename.
 We must adjust parent link count when renaming directories 
			
			  @old_dir loses a link because we are moving
			  @old_inode to a different directory.
			
			  @new_dir only gains a link if we are not also
			  overwriting an existing directory.
			
			  @old_inode is not moving to a different directory,
			  but @old_dir still loses a link if we are
			  overwriting an existing directory.
	
	  And finally, if we unlinked a direntry which happened to have the
	  same name as the moved direntry, we have to decrement @i_nlink of
	  the unlinked inode and change its ctime.
		
		  Directories cannot have hard-links, so if this is a
		  directory, just clear @i_nlink.
	
	  Do not ask 'ubifs_jnl_rename()' to flush write-buffer if @old_inode
	  is dirty, because this will be done later on at the end of
	  'ubifs_rename()'.
	
	  Unfortunately, the 'stat()' system call was designed for block
	  device based file systems, and it is not appropriate for UBIFS,
	  because UBIFS does not have notion of "block". For example, it is
	  difficult to tell how many block a directory takes - it actually
	  takes less than 300 bytes, but we have to round it to block size,
	  which introduces large mistake. This makes utilities like 'du' to
	  report completely senseless numbers. This is the reason why UBIFS
	  goes the same way as JFFS2 - it reports zero blocks for everything
	  but regular files, which makes more sense than reporting completely
	  wrong sizes.
		
		  Note, user-space expects 512-byte blocks count irrespectively
		  of what was reported in @stat->size.
 SPDX-License-Identifier: GPL-2.0
	
	  Creating an encryption context is done unlocked since we
	  operate on a new inode which is not visible to other users
	  at this point. So, no need to check whether inode is locked.
 pad to full block cipher length 
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
  This file implements the budgeting sub-system which is responsible for UBIFS
  space management.
  Factors such as compression, wasted space at the ends of LEBs, space in other
  journal heads, the effect of updates on the index, and so on, make it
  impossible to accurately predict the amount of space needed. Consequently
  approximations are used.
  When pessimistic budget calculations say that there is no enough space,
  UBIFS starts writing back dirty inodes and pages, doing garbage collection,
  or committing. The below constant defines maximum number of times UBIFS
  repeats the operations.
  The below constant defines amount of dirty pages which should be written
  back at when trying to shrink the liability.
  shrink_liability - write-back some dirty pagesinodes.
  @c: UBIFS file-system description object
  @nr_to_write: how many dirty pages to write-back
  This function shrinks UBIFS liability by means of writing back some amount
  of dirty inodes and their pages.
  Note, this function synchronizes even VFS inodes which are locked
  (@i_mutex) by the caller of the budgeting function, because write-back does
  not touch @i_mutex.
  run_gc - run garbage collector.
  @c: UBIFS file-system description object
  This function runs garbage collector to make some more free space. Returns
  zero if a free LEB has been produced, %-EAGAIN if commit is required, and a
  negative error code in case of failure.
 Make some free space by garbage-collecting dirty space 
 GC freed one LEB, return it to lprops 
  get_liability - calculate current liability.
  @c: UBIFS file-system description object
  This function calculates and returns current UBIFS liability, i.e. the
  amount of bytes UBIFS has "promised" to write to the media.
  make_free_space - make more free space on the file-system.
  @c: UBIFS file-system description object
  This function is called when an operation cannot be budgeted because there
  is supposedly no free space. But in most cases there is some free space:
    o budgeting is pessimistic, so it always budgets more than it is actually
      needed, so shrinking the liability is one way to make free space - the
      cached data will take less space then it was budgeted for;
    o GC may turn some dark space into free space (budgeting treats dark space
      as not available);
    o commit may free some LEB, i.e., turn freeable LEBs into free LEBs.
  So this function tries to do the above. Returns %-EAGAIN if some free space
  was presumably made and the caller has to re-try budgeting the operation.
  Returns %-ENOSPC if it couldn't do more free space, and other negative error
  codes on failures.
		
		  We probably have some dirty pages or inodes (liability), try
		  to write them back.
 Liability did not shrink again, try GC 
 Some real error happened 
  ubifs_calc_min_idx_lebs - calculate amount of LEBs for the index.
  @c: UBIFS file-system description object
  This function calculates and returns the number of LEBs which should be kept
  for index usage.
 And make sure we have thrice the index size of space reserved 
	
	  We do not maintain 'old_idx_size' as 'old_idx_lebs''old_idx_bytes'
	  pair, nor similarly the two variables for the new index size, so we
	  have to do this costly 64-bit division on fast-path.
	
	  The index head is not available for the in-the-gaps method, so add an
	  extra LEB to compensate.
  ubifs_calc_available - calculate available FS space.
  @c: UBIFS file-system description object
  @min_idx_lebs: minimum number of LEBs reserved for the index
  This function calculates and returns amount of FS space available for use.
	
	  Now 'available' contains theoretically available flash space
	  assuming there is no index, so we have to subtract the space which
	  is reserved for the index.
 Take into account that GC reserves one LEB for its own needs 
	
	  The GC journal head LEB is not really accessible. And since
	  different write types go to different heads, we may count only on
	  one head's space.
 We also reserve one LEB for deletions, which bypass budgeting 
 Subtract the dead space which is not available for use 
	
	  Subtract dark space, which might or might not be usable - it depends
	  on the data which we have on the media and which will be written. If
	  this is a lot of uncompressed or not-compressible data, the dark
	  space cannot be used.
	
	  However, there is more dark space. The index may be bigger than
	  @min_idx_lebs. Those extra LEBs are assumed to be available, but
	  their dark space is not included in total_dark, so it is subtracted
	  here.
 The calculations are rough and may end up with a negative number 
  can_use_rp - check whether the user is allowed to use reserved pool.
  @c: UBIFS file-system description object
  UBIFS has so-called "reserved pool" which is flash space reserved
  for the superuser and for uses whose UIDGID is recorded in UBIFS superblock.
  This function checks whether current user is allowed to use reserved pool.
  Returns %1  current user is allowed to use reserved pool and %0 otherwise.
  do_budget_space - reserve flash space for index and data growth.
  @c: UBIFS file-system description object
  This function makes sure UBIFS has enough free LEBs for index growth and
  data.
  When budgeting index space, UBIFS reserves thrice as many LEBs as the index
  would take if it was consolidated and written to the flash. This guarantees
  that the "in-the-gaps" commit method always succeeds and UBIFS will always
  be able to commit dirty index. So this function basically adds amount of
  budgeted index space to the size of the current index, multiplies this by 3,
  and makes sure this does not exceed the amount of free LEBs.
  Notes about @c->bi.min_idx_lebs and @c->lst.idx_lebs variables:
  o @c->lst.idx_lebs is the number of LEBs the index currently uses. It might
     be large, because UBIFS does not do any index consolidation as long as
     there is free space. IOW, the index may take a lot of LEBs, but the LEBs
     will contain a lot of dirt.
  o @c->bi.min_idx_lebs is the number of LEBS the index presumably takes. IOW,
     the index may be consolidated to take up to @c->bi.min_idx_lebs LEBs.
  This function returns zero in case of success, and %-ENOSPC in case of
  failure.
 First budget index space 
 Now 'min_idx_lebs' contains number of LEBs to reserve 
	
	  The number of LEBs that are available to be used by the index is:
	 
	     @c->lst.empty_lebs + @c->freeable_cnt + @c->idx_gc_cnt -
	     @c->lst.taken_empty_lebs
	 
	  @c->lst.empty_lebs are available because they are empty.
	  @c->freeable_cnt are available because they contain only free and
	  dirty space, @c->idx_gc_cnt are available because they are index
	  LEBs that have been garbage collected and are awaiting the commit
	  before they can be used. And the in-the-gaps method will grab these
	  if it needs them. @c->lst.taken_empty_lebs are empty LEBs that have
	  already been allocated for some purpose.
	 
	  Note, @c->idx_gc_cnt is included to both @c->lst.empty_lebs (because
	  these LEBs are empty) and to @c->lst.taken_empty_lebs (because they
	  are taken until after the commit).
	 
	  Note, @c->lst.taken_empty_lebs may temporarily be higher by one
	  because of the way we serialize LEB allocations and budgeting. See a
	  comment in 'ubifs_find_free_space()'.
  calc_idx_growth - calculate approximate index growth from budgeting request.
  @c: UBIFS file-system description object
  @req: budgeting request
  For now we assume each new node adds one znode. But this is rather poor
  approximation, though.
  calc_data_growth - calculate approximate amount of new data from budgeting
  request.
  @c: UBIFS file-system description object
  @req: budgeting request
  calc_dd_growth - calculate approximate amount of data which makes other data
  dirty from budgeting request.
  @c: UBIFS file-system description object
  @req: budgeting request
  ubifs_budget_space - ensure there is enough space to complete an operation.
  @c: UBIFS file-system description object
  @req: budget request
  This function allocates budget for an operation. It uses pessimistic
  approximation of how much flash space the operation needs. The goal of this
  function is to make sure UBIFS always has flash space to flush all dirty
  pages, dirty inodes, and dirty znodes (liability). This function may force
  commit, garbage-collection or write-back. Returns zero in case of success,
  %-ENOSPC if there is no free space and other negative error codes in case of
  failures.
 Restore the old values 
  ubifs_release_budget - release budgeted free space.
  @c: UBIFS file-system description object
  @req: budget request
  This function releases the space budgeted by 'ubifs_budget_space()'. Note,
  since the index changes (which were budgeted for in @req->idx_growth) will
  only be written to the media on commit, this function moves the index budget
  from @c->bi.idx_growth to @c->bi.uncommitted_idx. The latter will be zeroed
  by the commit operation.
  ubifs_convert_page_budget - convert budget of a new page.
  @c: UBIFS file-system description object
  This function converts budget which was allocated for a new page of data to
  the budget of changing an existing page of data. The latter is smaller than
  the former, so this function only does simple re-calculation and does not
  involve any write-back.
 Release the index growth reservation 
 Release the data growth reservation 
 Increase the dirty data growth reservation instead 
 And re-calculate the indexing space reservation 
  ubifs_release_dirty_inode_budget - release dirty inode budget.
  @c: UBIFS file-system description object
  @ui: UBIFS inode to release the budget for
  This function releases budget corresponding to a dirty inode. It is usually
  called when after the inode has been written to the media and marked as
  clean. It also causes the "no space" flags to be cleared.
 The "no space" flags will be cleared because dd_growth is > 0 
  ubifs_reported_space - calculate reported free space.
  @c: the UBIFS file-system description object
  @free: amount of free space
  This function calculates amount of free space which will be reported to
  user-space. User-space application tend to expect that if the file-system
  (e.g., via the 'statfs()' call) reports that it has N bytes available, they
  are able to write a file of size N. UBIFS attaches node headers to each data
  node and it has to write indexing nodes as well. This introduces additional
  overhead, and UBIFS has to report slightly less free space to meet the above
  expectations.
  This function assumes free space is made up of uncompressed data nodes and
  full index nodes (one per data node, tripled because we always allow enough
  space to write the index thrice).
  Note, the calculation is pessimistic, which means that most of the time
  UBIFS reports less space than it actually has.
	
	  Reported space size is @free  X, where X is UBIFS block size
	  divided by UBIFS block size + all overhead one data block
	  introduces. The overhead is the node header + indexing overhead.
	 
	  Indexing overhead calculations are based on the following formula:
	  I = N(f - 1) + 1, where I - number of indexing nodes, N - number
	  of data nodes, f - fanout. Because effective UBIFS fanout is twice
	  as less than maximum fanout, we assume that each data node
	  introduces 3  @c->max_idx_node_sz  (@c->fanout2 - 1) bytes.
	  Note, the multiplier 3 is because UBIFS reserves thrice as more space
	  for the index.
  ubifs_get_free_space_nolock - return amount of free space.
  @c: UBIFS file-system description object
  This function calculates amount of free space to report to user-space.
  Because UBIFS may introduce substantial overhead (the index, node headers,
  alignment, wastage at the end of LEBs, etc), it cannot report real amount of
  free flash space it has (well, because not all dirty space is reclaimable,
  UBIFS does not actually know the real amount). If UBIFS did so, it would
  bread user expectations about what free space is. Users seem to accustomed
  to assume that if the file-system reports N bytes of free space, they would
  be able to fit a file of N bytes to the FS. This almost works for
  traditional file-systems, because they have way less overhead than UBIFS.
  So, to keep users happy, UBIFS tries to take the overhead into account.
	
	  When reporting free space to user-space, UBIFS guarantees that it is
	  possible to write a file of free space size. This means that for
	  empty LEBs we may use more precise calculations than
	  'ubifs_calc_available()' is using. Namely, we know that in empty
	  LEBs we would waste only @c->leb_overhead bytes, not @c->dark_wm.
	  Thus, amend the available space.
	 
	  Note, the calculations below are similar to what we have in
	  'do_budget_space()', so refer there for comments.
  ubifs_get_free_space - return amount of free space.
  @c: UBIFS file-system description object
  This function calculates and returns amount of free space to report to
  user-space.
 SPDX-License-Identifier: GPL-2.0
 Normal UBIFS messages 
 UBIFS error messages 
 UBIFS warning messages 
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Artem Bityutskiy ( )
           Adrian Hunter
  This file implements VFS file and inode operations for regular files, device
  nodes and symlinks as well as address space operations.
  UBIFS uses 2 page flags: @PG_private and @PG_checked. @PG_private is set if
  the page is dirty and is used for optimization purposes - dirty pages are
  not budgeted so the flag shows that 'ubifs_write_end()' should not release
  the budget for this page. The @PG_checked flag is set if full budgeting is
  required for the page e.g., when it corresponds to a file hole or it is
  beyond the file size. The budgeting is done in 'ubifs_write_begin()', because
  it is OK to fail in this function, and the budget is released in
  'ubifs_write_end()'. So the @PG_private and @PG_checked flags carry
  information about how the page was budgeted, to make it possible to release
  the budget properly.
  A thing to keep in mind: inode @i_mutex is locked in most VFS operations we
  implement. However, this is not true for 'ubifs_writepage()', which may be
  called with @i_mutex unlocked. For example, when flusher thread is doing
  background write-back, it calls 'ubifs_writepage()' with unlocked @i_mutex.
  At "normal" work-paths the @i_mutex is locked in 'ubifs_writepage()', e.g.
  in the "sys_write -> alloc_pages -> direct reclaim path". So, in
  'ubifs_writepage()' we are only guaranteed that the page is locked.
  Similarly, @i_mutex is not always locked in 'ubifs_readpage()', e.g., the
  read-ahead path does not lock it ("sys_read -> generic_file_aio_read ->
  ondemand_readahead -> readpage"). In case of readahead, @I_SYNC flag is not
  set as well. However, UBIFS disables readahead.
 Not found, so it must be a hole 
	
	  Data length can be less than a full block, even for blocks that are
	  not the last in the file (e.g., as a result of making a hole and
	  appending data). Ensure that the remainder is zeroed out.
 Reading beyond inode 
 Reading beyond inode 
 Not found, so it must be a hole 
  release_new_page_budget - release budget of a new page.
  @c: UBIFS file-system description object
  This is a helper function which releases budget corresponding to the budget
  of one new page of data.
  release_existing_page_budget - release budget of an existing page.
  @c: UBIFS file-system description object
  This is a helper function which releases budget corresponding to the budget
  of changing one page of data which already exists on the flash media.
	
	  At the slow path we have to budget before locking the page, because
	  budgeting may force write-back, which would wait on locked pages and
	  deadlock if we had the page locked. At this point we do not know
	  anything about the page, so assume that this is a new page which is
	  written to a hole. This corresponds to largest budget. Later the
	  budget will be amended if this is not true.
 We are appending data, budget for inode change 
		
		  The page is dirty, which means it was budgeted twice:
		    o first time the budget was allocated by the task which
		      made the page dirty and set the PG_private flag;
		    o and then we budgeted for it for the second time at the
		      very beginning of this function.
		 
		  So what we have to do is to release the page budget we
		  allocated.
		
		  We are changing a page which already exists on the media.
		  This means that changing the page does not make the amount
		  of indexing information larger, and this part of the budget
		  which we have already acquired may be released.
		
		  'ubifs_write_end()' is optimized from the fast-path part of
		  'ubifs_write_begin()' and expects the @ui_mutex to be locked
		  if data is appended.
			
			  The inode is dirty already, so we may free the
			  budget we allocated.
  allocate_budget - allocate budget for 'ubifs_write_begin()'.
  @c: UBIFS file-system description object
  @page: page to allocate budget for
  @ui: UBIFS inode object the page belongs to
  @appending: non-zero if the page is appended
  This is a helper function for 'ubifs_write_begin()' which allocates budget
  for the operation. The budget is allocated differently depending on whether
  this is appending, whether the page is dirty or not, and so on. This
  function leaves the @ui->ui_mutex locked in case of appending. Returns zero
  in case of success and %-ENOSPC in case of failure.
			
			  The page is dirty and we are not appending, which
			  means no budget is needed at all.
			
			  The page is dirty and we are appending, so the inode
			  has to be marked as dirty. However, it is already
			  dirty, so we do not need any budget. We may return,
			  but @ui->ui_mutex hast to be left locked because we
			  should prevent write-back from flushing the inode
			  and freeing the budget. The lock will be released in
			  'ubifs_write_end()'.
		
		  The page is dirty, we are appending, the inode is clean, so
		  we need to budget the inode change.
			
			  The page corresponds to a hole and does not
			  exist on the media. So changing it makes
			  make the amount of indexing information
			  larger, and we have to budget for a new
			  page.
			
			  Not a hole, the change will not add any new
			  indexing information, budget for page
			  change.
				
				  The inode is clean but we will have to mark
				  it as dirty because we are appending. This
				  needs a budget.
  This function is called when a page of data is going to be written. Since
  the page of data will not necessarily go to the flash straight away, UBIFS
  has to reserve space on the media for it, which is done by means of
  budgeting.
  This is the hot-path of the file-system and we are trying to optimize it as
  much as possible. For this reasons it is split on 2 parts - slow and fast.
  There many budgeting cases:
      o a new page is appended - we have to budget for a new page and for
        changing the inode; however, if the inode is already dirty, there is
        no need to budget for it;
      o an existing clean page is changed - we have budget for it; if the page
        does not exist on the media (a hole), we have to budget for a new
        page; otherwise, we may budget for changing an existing page; the
        difference between these cases is that changing an existing page does
        not introduce anything new to the FS indexing information, so it does
        not grow, and smaller budget is acquired in this case;
      o an existing dirty page is changed - no need to budget at all, because
        the page budget has been acquired by earlier, when the page has been
        marked dirty.
  UBIFS budgeting sub-system may force write-back if it thinks there is no
  space to reserve. This imposes some locking restrictions and makes it
  impossible to take into account the above cases, and makes it impossible to
  optimize budgeting.
  The solution for this is that the fast path of 'ubifs_write_begin()' assumes
  there is a plenty of flash space and the budget will be acquired quickly,
  without forcing write-back. The slow path does not make this assumption.
 Try out the fast-path part first 
 The page is not loaded from the flash 
			
			  We change whole page so no need to load it. But we
			  do not know whether this page exists on the media or
			  not, so we assume the latter because it requires
			  larger budget. The assumption is that it is better
			  to budget a bit more than to read the page from the
			  media. Thus, we are setting the @PG_checked flag
			  here.
		
		  If we skipped reading the page because we were going to
		  write all of it, then it is not up to date.
		
		  Budgeting failed which means it would have to force
		  write-back but didn't, because we set the @fast flag in the
		  request. Write-back cannot be done now, while we have the
		  page locked, because it would deadlock. Unlock and free
		  everything and fall-back to slow-path.
	
	  Whee, we acquired budgeting quickly - without involving
	  garbage-collection, committing or forcing write-back. We return
	  with @ui->ui_mutex locked if we are appending pages, and unlocked
	  otherwise. This is an optimization (slightly hacky though).
  cancel_budget - cancel budget.
  @c: UBIFS file-system description object
  @page: page to cancel budget for
  @ui: UBIFS inode object the page belongs to
  @appending: non-zero if the page is appended
  This is a helper function for a page write operation. It unlocks the
  @ui->ui_mutex in case of appending.
		
		  VFS copied less data to the page that it intended and
		  declared in its '->write_begin()' call via the @len
		  argument. If the page was not up-to-date, and @len was
		  @PAGE_SIZE, the 'ubifs_write_begin()' function did
		  not load it from the media (for optimization reasons). This
		  means that part of the page contains garbage. So read the
		  page now.
		
		  Return 0 to force VFS to repeat the whole operation, or the
		  error code if 'do_readpage()' fails.
		
		  Note, we do not set @I_DIRTY_PAGES (which means that the
		  inode has dirty pages), this has been done in
		  '__set_page_dirty_nobuffers()'.
  populate_page - copy data nodes into a page for bulk-read.
  @c: UBIFS file-system description object
  @page: page
  @bu: bulk-read information
  @n: next zbranch slot
  This function returns %0 on success and a negative error code on failure.
  ubifs_do_bulk_read - do bulk-read.
  @c: UBIFS file-system description object
  @bu: bulk-read information
  @page1: first page to read
  This function returns %1 if the bulk-read is done, otherwise %0 is returned.
 Turn off bulk-read at the end of the file 
		
		  This happens when there are multiple blocks per page and the
		  blocks for the first page we are looking for, are not
		  together. If all the pages were like this, bulk-read would
		  reduce performance, so we turn it off for a while.
			
			  Allocate bulk-read buffer depending on how many data
			  nodes we are going to read.
  ubifs_bulk_read - determine whether to bulk-read and, if so, do it.
  @page: page from which to start bulk-read.
  Some flash media are capable of reading sequentially at faster rates. UBIFS
  bulk-read facility is designed to take advantage of that, by reading in one
  go consecutive data nodes that are also located consecutively in the same
  LEB. This function returns %1 if a bulk-read is done and %0 otherwise.
	
	  Bulk-read is protected by @ui->ui_mutex, but it is an optimization,
	  so don't bother if we cannot lock the mutex.
 Turn off bulk-read if we stop reading sequentially 
 Three reads in a row, so switch on bulk-read 
	
	  If possible, try to use pre-allocated bulk-read information, which
	  is protected by @c->bu_mutex.
 Update radix tree tags 
  When writing-back dirty inodes, VFS first writes-back pages belonging to the
  inode, then the inode itself. For UBIFS this may cause a problem. Consider a
  situation when a we have an inode with size 0, then a megabyte of data is
  appended to the inode, then write-back starts and flushes some amount of the
  dirty pages, the journal becomes full, commit happens and finishes, and then
  an unclean reboot happens. When the file system is mounted next time, the
  inode size would still be 0, but there would be many pages which are beyond
  the inode size, they would be indexed and consume flash space. Because the
  journal has been committed, the replay would not be able to detect this
  situation and correct the inode size. This means UBIFS would have to scan
  whole index and correct all inode sizes, which is long an unacceptable.
  To prevent situations like this, UBIFS writes pages back only if they are
  within the last synchronized inode size, i.e. the size which has been
  written to the flash media last time. Otherwise, UBIFS forces inode
  write-back, thus making sure the on-flash inode contains current inode size,
  and then keeps writing pages back.
  Some locking issues explanation. 'ubifs_writepage()' first is called with
  the page locked, and it locks @ui_mutex. However, write-back does take inode
  @i_mutex, which means other VFS operations may be run on this inode at the
  same time. And the problematic one is truncation to smaller size, from where
  we have to call 'truncate_setsize()', which first changes @inode->i_size,
  then drops the truncated pages. And while dropping the pages, it takes the
  page lock. This means that 'do_truncation()' cannot call 'truncate_setsize()'
  with @ui_mutex locked, because it would deadlock with 'ubifs_writepage()'.
  This means that @inode->i_size is changed while @ui_mutex is unlocked.
  XXX(truncate): with the new truncate sequence this is not true anymore,
  and the calls to truncate_setsize can be move around freely.  They should
  be moved to the very end of the truncate sequence.
  But in 'ubifs_writepage()' we have to guarantee that we do not write beyond
  inode size. How do we do this if @inode->i_size may became smaller while we
  are in the middle of 'ubifs_writepage()'? The UBIFS solution is the
  @ui->ui_isize "shadow" field which UBIFS uses instead of @inode->i_size
  internally and updates it under @ui_mutex.
  Q: why we do not worry that if we race with truncation, we may end up with a
  situation when the inode is truncated while we are in the middle of
  'do_writepage()', so we do write beyond inode size?
  A: If we are in the middle of 'do_writepage()', truncation would be locked
  on the page lock and it would not write the truncated inode node to the
  journal before we have finished.
 Is the page fully outside @i_size? (truncate in progress) 
 Is the page fully inside @i_size? 
			
			  The inode has been written, but the write-buffer has
			  not been synchronized, so in case of an unclean
			  reboot we may end up with some pages beyond inode
			  size, but they would be in the journal (because
			  commit flushes write buffers) and recovery would deal
			  with this.
	
	  The page straddles @i_size. It must be zeroed out on each and every
	  writepage invocation because it may be mmapped. "A file is mapped
	  in multiples of the page size. For a file that is not a multiple of
	  the page size, the remaining memory is zeroed when mapped, and
	  writes to that region are not written out to the file."
  do_attr_changes - change inode attributes.
  @inode: inode to change attributes for
  @attr: describes attributes to change
  do_truncation - truncate an inode.
  @c: UBIFS file-system description object
  @inode: inode to truncate
  @attr: inode attribute changes description
  This function implements VFS '->setattr()' call when the inode is truncated
  to a smaller size. Returns zero in case of success and a negative error code
  in case of failure.
	
	  If this is truncation to a smaller size, and we do not truncate on a
	  block boundary, budget for changing one data block, because the last
	  block will be re-written.
 A funny way to budget for truncation node 
		
		  Treat truncations to zero as deletion and always allow them,
		  just like we do for '->unlink()'.
				
				  'ubifs_jnl_truncate()' will try to truncate
				  the last data node, but it contains
				  out-of-date data because the page is dirty.
				  Write the page now, so that
				  'ubifs_jnl_truncate()' will see an already
				  truncated (and up to date) data node.
				
				  We could now tell 'ubifs_jnl_truncate()' not
				  to read the last block.
				
				  We could 'kmap()' the page and pass the data
				  to 'ubifs_jnl_truncate()' to save it from
				  having to read it.
 Truncation changes inode [mc]time 
 Other attributes may be changed at the same time as well 
  do_setattr - change inode attributes.
  @c: UBIFS file-system description object
  @inode: inode to change attributes for
  @attr: inode attribute changes description
  This function implements VFS '->setattr()' call for all cases except
  truncations to smaller size. Returns zero in case of success and a negative
  error code in case of failure.
 Truncation changes inode [mc]time 
 'truncate_setsize()' changed @i_size, update @ui_size 
		
		  Inode length changed, so we have to make sure
		  @I_DIRTY_DATASYNC is set.
 Truncation to a smaller size 
 Partial page remains dirty 
		
		  For some really strange reasons VFS does not filter out
		  'fsync()' for RO mounted file-systems as per 2.6.39.
 Synchronize the inode unless this is a 'datasync()' call. 
	
	  Nodes related to this inode may still sit in a write-buffer. Flush
	  them.
  mctime_update_needed - check if mtime or ctime update is needed.
  @inode: the inode to do the check for
  @now: current time
  This helper function checks if the inode mtimectime should be updated or
  not. If current values of the time-stamps are within the UBIFS inode time
  granularity, they are not updated. This is an optimization.
  ubifs_update_time - update time of inode.
  @inode: inode to update
  This function updates time of the inode.
  update_mctime - update mtime and ctime of an inode.
  @inode: inode to update
  This function updates mtime and ctime of the inode if it is not equivalent to
  current time. Returns zero in case of success and a negative error code in
  case of failure.
	
	  An attempt to dirty a page without budgeting for it - should not
	  happen.
	
	  An attempt to release a dirty page without budgeting for it - should
	  not happen.
  mmap()d file has taken write protection fault and is being made writable.
  UBIFS must ensure page is budgeted for.
 -EROFS 
	
	  We have not locked @page so far so we may budget for changing the
	  page. Note, we cannot do this after we locked the page, because
	  budgeting may cause write-back which would cause deadlock.
	 
	  At the moment we do not know whether the page is dirty or not, so we
	  assume that it is not and budget for a new page. We could look at
	  the @PG_private flag and figure this out, but we may race with write
	  back and the page state may change by the time we lock it, so this
	  would need additional care. We do not bother with this at the
	  moment, although it might be good idea to do. Instead, we allocate
	  budget for a new page and amend it later on if the page was in fact
	  dirty.
	 
	  The budgeting-related logic of this function is similar to what we
	  do in 'ubifs_write_begin()' and 'ubifs_write_end()'. Glance there
	  for more comments.
		
		  We have to change inode time stamp which requires extra
		  budgeting.
 Page got truncated out from underneath us 
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Artem Bityutskiy ( )
           Adrian Hunter
  This file implements UBIFS superblock. The superblock is stored at the first
  LEB of the volume and is never changed by UBIFS. Only user-space tools may
  change it. The superblock node mostly contains geometry information.
  Default journal size in logical eraseblocks as a percent of total
  flash size.
 Default maximum journal size in bytes 
 Default indexing tree fanout 
 Default number of data journal heads 
 Default positions of different LEBs in the main area 
 Default number of LEB numbers in LPT's save table 
 Default reserved pool size as a percent of maximum free space 
 The default maximum size of reserved pool in bytes 
 Default time granularity in nanoseconds 
  create_default_filesystem - format empty UBI volume.
  @c: UBIFS file-system description object
  This function creates default empty file-system. Returns zero in case of
  success and a negative error code in case of failure.
 Some functions called from here depend on the @c->key_len filed 
	
	  First of all, we have to calculate default file-system geometry -
	  log size, journal size, etc.
 We can first multiply then divide and have no overflow 
	
	  The log should be large enough to fit reference nodes for all bud
	  LEBs. Because buds do not have to start from the beginning of LEBs
	  (half of the LEB may contain committed data), the log should
	  generally be larger, make it twice as large.
 Plus one LEB reserved for commit 
 And some extra space to allow writes while committing 
	
	  Orphan nodes are stored in a separate area. One node can store a lot
	  of orphan inode numbers, but when new orphan comes we just add a new
	  orphan node. At some point the nodes are consolidated into one
	  orphan node.
		
		  For debugging purposes it is better to have at least 2
		  orphan LEBs, because the orphan subsystem would need to do
		  consolidations and would be stressed more.
 Create default superblock 
 Create default master node 
 Calculate lprops statistics 
  The indexing LEB does not contribute to dark space 
 Create the root indexing node 
 Create default root inode 
 Set compression enabled by default 
	
	  The first node in the log has to be the commit start node. This is
	  always the case during normal file-system operation. Write a fake
	  commit start node to the log.
  validate_sb - validate superblock node.
  @c: UBIFS file-system description object
  @sup: superblock node
  This function validates superblock node @sup. Since most of data was read
  from the superblock and stored in @c, the function validates fields in @c
  instead. Returns zero in case of success and %-EINVAL in case of validation
  failure.
	
	  Calculate minimum allowed amount of main area LEBs. This is very
	  similar to %UBIFS_MIN_LEB_CNT, but we take into account real what we
	  have just read from the superblock.
  ubifs_read_sb_node - read superblock node.
  @c: UBIFS file-system description object
  This function returns a pointer to the superblock node or a negative error
  code. Note, the user of this function is responsible of kfree()'ing the
  returned superblock buffer.
	
	  The super block node can either be authenticated by a HMAC or
	  by a signature in a ubifs_sig_node directly following the
	  super block node to support offline image creation.
  ubifs_write_sb_node - write superblock node.
  @c: UBIFS file-system description object
  @sup: superblock node read with 'ubifs_read_sb_node()'
  This function returns %0 on success and a negative error code on failure.
  ubifs_read_superblock - read superblock.
  @c: UBIFS file-system description object
  This function finds, reads and checks the superblock. If an empty UBI volume
  is being mounted, this function creates default superblock. Returns zero in
  case of success, and a negative error code in case of failure.
	
	  The software supports all previous versions but not future versions,
	  due to the unavailability of time-travelling equipment.
		
		  The FS is mounted RO, and the media format is
		  RO-compatible with the UBIFS implementation, so we can
		  mount.
 Automatically increase file system size to the maximum size 
  fixup_leb - fixupunmap an LEB containing free space.
  @c: UBIFS file-system description object
  @lnum: the LEB number to fix up
  @len: number of used bytes in LEB (starting at offset 0)
  This function reads the contents of the given LEB number @lnum, then fixes
  it up, so that empty min. IO units in the end of LEB are actually erased on
  flash (rather than being just all-0xff real data). If the LEB is completely
  empty, it is simply unmapped.
  fixup_free_space - find & remap all LEBs containing free space.
  @c: UBIFS file-system description object
  This function walks through all LEBs in the filesystem and fiexes up those
  containing freeempty space.
 Fixup LEBs in the master area 
 Unmap unused log LEBs 
	
	  Fixup the log head which contains the only a CS node at the
	  beginning.
 Fixup LEBs in the LPT area 
 Unmap LEBs in the orphans area 
 Fixup LEBs in the main area 
  ubifs_fixup_free_space - find & fix all LEBs with free space.
  @c: UBIFS file-system description object
  This function fixes up LEBs containing free space on first mount, if the
  appropriate flag was set when the FS was created. Each LEB with one or more
  empty min. IO unit (i.e. free-space-count > 0) is re-written, to make sure
  the free space is actually erased. E.g., this is necessary for some NAND
  chips, since the free space may have been programmed like real "0xff" data
  (generating a non-0xff ECC), causing future writes to the not-really-erased
  NAND pages to behave badly. After the space is fixed up, the superblock flag
  is cleared, so that this is skipped for all future mounts.
 Free-space fixup is no longer required 
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
 This file implements TNC functions for committing 
  make_idx_node - make an index node for fill-the-gaps method of TNC commit.
  @c: UBIFS file-system description object
  @idx: buffer in which to place new index node
  @znode: znode from which to make new index node
  @lnum: LEB number where new index node will be written
  @offs: offset where new index node will be written
  @len: length of new index node
 Make index node 
 Update the parent 
	
	  Note, unlike 'write_index()' we do not add memory barriers here
	  because this function is called with @c->tnc_mutex locked.
  fill_gap - make index nodes in gaps in dirty index LEBs.
  @c: UBIFS file-system description object
  @lnum: LEB number that gap appears in
  @gap_start: offset of start of gap
  @gap_end: offset of end of gap
  @dirt: adds dirty space to this
  This function returns the number of index nodes written into the gap.
 Pad to end of min_io_size 
 Pad to end of gap 
  find_old_idx - find an index node obsoleted since the last commit start.
  @c: UBIFS file-system description object
  @lnum: LEB number of obsoleted index node
  @offs: offset of obsoleted index node
  Returns %1 if found and %0 otherwise.
  is_idx_node_in_use - determine if an index node can be overwritten.
  @c: UBIFS file-system description object
  @key: key of index node
  @level: index node level
  @lnum: LEB number of index node
  @offs: offset of index node
  If @key  @lnum  @offs identify an index node that was not part of the old
  index, then this function returns %0 (obsolete).  Else if the index node was
  part of the old index but is now dirty %1 is returned, else if it is clean %2
  is returned. A negative error code is returned on failure.
 Error code 
  layout_leb_in_gaps - layout index nodes using in-the-gaps method.
  @c: UBIFS file-system description object
  @p: return LEB number in @c->gap_lebs[p]
  This function lays out new index nodes for dirty znodes using in-the-gaps
  method of TNC commit.
  This function merely puts the next znode into the next gap, making no attempt
  to try to maximise the number of znodes that fit.
  This function returns the number of index nodes written into the gaps, or a
  negative error code on failure.
 Get an index LEB with lots of obsolete index nodes 
		
		  There also may be dirt in the index head that could be
		  filled, however we do not check there at present.
 Error code 
	
	  Scan the index LEB.  We use the generic scan for this even though
	  it is more comprehensive and less efficient than is needed for this
	  purpose.
 Determine if the index node is in use (not obsolete) 
 Error code 
			
			  The obsolete index nodes form gaps that can be
			  overwritten.  This gap has ended because we have
			  found an index node that is still in use
			  i.e. not obsolete
 Try to fill gap 
 Error code 
 Try to fill gap 
 Error code 
			
			  We must have snatched this LEB from the idx_gc list
			  so we need to correct the free and dirty space.
  get_leb_cnt - calculate the number of empty LEBs needed to commit.
  @c: UBIFS file-system description object
  @cnt: number of znodes to commit
  This function returns the number of empty LEBs needed to commit @cnt znodes
  to the current index head.  The number is not exact and may be more than
  needed.
 Assume maximum index node size (i.e. overestimate space needed) 
  layout_in_gaps - in-the-gaps method of committing TNC.
  @c: UBIFS file-system description object
  @cnt: number of dirty znodes to commit.
  This function lays out new index nodes for dirty znodes using in-the-gaps
  method of TNC commit.
  This function returns %0 on success and a negative error code on failure.
				
				  Do not print scary warnings if the debugging
				  option which forces in-the-gaps is enabled.
 Try to commit anyway 
		
		  Dynamically change the size of @c->gap_lebs to prevent
		  oob, because @c->lst.idx_lebs could be increased by
		  function @get_idx_gc_leb (called by layout_leb_in_gaps->
		  ubifs_find_dirty_idx_leb) during loop. Only enlarge
		  @c->gap_lebs when needed.
		 
  layout_in_empty_space - layout index nodes in empty space.
  @c: UBIFS file-system description object
  This function lays out new index nodes for dirty znodes using empty LEBs.
  This function returns %0 on success and a negative error code on failure.
 Ensure there is enough room for first write 
 Determine the index node position 
 Update the parent 
		
		  Once lprops is updated, we can decrease the dirty znode count
		  but it is easier to just do it here.
		
		  Calculate the next index node length to see if there is
		  enough room for it
 Update buffer positions 
 The buffer is full or there are no more znodes to do 
  layout_commit - determine positions of index nodes to commit.
  @c: UBIFS file-system description object
  @no_space: indicates that insufficient empty LEBs were allocated
  @cnt: number of znodes to commit
  Calculate and update the positions of index nodes to commit.  If there were
  an insufficient number of empty LEBs allocated, then index nodes are placed
  into the gaps created by obsolete index nodes in non-empty index LEBs.  For
  this purpose, an obsolete index node is one that was not in the index as at
  the end of the last commit.  To write "in-the-gaps" requires that those index
  LEBs are updated atomically in-place.
  find_first_dirty - find first dirty znode.
  @znode: znode to begin searching from
  find_next_dirty - find next dirty znode.
  @znode: znode to begin searching from
  get_znodes_to_commit - create list of dirty znodes to commit.
  @c: UBIFS file-system description object
  This function returns the number of znodes to commit.
  alloc_idx_lebs - allocate empty LEBs to be used to commit.
  @c: UBIFS file-system description object
  @cnt: number of znodes to commit
  This function returns %-ENOSPC if it cannot allocate a sufficient number of
  empty LEBs.  %0 is returned on success, otherwise a negative error code
  is returned.
  free_unused_idx_lebs - free unused LEBs that were allocated for the commit.
  @c: UBIFS file-system description object
  It is possible that we allocate more empty LEBs for the commit than we need.
  This functions frees the surplus.
  This function returns %0 on success and a negative error code on failure.
  free_idx_lebs - free unused LEBs after commit end.
  @c: UBIFS file-system description object
  This function returns %0 on success and a negative error code on failure.
  ubifs_tnc_start_commit - start TNC commit.
  @c: UBIFS file-system description object
  @zroot: new index root position is returned here
  This function prepares the list of indexing nodes to commit and lays out
  their positions on flash. If there is not enough free space it uses the
  in-gap commit method. Returns zero in case of success and a negative error
  code in case of failure.
	
	  Although we have not finished committing yet, update size of the
	  committed index ('c->bi.old_idx_sz') and zero out the index growth
	  budget. It is OK to do this now, because we've reserved all the
	  space which is needed to commit the index, and it is save for the
	  budgeting subsystem to assume the index is already committed,
	  even though it is not.
  write_index - write index nodes.
  @c: UBIFS file-system description object
  This function writes the index nodes whose positions were laid out in the
  layout_in_empty_space function.
	
	  Always write index nodes to the index head so that index nodes and
	  other types of nodes are never mixed in the same erase block.
 Allocate commit buffer 
 Ensure there is enough room for first write 
 Make index node 
 Determine the index node position 
 Grab some stuff from znode while we still can 
		
		  It is important that other threads should see %DIRTY_ZNODE
		  flag cleared before %COW_ZNODE. Specifically, it matters in
		  the 'dirty_cow_znode()' function. This is the reason for the
		  first barrier. Also, we want the bit changes to be seen to
		  other threads ASAP, to avoid unnecessary copying, which is
		  the reason for the second barrier.
		
		  We have marked the znode as clean but have not updated the
		  @c->clean_zn_cnt counter. If this znode becomes dirty again
		  before 'free_obsolete_znodes()' is called, then
		  @c->clean_zn_cnt will be decremented before it gets
		  incremented (resulting in 2 decrements for the same znode).
		  This means that @c->clean_zn_cnt may become negative for a
		  while.
		 
		  Q: why we cannot increment @c->clean_zn_cnt?
		  A: because we do not have the @c->tnc_mutex locked, and the
		     following code would be racy and buggy:
		 
		     if (!ubifs_zn_obsolete(znode)) {
		             atomic_long_inc(&c->clean_zn_cnt);
		             atomic_long_inc(&ubifs_clean_zn_cnt);
		     }
		 
		     Thus, we just delay the @c->clean_zn_cnt update until we
		     have the mutex locked.
 Do not access znode from this point on 
 Update buffer positions 
		
		  Calculate the next index node length to see if there is
		  enough room for it
 The buffer is full or there are no more znodes to do 
  free_obsolete_znodes - free obsolete znodes.
  @c: UBIFS file-system description object
  At the end of commit end, obsolete znodes are freed.
  return_gap_lebs - return LEBs used by the in-gap commit method.
  @c: UBIFS file-system description object
  This function clears the "taken" flag for the LEBs which were used by the
  "commit in-the-gaps" method.
  ubifs_tnc_end_commit - update the TNC for commit end.
  @c: UBIFS file-system description object
  Write the dirty znodes.
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Artem Bityutskiy ( )
           Adrian Hunter
  This file implements UBIFS shrinker which evicts clean znodes from the TNC
  tree when Linux VM needs more RAM.
  We do not implement any LRU lists to find oldest znodes to free because it
  would add additional overhead to the file system fast paths. So the shrinker
  just walks the TNC tree when searching for znodes to free.
  If the root of a TNC sub-tree is clean and old enough, then the children are
  also clean and old enough. So the shrinker walks the TNC in level order and
  dumps entire sub-trees.
  The age of znodes is just the time-stamp when they were last looked at.
  The current shrinker first tries to evict old znodes, then young ones.
  Since the shrinker is global, it has to protect against races with FS
  un-mounts, which is done by the 'ubifs_infos_lock' and 'c->umount_mutex'.
 List of all UBIFS file-system instances 
  We number each shrinker run and record the number on the ubifs_info structure
  so that we can easily work out which ubifs_info structures have already been
  done by the current run.
 Protects 'ubifs_infos' list 
 Global clean znode counter (for all mounted UBIFS instances) 
  shrink_tnc - shrink TNC tree.
  @c: UBIFS file-system description object
  @nr: number of znodes to free
  @age: the age of znodes to free
  @contention: if any contention, this is set to %1
  This function traverses TNC tree and frees clean znodes. It does not free
  clean znodes which younger then @age. Returns number of freed znodes.
	
	  Traverse the TNC tree in levelorder manner, so that it is possible
	  to destroy large sub-trees. Indeed, if a znode is old, then all its
	  children are older or of the same age.
	 
	  Note, we are holding 'c->tnc_mutex', so we do not have to lock the
	  'c->space_lock' when _reading_ 'c->clean_zn_cnt', because it is
	  changed only when the 'c->tnc_mutex' is held.
		
		  If the znode is clean, but it is in the 'c->cnext' list, this
		  means that this znode has just been written to flash as a
		  part of commit and was marked clean. They will be removed
		  from the list at end commit. We cannot change the list,
		  because it is not protected by any mutex (design decision to
		  make commit really independent and parallel to main IO). So
		  we just skip these znodes.
		 
		  Note, the 'clean_zn_cnt' counters are not updated until
		  after the commit, so the UBIFS shrinker does not report
		  the znodes which are in the 'c->cnext' list as freeable.
		 
		  Also note, if the root of a sub-tree is not in 'c->cnext',
		  then the whole sub-tree is not in 'c->cnext' as well, so it
		  is safe to dump whole sub-tree.
			
			  Very soon these znodes will be removed from the list
			  and become freeable.
  shrink_tnc_trees - shrink UBIFS TNC trees.
  @nr: number of znodes to free
  @age: the age of znodes to free
  @contention: if any contention, this is set to %1
  This function walks the list of mounted UBIFS file-systems and frees clean
  znodes which are older than @age, until at least @nr znodes are freed.
  Returns the number of freed znodes.
 Iterate over all mounted UBIFS file-systems and try to shrink them 
		
		  We move the ones we do to the end of the list, so we stop
		  when we see one we have already done.
 Some un-mount is in progress, try next FS 
		
		  We're holding 'c->umount_mutex', so the file-system won't go
		  away.
		
		  OK, now we have TNC locked, the file-system cannot go away -
		  it is safe to reap the cache.
 Get the next list element before we move this one 
		
		  Move this one to the end of the list to provide some
		  fairness.
  kick_a_thread - kick a background thread to start commit.
  This function kicks a background thread to start background commit. Returns
  %-1 if a thread was kicked or there is another reason to assume the memory
  will soon be freed or become freeable. If there are no dirty znodes, returns
  %0.
	
	  Iterate over all mounted UBIFS file-systems and find out if there is
	  already an ongoing commit operation there. If no, then iterate for
	  the second time and initiate background commit.
				
				  Some un-mount is in progress, it will
				  certainly free memory, so just return.
	
	  Due to the way UBIFS updates the clean znode counter it may
	  temporarily be negative.
		
		  No clean znodes, nothing to reap. All we can do in this case
		  is to kick background threads to start commit, which will
		  probably make clean znodes which, in turn, will be freeable.
		  And we return -1 which means will make VM call us again
		  later.
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
  This file implements functions needed to recover from unclean un-mounts.
  When UBIFS is mounted, it checks a flag on the master node to determine if
  an un-mount was completed successfully. If not, the process of mounting
  incorporates additional checking and fixing of on-flash data structures.
  UBIFS always cleans away all remnants of an unclean un-mount, so that
  errors do not accumulate. However UBIFS defers recovery if it is mounted
  read-only, and the flash is not modified in that case.
  The general UBIFS approach to the recovery is that it recovers from
  corruptions which could be caused by power cuts, but it refuses to recover
  from corruption caused by other reasons. And UBIFS tries to distinguish
  between these 2 reasons of corruptions and silently recover in the former
  case and loudly complain in the latter case.
  UBIFS writes only to erased LEBs, so it writes only to the flash space
  containing only 0xFFs. UBIFS also always writes strictly from the beginning
  of the LEB to the end. And UBIFS assumes that the underlying flash media
  writes in @c->max_write_size bytes at a time.
  Hence, if UBIFS finds a corrupted node at offset X, it expects only the min.
  IO unit corresponding to offset X to contain corrupted data, all the
  following min. IO units have to contain empty space (all 0xFFs). If this is
  not true, the corruption cannot be the result of a power cut, and UBIFS
  refuses to mount.
  is_empty - determine whether a buffer is empty (contains all 0xff).
  @buf: buffer to clean
  @len: length of buffer
  This function returns %1 if the buffer is empty (contains all 0xff) otherwise
  %0 is returned.
  first_non_ff - find offset of the first non-0xff byte.
  @buf: buffer to search in
  @len: length of buffer
  This function returns offset of the first non-0xff byte in @buf or %-1 if
  the buffer contains only 0xff bytes.
  get_master_node - get the last valid master node allowing for corruption.
  @c: UBIFS file-system description object
  @lnum: LEB number
  @pbuf: buffer containing the LEB read, is returned here
  @mst: master node, if found, is returned here
  @cor: corruption, if found, is returned here
  This function allocates a buffer, reads the LEB into it, and finds and
  returns the last valid master node allowing for one area of corruption.
  The corrupt area, if there is one, must be consistent with the assumption
  that it is the result of an unclean unmount while the master node was being
  written. Under those circumstances, it is valid to use the previously written
  master node.
  This function returns %0 on success and a negative error code on failure.
 Find the first position that is definitely not a node 
 See if there was a valid master node before that 
 Could have been corruption so check one place back 
				
				  We accept only one area of corruption because
				  we are assuming that it was caused while
				  trying to write a master node.
 Check for corruption 
 Check remaining empty space 
  write_rcvrd_mst_node - write recovered master node.
  @c: UBIFS file-system description object
  @mst: master node
  This function returns %0 on success and a negative error code on failure.
  ubifs_recover_master_node - recover the master node.
  @c: UBIFS file-system description object
  This function recovers the master node from corruption that may occur due to
  an unclean unmount.
  This function returns %0 on success and a negative error code on failure.
			
			  mst1 was written by recovery at offset 0 with no
			  corruption.
 Same offset, so must be the same 
 1st LEB was written, 2nd was not 
 1st LEB was unmapped and written, 2nd not 
			
			  2nd LEB was unmapped and about to be written, so
			  there must be only one master node in the first LEB
			  and no corruption.
		
		  1st LEB was unmapped and about to be written, so there must
		  be no room left in 2nd LEB.
 Read-only mode. Keep a copy for switching to rw mode 
		
		  We had to recover the master node, which means there was an
		  unclean reboot. However, it is possible that the master node
		  is clean at this point, i.e., %UBIFS_MST_DIRTY is not set.
		  E.g., consider the following chain of events:
		 
		  1. UBIFS was cleanly unmounted, so the master node is clean
		  2. UBIFS is being mounted RW and starts changing the master
		     node in the first (%UBIFS_MST_LNUM). A power cut happens,
		     so this LEB ends up with some amount of garbage at the
		     end.
		  3. UBIFS is being mounted RO. We reach this place and
		     recover the master node from the second LEB
		     (%UBIFS_MST_LNUM + 1). But we cannot update the media
		     because we are being mounted RO. We have to defer the
		     operation.
		  4. However, this master node (@c->mst_node) is marked as
		     clean (since the step 1). And if we just return, the
		     mount code will be confused and won't recover the master
		     node when it is re-mounter RW later.
		 
		     Thus, to force the recovery by marking the master node as
		     dirty.
 Write the recovered master node 
  ubifs_write_rcvrd_mst_node - write the recovered master node.
  @c: UBIFS file-system description object
  This function writes the master node that was recovered during mounting in
  read-only mode and must now be written because we are remounting rw.
  This function returns %0 on success and a negative error code on failure.
  is_last_write - determine if an offset was in the last write to a LEB.
  @c: UBIFS file-system description object
  @buf: buffer to check
  @offs: offset to check
  This function returns %1 if @offs was in the last write to the LEB whose data
  is in @buf, otherwise %0 is returned. The determination is made by checking
  for subsequent empty space starting from the next @c->max_write_size
  boundary.
	
	  Round up to the next @c->max_write_size boundary i.e. @offs is in
	  the last wbuf written. After that should be empty space.
  clean_buf - clean the data from an LEB sitting in a buffer.
  @c: UBIFS file-system description object
  @buf: buffer to clean
  @lnum: LEB number to clean
  @offs: offset from which to clean
  @len: length of buffer
  This function pads up to the next min_io_size boundary (if there is one) and
  sets empty space to all 0xff. @buf, @offs and @len are updated to the next
  @c->min_io_size boundary.
  no_more_nodes - determine if there are no more nodes in a buffer.
  @c: UBIFS file-system description object
  @buf: buffer to check
  @len: length of buffer
  @lnum: LEB number of the LEB from which @buf was read
  @offs: offset from which @buf was read
  This function ensures that the corrupted node at @offs is the last thing
  written to a LEB. This function returns %1 if more data is not found and
  %0 if more data is found.
 Check for empty space after the corrupt node's common header 
	
	  The area after the common header size is not empty, so the common
	  header must be intact. Check it.
 Now we know the corrupt node's length we can skip over it 
 After which there should be empty space 
  fix_unclean_leb - fix an unclean LEB.
  @c: UBIFS file-system description object
  @sleb: scanned LEB information
  @start: offset where scan started
 Get the end offset of the last node we are keeping 
 Add to recovery list 
 Write the fixed LEB back to flash 
 Pad to min_io_size 
  drop_last_group - drop the last group of nodes.
  @sleb: scanned LEB information
  @offs: offset of dropped nodes is returned here
  This is a helper function for 'ubifs_recover_leb()' which drops the last
  group of nodes of the scanned LEB.
  drop_last_node - drop the last node.
  @sleb: scanned LEB information
  @offs: offset of dropped nodes is returned here
  This is a helper function for 'ubifs_recover_leb()' which drops the last
  node of the scanned LEB.
  ubifs_recover_leb - scan and recover a LEB.
  @c: UBIFS file-system description object
  @lnum: LEB number
  @offs: offset
  @sbuf: LEB-sized buffer to use
  @jhead: journal head number this LEB belongs to (%-1 if the LEB does not
          belong to any journal head)
  This function does a scan of a LEB, but caters for errors that might have
  been caused by the unclean unmount from which we are attempting to recover.
  Returns the scanned information on success and a negative error code on
  failure.
		
		  Scan quietly until there is an error from which we cannot
		  recover
 A valid node, and not a padding node 
 Padding bytes or a valid padding node 
			
			  See header comment for this file for more
			  explanations about the reasons we have this check.
 Make sure we dump interesting non-0xFF data 
		
		  If nodes are grouped, always drop the incomplete group at
		  the end.
		
		  If this LEB belongs to the GC head then while we are in the
		  middle of the same min. IO unit keep dropping nodes. So
		  basically, what we want is to make sure that the last min.
		  IO unit where we saw the corruption is dropped completely
		  with all the uncorrupted nodes which may possibly sit there.
		 
		  In other words, let's name the min. IO unit where the
		  corruption starts B, and the previous min. IO unit A. The
		  below code tries to deal with a situation when half of B
		  contains valid nodes or the end of a valid node, and the
		  second half of B contains corrupted data or garbage. This
		  means that UBIFS had been writing to B just before the power
		  cut happened. I do not know how realistic is this scenario
		  that half of the min. IO unit had been written successfully
		  and the other half not, but this is possible in our 'failure
		  mode emulation' infrastructure at least.
		 
		  So what is the problem, why we need to drop those nodes? Why
		  can't we just clean-up the second half of B by putting a
		  padding node there? We can, and this works fine with one
		  exception which was reproduced with power cut emulation
		  testing and happens extremely rarely.
		 
		  Imagine the file-system is full, we run GC which starts
		  moving valid nodes from LEB X to LEB Y (obviously, LEB Y is
		  the current GC head LEB). The @c->gc_lnum is -1, which means
		  that GC will retain LEB X and will try to continue. Imagine
		  that LEB X is currently the dirtiest LEB, and the amount of
		  used space in LEB Y is exactly the same as amount of free
		  space in LEB X.
		 
		  And a power cut happens when nodes are moved from LEB X to
		  LEB Y. We are here trying to recover LEB Y which is the GC
		  head LEB. We find the min. IO unit B as described above.
		  Then we clean-up LEB Y by padding min. IO unit. And later
		  'ubifs_rcvry_gc_commit()' function fails, because it cannot
		  find a dirty LEB which could be GC'd into LEB Y! Even LEB X
		  does not match because the amount of valid nodes there does
		  not fit the free space in LEB Y any more! And this is
		  because of the padding node which we added to LEB Y. The
		  user-visible effect of this which I once observed and
		  analysed is that we cannot mount the file-system with
		  -ENOSPC error.
		 
		  So obviously, to make sure that situation does not happen we
		  should free min. IO unit B in LEB Y completely and the last
		  used min. IO unit in LEB Y should be A. This is basically
		  what the below code tries to do.
 Re-scan the corrupted data with verbose messages 
  get_cs_sqnum - get commit start sequence number.
  @c: UBIFS file-system description object
  @lnum: LEB number of commit start node
  @offs: offset of commit start node
  @cs_sqnum: commit start sequence number is returned here
  This function returns %0 on success and a negative error code on failure.
  ubifs_recover_log_leb - scan and recover a log LEB.
  @c: UBIFS file-system description object
  @lnum: LEB number
  @offs: offset
  @sbuf: LEB-sized buffer to use
  This function does a scan of a LEB, but caters for errors that might have
  been caused by unclean reboots from which we are attempting to recover
  (assume that only the last log LEB can be corrupted by an unclean reboot).
  This function returns %0 on success and a negative error code on failure.
		
		  We can only recover at the end of the log, so check that the
		  next log LEB is empty or out of date.
  recover_head - recover a head.
  @c: UBIFS file-system description object
  @lnum: LEB number of head to recover
  @offs: offset of head to recover
  @sbuf: LEB-sized buffer to use
  This function ensures that there is no data on the flash at a head location.
  This function returns %0 on success and a negative error code on failure.
 Read at the head location and check it is empty flash 
  ubifs_recover_inl_heads - recover index and LPT heads.
  @c: UBIFS file-system description object
  @sbuf: LEB-sized buffer to use
  This function ensures that there is no data on the flash at the index and
  LPT head locations.
  This deals with the recovery of a half-completed journal commit. UBIFS is
  careful never to overwrite the last version of the index or the LPT. Because
  the index and LPT are wandering trees, data from a half-completed commit will
  not be referenced anywhere in UBIFS. The data will be either in LEBs that are
  assumed to be empty and will be unmapped anyway before use, or in the index
  and LPT heads.
  This function returns %0 on success and a negative error code on failure.
  clean_an_unclean_leb - read and write a LEB to remove corruption.
  @c: UBIFS file-system description object
  @ucleb: unclean LEB information
  @sbuf: LEB-sized buffer to use
  This function reads a LEB up to a point pre-determined by the mount recovery,
  checks the nodes, and writes the result back to the flash, thereby cleaning
  off any following corruption, or non-fatal ECC errors.
  This function returns %0 on success and a negative error code on failure.
 Nothing to read, just unmap it 
 Scan quietly until there is an error 
 A valid node, and not a padding node 
 Padding bytes or a valid padding node 
 Redo the last scan but noisily 
 Pad to min_io_size 
 Write back the LEB atomically 
  ubifs_clean_lebs - clean LEBs recovered during read-only mount.
  @c: UBIFS file-system description object
  @sbuf: LEB-sized buffer to use
  This function cleans a LEB identified during recovery that needs to be
  written but was not because UBIFS was mounted read-only. This happens when
  remounting to read-write mode.
  This function returns %0 on success and a negative error code on failure.
  grab_empty_leb - grab an empty LEB to use as GC LEB and run commit.
  @c: UBIFS file-system description object
  This is a helper function for 'ubifs_rcvry_gc_commit()' which grabs an empty
  LEB to be used as GC LEB (@c->gc_lnum), and then runs the commit. Returns
  zero in case of success and a negative error code in case of failure.
	
	  Note, it is very important to first search for an empty LEB and then
	  run the commit, not vice-versa. The reason is that there might be
	  only one empty LEB at the moment, the one which has been the
	  @c->gc_lnum just before the power cut happened. During the regular
	  UBIFS operation (not now) @c->gc_lnum is marked as "taken", so no
	  one but GC can grab it. But at this moment this single empty LEB is
	  not marked as taken, so if we run commit - what happens? Right, the
	  commit will grab it and write the index there. Remember that the
	  index always expands as long as there is free space, and it only
	  starts consolidating when we run out of space.
	 
	  IOW, if we run commit now, we might not be able to find a free LEB
	  after this.
 Reset the index flag 
  ubifs_rcvry_gc_commit - recover the GC LEB number and run the commit.
  @c: UBIFS file-system description object
  Out-of-place garbage collection requires always one empty LEB with which to
  start garbage collection. The LEB number is recorded in c->gc_lnum and is
  written to the master node on unmounting. In the case of an unclean unmount
  the value of gc_lnum recorded in the master node is out of date and cannot
  be used. Instead, recovery must allocate an empty LEB for this purpose.
  However, there may not be enough empty space, in which case it must be
  possible to GC the dirtiest LEB into the GC head LEB.
  This function also runs the commit which causes the TNC updates from
  size-recovery and orphans to be written to the flash. That is important to
  ensure correct replay order for subsequent mounts.
  This function returns %0 on success and a negative error code on failure.
	
	  We run the commit before garbage collection otherwise subsequent
	  mounts will see the GC and orphan deletion in a different order.
  struct size_entry - inode size information for recovery.
  @rb: link in the RB-tree of sizes
  @inum: inode number
  @i_size: size on inode
  @d_size: maximum size based on data nodes
  @exists: indicates whether the inode exists
  @inode: inode if pinned in memory awaiting rw mode to fix it
  add_ino - add an entry to the size tree.
  @c: UBIFS file-system description object
  @inum: inode number
  @i_size: size on inode
  @d_size: maximum size based on data nodes
  @exists: indicates whether the inode exists
  find_ino - find an entry on the size tree.
  @c: UBIFS file-system description object
  @inum: inode number
  remove_ino - remove an entry from the size tree.
  @c: UBIFS file-system description object
  @inum: inode number
  ubifs_destroy_size_tree - free resources related to the size tree.
  @c: UBIFS file-system description object
  ubifs_recover_size_accum - accumulate inode sizes for recovery.
  @c: UBIFS file-system description object
  @key: node key
  @deletion: node is for a deletion
  @new_size: inode size
  This function has two purposes:
      1) to ensure there are no data nodes that fall outside the inode size
      2) to ensure there are no data nodes for inodes that do not exist
  To accomplish those purposes, a rb-tree is constructed containing an entry
  for each inode number in the journal that has not been deleted, and recording
  the size from the inode node, the maximum size of any data node (also altered
  by truncations) and a flag indicating a inode number for which no inode node
  was present in the journal.
  Note that there is still the possibility that there are data nodes that have
  been committed that are beyond the inode size, however the only way to find
  them would be to scan the entire index. Alternatively, some provision could
  be made to record the size of inodes at the start of commit, which would seem
  very cumbersome for a scenario that is quite unlikely and the only negative
  consequence of which is wasted space.
  This functions returns %0 on success and a negative error code on failure.
  fix_size_in_place - fix inode size in place on flash.
  @c: UBIFS file-system description object
  @e: inode size information for recovery
 Locate the inode node LEB number and offset 
	
	  If the size recorded on the inode node is greater than the size that
	  was calculated from nodes in the journal then don't change the inode.
 Read the LEB 
 Change the size field and recalculate the CRC 
 Work out where data in the LEB ends and free space begins 
 Atomically write the fixed LEB back again 
  inode_fix_size - fix inode size
  @c: UBIFS file-system description object
  @e: inode size information for recovery
 Remounting rw, pick up inode we stored earlier 
			
			  The original inode in the index already has a size
			  big enough, nothing to do
	
	  In readonly mode just keep the inode pinned in memory until we go
	  readwrite. In readwrite mode write the inode to the journal with the
	  fixed size.
  ubifs_recover_size - recover inode size.
  @c: UBIFS file-system description object
  @in_place: If true, do a in-place size fixup
  This function attempts to fix inode size discrepancies identified by the
  'ubifs_recover_size_accum()' function.
  This functions returns %0 on success and a negative error code on failure.
 Remove data nodes that have no inode 
			
			  We found data that is outside the found inode size,
			  fixup the inode size
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
  This file implements commit-related functionality of the LEB properties
  subsystem.
  first_dirty_cnode - find first dirty cnode.
  @c: UBIFS file-system description object
  @nnode: nnode at which to start
  This function returns the first dirty cnode or %NULL if there is not one.
  next_dirty_cnode - find next dirty cnode.
  @c: UBIFS file-system description object
  @cnode: cnode from which to begin searching
  This function returns the next dirty cnode or %NULL if there is not one.
 cnode is a pnode 
 cnode is a nnode 
  get_cnodes_to_commit - create list of dirty cnodes to commit.
  @c: UBIFS file-system description object
  This function returns the number of cnodes to commit.
  upd_ltab - update LPT LEB properties.
  @c: UBIFS file-system description object
  @lnum: LEB number
  @free: amount of free space
  @dirty: amount of dirty space to add
  alloc_lpt_leb - allocate an LPT LEB that is empty.
  @c: UBIFS file-system description object
  @lnum: LEB number is passed and returned here
  This function finds the next empty LEB in the ltab starting from @lnum. If a
  an empty LEB is found it is returned in @lnum and the function returns %0.
  Otherwise the function returns -ENOSPC.  Note however, that LPT is designed
  never to run out of space.
  layout_cnodes - layout cnodes for commit.
  @c: UBIFS file-system description object
  This function returns %0 on success and a negative error code on failure.
 Try to place lsave and ltab nicely 
 Try to place lsave and ltab nicely 
 Make sure to place LPT's save table 
 Make sure to place LPT's own lprops table 
  realloc_lpt_leb - allocate an LPT LEB that is empty.
  @c: UBIFS file-system description object
  @lnum: LEB number is passed and returned here
  This function duplicates exactly the results of the function alloc_lpt_leb.
  It is used during end commit to reallocate the same LEB numbers that were
  allocated by alloc_lpt_leb during start commit.
  This function finds the next LEB that was allocated by the alloc_lpt_leb
  function starting from @lnum. If a LEB is found it is returned in @lnum and
  the function returns %0. Otherwise the function returns -ENOSPC.
  Note however, that LPT is designed never to run out of space.
  write_cnodes - write cnodes for commit.
  @c: UBIFS file-system description object
  This function returns %0 on success and a negative error code on failure.
 Ensure empty LEB is unmapped 
 Try to place lsave and ltab nicely 
 Loop for each cnode 
 Try to place lsave and ltab nicely 
		
		  The reason for the barriers is the same as in case of TNC.
		  See comment in 'write_index()'. 'dirty_cow_nnode()' and
		  'dirty_cow_pnode()' are the functions for which this is
		  important.
 Make sure to place LPT's save table 
 Make sure to place LPT's own lprops table 
 Write remaining data in buffer 
  next_pnode_to_dirty - find next pnode to dirty.
  @c: UBIFS file-system description object
  @pnode: pnode
  This function returns the next pnode to dirty or %NULL if there are no more
  pnodes.  Note that pnodes that have never been written (lnum == 0) are
  skipped.
 Try to go right 
 Go up while can't go right 
 Go right 
 Go down to level 1 
			
			  Should not happen, but we need to keep going
			  if it does.
 Should not happen, but we need to keep going if it does 
  add_pnode_dirt - add dirty space to LPT LEB properties.
  @c: UBIFS file-system description object
  @pnode: pnode for which to add dirt
  do_make_pnode_dirty - mark a pnode dirty.
  @c: UBIFS file-system description object
  @pnode: pnode to mark dirty
 Assumes cnext list is empty i.e. not called during commit 
 Mark parent and ancestors dirty too 
  make_tree_dirty - mark the entire LEB properties tree dirty.
  @c: UBIFS file-system description object
  This function is used by the "small" LPT model to cause the entire LEB
  properties tree to be written.  The "small" LPT model does not use LPT
  garbage collection because it is more efficient to write the entire tree
  (because it is small).
  This function returns %0 on success and a negative error code on failure.
  need_write_all - determine if the LPT area is running out of free space.
  @c: UBIFS file-system description object
  This function returns %1 if the LPT area is running out of free space and %0
  if it is not.
 Less than twice the size left 
  lpt_tgc_start - start trivial garbage collection of LPT LEBs.
  @c: UBIFS file-system description object
  LPT trivial garbage collection is where a LPT LEB contains only dirty and
  free space and so may be reused as soon as the next commit is completed.
  This function is called during start commit to mark LPT LEBs for trivial GC.
  lpt_tgc_end - end trivial garbage collection of LPT LEBs.
  @c: UBIFS file-system description object
  LPT trivial garbage collection is where a LPT LEB contains only dirty and
  free space and so may be reused as soon as the next commit is completed.
  This function is called after the commit is completed (master node has been
  written) and un-maps LPT LEBs that were marked for trivial GC.
  populate_lsave - fill the lsave array with important LEB numbers.
  @c: the UBIFS file-system description object
  This function is only called for the "big" model. It records a small number
  of LEB numbers of important LEBs.  Important LEBs are ones that are (from
  most important to least important): empty, freeable, freeable index, dirty
  index, dirty or free. Upon mount, we read this list of LEB numbers and bring
  their pnodes into memory.  That will stop us from having to scan the LPT
  straight away. For the "small" model we assume that scanning the LPT is no
  big deal.
 Fill it up completely 
  nnode_lookup - lookup a nnode in the LPT.
  @c: UBIFS file-system description object
  @i: nnode number
  This function returns a pointer to the nnode on success or a negative
  error code on failure.
  make_nnode_dirty - find a nnode and, if found, make it dirty.
  @c: UBIFS file-system description object
  @node_num: nnode number of nnode to make dirty
  @lnum: LEB number where nnode was written
  @offs: offset where nnode was written
  This function is used by LPT garbage collection.  LPT garbage collection is
  used only for the "big" LPT model (c->big_lpt == 1).  Garbage collection
  simply involves marking all the nodes in the LEB being garbage-collected as
  dirty.  The dirty nodes are written next commit, after which the LEB is free
  to be reused.
  This function returns %0 on success and a negative error code on failure.
 nnode is obsolete 
 nnode is obsolete 
 Assumes cnext list is empty i.e. not called during commit 
 Mark parent and ancestors dirty too 
  make_pnode_dirty - find a pnode and, if found, make it dirty.
  @c: UBIFS file-system description object
  @node_num: pnode number of pnode to make dirty
  @lnum: LEB number where pnode was written
  @offs: offset where pnode was written
  This function is used by LPT garbage collection.  LPT garbage collection is
  used only for the "big" LPT model (c->big_lpt == 1).  Garbage collection
  simply involves marking all the nodes in the LEB being garbage-collected as
  dirty.  The dirty nodes are written next commit, after which the LEB is free
  to be reused.
  This function returns %0 on success and a negative error code on failure.
  make_ltab_dirty - make ltab node dirty.
  @c: UBIFS file-system description object
  @lnum: LEB number where ltab was written
  @offs: offset where ltab was written
  This function is used by LPT garbage collection.  LPT garbage collection is
  used only for the "big" LPT model (c->big_lpt == 1).  Garbage collection
  simply involves marking all the nodes in the LEB being garbage-collected as
  dirty.  The dirty nodes are written next commit, after which the LEB is free
  to be reused.
  This function returns %0 on success and a negative error code on failure.
 This ltab node is obsolete 
  make_lsave_dirty - make lsave node dirty.
  @c: UBIFS file-system description object
  @lnum: LEB number where lsave was written
  @offs: offset where lsave was written
  This function is used by LPT garbage collection.  LPT garbage collection is
  used only for the "big" LPT model (c->big_lpt == 1).  Garbage collection
  simply involves marking all the nodes in the LEB being garbage-collected as
  dirty.  The dirty nodes are written next commit, after which the LEB is free
  to be reused.
  This function returns %0 on success and a negative error code on failure.
 This lsave node is obsolete 
  make_node_dirty - make node dirty.
  @c: UBIFS file-system description object
  @node_type: LPT node type
  @node_num: node number
  @lnum: LEB number where node was written
  @offs: offset where node was written
  This function is used by LPT garbage collection.  LPT garbage collection is
  used only for the "big" LPT model (c->big_lpt == 1).  Garbage collection
  simply involves marking all the nodes in the LEB being garbage-collected as
  dirty.  The dirty nodes are written next commit, after which the LEB is free
  to be reused.
  This function returns %0 on success and a negative error code on failure.
  get_lpt_node_len - return the length of a node based on its type.
  @c: UBIFS file-system description object
  @node_type: LPT node type
  get_pad_len - return the length of padding in a buffer.
  @c: UBIFS file-system description object
  @buf: buffer
  @len: length of buffer
  get_lpt_node_type - return type (and node number) of a node in a buffer.
  @c: UBIFS file-system description object
  @buf: buffer
  @node_num: node number is returned here
  is_a_node - determine if a buffer contains a node.
  @c: UBIFS file-system description object
  @buf: buffer
  @len: length of buffer
  This function returns %1 if the buffer contains a node or %0 if it does not.
  lpt_gc_lnum - garbage collect a LPT LEB.
  @c: UBIFS file-system description object
  @lnum: LEB number to garbage collect
  LPT garbage collection is used only for the "big" LPT model
  (c->big_lpt == 1).  Garbage collection simply involves marking all the nodes
  in the LEB being garbage-collected as dirty.  The dirty nodes are written
  next commit, after which the LEB is free to be reused.
  This function returns %0 on success and a negative error code on failure.
  lpt_gc - LPT garbage collection.
  @c: UBIFS file-system description object
  Select a LPT LEB for LPT garbage collection and call 'lpt_gc_lnum()'.
  Returns %0 on success and a negative error code on failure.
  ubifs_lpt_start_commit - UBIFS commit starts.
  @c: the UBIFS file-system description object
  This function has to be called when UBIFS starts the commit operation.
  This function "freezes" all currently dirty LEB properties and does not
  change them anymore. Further changes are saved and tracked separately
  because they are not part of this commit. This function returns zero in case
  of success and a negative error code in case of failure.
		
		  We ensure there is enough free space in
		  ubifs_lpt_post_commit() by marking nodes dirty. That
		  information is lost when we unmount, so we also need
		  to check free space once after mounting also.
 If needed, write everything 
 Copy the LPT's own lprops for end commit to write 
  free_obsolete_cnodes - free obsolete cnodes for commit end.
  @c: UBIFS file-system description object
  ubifs_lpt_end_commit - finish the commit operation.
  @c: the UBIFS file-system description object
  This function has to be called when the commit operation finishes. It
  flushes the changes which were "frozen" by 'ubifs_lprops_start_commit()' to
  the media. Returns zero in case of success and a negative error code in case
  of failure.
  ubifs_lpt_post_commit - post commit LPT trivial GC and LPT GC.
  @c: UBIFS file-system description object
  LPT trivial GC is completed after a commit. Also LPT GC is done after a
  commit for the "big" LPT model.
  first_nnode - find the first nnode in memory.
  @c: UBIFS file-system description object
  @hght: height of tree where nnode found is returned here
  This function returns a pointer to the nnode found or %NULL if no nnode is
  found. This function is a helper to 'ubifs_lpt_free()'.
  next_nnode - find the next nnode in memory.
  @c: UBIFS file-system description object
  @nnode: nnode from which to start.
  @hght: height of tree where nnode is, is passed and returned here
  This function returns a pointer to the nnode found or %NULL if no nnode is
  found. This function is a helper to 'ubifs_lpt_free()'.
  ubifs_lpt_free - free resources owned by the LPT.
  @c: UBIFS file-system description object
  @wr_only: free only resources used for writing
 Free write-only things first 
 Leftover from a failed commit 
 Now free the rest 
  Everything below is related to debugging.
  dbg_is_all_ff - determine if a buffer contains only 0xFF bytes.
  @buf: buffer
  @len: buffer length
  dbg_is_nnode_dirty - determine if a nnode is dirty.
  @c: the UBIFS file-system description object
  @lnum: LEB number where nnode was written
  @offs: offset where nnode was written
 Entire tree is in memory so first_nnode  next_nnode are OK 
  dbg_is_pnode_dirty - determine if a pnode is dirty.
  @c: the UBIFS file-system description object
  @lnum: LEB number where pnode was written
  @offs: offset where pnode was written
  dbg_is_ltab_dirty - determine if a ltab node is dirty.
  @c: the UBIFS file-system description object
  @lnum: LEB number where ltab node was written
  @offs: offset where ltab node was written
  dbg_is_lsave_dirty - determine if a lsave node is dirty.
  @c: the UBIFS file-system description object
  @lnum: LEB number where lsave node was written
  @offs: offset where lsave node was written
  dbg_is_node_dirty - determine if a node is dirty.
  @c: the UBIFS file-system description object
  @node_type: node type
  @lnum: LEB number where node was written
  @offs: offset where node was written
  dbg_check_ltab_lnum - check the ltab for a LPT LEB number.
  @c: the UBIFS file-system description object
  @lnum: LEB number where node was written
  This function returns %0 on success and a negative error code on failure.
  dbg_check_ltab - check the free and dirty space in the ltab.
  @c: the UBIFS file-system description object
  This function returns %0 on success and a negative error code on failure.
 Bring the entire tree into memory 
 Check nodes 
 Check each LEB 
  dbg_chk_lpt_free_spc - check LPT free space is enough to write entire LPT.
  @c: the UBIFS file-system description object
  This function returns %0 on success and a negative error code on failure.
  dbg_chk_lpt_sz - check LPT does not write more than LPT size.
  @c: the UBIFS file-system description object
  @action: what to do
  @len: length written
  This function returns %0 on success and a negative error code on failure.
  The @action argument may be one of:
    o %0 - LPT debugging checking starts, initialize debugging variables;
    o %1 - wrote an LPT node, increase LPT size by @len bytes;
    o %2 - switched to a different LEB and wasted @len bytes;
    o %3 - check that we've written the right number of bytes.
    o %4 - wasted @len bytes;
  dump_lpt_leb - dump an LPT LEB.
  @c: UBIFS file-system description object
  @lnum: LEB number to dump
  This function dumps an LEB from LPT area. Nodes in this area are very
  different to nodes in the main area (e.g., they do not have common headers,
  they do not have 8-byte alignments, etc), so we have a separate function to
  dump LPT area LEBs. Note, LPT has to be locked by the caller.
  ubifs_dump_lpt_lebs - dump LPT lebs.
  @c: UBIFS file-system description object
  This function dumps all LPT LEBs. The caller has to make sure the LPT is
  locked.
  dbg_populate_lsave - debugging version of 'populate_lsave()'
  @c: UBIFS file-system description object
  This is a debugging version for 'populate_lsave()' which populates lsave
  with random LEBs instead of useful LEBs, which is good for test coverage.
  Returns zero if lsave has not been populated (this debugging feature is
  disabled) an non-zero if lsave has been populated.
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation
  Authors: Artem Bityutskiy ( )
           Adrian Hunter
  This file implements most of the debugging stuff which is compiled in only
  when it is enabled. But some debugging check functions are implemented in
  corresponding subsystem, just because they are closely related and utilize
  various local functions of those subsystems.
 If the magic is incorrect, just hexdump the first bytes 
 Skip dumping unknown type node 
		
		  If we are dumping saved budgeting data, do not print
		  additional information which is about the current state, not
		  the old one which corresponded to the saved budgeting data.
 If we are in RO mode, journal heads do not exist 
 Print budgeting predictions 
				
				  Note, if we are in RO mode or in the middle
				  of mountingre-mounting, the write-buffers do
				  not exist.
  ubifs_dump_index - dump the on-flash index.
  @c: UBIFS file-system description object
  This function dumps whole UBIFS indexing B-tree, unlike 'ubifs_dump_tnc()'
  which dumps only in-memory znodes and does not read znodes which from flash.
  dbg_save_space_info - save information about flash space.
  @c: UBIFS file-system description object
  This function saves information about UBIFS free space, dirty space, etc, in
  order to check it later.
	
	  We use a dirty hack here and zero out @c->freeable_cnt, because it
	  affects the free space calculations, and UBIFS might not know about
	  all freeable eraseblocks. Indeed, we know about freeable eraseblocks
	  only when we read their lprops, and we do this only lazily, upon the
	  need. So at any given point of time @c->freeable_cnt might be not
	  exactly accurate.
	 
	  Just one example about the issue we hit when we did not zero
	  @c->freeable_cnt.
	  1. The file-system is mounted RO, c->freeable_cnt is %0. We save the
	     amount of free space in @d->saved_free
	  2. We re-mount RW, which makes UBIFS to read the "lsave"
	     information from flash, where we cache LEBs from various
	     categories ('ubifs_remount_fs()' -> 'ubifs_lpt_init()'
	     -> 'lpt_init_wr()' -> 'read_lsave()' -> 'ubifs_lpt_lookup()'
	     -> 'ubifs_get_pnode()' -> 'update_cats()'
	     -> 'ubifs_add_to_cat()').
	  3. Lsave contains a freeable eraseblock, and @c->freeable_cnt
	     becomes %1.
	  4. We calculate the amount of free space when the re-mount is
	     finished in 'dbg_check_space_info()' and it does not match
	     @d->saved_free.
  dbg_check_space_info - check flash space information.
  @c: UBIFS file-system description object
  This function compares current flash space information with the information
  which was saved when the 'dbg_save_space_info()' function was called.
  Returns zero if the information has not changed, and %-EINVAL if it has
  changed.
  dbg_check_synced_i_size - check synchronized inode size.
  @c: UBIFS file-system description object
  @inode: inode to check
  If inode is clean, synchronized inode size has to be equivalent to current
  inode size. This function has to be called only for locked inodes (@i_mutex
  has to be locked). Returns %0 if synchronized inode size if correct, and
  %-EINVAL if not.
  dbg_check_dir - check directory inode size and link count.
  @c: UBIFS file-system description object
  @dir: the directory to calculate size for
  @size: the result is returned here
  This function makes sure that directory size and link count are correct.
  Returns zero in case of success and a negative error code in case of
  failure.
  Note, it is good idea to make sure the @dir->i_mutex is locked before
  calling this function.
  dbg_check_key_order - make sure that colliding keys are properly ordered.
  @c: UBIFS file-system description object
  @zbr1: first zbranch
  @zbr2: following zbranch
  In UBIFS indexing B-tree colliding keys has to be sorted in binary order of
  names of the direntriesxentries which are referred by the keys. This
  function reads direntriesxentries referred by @zbr1 and @zbr2 and makes
  sure the name of direntryxentry referred by @zbr1 is less than
  direntryxentry referred by @zbr2. Returns zero if this is true, %1 if not,
  and a negative error code in case of failure.
 Make sure node keys are the same as in zbranch 
  dbg_check_znode - check if znode is all right.
  @c: UBIFS file-system description object
  @zbr: zbranch which points to this znode
  This function makes sure that znode referred to by @zbr is all right.
  Returns zero if it is, and %-EINVAL if it is not.
 Only dirty zbranch may have no on-flash nodes 
		
		  If znode is dirty, its parent has to be dirty as well. The
		  order of the operation is important, so we have to have
		  memory barriers.
			
			  The dirty flag is atomic and is cleared outside the
			  TNC mutex, so znode's dirty flag may now have
			  been cleared. The child is always cleared before the
			  parent, so we just need to check again.
 Make sure the 'parent' pointer in our znode is correct 
 This zbranch does not exist in the parent 
 This may happen only in case of collisions 
		
		  Make sure that the first key in our znode is greater than or
		  equal to the key in the pointing zbranch.
			
			  Make sure the last key in our znode is less or
			  equivalent than the key in the zbranch which goes
			  after our pointing zbranch.
 This may only be root znode 
	
	  Make sure that next key is greater or equivalent then the previous
	  one.
 This can only be keys with colliding hash 
			
			  Colliding keys should follow binary order of
			  corresponding xentrydentry names.
  dbg_check_tnc - check TNC tree.
  @c: UBIFS file-system description object
  @extra: do extra checks that are possible at start commit
  This function traverses whole TNC tree and checks every znode. Returns zero
  if everything is all right and %-EINVAL if something is wrong with TNC.
		
		  If the last key of this znode is equivalent to the first key
		  of the next znode (collision), then check order of the keys.
  dbg_walk_index - walk the on-flash index.
  @c: UBIFS file-system description object
  @leaf_cb: called for each leaf node
  @znode_cb: called for each indexing node
  @priv: private data which is passed to callbacks
  This function walks the UBIFS index and calls the @leaf_cb for each leaf
  node and @znode_cb for each indexing node. Returns zero in case of success
  and a negative error code in case of failure.
  It would be better if this function removed every znode it pulled to into
  the TNC, so that the behavior more closely matched the non-debugging
  behavior.
 If the root indexing node is not in TNC - pull it 
	
	  We are going to traverse the indexing tree in the postorder manner.
	  Go down and find the leftmost indexing node where we are going to
	  start from.
 Iterate over all indexing nodes 
 Switch to the next index in the parent 
			
			  This is the last child, switch to the parent and
			  continue.
 Go to the lowest leftmost znode in the new sub-tree 
  add_size - add znode size to partially calculated index size.
  @c: UBIFS file-system description object
  @znode: znode to add size for
  @priv: partially calculated index size
  This is a helper function for 'dbg_check_idx_size()' which is called for
  every indexing node and adds its size to the 'long long' variable pointed to
  by @priv.
  dbg_check_idx_size - check index size.
  @c: UBIFS file-system description object
  @idx_size: size to check
  This function walks the UBIFS index, calculates its size and checks that the
  size is equivalent to @idx_size. Returns zero in case of success and a
  negative error code in case of failure.
  struct fsck_inode - information about an inode used when checking the file-system.
  @rb: link in the RB-tree of inodes
  @inum: inode number
  @mode: inode type, permissions, etc
  @nlink: inode link count
  @xattr_cnt: count of extended attributes
  @references: how many directoryxattr entries refer this inode (calculated
               while walking the index)
  @calc_cnt: for directory inode count of child directories
  @size: inode size (read from on-flash inode)
  @xattr_sz: summary size of all extended attributes (read from on-flash
             inode)
  @calc_sz: for directories calculated directory size
  @calc_xcnt: count of extended attributes
  @calc_xsz: calculated summary size of all extended attributes
  @xattr_nms: sum of lengths of all extended attribute names belonging to this
              inode (read from on-flash inode)
  @calc_xnms: calculated sum of lengths of all extended attribute names
  struct fsck_data - private FS checking information.
  @inodes: RB-tree of all inodes (contains @struct fsck_inode objects)
  add_inode - add inode information to RB-tree of inodes.
  @c: UBIFS file-system description object
  @fsckd: FS checking information
  @ino: raw UBIFS inode to add
  This is a helper function for 'check_leaf()' which adds information about
  inode @ino to the RB-tree of inodes. Returns inode information pointer in
  case of success and a negative error code in case of failure.
	
	  If the inode is present in the VFS inode cache, use it instead of
	  the on-flash inode which might be out-of-date. E.g., the size might
	  be out-of-date. If we do not do this, the following may happen, for
	  example:
	    1. A power cut happens
	    2. We mount the file-system RO, the replay process fixes up the
	       inode size in the VFS cache, but on on-flash.
	    3. 'check_leaf()' fails because it hits a data node beyond inode
	       size.
  search_inode - search inode in the RB-tree of inodes.
  @fsckd: FS checking information
  @inum: inode number to search
  This is a helper function for 'check_leaf()' which searches inode @inum in
  the RB-tree of inodes and returns an inode information pointer or %NULL if
  the inode was not found.
  read_add_inode - read inode node and add it to RB-tree of inodes.
  @c: UBIFS file-system description object
  @fsckd: FS checking information
  @inum: inode number to read
  This is a helper function for 'check_leaf()' which finds inode node @inum in
  the index, reads it, and adds it to the RB-tree of inodes. Returns inode
  information pointer in case of success and a negative error code in case of
  failure.
  check_leaf - check leaf node.
  @c: UBIFS file-system description object
  @zbr: zbranch of the leaf node to check
  @priv: FS checking information
  This is a helper function for 'dbg_check_filesystem()' which is called for
  every single leaf node while walking the indexing tree. It checks that the
  leaf node referred from the indexing tree exists, has correct CRC, and does
  some other basic validation. This function is also responsible for building
  an RB-tree of inodes - it adds all inodes into the RB-tree. It also
  calculates reference count, size, etc for each inode in order to later
  compare them to the information stored inside the inodes and detect possible
  inconsistencies. Returns zero in case of success and a negative error code
  in case of failure.
 If this is an inode node, add it to RB-tree of inodes 
		
		  Search the inode node this data node belongs to and insert
		  it to the RB-tree of inodes.
 Make sure the data node is within inode size 
		
		  Search the inode node this entry refers to and the parent
		  inode node and insert them to the RB-tree of inodes.
 Count how many direntries or xentries refers this inode 
  free_inodes - free RB-tree of inodes.
  @fsckd: FS checking information
  check_inodes - checks all inodes.
  @c: UBIFS file-system description object
  @fsckd: FS checking information
  This is a helper function for 'dbg_check_filesystem()' which walks the
  RB-tree of inodes after the index scan has been finished, and checks that
  inode nlink, size, etc are correct. Returns zero if inodes are fine,
  %-EINVAL if not, and a negative error code in case of failure.
			
			  Directories have to have exactly one reference (they
			  cannot have hardlinks), although root inode is an
			  exception.
 Read the bad inode and dump it 
  dbg_check_filesystem - check the file-system.
  @c: UBIFS file-system description object
  This function checks the file system, namely:
  o makes sure that all leaf nodes exist and their CRCs are correct;
  o makes sure inode nlink, size, xattr sizecount are correct (for all
    inodes).
  The function reads whole indexing tree and all nodes, so it is pretty
  heavy-weight. Returns zero if the file-system is consistent, %-EINVAL if
  not, and a negative error code in case of failure.
  dbg_check_data_nodes_order - check that list of data nodes is sorted.
  @c: UBIFS file-system description object
  @head: the list of nodes ('struct ubifs_scan_node' objects)
  This function returns zero if the list of data nodes is sorted correctly,
  and %-EINVAL if not.
  dbg_check_nondata_nodes_order - check that list of data nodes is sorted.
  @c: UBIFS file-system description object
  @head: the list of nodes ('struct ubifs_scan_node' objects)
  This function returns zero if the list of non-data nodes is sorted correctly,
  and %-EINVAL if not.
 Inode nodes are sorted in descending size order 
		
		  This is either a dentry or xentry, which should be sorted in
		  ascending (parent ino, hash) order.
 First call - decide delay to the power cut 
 Fail within 1 minute 
 Fail within 10000 operations 
 Determine if failure delay has expired 
 Corruption span max to end of write unit 
  Root directory for UBIFS stuff in debugfs. Contains sub-directories which
  contain the stuff specific to particular file-system mounts.
  provide_user_output - provide output to the user reading a debugfs file.
  @val: boolean value for the answer
  @u: the buffer to store the answer at
  @count: size of the buffer
  @ppos: position in the @u output buffer
  This is a simple helper function which stores @val boolean value in the user
  buffer when the user reads one of UBIFS debugfs files. Returns amount of
  bytes written to @u in case of success and a negative error code in case of
  failure.
  interpret_user_input - interpret user debugfs file input.
  @u: user-provided buffer with the input
  @count: buffer size
  This is a helper function which interpret user input to a boolean UBIFS
  debugfs file. Returns %0 or %1 in case of success and a negative error code
  in case of failure.
  dbg_debugfs_init_fs - initialize debugfs for UBIFS instance.
  @c: UBIFS file-system description object
  This function creates all debugfs files for this instance of UBIFS.
  Note, the only reason we have not merged this function with the
  'ubifs_debugging_init()' function is because it is better to initialize
  debugfs interfaces at the very end of the mount process, and remove them at
  the very beginning of the mount process.
 The array size is too small 
  dbg_debugfs_exit_fs - remove all debugfs files.
  @c: UBIFS file-system description object
  dbg_debugfs_init - initialize debugfs file-system.
  UBIFS uses debugfs file-system to expose various debugging knobs to
  user-space. This function creates "ubifs" directory in the debugfs
  file-system.
  dbg_debugfs_exit - remove the "ubifs" directory from debugfs file-system.
  ubifs_debugging_init - initialize UBIFS debugging.
  @c: UBIFS file-system description object
  This function initializes debugging-related data for the file system.
  Returns zero in case of success and a negative error code in case of
  failure.
  ubifs_debugging_exit - free debugging data.
  @c: UBIFS file-system description object
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Artem Bityutskiy ( )
           Adrian Hunter
 This file implements reading and writing the master node 
  ubifs_compare_master_node - compare two UBIFS master nodes
  @c: UBIFS file-system description object
  @m1: the first node
  @m2: the second node
  This function compares two UBIFS master nodes. Returns 0 if they are equal
  and nonzero if not.
	
	  Do not compare the common node header since the sequence number and
	  hence the CRC are different.
	
	  Do not compare the embedded HMAC as well which also must be different
	  due to the different common node header.
 mst_node_check_hash - Check hash of a master node
  @c: UBIFS file-system description object
  @mst: The master node
  @expected: The expected hash of the master node
  This checks the hash of a master node against a given expected hash.
  Note that we have two master nodes on a UBIFS image which have different
  sequence numbers and consequently different CRCs. To be able to match
  both master nodes we exclude the common node header containing the sequence
  number and CRC from the hash.
  Returns 0 if the hashes are equal, a negative error code otherwise.
  scan_for_master - search the valid master node.
  @c: UBIFS file-system description object
  This function scans the master node LEBs and search for the latest master
  node. Returns zero in case of success, %-EUCLEAN if there master area is
  corrupted and requires recovery, and a negative error code in case of
  failure.
  validate_master - validate master node.
  @c: UBIFS file-system description object
  This function validates data which was read from master node. Returns zero
  if the data is all right and %-EINVAL if not.
  ubifs_read_master - read master node.
  @c: UBIFS file-system description object
  This function finds and reads the master node during file-system mount. If
  the flash is empty, it creates default master node as well. Returns zero in
  case of success and a negative error code in case of failure.
			
			  Note, we do not free 'c->mst_node' here because the
			  unmount routine will take care of this.
 Make sure that the recovery flag is clear 
 The file system has been resized 
		
		  Reflect changes back onto the master node. N.B. the master
		  node gets written immediately whenever mounting (or
		  remounting) in read-write mode, so we do not need to write it
		  here.
  ubifs_write_master - write master node.
  @c: UBIFS file-system description object
  This function writes the master node. Returns zero in case of success and a
  negative error code in case of failure. The master node is written twice to
  enable recovery.
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
  This file implements the functions that access LEB properties and their
  categories. LEBs are categorized based on the needs of UBIFS, and the
  categories are stored as either heaps or lists to provide a fast way of
  finding a LEB in a particular category. For example, UBIFS may need to find
  an empty LEB for the journal, or a very dirty LEB for garbage collection.
  get_heap_comp_val - get the LEB properties value for heap comparisons.
  @lprops: LEB properties
  @cat: LEB category
  move_up_lpt_heap - move a new heap entry up as far as possible.
  @c: UBIFS file-system description object
  @heap: LEB category heap
  @lprops: LEB properties to move
  @cat: LEB category
  New entries to a heap are added at the bottom and then moved up until the
  parent's value is greater.  In the case of LPT's category heaps, the value
  is either the amount of free space or the amount of dirty space, depending
  on the category.
 Already top of the heap 
 Compare to parent and, if greater, move up the heap 
 Greater than parent so move up 
  adjust_lpt_heap - move a changed heap entry up or down the heap.
  @c: UBIFS file-system description object
  @heap: LEB category heap
  @lprops: LEB properties to move
  @hpos: heap position of @lprops
  @cat: LEB category
  Changed entries in a heap are moved up or down until the parent's value is
  greater.  In the case of LPT's category heaps, the value is either the amount
  of free space or the amount of dirty space, depending on the category.
 Compare to parent and, if greater than parent, move up the heap 
 Greater than parent so move up 
 Still greater than parent so keep going 
 Not greater than parent, so compare to children 
 Compare to left child 
 Less than left child, so promote biggest child 
 Right child is bigger 
 Compare to right child 
 Less than right child, so promote right child 
  add_to_lpt_heap - add LEB properties to a LEB category heap.
  @c: UBIFS file-system description object
  @lprops: LEB properties to add
  @cat: LEB category
  This function returns %1 if @lprops is added to the heap for LEB category
  @cat, otherwise %0 is returned because the heap is full.
 Compare to some other LEB on the bottom of heap 
 Pick a position kind of randomly 
 Added to heap 
 Not added to heap 
 Added to heap 
  remove_from_lpt_heap - remove LEB properties from a LEB category heap.
  @c: UBIFS file-system description object
  @lprops: LEB properties to remove
  @cat: LEB category
  lpt_heap_replace - replace lprops in a category heap.
  @c: UBIFS file-system description object
  @new_lprops: LEB properties with which to replace
  @cat: LEB category
  During commit it is sometimes necessary to copy a pnode (see dirty_cow_pnode)
  and the lprops that the pnode contains.  When that happens, references in
  the category heaps to those lprops must be updated to point to the new
  lprops.  This function does that.
  ubifs_add_to_cat - add LEB properties to a category list or heap.
  @c: UBIFS file-system description object
  @lprops: LEB properties to add
  @cat: LEB category to which to add
  LEB properties are categorized to enable fast find operations.
 No more room on heap so make it un-categorized 
  ubifs_remove_from_cat - remove LEB properties from a category list or heap.
  @c: UBIFS file-system description object
  @lprops: LEB properties to remove
  @cat: LEB category from which to remove
  LEB properties are categorized to enable fast find operations.
  ubifs_replace_cat - replace lprops in a category list or heap.
  @c: UBIFS file-system description object
  @old_lprops: LEB properties to replace
  @new_lprops: LEB properties with which to replace
  During commit it is sometimes necessary to copy a pnode (see dirty_cow_pnode)
  and the lprops that the pnode contains. When that happens, references in
  category lists and heaps must be replaced. This function does that.
  ubifs_ensure_cat - ensure LEB properties are categorized.
  @c: UBIFS file-system description object
  @lprops: LEB properties
  A LEB may have fallen off of the bottom of a heap, and ended up as
  un-categorized even though it has enough space for us now. If that is the
  case this function will put the LEB back onto a heap.
  ubifs_categorize_lprops - categorize LEB properties.
  @c: UBIFS file-system description object
  @lprops: LEB properties to categorize
  LEB properties are categorized to enable fast find operations. This function
  returns the LEB category to which the LEB properties belong. Note however
  that if the LEB category is stored as a heap and the heap is full, the
  LEB properties may have their category changed to %LPROPS_UNCAT.
  change_category - change LEB properties category.
  @c: UBIFS file-system description object
  @lprops: LEB properties to re-categorize
  LEB properties are categorized to enable fast find operations. When the LEB
  properties change they must be re-categorized.
 lprops on a heap now must be moved up or down 
 Not on a heap 
  ubifs_calc_dark - calculate LEB dark space size.
  @c: the UBIFS file-system description object
  @spc: amount of free and dirty space in the LEB
  This function calculates and returns amount of dark space in an LEB which
  has @spc bytes of free and dirty space.
  UBIFS is trying to account the space which might not be usable, and this
  space is called "dark space". For example, if an LEB has only %512 free
  bytes, it is dark space, because it cannot fit a large data node.
	
	  If we have slightly more space then the dark space watermark, we can
	  anyway safely assume it we'll be able to write a node of the
	  smallest size there.
  is_lprops_dirty - determine if LEB properties are dirty.
  @c: the UBIFS file-system description object
  @lprops: LEB properties to test
  ubifs_change_lp - change LEB properties.
  @c: the UBIFS file-system description object
  @lp: LEB properties to change
  @free: new free space amount
  @dirty: new dirty space amount
  @flags: new flags
  @idx_gc_cnt: change to the count of @idx_gc list
  This function changes LEB properties (@free, @dirty or @flag). However, the
  property which has the %LPROPS_NC value is not changed. Returns a pointer to
  the updated LEB properties on success and a negative error code on failure.
  Note, the LEB properties may have had to be copied (due to COW) and
  consequently the pointer returned may not be the same as the pointer
  passed.
	
	  This is the only function that is allowed to change lprops, so we
	  discard the "const" qualifier.
 Increase or decrease empty LEBs counter if needed 
 Take care about indexing LEBs counter if needed 
  ubifs_get_lp_stats - get lprops statistics.
  @c: UBIFS file-system description object
  @lst: return statistics
  ubifs_change_one_lp - change LEB properties.
  @c: the UBIFS file-system description object
  @lnum: LEB to change properties for
  @free: amount of free space
  @dirty: amount of dirty space
  @flags_set: flags to set
  @flags_clean: flags to clean
  @idx_gc_cnt: change to the count of idx_gc list
  This function changes properties of LEB @lnum. It is a helper wrapper over
  'ubifs_change_lp()' which hides lprops getrelease. The arguments are the
  same as in case of 'ubifs_change_lp()'. Returns zero in case of success and
  a negative error code in case of failure.
  ubifs_update_one_lp - update LEB properties.
  @c: the UBIFS file-system description object
  @lnum: LEB to change properties for
  @free: amount of free space
  @dirty: amount of dirty space to add
  @flags_set: flags to set
  @flags_clean: flags to clean
  This function is the same as 'ubifs_change_one_lp()' but @dirty is added to
  current dirty space, not substitutes it.
  ubifs_read_one_lp - read LEB properties.
  @c: the UBIFS file-system description object
  @lnum: LEB to read properties for
  @lp: where to store read properties
  This helper function reads properties of a LEB @lnum and stores them in @lp.
  Returns zero in case of success and a negative error code in case of
  failure.
  ubifs_fast_find_free - try to find a LEB with free space quickly.
  @c: the UBIFS file-system description object
  This function returns LEB properties for a LEB with free space or %NULL if
  the function is unable to find a LEB quickly.
  ubifs_fast_find_empty - try to find an empty LEB quickly.
  @c: the UBIFS file-system description object
  This function returns LEB properties for an empty LEB or %NULL if the
  function is unable to find an empty LEB quickly.
  ubifs_fast_find_freeable - try to find a freeable LEB quickly.
  @c: the UBIFS file-system description object
  This function returns LEB properties for a freeable LEB or %NULL if the
  function is unable to find a freeable LEB quickly.
  ubifs_fast_find_frdi_idx - try to find a freeable index LEB quickly.
  @c: the UBIFS file-system description object
  This function returns LEB properties for a freeable index LEB or %NULL if the
  function is unable to find a freeable index LEB quickly.
  Everything below is related to debugging.
  dbg_check_cats - check category heaps and lists.
  @c: UBIFS file-system description object
  This function returns %0 on success and a negative error code on failure.
  scan_check_cb - scan callback.
  @c: the UBIFS file-system description object
  @lp: LEB properties to scan
  @in_tree: whether the LEB properties are in main memory
  @lst: lprops statistics to update
  This function returns a code that indicates whether the scan should continue
  (%LPT_SCAN_CONTINUE), whether the LEB properties should be added to the tree
  in main memory (%LPT_SCAN_ADD), or whether the scan should stop
  (%LPT_SCAN_STOP).
 Check lp is on its category list (if it has one) 
 Check lp is on its category heap (if it has one) 
	
	  After an unclean unmount, empty and freeable LEBs
	  may contain garbage - do not scan them.
			
			  Empty or freeable LEBs could contain index
			  nodes from an uncompleted commit due to an
			  unclean unmount. Or they could be empty for
			  the same reason. Or it may simply not have been
			  unmapped.
		
		  After an unclean unmount, an index LEB could have a different
		  amount of free space than the value recorded by lprops. That
		  is because the in-the-gaps method may use free space or
		  create free space (as a side-effect of using ubi_leb_change
		  and not writing the whole LEB). The incorrect free space
		  value is not a problem because the index is only ever
		  allocated empty LEBs, so there will never be an attempt to
		  write to the free space at the end of an index LEB - except
		  by the in-the-gaps method for which it is not a problem.
 Free but not unmapped LEB, it's fine 
  dbg_check_lprops - check all LEB properties.
  @c: UBIFS file-system description object
  This function checks all LEB properties and makes sure they are all correct.
  It returns zero if everything is fine, %-EINVAL if there is an inconsistency
  and other negative error codes in case of other errors. This function is
  called while the file system is locked (because of commit start), so no
  additional locking is required. Note that locking the LPT mutex would cause
  a circular lock dependency with the TNC mutex.
	
	  As we are going to scan the media, the write buffers have to be
	  synchronized.
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Adrian Hunter
           Artem Bityutskiy ( )
  This file implements the LEB properties tree (LPT) area. The LPT area
  contains the LEB properties tree, a table of LPT area eraseblocks (ltab), and
  (for the "big" model) a table of saved LEB numbers (lsave). The LPT area sits
  between the log and the orphan area.
  The LPT area is like a miniature self-contained file system. It is required
  that it never runs out of space, is fast to access and update, and scales
  logarithmically. The LEB properties tree is implemented as a wandering tree
  much like the TNC, and the LPT area has its own garbage collection.
  The LPT has two slightly different forms called the "small model" and the
  "big model". The small model is used when the entire LEB properties table
  can be written into a single eraseblock. In that case, garbage collection
  consists of just writing the whole table, which therefore makes all other
  eraseblocks reusable. In the case of the big model, dirty eraseblocks are
  selected for garbage collection, which consists of marking the clean nodes in
  that LEB as dirty, and then only the dirty nodes are written out. Also, in
  the case of the big model, a table of LEB numbers is saved so that the entire
  LPT does not to be scanned looking for empty eraseblocks when UBIFS is first
  mounted.
  do_calc_lpt_geom - calculate sizes for the LPT area.
  @c: the UBIFS file-system description object
  Calculate the sizes of LPT bit fields, nodes, and tree, based on the
  properties of the flash and whether LPT is "big" (c->big_lpt).
 Calculate the minimum LPT size 
 Add wastage 
  ubifs_calc_lpt_geom - calculate and check sizes for the LPT area.
  @c: the UBIFS file-system description object
  This function returns %0 on success and a negative error code on failure.
 Verify that lpt_lebs is big enough 
 Must have at least 2 times the size 
 Verify that ltab fits in a single LEB (since ltab is a single node 
  calc_dflt_lpt_geom - calculate default LPT geometry.
  @c: the UBIFS file-system description object
  @main_lebs: number of main area LEBs is passed and returned here
  @big_lpt: whether the LPT area is "big" is returned here
  The size of the LPT area depends on parameters that themselves are dependent
  on the size of the LPT area. This function, successively recalculates the LPT
  area geometry until the parameters and resultant geometry are consistent.
  This function returns %0 on success and a negative error code on failure.
 Start by assuming the minimum number of LPT LEBs 
 And assume we will use the small LPT model 
	
	  Calculate the geometry based on assumptions above and then see if it
	  makes sense
 Small LPT model must have lpt_sz < leb_size 
 Nope, so try again using big LPT model 
 Now check there are enough LPT LEBs 
 Allow 4 times the size 
 Not enough LPT LEBs so try again with more 
  pack_bits - pack bit fields end-to-end.
  @c: UBIFS file-system description object
  @addr: address at which to pack (passed and next address returned)
  @pos: bit position at which to pack (passed and next position returned)
  @val: value to pack
  @nrbits: number of bits of value to pack (1-32)
  ubifs_unpack_bits - unpack bit fields.
  @c: UBIFS file-system description object
  @addr: address at which to unpack (passed and next address returned)
  @pos: bit position at which to unpack (passed and next position returned)
  @nrbits: number of bits of value to unpack (1-32)
  This functions returns the value unpacked.
  ubifs_pack_pnode - pack all the bit fields of a pnode.
  @c: UBIFS file-system description object
  @buf: buffer into which to pack
  @pnode: pnode to pack
  ubifs_pack_nnode - pack all the bit fields of a nnode.
  @c: UBIFS file-system description object
  @buf: buffer into which to pack
  @nnode: nnode to pack
  ubifs_pack_ltab - pack the LPT's own lprops table.
  @c: UBIFS file-system description object
  @buf: buffer into which to pack
  @ltab: LPT's own lprops table to pack
  ubifs_pack_lsave - pack the LPT's save table.
  @c: UBIFS file-system description object
  @buf: buffer into which to pack
  @lsave: LPT's save table to pack
  ubifs_add_lpt_dirt - add dirty space to LPT LEB properties.
  @c: UBIFS file-system description object
  @lnum: LEB number to which to add dirty space
  @dirty: amount of dirty space to add
  set_ltab - set LPT LEB properties.
  @c: UBIFS file-system description object
  @lnum: LEB number
  @free: amount of free space
  @dirty: amount of dirty space
  ubifs_add_nnode_dirt - add dirty space to LPT LEB properties.
  @c: UBIFS file-system description object
  @nnode: nnode for which to add dirt
  add_pnode_dirt - add dirty space to LPT LEB properties.
  @c: UBIFS file-system description object
  @pnode: pnode for which to add dirt
  calc_nnode_num - calculate nnode number.
  @row: the row in the tree (root is zero)
  @col: the column in the row (leftmost is zero)
  The nnode number is a number that uniquely identifies a nnode and can be used
  easily to traverse the tree from the root to that nnode.
  This function calculates and returns the nnode number for the nnode at @row
  and @col.
  calc_nnode_num_from_parent - calculate nnode number.
  @c: UBIFS file-system description object
  @parent: parent nnode
  @iip: index in parent
  The nnode number is a number that uniquely identifies a nnode and can be used
  easily to traverse the tree from the root to that nnode.
  This function calculates and returns the nnode number based on the parent's
  nnode number and the index in parent.
  calc_pnode_num_from_parent - calculate pnode number.
  @c: UBIFS file-system description object
  @parent: parent nnode
  @iip: index in parent
  The pnode number is a number that uniquely identifies a pnode and can be used
  easily to traverse the tree from the root to that pnode.
  This function calculates and returns the pnode number based on the parent's
  nnode number and the index in parent.
  ubifs_create_dflt_lpt - create default LPT.
  @c: UBIFS file-system description object
  @main_lebs: number of main area LEBs is passed and returned here
  @lpt_first: LEB number of first LPT LEB
  @lpt_lebs: number of LEBs for LPT is passed and returned here
  @big_lpt: use big LPT model is passed and returned here
  @hash: hash of the LPT is returned here
  This function returns %0 on success and a negative error code on failure.
 Needed by 'ubifs_pack_nnode()' and 'set_ltab()' 
 Needed by 'set_ltab()' 
 Needed by 'ubifs_pack_lsave()' 
 Needed by set_ltab 
 Initialize LPT's own lprops 
 Number of leaf nodes (pnodes) 
	
	  The first pnode contains the LEB properties for the LEBs that contain
	  the root inode node and the root index node of the index tree.
 Add first pnode 
 Reset pnode values for remaining pnodes 
	
	  To calculate the internal node branches, we keep information about
	  the level below.
 LEB number of level below 
 Offset of level below 
 Number of nodes in level below 
 Size of nodes in level below 
 Add all remaining pnodes 
		
		  pnodes are simply numbered left to right starting at zero,
		  which means the pnode number can be used easily to traverse
		  down the tree to the corresponding pnode.
 Add all nnodes, one level at a time 
 Number of internal nodes (nnodes) at next level 
 Only 1 nnode at this level, so it is the root 
 Set branches to the level below 
 Only 1 nnode at this level, so it is the root 
 Update the information about the level below 
 Need to add LPT's save table 
 Need to add LPT's own LEB properties table 
 Update ltab before packing it 
 Write remaining buffer 
  update_cats - add LEB properties of a pnode to LEB category lists and heaps.
  @c: UBIFS file-system description object
  @pnode: pnode
  When a pnode is loaded into memory, the LEB properties it contains are added,
  by this function, to the LEB category lists and heaps.
  replace_cats - add LEB properties of a pnode to LEB category lists and heaps.
  @c: UBIFS file-system description object
  @old_pnode: pnode copied
  @new_pnode: pnode copy
  During commit it is sometimes necessary to copy a pnode
  (see dirty_cow_pnode).  When that happens, references in
  category lists and heaps must be replaced.  This function does that.
  check_lpt_crc - check LPT node crc is correct.
  @c: UBIFS file-system description object
  @buf: buffer containing node
  @len: length of node
  This function returns %0 on success and a negative error code on failure.
  check_lpt_type - check LPT node type is correct.
  @c: UBIFS file-system description object
  @addr: address of type bit field is passed and returned updated here
  @pos: position of type bit field is passed and returned updated here
  @type: expected type
  This function returns %0 on success and a negative error code on failure.
  unpack_pnode - unpack a pnode.
  @c: UBIFS file-system description object
  @buf: buffer containing packed pnode to unpack
  @pnode: pnode structure to fill
  This function returns %0 on success and a negative error code on failure.
  ubifs_unpack_nnode - unpack a nnode.
  @c: UBIFS file-system description object
  @buf: buffer containing packed nnode to unpack
  @nnode: nnode structure to fill
  This function returns %0 on success and a negative error code on failure.
  unpack_ltab - unpack the LPT's own lprops table.
  @c: UBIFS file-system description object
  @buf: buffer from which to unpack
  This function returns %0 on success and a negative error code on failure.
  unpack_lsave - unpack the LPT's save table.
  @c: UBIFS file-system description object
  @buf: buffer from which to unpack
  This function returns %0 on success and a negative error code on failure.
  validate_nnode - validate a nnode.
  @c: UBIFS file-system description object
  @nnode: nnode to validate
  @parent: parent nnode (or NULL for the root nnode)
  @iip: index in parent
  This function returns %0 on success and a negative error code on failure.
  validate_pnode - validate a pnode.
  @c: UBIFS file-system description object
  @pnode: pnode to validate
  @parent: parent nnode
  @iip: index in parent
  This function returns %0 on success and a negative error code on failure.
  set_pnode_lnum - set LEB numbers on a pnode.
  @c: UBIFS file-system description object
  @pnode: pnode to update
  This function calculates the LEB numbers for the LEB properties it contains
  based on the pnode number.
  ubifs_read_nnode - read a nnode from flash and link it to the tree in memory.
  @c: UBIFS file-system description object
  @parent: parent nnode (or NULL for the root)
  @iip: index in parent
  This function returns %0 on success and a negative error code on failure.
		
		  This nnode was not written which just means that the LEB
		  properties in the subtree below it describe empty LEBs. We
		  make the nnode as though we had read it, which in fact means
		  doing almost nothing.
  read_pnode - read a pnode from flash and link it to the tree in memory.
  @c: UBIFS file-system description object
  @parent: parent nnode
  @iip: index in parent
  This function returns %0 on success and a negative error code on failure.
		
		  This pnode was not written which just means that the LEB
		  properties in it describe empty LEBs. We make the pnode as
		  though we had read it.
  read_ltab - read LPT's own lprops table.
  @c: UBIFS file-system description object
  This function returns %0 on success and a negative error code on failure.
  read_lsave - read LPT's save table.
  @c: UBIFS file-system description object
  This function returns %0 on success and a negative error code on failure.
		
		  Due to automatic resizing, the values in the lsave table
		  could be beyond the volume size - just ignore them.
  ubifs_get_nnode - get a nnode.
  @c: UBIFS file-system description object
  @parent: parent nnode (or NULL for the root)
  @iip: index in parent
  This function returns a pointer to the nnode on success or a negative error
  code on failure.
  ubifs_get_pnode - get a pnode.
  @c: UBIFS file-system description object
  @parent: parent nnode
  @iip: index in parent
  This function returns a pointer to the pnode on success or a negative error
  code on failure.
  ubifs_pnode_lookup - lookup a pnode in the LPT.
  @c: UBIFS file-system description object
  @i: pnode number (0 to (main_lebs - 1)  UBIFS_LPT_FANOUT)
  This function returns a pointer to the pnode on success or a negative
  error code on failure.
  ubifs_lpt_lookup - lookup LEB properties in the LPT.
  @c: UBIFS file-system description object
  @lnum: LEB number to lookup
  This function returns a pointer to the LEB properties on success or a
  negative error code on failure.
  dirty_cow_nnode - ensure a nnode is not being committed.
  @c: UBIFS file-system description object
  @nnode: nnode to check
  Returns dirtied nnode on success or negative error code on failure.
 nnode is not being committed 
 nnode is being committed, so copy it 
 The children now have new parent 
  dirty_cow_pnode - ensure a pnode is not being committed.
  @c: UBIFS file-system description object
  @pnode: pnode to check
  Returns dirtied pnode on success or negative error code on failure.
 pnode is not being committed 
 pnode is being committed, so copy it 
  ubifs_lpt_lookup_dirty - lookup LEB properties in the LPT.
  @c: UBIFS file-system description object
  @lnum: LEB number to lookup
  This function returns a pointer to the LEB properties on success or a
  negative error code on failure.
  ubifs_lpt_calc_hash - Calculate hash of the LPT pnodes
  @c: UBIFS file-system description object
  @hash: the returned hash of the LPT pnodes
  This function iterates over the LPT pnodes and creates a hash over them.
  Returns 0 for success or a negative error code otherwise.
 Go right 
 Go down 
 Go up and to the right 
  lpt_check_hash - check the hash of the LPT.
  @c: UBIFS file-system description object
  This function calculates a hash over all pnodes in the LPT and compares it with
  the hash stored in the master node. Returns %0 on success and a negative error
  code on failure.
  lpt_init_rd - initialize the LPT for reading.
  @c: UBIFS file-system description object
  This function returns %0 on success and a negative error code on failure.
  lpt_init_wr - initialize the LPT for writing.
  @c: UBIFS file-system description object
  'lpt_init_rd()' must have been called already.
  This function returns %0 on success and a negative error code on failure.
  ubifs_lpt_init - initialize the LPT.
  @c: UBIFS file-system description object
  @rd: whether to initialize lpt for reading
  @wr: whether to initialize lpt for writing
  For mounting 'rw', @rd and @wr are both true. For mounting 'ro', @rd is true
  and @wr is false. For mounting from 'ro' to 'rw', @rd is false and @wr is
  true.
  This function returns %0 on success and a negative error code on failure.
  struct lpt_scan_node - somewhere to put nodes while we scan LPT.
  @nnode: where to keep a nnode
  @pnode: where to keep a pnode
  @cnode: where to keep a cnode
  @in_tree: is the node in the tree in memory
  @ptr.nnode: pointer to the nnode (if it is an nnode) which may be here or in
  the tree
  @ptr.pnode: ditto for pnode
  @ptr.cnode: ditto for cnode
  scan_get_nnode - for the scan, get a nnode from either the tree or flash.
  @c: the UBIFS file-system description object
  @path: where to put the nnode
  @parent: parent of the nnode
  @iip: index in parent of the nnode
  This function returns a pointer to the nnode on success or a negative error
  code on failure.
		
		  This nnode was not written which just means that the LEB
		  properties in the subtree below it describe empty LEBs. We
		  make the nnode as though we had read it, which in fact means
		  doing almost nothing.
  scan_get_pnode - for the scan, get a pnode from either the tree or flash.
  @c: the UBIFS file-system description object
  @path: where to put the pnode
  @parent: parent of the pnode
  @iip: index in parent of the pnode
  This function returns a pointer to the pnode on success or a negative error
  code on failure.
		
		  This pnode was not written which just means that the LEB
		  properties in it describe empty LEBs. We make the pnode as
		  though we had read it.
  ubifs_lpt_scan_nolock - scan the LPT.
  @c: the UBIFS file-system description object
  @start_lnum: LEB number from which to start scanning
  @end_lnum: LEB number at which to stop scanning
  @scan_cb: callback function called for each lprops
  @data: data to be passed to the callback function
  This function returns %0 on success and a negative error code on failure.
 Descend to the pnode containing start_lnum 
 Loop for each lprops 
 Add all the nodes in path to the tree in memory 
 Get the next lprops 
			
			  We got to the end without finding what we were
			  looking for
 Wrap-around to the beginning 
 Next lprops is in the same pnode 
 We need to get the next pnode. Go up until we can go right 
 Go right 
 Descend to the pnode 
  dbg_chk_pnode - check a pnode.
  @c: the UBIFS file-system description object
  @pnode: pnode to check
  @col: pnode column
  This function returns %0 on success and a negative error code on failure.
  dbg_check_lpt_nodes - check nnodes and pnodes.
  @c: the UBIFS file-system description object
  @cnode: next cnode (nnode or pnode) to check
  @row: row of cnode (root is zero)
  @col: column of cnode (leftmost is zero)
  This function returns %0 on success and a negative error code on failure.
 cnode is a nnode 
 Go down 
 Go right 
 cnode is a pnode 
 Go up and to the right 
 SPDX-License-Identifier: GPL-2.0-only
  This file is part of UBIFS.
  Copyright (C) 2006-2008 Nokia Corporation.
  Authors: Artem Bityutskiy ( )
           Adrian Hunter
  This file is a part of UBIFS journal implementation and contains various
  functions which manipulate the log. The log is a fixed area on the flash
  which does not contain any data but refers to buds. The log is a part of the
  journal.
  ubifs_search_bud - search bud LEB.
  @c: UBIFS file-system description object
  @lnum: logical eraseblock number to search
  This function searches bud LEB @lnum. Returns bud description object in case
  of success and %NULL if there is no bud with this LEB number.
  ubifs_get_wbuf - get the wbuf associated with a LEB, if there is one.
  @c: UBIFS file-system description object
  @lnum: logical eraseblock number to search
  This functions returns the wbuf for @lnum or %NULL if there is not one.
  empty_log_bytes - calculate amount of empty space in the log.
  @c: UBIFS file-system description object
  ubifs_add_bud - add bud LEB to the tree of buds and its journal head list.
  @c: UBIFS file-system description object
  @bud: the bud to add
	
	  Note, although this is a new bud, we anyway account this space now,
	  before any data has been written to it, because this is about to
	  guarantee fixed mount time, and this bud will anyway be read and
	  scanned.
  ubifs_add_bud_to_log - add a new bud to the log.
  @c: UBIFS file-system description object
  @jhead: journal head the bud belongs to
  @lnum: LEB number of the bud
  @offs: starting offset of the bud
  This function writes a reference node for the new bud LEB @lnum to the log,
  and adds it to the buds trees. It also makes sure that log size does not
  exceed the 'c->max_bud_bytes' limit. Returns zero in case of success,
  %-EAGAIN if commit is required, and a negative error code in case of
  failure.
 Make sure we have enough space in the log 
	
	  Make sure the amount of space in buds will not exceed the
	  'c->max_bud_bytes' limit, because we want to guarantee mount time
	  limits.
	 
	  It is not necessary to hold @c->buds_lock when reading @c->bud_bytes
	  because we are holding @c->log_mutex. All @c->bud_bytes take place
	  when both @c->log_mutex and @c->bud_bytes are locked.
	
	  If the journal is full enough - start background commit. Note, it is
	  OK to read 'c->cmt_state' without spinlock because integer reads
	  are atomic in the kernel.
 Must ensure next log LEB has been unmapped 
		
		  Before writing the LEB reference which refers an empty LEB
		  to the log, we have to make sure it is mapped, because
		  otherwise we'd risk to refer an LEB with garbage in case of
		  an unclean reboot, because the target LEB might have been
		  unmapped, but not yet physically erased.
  remove_buds - remove used buds.
  @c: UBIFS file-system description object
  This function removes use buds from the buds tree. It does not remove the
  buds which are pointed to by journal heads.
			
			  Do not remove buds which are pointed to by journal
			  heads (non-closed buds).
			
			  If the commit does not finish, the recovery will need
			  to replay the journal, in which case the old buds
			  must be unchanged. Do not release them until post
			  commit i.e. do not allow them to be garbage
			  collected.
  ubifs_log_start_commit - start commit.
  @c: UBIFS file-system description object
  @ltail_lnum: return new log tail LEB number
  The commit operation starts with writing "commit start" node to the log and
  reference nodes for all journal heads which will define new journal after
  the commit has been finished. The commit start and reference nodes are
  written in one go to the nearest empty log LEB (hence, when commit is
  finished UBIFS may safely unmap all the previous log LEBs). This function
  returns zero in case of success and a negative error code in case of
  failure.
	
	  Note, we do not lock 'c->log_mutex' because this is the commit start
	  phase and we are exclusively using the log. And we do not lock
	  write-buffer because nobody can write to the file-system at this
	  phase.
 Switch to the next log LEB 
 Must ensure next LEB has been unmapped 
	
	  We have started the commit and now users may use the rest of the log
	  for new writes.
  ubifs_log_end_commit - end commit.
  @c: UBIFS file-system description object
  @ltail_lnum: new log tail LEB number
  This function is called on when the commit operation was finished. It
  moves log tail to new position and updates the master node so that it stores
  the new log tail LEB number. Returns zero in case of success and a negative
  error code in case of failure.
	
	  At this phase we have to lock 'c->log_mutex' because UBIFS allows FS
	  writes during commit. Its only short "commit" start phase when
	  writers are blocked.
	
	  The commit is finished and from now on it must be guaranteed that
	  there is always enough space for the next commit.
  ubifs_log_post_commit - things to do after commit is completed.
  @c: UBIFS file-system description object
  @old_ltail_lnum: old log tail LEB number
  Release buds only after commit is completed, because they must be unchanged
  if recovery is needed.
  Unmap log LEBs only after commit is completed, because they may be needed for
  recovery.
  This function returns %0 on success and a negative error code on failure.
  struct done_ref - references that have been done.
  @rb: rb-tree node
  @lnum: LEB number
  done_already - determine if a reference has been done already.
  @done_tree: rb-tree to store references that have been done
  @lnum: LEB number of reference
  This function returns %1 if the reference has been done, %0 if not, otherwise
  a negative error code is returned.
  destroy_done_tree - destroy the done tree.
  @done_tree: done tree to destroy
  add_node - add a node to the consolidated log.
  @c: UBIFS file-system description object
  @buf: buffer to which to add
  @lnum: LEB number to which to write is passed and returned here
  @offs: offset to where to write is passed and returned here
  @node: node to add
  This function returns %0 on success and a negative error code on failure.
  ubifs_consolidate_log - consolidate the log.
  @c: UBIFS file-system description object
  Repeated failed commits could cause the log to be full, but at least 1 LEB is
  needed for commit. This function rewrites the reference nodes in the log
  omitting duplicates, and failed CS nodes, and leaving no gaps.
  This function returns %0 on success and a negative error code on failure.
 Unmap remaining LEBs 
  dbg_check_bud_bytes - make sure bud bytes calculation are all right.
  @c: UBIFS file-system description object
  This function makes sure the amount of flash space used by closed buds
  ('c->bud_bytes' is correct). Returns zero in case of success and %-EINVAL in
  case of failure.
 SPDX-License-Identifier: GPL-2.0-only
  kvm asynchronous fault support
  Copyright 2010 Red Hat, Inc.
  Author:
       Gleb Natapov <gleb@redhat.com>
	
	  This work is run asynchronously to the task which owns
	  mm and might be done in another context, so we must
	  access remotely.
	
	  apf may be freed by kvm_check_async_pf_completion() after
	  this point
 cancel outstanding work queue item 
		
		  We know it's present in vcpu->async_pf.done, do
		  nothing here.
 == work->vcpu->kvm 
  Try to schedule a job to handle page fault asynchronously. Returns 'true' on
  success, 'false' on failure (page fault has to be handled synchronously).
 Arch specific code should not do async PF in this case 
	
	  do alloc nowait since if we are going to sleep anyway we
	  may as well sleep faulting in page
 for list_del to work 
 SPDX-License-Identifier: GPL-2.0
  KVM coalesced MMIO
  Copyright (c) 2008 Bull S.A.S.
  Copyright 2009 Red Hat, Inc. andor its affiliates.
   Author: Laurent Vivier <Laurent.Vivier@bull.net>
	 is it in a batchable area ?
	  (addr,len) is fully included in
	  (zone->addr, zone->size)
 Are we able to batch it ? 
	 last is the first free entry
	  check if we don't meet the first used entry
	  there is always one unused entry in the buffer
 full 
 copy data in first free entry of the ring 
	
	  We're using this spinlock to sync access to the coalesced ring.
	  The list doesn't need its own lock since device registration and
	  unregistration should only happen when kvm->slots_lock is held.
			
			  On failure, unregister destroys all devices on the
			  bus _except_ the target device, i.e. coalesced_zones
			  has been modified.  No need to restart the walk as
			  there aren't any zones left.
	
	  Ignore the result of kvm_io_bus_unregister_dev(), from userspace's
	  perspective, the coalesced MMIO is most definitely unregistered.
 SPDX-License-Identifier: GPL-2.0-only
  Kernel-based Virtual Machine driver for Linux
  This module enables machines with Intel VT-x extensions to run virtual
  machines without emulation or binary translation.
  Copyright (C) 2006 Qumranet, Inc.
  Copyright 2010 Red Hat, Inc. andor its affiliates.
  Authors:
    Avi Kivity   <avi@qumranet.com>
    Yaniv Kamay  <yaniv@qumranet.com>
 Worst case buffer size needed for holding an integer. 
 Architectures should define their poll value according to the halt latency 
 Default doubles per-vcpu halt_poll_ns. 
 The start value to grow halt_poll_ns from 
 10us 
 Default resets per-vcpu halt_poll_ns . 
  Ordering of locks:
 	kvm->lock --> kvm->slots_lock --> kvm->irq_lock
  For architectures that don't implement a compat infrastructure,
  adopt a double line of defense:
  - Prevent a compat task from opening devkvm
  - If the open has been done by a 64bit task, and the KVM fd
    passed to a compat task, let the ioctls fail.
	
	  The metadata used by is_zone_device_page() to determine whether or
	  not a page is ZONE_DEVICE is guaranteed to be valid if and only if
	  the device has been pinned, e.g. by get_user_pages().  WARN if the
	  page_count() is zero to help detect bad usage of this helper.
	
	  ZONE_DEVICE pages currently set PG_reserved, but from a refcounting
	  perspective they are "normal" pages, albeit with slightly different
	  usage rules.
  Switches to specified vcpu, until a matching vcpu_put()
 TODO: merge with kvm_arch_vcpu_should_kick 
	
	  We need to wait for the VCPU to reenable interrupts and get out of
	  READING_SHADOW_PAGE_TABLES mode.
	
	  Need to kick a running VCPU, but otherwise there is nothing to do.
	
	  Note, the vCPU could get migrated to a different pCPU at any point
	  after kvm_request_needs_ipi(), which could result in sending an IPI
	  to the previous pCPU.  But, that's OK because the purpose of the IPI
	  is to ensure the vCPU returns to OUTSIDE_GUEST_MODE, which is
	  satisfied if the vCPU migrates. Entering READING_SHADOW_PAGE_TABLES
	  after this point is also OK, as the requirement is only that KVM wait
	  for vCPUs that were reading SPTEs _before_ any changes were
	  finalized. See kvm_vcpu_kick() for more details on handling requests.
	
	  We want to publish modifications to the page tables before reading
	  mode. Pairs with a memory barrier in arch-specific code.
	  - x86: smp_mb__after_srcu_read_unlock in vcpu_enter_guest
	  and smp_mb in walk_shadow_page_lockless_beginend.
	  - powerpc: smp_mb in kvmppc_prepare_to_enter.
	 
	  There is already an smp_mb__after_atomic() before
	  kvm_make_all_cpus_request() reads vcpu->mode. We reuse that
	  barrier here.
	
	  No need for rcu_read_lock as VCPU_RUN is the only place that changes
	  the vcpu->pid pointer, and at destruction time all file descriptors
	  are already gone.
  Use a dedicated stub instead of NULL to indicate that there is no callback
  functionhandler.  The compiler technically can't guarantee that a real
  function will have a non-zero address, and so it will generate code to
  check for !NULL, whereas comparing against a stub will be elided at compile
  time (unless the compiler is getting long in the tooth, e.g. gcc 4.9).
 A null handler is allowed if and only if on_lock() is provided. 
			
			  To optimize for the likely case where the address
			  range is covered by zero or one memslots, don't
			  bother making these conditional (to avoid writes on
			  the second or later invocation of the handler).
			
			  {gfn(page) | page intersects with [hva_start, hva_end)} =
			  {gfn_start, gfn_start+1, ..., gfn_end-1}.
 The notifiers are averse to booleans. :-( 
	
	  .change_pte() must be surrounded by .invalidate_range_{start,end}().
	  If mmu_notifier_count is zero, then no in-progress invalidations,
	  including this one, found a relevant memslot at start(); rechecking
	  memslots here is unnecessary.  Note, a false positive (count elevated
	  by a different invalidation) is sub-optimal but functionally ok.
	
	  The count increase must become visible at unlock time as no
	  spte can be established without taking the mmu_lock and
	  count is also read inside the mmu_lock critical section.
		
		  Fully tracking multiple concurrent ranges has dimishing
		  returns. Keep things simple and just find the minimal range
		  which includes the current and new ranges. As there won't be
		  enough information to subtract a range after its invalidate
		  completes, any ranges invalidated concurrently will
		  accumulate and persist until all outstanding invalidates
		  complete.
	
	  Prevent memslot modification between range_start() and range_end()
	  so that conditionally locking provides the same result in both
	  functions.  Without that guarantee, the mmu_notifier_count
	  adjustments will be imbalanced.
	 
	  Pairs with the decrement in range_end().
	
	  This sequence increase will notify the kvm page fault that
	  the page that is going to be mapped in the spte could have
	  been freed.
	
	  The above sequence increase must be visible before the
	  below count decrease, which is ensured by the smp_wmb above
	  in conjunction with the smp_rmb in mmu_notifier_retry().
 Pairs with the increment in range_start(). 
	
	  There can only be one waiter, since the wait happens under
	  slots_lock.
	
	  Even though we do not flush TLB, this will still adversely
	  affect performance on pre-Haswell Intel EPT, where there is
	  no EPT Access Bit to clear so that we have to tear down EPT
	  tables instead. If we find this unacceptable, we can always
	  add a parameter to kvm_age_hva so that it effectively doesn't
	  do anything on clear_young.
	 
	  Also note that currently we never issue secondary TLB flushes
	  from clear_young, leaving this job up to the regular system
	  cadence. If we find this inaccurate, we might come up with a
	  more sophisticated heuristic later.
 !(CONFIG_MMU_NOTIFIER && KVM_ARCH_WANT_MMU_NOTIFIER) 
 CONFIG_MMU_NOTIFIER && KVM_ARCH_WANT_MMU_NOTIFIER 
 Suspend KVM before we suspend ftrace, RCU, etc. 
 !CONFIG_HAVE_KVM_PM_NOTIFIER 
 CONFIG_HAVE_KVM_PM_NOTIFIER 
  Called after the VM is otherwise initialized, but just before adding it to
  the vm_list.
  Called just after removing the VM from the vm_list, but before doing any
  other destruction.
  Called after per-vm debugfs created.  When called kvm->debugfs_dentry should
  be setup already, so we can create arch-specific debugfs entries under it.
  Cleanup should be automatic done in kvm_destroy_vm_debugfs() recursively, so
  a per-arch destroy interface is not needed.
 Generations must be different for each address space. 
	
	  We do not need to take the kvm->lock here, because nobody else
	  has a reference to the struct kvm at this point and therefore
	  cannot access the devices list anyhow.
	
	  At this point, pending calls to invalidate_range_start()
	  have completed but no more MMU notifiers will run, so
	  mn_active_invalidate_count may remain unbalanced.
	  No threads can be waiting in install_new_memslots as the
	  last reference on KVM has been dropped, but freeing
	  memslots would deadlock without this manual intervention.
  Make sure the vm is not during destruction, which is a safe version of
  kvm_get_kvm().  Return true if kvm referenced successfully, false otherwise.
  Used to put a reference that was taken on behalf of an object associated
  with a user-visible file descriptor, e.g. a vcpu or device, if installation
  of the new file descriptor fails and the reference cannot be transferred to
  its final owner.  In such cases, the caller is still actively using @kvm and
  will fail miserably if the refcount unexpectedly hits zero.
  Allocation size is twice as large as the actual dirty bitmap size.
  See kvm_vm_ioctl_get_dirty_log() why this is needed.
  Delete a memslot by decrementing the number of used slots and shifting all
  other entries in the array forward one spot.
  "Insert" a new memslot by incrementing the number of used slots.  Returns
  the new slot's initial index into the memslots array.
  Move a changed memslot backwards in the array by shifting existing slots
  with a higher GFN toward the front of the array.  Note, the changed memslot
  itself is not preserved in the array, i.e. not swapped at this time, only
  its new index into the array is tracked.  Returns the changed memslot's
  current index into the memslots array.
	
	  Move the target memslot backward in the array by shifting existing
	  memslots with a higher GFN (than the target memslot) towards the
	  front of the array.
 Shift the next memslot forward one and update its index. 
  Move a changed memslot forwards in the array by shifting existing slots with
  a lower GFN toward the back of the array.  Note, the changed memslot itself
  is not preserved in the array, i.e. not swapped at this time, only its new
  index into the array is tracked.  Returns the changed memslot's final index
  into the memslots array.
 Shift the next memslot back one and update its index. 
  Re-sort memslots based on their GFN to account for an added, deleted, or
  moved memslot.  Sorting memslots by GFN allows using a binary search during
  memslot lookup.
  IMPORTANT: Slots are sorted from highest GFN to lowest GFN!  I.e. the entry
  at memslots[0] has the highest GFN.
  The sorting algorithm takes advantage of having initially sorted memslots
  and knowing the position of the changed memslot.  Sorting is also optimized
  by not swapping the updated memslot and instead only shifting other memslots
  and tracking the new index for the update memslot.  Only once its final
  index is known is the updated memslot copied into its position in the array.
   - When deleting a memslot, the deleted memslot simply needs to be moved to
     the end of the array.
   - When creating a memslot, the algorithm "inserts" the new memslot at the
     end of the array and then it forward to its correct location.
   - When moving a memslot, the algorithm first moves the updated memslot
     backward to handle the scenario where the memslot's GFN was changed to a
     lower value.  update_memslots() then falls through and runs the same flow
     as creating a memslot to move the memslot forward to handle the scenario
     where its GFN was changed to a higher value.
  Note, slots are sorted from highest->lowest instead of lowest->highest for
  historical reasons.  Originally, invalid memslots where denoted by having
  GFN=0, thus sorting from highest->lowest naturally sorted invalid memslots
  to the end of the array.  The current algorithm uses dedicated logic to
  delete a memslot and thus does not rely on invalid memslots having GFN=0.
  The other historical motiviation for highest->lowest was to improve the
  performance of memslot lookup.  KVM originally used a linear search starting
  at memslots[0].  On x86, the largest memslot usually has one of the highest,
  if not the highest, GFN, as the bulk of the guest's RAM is located in a
  single memslot above the 4gb boundary.  As the largest memslot is also the
  most likely to be referenced, sorting it to the front of the array was
  advantageous.  The current binary search starts from the middle of the array
  and uses an LRU pointer to improve performance for all memslots and GFNs.
		
		  Copy the memslot to its new position in memslots and update
		  its index accordingly.
	
	  Do not store the new memslots while there are invalidations in
	  progress, otherwise the locking in invalidate_range_start and
	  invalidate_range_end will be unbalanced.
	
	  Acquired in kvm_set_memslot. Must be released before synchronize
	  SRCU below in order to avoid deadlock with another thread
	  acquiring the slots_arch_lock in an srcu critical section.
	
	  Increment the new memslot generation a second time, dropping the
	  update in-progress flag and incrementing the generation based on
	  the number of address spaces.  This provides a unique and easily
	  identifiable generation number while the memslots are in flux.
	
	  Generations must be unique even across address spaces.  We do not need
	  a global counter for that, instead the generation space is evenly split
	  across address spaces.  For example, with two address spaces, address
	  space 0 will use generations 0, 2, 4, ... while address space 1 will
	  use generations 1, 3, 5, ...
  Note, at a minimum, the current number of used slots must be allocated, even
  when deleting a memslot, as we need a complete duplicate of the memslots for
  use when invalidating a memslot prior to deletingmoving the memslot.
	
	  Released in install_new_memslots.
	 
	  Must be held from before the current memslots are copied until
	  after the new memslots are installed with rcu_assign_pointer,
	  then released before the synchronize srcu in install_new_memslots.
	 
	  When modifying memslots outside of the slots_lock, must be held
	  before reading the pointer to the current memslots until after all
	  changes to those memslots are complete.
	 
	  These rules ensure that installing new memslots does not lose
	  changes made to the previous memslots.
		
		  Note, the INVALID flag needs to be in the appropriate entry
		  in the freshly allocated memslots, not in @old or @new.
		
		  We can re-use the memory from the old memslots.
		  It will be overwritten with a copy of the new memslots
		  after reacquiring the slots_arch_lock below.
		 From this point no new shadow pages pointing to a deleted,
		  or moved, memslot will be created.
		 
		  validation of sp->gfn happens in:
		 	- gfn_to_hva (kvm_read_guest, gfn_to_pfn)
		 	- kvm_is_visible_gfn (mmu_check_root)
 Released in install_new_memslots. 
		
		  The arch-specific fields of the memslots could have changed
		  between releasing the slots_arch_lock in
		  install_new_memslots and here, so get a fresh copy of the
		  slots.
	
	  This is only for debugging purpose; it should never be referenced
	  for a removed memslot.
  Allocate some memory and give it an address in the guest physical address
  space.
  Discontiguous memory is allowed, mostly for framebuffers.
  Must be called holding kvm->slots_lock for write.
 General sanity checks 
 We can read the guest memory with __xxx_user() later on. 
	
	  Make a full copy of the old memslot, the pointer will become stale
	  when the memslots are re-sorted by update_memslots(), and the old
	  memslot needs to be referenced after calling update_memslots(), e.g.
	  to free its resources and for arch specific behavior.
 Modify an existing slot. 
 Nothing to change. 
 Copy dirty_bitmap and arch from the current memslot. 
 Check for overlaps 
 Allocatefree page dirty bitmap as needed 
  kvm_get_dirty_log - get a snapshot of dirty pages
  @kvm:	pointer to kvm instance
  @log:	slot id and address to which we copy the log
  @is_dirty:	set to '1' if any dirty pages were found
  @memslot:	set to the associated memslot, always valid on success
 Dirty ring tracking is exclusive to dirty log tracking 
 CONFIG_KVM_GENERIC_DIRTYLOG_READ_PROTECT 
  kvm_get_dirty_log_protect - get a snapshot of dirty pages
 	and reenable dirty page tracking for the corresponding pages.
  @kvm:	pointer to kvm instance
  @log:	slot id and address to which we copy the log
  We need to keep it in mind that VCPU threads can write to the bitmap
  concurrently. So, to avoid losing track of dirty pages we keep the
  following order:
     1. Take a snapshot of the bit and clear it if needed.
     2. Write protect the corresponding page.
     3. Copy the snapshot to the userspace.
     4. Upon return caller flushes TLB's if needed.
  Between 2 and 4, the guest may write to the page using the remaining TLB
  entry.  This is not a problem because the page is reported dirty using
  the snapshot taken before and step 4 ensures that writes done after
  exiting to userspace will be logged for the next call.
 Dirty ring tracking is exclusive to dirty log tracking 
		
		  Unlike kvm_get_dirty_log, we always return false in flush,
		  because no flush is needed until KVM_CLEAR_DIRTY_LOG.  There
		  is some code duplication between this function and
		  kvm_get_dirty_log, but hopefully all architecture
		  transition to kvm_get_dirty_log_protect and kvm_get_dirty_log
		  can be eliminated.
  kvm_vm_ioctl_get_dirty_log - get and clear the log of dirty pages in a slot
  @kvm: kvm instance
  @log: slot id and address to which we copy the log
  Steps 1-4 below provide general overview of dirty page logging. See
  kvm_get_dirty_log_protect() function description for additional details.
  We call kvm_get_dirty_log_protect() to handle steps 1-3, upon return we
  always flush the TLB (step 4) even if previous step failed  and the dirty
  bitmap may be corrupt. Regardless of previous outcome the KVM logging API
  does not preclude user space subsequent dirty log read. Flushing TLB ensures
  writes will be marked dirty for next log read.
    1. Take a snapshot of the bit and clear it if needed.
    2. Write protect the corresponding page.
    3. Copy the snapshot to the userspace.
    4. Flush TLB's if needed.
  kvm_clear_dirty_log_protect - clear dirty bits in the bitmap
 	and reenable dirty page tracking for the corresponding pages.
  @kvm:	pointer to kvm instance
  @log:	slot id and address from which to fetch the bitmap of dirty pages
 Dirty ring tracking is exclusive to dirty log tracking 
		
		  mask contains the bits that really have been cleared.  This
		  never includes any bits beyond the length of the memslot (if
		  the length is not aligned to 64 pages), therefore it is not
		  a problem if userspace sets them in log->dirty_bitmap.
 CONFIG_KVM_GENERIC_DIRTYLOG_READ_PROTECT 
	
	  Fall back to searching all memslots. We purposely use
	  search_memslots() instead of __gfn_to_memslot() to avoid
	  thrashing the VM-wide last_used_index in kvm_memslots.
  Return the hva of a @gfn and the RW attribute if possible.
  @slot: the kvm_memory_slot which contains @gfn
  @gfn: the gfn to be translated
  @writable: used to return the readwrite attribute of the @slot if the hva
  is valid and @writable is not NULL
  The fast path to get the writable pfn which will be stored in @pfn,
  true indicates success, otherwise false is returned.  It's also the
  only part that runs if we can in atomic context.
	
	  Fast pin a writable pfn only if it is a write fault request
	  or the caller allows to map a writable pfn for a read fault
	  request.
  The slow path to get the pfn of the specified host virtual address,
  1 indicates success, -errno is returned if error is detected.
 map read fault as writable if possible 
		
		  get_user_pages fails for VM_IO and VM_PFNMAP vmas and does
		  not call the fault handler, so do it here.
	
	  Get a reference here because callers of hva_to_pfn and
	  gfn_to_pfn ultimately call kvm_release_pfn_clean on the
	  returned pfn.  This is only needed if the VMA has VM_MIXEDMAP
	  set, but the kvm_try_get_pfnkvm_release_pfn_clean pair will
	  simply do nothing for reserved pfns.
	 
	  Whoever called remap_pfn_range is also going to call e.g.
	  unmap_mapping_range before the underlying pages are freed,
	  causing a call to our MMU notifier.
	 
	  Certain IO or PFNMAP mappings can be backed with valid
	  struct pages, but be allocated without refcounting e.g.,
	  tail pages of non-compound higher order allocations, which
	  would then underflow the refcount when the caller does the
	  required put_page. Don't allow those pages here.
  Pin guest page in memory and return its pfn.
  @addr: host virtual address which maps memory to the guest
  @atomic: whether this function can sleep
  @async: whether this function need to wait IO complete if the
          host page is not in the memory
  @write_fault: whether we should get a writable host page
  @writable: whether it allows to map a writable host page for !@write_fault
  The function will map a writable host page for these two cases:
  1): @write_fault = true
  2): @write_fault = false && @writable, @writable will tell the caller
      whether the mapping is writable.
 we can do it either atomically or asynchronously, not both 
 Do not map writable pfn in the readonly memslot. 
 Update ghc->generation before performing any error checks. 
	
	  If the requested region crosses two memslots, we still
	  verify that the entire region is valid here.
 Use the slow path for cross page reads and writes. 
	
	  This does a lockless modification of ->real_blocked, which is fine
	  because, only current can change ->real_blocked and all readers of
	  ->real_blocked don't care as long ->real_blocked is always a subset
	  of ->blocked.
  The vCPU has executed a HLT instruction with in-kernel mode enabled.
			
			  This sets KVM_REQ_UNHALT if an interrupt
			  arrives.
 we had a long block, shrink polling 
 we had a short halt and our poll time is too small 
  Kick a sleeping VCPU, or a guest VCPU in guest mode, into host kernel mode.
	
	  Note, the vCPU could get migrated to a different pCPU at any point
	  after kvm_arch_vcpu_should_kick(), which could result in sending an
	  IPI to the previous pCPU.  But, that's ok because the purpose of the
	  IPI is to force the vCPU to leave IN_GUEST_MODE, and migrating the
	  vCPU also requires it to leave IN_GUEST_MODE.
 !CONFIG_S390 
  Helper that checks whether a VCPU is eligible for directed yield.
  Most eligible candidate to yield is decided by following heuristics:
   (a) VCPU which has not done pl-exit or cpu relax intercepted recently
   (preempted lock holder), indicated by @in_spin_loop.
   Set at the beginning and cleared at the end of interceptionPLE handler.
   (b) VCPU which has done pl-exit cpu relax intercepted but did not get
   chance last time (mostly it has become eligible now since we have probably
   yielded to lockholder in last iteration. This is done by toggling
   @dy_eligible each time a VCPU checked for eligibility.)
   Yielding to a recently pl-exitedcpu relax intercepted VCPU before yielding
   to preempted lock-holder could result in wrong VCPU selection and CPU
   burning. Giving priority for a potential lock-holder increases lock
   progress.
   Since algorithm is based on heuristics, accessing another VCPU data without
   locking does not harm. It may result in trying to yield to  same VCPU, fail
   and continue with next VCPU and so on.
  Unlike kvm_arch_vcpu_runnable, this function is called outside
  a vcpu_loadvcpu_put pair.  However, for most architectures
  kvm_arch_vcpu_runnable does not require vcpu_load.
	
	  We boost the priority of a VCPU that is runnable but not
	  currently running, because it got preempted by something
	  else and called schedule in __vcpu_run.  Hopefully that
	  VCPU is holding the lock that we need and will release it.
	  We approximate round-robin by starting at the last boosted VCPU.
 Ensure vcpu is not eligible during next spinloop 
  Allocates an inode for the vcpu.
  Creates some virtual cpus.  Good luck creating more than one.
 Fill the stats id string for the vcpu 
 Now it's all set up, let userspace reach it 
	
	  Pairs with smp_rmb() in kvm_get_vcpu.  Write kvm->vcpus
	  before kvm->online_vcpu's incremented value.
	
	  Some architectures have vcpu ioctls that are asynchronous to vcpu
	  execution; mutex_lock() would break them.
 The thread running this VCPU changed. 
 the size should be power of 2 
 Should be bigger to keep the reserved entries, or a page 
 We only allow it to set once 
 We don't allow to change this value after vcpu created 
 CONFIG_HAVE_KVM_IRQ_ROUTING 
 one bit per page 
 one bit per page 
	
	  Don't call kvm_put_kvm anymore at this point; file->f_op is
	  already set, with ->release() being kvm_vm_release().  In error
	  cases it will be called by the final fput(file) and will take
	  care of doing kvm_put_kvm(kvm).
 struct kvm_run 
 pio data page 
 coalesced mmio ring page 
	
	  Some (well, at least mine) BIOSes hang on reboot if
	  in vmx root mode.
	 
	  And Intel TXT required VMX off for all cpu when system shutdown.
	 If r2->len == 0, match the exact address.  If r2->len != 0,
	  accept any overlapping write.  Any order is acceptable for
	  overlapping ranges, because kvm_io_bus_get_first_dev ensures
	  we process all of them.
 kvm_io_bus_write - called under kvm->slots_lock 
 kvm_io_bus_write_cookie - called under kvm->slots_lock 
 First try the device referenced by cookie. 
	
	  cookie contained garbage; fall back to search and return the
	  correct cookie value.
 kvm_io_bus_read - called under kvm->slots_lock 
 Caller must hold slots_lock. 
 exclude ioeventfd which is limited by maximum fd 
 Destroy the old bus _after_ installing the (null) bus. 
	
	  The debugfs files are a reference to the kvm struct which
         is still valid when kvm_destroy_vm is called.  kvm_get_kvm_safe
         avoids the race between open and the removal of the debugfs directory.
 no need for checks, since we are adding at most only 5 keys 
  kvm_get_running_vcpu - get the vcpu running on the current CPU.
  We can disable preemption locally around accessing the per-CPU variable,
  and use the resolved vcpu pointer after enabling preemption again,
  because even if the current thread is migrated to another CPU, reading
  the per-CPU value later will give us the same value as we update the
  per-CPU variable in the preempt notifier handlers.
  kvm_get_running_vcpus - get the per-CPU array of currently running vcpus.
	
	  kvm_arch_init makes sure there's at most one caller
	  for architectures that support multiple implementations,
	  like intel and amd on x86.
	  kvm_arch_init must be called before kvm_irqfd_init to avoid creating
	  conflicts in case kvm is already setup for another implementation.
 A kmem cache lets us meet the alignment requirements of fx_save. 
	
	  The init_context is allocated on the stack of the parent thread, so
	  we have to locally copy anything that is needed beyond initialization
 kthread_park(current) is never supposed to return an error 
 Wait to be woken up by the spawner before proceeding. 
 kthread_run is never supposed to return NULL 
 SPDX-License-Identifier: GPL-2.0-only
  VFIO-KVM bridge pseudo device
  Copyright (C) 2013 Red Hat, Inc.  All rights reserved.
      Author: Alex Williamson <alex.williamson@redhat.com>
  Groups can use the same or different IOMMU domains.  If the same then
  adding a new group may change the coherency of groups we've previously
  been told about.  We don't want to care about any of that so we retest
  each group and bail as soon as we find one that's noncoherent.  This
  means we only ever [un]register_noncoherent_dma once for the whole device.
 CONFIG_SPAPR_TCE_IOMMU 
 alloc by kvm_ioctl_create_device, free by .destroy 
 Only one VFIO "device" per VM 
 SPDX-License-Identifier: GPL-2.0-only
  irqchip.c: Common API for in kernel interrupt controllers
  Copyright (c) 2007, Intel Corporation.
  Copyright 2010 Red Hat, Inc. andor its affiliates.
  Copyright (c) 2013, Alexander Graf <agraf@suse.de>
  This file is derived from virtkvmirq_comm.c.
  Authors:
    Yaozu (Eddie) Dong <Eddie.dong@intel.com>
    Alexander Graf <agraf@suse.de>
  Return value:
   < 0   Interrupt was ignored (masked or not delivered for other reasons)
   = 0   Interrupt was coalesced (previous irq is still pending)
   > 0   Number of CPUs interrupt was delivered to
	 Not possible to detect if the guest uses the PIC or the
	  IOAPIC.  So set the bit in both. The guest will ignore
	  writes to the unused one.
	 Called only during vm destruction. Nobody can use the pointer
	
	  Do not allow GSI to be mapped to the same irqchip more than once.
	  Allow only one to one mapping between GSI and non-irqchip routing.
 SPDX-License-Identifier: GPL-2.0-only 
  KVM dirty ring implementation
  Copyright 2019 Red Hat, Inc.
 This is only needed to make compilers happy 
 Update the flags to reflect that this GFN is reset 
		
		  Try to coalesce the reset operations when the guest is
		  scanning pages in the same slot.
 Backwards visit, careful about overflows!  
 It should never get full 
	
	  Make sure the data is filled in before we publish this to
	  the userspace program.  There's no paired kernel-side reader.
 SPDX-License-Identifier: GPL-2.0-only
  kvm eventfd support - use eventfd objects to signal various KVM events
  Copyright 2009 Novell.  All Rights Reserved.
  Copyright 2010 Red Hat, Inc. andor its affiliates.
  Author:
 	Gregory Haskins <ghaskins@novell.com>
  Since resampler irqfds share an IRQ source ID, we de-assert once
  then notify all of the resampler irqfds using this GSI.  We can't
  do multiple de-asserts or we risk racing with incoming re-asserts.
  Race-free decouple logic (ordering is critical)
 Make sure irqfd has been initialized in assign path. 
	
	  Synchronize with the wait-queue and unhook ourselves to prevent
	  further events.
	
	  We know no new events will be scheduled at this point, so block
	  until all previously outstanding events have completed
	
	  It is now safe to release the object's resources
 assumes kvm->irqfds.lock is held 
  Mark the irqfd as inactive and schedule it for removal
  assumes kvm->irqfds.lock is held
  Called with wqh->lock held and interrupts disabled
 An event has been signaled, inject an interrupt 
 The eventfd is closing, detach from KVM 
		
		  We must check if someone deactivated the irqfd before
		  we could acquire the irqfds.lock since the item is
		  deactivated from the KVM side before it is unhooked from
		  the wait-queue.  If it is already deactivated, we can
		  simply return knowing the other side will cleanup for us.
		  We cannot race against the irqfd going away since the
		  other side is required to acquire wqh->lock, which we hold
 Must be called under irqfds.lock 
	
	  Install our own custom wake-up handling so we are notified via
	  a callback whenever someone signals the underlying eventfd
 This fd is used for another irq already. 
	
	  Check if there was an event already pending on the eventfd
	  before we registered, and trigger it as if we didn't miss it.
	
	  do not drop the file until the irqfd is fully initialized, otherwise
	  we might race against the EPOLLHUP
  shutdown any irqfd's that match fd+gsi
			
			  This clearing of irq_entry.type is needed for when
			  another thread calls kvm_irq_routing_update before
			  we flush workqueue below (we synchronize with
			  kvm_irq_routing_update using irqfds.lock).
	
	  Block until we know all outstanding shutdown jobs have completed
	  so that we guarantee there will not be any more interrupts on this
	  gsi once this deassign function returns.
  This function is called as the kvm VM fd is being released. Shutdown all
  irqfds that still remain open
	
	  Block until we know all outstanding shutdown jobs have completed
	  since we do not take a kvm reference.
  Take note of a change in irq routing.
  Caller must invoke synchronize_srcu(&kvm->irq_srcu) afterwards.
 Under irqfds.lock, so can read irq_entry safely 
  create a host-wide workqueue for issuing deferred shutdown requests
  aggregated from all vm instances. We need our own isolated
  queue to ease flushing work items when a VM exits.
  --------------------------------------------------------------------
  ioeventfd: translate a PIOMMIO memory write to an eventfd signal.
  userspace can register a PIOMMIO address with an eventfd for receiving
  notification when the memory has been touched.
  --------------------------------------------------------------------
 address must be precise for a hit 
 length = 0 means only look at the address, so always a hit 
 address-range must be precise for a hit 
 all else equal, wildcard is always a hit 
 otherwise, we have to actually compare the data 
 MMIOPIO writes trigger an event if the addrval match 
  This function is called as KVM is completely shutting down.  We do not
  need to worry about locking just nuke anything we have as quickly as possible
 assumes kvm->slots_lock held 
 The datamatch feature is optional, otherwise this is a wildcard 
 Verify that there isn't a match already 
 must be natural-word sized, or 0 to ignore length 
 check for range overflow 
 check for extra flags that we don't understand 
 ioeventfd with no length can't be combined with DATAMATCH 
	 When length is ignored, MMIO is also put on a separate bus, for
	  faster lookups.
 SPDX-License-Identifier: GPL-2.0-only
  KVM binary statistics interface implementation
  Copyright 2021 Google LLC
  kvm_stats_read() - Common function to read from the binary statistics
  file descriptor.
  @id: identification string of the stats
  @header: stats header for a vm or a vcpu
  @desc: start address of an array of stats descriptors for a vm or a vcpu
  @stats: start address of stats data block for a vm or a vcpu
  @size_stats: the size of stats data block pointed by @stats
  @user_buffer: start address of userspace buffer
  @size: requested read size from userspace
  @offset: the start position from which the content will be read for the
           corresponding vm or vcp file descriptor
  The file content of a vmvcpu file descriptor is now defined as below:
  +-------------+
  |   Header    |
  +-------------+
  |  id string  |
  +-------------+
  | Descriptors |
  +-------------+
  | Stats Data  |
  +-------------+
  Although this function allows userspace to read any amount of data (as long
  as in the limit) from any position, the typical usage would follow below
  steps:
  1. Read header from offset 0. Get the offset of descriptors and stats data
     and some other necessary information. This is a one-time work for the
     lifecycle of the corresponding vmvcpu stats fd.
  2. Read id string from its offset. This is a one-time work for the lifecycle
     of the corresponding vmvcpu stats fd.
  3. Read descriptors from its offset and discover all the stats by parsing
     descriptors. This is a one-time work for the lifecycle of the
     corresponding vmvcpu stats fd.
  4. Periodically read stats data from its offset using pread.
  Return: the number of bytes that has been successfully read
	
	  Copy kvm stats header.
	  The header is the first block of content userspace usually read out.
	  The pos is 0 and the copylen and remain would be the size of header.
	  The copy of the header would be skipped if offset is larger than the
	  size of header. That usually happens when userspace reads stats
	  descriptors and stats data.
	
	  Copy kvm stats header id string.
	  The id string is unique for every vmvcpu, which is stored in kvm
	  and kvm_vcpu structure.
	  The id string is part of the stat header from the perspective of
	  userspace, it is usually read out together with previous constant
	  header part and could be skipped for later descriptors and stats
	  data readings.
	
	  Copy kvm stats descriptors.
	  The descriptors copy would be skipped in the typical case that
	  userspace periodically read stats data, since the pos would be
	  greater than the end address of descriptors
	  (header->header.desc_offset + size_desc) causing copylen <= 0.
 Copy kvm stats values 
 SPDX-License-Identifier: GPL-2.0-only
  IRQ offloadbypass manager
  Copyright (C) 2015 Red Hat, Inc.
  Copyright (c) 2015 Linaro Ltd.
  Various virtualization hardware acceleration techniques allow bypassing or
  offloading interrupts received from devices around the host kernel.  Posted
  Interrupts on Intel VT-d systems can allow interrupts to be received
  directly by a virtual machine.  ARM IRQ Forwarding allows forwarded physical
  interrupts to be directly deactivated by the guest.  This manager allows
  interrupt producers and consumers to find each other to enable this sort of
  bypass.
 @lock must be held when calling connect 
 @lock must be held when calling disconnect 
  irq_bypass_register_producer - register IRQ bypass producer
  @producer: pointer to producer structure
  Add the provided IRQ producer to the list of producers and connect
  with any matching token found on the IRQ consumers list.
  irq_bypass_unregister_producer - unregister IRQ bypass producer
  @producer: pointer to producer structure
  Remove a previously registered IRQ producer from the list of producers
  and disconnect it from any connected IRQ consumer.
 nothing in the list anyway 
  irq_bypass_register_consumer - register IRQ bypass consumer
  @consumer: pointer to consumer structure
  Add the provided IRQ consumer to the list of consumers and connect
  with any matching token found on the IRQ producer list.
  irq_bypass_unregister_consumer - unregister IRQ bypass consumer
  @consumer: pointer to consumer structure
  Remove a previously registered IRQ consumer from the list of consumers
  and disconnect it from any connected IRQ producer.
 nothing in the list anyway 
 SPDX-License-Identifier: GPL-2.0-or-later
 System hash blacklist.
  Copyright (C) 2016 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  The description must be a type prefix, a colon and then an even number of
  hex digits.  The hash is kept in the description.
  The hash to be blacklisted is expected to be in the description.  There will
  be no payload.
  mark_hash_blacklisted - Add a hash to the system blacklist
  @hash: The hash as a hex string with a type prefix (eg. "tbs:23aa429783")
  is_hash_blacklisted - Determine if a hash is blacklisted
  @hash: The hash to be checked as a binary blob
  @hash_len: The length of the binary hash
  @type: Type of hash
  add_key_to_revocation_list - Add a revocation certificate to the blacklist
  @data: The data blob containing the certificate
  @size: The size of data blob
  is_key_on_revocation_list - Determine if the key for a PKCS#7 message is revoked
  @pkcs7: The PKCS#7 message to check
  Initialise the blacklist
  Must be initialised before we try and load the keys into the keyring.
  Load the compiled-in list of revocation X.509 certificates.
 SPDX-License-Identifier: GPL-2.0-or-later
 System trusted keyring for trusted public keys
  Copyright (C) 2012 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  restrict_link_to_builtin_trusted - Restrict keyring addition by built in CA
  Restrict the addition of keys into a keyring based on the key-to-be-added
  being vouched for by a key in the built in system keyring.
  restrict_link_by_builtin_and_secondary_trusted - Restrict keyring
    addition by both builtin and secondary keyrings
  Restrict the addition of keys into a keyring based on the key-to-be-added
  being vouched for by a key in either the built-in or the secondary system
  keyrings.
	 If we have a secondary trusted keyring, then that contains a link
	  through to the builtin keyring and the search will follow that link.
 Allow the builtin keyring to be added to the secondary 
  Allocate a struct key_restriction for the "builtin and secondary trust"
  keyring. Only for use in system_trusted_keyring_init().
  Create the trusted keyrings
  Must be initialised before we try and load the keys into the keyring.
  Load the compiled-in list of X.509 certificates.
  verify_pkcs7_message_sig - Verify a PKCS#7-based signature on system data.
  @data: The data to be verified (NULL if expecting internal data).
  @len: Size of @data.
  @pkcs7: The PKCS#7 message that is the signature.
  @trusted_keys: Trusted keys to use (NULL for builtin trusted keys only,
 					(void )1UL for all trusted keys).
  @usage: The use to which the key is being put.
  @view_content: Callback to gain access to content.
  @ctx: Context for callback.
 The data should be detached - so we need to supply it. 
  verify_pkcs7_signature - Verify a PKCS#7-based signature on system data.
  @data: The data to be verified (NULL if expecting internal data).
  @len: Size of @data.
  @raw_pkcs7: The PKCS#7 message that is the signature.
  @pkcs7_len: The size of @raw_pkcs7.
  @trusted_keys: Trusted keys to use (NULL for builtin trusted keys only,
 					(void )1UL for all trusted keys).
  @usage: The use to which the key is being put.
  @view_content: Callback to gain access to content.
  @ctx: Context for callback.
 CONFIG_SYSTEM_DATA_VERIFICATION 
 SPDX-License-Identifier: GPL-2.0
 SPDX-License-Identifier: GPL-2.0
 SPDX-License-Identifier: GPL-2.0-or-later
		 Each cert begins with an ASN.1 SEQUENCE tag and must be more
		  than 256 bytes in size.
 SPDX-License-Identifier: GPL-2.0-or-later
   Advanced Linux Sound Architecture
   Copyright (c) by Jaroslav Kysela <perex@perex.cz>
 SPDX-License-Identifier: GPL-2.0-or-later
  Linux driver model AC97 bus interface
  Author:	Nicolas Pitre
  Created:	Jan 14, 2005
  Copyright:	(C) MontaVista Software Inc.
  snd_ac97_check_id() - Reads and checks the vendor ID of the device
  @ac97: The AC97 device to check
  @id: The ID to compare to
  @id_mask: Mask that is applied to the device ID before comparing to @id
  If @id is 0 this function returns true if the read device vendor ID is
  a valid ID. If @id is non 0 this functions returns true if @id
  matches the read vendor ID. Otherwise the function returns false.
  snd_ac97_reset() - Reset AC'97 device
  @ac97: The AC'97 device to reset
  @try_warm: Try a warm reset first
  @id: Expected device vendor ID
  @id_mask: Mask that is applied to the device ID before comparing to @id
  This function resets the AC'97 device. If @try_warm is true the function
  first performs a warm reset. If the warm reset is successful the function
  returns 1. Otherwise or if @try_warm is false the function issues cold reset
  followed by a warm reset. If this is successful the function returns 0,
  otherwise a negative error code. If @id is 0 any valid device ID will be
  accepted, otherwise only the ID that matches @id and @id_mask is accepted.
  Let drivers decide whether they want to support given codec from their
  probe method. Drivers have direct access to the struct snd_ac97
  structure and may  decide based on the id field amongst other things.
 SPDX-License-Identifier: GPL-2.0-or-later
 	Sound core.  This file is composed of two parts.  sound_class
 	which is common to both OSS and ALSA and OSS sound core which
 	is used OSS or emulation of it.
  First, the common part.
 	OSS sound core handling. Breaks out sound functions to submodules
 	
 	Author:		Alan Cox <alan@lxorguk.ukuu.org.uk>
 	Fixes:
                          --------------------
 	Top level handler for the sound subsystem. Various devices can
 	plug into this. The fact they don't all go via OSS doesn't mean 
 	they don't have to implement the OSS API. There is a lot of logic
 	to keeping much of the OSS weight out of the code in a compatibility
 	module, but it's up to the driver to rember to load it...
 	The code provides a set of functions for registration of devices
 	by type. This is done rather than providing a single call so that
 	we can hide any future changes in the internals (eg when we go to
 	32bit dev_t) from the modules and their interface.
 	Secondly we need to allocate the dsp, dsp16 and audio devices as
 	one. Thus we misuse the chains a bit to simplify this.
 	Thirdly to make it more fun and for 2.3.x and above we do all
 	of this using fine grained locking.
 	FIXME: we have to resolve modules and fine grained loadunload
 	locking at some point in 2.3.x.
  By default, OSS sound_core claims full legacy minor range (0-255)
  of SOUND_MAJOR to trap open attempts to any sound minor and
  requests modules using custom sound-slotservice- module aliases.
  The only benefit of doing this is allowing use of custom module
  aliases instead of the standard char-major- ones.  This behavior
  prevents alternative OSS implementation and is scheduled to be
  removed.
  CONFIG_SOUND_OSS_CORE_PRECLAIM and soundcore.preclaim_oss kernel
  parameter are added to allow distros and developers to try and
  switch to alternative implementations without needing to rebuild
  the kernel in the meantime.  If preclaim_oss is non-zero, the
  kernel will behave the same as before.  All SOUND_MAJOR minors are
  preclaimed and the custom module aliases along with standard chrdev
  ones are emitted if a missing device is opened.  If preclaim_oss is
  zero, sound_core only grabs what's actually in use and for missing
  devices only the standard chrdev aliases are requested.
  All these clutters are scheduled to be removed along with
  sound-slotservice- module aliases.
 We must have an owner or the module locking fails 
 	Low level list operator. Scan the ordered list, find a hole and
 	join into it. Called with the lock asserted
 first free 
 Found a hole ? 
	
	 	Fill it in
	
	 	Link it
 	Remove a node from the chain. Called with the lock asserted
 	This lock guards the sound loader list.
 	Allocate the controlling structure and add it to the sound driver
 	list. Acquires locks as needed
		
		  Something else might have grabbed the minor.  If
		  first free slot is requested, rescan with @low set
		  to the next unit; otherwise, -EBUSY.
 	Remove a unit. Acquires locks as needed. The drivers MUST have
 	completed the removal before their file operations become
 	invalid.
 	Allocations
 	0	16		Mixers
 	1	8		Sequencers
 	2	16		Midi
 	3	16		DSP
 	4	16		SunDSP
 	5	16		DSP16
 	6	--		sndstat (obsolete)
 	7	16		unused
 	8	--		alternate sequencer (see above)
 	9	16		raw synthesizer access
 	10	16		unused
 	11	16		unused
 	12	16		unused
 	13	16		unused
 	14	16		unused
 	15	16		unused
 	register_sound_special_device - register a special sound node
 	@fops: File operations for the driver
 	@unit: Unit number to allocate
       @dev: device pointer
 	Allocate a special sound device by minor number from the sound
 	subsystem.
 	Return: The allocated number is returned on success. On failure,
 	a negative error code is returned.
 	register_sound_mixer - register a mixer device
 	@fops: File operations for the driver
 	@dev: Unit number to allocate
 	Allocate a mixer device. Unit is the number of the mixer requested.
 	Pass -1 to request the next free mixer unit.
 	Return: On success, the allocated number is returned. On failure,
 	a negative error code is returned.
 	DSP's are registered as a triple. Register only one and cheat
 	in open - see below.
 	register_sound_dsp - register a DSP device
 	@fops: File operations for the driver
 	@dev: Unit number to allocate
 	Allocate a DSP device. Unit is the number of the DSP requested.
 	Pass -1 to request the next free DSP unit.
 	This function allocates both the audio and dsp device entries together
 	and will always allocate them as a matching pair - eg dsp3audio3
 	Return: On success, the allocated number is returned. On failure,
 	a negative error code is returned.
 	unregister_sound_special - unregister a special sound device
 	@unit: unit number to allocate
 	Release a sound device that was allocated with
 	register_sound_special(). The unit passed is the return value from
 	the register function.
 	unregister_sound_mixer - unregister a mixer
 	@unit: unit number to allocate
 	Release a sound device that was allocated with register_sound_mixer().
 	The unit passed is the return value from the register function.
 	unregister_sound_dsp - unregister a DSP device
 	@unit: unit number to allocate
 	Release a sound device that was allocated with register_sound_dsp().
 	The unit passed is the return value from the register function.
 	Both of the allocated units are released together automatically.
 dspaudiodsp16 
		
		   Please, don't change this order or code.
		   For ALSA slot means soundcard and OSS emulation code
		   comes as add-on modules which aren't depend on
		   ALSA toplevel modules for soundcards, thus we need
		   load them at first.	  [Jaroslav Kysela <perex@jcu.cz>]
		
		  sound-slotservice- module aliases are scheduled
		  for removal in favor of the standard char-major-
		  module aliases.  For the time being, generate both
		  the legacy and standard module aliases to ease
		  transition.
		
		  We rely upon the fact that we can't be unloaded while the
		  subdriver is there.
	 We have nothing to really do here - we know the lists must be
 CONFIG_SOUND_OSS_CORE 
 SPDX-License-Identifier: GPL-2.0-only
   linuxsoundossdmasounddmasound_paula.c
   Amiga `Paula' DMA Sound Driver
   See linuxsoundossdmasounddmasound_core.c for copyright and credits
   prior to 28012001
   28012001 [0.1] Iain Sandoe
 		     - added versioning
 		     - put in and populated the hardware_afmts field.
              [0.2] - put in SNDCTL_DSP_GETCAPS value.
 	       [0.3] - put in constraint on state buffer usage.
 	       [0.4] - put in default hardsoft settings
    	The minimum period for audio depends on htotal (for OCSECSAGA)
    	(Imported from archm68kamigaamisound.c)
    	amiga_mksound() should be able to restore the period after beeping
    	(Imported from archm68kamigaamisound.c)
    	Audio DMA masks
       Helper pointers for 16(14)-bit sound
 Low level stuff 
       Heartbeat interferes with sound since the 7 kHz low-pass filter and the
       power LED are controlled by the same line.
 !CONFIG_HEARTBEAT 
 !CONFIG_HEARTBEAT 
 Mid level stuff 
 Translations 
 ++TeSche: radically changed for new expanding purposes...
  These two routines now deal with copyingexpandingtranslating the samples
  from user space into our buffer at the right frequency. They take care about
  how much data there's actually to read, how much buffer space there is and
  to convert samples into the right frequencyencoding. They will only work on
  complete samples so it may happen they leave some bytes in the input stream
  if the user didn't write a multiple of the current sample size. They both
  return the number of bytes they've used from both streams so you may detect
  such a situation. Luckily all programs should be able to cope with that.
  I think I've optimized anything as far as one can do in plain C, all
  variables should fit in registers and the loops are really short. There's
  one loop for every possible situation. Writing a more generalized and thus
  parameterized loop would only produce slower code. Feel free to optimize
  this in assembler if you like. :)
  I think these routines belong here because they're not yet really hardware
  independent, especially the fact that the Falcon can play 16bit samples
  only in stereo is hardcoded in both of them!
  ++geert: split in even more functions (one per format)
       Native format
       Copy and convert 8 bit data
       Copy and convert 16 bit data
 Low level stuff 
 turn off DMA for audio channels 
 Register interrupt handler. 
 turn off DMA for audio channels 
 release the interrupt 
 MODULE 
 turn off DMA for audio channels 
 we would need to squeeze the sound, but we won't do that 
 Amiga sound DMA supports 8bit and 16bit (pseudo 14 bit) modes 
 :-) 
	 used by AmiPlay() if all doubts whether there really is something
	  to be played are already wiped out.
 We can play pseudo 14-bit only with the maximum volume 
 we are being affected by the beeps 
 restoring volume here helps a bit 
 There's already a frame loaded 
 Increase threshold: frame 1 is already being played 
 Nothing to do 
		 hmmm, the only existing frame is not
		  yet filled and we're not syncing?
		 Playing was interrupted and sq_reset() has already cleared
		  the sq variables, so better don't do anything here.
 We've just finished a frame 
 Increase threshold: frame 1 is already being played 
 Shift the flags 
 No frame is playing, disable audio DMA 
 Try to play the next frame 
		 Nothing to play anymore.
 Mid level stuff 
  devmixer abstraction
 For pseudo 14bit 
 For pseudo 14bit 
 Machine definitions 
 MODULE 
 h'ware-supported formats only here 
 As per SNDCTL_DSP_GETCAPS 
 Config & Setup 
 SPDX-License-Identifier: GPL-2.0-only
   linuxsoundossdmasounddmasound_atari.c
   Atari TT and Falcon DMA Sound Driver
   See linuxsoundossdmasounddmasound_core.c for copyright and credits
   prior to 28012001
   28012001 [0.1] Iain Sandoe
 		     - added versioning
 		     - put in and populated the hardware_afmts field.
              [0.2] - put in SNDCTL_DSP_GETCAPS value.
   01022001 [0.3] - put in default hardsoft settings.
 ++TeSche: used for Falcon 
 Balance factor for expanding (not volume!) 
 Data for expanding 
 Translations 
 ++TeSche: radically changed for new expanding purposes...
  These two routines now deal with copyingexpandingtranslating the samples
  from user space into our buffer at the right frequency. They take care about
  how much data there's actually to read, how much buffer space there is and
  to convert samples into the right frequencyencoding. They will only work on
  complete samples so it may happen they leave some bytes in the input stream
  if the user didn't write a multiple of the current sample size. They both
  return the number of bytes they've used from both streams so you may detect
  such a situation. Luckily all programs should be able to cope with that.
  I think I've optimized anything as far as one can do in plain C, all
  variables should fit in registers and the loops are really short. There's
  one loop for every possible situation. Writing a more generalized and thus
  parameterized loop would only produce slower code. Feel free to optimize
  this in assembler if you like. :)
  I think these routines belong here because they're not yet really hardware
  independent, especially the fact that the Falcon can play 16bit samples
  only in stereo is hardcoded in both of them!
  ++geert: split in even more functions (one per format)
 Low level stuff 
 MODULE 
 Mid level stuff 
 Translations 
 this should help gcc to stuff everything into registers 
 this should help gcc to stuff everything into registers 
 this should help gcc to stuff everything into registers 
 this should help gcc to stuff everything into registers 
 this should help gcc to stuff everything into registers 
 this should help gcc to stuff everything into registers 
 this should help gcc to stuff everything into registers 
 Low level stuff 
  Atari (TTFalcon)
	 Set up timer A. Timer A
	   will receive a signal upon end of playing from the sound
	   hardware. Furthermore Timer A is able to count events
	   and will cause an interrupt after a programmed number
	   of events. So all we need to keep the music playing is
	   to provide the sound hardware with new data upon
 ++roman: Stop timer before programming! 
 Cause interrupt after first event. 
 Turn on event counting. 
 Register interrupt handler. 
 Turn interrupt on. 
 stop timer 
 turn interrupt off 
 MODULE 
  TT
 mix in PSG signal 1:1 
 search a frequency that fits into the allowed error range 
		 this isn't as much useful for a TT than for a Falcon, but
		  then it doesn't hurt very much to implement it for a TT too.
 we would need to squeeze the sound, but we won't do that 
 TT sound DMA supports only 8bit modes 
  Falcon
 stop playback, set sample rate 50kHz for PSG sound 
 STE compatible divider 
 no matrix inputs 
 no matrix outputs 
 connect ADC to DAC, disconnect matrix 
 ADC Input = PSG 
 search a frequency that fits into the allowed error range 
		 if we will tolerate 3% error 8000Hz->8195Hz (2.38%) would
		  be playable without expanding, but that now a kernel runtime
		  option
 the Falcon can play 16bit samples only in stereo 
 we would need to squeeze the sound, but we won't do that 
 Setup Falcon sound DMA for playback 
 Timer A int at play end 
 play 1 track, track 1 
 DMA(25MHz) --> DAC 
 connect matrix to DAC 
 ADC Input = Mic 
 Falcon sound DMA supports 8bit and 16bit modes 
 :-) 
 This is for the Falcon output attenuation in 1.5dB steps,
  i.e. output level from 0 to -22.5dB in -1.5dB steps.
	 used by AtaPlay() if all doubts whether there really is something
	  to be played are already wiped out.
 end might not be a legal virtual address. 
	 Since only an even number of samples per frame can
	 ++TeSche: Note that write_sq.active is no longer just a flag but
	  holds the number of frames the DMA is currently programmed for
	  instead, may be 0, 1 (currently being played) or 2 (pre-programmed).
	 
	  Changes done to write_sq.count and write_sq.active are a bit more
	  subtle again so now I must admit I also prefer disabling the irq
	  here rather than considering all possible situations. But the point
	  is that disabling the irq doesn't have any bad influence on this
	  version of the driver as we benefit from having pre-programmed the
	  DMA wherever possible: There's no need to reload the DMA at the
	  exact time of an interrupt but only at some time while the
	  pre-programmed frame is playing!
 DMA is 'full' 
 nothing to do 
		 looks like there's nothing 'in' the DMA yet, so try
		  to put two frames into it (at least one is available).
			 hmmm, the only existing frame is not
			  yet filled and we're not syncing?
 no more frames 
			 hmmm, there were two frames, but the second
			  one is not yet filled and we're not syncing?
		 there's already a frame being played so we may only stuff
		  one new into the DMA, but even if this may be the last
		  frame existing the previous one is still on write_sq.count.
			 hmmm, the only existing frame is not
			  yet filled and we're not syncing?
 ++TeSche: if you should want to test this... 
 simulate losing an interrupt 
		 ++TeSche: Falcon only: ignore first irq because it comes
		  immediately after starting a frame. after that, irqs come
		  (almost) like on the TT.
		 playing was interrupted and sq_reset() has already cleared
		  the sq variables, so better don't do anything here.
	 Probably ;) one frame is finished. Well, in fact it may be that a
	  pre-programmed one is also finished because there has been a long
	  delay in interrupt delivery and we've completely lost one, but
	  there's no way to detect such a situation. In such a case the last
	  frame will be played more than once and the situation will recover
	  as soon as the irq gets through.
	 At least one block of the queue is free now
	   so wake up a writing process blocked because
		 We must be a bit carefully here: write_sq.count indicates the
		  number of buffers used and not the number of frames to be
		  played. If write_sq.count==1 and write_sq.active==1 that
		  means the only remaining frame was already programmed
		  earlier (and is currently running) so we mustn't call
		  AtaPlay() here, otherwise we'll play one frame too much.
	 We are not playing after AtaPlay(), so there
	   is nothing to play any more. Wake up a process
 Mid level stuff 
  devmixer abstraction
 return set value 
 Machine definitions 
 MODULE 
 h'ware-supported formats only here 
 As per SNDCTL_DSP_GETCAPS 
 MODULE 
 h'ware-supported formats only here 
 As per SNDCTL_DSP_GETCAPS 
 Config & Setup 
   linuxsoundossdmasounddmasound_core.c
   OSSFree compatible Atari TTFalcon and Amiga DMA sound driver for
   Linuxm68k
   Extended to support Power Macintosh for Linuxppc by Paul Mackerras
   (c) 1995 by Michael Schlueter & Michael Marte
   Michael Schlueter (michael@duck.syd.de) did the basic structure of the VFS
   interface and the u-law to signed byte conversion.
   Michael Marte (marte@informatik.uni-muenchen.de) did the sound queue,
   devmixer, devsndstat and complemented the VFS interface. He would like
   to thank:
     - Michael Schlueter for initial ideas and documentation on the MFP and
 	the DMA sound hardware.
     - Therapy? for their CD 'Troublegum' which really made me rock.
   devsndstat is based on code by Hannu Savolainen, the author of the
   VoxWare family of drivers.
   This file is subject to the terms and conditions of the GNU General Public
   License.  See the file COPYING in the main directory of this archive
   for more details.
   History:
 	1995825	First release
 	1995902	Roman Hodek:
 			  - Fixed atari_stram_alloc() call, the timer
 			    programming and several race conditions
 	1995914	Roman Hodek:
 			  - After some discussion with Michael Schlueter,
 			    revised the interrupt disabling
 			  - Slightly speeded up U8->S8 translation by using
 			    long operations where possible
 			  - Added 4:3 interpolation for devaudio
 	1995920	Torsten Scherer:
 			  - Fixed a bug in sq_write and changed devaudio
 			    converting to play at 12517Hz instead of 6258Hz.
 	1995923	Torsten Scherer:
 			  - Changed sq_interrupt() and sq_play() to pre-program
 			    the DMA for another frame while there's still one
 			    running. This allows the IRQ response to be
 			    arbitrarily delayed and playing will still continue.
 	19951014	Guenther Kelleter, Torsten Scherer:
 			  - Better support for Falcon audio (the Falcon doesn't
 			    raise an IRQ at the end of a frame, but at the
 			    beginning instead!). uses 'if (codec_dma)' in lots
 			    of places to simply switch between Falcon and TT
 			    code.
 	19951106	Torsten Scherer:
 			  - Started introducing a hardware abstraction scheme
 			    (may perhaps also serve for Amigas?)
 			  - Can now play samples at almost all frequencies by
 			    means of a more generalized expand routine
 			  - Takes a good deal of care to cut data only at
 			    sample sizes
 			  - Buffer size is now a kernel runtime option
 			  - Implemented fsync() & several minor improvements
 			Guenther Kelleter:
 			  - Useful hints and bug fixes
 			  - Cross-checked it for Falcons
 	199639	Geert Uytterhoeven:
 			  - Support added for Amiga, A-law, 16-bit little
 			    endian.
 			  - Unification to driverssounddmasound.c.
 	199646	Martin Mitchell:
 			  - Updated to 1.3 kernel.
 	1996613       Topi Kanerva:
 			  - Fixed things that were broken (mainly the amiga
 			    14-bit routines)
 			  - devsndstat shows now the real hardware frequency
 			  - The lowpass filter is disabled by default now
 	1996925	Geert Uytterhoeven:
 			  - Modularization
 	1998610	Andreas Schwab:
 			  - Converted to use sound_core
 	19991228	Richard Zidlicky:
 			  - Added support for Q40
 	2000227	Geert Uytterhoeven:
 			  - Clean up and split the code into 4 parts:
 			      o dmasound_core: machine-independent code
 			      o dmasound_atari: Atari TT and Falcon support
 			      o dmasound_awacs: Apple PowerMac support
 			      o dmasound_paula: Amiga support
 	2000325	Geert Uytterhoeven:
 			  - Integration of dmasound_q40
 			  - Small clean ups
 	20010126 [1.0] Iain Sandoe
 			  - make devsndstat show revision & edition info.
 			  - since dmasound.mach.sq_setup() can fail on pmac
 			    its type has been changed to int and the returns
 			    are checked.
 		   [1.1]  - stop missing translations from being called.
 	20010208 [1.2]  - remove unused translation tables & move machine-
 			    specific tables to low-level.
 			  - return correct info. for SNDCTL_DSP_GETFMTS.
 		   [1.3]  - implement SNDCTL_DSP_GETCAPS fully.
 		   [1.4]  - make devsndstat text length usage deterministic.
 			  - make devsndstat call to low-level
 			    dmasound.mach.state_info() pass max space to ll driver.
 			  - tidy startup banners and output info.
 		   [1.5]  - tidy up a little (removed some unused #defines in
 			    dmasound.h)
 			  - fix up HAS_RECORD conditionalisation.
 			  - add record code in places it is missing...
 			  - change buf-sizes to bytes to allow < 1kb for pmac
 			    if user param entry is < 256 the value is taken to
 			    be in kb > 256 is taken to be in bytes.
 			  - make default bufffrag params conditional on
 			    machine to allow smaller values for pmac.
 			  - made the ioctls, read & write comply with the OSS
 			    rules on setting params.
 			  - added parsing of _setup() params for record.
 	20010404 [1.6]  - fix bug where sample rates higher than maximum were
 			    being reported as OK.
 			  - fix open() to return -EBUSY as per OSS doc. when
 			    audio is in use - this is independent of O_NOBLOCK.
 			  - fix bug where SNDCTL_DSP_POST was blocking.
  Record capability notes 30012001:
   At present these observations apply only to pmac LL driver (the only one
   that can do record, at present).  However, if other LL drivers for machines
   with record are added they may apply.
   The fragment parameters for the record and play channels are separate.
   However, if the driver is opened O_RDWR there is no way (in the current OSS
   API) to specify their values independently for the record and playback
   channels.  Since the only common factor between the input & output is the
   sample rate (on pmac) it should be possible to open devdspX O_WRONLY and
   devdspY O_RDONLY.  The input & output channels could then have different
   characteristics (other than the first that sets sample rate claiming the
   right to set it for ever).  As it stands, the format, channels, number of
   bits & sample rate are assumed to be common.  In the future perhaps these
   should be the responsibility of the LL driver - and then if a card really
   does not share items between record & playback they can be specified
   separately.
 Thread-safeness of shared_resources notes: 31012001
  If the user opens O_RDWR and then splits record & play between two threads
  both of which inherit the fd - and then starts changing things from both
  - we will have difficulty telling.
  It's bad application coding - but ...
  TODO: think about how to sort this out... without bogging everything down in
  semaphores.
  Similarly, the OSS spec says "all changes to parameters must be between
  open() and the first read() or write(). - and a bit later on (by
  implication) "between SNDCTL_DSP_RESET and the first read() or write() after
  it".  If the app is multi-threaded and this rule is broken between threads
  we will have trouble spotting it - and the fault will be rather obscure :-(
  We will try and put out at least a kmsg if we see it happen... but I think
  it will be quite hard to trap it with an -EXXX return... because we can't
  see the fault until after the damage is done.
       Declarations
 in bytes 
 MODULE 
 control over who can modify resources shared between playrecord 
       Mid level stuff
 _MUST_ stop DMA 
	 trap out-of-range speed settings.
	   at present we allow (arbitrarily) low rates - using soft
	   up-conversion - but we can't allow > max because there is
	   no soft down-conversion.
 should be 0 or 1 now 
	 if the user has requested a non-existent translation don't try
	   to call it but just return 0 bytes moved
       devmixer abstraction
       Sound queue stuff, the heart of the driver
 are we already set? - and not changeable 
 don't think we have a race prob. here _check_ 
	 make sure that the parameters are set up
	   This should have been done already...
	 OK.  If the user has set fragment parameters explicitly, then we
	   should leave them alone... as long as they are valid.
	   Invalid user fragment params can occur if we allow the whole buffer
	   to be used when the user requests the fragments sizes (with no soft
	   x-lation) and then the user subsequently sets a soft x-lation that
	   requires increased internal buffering.
	   Othwerwise (if the user did not set them) OSS says that we should
	   select frag params on the basis of 0.5 s output & 0.1 s input
	   latency. (TODO.  For now we will copy in the defaults.)
 set up the user info 
 work out requested block size 
 the user wants to write frag-size chunks 
 this only works for size values which are powers of 2 
 make sure we are aligned 
 let's just check for obvious mistakes 
 if user has set max_active - then use it 
	 ++TeSche: Is something like this necessary?
	  Hey, that's an honest question! Or does any other part of the
	  filesystem already checks this situation? I really don't know.
	 implement any changes we have made to the softhard params.
	   this is not satisfactory really, all we have done up to now is to
	   say what we would like - there hasn't been any real checking of capability
	 set up the sq if it is not already done. This may seem a dumb place
	   to do it - but it is what OSS requires.  It means that write() can
	   return memory allocation errors.  To avoid this possibility use the
	   GETBLKSIZE or GETOSPACE ioctls (after you've fiddled with all the
	   params you want to change) - these ioctls also force the setup.
 FIXME: I think that this may be the wrong behaviour when we get strapped
	for time and the cpu is close to being (or actually) behind in sending data.
	- because we've lost the time that the N samples, already in the buffer,
	would have given us to get here with the next lot from the user.
	 The interrupt doesn't start to play the last, incomplete frame.
	  Thus we can append to it without disabling the interrupts! (Note
	  also that write_sq.rear isn't affected by the interrupt.)
	 as of 1.6 this behaviour changes if SNDCTL_DSP_POST has been issued:
	   this will mimic the behaviour of syncing and allow the sq_play() to
	   queue a partial fragment.  Since sq_play() maywill be called from
	   the IRQ handler - at least on Pmac we have to deal with it.
	   The strategy - possibly not optimum - is to kill _POST status if we
	   get here.  This seems, at least, reasonable - in the sense that POST
	   is supposed to indicate that we might not write before the queue
	   is drained - and if we get here in time then it does not apply.
 take out POST status 
 paranoia 
		 Here, we can avoid disabling the interrupt by first
		  copying and translating the data, and then updating
		  the write_sq variables. Until this is done, the interrupt
		  won't see the new frame and we can work on it
		  undisturbed.
 paranoia 
 uUsed may have been 0 
 blocking open() 
 CHECK: IS THIS OK??? 
 blocking open() 
			 OSS manual says we will return EBUSY regardless
			   of O_NOBLOCK.
 Let's play spot-the-race-condition 
		 allocate the default number & size of buffers.
		   (i.e. specified in _setup() or as module params)
		   can't be changed at the moment - but _could_ be perhaps
		   in the setfragments ioctl.
 blocking open() 
 blocking open() 
 checks the f_mode 
 TODO: if O_RDWR, release any resources grabbed by write part 
 I think this is what is required by open(2) 
	 CHECK whether this is sensible - in the case that dsp0 could be opened
	  O_RDONLY and dsp1 could be opened O_WRONLY
	 OK. - we should make some attempt at consistency. At least the H'ware
	   options should be set with a valid mode.  We will make it that the LL
	   driver must supply defaults for hard & soft params.
		 you can make this AFMT_U8mono8K if you want to mimic old
	 none of the current LL drivers can actually do this "native" at the moment
	   OSS does not really require us to supply devaudio if we can't do it.
 this _must_ stop DMA, we might be about to lose the buffers 
 write_sq.front = (write_sq.rear+1) % write_sq.max_count;
 same as for set-up 
 OK - we can unlock the parameters and fragment settings 
	 we could consider resetting the shared_resources_owner here... but I
	   think it is probably still rather non-obvious to application writer
 we release everything else though 
 there may be an incomplete frame waiting 
			 While waiting for audio output to drain, an
			  interrupt occurred.  Stop audio output immediately
 flag no sync regardless of whether we had a DSP_POST or not 
 make sure dma is stopped and all is quiet 
 it's us that has them 
 blocking open() 
	 Wake up a process waiting for the queue being released.
	  Note: There may be several processes waiting for a call
 Iain: hmm I don't understand this next comment ... 
 There is probably a DOS atack here. They change the mode flag. 
 XXX add check here,
 checks f_mode 
 checks f_mode 
 blocking open() 
 here we see if we have a right to modify format, channels, size and so on
   if no-one else has claimed it already then we do...
   TODO: We might change this to mask O_RDWR such that only one or the other channel
   is the owner - if we have problems.
 if either queue is locked we must deny the right to change shared params
 check and set a queue's fragments per user's wishes...
   we will check against the pre-defined literals and the actual sizes.
   This is a bit fraught - because soft translations can mess with our
   buffer requirements after this call - OSS says "call setfrags first"
 It is possible to replace all the -EINVAL returns with an override that
   just puts the allowable value in.  This may be what many OSS apps require
 now in bytes 
 this might still not work 
 the user is allowed say "don't care" with 0x7fff 
	 there is, currently, no way to specify max_active separately
	   from max_count.  This could be a LL driver issue - I guess
	   if there is a requirement for these values to be different then
	  we will have to pass that info. up to this level.
 this is what OSS says.. 
		 this should tell the caller about bytes that the app can
		   readwrite - the app doesn't care about our internal buffers.
		   We force sq_setup() here as per OSS 1.1 (which should
		   compute the values necessary).
		   Since there is no mechanism to specify readwrite separately, for
		   fds opened O_RDWR, the write_sq values will, arbitrarily, overwrite
		   the read_sq ones.
		 all we are going to do is to tell the LL that any
		   partial frags can be queued for output.
		   The LL will have to clear this flag when last output
		   is queued.
		 This call, effectively, has the same behaviour as SNDCTL_DSP_RESET
		   except that it waits for output to finish before resetting
		   everything - read, however, is killed immediately.
 if we are the shared resource owner then release them 
		 changing this on the fly will have weird effects on the sound.
		   Where there are rate conversions implemented in soft form - it
		   will cause the _ctx_xxx() functions to be substituted.
		   However, there doesn't appear to be any reason to dis-allow it from
		   a driver pov.
	 OSS says these next 4 actions are undefined when the device is
	   busyactive - we will just return -EINVAL.
	   To be allowed to change one - (a) you have to own the right
	    (b) the queue(s) must be quiescent
 the user might ask for 20 channels, we will return 1 or 2 
		 we can do this independently for the two queues - with the
		   proviso that for fds opened O_RDWR we cannot separate the
		   actions and both queues will be set per the last call.
		   NOTE: this does NOT actually set the queue up - merely
		   registers our intentions.
 0x7fff is 'use maximum' 
		 NOTE: this return value is irrelevant - OSS specifically says that
		   the value is 'random' and that the user _must_ check the actual
		
	 These parameters will be restored for every clean open()
	  in the case of multiple open()s (e.g. dsp0 & dsp1) they
	  will be set so long as the shared resources have no owner.
       devsndstat
 we allow more space for record-enabled because there are extra output lines.
   the number here must include the amount we are prepared to give to the low-level
   driver.
 this is how much space we will allow the low-level driver to use
   in the stat buffer.  Currently, 2  (80 character line + <NL>).
   We do not police this (it is up to the ll driver to be honest).
 state.buf should not overflow! 
 publish this function for use by low-level code, if required 
	 call the low-level module to fill in any stat info. that it has
	   if present.  Maximum buffer usage is specified.
	 make usage of the state buffer as deterministic as poss.
	   exceptional conditions could cause overrun - and this is flagged as
	   a kernel error.
 formats and settings 
 sound queue status 
       Config & Setup
       This function is called by _one_ chipset-specific driver
 Set up sound queue, devaudio and devdsp. 
 Set default settings. 
 Set up devsndstat. 
 Set up devmixer. 
 !MODULE 
 check the bootstrap parameter for "dmasound=" 
	 FIXME: other than in the most naive of cases there is no sense in these
	 	  buffers being other than powers of two.  This is not checked yet.
 check for small buffer specs 
 !MODULE 
       Conversion tables
 8 bit mu-law 
 8 bit A-law 
 HAS_8BIT_TABLES 
       Visible symbols for modules
 SPDX-License-Identifier: GPL-2.0-only
   linuxsoundossdmasounddmasound_q40.c
   Q40 DMA Sound Driver
   See linuxsoundossdmasounddmasound_core.c for copyright and credits
   prior to 28012001
   28012001 [0.1] Iain Sandoe
 		     - added versioning
 		     - put in and populated the hardware_afmts field.
              [0.2] - put in SNDCTL_DSP_GETCAPS value.
 	       [0.3] - put in default hardsoft settings.
 Balance factor for expanding (not volume!) 
 Data for expanding 
 Low level stuff 
 Mid level stuff 
 userCount, frameUsed, frameLeft == byte counts 
 a bit too complicated to optimise right now ..
 compressing versions 
 Low level stuff 
 change to vmalloc 
 Register interrupt handler. 
 MODULE 
	 used by Q40Play() if all doubts whether there really is something
	  to be played are already wiped out.
 There's already a frame loaded 
 nothing in the queue 
	          hmmm, the only existing frame is not
		   yet filled and we're not syncing?
	           playing was interrupted and sq_reset() has already cleared
		    the sq variables, so better don't do anything here.
 better safe 
 there was nothing to play, disable irq 
 search a frequency that fits into the allowed error range 
sound.hard.stereo=1; 
 squeeze the sound, we do that 
 Q40 sound supports only 8bit modes 
 Machine definitions 
 MODULE 
 h'ware-supported formats only here 
 As per SNDCTL_DSP_GETCAPS 
 Config & Setup 
 SPDX-License-Identifier: GPL-2.0-or-later
  Driver for Sound Core PDAudioCF soundcard
  Copyright (c) 2003 by Jaroslav Kysela <perex@perex.cz>
 IRQ_NONE here? 
 should never happen 
 check rate 
 printk(KERN_DEBUG "TASKLET: rdp = %x, wdp = %x\n", rdp, wdp); 
 SPDX-License-Identifier: GPL-2.0-or-later
  Driver for Sound Core PDAudioCF soundcard
  Copyright (c) 2003 by Jaroslav Kysela <perex@perex.cz>
 Index 0-MAX 
 ID for this card 
 Enable switches 
  prototypes
  destructor
  snd_pdacf_attach - attach callback for cs
 find an empty slot from the card list 
 disabled explicitly 
 ok, create a card instance 
  snd_pdacf_assign_resources - initialize the hardware and card instance.
  @pdacf: context
  @port: io port for the card
  @irq: irq number for the card
  this function assigns the specified port and irq, boot the card,
  create pcm and control instances, and initialize the rest hardware.
  returns 0 if successful, or a negative error code.
  snd_pdacf_detach - detach callback for cs
 to be sure 
  configuration callback
  Module entry points
 this is too general PCMCIA_DEVICE_MANF_CARD(0x015d, 0x4c45), 
 SPDX-License-Identifier: GPL-2.0-or-later
  Driver for Sound Core PDAudioCF soundcard
  Copyright (c) 2003 by Jaroslav Kysela <perex@perex.cz>
 for sure 
 design note: if we unmask PLL unlock, parity, valid, audio or auto bit interrupts 
 from AK4117 then INT1 pin from AK4117 will be high all time, because PCMCIA interrupts are 
 egde based and FPGA does logical OR for all interrupt sources, we cannot use these 
 high-rate sources 
 AK4117_REG_PWRDN 
 AK4117_REQ_CLOCK 
 AK4117_REG_IO 
 AK4117_REG_INT0_MASK 
 AK4117_REG_INT1_MASK 
 normal operation 
 debug 
 setup the FPGA to match AK4117 setup 
 use 24.576Mhz clock 
 24-bit data 
 setup LEDs and IRQ 
 update LED status 
 disable interrupts, but use direct write to preserve old register value in chip->regmap 
 disable interrupts, but use direct write to preserve old register value in chip->regmap 
 ignore interrupts from now 
 wait for AK4117's PLL 
 SPDX-License-Identifier: GPL-2.0-or-later
  Driver for Sound Core PDAudioCF soundcards
  PCM part
  Copyright (c) 2003 by Jaroslav Kysela <perex@perex.cz>
  clear the SRAM contents
  pdacf_pcm_trigger - trigger callback for capture
  pdacf_pcm_prepare - prepare callback for playback and capture
 24-bit 
 24-bit 
  capture hw information
  pdacf_pcm_capture_open - open callback for capture
  pdacf_pcm_capture_close - close callback for capture
  pdacf_pcm_capture_pointer - pointer callback for capture
  operators for PCM capture
  snd_pdacf_pcm_new - create and initialize a pcm
 SPDX-License-Identifier: GPL-2.0-or-later
  Driver for Digigram VXpocket soundcards
  lowlevel routines for VXpocket soundcards
  Copyright (c) 2002 by Takashi Iwai <tiwai@suse.de>
 ICR
 CVR
 ISR
 IVR
 RXH
 RXM
 RXL
 DMA
 CDSP
 LFREQ
 HFREQ
 DATA
 MICRO
 DIALOG
 CSUER
 RUER
  snd_vx_inb - read a byte from the register
  @offset: register offset
  snd_vx_outb - write a byte on the register
  @offset: the register offset
  @val: the value to write
  redefine macros to call directly
  vx_check_magic - check the magic word on xilinx
  returns zero if a magic word is detected, or a negative error code.
  vx_reset_dsp - reset the DSP
 ms 
 set the reset dsp bit to 1 
 reset the bit 
  reset codec bit
 Set the reset CODEC bit to 1. 
 Set the reset CODEC bit to 0. 
  vx_load_xilinx_binary - load the xilinx binary image
  the binary image is the binary array converted from the bitstream file.
 Switch to programmation mode 
 Save register CSUER and RUER 
 reset HF0 and HF1 
 Wait for answer HF2 equal to 1 
 set HF1 for loading xilinx binary 
 wait for reading 
 reset HF1 
 wait for HF3 
 read the number of bytes received 
 TEMPO 250ms : wait until Xilinx is downloaded 
 test magical word 
 Restore register 0x0E and 0x0F (thus replacing COR and FCSR) 
 Reset the Xilinx's signal enabling IO access 
 Reset of the Codec 
  vxp_load_dsp - load_dsp callback
 xilinx boot 
 xilinx image 
 DSP boot 
 DSP image 
  vx_test_and_ack - test and acknowledge interrupt
  called from irq hander, too
  spinlock held!
 not booted yet? 
 ok, interrupts generated, now ack it 
 set ACQUIT bit up and down 
	 useless read just to spend some time and maintain
	  the ACQUIT signal up for a while ( a bus cycle )
  vx_validate_irq - enabledisable IRQ
 Set the interrupt enable bit to 1 in CDSP register 
  vx_setup_pseudo_dma - set up the pseudo dma readwrite mode.
  @do_write: 0 = read, 1 = set up for DMA write
 Interrupt mode and HREQ pin enabled for host transmit  receive data transfers 
 Reset the pseudo-dma register 
 Select DMA in readwrite transfer mode and in 16-bit accesses 
  vx_release_pseudo_dma - disable the pseudo-DMA mode
 Disable DMA and 16-bit accesses 
 HREQ pin disabled. 
  vx_pseudo_dma_write - write bulk data on pseudo-DMA mode
  @count: data length to transfer in bytes
  data size must be aligned to 6 bytes to ensure the 24bit alignment on DSP.
  NB: call with a certain lock!
 in 16bit words 
 Transfer using pseudo-dma. 
 in 16bit words 
 Transfer using pseudo-dma. 
  vx_pseudo_dma_read - read bulk data on pseudo DMA mode
  @offset: buffer offset in bytes
  @count: data length to transfer in bytes
  the read length must be aligned to 6 bytes, as well as write.
  NB: call with a certain lock!
 in 16bit words 
 Transfer using pseudo-dma. 
 in 16bit words 
 Transfer using pseudo-dma. 
 Disable DMA 
 Read the last word (16 bits) 
 Disable 16-bit accesses 
 HREQ pin disabled. 
  write a codec data (24bit)
 Activate access to the corresponding codec register 
 We have to send 24 bits (3 x 8 bits). Start with most signif. Bit 
 Terminate access to codec registers 
  vx_set_mic_boost - set mic boost level (on vxp440 only)
  @boost: 0 = 20dB, 1 = +38dB
 boost: 38 dB 
 minimum value: 20 dB 
  remap the linear value (0-8) to the actual value (0-15)
  vx_set_mic_level - set mic level (on vxpocket only)
  @level: the mic level = 0 - 8 (max)
  change the input audio source
 reset mic levels 
  change the clock source
  source = INTERNAL_QUARTZ or UER_SYNC
  reset the board
  callbacks
 exported 
 SPDX-License-Identifier: GPL-2.0-or-later
  Driver for Digigram VXpocket V2440 soundcards
  Copyright (c) 2002 by Takashi Iwai <tiwai@suse.de>
 Index 0-MAX 
 ID for this card 
 Enable switches 
  Hardware information
 VX-pocket V2
  1 DSP, 1 sync UER
  1 programmable clock (NIY)
  1 stereo analog input (linemicro)
  1 stereo analog output
  Only output levels can be modified
 hardware specs 
 VX-pocket 440
  1 DSP, 1 sync UER, 1 sync World Clock (NIY)
  SMPTE (NIY)
  2 stereo analog input (linemicro)
  2 stereo analog output
  Only output levels can be modified
  UER, but only for the first two inputs and outputs.
 hardware specs 
  create vxpocket instance
  snd_vxpocket_assign_resources - initialize the hardware and card instance.
  @chip: VX core instance
  @port: io port for the card
  @irq: irq number for the card
  this function assigns the specified port and irq, boot the card,
  create pcm and control instances, and initialize the rest hardware.
  returns 0 if successful, or a negative error code.
  configuration callback
 redefine hardware record according to the VERSION1 string 
 overwrite the hardware information 
struct snd_vxpocket vxp = (struct snd_vxpocket )chip;
 find an empty slot from the card list 
 disabled explicitly 
 ok, create a card instance 
 to be sure 
  Module entry points
 SPDX-License-Identifier: GPL-2.0-or-later
  Driver for Digigram VXpocket soundcards
  VX-pocket mixer
  Copyright (c) 2002 by Takashi Iwai <tiwai@suse.de>
  mic level control (for VXPocket)
  mic boost level control (for VXP440)
 mute input levels 
 mic level 
 SPDX-License-Identifier: GPL-2.0-only
 Copyright Adrian McMenamin 2005, 2006, 2007
 <adrian@mcmen.demon.co.uk>
 Requires firmware (BSD licenced) available from:
 http:linuxdc.cvs.sourceforge.netlinuxdclinux-sh-dcsoundossaicafirmware
 or the maintainer
 module parameters 
 Simple platform device 
 SPU specific functions 
 spu_write_wait - wait for G2-SH FIFO to clear 
 To ensure hardware failure doesn't wedge kernel 
 spu_memset - write to memory in SPU address space 
 spu_memload - write to SPU address space 
 spu_disable - set spu registers to stop sound output 
 spu_enable - set spu registers to enable sound output 
  Halt the sound processor, clear the memory,
  load some default ARM7 code, and then restart ARM7
 Put ARM7 in endless loop 
 aica_chn_start - write to spu to start playback 
 aica_chn_halt - write to spu to halt playback 
 ALSA code below 
timer function - so cannot sleep 
 Have we played out an additional period? 
 reschedule the timer 
get the queue to do the work 
 set defaults for channel 
 default to mono 
 TO DO: set up to handle more than one pcm instance 
 AICA has no capture ability 
 Allocate the DMA buffers 
 Mixer controls 
 TO DO: Fix me 
 TO DO: Fix me 
 we've not yet been set up 
 write firmware into memory 
 Prepare to use the queue 
 Load the PCM 'chip' 
 Add basic controls 
 Register the card with ALSA subsystem 
 Load the firmware 
 Kill any sound still playing and reset ARM7 to safe state 
 SPDX-License-Identifier: GPL-2.0-or-later
  sh_dac_audio.c - SuperH DAC audio driver for ALSA
  Copyright (c) 2009 by Rafael Ignacio Zurita <rizurita@yahoo.com>
  Based on sh_dac_audio.c (Copyright (C) 2004, 2005 by Andriy Skulysh)
 Module Parameters 
 main struct 
 bytes proccesed, to compare with period_size 
 PCM INTERFACE 
 channel is not used (interleaved data) 
 channel is not used (interleaved data) 
 channel is not used (interleaved data) 
 pcm ops 
 device should be always 0 for us 
 buffer size=48K 
 END OF PCM INTERFACE 
 driver .remove  --  destructor 
 free -- it has been defined by create 
 release the data 
 create  --  chip-specific constructor for the cards components 
 driver .probe  --  constructor 
  "driver" definition
 SPDX-License-Identifier: GPL-2.0-or-later
  Driver for PowerMac AWACS
  Copyright (c) 2001 by Takashi Iwai <tiwai@suse.de>
    based on dmasound.c.
 Index 0-MAX 
 ID for this card 
 SPDX-License-Identifier: GPL-2.0-or-later
  Beep using pcm
  Copyright (c) by Takashi Iwai <tiwai@suse.de>
 boolean 
 mixer volume: 0-100 
 currently playing volume 
 allocated wave buffer 
 physical address of buffer 
  stop beep if running
  Stuff for outputting a beep.  The values range from -327 to +327
  so we can multiply by an amplitude in the range 0..100 to get a
  signed short value to put in the output buffer.
 22050 Hz sample rate 
 0 - 100 
 fixed point 
  beep volume mixer
 Initialize beep stuff 
 FIXME: set more better values 
 SPDX-License-Identifier: GPL-2.0-or-later
  PMac TumblerSnapper lowlevel functions
  Copyright (c) by Takashi Iwai <tiwai@suse.de>
    Rene Rebe <rene.rebe@gmx.net>:
       update from shadow registers on wakeup and headphone plug
       automatically toggle DRC on headphone plug
 i2c address for tumbler 
 registers 
 main control 
 tas3001c 
 tas3004 
 main control 2 
 analog control 
 mono volumes for tas3001ctas3004 
 tas3001c only 
 stereo volumes for tas3004 
 stereo volumes for tas3004 
 normal operation, SCLK=64fps, i2s output, i2s input, 16bit width 
 terminator 
 normal operation, SCLK=64fps, i2s output, 16bit width 
 normal operation, all-pass mode 
 normal output, no deemphasis, A input, power-up, line-in 
 terminator 
  gpio access
 NOP 
  update master volume
 output volume 
 output switch 
  TAS3001c dynamic range compression
 enable, 3:1 compression 
  TAS3004
 3:1 above threshold 
 disabled 
 1:1 below threshold 
  mono volumes
 TAS3001c mono volumes 
 TAS3004 mono volumes 
  snapper mixer volumes
  mute switches. FIXME: Turn that into software mute when both outputs are muted
  to avoid codec reset on ibook M7
 don't touch in the auto-mute mode 
 Alternative PCM is assigned to Mic analog loopback on iBook G4 
 FIXME: "Capture Source" doesn't work properly 
  auto-mute stuffs
 unmute headphonelineout & mute speaker 
 unmute speaker, mute others 
 reset the master volume so the correct amplification is applied 
 PMAC_SUPPORT_AUTOMUTE 
 interrupt - headphone plug changed 
 look for audio-gpio device 
 look for audio-gpio device 
 find an audio device and get its address 
 Try to find the active state, default to 0 ! 
		 Here are some crude hacks to extract the GPIO polarity and
		  open collector informations out of the do-platform script
		  as we don't yet have an interpreter for these things
 reset audio 
 suspend mixer 
 resume mixer 
 activate headphone status interrupts 
 initialize tumbler 
 exported 
 set up TAS 
	
	  build mixers
 set initial DRC range to 60% 
 will be changed later if AUTO_DRC is set 
 update the status only 
 activate headphone status interrupts 
 activate headphone status interrupts 
 activate headphone status interrupts 
 SPDX-License-Identifier: GPL-2.0-or-later
  PMac AWACS lowlevel functions
  Copyright (c) by Takashi Iwai <tiwai@suse.de>
  code based on dmasound.c.
 PMAC_AMP_AVAIL 
  write AWACS register
 Recalibrate chip 
	 Sorry for the horrible delays... I hope to get that improved
	  by making the whole PM process asynchronous in a future version
 delay for broken crystal part 
 NOP 
  additional callback to set the pcm format
  AWACS volume callbacks
  volumes: 0-15 stereo
  mute masterogain for AWACS: mono
  controls for perchwhisper extension cards, e.g. G3 desktop
  TDA7433 connected via i2c address 0x45 (= 0x8a),
  accessed through cuda
  level = 0 - 14, 7 = 0 dB
  vol = 0 - 31 (attenuation), 32 = mute bit, stereo
  0 = -79 dB, 79 = 0 dB, 99 = +20 dB
  mixer controls
 PMAC_AMP_AVAIL 
  mic boost for screamer
  lists of mixer elements
	AWACS_SWITCH("Unknown Playback Switch", 6, SHIFT_PAROUT0, 0), 
 FIXME: is this correct order?
  screamer (powerbook G3 pismo) seems to have different bits...
  add new mixer elements to the card
  restore all registers
 reset power bits in reg 6 
 CONFIG_PM 
  auto-mute stuffs
 PMAC_SUPPORT_AUTOMUTE 
  initialize chip
	 looks like MASK_GAINLINE triggers something, so we set here
	  as start-up
 FIXME: Only machines with external SRS module need MASK_PAROUT 
 chip->_device_id == 0x8 || 
 get default volume from nvram 
 vol = (~nvram_read_byte(0x1308) & 7) << 1;
 vol = ((pmac_xpram_read( 8 ) & 7 ) << 1 );
 no, on alsa, muted as default 
 FIXME: screamer has loopthru vol control 
 FIXME: maybe should be vol << 3 for PCMCIA speaker 
 mute and zero vol 
 0 dB 
 0 dB 
 PMAC_AMP_AVAIL 
 set headphone-jack detection bit 
	
	  build mixers
		 use amplifier.  the signal is connected from route A
		  to the amp.  the amp has its headphone and speaker
		  volumes and mute switches, so we use them instead of
		  screamer registers.
		  in this case, it seems the route C is not used.
 overwrite 
 PMAC_AMP_AVAIL 
 route A = headphone, route C = speaker 
	
	  set lowlevel callbacks
 update the status only 
 SPDX-License-Identifier: GPL-2.0-or-later
  PMac DBDMA lowlevel functions
  Copyright (c) by Takashi Iwai <tiwai@suse.de>
  code based on dmasound.c.
 fixed frequency table for awacs, screamer, burgundy, DACA (44100 max) 
 fixed frequency table for tumbler 
  we will allocate a single 'emergency' dbdma cmd block to use if the
  tx status comes up "DEAD".  This happens on some PowerComputing Pmac
  clones, either owing to a bug in dbdma or some interaction between
  IDE and sound.  However, this measure would deal with DEAD status if
  it appeared elsewhere.
  allocate DBDMA command arrays
  pcm stuff
  look up frequency table
  check whether another stream is active
  get a stream of the opposite direction
  wait while run status is on
  set the format and rate to the chip.
  call the lowlevel function if defined (e.g. for AWACS).
 set up frequency and format 
  stop the DMA transfer
  set the command pointer address
  start the DMA
  prepare playbackcapture stream
 set up constraints 
	 We really want to execute a DMA stop command, after the AWACS
	  is initialized.
	  For reasons I don't understand, it stops the hissing noise
	  common to many PowerBook G3 systems and random noise otherwise
	  captured on iBook2's about every third time. -ReneR
	 continuous DMA memory type doesn't provide the physical address,
	  so we need to resolve the address here...
cp->res_count = cpu_to_le16(0);
 make loop 
  PCM triggerstop
printk(KERN_DEBUG "stopped!!\n");
  return the current pointer
 hmm.. how can we get the current dma pointer?? 
printk(KERN_DEBUG "pointer=%d\n", count);
  playback
  capture
  Handle DEAD DMA transfers:
  if the TX status comes up "DEAD" - reported on some Power Computing machines
  we need to re-start the dbdma - but from a different physical start address
  and with a different transfer length.  It would get very messy to do this
  with the normal dbdma_cmd blocks - we would have to re-write the buffer start
  addresses each time.  So, we will keep a single dbdma_cmd block which can be
  fiddled with.
  When DEAD status is first reported the content of the faulted dbdma block is
  copied into the emergency buffer and we note that the buffer is in use.
  we then bump the start physical address by the amount that was successfully
  output before it died.
  On any subsequent DEAD result we just do the bump-ups (we know that we are
  already using the emergency dbdma_cmd).
  CHECK: this just tries to "do it".  It is possible that we should abandon
  xfers when the number of residual bytes gets below a certain value - I can
  see that this might cause a loop-forever if a too small transfer causes
  DEAD status.  However this is a TODO for now - we'll see what gets reported.
  When we get a successful transfer result with the emergency buffer we just
  pretend that it completed using the original dmdma_cmd and carry on.  The
  'next_cmd' field will already point back to the original loop of blocks.
 printk(KERN_WARNING "snd-powermac: DMA died - patching it up!\n"); 
	 to clear DEAD status we must first clear RUN
 new problem 
	 now bump the values to reflect the amount
 point at our patched up command block 
 we must re-start the controller 
 should complete clearing the DEAD status 
  update playbackcapture pointer from interrupts
 at most all fragments 
 already using DEAD xfer? 
 this block is still going 
 done that 
printk(KERN_DEBUG "update frag %d\n", rec->cur_period);
cp->res_count = cpu_to_le16(0);
  hw info
 NYI
 NYI
 look up frequency table and fill bit mask 
 check for minimum and maximum rates 
 FIXME: still under development.. 
 constraints to fix choppy sound 
 reset constraints 
 preallocate 64k buffer 
  handling beep
 reset format 
  interrupt handlers
printk(KERN_DEBUG "pmac: control interrupt.. 0x%x\n", ctrl);
 do something when headphone is pluggedunplugged? 
 Writing 1s to the CNTLERR and PORTCHG bits clears them... 
  a wrapper to feature call for compatibility
  release resources
 stop sounds 
 disable interrupts from awacs interface 
 clean up mixer if any 
 release resources 
  free the device
  check the machine support byteswap (little-endian)
 if seems that Keylargo can't byte-swap  
 it seems the Pismo & iBook can't byte-swap in hardware. 
  detect a sound chip
 all ok 
 default 
 check machine type 
	
	  powermac G3 models have a node called "davbus"
	  with a child called "sound".
	
	  if we didn't find a davbus device, try 'i2s-a' since
	  this seems to be what iBooks have
		 partly deprecate snd-powermac, for those machines
 This should be verified on older screamers 
 chip->can_byte_swap = 0;  FIXME: check this 
 disable IEE 
 no capture 
 chip->can_byte_swap = 0;  FIXME: check this 
 disable IEE 
 chip->can_byte_swap = 0;  FIXME: check this 
 disable IEE 
 chip->can_byte_swap = 0;  FIXME: check this 
 disable IEE 
	 We need the PCI device for DMA allocations, let's use a crude method
	  for now ...
	 look for a property saying what sample rates
 Apple 'Fixed' format 
 assume only 44.1khz 
  auto-mute
 PMAC_SUPPORT_AUTOMUTE 
  create and detect a pmac chip record
 reset & enable interrupts 
	 Powerbooks have odd ways of enabling inputs such as
	   an expansion-bay CD or sound from an internal modem
 Enable CD and PC-card sound inputs. 
		 This is done by reading from address
		  f301a000, + 0x10 to enable the expansion-bay
		  CD sound input, + 0x80 to enable the PC-card
		  sound input.  The 0x100 enables the SCSI bus
		  terminator power.
 Enable CD sound input. 
		 The relevant bits for writing to this byte are 0x8f.
		  I haven't found out what the 0x80 bit does.
		  For the 0xf bits, writing 3 or 7 enables the CD
		  input, any other value disables it.  Values
		  1, 3, 5, 7 enable the microphone.  Values 0, 2,
		  4, 6, 8 - f enable the input from the modem.
 Reset dbdma channels 
  sleep notify for powerbook
  Save state when going to sleep, restore it afterwards.
 enable CD sound input 
 CONFIG_PM 
 SPDX-License-Identifier: GPL-2.0-or-later
  PMac DACA lowlevel functions
  Copyright (c) by Takashi Iwai <tiwai@suse.de>
 i2c address 
 registers 
 maximum volume value 
  initialize  detect DACA
 SR: no swap, 1bit delay, 32-48kHz 
 GCFG: power amp inverted, DAC on 
  update volume
 deemphasis switch 
 output volume 
 amplifier switch 
 CONFIG_PM 
 exported 
 default on 
	
	  build mixers
 SPDX-License-Identifier: GPL-2.0-or-later
  common keywest i2c layer
  Copyright (c) by Takashi Iwai <tiwai@suse.de>
 If instantiated via i2c-powermac, we still need to set the client 
  This is kind of a hack, best would be to turn powermac to fixed i2c
  bus numbers and declare the sound device as part of platform
  initialization
 ignored 
	
	  We know the driver is already loaded, so the device should be
	  already bound. If not it means binding failed, and then there
	  is no point in keeping the device instantiated.
	
	  Let i2c-core delete that device on driver removal.
	  This is safe because i2c-core holds the core_lock mutex for us.
 instantiated by i2c-powermac 
 instantiated by us if needed 
 exported 
 exported 
 There was already a device from i2c-powermac. Great, let's return 
 We assume Macs have consecutive I2C bus numbers starting at 0 
 Scan for devices to be bound to 
 SPDX-License-Identifier: GPL-2.0-or-later
  PMac Burgundy lowlevel functions
  Copyright (c) by Takashi Iwai <tiwai@suse.de>
  code based on dmasound.c.
 Waits for busy flag to clear 
  Burgundy volume: 0 - 100, stereo, word reg
 -EINVAL 
  Burgundy volume: 0 - 100, stereo, 2-byte reg
  Burgundy gainattenuation: 0 - 15, monostereo, byte reg
  Burgundy switch: 01, monostereo, word reg
  Burgundy switch: 01, monostereo, byte reg, bit mask
  Burgundy mixers
	BURGUNDY_SWITCH_W("Loop Capture Switch", 0,
 		MASK_ADDR_BURGUNDY_CAPTURESELECTS, 8, 24, 1),
 	BURGUNDY_SWITCH_B("Mixer out Capture Switch", 0,
 		MASK_ADDR_BURGUNDY_HOSTIFAD, 0x02, 0, 0),
 	BURGUNDY_SWITCH_B("Mixer Capture Switch", 0,
 		MASK_ADDR_BURGUNDY_HOSTIFAD, 0x01, 0, 0),
 	BURGUNDY_SWITCH_B("PCM out Capture Switch", 0,
 		MASK_ADDR_BURGUNDY_HOSTIFEH, 0x02, 0, 0),
	BURGUNDY_SWITCH_B("Line in Boost Capture Switch", 0,
  auto-mute stuffs
 PMAC_SUPPORT_AUTOMUTE 
  initialize burgundy
 Checks to see the chip is alive and kicking 
 set headphone-jack detection bit 
	
	  build burgundy mixers
 update the status only 
 SPDX-License-Identifier: GPL-2.0-only
  Audio support for PS3
  Copyright (C) 2007 Sony Computer Entertainment Inc.
  All rights reserved.
  Copyright 2006, 2007 Sony Corporation
  global
  PS3 audio register access
  ALSA defs
 stereo only 
 interrupt by four stages 
 buffer_size_max period_bytes_max 
			 last resort. force to stop dma.
			   NOTE: this cause DMA done interrupts
  wait for all dma is done.
  NOTE: caller should reset card->running before call.
        If not, the interrupt handler will re-start DMA,
        then DMA is never stopped.
	
	  wait for the last dma is done
	
	  expected maximum DMA done time is 5.7ms + something (DMA itself).
	  5.7ms is from 16bitsample 2ch 44.1Khz; the time next
	  DMA kick event would occur.
	
	  clear outstanding interrupts.
	
	 revert CLEAR bit since it will not reset automatically after DMA stop
 ensure the hardware sees changes 
 ensure the hardware sees the change 
  convert virtual addr to ioif bus addr.
  increment ring buffer pointer.
  NOTE: caller must hold write spinlock
  setup dmac to send data to audio and attenuate samples on the ring buffer
 this dmac does not support over 4G 
 initialize to mute gcc 
 dst: fixed to 3wire#0 
 count always 1 DMA block (12 stage = 128 bytes) 
 bump pointer if needed 
 kick event  
 ensure the hardware sees the change 
  Interrupt handler
	
	 serial buffer empty detected (every 4 times),
	 program next dma and kick it
 we are still in silent time 
		
		  serial out underflow, but buffer empty not detected.
		  in this case, fill fifo with 0 to recover.  After
		  filling dummy data, serial automatically start to
		  consume them and then will generate normal buffer
		  empty interrupts.
		  If both buffer underflow and buffer empty are occurred,
		  it is better to do nomal data transfer than empty one
 clear interrupt cause 
  audio mute onoff
  mute_on : 0 output enabled
            1 mute
  av setting
  NOTE: calling this function may generate audio interrupt.
	
	  Reset the following unwanted settings:
 disable all 3wire buffers 
 ensure the hardware sees the change 
 wait for actually stopped 
 reset buffer pointer 
 ensure the hardware actually start resetting 
 enable 3wire#0 buffer 
 In 24bit mode,ALSA inserts a zero byte at first byte of per sample 
 ensure all the setting above is written back to register 
 avsetting driver altered AX_IE, caller must reset it if you want 
   set sampling rate according to the substream
 sample rate 
 width 
 check CS non-audio bit and mute accordingly 
 mute if non-audio 
  PCM operators
 to retrieve substreamruntime in interrupt handler 
 mute off 
 this function sleep 
 mute on 
 some parameter changed 
		
		  let SPDIF device re-lock with SPDIF signal,
		  start with some silence
 every 4 times 
 restart ring buffer pointer 
 ensure the hardware sees the change 
 clear outstanding interrupts  
  report current pointer
  SPDIF status bits controls
 FIXME: ps3av_set_audio_mode() assumes only consumer mode 
 FIXME: move this to device_init (HW probe) 
 get irq outlet 
 irq 
	
	  avsetting driver seems to never change the following
	  so, init them here once
 no dma interrupt needed 
 use every 4 buffer empty interrupt 
 enable 3wire clocks 
 to start to generate SPDIF signal, fill data 
 setup MMIO 
 setup DMA area 
 use system page size 
 dma type; not used 
 CONFIG_SND_PS3_DEFAULT_START_DELAY 
 irq 
 create card instance 
 create control elements 
 create PCM devices instance 
 NOTE:this driver works assuming pcm:substream = 1:1 
 instance index, will be stored pcm.device
 output substream 
 input substream 
 set pcm ops 
 pre-alloc PCM DMA buffer
	
	  allocate null buffer
	  its size should be lager than PS3_AUDIO_FIFO_STAGE_SIZE  2
	  PAGE_SIZE is enogh
 set default sample rateword width 
 register the card 
	
	  there is no destructor function to pcm.
	  midlayer automatically releases if the card removed
 snd_ps3_probe 
 called when module removal 
	
	  ctl and preallocate buffer will be freed in
	  snd_card_free
 snd_ps3_remove 
  modulesubsystem initializeterminate
 register systembus DRIVER, this calls our probe() func 
 SPDX-License-Identifier: GPL-2.0-or-later
    (Tentative) USB Audio Driver for ALSA
    Mixer control part
    Copyright (c) 2002 by Takashi Iwai <tiwai@suse.de>
    Many codes borrowed from audio.c by
 	    Alan Cox (alan@lxorguk.ukuu.org.uk)
 	    Thomas Sailer (sailer@ife.ee.ethz.ch)
  TODOs, for both the mixer and the streaming interfaces:
   - support for UAC2 effect units
   - support for graphical equalizers
   - RANGE and MEM set commands (UAC2)
   - RANGE and MEM interrupt dispatchers (UAC2)
   - audio channel clustering (UAC2)
   - audio sample rate converter units (UAC2)
   - proper handling of clock multipliers (UAC2)
   - dispatch clock change notifications (UAC2)
   	- stop PCM streams which use a clock that became invalid
   	- stop PCM streams which use a clock selector that has changed
   	- parse available sample rates again when clock sources changed
E-mu 020204040204 eXtension Unit(XU) control
 clock source
 clock rate 
 the spdif format 
 soft limiter 
  manual mapping of mixer names
  if the mixer topology is too complicated and the parsed names are
  ambiguous, add the entries in usbmixer_maps.c.
 get the mapped name if the unit matches 
 ignore the error value if ignore_ctl_error flag is set 
 check whether the control should be ignored 
 dB mapping 
 get the mapped selector source name 
  find an audio control unit with the given unit id
 we just parse the header 
  copy a string with the given id
  convert from the byteword on usb descriptor to the zero-based integer
  convert from the zero-based int to the byteword for usb descriptor
 not reached 
 unreachable 
  retrieve a mixer value
 enough space for one range 
 FIXME: how should we handle multiple triplets here? 
 channel = 0: master, 1 = first channel 
  set a mixer value
 UAC_VERSION_23 
 FIXME 
  TLV callback for mixer volume controls
  parser routines begin here...
  check if the inputoutput channel routing is enabled on the given bitmap.
  used for mixer unit parser
  add an alsa control element
  search and increment the index until an empty slot is found.
  if failed, give up and free the control instance.
  get a terminal name string
 virtual type - not a real terminal 
  Get logical cluster information for UAC3 devices.
  Get number of channels for a Mixer Unit.
 no bmControls -> skip 
  Parse Input Terminal Unit
 call recursively to verify the referenced clock entity 
	 save input term properties after recursion,
	  to ensure they are not overriden by the recursion calls
 call recursively to verify the referenced clock entity 
	 save input term properties after recursion,
	  to ensure they are not overriden by the recursion calls
 REVISIT: UAC3 IT doesn't have channels cfg 
 virtual type 
 call recursively to retrieve the channel info 
 virtual type 
 call recursively to retrieve the channel info 
 virtual type 
 virtual type 
 virtual type 
 virtual type 
  parse the source unit recursively until it reaches to a terminal
  or a branched unit.
 a loop in the terminal chain? 
 bad descriptor 
 the header is the same for all versions 
 continue to parse 
  Feature Unit
 feature unit control information 
 data type for uac1 
 data type for uac2 if different from uac1, else -1 
 FIXME: not implemented yet 
 UAC2 specific 
 private_free callback 
  interface to ALSA control for featuremixer units
 volume control quirks 
 M-Audio Fast Track C400 
 M-Audio Fast Track C600 
 -73 dB = 0xb6ff 
 M-Audio Fast Track Ultra 8R 
 M-Audio Fast Track Ultra 
	 quirk for UDA1321N101.
	  note that detection between firmware 2.1.1.7 (N101)
	  and later 2.1.1.21 is not very clear from datasheets.
	  I hope that the min value is -15360 for newer firmware --jk
 Logitech Webcam C500 
 Logitech Webcam C210 
 HD Webcam c310 
 HD Webcam c510 
 HD Webcam c270 
 HD Webcam c525 
 Logitech Quickcam Fusion 
 QuickCam Communicate DeluxeS7500 
	 Most audio usb devices lie about volume resolution.
	  Most Logitech webcams have res = 384.
	  Probably there is some logitech magic behind this number --fishor
 ESS Technology Asus USB DAC 
 Jieli Technology USB PHY 2.0 
 forcibly initialize the current mixer value; if GET_CUR fails, set to
  the minimum as default
  retrieve the minimum and maximum values for the specified control
 for failsafe 
		 Additional checks for the proper resolution
		 
		  Some devices report smaller resolutions than actually
		  reacting.  They don't return errors but simply clip
		  to the lower aligned value.
	 USB descriptions contain the dB scale in 1256 dB unit
	  while ALSA TLV contains in 1100 dB unit
 something is wrong; assume it's either fromto 0dB 
 totally crap, return an error 
		 if the max volume is too low, it's likely a bogus range;
		  here we use -96dB as the threshold
 initialize all elements 
 get a featuremixer unit info 
 get the current value from featuremixer unit 
 master channel 
 put the current value to featuremixer unit 
 master channel 
 get the boolean value from the master channel of a UAC control 
 UAC_VERSION_3 
 get the connectors status and report it as boolean type 
 will be filled later manually 
 the read-only variant 
 will be filled later manually 
  A control which shows the boolean value from reading a UAC control on
  the master channel.
 will be filled later manually 
 will be filled later manually 
  This symbol is exported in order to allow the mixer quirks to
  hook up to the standard feature unit control mechanism
  build a feature control
  A lot of headsetsheadphones have a "Speaker" mixer. Make sure we
  rename it to "Headphone". We determine if something is a headphone
  similar to how udev determines form factor.
 FIXME: not supported yet 
 UAC_VERSION_2 
 master channel 
	
	  If all channels in the mask are marked read-only, make the control
	  read-only. snd_usb_set_cur_mix_value() will check the mask again and won't
	  issue write commands to read-only channels.
		
		  determine the control name.  the rule is:
		  - if a name id is given in descriptor, use it.
		  - if the connected input can be determined, then use the name
		    of terminal type.
		  - if the connected output can be determined, use it.
		  - otherwise, anonymous name.
		
		  determine the stream direction:
		  if the connected output is USB stream, then it's likely a
		  capture stream.  otherwise it should be playback (hopefully :)
 get minmax values 
 skip a bogus volume range 
	
	  Are there devices with volume range more than 255? I use a bit more
	  to be sure. 384 is a resolution magic number found on Logitech
	  devices. It will definitively catch all buggy Logitech devices.
	
	   soundcorectljack.c has a convention of naming jack controls
	  by ending in " Jack".  Make it slightly more useful by
	  indicating Input or Output after the terminal name.
 get connector value to "wake up" the USB audio 
 Build a mixer control for a UAC connector control (jack-detect) 
 set up a specific resume callback 
	
	  UAC2: The first byte from reading the UAC2_TE_CONNECTOR control returns the
	  number of channels connected.
	 
	  UAC3: The first byte specifies size of bitmap for the inserted controls. The
	  following byte(s) specifies which connectors are inserted.
	 
	  This boolean ctl will simply report if any channels are connected
	  or not.
 UAC_VERSION_3 
 report true if any channel is connected 
	
	  The only property of this unit we are interested in is the
	  clock source validity. If that isn't readable, just bail out.
 From UAC2 5.2.5.1.2 "Only the get request is supported." 
  parse a feature unit
  most of controls are defined here.
 UAC_VERSION_3 
 parse the source unit 
 determine the input source type and name 
 master configuration quirks 
 disable non-functional volume control 
 disable non-functional volume control 
 check all control types 
 audio class v1 controls are never read-only 
			
			  The first channel must be set
			  (for ease of programming).
 UAC_VERSION_23 
			
			  NOTE: build_feature_ctl() will mark the control
			  read-only if all channels are marked read-only in
			  the descriptors. Otherwise, the control will be
			  reported as writeable, but the driver will not
			  actually issue a write command for read-only
			  channels.
			
			  The first channel must be set
			  (for ease of programming).
  Mixer Unit
 check whether the given inout overflows bmMixerControls matrix 
 remaining bytes after bmMixerControls 
 iMixer 
 bmControls + iMixer 
 bmControls + wMixerDescrStr 
 overflow? 
  build a mixer unit control
  the callbacks are identical with feature unit.
  input channel number (zero based) is given in control field instead.
 based on 1 
 get minmax values 
 UAC1. No Insertion control 
 Check for jack detection. 
  parse a mixer unit
 no bmControls field (e.g. Maya44) -> ignore 
  Processing Unit  Extension Unit
 get callback for processingextension unit 
 put callback for processingextension unit 
 alsa control interface for processingextension unit 
 will be filled later 
  predefined data for processing units
  predefined data for extension units
  build a processingextension unit
 UAC_VERSION_23 
 get minmax values 
			
			  E-Mu USB 04040202TrackerPre0204
			  samplerate control quirk
 nothing  ;
	
	  Note that we parse extension units with processing unit descriptors.
	  That's ok as the layout is the same.
  Selector Unit
  info callback for selector unit
  use an enumerator type for routing
 get callback for selector unit 
 put callback for selector unit 
 alsa control interface for selector unit 
 will be filled later 
  private free callback.
  free both private_data and private_value
  parse a selector unit
 only one ? nonsense! 
 UAC23_SELECTOR_UNIT 
 check the static mapping table at first 
 no mapping ? 
 if iSelector is given, use it 
 TODO: Class-Specific strings not yet supported 
 ... or pick up the terminal name at next 
 ... or use the fixed string "USB" as the last resort 
 and add the proper suffix 
  parse an audio unit recursively
 the unit already visited 
 skip invalid unit 
 FIXME - effect units not implemented yet 
 kill pending URBs 
 UAC3 predefined channels configuration 
 capture channels mask 
 playback channels mask 
 side tone mixing channel mask 
		
		  BAIF, BAOF or combination of both
		  IN: Mono or Stereo cfg, Mono alt possible
		  OUT: Mono or Stereo cfg, Mono alt possible
 dynamic channels 
 dynamic channels 
 BAOF; Stereo only cfg, Mono alt possible 
 BAOF; Mono or Stereo cfg, Mono alt possible 
 dynamic channels 
 BAIF; Mono or Stereo cfg, Mono alt possible 
 dynamic channels 
		
		  BAIOF topology
		  IN: Mono only
		  OUT: Mono or Stereo cfg, Mono alt possible
 dynamic channels 
 BAIOF; IN: Mono only; OUT: Stereo only, Mono alt possible 
 BAIF + BAOF; IN: Mono only; OUT: Mono only 
 terminator 
	
	  If both playbackcapture channels are dynamic, make sure
	  at least one channel is present
  create mixer controls for UAC3 BADD profiles
  UAC3 BADD device doesn't contain CS descriptors thus we will guess everything
  BADD device may contain Mixer Unit, which doesn't have any controls, skip it
 Detect BADD captureplayback channels from AS EP descriptors 
		
		  The number of Channels in an AudioStreaming interface
		  and the audio sample bit resolution (16 bits or 24
		  bits) can be derived from the wMaxPacketSize field in
		  the Standard AS Audio Data Endpoint descriptor in
		  Alternate Setting 1
 check direction 
 check the mapping table 
 Playback 
 Master channel, always writable 
 MonoStereo volume channels, always writable 
 Capture 
 Master channel, always writable 
 MonoStereo volume channels, always writable 
 Side tone-mixing 
 Master channel, always writable 
 Mono volume channel, always writable 
 Insertion Control 
 Input Term - Insertion control 
 Output Term - Insertion control 
  create mixer controls
  walk through all UAC_OUTPUT_TERMINAL descriptors to search for mixers
 check the mapping table 
 skip invalid descriptor 
 mark terminal ID as visited 
 mark terminal ID as visited 
			
			  For UAC2, use the same approach to also add the
			  clock selectors
 UAC_VERSION_3 
 mark terminal ID as visited 
			
			  For UAC3, use the same approach to also add the
			  clock selectors
 invalidate cache, so the value is read from the device 
 invalidate cache, so the value is read from the device 
 master channel 
 TODO 
 TODO 
 switch 
 ignore any notifications not from the control interface 
 UAC_VERSION_2 
 drop vendor specific and endpoint requests 
 create the handler for the optional status interrupt endpoint 
 we need one interrupt input endpoint 
 stop any bus activity of a mixer 
 master 
 restore cached mixer values 
 SPDX-License-Identifier: GPL-2.0-or-later
 unlink for avoiding double-free 
  free a substream
 not initialized 
  free a usb stream instance
  initialize the substream instance.
 Initialize Power Domain to idle status D1 
 kctl callbacks for usb-audio channel maps 
 check whether a duplicated entry exists in the audiofmt list 
 copy the entry 
 create a chmap kctl assigned to the given USB substream 
 no chmap is found 
 override handlers 
 convert from USB ChannelConfig bits to ALSA chmap element 
 left front 
 right front 
 center front 
 LFE 
 left surround 
 right surround 
 left of center 
 right of center 
 surround 
 side left 
 side right 
 top 
 terminator 
 front left 
 front right 
 front center 
 LFE 
 back left 
 back right 
 front left of center 
 front right of center 
 back center 
 side left 
 side right 
 top center 
 top front left 
 top front center 
 top front right 
 top back left 
 top back center 
 top back right 
 top front left of center 
 top front right of center 
 left LFE 
 right LFE 
 top side left 
 top side right 
 bottom center 
 back left of center 
 back right of center 
 terminator 
		 If we're missing wChannelConfig, then guess something
 UAC3 device stores channels information in Cluster Descriptors 
			
			  TODO: this conversion is not complete, update it
			  after adding UAC3 values to asound.h
  add this endpoint to the chip instance.
  if a stream with the same endpoint already exists, append to it.
  if not, create a new pcm stream. note, fp is added to the substream
  fmt_list and will be freed on the chip instance release. do not free
  fp or do remove it from the substream fmt_list to avoid double-free.
 look for an empty stream 
 create a new pcm 
	
	  Keep using head insertion for M-Audio Audiophile USB (tm) which has a
	  fix to swap capture stream order in confcardsUSB-audio.conf
	 parsed with a v1 header here. that's ok as we only look at the
 Creamware Noah has this descriptor after the 2nd endpoint 
	
	  If we can't locate the USB_DT_CS_ENDPOINT descriptor in the extra
	  bytes after the first endpoint, go search the entire interface.
	  Some devices have it directly before the standard endpoint.
 emulate the endpoint attributes of a v1 device 
 UAC_VERSION_3 
 emulate the endpoint attributes of a v1 device 
 find an input terminal descriptor (either UAC1 or UAC2) with the given
  terminal id
 OK to use with both UAC2 and UAC3 
 get audio formats 
 remember the format value 
 UAC_VERSION_2 
		
		  lookup the terminal associated to this interface
		  to extract the clock
 get format type 
	
	  Blue Microphones workaround: The last altsetting is
	  identical with the previous one, except for a larger
	  packet size, but is actually a mislabeled two-channel
	  setting; ignore it.
	 
	  Part 2: analyze quirk flag and format
 some quirks for attributes here 
 ok, let's parse further... 
 Create chmap 
	
	  Get number of channels and channel map through
	  High Capability Cluster Descriptor
	 
	  First step: get High Capability header and
	  read size of Cluster Descriptor
	
	  Second step: allocate needed amount of memory
	  and request Cluster Descriptor
	
	  lookup the terminal associated to this interface
	  to extract the clock
 No attributes 
 SNDRV_PCM_RATE_CONTINUOUS 
 ok, let's parse further... 
 parse the interface's altsettings 
	
	  Dallas DS4201 workaround: It presents 5 altsettings, but the last
	  one misses syncpipe, and does not produce any sound.
 skip invalid one 
 must be isochronous 
 check direction 
		
		  Roland audio streaming interfaces are marked with protocols
		  012, but are UAC 1 compatible.
			
			  Blue Microphones workaround: The last altsetting is
			  identical with the previous one, except for a larger
			  packet size, but is actually a mislabeled two-channel
			  setting; ignore it.
			 
			  Part 1: prepare quirk flag
 add endpoints 
 try to set the interface... 
 parse PCM formats 
 parse non-PCM formats 
 SPDX-License-Identifier: GPL-2.0-or-later
    USB Audio Driver for ALSA
    Quirks and vendor-specific extensions for mixer interfaces
    Copyright (c) 2002 by Takashi Iwai <tiwai@suse.de>
    Many codes borrowed from audio.c by
 	    Alan Cox (alan@lxorguk.ukuu.org.uk)
 	    Thomas Sailer (sailer@ife.ee.ethz.ch)
    Audio Advantage Micro II support added by:
 	    Przemek Rudy (prudy1@o2.pl)
 This function allows for the creation of standard UAC controls.
  See the quirks for M-Audio FTUs or Ebox-44.
  If you don't want to set a TLV callback pass NULL.
  Since there doesn't seem to be a devices that needs a multichannel
  version, we keep it mono for simplicity.
	 get_min_max() is called only for integer volumes later,
 Create control 
 Set name 
 set TLV 
 Add control to mixer 
 Offset , name, tlv_callback);
  Create a set of standard UAC controls from a table
 don't use snd_usb_mixer_add_control() here, this is a special list element 
  Sound Blaster remote control configuration
  format of remote control data:
  Extigy:       xx 00
  Audigy 2 NX:  06 80 xx 00 00 00
  Live! 24-bit: 06 80 xx yy 22 83
 minimum accepted length of the URB result 
 Extigy       
 Audigy 2 NX  
 Live! 24-bit 
 Usb X-Fi S51 
 Usb X-Fi S51 Pro 
 Usb X-Fi S51 Pro 
 Usb X-Fi S51 Pro 
 Toshiba SB0500 
 the Mute button actually changes the mixer control 
 USB X-Fi S51 Pro 
 name and private_value are set dynamically 
 USB X-Fi S51 doesn't have a CMSS LED 
 USB X-Fi S51 Pro doesn't have one either 
 Live24ext has 2 LEDs only 
 LED on as default 
 &1=Line, &2=Mic
 headphones 
 last command, 6 bytes see rc_config above 
 EMU0204 
 ASUS Xonar U1  U3 controls 
 Digidesign Mbox 1 helper functions 
 Read clock source 
 spdif sync: buff is all zeroes 
	 2 possibilities:	Internal    -> expects sample rate
	 			SPDIF sync -> expects rate = 0
 Set clock source 
	 Hardware gives 2 possibilities:	ANALOG Source  -> 0x01
	 					SPDIF Source  -> 0x02
 Read input source 
	 NB: Setting the input source to SPDIF resets the clock source to SPDIF
	  Hardware expects 2 possibilities:	ANALOG Source  -> 0x01
	 					SPDIF Source  -> 0x02
 Set input source 
 Digidesign Mbox 1 clock source switch (internalspdif) 
 FIXME: hardcoded sample rate 
 Digidesign Mbox 1 input source switch (analogspdif) 
 Native Instruments device quirks 
 M-Audio FastTrack Ultra quirks 
 FTU Effect switch (also used by C400C600) 
 Create volume controls for FTU devices
 This control needs a volume quirk, see mixer.c 
 This control needs a volume quirk, see mixer.c 
 This control needs a volume quirk, see mixer.c 
 SampleRate ExtensionUnit ID 
 M-Audio Fast Track C400C600 
 C400C600 volume controls, this control needs a volume quirk, see mixer.c 
 This control needs a volume quirk, see mixer.c 
 This control needs a volume quirk, see mixer.c 
 This control needs a volume quirk, see mixer.c 
 { 0x3c, 0x43, 0x3e, 0x45, 0x40, 0x47 } 
 { 0x70, 0x79, 0x72, 0x7b, 0x74, 0x7d, 0x76, 0x7f } 
  The mixer units for Ebox-44 are corrupt, and even where they
  are valid they presents mono controls as L and R channels of
  stereo. So we provide a good mixer here.
 Audio Advantage Micro II findings:
  Mapping spdif AES bits to vendor register.bit:
  AES0: [0 0 0 0 2.3 2.2 2.1 2.0] - default 0x00
  AES1: [3.3 3.2.3.1.3.0 2.7 2.6 2.5 2.4] - default: 0x01
  AES2: [0 0 0 0 0 0 0 0]
  AES3: [0 0 0 0 0 0 x 0] - 'x' bit is set basing on standard usb request
                            (UAC_EP_CS_ATTR_SAMPLE_RATE) for Audio Devices
  power on values:
  r2: 0x10
  r3: 0x20 (b7 is zeroed just before playback (except IEC61937) and set
            just after it to 0xa0, presumably it disablesmutes some analog
            parts when there is no audio.)
  r9: 0x28
  Optical transmitter onoff:
  vendor register.bit: 9.1
  0 - on (0x28 register value)
  1 - off (0x2a register value)
 use known values for that card: interface#1 altsetting#1 
 The frequency bits in AES3 cannot be set via register access. 
 Silently ignore any bits from the request that cannot be set. 
 reset value 
 reset value 
 Creative Sound Blaster E1 
 fix to 0dB playback volumes 
 RME Class Compliant device quirks 
 maximum number of items for AES and SPDIF rates for above table 
 AES 
 SPDIF 
  RME Babyface Pro (FS)
  These devices exposes a couple of DSP functions via request to EP0.
  Switches are available via control registers, while routing is controlled
  by controlling the volume on each possible crossing point.
  Volume control is linear, from -inf (dec. 0) to +6dB (dec. 65536) with
  0dB being at dec. 32768.
 -inf
 +6dB
 18 bit linear volume, split so 2 bits end up in index.
 Predfine elements
 Line routing
 PCM routing... yes, it is output remapping
 Control Reg 1
 Control Reg 2
  Pioneer DJ DJM Mixers
  These devices generally have options for soft-switching the playback and
  capture sources in addition to the recording level. Although different
  devices have different configurations, there seems to be canonical values
  for specific captureplayback types:  See the definitions of these below.
  The wValue is masked with the stereo channel number. e.g. Setting Ch2 to
  capture phono would be 0x0203. Capture, playback and capture level have
  different wIndexes.
 Capture types
 Playback types
 kcontrol->private_value layout
 device table index
 used for the snd_djm_devices table, so please update accordingly
 The DJM-850 has different values for CDLINE and LINE capture
 control options than the other DJM declared in this file.
 common DJM capture level option values
 DJM-250MK2
 DJM-750
 DJM-850
 DJM-900NXS2
 DJM-750MK2
 Tascam US-16x08 
 EMU0204 
 M-Audio Fast Track C400 
 M-Audio Fast Track C400 
 M-Audio Fast Track Ultra 
 M-Audio Fast Track Ultra 8R 
 ASUS Xonar U1 
 ASUS Xonar U1 (2) 
 ASUS Xonar U3 
 Audio Advantage Micro II 
 Digidesign Mbox 1 
 Traktor Audio 6 
 Traktor Audio 10 
 Electrix Ebox-44 
 detection is disabled in mixer_maps.c 
 Focusrite Scarlett 6i6 
 Focusrite Scarlett 8i6 
 Focusrite Scarlett 18i6 
 Focusrite Scarlett 18i8 
 Focusrite Scarlett 18i20 
 Focusrite Scarlett 6i6 2nd Gen 
 Focusrite Scarlett 18i8 2nd Gen 
 Focusrite Scarlett 18i20 2nd Gen 
 Focusrite Scarlett Solo 3rd Gen 
 Focusrite Scarlett 2i2 3rd Gen 
 Focusrite Scarlett 4i4 3rd Gen 
 Focusrite Scarlett 8i6 3rd Gen 
 Focusrite Scarlett 18i8 3rd Gen 
 Focusrite Scarlett 18i20 3rd Gen 
 Creative Sound Blaster E1 
 Dell WD15 dock 
 RME ADI-2 Pro 
 RME ADI-2 DAC 
 RME 
 Presonus Studio 1810c 
 RME Babyface Pro FS 
 Pioneer DJ DJM-250MK2 
 Pioneer DJ DJM-750 
 Pioneer DJ DJM-750MK2 
 Pioneer DJ DJM-850 
 Pioneer DJ DJM-900NXS2 
 Dell WD15 dock 
 unit ids specific to ExtigyAudigy 2 NX: 
 remote control 
 digital in jack 
 line in jacks 
 speaker out jacks 
 headphones out jack 
 live24ext: 4 = line-in jack 
 hp-out jack (may actuate Mute) 
	 Approximation using 10 ranges based on output measurement on hw v1.2.
		 Some other clearly broken DragonFly variant.
		  At least a 0..53 variant (hw v1.0) exists.
 AudioQuest DragonFly 
 lowest playback value is muted on C-Media devices 
 SPDX-License-Identifier: GPL-2.0-or-later
    Clock domain and sample rate management functions
		
		  Assume the clock is valid if clock source supports only one
		  single sample rate, the terminal is connected directly to it
		  (there is no clock selector) and clock type is internal.
		  This is to deal with some Denon DJ controllers that always
		  reports that clock is invalid.
	
	  MOTU MicroBook IIc
	  Sample rate changes takes more than 2 seconds for this device. Clock
	  validity request returns false during that period.
 If a clock source can't tell us whether it's valid, we assume it is 
 first, see if the ID we're looking at is a clock source already 
		 the entity ID we are looking at is a selector.
 Selector values are one-based 
 Skip setting clock selector again for some devices 
 The current clock source is invalid, try others. 
 FIXME: multipliers only act as pass-thru element for now 
  For all kinds of sample rate settings and other device queries,
  the clock source (end-leaf) must be used. However, clock selectors,
  clock multipliers and sample rate converters may be specified as
  clock source input to terminal. This functions walks the clock path
  to its end and tries to find the source.
  The 'visited' bitfield is used internally to detect recursive loops.
  Returns the clock source UnitID (>=0) on success, or an error.
 if endpoint doesn't have sampling rate control, bail out 
	 Don't check the sample rate for devices which we know don't
 the firmware is likely buggy, don't repeat to fail too many times 
 some devices don't support reading 
 three strikes, see above 
 runtime->rate = crate;
  Try to set the given sample rate:
  Return 0 if the clock source is read-only, the actual rate on success,
  or a negative error code.
  This function gets called from format.c to validate each sample rate, too.
  Hence no message is shown upon error
	 First, try to find a valid clock. This may trigger
	  automatic clock selection if the current clock is not
	  valid.
		 We did not find a valid clock, but that might be
		  because the current sample rate does not match an
		  external clock source. Try again without validation
		  and we will do another validation after setting the
		  rate.
 Hardcoded sample rates 
 continue processing 
 validate clock after rate change 
 SPDX-License-Identifier: GPL-2.0-or-later
    (Tentative) USB Audio Driver for ALSA
    Copyright (c) 2002 by Takashi Iwai <tiwai@suse.de>
    Many codes borrowed from audio.c by
 	    Alan Cox (alan@lxorguk.ukuu.org.uk)
 	    Thomas Sailer (sailer@ife.ee.ethz.ch)
    Audio Class 3.0 support by Ruslan Bilovol <ruslan.bilovol@gmail.com>
   NOTES:
    - the linked URBs would be preferred but not used so far because of
      the instability of unlinking.
    - type II is not supported properly.  there is no device which supports
      this type correctly.  SB extigy looks as if it supports, but it's
      indeed an AC3 stream packed in SPDIF frames (i.e. no real AC3 stream).
 Index 0-MAX 
 ID for this card 
 Enable this card 
 Vendorproduct IDs for this card 
 device parameter for this card 
  we keep the snd_usb_audio_t instances by ourselves for merging
  the all interfaces on the same card as one sound device.
  disconnect streams
  called from usb_audio_disconnect()
	
	  Android with both accessory and audio interfaces enabled gets the
	  interface numbers wrong.
 skip non-supported classes 
 reset the current interface 
  parse audio control descriptor and create pcmmidi streams
 find audiocontrol interface 
 just to be sure -- this shouldn't hit at all 
			
			  Firmware writers cannot count to three.  So to find
			  the IAD on the NuForce UDH-100, also check the next
			  interface.
  Profile name preset table
 override card->longname 
 vendorproduct and profile name presets, sorted in device id order 
 HP Thunderbolt Dock Audio Headset 
 HP Thunderbolt Dock Audio Module 
	 Two entries for Gigabyte TRX40 Aorus Master:
	  TRX40 Aorus Master has two USB-audio devices, one for the front
	  headphone with ESS SABRE9218 DAC chip, while another for the rest
	  IO (the rear panel and the front mic) with Realtek ALC1220-VB.
	  Here we provide two distinct names for making UCM profiles easier.
 Gigabyte TRX40 Aorus Pro WiFi 
 CreativeE-Mu devices 
 CreativeToshiba Multimedia Center SB-0500 
 ASUS ROG Strix 
 ASUS PRIME TRX40 PRO-S 
 Dell WD15 Dock 
 Dell WD19 Dock 
	
	  The original product_name is "USB Sound Device", however this name
	  is also used by the CM106 based cards, so make it unique.
 MSI TRX40 Creator 
 MSI TRX40 
 StantonN2IT Final Scratch v1 device ('Scratchamp') 
 aka. Serato Scratch Live DJ Box 
 Lenovo ThinkStation P620 Rear Line-in, Line-out and Microphone 
 Lenovo ThinkStation P620 Internal Speaker + Front Headset 
 Asrock TRX40 Creator 
 terminator 
  free the chip instance
  here we have to do not much, since pcm and controls are already freed
 retrieve the device string as shortname 
 no name available from anywhere, so use ID 
 shortcut - if any pre-defined string is given, use it 
 retrieve the vendor and device strings as longname 
 we don't really care if there isn't any vendor string 
  create a chip instance and set its names.
 avoid autopm during probing 
 look for a matching quirk alias id 
 defined below 
 look for the corresponding quirk 
 FIXME: this checks only vendor:product pair in the list 
  probe the active usb device
  note that this can be called multiple times per a device, when it
  includes multiple audio control interfaces.
  thus we check the usb device pointer and creates the card instance
  only at the first time.  the successive calls of this function will
  append the pcm interface to the corresponding card.
	
	  found a config.  now register to ALSA
 check whether it's already registered 
 avoid autopm 
		 it's a fresh one.
		  now look for an empty slot and create a new card instance
	
	  For devices with more than one control interface, we assume the
	  first contains the audio controls. We might need a more specific
	  check here in the future.
 continue 
 need some special handlings 
 create normal USB audio interfaces 
 clear again 
	 we are allowed to call snd_card_register() many times, but first
	  check to see if a device needs to skip it or do anything special
 don't want to fail when snd_media_device_create() fails 
		 chip->active is inside the chip->card object,
		  decrement before memory is possibly returned.
  we need to take care of counter, since disconnection can be called also
  many times as well as usb_audio_probe().
		 wait until all pending tasks done;
		  they are protected by snd_usb_lock_shutdown()
 release the pcm resources 
 release the endpoint resources 
 release the midi resources 
		
		  Nice to check quirk && quirk->shares_media_device and
		  then call the snd_media_device_delete(). Don't have
		  access to the quirk here. snd_media_device_delete()
		  accesses mixer_list
 release mixer resources 
 lock the shutdown (disconnect) task and autoresume 
 autosuspend and unlock the shutdown 
 rollback 
 avoid autopm 
	
	  ALSA leaves material resumption to user space
	  we just notify and restart the mixers
 allow autopm after this point 
 CONFIG_PM 
 Terminating entry 
  entry point for linux usb interface
 SPDX-License-Identifier: GPL-2.0
    Focusrite Scarlett Gen 23 Driver for ALSA
    Supported models:
    - 6i618i818i20 Gen 2
    - Solo2i24i48i618i818i20 Gen 3
    Copyright (c) 2018-2021 by Geoffrey D. Bennett <g at b4.vu>
    Copyright (c) 2020-2021 by Vladimir Sadovnikov <sadko4u@gmail.com>
    Based on the Scarlett (Gen 1) Driver for ALSA:
    Copyright (c) 2013 by Tobias Hoffmann
    Copyright (c) 2013 by Robin Gareus <robin at gareus.org>
    Copyright (c) 2002 by Takashi Iwai <tiwai at suse.de>
    Copyright (c) 2014 by Chris J Arges <chris.j.arges at canonical.com>
    Many codes borrowed from audio.c by
      Alan Cox (alan at lxorguk.ukuu.org.uk)
      Thomas Sailer (sailer at ife.ee.ethz.ch)
    Code cleanup:
    David Henningsson <david.henningsson at canonical.com>
 The protocol was reverse engineered by looking at the communication
  between Focusrite Control 2.3.4 and the Focusrite(R) Scarlett 18i20
  (firmware 1083) using usbmon in July-August 2018.
  Scarlett 18i8 support added in April 2019.
  Scarlett 6i6 support added in June 2019 (thanks to Martin Wittmann
  for providing usbmon output and testing).
  Scarlett 4i48i6 Gen 3 support added in May 2020 (thanks to Laurent
  Debricon for donating a 4i4 and to Fredrik Unger for providing 8i6
  usbmon output and testing).
  Scarlett 18i818i20 Gen 3 support added in June 2020 (thanks to
  Darren Jaeckel, Alex Sedlack, and Clovis Lunel for providing usbmon
  output, protocol traces and testing).
  Support for loading mixer volume and mux configuration from the
  interface during driver initialisation added in May 2021 (thanks to
  Vladimir Sadovnikov for figuring out how).
  Support for Solo2i2 Gen 3 added in May 2021 (thanks to Alexander
  Vorona for 2i2 protocol traces).
  Support for phantom power, direct monitoring, speaker switching,
  and talkback added in May-June 2021.
  This ALSA mixer gives access to (model-dependent):
   - input, output, mixer-matrix muxes
   - mixer-matrix gain stages
   - gainvolumemute controls
   - level meters
   - lineinst level, pad, and air controls
   - phantom power, direct monitor, speaker switching, and talkback
     controls
   - disableenable MSD mode
  <ditaa>
     --------------\    18chn            20chn     --------------\
     | Hardware  in +--+------\    -------------+--+ ALSA PCM out |
     \--------------  |      |    |             |  \--------------
                       |      |    |    -----\  |
                       |      |    |    |     |  |
                       |      v    v    v     |  |
                       |   +---------------+  |  |
                       |    \ Matrix  Mux    |  |
                       |     +-----+-----+    |  |
                       |           |          |  |
                       |           |18chn     |  |
                       |           |          |  |
                       |           |     10chn|  |
                       |           v          |  |
                       |     +------------+   |  |
                       |     | Mixer      |   |  |
                       |     |     Matrix |   |  |
                       |     |            |   |  |
                       |     | 18x10 Gain |   |  |
                       |     |   stages   |   |  |
                       |     +-----+------+   |  |
                       |           |          |  |
                       |18chn      |10chn     |  |20chn
                       |           |          |  |
                       |           +----------  |
                       |           |             |
                       v           v             v
                       ===========================
                +---------------+       +--------------+
                 \ Output  Mux          \ Capture Mux 
                  +---+---+---+           +-----+-----+
                      |   |                     |
                 10chn|   |                     |18chn
                      |   |                     |
   --------------\   |   |                     |   --------------\
   | SPDIF, ADAT |<--   |10chn                \-->| ALSA PCM in  |
   | Hardware out |       |                         \--------------
   \--------------       |
                          v
                   +-------------+    Software gain per channel.
                   | Master Gain |<-- 18i20 only: Switch per channel
                   +------+------+    to select HW or SW gain control.
                          |
                          |10chn
   --------------\       |
   | Analogue     |<------
   | Hardware out |
   \--------------
  <ditaa>
  Gen 3 devices have a Mass Storage Device (MSD) mode where a small
  disk with registration and driver download information is presented
  to the host. To access the full functionality of the device without
  proprietary software, MSD mode can be disabled by:
  - holding down the 48V button for five seconds while powering on
    the device, or
  - using this driver and alsamixer to change the "MSD Mode" setting
    to Off and power-cycling the device
 device_setup value to enable 
 device_setup value to allow turning MSD mode back on 
 some gui mixers can't handle negative ctl values 
 mixer range from -80dB to +6dB in 0.5dB steps 
 map from (dB + 80)  2 to mixer value
  for dB in 0 .. 172: int(8192  pow(10, ((dB - 160)  2  20)))
 Maximum number of analogue outputs 
 Maximum number of level and pad switches 
 Maximum number of inputs to the mixer 
 Maximum number of outputs from the mixer 
 Maximum size of the data in the USB mux assignment message:
  20 inputs, 20 outputs, 25 matrix inputs, 12 spare
 Maximum number of meters (sum of output port counts) 
 Hardware port types:
  - None (no input to mux)
  - Analogue IO
  - SPDIF IO
  - ADAT IO
  - Mixer IO
  - PCM IO
 IO count of each port type kept in struct scarlett2_ports 
 DimMute buttons on the 18i20 
 Description of each hardware port type:
  - id: hardware ID of this port type
  - src_descr: printf format string for mux input selections
  - src_num_offset: added to channel number for the fprintf
  - dst_descr: printf format string for mixer controls
 Number of mux tables: one for each band of sample rates
  (44.148kHz, 88.296kHz, and 176.4176kHz)
 Maximum number of entries in a mux table 
 One entry within mux_assignment defines the port type and range of
  ports to add to the set_mux message. The end of the list is marked
  with count == 0.
 USB device identifier 
	 Gen 3 devices have an internal MSD mode switch that needs
	  to be disabled in order to access the full functionality of
	  the device.
	 Gen 3 devices without a mixer have a different
	  configuration set
 line out hw volume is sw controlled 
 support for mainalt speaker switching 
 support for talkback microphone 
	 the number of analogue inputs with a software switchable
	  level control that can be set to line or instrument
 the first input with a level control (0-based) 
	 the number of analogue inputs with a software switchable
	  10dB pad control
	 the number of analogue inputs with a software switchable
	  "air" control
 the number of phantom (48V) software switchable controls 
 the number of inputs each phantom switch controls 
	 the number of direct monitor options
	  (0 = none, 1 = mono only, 2 = monostereo)
	 remap analogue outputs; 18i8 Gen 3 has "line 34" connected
	  internally to the analogue 78 outputs
 additional description for the line out volume controls 
 number of sourcesdestinations of each port type 
 layoutorder of the entries in the set_mux message 
 prevent sending concurrent USB requests 
 lock access to this data 
 Model-specific data 
 Supported Gen 2 devices 
 Supported Gen 3 devices 
 End of list 
 get the starting port index number for a given port typedirection 
 USB Interactions 
 Notifications from the interface 
 Commands for sendingreceiving requestsresponses 
 volume status is read together (matches scarlett2_config_items[1]) 
 dimmute buttons 
 software volume setting 
 actual volume of output inc. dim (-18dB) 
 internal mute buttons 
 sw (0) or hw (1) controlled 
 front panel volume knob 
 Configuration parameters that can be read and written 
 Location, size, and activation command number for the configuration
  parameters. Size is in bits and may be 1, 8, or 16.
 scarlett2_config_items[0] is for devices without a mixer
  scarlett2_config_items[1] is for devices with a mixer
 Devices without a mixer (Solo and 2i2 Gen 3) 
 Devices with a mixer (Gen 2 and all other Gen 3) 
 proprietary requestresponse format 
 sequence must go up by 1 for each request 
 Send a proprietary format request to the Scarlett interface 
 build request message and send it 
 send a second message to get the response 
 validate the response 
	 cmdseqsize should match except when initialising
	  seq sent = 1, response = 0
 Send a USB message to get data; result placed in buf 
 Send a USB message to get configuration parameters; result placed in buf 
 For byte-sized parameters, retrieve directly into buf 
 For bit-sized parameters, retrieve into value 
 then unpack from value into buf[] 
 Send SCARLETT2_USB_DATA_CMD SCARLETT2_USB_CONFIG_SAVE 
 Delayed work to save config 
 Send a USB message to set a SCARLETT2_CONFIG_ parameter 
 Cancel any pending NVRAM save 
	 Convert config_item->size in bits to size in bytes and
	  calculate offset
	 If updating a bit, retrieve the old value, setclear the
	  bit as needed, and update value
 Send the configuration parameter data 
 Activate the change 
 Schedule the change to be written to NVRAM 
 Send a USB message to get sync status; result placed in sync 
 Send a USB message to get volume status; result placed in buf 
 Send a USB message to get the volumes for all inputs of one mix
  and put the values into private->mix[]
 Send a USB message to set the volumes for all inputs of one mix
  (values obtained from private->mix[])
 Convert a port number index (per info->port_count) to a hardware ID 
 Oops 
 Convert a hardware ID to a port number index 
 Oops 
 Convert one mux entry from the interface and load into private->mux[] 
 Send USB message to get mux inputs and then populate private->mux[] 
 Send USB messages to set mux inputs 
 set mux settings for each rate 
 i counts over the output array 
 loop through each entry 
 Empty slots 
			 Non-empty mux slots use the lower 12 bits
			  for the destination and next 12 bits for
			  the source
 Send USB message to get meter levels 
 copy, convert to u16 
 Control Functions 
 helper function to create a new control 
	 We set USB_MIXER_BESPOKEN type, so that the core USB mixer code
	  ignores them for resume and other operations.
	  Also, the head.id field is set to 0, as we don't use this field.
 Sync Control 
 Update sync control after receiving notification that the status
  has changed
 devices without a mixer also don't support reporting sync status 
 Analogue Line Out Volume Controls 
 Update hardware volume controls after receiving notification that
  they have changed
 max value 
 max value 
 Mute Switch Controls 
 Send mute change to the device 
 HWSW Volume Switch Controls 
 SetClear write bits 
 Notify of write bit and possible value change 
	 Change access mode to RO (hardware controlled volume)
	  or RW (software controlled volume)
 Reset volumemute to master volumemute 
 Set SW volume to current HW volume 
 Set SW mute to current HW mute 
 Send SWHW switch change to the device 
 Line LevelInstrument Level Switch Controls 
 Send switch change to the device 
 Pad Switch Controls 
 Send switch change to the device 
 Air Switch Controls 
 Send switch change to the device 
 Phantom Switch Controls 
 Send switch change to the device 
 Phantom Persistence Control 
 Send switch change to the device 
 Direct Monitor Control 
	 monitor_other_enable[0] enables speaker switching
	  monitor_other_enable[1] enables talkback
	 monitor_other_switch[0] activates the alternate speakers
	  monitor_other_switch[1] activates talkback
	 if it doesn't do speaker switching then it also doesn't do
	  talkback
 Send switch change to the device 
 Direct Monitor for Solo is mono-only and only needs a boolean control
  Direct Monitor for 2i2 is selectable between OffMonoStereo
 Speaker Switching Control 
 when speaker switching gets enabled, switch the mainalt speakers
  to HW volume and disable those controls
 switch the mainalt speakers to HW volume 
 disable the line out SWHW switch 
	 when the next monitor-other notify comes in, update the mux
	  configuration
 when speaker switching gets disabled, reenable the hwsw controls
  and invalidate the routing
 enable the line out SWHW switch 
	 when the next monitor-other notify comes in, update the mux
	  configuration
 enabledisable speaker switching 
 if speaker switching is enabled, select main or alt 
 update controls if speaker switching gets enabled or disabled 
 Talkback and Talkback Map Controls 
 enabledisable talkback 
 if talkback is enabled, select main or alt 
 Send updated bitmap to the device 
 DimMute Controls 
 Send switch change to the device 
 Create the analogue output controls 
 Add RO HW volume control 
 Add volume controls 
 Fader 
 Mute Switch 
		 Make the fader and mute controls read-only if the
		  SWHW switch is set to HW
 SWHW Switch 
			 Make the switch read-only if the line is
			  involved in speaker switching
 Add dimmute controls 
 Create the analogue input controls 
 Add input level (lineinst) controls 
 Add input pad controls 
 Add input air controls 
 Add input phantom controls 
 Mixer Volume Controls 
 max value 
 Mux Source Selection Controls 
 Meter Controls 
 devices without a mixer also don't support reporting levels 
 MSD Controls 
 Send switch change to the device 
 If MSD mode is off, hide the switch by default 
 Add MSD control 
 CleanupSuspend Callbacks 
 Initialisation 
 Look through the interface descriptors for the Focusrite Control
  interface (bInterfaceClass = 255 Vendor Specific Class) and set
  bInterfaceNumber, bEndpointAddress, wMaxPacketSize, and bInterval
  in private
 Initialise private data 
 Cargo cult proprietary initialisation sequence 
 step 0 
 step 1 
 step 2 
 Read configuration from the interface on start 
 no other controls are created if MSD mode is on 
 the rest of the configuration is for devices with a mixer 
 Notify on sync change 
 Notify on monitor change 
 if line_out_hw_vol is 0, there are no controls to update 
 Notify on dimmute change 
 Notify on "input other" change (levelpadair) 
 Notify on "monitor other" change (direct monitor, speaker
  switching, talkback)
	 if speaker switching was recently enabled or disabled,
	  invalidate the dimmute and mux enum controls
 Interrupt callback 
 Find device in scarlett2_devices 
 Initialise private data 
 Send proprietary USB initialisation sequence 
 Read volume levels and controls from the interface 
 Create the MSD control 
 If MSD mode is enabled, don't create any other controls 
 Create the analogue output controls 
 Create the analogue input controls 
 Create the input, output, and mixer mux input selections 
 Create the matrix mixer controls 
 Create the level meter controls 
 Create the sync control 
 Create the direct monitor control 
 Create the speaker switching control 
 Create the talkback controls 
 Set up the interrupt polling 
 only use UAC_VERSION_2 
 SPDX-License-Identifier: GPL-2.0-or-later
  handle the quirks for the contained interfaces
  create a stream for an interface with proper descriptors
 reset the current interface 
 create the audio stream and the corresponding endpoints from the fixed
  audioformat object; this is used for quirks with the fixed EPs
  create a stream for an endpointaltsetting without proper descriptors
 unlink for avoiding double-free 
	
	  Most RolandYamaha audio streaming interfaces have more or less
	  standard descriptors, but older devices might lack descriptors, and
	  future ones might change, so ensure that we fail silently if the
	  interface doesn't look exactly right.
 must have a non-zero altsetting for streaming 
 must have an isochronous endpoint for streaming 
 must have format descriptors 
 must have some valid jack descriptors 
 might have a vendor-specific descriptor <06 24 F1 02 ...> 
 must have the MIDIStreaming interface header descriptor
 must have the MIDIStreaming endpoint descriptor
 must have at least one bulkinterrupt endpoint for streaming 
 Yamaha 
 Roland 
	
	  ALSA PCM playbackcapture devices cannot be registered in two steps,
	  so we have to claim the other corresponding interface here.
  Create a stream for an Edirol UA-700UA-25UA-4FX interface.  
  The only way to detect the sample rate is by looking at wMaxPacketSize.
 both PCM and MIDI interfaces have 2 or more altsettings 
 unlink for avoiding double-free 
  Create a standard mixer for the specified interface.
  audio-interface quirks
  returns zero if no standard audioMIDI parsing is needed.
  returns a positive value if standard audiomidi interfaces are parsed
  after this.
  returns a negative value at error.
  boot quirks
 Send message to force it to reconnect with full interface. 
 quit this anyway 
		 This function has to be available by the usb core module.
		  if it is not avialable the boot quirk has to be left out
		  and the configuration has to be set by udev or hotplug
		  rules
		 Always return an error, so that we stop creating a device
		   that will just be destroyed and recreated with a new
  C-Media CM106CM106+ have four 16-bit internal registers that are nicely
  documented in the device's data sheet.
	
	  Enable line-out driver mode, set headphone source to front
	  channels, enable stereo mic.
  CM6206 registers from the CM6206 datasheet rev 2.1
 Bit 4 thru 11 is the SPDIF category code 
 Bit 11..13 sets the sensitivity to FLY tuner volume control VPVD signal 
		
		  Values here are chosen based on sniffing USB traffic
		  under Windows.
		 
		  REG0: DAC is master, sample rate 48kHz, no copyright
		
		  REG1: PLL binary search enable, soft mute enable.
		
		  REG2: enable output drivers,
		  select front channels to the headphone output,
		  then mute the headphone channels, run the MCU
		  at 1.5 MHz.
		
		  REG3: default flyspeed, set 2.5V mic bias
		  enable all line out ports and enable SPDIF
 REG4 is just a bunch of GPIO lines 
 REG5: de-assert ADDA reset signals 
 quirk for Plantronics GameCom 780 with CM6302 chip 
	 set the initial volume and don't change; other values are either
	  too loud or silent due to firmware bug (bko#65251)
  Novation Twitch DJ controller
  Focusrite Novation Saffire 6 USB audio card
	 preemptively set up the device because otherwise the
  This call will put the synth in "USB send" mode, i.e it will send MIDI
  messages through USB (this is disabled at startup). The synth will
  acknowledge by sending a sysex on endpoint 0x85 and by displaying a USB
  sign on its LCD. Values here are chosen based on sniffing USB traffic
  under Windows.
 "midi send" enable 
  Some sound cards from Native Instruments are in fact compliant to the USB
  audio standard of version 2 and other approved USB standards, even though
  they come up as vendor-specific device when first connected.
  However, they can be told to come up with a new set of descriptors
  upon their next enumeration, and the interfaces announced by the new
  descriptors will then be handled by the kernel's class drivers. As the
  product ID will also change, no further checks are required.
	 return -EAGAIN, so the creation of an audio interface for this
	  temporary device is aborted. The device will reconnect with a
 Choose 48000Hz permanently 
 Send the magic! 
 Digidesign Mbox 2 needs to load firmware onboard
  and driver must wait a few seconds for initialisation.
 Hard coded into the device 
 Hard coded into the device 
 0.5 second delay 
 Control magic - load onboard firmware 
 Successful boot 
	 If the Axe-Fx III has not fully booted, it will timeout when trying
	  to enable the audio streaming interface. A more generous timeout is
	  used here to detect when the Axe-Fx III has finished booting as the
	  set interface message will be acked once it has
 First we tell the device which sample rate to use. 
 Then we poll every 100 ms until the device informs of its readiness. 
		 the device signals its readiness through a message of the
		  form
		            XX 06 00 00 00 00 0b 18  00 00 00 01
		  If the device is not yet ready to accept audio data, the
		  last byte of that sequence is 00.
  Setup quirks
 parse device_setup 
 use only "win-compatible" interfaces 
 enable DTS Digital Output 
 48-96kHz rate if set, 8-48kHz otherwise 
 24bits sample if set, 16bits otherwise 
 enable Digital Input 
 bit mask for setup value 
 24bits+48kHz+Digital Input 
 24bits+48kHz+No Digital Input 
 16bits+48kHz+Digital Input 
 16bits+48kHz+No Digital Input 
	 Reset ALL ifaces to 0 altsetting.
	  Call it for every possible altsetting of every interface.
 skip all interfaces but 1 and 2 
 skip interfaces 1 and 2 
 skip this altsetting 
 skip this altsetting 
 skip this altsetting 
 skip this altsetting 
 keep this altsetting 
	 Reset ALL ifaces to 0 altsetting.
	  Call it for every possible altsetting of every interface.
 skip this altsetting 
 skip this altsetting 
 skip this altsetting 
 skip this altsetting 
 skip this altsetting 
 skip this altsetting 
 keep this altsetting 
	 Reset ALL ifaces to 0 altsetting.
	  Call it for every possible altsetting of every interface.
	 possible configuration where both inputs and only one output is
	 used is not supported by the current setup
 no analog input 
 enable only altsets 2 and 5 
 disable digialt input 
 enalbe only altsets 2 and 5 
 keep only 16-Bit mode 
 keep this altsetting 
	
	  Altno settings:
	 
	  Playback (Interface 1):
	  1: 6 Analog + 2 SPDIF
	  2: 6 Analog + 2 SPDIF
	  3: 6 Analog
	 
	  Capture (Interface 2):
	  1: 8 Analog + 2 SPDIF + 8 ADAT
	  2: 8 Analog + 2 SPDIF + 4 ADAT
	  3: 8 Analog
	
	  I'll leave 2 as the default one and
	  use device_setup to switch to the
	  other two.
 audiophile usb: skip altsets incompatible with device_setup 
 quattro usb: skip altsets incompatible with device_setup 
 fasttrackpro usb: skip altsets incompatible with device_setup 
 presonus studio 1810c: skip altsets incompatible with device_setup 
 SB Extigy needs special boot-up sequence 
 if more models come, this will go to the quirk list. 
 SB Audigy 2 NX needs its own boot-up magic, too 
 C-Media CM106  Turtle Beach Audio Advantage Roadie 
 C-Media CM6206  CM106-Like Sound Device 
 Terratec Aureon 7.1 USB 
 Digidesign Mbox 2 
 Focusrite Novation Saffire 6 USB 
 Focusrite Novation Twitch 
 Access Music VirusTI Desktop 
 Komplete Audio 6 
 Traktor Audio 6 
 Traktor Audio 10 
 M-Audio Fast Track Pro USB 
 Plantronics Gamecom 780 
 Fractal Audio Axe-Fx 3 
 MOTU MicroBook II 
		
		  For some reason interface 3 with vendor-spec class is
		  detected on MicroBook IIc.
 MOTU M Series 
  check if the device uses big-endian samples
 it depends on altsetting whether the device is big-endian or not 
 M-Audio Quattro: captured data only 
 M-Audio Audiophile USB 
 M-Audio Fast Track Pro 
  For E-Mu 0404USB0202USBTrackerPre0204 sample rate should be set for device,
  not for interface.
	 When capture is active
	  sample rate shouldn't be changed
	  by playback substream
 Convert to little endian
 we should derive windex from fmt-sync_ep but it's not set
 E-Mu 0202 USB 
 E-Mu 0404 USB 
 E-Mu Tracker Pre 
 E-Mu 0204 USB 
 MacroSilicon MS2109 
 Pioneer DJM-450 
 Pioneer DJM-750 
 Pioneer DJM-850 
		 First switch to alt set 0, otherwise the mode switch cmd
		  will not be accepted by the DAC
 Delay needed after setting the interface 
 Vendor mode switch cmd is required. 
 DSD mode (DSD_U32) requested 
 PCM or DOP mode (S32) requested 
 PCM mode (S16) requested 
	
	  "Playback Design" products send bogus feedback data at the start
	  of the stream. Ignore them.
	
	  M-Audio Fast Track C400C600 - when packets are not skipped, real
	  world latency varies by approx. +- 50 frames (at 96kHz) each time
	  the stream is (re)started. When skipping packets 16 at endpoint
	  start up, the real world latency is stable within +- 1 frame (also
	  across power cycles).
 Work around devices that report unreasonable feedback data 
 TEAC UD-H01 
 T+A Dac8 
 quirk applied after snd_usb_ctl_msg(); not applied during boot quirks 
  snd_usb_interface_dsd_format_quirks() is called from format.c to
  augment the PCM format bit-field for DSD types. The UAC standards
  don't have a designated bit field to denote DSD-capable interfaces,
  hence all hardware that is known to support this format has to be
  listed here.
 Playback Designs 
 XMOS based USB DACs 
 AURALiC VEGA 
 LH Labs VI DAC Infinity 
 Pro-Ject Pre Box S2 Digital 
 Hegel HD12 DSD 
 The Bit Opus #3; with fp->dsd_raw 
 NuPrime DAC-10 
 Encore mDSD 
 Furutech ADL Stratos 
 NuPrime Audio DAC-9 
 Bryston BDA3 
 HDTA Serenade DSD 
 M2Tech Young MkIII 
 PS Audio NuWave DAC 
 Audiolab M-DAC+ 
 W4S DAC-2v2SE 
 Mutec MC3+ USB 
 MSB Technology 
 Amanero Combo384 USB based DACs with native DSD support 
 Amanero - Combo384 
 T+A DAC8DSD-V2.0, MP1000E-V2.0, MP2000R-V2.0, MP2500R-V2.0, MP3100HV-V2.0 
 T+A USB HD Audio 1 
 T+A USB HD Audio 2 
 ITF-USB DSD based DACs 
		 Altsetting 2 support native DSD if the num of altsets is
		  three (0-2),
		  Altsetting 3 support native DSD if the num of altsets is
		  four (0-3).
 Mostly generic method to detect many DSD-capable implementations 
 AudioTrak Optoplay 
		 Optoplay sets the sample rate attribute although
		  it seems not supporting it in fact.
 Creative SB Audigy 2 NX 
 M-Audio Audiophile USB 
 doesn't set the sample rate attribute, but supports it 
 M-Audio Quattro USB 
 M-Audio Fast Track Pro USB 
 plantronics headset 
	case USB_ID(0x077d, 0x07af):  Griffin iMic (note that there is
	
	  plantronics headset and Griffin iMic have set adaptive-in
	  although it's really not...
 MOTU MicroBook IIc 
		
		  MaxPacketsOnly attribute is erroneously set in endpoint
		  descriptors. As a result this card produces noise with
		  all sample rates other than 96 kHz.
 Jieli Technology USB PHY 2.0 
 mic works only when ep packet size is set to wMaxPacketSize 
  registration quirk:
  the registration is skipped if a device matches with the given ID,
  unless the interface reaches to the defined one.  This is for delaying
  the registration until the last known interface, so that the card and
  devices appear at the same time.
 composed via USB_ID() 
 the interface to trigger register 
 Kingston HyperX AMP 
 Kingston HyperX Cloud Alpha S 
 Kingston HyperX Cloud Flight S 
 JBL Quantum 600 
 JBL Quantum 800 
 JBL Quantum 400 
 JBL Quantum 400 
 JBL Quantum 600 
 JBL Quantum 800 
 terminator 
 return true if skipping registration 
 Register as normal 
  driver behavior quirk flags
 Device matches 
 Creative SB Extigy 
 Creative Live Cam VF0610 
 Logitech ConferenceCam Connect 
 Logitech QuickCam Pro 
 Logitech QuickCam E 3500 
 Benchmark DAC1 Pre 
 Samsung USBC Headset (AKG) 
 Sony WALKMAN NW-A45 DAC 
 Phoenix Audio TMX320VC 
 ELP HD USB Camera 
 Bose Companion 5 
 Syntek STK1160 
 Hauppauge Woodbury 
 TEAC UD-501UD-501V2UD-503NT-503 
 Esoteric D-05X 
 TEAC UD-301 
 Hercules DJ Console (Windows Edition) 
 Hercules DJ Console (Macintosh Edition) 
 Outlaw RR2150 (Micronas UAC3553B) 
 LineX FM Transmitter 
 Kingston HyperX 
 Jabra 550a 
 Hauppauge HVR-950Q 
 Sennheiser DECT 
 Serato Phono 
 Denon DCD-1500RE 
 Denon DA-300USB 
 Marantz HD-DAC1 
 Marantz SA-14S1 
 Denon DN-X1600 
 Zoom R1624 
 Lenovo ThinkStation P620 Rear Line-in, Line-out and Microphone 
 Lenovo ThinkStation P620 Internal Speaker + Front Headset 
 Luxman DA-06 
 GE B850V3 CP2114 audio interface 
 Hauppauge HVR-950Q 
 Hauppauge HVR-950Q-MXL 
 Hauppauge HVR-950Q 
 Hauppauge HVR-950Q-MXL 
 Hauppauge HVR-950Q 
 Hauppauge HVR-950Q 
 Hauppauge HVR-950Q 
 Hauppauge HVR-950Q 
 Hauppauge HVR-950Q 
 Hauppauge HVR-850 
 Hauppauge HVR-950Q 
 Hauppauge HVR-950Q 
 Hauppauge HVR-950Q 
 Hauppauge HVR-950Q-MXL 
 Hauppauge Woodbury 
 AudioQuest DragonFly 
 Audient iD14 
 Audioengine D1 
 Schiit Hel 
 Dell AE515 sound bar 
 MacroSilicon MS2109 
 Jieli Technology USB PHY 2.0 
 Vendor matches 
 MS Lifecam 
 Logitech 
 Plantronics 
 TEAC Corp. 
 MOTU 
 Focusrite Novation 
 Thesycon devices 
 Phoenix Audio 
 XMOS based devices 
 Oppo 
 Playback Design 
 Mytek devices 
 Rotel? 
 GustardEss based devices 
 FiiO devices 
 T+A devices 
 Khadas devices 
 EVGA 
 HiBy devices 
 terminator 
 SPDX-License-Identifier: GPL-2.0-or-later
 Special handling for implicit feedback mode
 generic playback + capture (for BOSS) 
 Implicit feedback quirk table for playback 
 Generic matching 
 Steinberg UR22 
 M-Audio FastTrack Ultra 
 M-Audio FastTrack Ultra 
 M-Audio Fast Track C400 
 M-Audio Fast Track C600 
 Fixed EP 
 FIXME: check the availability of generic matching 
 Fractal Audio Axe-Fx III 
 Solid State Logic SSL2 
 Solid State Logic SSL2+ 
 Steinberg UR22C 
 RTX6001 
 Allen&Heath Qu-16 
 Zoom UAC-2 
 Fractal Audio Axe-Fx II 
 Yamaha MODX 
 Special matching 
 MicroBook IIc 
 ep = 0x84, ifnum = 0 
 MOTU MicroBook II 
 terminator 
 Implicit feedback quirk table for capture: only FIXED type 
 terminator 
 set up sync EP information on the audioformat 
 Check whether the given UAC2 iface:altset points to an implicit fb source 
 Like the UAC2 case above, but specific to Roland with vendor class and hack 
 only when both streams are with ASYNC type 
 check capture EP 
 capture quirk for Roland device; always full-duplex 
 Playback and capture EPs on Pioneer devices share the same ifacealtset
  for the implicit feedback operation
 More generic quirk: look for the sync EP next to the data EP 
 Setup an implicit feedback endpoint from a quirk. Returns 0 if no quirk
  applies. Returns 1 if a quirk was found.
 No quirk 
 Special handling for devices with capture quirks 
 no quirk 
 Generic UAC2 implicit feedback 
 RolandBOSS implicit feedback with vendor spec class 
 Pioneer devices with vendor spec class 
 Try the generic implicit fb if available 
 No quirk 
 same for capture, but only handling FIXED entry 
 RolandBOSS need full-duplex streams 
 skip the quirk, also don't handle generic sync EP 
  Parse altset and set up implicit feedback endpoint on the audioformat
  Return the score of matching two audioformats.
  Veto the audioformat if:
  - It has no channels for some reason.
  - Requested PCM format is not supported.
  - Requested sample rate is not supported.
  Return the audioformat that is suitable for the implicit fb
 Use the original audioformat as fallback for the shared altset 
 SPDX-License-Identifier: GPL-2.0
    UAC3 Power Domain state management functions
 SPDX-License-Identifier: GPL-2.0-or-later
    Tascam US-16x08 ALSA driver
    Copyright (c) 2016 by Detlef Urban (onkel@paraair.de)
 USB control message templates 
 input from master (0x02) or input from computer bus (0x03) 
 input index (0x010x02 eq. leftright) or bus (0x01-0x08) 
 output index (0x01-0x08) 
 default message head, equal to all mixers 
 0x06: Controller ID 
 0x07:  
 0x08: Value of common mixer 
 default message head, equal to all mixers 
 0x06: Controller ID 
                    0x07:  
                    0x08: Value of common mixer 
 onoff flag 
 onoff flag 
 default message head, equal to all mixers 
 0x08: Threshold db (8) (e0 ... 00) (+-0dB -- -32dB) x-32 
 0x0b: Ratio (0a,0b,0d,0f,11,14,19,1e,23,28,32,3c,50,a0,ff)  
 0x0e: Attack (0x02 ... 0xc0) (2ms ... 200ms) 
 0x11: Release (0x01 ... 0x64) (10ms ... 1000ms) x10  
 0x14: gain (0 ... 20) (0dB .. 20dB) 
 0x1a: main Comp switch (0 ... 1) (off ... on)) 
 default message head, equal to all mixers 
                0x06: Controller ID  
 0x08: EQ set num (0x01..0x04) (LOW, LOWMID, HIGHMID, HIGH)) 
 0x0b: value dB (0 ... 12) (-12db .. +12db)  x-6 
 0x0e: value freq (32-47) (1.7kHz..18kHz) 
 0x11: band width (0-6) (Q16-Q0.25)  2^x4 (EQ xxMID only) 
 0x14: main EQ switch (0 ... 1) (off ... on)) 
 compressor ratio map 
 route enumeration names 
 wrapper function to send prepared URB buffer to usb device. Return an error
  code if something went wrong
 route has no bias 
  get the new value (no bias for routes) 
 sanity check 
 prepare the message buffer from template 
 input comes from a master channel 
 input comes from a computer channel 
 place new route selection in URB message 
 place route selector in URB message 
 new control value incl. bias
 sanity check 
 prepare the message buffer from template 
 place channel selector in URB message 
 prepare the message buffer from template 
 gets a current mixer value from common store 
 sanity check 
 prepare URB message from template 
 add the bias to the new value 
 sanity check 
 new control value incl. bias
 prepare compressor URB message from template  
 place comp values in message buffer watch bias! 
 place channel selector in message buffer 
 get low switch from cache is enough, cause all bands are together 
 new control value incl. bias
 prepare URB message from EQ template 
 place channel index in URB message 
 all four EQ bands have to be enableddisabled in once 
 sanity check 
 copy URB buffer from EQ template 
 place channel index in URB buffer 
 place EQ band in URB buffer 
 store new value in EQ band cache 
 calculate compressor index for reduction level request 
 any channel active 
 check for stereo link 
 reset comp_index to left channel
 no stereo link 
 skip channels with no compressor active 
 retrieve the meter level values from URB message 
 Function to retrieve current meter values from the device.
  The device needs to be polled for meter values with an initial
  requests. It will return with a sequence of different meter value
  packages. The first request (case 0:) initiate this meter response sequence.
  After the third response, an additional request can be placed,
  to retrieve compressor reduction level value for given channel. This round
  trip channel selector will skip all inactive compressors.
  A mixer can interrupt this round-trip by selecting one ore two (stereo-link)
  specific channels.
 sanity check 
max
 control store preparation 
 setup compressor store and assign default value 
 setup EQ store and assign default values 
 EQ Low 
 EQ Mid low 
 EQ Mid High 
 EQ High 
 release elem->private_free as well; called only once for each _store 
 table of EQ controls 
 EQ switch 
 EQ low gain 
 EQ low freq 
 EQ mid low gain 
 EQ mid low freq 
 EQ mid low Q 
 EQ mid high gain 
 EQ mid high freq 
 EQ mid high Q 
 EQ high gain 
 EQ low freq 
 table of compressor controls 
 Comp enable 
 Comp threshold 
 Comp ratio 
 Comp attack 
 Comp release 
 Comp gain 
 table of channel controls 
 Phase 
 Fader 
 Mute 
 Pan 
 table of master controls 
 Master 
 Bypass 
 Buss out 
 Master mute 
 just check for non-MIDI interface 
 add routing control 
 create compressor mixer elements 
 add master controls 
 release comp_store only once 
 add channel controls 
 create eq store 
 add EQ controls 
 release eq_store only once 
 add compressor controls 
 create meters store 
		 meter function 'get' must access to compressor store
		  so place a reference here
  usbmidi.c - ALSA USB MIDI driver
  Copyright (c) 2002-2009 Clemens Ladisch
  All rights reserved.
  Based on the OSS usb-midi driver by NAGANO Daisuke,
           NetBSD's umidi driver by Takuya SHIOZAKI,
           the "USB Device Class Definition for MIDI Devices" by Roland
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions
  are met:
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions, and the following disclaimer,
     without modification.
  2. The name of the author may not be used to endorse or promote products
     derived from this software without specific prior written permission.
  Alternatively, this software may be distributed andor modified under the
  terms of the GNU General Public License as published by the Free Software
  Foundation; either version 2 of the License, or (at your option) any later
  version.
  THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
  ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
  OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  SUCH DAMAGE.
  define this to log all USB packets
 #define DUMP_PACKETS 
  how long to wait after some USB errors, so that hub_wq can disconnect() us
  without too many spurious errors
 size of urb buffer 
 cable number << 4 
  Submits the URB, with error handling.
  Error handling for URB completion functions.
 manually unlinked, or device gone 
 errors that might occur during unplugging 
 continue 
  Receives a chunk of MIDI data.
 nothing 
  Processes the data read from the device.
  This is called when some data should be transferred to the device
  (from one or more substreams).
 called after transfers had been interrupted due to some USB error 
 helper function to send static data that may not DMA-able 
  Standard USB MIDI protocol: see the spec.
  Midiman protocol: like the standard protocol, but the control byte is the
  fourth byte in each packet, and uses length instead of CIN.
  Buggy M-Audio device: running status on input results in a packet that has
  the data bytes but not the status byte and that is marked with CIN 4.
 realtime msg: no running status change 
 channel msg 
 CIN 4 that is not a SysEx 
				
				  All other msgs cannot begin running status.
				  (A channel msg sent as two or three CIN 0xF
				  packets could in theory, but this device
				  doesn't use this format.)
  QinHeng CH345 is buggy: every second packet inside a SysEx has not CIN 4
  but the previously seen CIN, but still with three data bytes.
			
			  Either a corrupted running status or a real note-on
			  message; impossible to detect reliably.
  CME protocol: like the standard protocol, but SysEx commands are sent as a
  single USB packet preceded by a 0x0F byte.
  Adds one USB MIDI packet to the output buffer.
  Adds one Midiman packet to the output buffer.
  Converts MIDI commands to USB MIDI packets.
 b < 0x80 
 FIXME: lower-numbered ports can starve higher-numbered ports 
  AKAI MPD16 protocol:
  For control port (endpoint 1):
  ==============================
  One or more chunks consisting of first byte of (0x10 | msg_len) and then a
  SysEx message (msg_len=9 bytes long).
  For data port (endpoint 2):
  ===========================
  One or more chunks consisting of first byte of (0x20 | msg_len) and then a
  MIDI message (msg_len bytes long)
  Messages sent: Active Sense, Note On, Poly Pressure, Control Change.
 only try adding more data when there's space for at least 1 SysEx 
 try to skip non-SysEx data 
 look for the start or end marker 
 next SysEx started before the end of current one 
 it's incomplete - drop it 
 SysEx complete 
 queue it, ack it, and get the next one 
 less than 9 bytes and no end byte - wait for more 
 9 bytes and no end marker in sight - malformed, skip it 
  Novation USB MIDI protocol: number of data bytes is in the first byte
  (when receiving) (+1!) or in the second byte (when sending); data begins
  at the third byte.
  "raw" protocol: just move raw MIDI bytes fromto the endpoint
  FTDI protocol: raw MIDI bytes, but input packets have two modem status bytes.
  Emagic USB MIDI protocol: raw MIDI with "F5 xx" port switching.
 initialization magic: "get version" 
 Emagic 
 Unitor8 
 version number request 
 command version 
 EEPROM, box 0 
 while we're at it, pour on more magic 
 switch to patch mode with last preset 
 Emagic 
 Unitor8 
 patch switch command 
 command version 
 to all boxes 
 last preset in EEPROM 
 FF indicates end of valid data 
 handle F5 at end of last buffer 
 determine size of data until next F5 
 assert(buffer[0] == 0xf5); 
 round-robin, starting at the last current port 
			 gobble up remaining bytes to prevent wait in
	
	  The substream buffer is empty, but some data might still be in the
	  currently active URBs, so we have to wait for those to complete.
  Frees an input endpoint.
  May be called when ep hasn't been initialized completely.
  Creates an input endpoint.
  Frees an output endpoint.
  May be called when ep hasn't been initialized completely.
  Creates an output endpoint, and initializes output ports.
		
		  Various chips declare a packet size larger than 4 bytes, but
		  do not actually work with larger packets:
 Medeli DD305 
 ESI M4U 
 RedOctane GH MIDI INTERFACE 
 Textech USB Midi Cable 
 Textech USB Midi Cable 
 QinHeng CH345 "USB2.0-MIDI" 
 Unknown vendor Cable 
		
		  Some devices only work with 9 bytes packet size:
 Tascam US-122L 
 Tascam US-144 
  Frees everything.
  Unlinks all URBs (must be done before the usb_device is deleted).
	
	  an URB's completion handler may start the timer and
	  a timer may submit an URB. To reliably break the cycle
	  a flag under lock must be used
 free endpoints here; later call can result in Oops 
  This list specifies names for ports that do not fit into the standard
  "(product) MIDI (n)" schema because they aren't external MIDI ports,
  such as internal control or synthesizer ports.
 Yamaha MOTIF XF 
 Roland UA-100 
 Roland SC-8850 
 Roland U-8 
 Roland SC-8820 
 Roland SK-500 
 Roland SC-D70 
 Edirol UM-880 
 Edirol SD-90 
 Edirol UM-550 
 Edirol SD-20 
 Edirol SD-80 
 Edirol UA-700 
 Roland VariOS 
 Edirol PCR 
 BOSS GS-10 
 Edirol UA-1000 
 Edirol UR-80 
 Edirol PCR-A 
 BOSS GT-PRO 
 Edirol UM-3EX 
 Roland VG-99 
 Cakewalk Sonar V-Studio 100 
 Roland VB-99 
 Roland A-PRO 
 Roland SD-50 
 Roland OCTA-CAPTURE 
 Roland SPD-SX 
 Roland A-Series 
 Roland INTEGRA-7 
 M-Audio MidiSport 8x8 
 MOTU Fastlane 
 Emagic Unitor8AMT8MT4 
 Akai MPD16 
 Access Music Virus TI 
 TODO: read port flags from descriptors 
 in jacks connect to outs 
 and out jacks connect to ins 
  Creates the endpoints and their ports.
  Returns MIDIStreaming device capabilities.
				
				  Low speed bulk transfers don't exist, so
				  force interrupt transfers for devices like
				  ESI MIDI Mate that try to use them anyway.
  On Roland devices, use the second alternate setting to be able to use
  the interrupt input endpoint.
        If either or both of the endpoints support interrupt transfer,
         then use the alternate setting
  Try to find any usable endpoints in the interface.
  Detects the endpoints for one-port-per-endpoint protocols.
  Detects the endpoints and ports of Yamaha devices.
	
	  For each port there is one MIDI_INOUT_JACK descriptor, not
	  necessarily with any useful contents.  So simply count 'em.
  Detects the endpoints and ports of Roland devices.
	
	  Some devices have a descriptor <06 24 F1 02 <inputs> <outputs>>,
	  some have standard class descriptors, or both kinds, or neither.
  Creates the endpoints and their ports for Midiman devices.
	
	  The various MidiSport devices have more or less random endpoint
	  numbers, so we have to identify the endpoints by their index in
	  the descriptor array, like the driver for that other OS does.
	 
	  There is one interrupt input endpoint for all input ports, one
	  bulk output endpoint for even-numbered ports, and one for odd-
	  numbered ports.  Both bulk output endpoints have corresponding
	  input bulk endpoints (at indices 1 and 3) which aren't used.
 prevent trying to find jack ,
 prevent trying to find jack ,
  Temporarily stop input.
  Resume input after a call to snd_usbmidi_input_stop().
  Prepare for suspend. Typically called from the USB suspend callback.
  Resume. Typically called from the USB resume callback.
  Creates and registers everything needed for a MIDI streaming interface.
 detect the endpoint(s) to use 
 M-Audio Uno 
		
		  Interface 1 contains isochronous endpoints, but with the same
		  numbers as in interface 0.  Since it is interface 1 that the
		  USB core has most recently seen, these descriptors are now
		  associated with the endpoint numbers.  This will foul up our
		  attempts to submit bulkinterrupt URBs to the endpoints in
		  interface 0, so we have to make sure that the USB core looks
		  again at interface 0 by calling usb_set_interface() on it.
 MOTU Fastlane 
 endpoint 1 is input-only 
 set baud rate to 31250 (48 MHz  16  96) 
 create rawmidi device 
 create endpointport structures 
 SPDX-License-Identifier: GPL-2.0-or-later
 return the estimated delay based on USB frame counters 
	
	  HCD implementations use different widths, use lower 8 bits.
	  The delay will be managed up to 256ms, which is more than
	  enough
	 Approximation based on number of samples per USB frame (ms),
  return the current pcm pointer.  just based on the hwptr_done value.
  find a matching audio format
		 avoid async out and adaptive in if the other method
		  supports the same format.
		  this is a workaround for the case like
		  M-audio audiophile USB.
 find the format with the largest max. packet size 
  initialize the pitch control and sample rate
 if endpoint doesn't have pitch control, bail out 
 PCM sync_stop callback 
 Set up sync endpoint 
 matched 
	
	  Generic sync EP handling
	
	  In case of illegal SYNC_NONE for OUT endpoint, we keep going to see
	  if we don't find a sync endpoint, as on M-Audio Transit. In case of
	  error fall back to SYNC mode and don't create sync endpoint
 check sync-pipe endpoint 
	 ... and check descriptor size before accessing bSynchAddress
	   because there is a version of the SB Audigy 2 NX firmware lacking
 stop any running stream beforehand 
  hw_params callback
  allocate a buffer and set the given audio format.
  so far we use a physically linear buffer although packetize transfer
  doesn't need a continuous area.
  if sg buffer is supported on the later version of alsa, we'll follow
  that.
  hw_free callback
  reset the audio format and release the buffer
 check whether early start is needed for playback stream 
 disabled via module option? 
 free-wheeling mode? (e.g. dmix) 
 implicit feedback mode has own operation mode 
  prepare callback
  only a few subtle things...
 reset the pointer 
  hw constraints
 check the format 
 check the channels 
 check the rate is within the range 
 check whether the period time is >= the data packet interval 
 get the EP or the sync EP for implicit fb when it's already set up 
			 if EP is already opened solely for this substream,
			  we still allow us to change the parameter; otherwise
			  this substream has to follow the existing parameter
 for the implicit fb, check the sync ep as well 
 additional hw constraints for implicit feedback mode 
  set up the runtime hardware information.
 check minmax rates and channels 
 FIXME: there might be more than one audio formats... 
 full speed devices have fixed data packet interval 
 if period time doesn't go below 1 ms, no rules needed 
 additional hw constraints for implicit fb 
 need an explicit sync to catch applptr update in low-latency mode 
 runtime PM is also done there 
 initialize DSDDOP context 
 Since a URB can handle only a single linear buffer, we must use double
  buffering when the data to be transferred overflows the buffer boundary.
  To avoid inconsistencies when updating hwptr_done, we use double buffering
  for all URBs.
 read frame number here, update pointer in critical section 
 continue;
 update the current pointer 
 realign last_frame_number 
 copy a data chunk 
	
	  The DSP DOP format defines a way to transport DSD samples over
	  normal PCM data endpoints. It requires stuffing of marker bytes
	  (0x05 and 0xfa, alternating per sample frame), and then expects
	  2 additional bytes of actual payload. The whole frame is stored
	  LSB.
	 
	  Hence, for a stereo transport, the buffer layout looks like this,
	  where L refers to left channel samples and R to right.
	 
	    L1 L2 0x05   R1 R2 0x05   L3 L4 0xfa  R3 R4 0xfa
	    L5 L6 0x05   R5 R6 0x05   L7 L8 0xfa  R7 R8 0xfa
	    .....
	 
 frame boundary? 
 alternate the marker 
 stuff the DSD payload 
 copy bit-reversed bytes onto transfer buffer 
 err, the transferred area goes over buffer boundary. 
 Put __le32 length descriptor at start of each packet. 
 Adjust transfer size accordingly. 
 calculate the byte offset-in-buffer of the appl_ptr 
 set up descriptor 
					 FIXME: fill-max mode is not
 add a transfer delimiter 
 finish at the period boundary or after enough frames 
 usual PCM 
 bytes is now amount of outgoing data 
		 this is the first actual URB submitted,
		  update trigger timestamp to reflect actual start time
  process after playback data complete
  - decrease the delay count again
 PCM ack callback for the playback stream;
  this plays a role only when the stream is running in low-latency mode.
	 When no more in-flight URBs available, try to process the pending
	  outputs here
 keep retire_data_urb for delay calculation 
 SPDX-License-Identifier: GPL-2.0-or-later
    Scarlett Driver for ALSA
    Copyright (c) 2013 by Tobias Hoffmann
    Copyright (c) 2013 by Robin Gareus <robin at gareus.org>
    Copyright (c) 2002 by Takashi Iwai <tiwai at suse.de>
    Copyright (c) 2014 by Chris J Arges <chris.j.arges at canonical.com>
    Many codes borrowed from audio.c by
 	    Alan Cox (alan at lxorguk.ukuu.org.uk)
 	    Thomas Sailer (sailer at ife.ee.ethz.ch)
    Code cleanup:
    David Henningsson <david.henningsson at canonical.com>
  Rewritten and extended to support more models, e.g. Scarlett 18i8.
  Auto-detection via UAC2 is not feasible to properly discover the vast
  majority of features. It's related to both LinuxALSA's UAC2 as well as
  Focusrite's implementation of it. Eventually quirks may be sufficient but
  right now it's a major headache to work around these things.
  NB. Neither the OSX nor the win driver provided by Focusrite performs
  discovery, they seem to operate the same as this driver.
 Mixer Interface for the Focusrite Scarlett 18i6 audio interface.
  The protocol was reverse engineered by looking at communication between
  Scarlett MixControl (v 1.2.128.0) and the Focusrite(R) Scarlett 18i6
  (firmware v305) using wireshark and usbmon in January 2013.
  Extended in July 2013.
  this mixer gives complete access to all features of the device:
   - change Impedance of inputs (Line-in, Mic  Instrument, Hi-Z)
   - select clock source
   - dynamic input to mixer-matrix assignment
   - 18 x 6 mixer-matrix gain stages
   - bus routing & volume control
   - automatic re-initialization on connect if device was power-cycled
  USB URB commands overview (bRequest = 0x01 = UAC2_CS_CUR)
  wIndex
  0x01 Analog Input lineinstrument impedance switch, wValue=0x0901 +
       channel, data=LineInst (2bytes)
       pad (-10dB) switch, wValue=0x0b01 + channel, data=OffOn (2bytes)
       ?? wValue=0x080304, ?? (2bytes)
  0x0a Master Volume, wValue=0x0200+bus[0:all + only 1..4?] data(2bytes)
       Bus MuteUnmute wValue=0x0100+bus[0:all + only 1..4?], data(2bytes)
  0x28 Clock source, wValue=0x0100, data={1:int,2:spdif,3:adat} (1byte)
  0x29 Set Sample-rate, wValue=0x0100, data=sample-rate(4bytes)
  0x32 Mixer mux, wValue=0x0600 + mixer-channel, data=input-to-connect(2bytes)
  0x33 Output mux, wValue=bus, data=input-to-connect(2bytes)
  0x34 Capture mux, wValue=0...18, data=input-to-connect(2bytes)
  0x3c Matrix Mixer gains, wValue=mixer-node  data=gain(2bytes)
       ?? [sometimes](4bytes, e.g 0x000003be 0x000003bf ...03ff)
  USB reads: (i.e. actually issued by original software)
  0x01 wValue=0x0901+channel (1byte!!), wValue=0x0b01+channed (1byte!!)
  0x29 wValue=0x0100 sample-rate(4bytes)
       wValue=0x0200 ?? 1byte (only once)
  0x2a wValue=0x0100 ?? 4bytes, sample-rate2 ??
  USB reads with bRequest = 0x03 = UAC2_CS_MEM
  0x3c wValue=0x0002 1byte: sync status (locked=1)
       wValue=0x0000 182byte: peak meter (inputs)
       wValue=0x0001 8(?)2byte: peak meter (mix)
       wValue=0x0003 62byte: peak meter (pcmdaw)
  USB write with bRequest = 0x03
  0x3c Save settings to hardware: wValue=0x005a, data=0xa5
  <ditaa>
   --------------\    18chn            6chn    --------------\
   | Hardware  in +--+-------\        ------+--+ ALSA PCM out |
   \--------------  |       |        |      |  \--------------
                     |       |        |      |
                     |       v        v      |
                     |   +---------------+   |
                     |    \ Matrix  Mux     |
                     |     +-----+-----+     |
                     |           |           |
                     |           | 18chn     |
                     |           v           |
                     |     +-----------+     |
                     |     | Mixer     |     |
                     |     |    Matrix |     |
                     |     |           |     |
                     |     | 18x6 Gain |     |
                     |     |   stages  |     |
                     |     +-----+-----+     |
                     |           |           |
                     |           |           |
                     | 18chn     | 6chn      | 6chn
                     v           v           v
                     =========================
              +---------------+     +--------------+
               \ Output  Mux        \ Capture Mux 
                +-----+-----+         +-----+-----+
                      |                     |
                      | 6chn                |
                      v                     |
               +-------------+              |
               | Master Gain |              |
               +------+------+              |
                      |                     |
                      | 6chn                | 18chn
                      | (3 stereo pairs)    |
   --------------\   |                     |   --------------\
   | Hardware out |<--                     \-->| ALSA PCM  in |
   \--------------                             \--------------
  <ditaa>
 some gui mixers can't handle negative ctl values 
 initial values for matrix mux 
 number of items in controls 
 Enum Strings 
 invert mute logic for mixer 
 generate name dynamically based on item number and offset info 
 max value 
 max value 
 add scarlett_mixer_elem_enum_info struct 
 Add mute switch 
 Add volume control and initialize to 0 
 Add L channel source playback enumeration 
 Add R channel source playback enumeration 
 device-specific config 
  untested...  
 Analog -> 1..4 
 SPDIF -> 5,6 
 PCM[1..12] -> 7..18 
  untested...  
 Analog -> 1..4 
 SPDIF -> 5,6 
 PCM[1..12] -> 7..18 
 Analog -> 1..8 
 ADAT[1..6] -> 9..14 
 SPDIF -> 15,16 
 PCM[1,2] -> 17,18 
 Analog -> 1..8 
 ADAT[1..6] -> 9..14 
 SPDIF -> 15,16 
 PCM[1,2] -> 17,18 
		{ .num = 1, .type = SCARLETT_SWITCH_IMPEDANCE, .name = NULL},
		{ .num = 1, .type = SCARLETT_SWITCH_PAD, .name = NULL},
		{ .num = 2, .type = SCARLETT_SWITCH_IMPEDANCE, .name = NULL},
		{ .num = 2, .type = SCARLETT_SWITCH_PAD, .name = NULL},
		{ .num = 3, .type = SCARLETT_SWITCH_PAD, .name = NULL},
 Analog -> 1..8 
 ADAT[1..6] -> 9..14 
 SPDIF -> 15,16 
 PCM[1,2] -> 17,18 
 create master switch and playback volume 
 iterate through controls in info struct and create each one 
  Create and initialize a mixer for the Focusrite(R) Scarlett
 only use UAC_VERSION_2 
 device not (yet) supported 
 generic function to create controls 
 setup matrix controls 
 val_len == 1 needed here 
 val_len == 1 and UAC2_CS_MEM 
 initialize sampling rate to 48000 
 SPDX-License-Identifier: GPL-2.0-or-later
  parse the audio format type I descriptor
  and returns the corresponding pcm format
  @dev: usb device
  @fp: audioformat record
  @format: the format tag (wFormatTag)
  @fmt: the format type descriptor (v1v2) or AudioStreaming descriptor (v3)
 invalid format 
 flag potentially raw DSD capable altsettings 
 some devices don't define this correctly... 
 Edirol SD-90 
 Roland SC-D70 
 check the format byte size 
 grrr, big endian!! 
 grrr, big endian!! 
		 Dallas DS4201 workaround: it advertises U8 format, but really
 set up rate_min, rate_max and rates from the rate table 
  parse the format descriptor and stores the possible sample rates
  on the audioformat table (audio class v1).
  @dev: usb device
  @fp: audioformat record
  @fmt: the format descriptor
  @offset: the start offset of descriptor pointing the rate type
           (7 for type I and II, 8 for type II)
		
		  build the rate table and bitmap flags
 C-Media CM6501 mislabels its 96 kHz altsetting 
 Terratec Aureon 7.1 USB C-Media 6206, too 
 Ozone Z90 USB C-Media, too 
 Creative VF0420VF0470 Live Cams report 16 kHz instead of 8kHz 
 continuous rates 
 Jabra Evolve 65 headset 
 only 48kHz for playback while keeping 16kHz for capture 
  Presonus Studio 1810c supports a limited set of sampling
  rates per altsetting but reports the full set each time.
  If we don't filter out the unsupported rates and attempt
  to configure the card, it will hang refusing to do any
  further audio IO until a hard reset is performed.
  The list of supported rates per altsetting (set of available
  IO channels) is described in the owner's manual, section 2.2.
 All ADAT ports available 
 Half of ADAT ports available 
 Analog IO only (no SPDIF nor ADAT) 
  Many Focusrite devices supports a limited set of sampling rates per
  altsetting. Maximum rate is exposed in the last 4 bytes of Format Type
  descriptor which has a non-standard bLength = 10.
 bLength 
 Validate max rate 
  Helper function to walk the array of sample rate triplets reported by
  the device. The problem is that we need to parse whole array first to
  get to know how many sample rates we have to expect.
  Then fp->rate_table can be allocated and filled.
		
		  for ranges with res == 1, we announce a continuous sample
		  rate range, and this function should return 0 for no further
		  parsing.
 Filter out invalid rates on Presonus Studio 1810c 
 Filter out invalid rates on Focusrite devices 
 avoid endless loop 
 Line6 Helix series and the Rode Rodecaster Pro don't support the
  UAC2_CS_RANGE usb function call. Return a static table of known
  clock rates.
 Line6 Helix 
 Line6 Helix Rack 
 Line6 Helix LT 
 Line6 HX-Stomp 
 Line6 HX-Stomp XL 
 Line6 Pod Go 
 Line6 Helix >= fw 2.82 
 Line6 Helix Rack >= fw 2.82 
 Line6 Helix LT >= fw 2.82 
 Rode Rodecaster Pro 
 check whether the given altsetting is supported for the already set rate 
 we assume 64bit is enough for any altsettings 
 first byte contains the bitmap size 
  Validate each sample rate with the altsetting
  Rebuild the rate table if only partial values are valid
	 performing the rate verification may lead to unexpected USB bus
	  behavior afterwards by some unknown reason.  Do this only for the
	  known devices.
 don't perform the validation as default 
 clear the interface altsetting at first 
 continue as is 
  parse the format descriptor and stores the possible sample rates
  on the audioformat table (audio class v2 and v3).
 get the number of sample rates first by only fetching 2 bytes 
 line6 helix devices don't support UAC2_CS_CONTROL_SAM_FREQ call 
 no line6 device found continue showing the error 
 now get the full information 
	 Call the triplet parser, and make sure fp->rate_table is NULL.
	  We just use the return value to know how many sample rates we
 SNDRV_PCM_RATE_CONTINUOUS 
	 Call the triplet parser again, but this time, fp->rate_table is
  parse the format type I and III descriptors
 fp->fmt_type is already set in this case 
		 FIXME: the format type is really IECxxx
		         but we give normal PCM format to get the existing
		         apps working...
 M-Audio Audiophile USB 
 gather possible sample rates 
	 audio class v1 reports possible sample rates as part of the
	  proprietary class specific descriptor.
	  audio class v2 uses class specific EP0 range requests for that.
 fp->channels is already set in this case 
  parse the format type II descriptor
 FIXME: there is no AC3 format defined yet 
 fp->formats = SNDRV_PCM_FMTBIT_AC3;
 temporary hack to receive byte streams 
 fmt[8..] sample rates 
 FIXME: temporary hack for extigyaudigy 2 nxzs 
	 extigy apparently supports sample rates other than 48k
	  but not in ordinary way.  so we enable only 48k atm.
	
	  Type I format bits are D0..D6
	  This test works because type IV is not supported
 SPDX-License-Identifier: GPL-2.0-or-later
    Additional mixer mapping
    Copyright (c) 2002 by Takashi Iwai <tiwai@suse.de>
  USB control mappers for SB Exitigy
  Topology of SB Extigy (see on the wide screen :)
USB_IN[1] --->FU[2]------------------------------+->MU[16]-->PU[17]-+->FU[18]--+->EU[27]--+->EU[21]-->FU[22]--+->FU[23] > Dig_OUT[24]
                                                 ^                  |          |          |                   |
USB_IN[3] -+->SU[5]-->FU[6]--+->MU[14] ->PU[15]->+                  |          |          |                   +->FU[25] > Dig_OUT[26]
           ^                 ^                   |                  |          |          |
Dig_IN[4] -+                 |                   |                  |          |          +->FU[28]---------------------> Spk_OUT[19]
                             |                   |                  |          |
Lin-IN[7] -+-->FU[8]---------+                   |                  |          +----------------------------------------> Hph_OUT[20]
           |                                     |                  |
Mic-IN[9] --+->FU[10]----------------------------+                  |
           ||                                                       |
           ||  +----------------------------------------------------+
           VV  V
           ++--+->SU[11]-->FU[12] --------------------------------------------------------------------------------------> USB_OUT[13]
 1: IT pcm 
 FU 
 3: IT pcm 
 4: IT digital in 
 DISABLED: this seems to be bogus on some firmware 
 FU 
 7: IT line 
 FU 
 9: IT mic 
 FU 
 SU 
 FU 
 13: OT pcm capture 
 14: MU (wo controls) 
 15: PU (3D enh) 
 16: MU (wo controls) 
 DISABLED: PU-switch (any effect?) 
 PU: mode select 
 FU 
 FU 
 FU; others 
 19: OT speaker 
 20: OT headphone 
 DISABLED: EU (for what?) 
 FU 
 FU   
 24: OT digital out 
 FU 
 OT 
 DISABLED: EU (for what?) 
 28: FU speaker (mute) 
 Digital Input Playback Source? 
 terminator 
 Sound Blaster MP3+ controls mapping
  The default mixer channels have totally misleading names,
  e.g. no Master and fake PCM volume
 			Pavel Mihaylov <bin@bash.info>
 just guess 
 just guess 
 1: IT pcm 
 2: IT mic 
 3: IT line 
 4: IT digital in 
 5: OT digital out 
 6: OT speaker 
 7: OT pcm capture 
 FU, default PCM Capture Source 
 (Mic, Input 1 = Line input, Input 2 = Optical input) 
 FU, default Speaker 1 
 { 10, "Mic Capture", 1 },  
 "Mic Capture",  NULL, 2, .dB = &mp3plus_dB_2 },
 FU, Mic Capture 
 FU, default Auto Gain Input 
 FU, default PCM Capture 
 FU, default PCM 1 
 "Mic Playback",  .dB = &mp3plus_dB_1 },
 FU, default Mic Playback 
 FU, default Speaker 
 15: MU 
 terminator 
 Topology of SB Audigy 2 NX
          +----------------------------->EU[27]--+
          |                                      v
          | +----------------------------------->SU[29]---->FU[22]-->Dig_OUT[24]
          | |                                    ^
USB_IN[1]-+------------+              +->EU[17]->+->FU[11]-+
            |          v              |          v         |
Dig_IN[4]---+->FU[6]-->MU[16]->FU[18]-+->EU[21]->SU[31]----->FU[30]->Hph_OUT[20]
            |          ^              |                    |
Lin_IN[7]-+--->FU[8]---+              +->EU[23]->FU[28]------------->Spk_OUT[19]
          | |                                              v
          +--->FU[12]------------------------------------->SU[14]--->USB_OUT[15]
            |                                              ^
            +->FU[13]--------------------------------------+
 1: IT pcm playback 
 4: IT digital in 
 FU 
 7: IT line in 
 FU 
 FU 
 FU 
 FU 
 SU 
 15: OT pcm capture 
 16: MU wo controls 
 DISABLED: EU (for what?) 
 FU 
 19: OT speaker 
 20: OT headphone 
 DISABLED: EU (for what?) 
 FU 
 DISABLED: EU (for what?) 
 24: OT digital out 
 DISABLED: EU (for what?) 
 FU 
 SU 
 FU 
 SU 
 terminator 
 terminator 
 terminator 
 Capture Source 
 Digital Out Source 
 Headphone Source 
 terminator 
 Creative SoundBlaster Live! 24-bit External 
 2: PCM Playback Volume 
 FU, default PCM Capture Volume 
 terminator 
 LineX FM Transmitter entry - needed to bypass controls bug 
 1: IT pcm 
 2: OT Speaker  
 FU: master volume - left  right  mute 
 terminator 
 1: IT line 
 FU 
 3: IT line 
 FU 
 5: IT pcm playback 
 6: MU 
 FU 
 8: OT speaker 
 9: IT line 
 FU 
 11: MU 
 12: OT pcm capture 
 Section "justlink_map" below added by James Courtier-Dutton <James@superbug.demon.co.uk>
  sourced from Maplin Electronics (https:www.maplin.co.uk), part number A56AK
  Part has 2 connectors that act as a single output. (TOSLINK Optical for digital out, and 3.5mm Jack for Analogue out.)
  The USB Mixer publishes a Microphone and extra Volume controls for it, but none exist on the device,
  so this map removes all unwanted sliders from alsamixer
 1: IT pcm playback 
 2: Not present 
 IT mic (No mic input on device) 
 4: Not present 
 5: OT speacker 
 6: OT pcm capture 
 Mutevolume for speaker 
 Capture Switch (No capture inputs on device) 
 Capture Mutevolume (No capture inputs on device 
 0xa: Not present 
 0xb: MU (wo controls) 
 Mic feedback Mutevolume (No capture inputs on device) 
 terminator 
 TerraTec Aureon 5.1 MkII USB 
 1: IT USB 
 2: IT Mic 
 3: IT Line 
 4: IT SPDIF 
 5: OT SPDIF 
 6: OT Speaker 
 7: OT USB 
 SU 
 FU 
 FU 
 FU 
 FU 
 FU 
 FU 
 15: MU 
 terminator 
 1: IT Line 1 (USB streaming) 
 2: OT Line 1 (Speaker) 
 3: IT Line 1 (Line connector) 
 FU 
 5: OT Line 1 (USB streaming) 
 6: IT Line 2 (USB streaming) 
 7: OT Line 2 (Speaker) 
 8: IT Line 2 (Line connector) 
 FU 
 10: OT Line 2 (USB streaming) 
 11: IT Mic (Line connector) 
 12: OT Mic (USB streaming) 
 terminator 
 FU 
 MU 
 FU 
 FU 
 MU 
 "Gamesurround Muse Pocket LT" looks same like "Sound Blaster MP3+"
   most importand difference is SU[8], it should be set to "Capture Source"
   to make alsamixer and PA working properly.
   FIXME: or mp3plus_map should use "Capture Source" too,
   so this maps can be merget
 SU, default "PCM Capture Source" 
 FU, default "Speaker Playback" 
 FU, default "Auto Gain Input" 
 FU, default "PCM Capture" 
 FU, default "Mic Playback" 
 FU, default "Line Playback" 
 terminator 
 Plantronics Gamecom 780 has a broken volume control, better to disable it 
 FU, speaker out 
 some (all?) SCMS USB3318 devices are affected by a firmware lock up
  when anything attempts to access FU 10 (control)
 Bose companion 5, the dB conversion factor is 16 instead of 256 
 terminator 
 Sennheiser Communications Headset [PC 8], the dB value is reported as -6 negative maximum  
 terminator 
  Dell usb dock with ALC4020 codec had a firmware problem where it got
  screwed up when zero volume is passed; just skip it as a workaround
  Also the extension unit gives an access error, so skip it as well.
 extension unit 
  Corsair Virtuoso calls everything "Headset" without this, leading to
  applications moving the sidetone control instead of the main one.
 Some mobos shipped with a dummy HD-audio show the invalid GET_MINGET_MAX
  response for Input Gain Pad (id=19, control=12) and the connector status
  for SPDIF terminal (id=18).  Skip them.
 OT, connector control 
 FU, Input Gain Pad 
 FU, Input Gain Pad 
 TRX40 mobos with Realtek ALC1220-VB 
 OT, IEC958 - broken response, disabled 
 FU, Input Gain Pad - broken response, disabled 
 OT 
 FU 
 IT 
 FU 
 OT 
 FU 
 IT 
 FU 
 IT 
 FU 
 FU 
 (Back) Speaker 
 Front Headphone 
 Line 
 Mic 
 Front Mic 
 Rear panel + front mic on Gigabyte TRX40 Aorus Master with ALC1220-VB 
 OT, IEC958?, disabled 
 FU, Input Gain Pad - broken response, disabled 
 OT 
 FU 
 IT 
 FU 
 IT 
 FU 
 IT 
 FU 
  Control map entries
 Plantronics GameCom 780 
		 Hercules Gamesurround Muse Pocket LT
		  (USB 5.1 Channel Audio Adapter)
 MAYA44 USB+ 
 KEF X300A 
 Arcam rPAC 
 Bose Companion 5 
 Corsair Virtuoso SE (wired mode) 
 Corsair Virtuoso SE (wireless mode) 
 Corsair Virtuoso (wired mode) 
 Corsair Virtuoso (wireless mode) 
 Gigabyte TRX40 Aorus Master (rear panel + front mic) 
 Gigabyte TRX40 Aorus Pro WiFi 
 ASUS ROG Zenith II 
 ASUS ROG Strix 
 MSI TRX40 Creator 
 MSI TRX40 
 Asrock TRX40 Creator 
 Lenovo ThinkStation P620 Rear 
 Sennheiser Communications Headset [PC 8] 
 terminator 
  Control map entries for UAC3 BADD profiles
 terminator 
 terminator 
 terminator 
 terminator 
 Covers also 'headset adapter' profile 
 terminator 
 terminator 
 terminator 
 SPDX-License-Identifier: GPL-2.0
  Presonus Studio 1810c driver for ALSA
  Copyright (C) 2019 Nick Kossifidis <mickflemm@gmail.com>
  Based on reverse engineering of the communication protocol
  between the windows driver  Univeral Control (UC) program
  and the device, through usbmon.
  For now this bypasses the mixer, with all channels split,
  so that the software can mix with greater flexibility.
  It also adds controls for the 4 buttons on the front of
  the device.
  DISCLAIMER: These are just guesses based on the
  dumps I got.
  It seems like a selects between
  device (0), mixer (0x64) and output (0x65)
  For mixer (0x64):
    b selects an input channel (see below).
    c selects an output channel pair (see below).
    d selects left (0) or right (1) of that pair.
    e 0-> disconnect, 0x01000000-> connect,
 	0x0109-> used for stereo-linking channels,
 	e is also used for setting volume levels
 	in which case b is also set so I guess
 	this way it is possible to set the volume
 	level from the specified input to the
 	specified output.
  IN Channels:
  0  - 7  MicInstLine (Analog inputs)
  8  - 9  SPDIF
  10 - 17 ADAT
  18 - 35 DAW (Inputs from the host)
  OUT Channels (pairs):
  0 -> Main out
  1 -> Line12
  2 -> Line34
  3 -> SPDIF
  4 -> ADAT?
  For device (0):
    b and c are not used, at least not on the
     dumps I got.
    d sets the control id to be modified
     (see below).
    e sets the setting for that control.
     (so for the switches I was interested
     in it's 01)
  For output (0x65):
     b is the output channel (see above).
     c is zero.
     e I guess the same as with mixer except 0x0109
 	 which I didn't see in my dumps.
  The two fixed fields have the same values for
  mixer and output but a different set for device.
  This packet includes mixer volumes and
  various other fields, it's an extended
  version of ctl_packet, with a and b
  being zero and different f1f2.
	
	  Value for settings 01 for this
	  output channel is always 0 (probably because
	  there is no ADAT output on 1810c)
  When opening Universal Control the program periodically
  sends and receives state packets for syncinc state between
  the device and the host.
  Note that if we send only the request to get data back we'll
  get an error, we need to first send an empty state packet and
  then ask to receive a filled. Their seqnumbers must also match.
  This is what I got when bypassing the mixer with
  all channels split. I'm not 100% sure of what's going
  on, I could probably clean this up based on my observations
  but I prefer to keep the same behavior as the windows driver.
 Set initial volume levels ? 
 This channel to all outputs ? 
 This channel to main output (again) 
		
		  I noticed on UC that DAW channels have different
		  initial volumes, so this makes sense.
 Connect analog outputs ? 
 Set initial volume levels for SPDIF mappings ? 
 Connect SPDIF output ? 
 Connect all outputs (again) ? 
 Basic routing to get sound out of the device 
 DAW12 -> Main 
 DAW34 -> Line34 
 DAW45 -> Line56 
 DAW56 -> SPDIF 
 Left 
 Right 
 Leave the rest disconnected 
 Set initial volume levels for SPDIF (again) ? 
 Connect SPDIF outputs (again) ? 
 Again ? 
  Sync state with the device and retrieve the requested field,
  whose index is specified in (kctl->private_value & 0xFF),
  from the received fields array.
  Send a control packet to the device for the control id
  specified in (kctl->private_value >> 8) with value
  specified in (kctl->private_value >> 16).
 Generic getsetinit functions for switch controls 
 Entry point, called from mixer_quirks.c 
 Run this only once 
 SPDX-License-Identifier: GPL-2.0-or-later
  combine bytes and get an integer value
  parse descriptor buffer and return the pointer starting the given
  descriptor type.
  find a class-specified interface descriptor with the given subtype.
  Wrapper for usb_control_msg().
  Allocates a temp buffer to prevent dmaing fromto the stack.
 SPDX-License-Identifier: GPL-2.0
  media.c - Media Controller specific ALSA driver code
  Copyright (c) 2019 Shuah Khan <shuah@kernel.org>
  This file adds Media Controller support to the ALSA driver
  to use the Media Controller API to share the tuner with DVB
  and V4L2 drivers that control the media device.
  The media device is created based on the existing quirks framework.
  Using this approach, the media controller API usage can be added for
  a specific device.
 allocate media_ctl 
 create link between mixer and audio 
 allocate media_mixer_ctl 
	 usb-audio driver is probed for each usb interface, and
	  there are multiple interfaces per device. Avoid calling
	  media_device_usb_allocate() each time usb_audio_probe()
	  is called. Do it only once.
 save media device - avoid lookups 
 Create media entities for mixer and control dev 
 media_device might be registered, print error and continue 
 don't register if snd_media_mixer_init() failed 
 register media_device 
 clear saved media_dev 
 release resources 
 SPDX-License-Identifier: GPL-2.0-or-later
 convert our full speed USB rate into sampling rate in Hz 
 convert our high speed USB rate into sampling rate in Hz 
  common proc files to show the usb device info
  proc interface for list the supported pcm formats
 snd_iprintf(buffer, "    Max Packet Size = %d\n", fp->maxpacksize);
 snd_iprintf(buffer, "    EP Attribute = %#x\n", fp->attributes);
