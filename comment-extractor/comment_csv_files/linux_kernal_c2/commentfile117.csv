 SPDX-License-Identifier: GPL-2.0
  High-level sync()-related operations
  Write out and wait upon all dirty data associated with this
  superblock.  Filesystem data as well as the underlying block
  device.  Takes the superblock lock.
	
	  We need to be protected against the filesystem going from
	  ro to rw or vice versa.
	
	  No point in syncing out anything if the filesystem is read-only.
	
	  Do the filesystem syncing work.  For simple filesystems
	  writeback_inodes_sb(sb) just dirties buffers with inodes so we have
	  to submit IO for these buffers via sync_blockdev().  This also
	  speeds up the wait == 1 case since in that case write_inode()
	  methods call sync_dirty_buffer() and thus effectively write one block
	  at a time.
  Sync everything. We start by waking flusher threads so that most of
  writeback runs on all devices in parallel. Then we sync all inodes reliably
  which effectively also waits for all flusher threads to finish doing
  writeback. At this point all data is on disk so metadata should be stable
  and we tell filesystems to sync their metadata via ->sync_fs() calls.
  Finally, we writeout all block devices because some filesystems (e.g. ext2)
  just write metadata (such as inodes or bitmaps) to block device page cache
  and do not sync it on their own in ->sync_fs().
	
	  Sync twice to reduce the possibility we skipped some inodes  pages
	  because they were temporarily locked
  sync a single super
  vfs_fsync_range - helper to sync a range of data & metadata to disk
  @file:		file to sync
  @start:		offset in bytes of the beginning of data range to sync
  @end:		offset in bytes of the end of data range (inclusive)
  @datasync:		perform only datasync
  Write back data in range @start..@end and metadata for @file to disk.  If
  @datasync is set only metadata needed to access modified file data is
  written.
  vfs_fsync - perform a fsync or fdatasync on a file
  @file:		file to sync
  @datasync:		only perform a fdatasync operation
  Write back data and metadata for @file to disk.  If @datasync is
  set only metadata needed to access modified file data is written.
 inclusive 
			
			  The range starts outside a 32 bit machine's
			  pagecache addressing capabilities.  Let it "succeed"
			
			  Out to EOF
 inclusive 
  ksys_sync_file_range() permits finely controlled syncing over a segment of
  a file in the range offset .. (offset+nbytes-1) inclusive.  If nbytes is
  zero then ksys_sync_file_range() will operate from offset out to EOF.
  The flag bits are:
  SYNC_FILE_RANGE_WAIT_BEFORE: wait upon writeout of all pages in the range
  before performing the write.
  SYNC_FILE_RANGE_WRITE: initiate writeout of all those dirty pages in the
  range which are not presently under writeback. Note that this may block for
  significant periods due to exhaustion of disk request structures.
  SYNC_FILE_RANGE_WAIT_AFTER: wait upon writeout of all pages in the range
  after performing the write.
  Useful combinations of the flag bits are:
  SYNC_FILE_RANGE_WAIT_BEFORE|SYNC_FILE_RANGE_WRITE: ensures that all pages
  in the range which were dirty on entry to ksys_sync_file_range() are placed
  under writeout.  This is a start-write-for-data-integrity operation.
  SYNC_FILE_RANGE_WRITE: start writeout of all dirty pages in the range which
  are not presently under writeout.  This is an asynchronous flush-to-disk
  operation.  Not suitable for data integrity operations.
  SYNC_FILE_RANGE_WAIT_BEFORE (or SYNC_FILE_RANGE_WAIT_AFTER): wait for
  completion of writeout of all pages in the range.  This will be used after an
  earlier SYNC_FILE_RANGE_WAIT_BEFORE|SYNC_FILE_RANGE_WRITE operation to wait
  for that operation to complete and to return the result.
  SYNC_FILE_RANGE_WAIT_BEFORE|SYNC_FILE_RANGE_WRITE|SYNC_FILE_RANGE_WAIT_AFTER
  (a.k.a. SYNC_FILE_RANGE_WRITE_AND_WAIT):
  a traditional sync() operation.  This is a write-for-data-integrity operation
  which will ensure that all pages in the range which were dirty on entry to
  ksys_sync_file_range() are written to disk.  It should be noted that disk
  caches are not flushed by this call, so there are no guarantees here that the
  data will be available on disk after a crash.
  SYNC_FILE_RANGE_WAIT_BEFORE and SYNC_FILE_RANGE_WAIT_AFTER will detect any
  IO errors or ENOSPC conditions and will return those to the caller, after
  clearing the EIO and ENOSPC flags in the address_space.
  It should be noted that none of these operations write out the file's
  metadata.  So unless the application is strictly performing overwrites of
  already-instantiated disk blocks, there are no guarantees here that the data
  will be available after a crash.
 It would be nice if people remember that not all the world's an i386
 SPDX-License-Identifier: GPL-2.0
   linuxfsioctl.c
   Copyright (C) 1991, 1992  Linus Torvalds
 So that the fiemap access checks can't overflow on 32 bit machines. 
  vfs_ioctl - call filesystem specific ioctl methods
  @filp:	open file to invoke ioctl method on
  @cmd:	ioctl command to execute
  @arg:	command-specific argument for ioctl
  Invokes filesystem specific ->unlocked_ioctl, if one exists; otherwise
  returns -ENOTTY.
  Returns 0 on success, -errno on error.
  fiemap_fill_next_extent - Fiemap helper function
  @fieinfo:	Fiemap context passed into ->fiemap
  @logical:	Extent logical start offset, in bytes
  @phys:	Extent physical start offset, in bytes
  @len:	Extent length, in bytes
  @flags:	FIEMAP_EXTENT flags that describe this extent
  Called from file system ->fiemap callback. Will populate extent
  info as passed in via arguments and copy to user memory. On
  success, extent count on fieinfo is incremented.
  Returns 0 on success, -errno on error, 1 if this was the last
  extent that will fit in user array.
 only count the extents 
  fiemap_prep - check validity of requested flags for fiemap
  @inode:	Inode to operate on
  @fieinfo:	Fiemap context passed into ->fiemap
  @start:	Start of the mapped range
  @len:	Length of the mapped range, can be truncated by this function.
  @supported_flags:	Set of fiemap flags that the file system understands
  This function must be called from each ->fiemap instance to validate the
  fiemap request against the file system parameters.
  Returns 0 on success, or a negative error on failure.
	
	  Shrink request scope to what the fs can actually handle.
  This provides compatibility with legacy XFS pre-allocation ioctls
  which predate the fallocate syscall.
  Only the l_start, l_len and l_whence fields of the 'struct space_resv'
  are used here, rest are ignored.
 on ia32 l_start is on a 32-bit boundary 
 just account for different alignment 
 SunOS compatibility item. 
 Did FASYNC state change ? 
 fasync() adjusts filp->f_flags 
 If filesystem doesn't support freeze feature, return. 
 Freeze 
 Thaw 
  fileattr_fill_xflags - initialize fileattr with xflags
  @fa:		fileattr pointer
  @xflags:	FS_XFLAG_ flags
  Set ->fsx_xflags, ->fsx_valid and ->flags (translated xflags).  All
  other fields are zeroed.
  fileattr_fill_flags - initialize fileattr with flags
  @fa:		fileattr pointer
  @flags:	FS__FL flags
  Set ->flags, ->flags_valid and ->fsx_xflags (translated flags).
  All other fields are zeroed.
  vfs_fileattr_get - retrieve miscellaneous file attributes
  @dentry:	the object to retrieve from
  @fa:		fileattr pointer
  Call i_op->fileattr_get() callback, if exists.
  Return: 0 on success, or a negative error on failure.
  copy_fsxattr_to_user - copy fsxattr to userspace.
  @fa:		fileattr pointer
  @ufa:	fsxattr user pointer
  Return: 0 on success, or -EFAULT on failure.
  Generic function to check FS_IOC_FSSETXATTRFS_IOC_SETFLAGS values and reject
  any invalid configurations.
  Note: must be called with inode lock held.
	
	  The IMMUTABLE and APPEND_ONLY flags can only be changed by
	  the relevant capability.
	
	  Project Quota ID state is only allowed to change from within the init
	  namespace. Enforce that restriction only if we are trying to change
	  the quota ID state. Everything else is allowed in user namespaces.
		
		  Caller is allowed to change the project ID. If it is being
		  changed, make sure that the new value is valid.
 Check extent size hints. 
	
	  It is only valid to set the DAX flag on regular files and
	  directories on filesystems.
 Extent size hints of zero turn off the flags. 
  vfs_fileattr_set - change miscellaneous file attributes
  @mnt_userns:	user namespace of the mount
  @dentry:	the object to change
  @fa:		fileattr pointer
  After verifying permissions, call i_op->fileattr_set() callback, if
  exists.
  Verifying attributes involves retrieving current attributes with
  i_op->fileattr_get(), this also allows initializing attributes that have
  not been set by the caller to current values.  Inode lock is held
  thoughout to prevent racing with another instance.
  Return: 0 on success, or a negative error on failure.
 initialize missing bits from old_ma 
 hint only 
 hint only 
  do_vfs_ioctl() is not for drivers and not intended to be EXPORT_SYMBOL()'d.
  It's just a simple helper for sys_ioctl and compat_sys_ioctl.
  When you add any new common ioctls to the switches above and below,
  please ensure they have compatible arguments in compat mode.
 anon_bdev filesystems may not have a block size 
  compat_ptr_ioctl - generic implementation of .compat_ioctl file operation
  This is not normally called as a function, but instead set in struct
  file_operations as
      .compat_ioctl = compat_ptr_ioctl,
  On most architectures, the compat_ptr_ioctl() just passes all arguments
  to the corresponding ->ioctl handler. The exception is archs390, where
  compat_ptr() clears the top bit of a 32-bit pointer value, so user space
  pointers to the second 2GB alias the first 2GB, as is the case for
  native 32-bit s390 user space.
  The compat_ptr_ioctl() function must therefore be used only with ioctl
  functions that either ignore the argument or pass a pointer to a
  compatible data type.
  If any ioctl command handled by fops->unlocked_ioctl passes a plain
  integer instead of a pointer, or any of the passed data types
  is incompatible between 32-bit and 64-bit architectures, a proper
  handler is required instead of compat_ptr_ioctl.
 RED-PEN how should LSM module know it's handling 32bit? 
 FICLONE takes an int argument, so don't use compat_ptr() 
 these get messy on amd64 due to alignment differences 
	
	  These access 32-bit values anyway so no further handling is
	  necessary.
	
	  everything else in do_vfs_ioctl() takes either a compatible
	  pointer argument or no argument -- call it with a modified
	  argument.
 SPDX-License-Identifier: GPL-2.0-only
  Performs necessary checks before doing a clone.
  Can adjust amount of bytes to clone via @req_count argument.
  Returns appropriate error code that caller should return or
  zero in case the clone should be allowed.
 The start of both ranges must be aligned to an fs block. 
 Ensure offsets don't wrap. 
 Dedupe requires both ranges to be within EOF. 
 Ensure the infile range is within the infile. 
	
	  If the user wanted us to link to the infile's EOF, round up to the
	  next block boundary for this check.
	 
	  Otherwise, make sure the count is also block-aligned, having
	  already confirmed the starting offsets' block alignment.
 Don't allow overlapped cloning within the same file. 
	
	  We shortened the request but the caller can't deal with that, so
	  bounce the request back to userspace.
  Ensure that we don't remap a partial EOF block in the middle of something
  else.  Assume that the offsets have already been checked for block
  alignment.
  For clone we only link a partial EOF block above or at the destination file's
  EOF.  For deduplication we accept a partial EOF block only if it ends at the
  destination file's EOF (can not link it into the middle of a file).
  Shorten the request if possible.
 Read a page's worth of file data into the page cache. 
  Lock two pages, ensuring that we lock in offset order if the pages are from
  the same file.
 Always lock in order of increasing index. 
 Unlock two pages, being careful not to unlock the same page twice. 
  Compare extents of two files to see if they are the same.
  Caller must have locked both inodes to prevent write races.
		
		  Now that we've locked both pages, make sure they're still
		  mapped to the file data we're interested in.  If not,
		  someone is invalidating pages on us and we lose.
  Check that the two inodes are eligible for cloning, the ranges make
  sense, and then flush all dirty data.  Caller must ensure that the
  inodes have been locked against any other modifications.
  If there's an error, then the usual negative error code is returned.
  Otherwise returns 0 with len set to the request length.
 Don't touch certain kinds of inodes 
 Don't reflink dirs, pipes, sockets... 
 Zero length dedupe exits immediately; reflink goes to EOF. 
 Check that we don't violate system file offset limits. 
 Wait for the completion of any pending IOs on both files 
	
	  Check that the extents are the same.
 If can't alter the file contents, we're done. 
	
	  FICLONEFICLONERANGE ioctls enforce that src and dest files are on
	  the same mount. Practically, they only need to be on the same file
	  system.
 Check whether we are allowed to dedupe the destination file 
	
	  This is redundant if called from vfs_dedupe_file_range(), but other
	  callers need it and it's not performance sesitive...
 Arbitrary 1G limit on a single dedupe request, can be raised. 
 pre-format output fields to sane values 
 SPDX-License-Identifier: GPL-2.0
 The maximal length of core_pattern is also specified in sysctl.c 
 racy but harmless 
		
		  Ensure that this coredump name component can't cause the
		  resulting corefile path to consist of a ".." or ".".
		
		  Empty names are fishy and could be used to create a "" in a
		  corefile name, causing the coredump to happen one directory
		  level too high. Enforce that all components of the core
		  pattern are at least one character long.
 format_corename will inspect the pattern parameter, and output a
  name into corename, which must have space for at least
  CORENAME_MAX_SIZE bytes plus one byte for the zero terminator.
	 Repeat as long as we have more pattern to process and more output
		
		  Split on spaces before doing template expansion so that
		  %e and %E don't get split if they have spaces in them
 single % at the end, drop that 
 Double percent, output one percent 
 pid 
 global pid 
 uid 
 gid 
 signal that caused the coredump 
 UNIX time of coredump 
 hostname 
 executable, could be changed by prctl PR_SET_NAME etc 
 file name of executable 
 core limit size 
	 Backward compatibility with core_uses_pid:
	 
	  If core_pattern does not include a %p (as is the default)
	  and core_uses_pid is set, then .%pid will be appended to
 ignore all signals except SIGKILL, see prepare_signal() 
		
		  Wait for all the threads to become inactive, so that
		  all the thread context (extended register state, like
		  fpu etc) gets copied to the memory.
		
		  see coredump_task_exit(), curr->task must not see
		  ->task == NULL before we read ->next.
	
	  SIGKILL or freezing() interrupt the coredumping. Perhaps we
	  can do try_to_freeze() and check __fatal_signal_pending(),
	  but then we need to teach dump_write() to restart and clear
	  TIF_SIGPENDING.
	
	  We actually want wait_event_freezable() but then we need
	  to clear TIF_SIGPENDING and improve dump_interrupted().
  umh_pipe_setup
  helper function to customize the process used
  to collect the core in userspace.  Specifically
  it sets up a pipe and installs it as fd 0 (stdin)
  for the process.  Returns 0 on success, or
  PTR_ERR on failure.
  Note that it also sets the core limit to 1.  This
  is a special value that we use to trap recursive
  core dumps
 and disallow core files too 
 require nonrelative corefile path and be extra careful 
		
		  We must use the same mm->flags while dumping core to avoid
		  inconsistency of bit flags, since this flag is not protected
		  by any locks.
	
	  We cannot trust fsuid as being the "true" uid of the process
	  nor do we know its entire history. We only know it was tainted
	  so we dump it as root in mode 2, and only into a controlled
	  environment (pipe handler or fully qualified path).
 Setuid core dump mode 
 Dump root private 
			 See umh_pipe_setup() which sets RLIMIT_CORE = 1.
			 
			  Normally core limits are irrelevant to pipes, since
			  we're not writing to the file system, but we use
			  cprm.limit of 1 here as a special value, this is a
			  consistent way to catch recursive crashes.
			  We can still crash if the core_pattern binary sets
			  RLIM_CORE = !1, but it runs as root, and can do
			  lots of stupid things.
			 
			  Note that we use task_tgid_vnr here to grab the pid
			  of the process group leader.  That way we get the
			  right pid if a thread in a multi-threaded
			  core_pattern process dies.
		
		  Unlink the file if it exists unless this is a SUID
		  binary - in that case, we're running around with root
		  privs and don't want to unlink another user's coredump.
			
			  If it doesn't exist, that's fine. If there's some
			  other problem, we'll catch it at the filp_open().
		
		  There is a race between unlinking and creating the
		  file, but if that causes an EEXIST here, that's
		  fine - another process raced with us while creating
		  the corefile, and the other process won. To userspace,
		  what matters is that at least one of the two processes
		  writes its coredump successfully, not which one.
			
			  Using user namespaces, normal user tasks can change
			  their current->fs->root to point to arbitrary
			  directories. Since the intention of the "only dump
			  with a fully qualified path" rule is to control where
			  coredumps may be placed using root privileges,
			  current->fs->root must not be used. Instead, use the
			  root directory of init_task.
		
		  AK: actually i see no reason to not allow this for named
		  pipes etc, but keep the previous behaviour for now.
		
		  Don't dump core if the filesystem changed owner or mode
		  of the file during file creation. This is an issue when
		  a process dumps core while its cwd is e.g. on a vfat
		  filesystem.
 get us an unshared descriptor table; almost always a no-op 
 The cell spufs coredump code reads the file descriptor tables 
		
		  umh disabled with CONFIG_STATIC_USERMODEHELPER_PATH="" would
		  have this set to NULL.
		
		  Ensures that file size is big enough to contain the current
		  file postion. This prevents gdb from complaining about
		  a truncated file if the last "write" to the file was
		  dump_skip.
  Core dumping helper functions.  These are the only things you should
  do on a core-file: use only these functions to write out all the
  necessary info.
		
		  To avoid having to allocate page tables for virtual address
		  ranges that have never been used yet, and also to make it
		  easy to generate sparse core files, use a helper that returns
		  NULL when encountering an empty page table entry that would
		  otherwise have been filled with the zero page.
  The purpose of always_dump_vma() is to make sure that special kernel mappings
  that are useful for post-mortem analysis are included in every core dump.
  In that way we ensure that the core dump is fully interpretable later
  without matching up the same kernel and hardware config to see what PC values
  meant. These special mappings include - vDSO, vsyscall, and other
  architecture specific mappings
 Any vsyscall mappings? 
	
	  Assume that all vmas with a .name op should always be dumped.
	  If this changes, a new vm_ops field can easily be added.
	
	  arch_vma_name() returns non-NULL for special architecture mappings,
	  such as vDSO sections.
  Decide how much of @vma's contents should be included in a core dump.
 always dump the vdso and vsyscall sections 
 support for DAX 
 Hugetlb memory check 
 Do not dump IO mapped devices or special mappings 
 By default, dump shared memory if mapped from an anonymous file. 
 Dump segments that have been written to.  
	
	  If this is the beginning of an executable file mapping,
	  dump the first page to aid in determining what was mapped here.
  Helper function for iterating across a vma list.  It ensures that the caller
  will visit `gate_vma' prior to terminating the search.
  Under the mmap_lock, take a snapshot of relevant information about the task's
  VMAs.
	
	  Once the stack expansion code is fixed to not change VMA bounds
	  under mmap_lock in read mode, this can be changed to take the
	  mmap_lock in read mode.
 SPDX-License-Identifier: GPL-2.0
   linuxfsnamei.c
   Copyright (C) 1991, 1992  Linus Torvalds
  Some corrections by tytso.
 [Feb 1997 T. Schoebel-Theuer] Complete rewrite of the pathname
  lookup logic.
 [Feb-Apr 2000, AV] Rewrite to the new namespace architecture.
 [Feb-1997 T. Schoebel-Theuer]
  Fundamental changes in the pathname lookup mechanisms (namei)
  were necessary because of omirr.  The reason is that omirr needs
  to know the _real_ pathname, not the user-supplied one, in case
  of symlinks (and also when transname replacements occur).
  The new code replaces the old recursive symlink resolution with
  an iterative one (in case of non-nested symlink chains).  It does
  this with calls to <fs>_follow_link().
  As a side effect, dir_namei(), _namei() and follow_link() are now 
  replaced with a single function lookup_dentry() that can handle all 
  the special cases of the former code.
  With the new dcache, the pathname is stored at each inode, at least as
  long as the refcount of the inode is positive.  As a side effect, the
  size of the dcache depends on the inode cache and thus is dynamic.
  [29-Apr-1998 C. Scott Ananian] Updated above description of symlink
  resolution to correspond with current state of the code.
  Note that the symlink resolution is not completely iterative.
  There is still a significant amount of tail- and mid- recursion in
  the algorithm.  Also, note that <fs>_readlink() is not used in
  lookup_dentry(): lookup_dentry() on the result of <fs>_readlink()
  may return different results than <fs>_follow_link().  Many virtual
  filesystems (including proc) exhibit this behavior.
 [24-Feb-97 T. Schoebel-Theuer] Side effects caused by new implementation:
  New symlink semantics: when open() is called with flags O_CREAT | O_EXCL
  and the name already exists in form of a symlink, try to create the new
  name indicated by the symlink. The old code always complained that the
  name already exists, due to not following the symlink even if its target
  is nonexistent.  The new semantics affects also mknod() and link() when
  the name is a symlink pointing to a non-existent name.
  I don't know which semantics is the right one, since I have no access
  to standards. But I found by trial that HP-UX 9.0 has the full "new"
  semantics implemented, while SunOS 4.1.1 and Solaris (SunOS 5.4) have the
  "old" one. Personally, I think the new semantics is much more logical.
  Note that "ln old new" where "new" is a symlink pointing to a non-existing
  file does succeed in both HP-UX and SunOs, but not in Solaris
  and in the old Linux semantics.
 [16-Dec-97 Kevin Buhr] For security reasons, we change some symlink
  semantics.  See the comments in "open_namei" and "do_link" below.
  [10-Sep-98 Alan Modra] Another symlink change.
 [Feb-Apr 2000 AV] Complete rewrite. Rules for symlinks:
 	inside the path - always follow.
 	in the last component in creationremovalrenaming - never follow.
 	if LOOKUP_FOLLOW passed - follow.
 	if the pathname has trailing slashes - follow.
 	otherwise - don't follow.
  (applied in that order).
  [Jun 2000 AV] Inconsistent behaviour of open() in case if flags==O_CREAT
  restored for 2.4. This is the last surviving part of old 4.2BSD bug.
  During the 2.4 we need to fix the userland stuff depending on it -
  hopefully we will be able to get rid of that wart in 2.5. So far only
  XEmacs seems to be relying on it...
  [Sep 2001 AV] Single-semaphore locking scheme (kudos to David Holland)
  implemented.  Let's see if raised priority of ->s_vfs_rename_mutex gives
  any extra contention...
 In order to reduce some races, while at the same time doing additional
  checking and hopefully speeding things up, we copy filenames to the
  kernel data space before using them..
  POSIX.1 2.4: an empty pathname is invalid (ENOENT).
  PATH_MAX includes the nul terminator --RR.
	
	  First, try to embed the struct filename inside the names_cache
	  allocation
	
	  Uh-oh. We have a name that's approaching PATH_MAX. Allocate a
	  separate struct filename so we can dedicate the entire
	  names_cache allocation for the pathname, and re-do the copy from
	  userland.
		
		  size is chosen that way we to guarantee that
		  result->iname[0] is within the same object and that
		  kname can't be equal to result->iname, no matter what.
 The empty path is special. 
  check_acl - perform ACL permission checking
  @mnt_userns:	user namespace of the mount the inode was found from
  @inode:	inode to check permissions on
  @mask:	right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC ...)
  This function performs the ACL permission checking. Since this function
  retrieve POSIX acls it needs to know whether it is called from a blocking or
  non-blocking context and thus cares about the MAY_NOT_BLOCK bit.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
 no ->get_acl() calls in RCU mode... 
  acl_permission_check - perform basic UNIX permission checking
  @mnt_userns:	user namespace of the mount the inode was found from
  @inode:	inode to check permissions on
  @mask:	right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC ...)
  This function performs the basic UNIX permission checking. Since this
  function may retrieve POSIX acls it needs to know whether it is called from a
  blocking or non-blocking context and thus cares about the MAY_NOT_BLOCK bit.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
 Are we the owner? If so, ACL's don't matter 
 Do we have ACL's? 
 Only RWX matters for groupother mode bits 
	
	  Are the group permissions different from
	  the other permissions in the bits we care
	  about? Need to check group ownership if so.
 Bits in 'mode' clear that we require? 
  generic_permission -  check for access rights on a Posix-like filesystem
  @mnt_userns:	user namespace of the mount the inode was found from
  @inode:	inode to check access rights for
  @mask:	right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC,
 		%MAY_NOT_BLOCK ...)
  Used to check for readwriteexecute permissions on a file.
  We use "fsuid" for this, letting us set arbitrary permissions
  for filesystem access without changing the "normal" uids which
  are used for other things.
  generic_permission is rcu-walk aware. It returns -ECHILD in case an rcu-walk
  request cannot be satisfied (eg. requires blocking or too much complexity).
  It would then be called again in ref-walk mode.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
	
	  Do the basic permission checks.
 DACs are overridable for directories 
	
	  Searching includes executable on directories, else just read.
	
	  Readwrite DACs are always overridable.
	  Executable DACs are overridable when there is
	  at least one exec bit set.
  do_inode_permission - UNIX permission checking
  @mnt_userns:	user namespace of the mount the inode was found from
  @inode:	inode to check permissions on
  @mask:	right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC ...)
  We _really_ want to just do "generic_permission()" without
  even looking at the inode->i_op values. So we keep a cache
  flag in inode->i_opflags, that says "this has not special
  permission function, use the fast case".
 This gets set once for the inode lifetime 
  sb_permission - Check superblock-level permissions
  @sb: Superblock of inode to check permission on
  @inode: Inode to check permission on
  @mask: Right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC)
  Separate out file-system wide checks from inode-specific permission checks.
 Nobody gets write access to a read-only fs. 
  inode_permission - Check for access rights to a given inode
  @mnt_userns:	User namespace of the mount the inode was found from
  @inode:	Inode to check permission on
  @mask:	Right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC)
  Check for readwriteexecute permissions on an inode.  We use fs[ug]id for
  this, letting us set arbitrary permissions for filesystem access without
  changing the "normal" UIDs which are used for other things.
  When checking for MAY_APPEND, MAY_WRITE must also be set in @mask.
		
		  Nobody gets write access to an immutable file.
		
		  Updating mtime will likely cause i_uid and i_gid to be
		  written back improperly if their true value is unknown
		  to the vfs.
  path_get - get a reference to a path
  @path: path to get the reference to
  Given a path increment the reference count to the dentry and the vfsmount.
  path_put - put a reference to a path
  @path: path to put the reference to
  Given a path decrement the reference count to the dentry and the vfsmount.
 path.dentry.d_inode 
  path_connected - Verify that a dentry is below mnt.mnt_root
  Rename can sometimes move a file or directory outside of a bind
  mount, path_connected allows those cases to be detected.
 Bind mounts can have disconnected paths 
 path_put is needed afterwards regardless of success or failure 
	
	  For scoped-lookups (where nd->root has been zeroed), we need to
	  restart the whole lookup from scratch -- because set_root() is wrong
	  for these lookups (nd->dfd is the root, not the filesystem root).
 Nothing to do if nd->root is zero or is managed by the VFS user. 
  Path walking has 2 modes, rcu-walk and ref-walk (see
  Documentationfilesystemspath-lookup.txt).  In situations when we can't
  continue in RCU mode, we attempt to drop out of rcu-walk mode and grab
  normal reference counts on dentries and vfsmounts to transition to ref-walk
  mode.  Refcounts are grabbed at the last known good point before rcu-walk
  got stuck, so ref-walk may continue from there. If this is not successful
  (eg. a seqcount has changed), then failure is returned and it's up to caller
  to restart the path walk from the beginning in ref-walk mode.
  try_to_unlazy - try to switch to ref-walk mode.
  @nd: nameidata pathwalk data
  Returns: true on success, false on failure
  try_to_unlazy attempts to legitimize the current nd->path and nd->root
  for ref-walk mode.
  Must be called from rcu-walk context.
  Nothing should touch nameidata between try_to_unlazy() failure and
  terminate_walk().
  try_to_unlazy_next - try to switch to ref-walk mode.
  @nd: nameidata pathwalk data
  @dentry: next dentry to step into
  @seq: seq number to check @dentry against
  Returns: true on success, false on failure
  Similar to to try_to_unlazy(), but here we have the next dentry already
  picked by rcu-walk and want to legitimize that in addition to the current
  nd->path and nd->root for ref-walk mode.  Must be called from rcu-walk context.
  Nothing should touch nameidata between try_to_unlazy_next() failure and
  terminate_walk().
	
	  We need to move both the parent and the dentry from the RCU domain
	  to be properly refcounted. And the sequence number in the dentry
	  validates both dentry counters, since we checked the sequence
	  number of the parent after we got the child sequence number. So we
	  know the parent must still be valid if the child sequence number is
	
	  Sequence counts matched. Now make sure that the root is
	  still valid and get it if required.
  complete_walk - successful completion of path walk
  @nd:  pointer nameidata
  If we had been in RCU mode, drop out of it and legitimize nd->path.
  Revalidate the final result, unless we'd already done that during
  the path walk or the filesystem doesn't ask for it.  Return 0 on
  success, -error on failure.  In case of failure caller does not
  need to drop nd->path.
		
		  We don't want to zero nd->root for scoped-lookups or
		  externally-managed nd->root.
		
		  While the guarantee of LOOKUP_IS_SCOPED is (roughly) "don't
		  ever step outside the root during lookup" and should already
		  be guaranteed by the rest of namei, we want to avoid a namei
		  BUG resulting in userspace being given a path that was not
		  scoped within the root at some point during the lookup.
		 
		  So, do a final sanity-check to make sure that in the
		  worst-case scenario (a complete bypass of LOOKUP_IS_SCOPED)
		  we won't silently return an fd completely outside of the
		  requested root to userspace.
		 
		  Userspace could move the path outside the root after this
		  check, but as discussed elsewhere this is not a concern (the
		  resolved file was inside the root at some point).
	
	  Jumping to the real root in a scoped-lookup is a BUG in namei, but we
	  still have to ensure it doesn't happen because it will cause a breakout
	  from the dirfd.
 Absolute path arguments to path_init() are allowed. 
  Helper to directly jump to a known parsed path from ->get_link,
  caller must have taken a reference to path beforehand.
 Not currently safe for scoped-lookups. 
  may_follow_link - Check symlink following for unsafe situations
  @nd: nameidata pathwalk data
  In the case of the sysctl_protected_symlinks sysctl being enabled,
  CAP_DAC_OVERRIDE needs to be specifically ignored if the symlink is
  in a sticky world-writable directory. This is to protect privileged
  processes from failing races against path names that may change out
  from under them by way of other users creating malicious symlinks.
  It will permit symlinks to be followed only when outside a sticky
  world-writable directory, or when the uid of the symlink and follower
  match, or when the directory owner matches the symlink's owner.
  Returns 0 if following the symlink is allowed, -ve on error.
 Allowed if owner and follower match. 
 Allowed if parent directory not sticky and world-writable. 
 Allowed if parent directory and link owner match. 
  safe_hardlink_source - Check for safe hardlink conditions
  @mnt_userns:	user namespace of the mount the inode was found from
  @inode: the source inode to hardlink from
  Return false if at least one of the following conditions:
     - inode is not a regular file
     - inode is setuid
     - inode is setgid and group-exec
     - access failure for read and write
  Otherwise returns true.
 Special files should not get pinned to the filesystem. 
 Setuid files should not get pinned to the filesystem. 
 Executable setgid files should not get pinned to the filesystem. 
 Hardlinking to unreadable or unwritable sources is dangerous. 
  may_linkat - Check permissions for creating a hardlink
  @mnt_userns:	user namespace of the mount the inode was found from
  @link: the source to hardlink from
  Block hardlink when all of:
   - sysctl_protected_hardlinks enabled
   - fsuid does not match inode
   - hardlink source is unsafe (see safe_hardlink_source() above)
   - not CAP_FOWNER in a namespace with the inode owner uid mapped
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
  Returns 0 if successful, -ve on error.
 Inode writeback is not safe when the uid or gid are invalid. 
	 Source inode owner (or CAP_FOWNER) can hardlink all they like,
	  otherwise, it must be a safe source.
  may_create_in_sticky - Check whether an O_CREAT open in a sticky directory
 			  should be allowed, or not, on files that already
 			  exist.
  @mnt_userns:	user namespace of the mount the inode was found from
  @nd: nameidata pathwalk data
  @inode: the inode of the file to open
  Block an O_CREAT open of a FIFO (or a regular file) when:
    - sysctl_protected_fifos (or sysctl_protected_regular) is enabled
    - the file already exists
    - we are in a sticky directory
    - we don't own the file
    - the owner of the directory doesn't own the file
    - the directory is world writable
  If the sysctl_protected_fifos (or sysctl_protected_regular) is set to 2
  the directory doesn't have to be world writable: being group writable will
  be enough.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
  Returns 0 if the open is allowed, -ve on error.
  follow_up - Find the mountpoint of path's vfsmount
  Given a path, find the mountpoint of its source file system.
  Replace @path with the path of the mountpoint in the parent mount.
  Up is towards .
  Return 1 if we went up a level and 0 if we were already at the
  root.
  Perform an automount
  - return -EISDIR to tell follow_managed() to stop and return the path we
    were called with.
	 We don't want to mount if someone's just doing a stat -
	  unless they're stat'ing a directory and appended a '' to
	  the name.
	 
	  We do, however, want to mount if someone wants to open or
	  create a file of any type under the mountpoint, wants to
	  traverse through the mountpoint or wants to open the
	  mounted directory.  Also, autofs may mark negative dentries
	  as being automount points.  These will need the attentions
	  of the daemon to instantiate them before they can be used.
  mount traversal - out-of-line part.  One note on ->d_flags accesses -
  dentries are pinned but not locked here, so negative dentry can go
  positive right under us.  Use of smp_load_acquire() provides a barrier
  sufficient for ->d_inode and ->d_flags consistency.
		 Allow the filesystem to manage the transit without i_mutex
 something's mounted on it..
 ... in our namespace
 here we know it's positive
 uncovered automount point
 possible if you race with several mount --move
 fastpath 
  Follow down to the covering mount currently visible to userspace.  At each
  point, the filesystem owning that dentry may be queried as to whether the
  caller is permitted to proceed or not.
  Try to skip to top of mountpoint pile in rcuwalk mode.  Fail if
  we meet a managed dentry that would need blocking.
		
		  Don't forget we might have a non-mountpoint managed dentry
		  that wants to block transit.
				
				  We don't need to re-check ->d_seq after this
				  ->d_inode read - there will be an RCU delay
				  between mount hash removal and ->mnt_root
				  becoming unpinned.
 path might've been clobbered by __follow_mount_rcu()
 out of RCU mode, so the value doesn't matter 
  This looks up the name in dcache and possibly revalidates the found dentry.
  NULL is returned if the dentry does not exist in the cache.
  Parent directory has inode locked exclusive.  This is one
  and only case when ->lookup() gets called on non in-lookup
  dentries - as the matter of fact, this only gets called
  when directory is guaranteed to have no in-lookup children
  at all.
 Don't create child dentry for a dead directory. 
	
	  Rename seqlock is not required here because in the off chance
	  of a false negative due to a concurrent rename, the caller is
	  going to fall back to non-racy lookup.
		
		  This sequence count validates that the inode matches
		  the dentry name information from lookup.
		
		  This sequence count validates that the parent had no
		  changes while we did the lookup of the dentry above.
		 
		  The memory barrier in read_seqcount_begin of child is
		   enough, we can use __read_seqcount_retry here.
 we'd been told to redo it in non-rcu mode 
 Fast lookup failed, do it the slow way 
 Don't go there if it's already dead 
 we need to grab link before we do unlazy.  And we can't skip
 unlazy even if we fail to grab the link - cleanup needs it
 pure jump
  Do we need to follow links? We _really_ want to be able
  to do this check without having to look at inode->i_op,
  so we keep a cache of "no, this doesn't need follow_link"
  for the common case.
 not a symlink or should not follow 
 make sure that d_is_symlink above matches inode 
 we know that mountpoint was pinned 
 rare case of legitimate dget_parent()... 
			
			  If there was a racing rename or mount along our
			  path, then we can't be sure that ".." hasn't jumped
			  above nd->root (and so userspace should retry or use
			  some fallback).
	
	  "." and ".." are special - ".." especially so because it has
	  to be able to know about the current root directory and
	  parent relationships.
  We can do the critical dentry name comparison and hashing
  operations one word at a time, but we are limited to:
  - Architectures with fast unaligned word accesses. We could
    do a "get_unaligned()" if this helps and is sufficiently
    fast.
  - non-CONFIG_DEBUG_PAGEALLOC configurations (so that we
    do not trap on the (extremely unlikely) case of a page
    crossing operation.
  - Furthermore, we need an efficient 64-bit compile for the
    64-bit case in order to generate the "number of bytes in
    the final mask". Again, that could be replaced with a
    efficient population count instruction or similar.
 Architecture provides HASH_MIX and fold_hash() in <asmhash.h> 
  Register pressure in the mixing function is an issue, particularly
  on 32-bit x86, but almost any function requires one state value and
  one temporary.  Instead, use a function designed for two state values
  and no temporaries.
  This function cannot create a collision in only two iterations, so
  we have two iterations to achieve avalanche.  In those two iterations,
  we have six layers of mixing, which is enough to spread one bit's
  influence out to 2^6 = 64 state bits.
  Rotate constants are scored by considering either 64 one-bit input
  deltas or 64632 = 2016 two-bit input deltas, and finding the
  probability of that delta causing a change to each of the 128 output
  bits, using a sample of random initial states.
  The Shannon entropy of the computed probabilities is then summed
  to produce a score.  Ideally, any input change has a 50% chance of
  toggling any given output bit.
  Mixing scores (in bits) for (12,45):
  Input delta: 1-bit      2-bit
  1 round:     713.3    42542.6
  2 rounds:   2753.7   140389.8
  3 rounds:   5954.1   233458.2
  4 rounds:   7862.6   256672.2
  Perfect:    8192     258048
             (64128) (64632  128)
  Fold two longs into one 32-bit hash value.  This must be fast, but
  latency isn't quite as critical, as there is a fair bit of additional
  work done before the hash value is used.
 32-bit case 
  Mixing scores (in bits) for (7,20):
  Input delta: 1-bit      2-bit
  1 round:     330.3     9201.6
  2 rounds:   1246.4    25475.4
  3 rounds:   1907.1    31295.1
  4 rounds:   2042.3    31718.6
  Perfect:    2048      31744
             (3264)   (32312  64)
 Use arch-optimized multiply if one exists 
  Return the hash of a string of known length.  This is carfully
  designed to match hash_name(), which is the more critical function.
  In particular, we must end by hashing a final word containing 0..7
  payload bytes, to match the way that hash_name() iterates until it
  finds the delimiter after the name.
 Return the "hash_len" (hash and length) of a null-terminated string 
  Calculate the length and hash of the path component, and
  return the "hash_len" as the result.
 !CONFIG_DCACHE_WORD_ACCESS: Slow, byte-at-a-time version 
 Return the hash of a string of known length 
 Return the "hash_len" (hash and length) of a null-terminated string 
  We know there's a real path component here of at least
  one character.
  Name resolution.
  This is the basic name resolution function, turning a pathname into
  the final dentry. We expect 'base' to be positive and a directory.
  Returns 0 and nd will have valid dentry and mnt on success.
  Returns error and drops reference to input namei data on failure.
 depth <= nd->depth
 short-circuit the 'hardening' idiocy
 At this point we know we have a real path component. 
		
		  If it wasn't NUL, we know it was ''. Skip that
		  slash, and continue until no more slashes.
 pathname or trailing symlink, done 
 last component of nested symlink 
 not the last component 
 a symlink to follow 
 must be paired with terminate_walk() 
 LOOKUP_CACHED requires RCU, ask caller to retry 
 Absolute pathname -- fetch the root (LOOKUP_IN_ROOT uses nd->dfd). 
 Relative pathname -- get the starting-point it is relative to. 
 Caller must check execute permissions on the starting path component 
 For scoped-lookups we need to set the root to the dirfd as well. 
 Returns 0 and nd will be valid on success; Retuns error, otherwise. 
 no d_weak_revalidate(), please...
 Returns 0 and nd will be valid on success; Retuns error, otherwise. 
 Note: this does not consume "name" 
 does lookup, returns the object with parent locked 
  vfs_path_lookup - lookup a file path relative to a dentry-vfsmount pair
  @dentry:  pointer to dentry of the base directory
  @mnt: pointer to vfs mount of the base directory
  @name: pointer to file name
  @flags: lookup flags
  @path: pointer to struct path to fill
 the first argument of filename_lookup() is ignored with root 
	
	  See if the low-level filesystem might want
	  to use its own hash..
  try_lookup_one_len - filesystem helper to lookup single pathname component
  @name:	pathname component to lookup
  @base:	base directory to lookup from
  @len:	maximum length @len should be interpreted to
  Look up a dentry by name in the dcache, returning NULL if it does not
  currently exist.  The function does not try to create a dentry.
  Note that this routine is purely a helper for filesystem usage and should
  not be called by generic code.
  The caller must hold base->i_mutex.
  lookup_one_len - filesystem helper to lookup single pathname component
  @name:	pathname component to lookup
  @base:	base directory to lookup from
  @len:	maximum length @len should be interpreted to
  Note that this routine is purely a helper for filesystem usage and should
  not be called by generic code.
  The caller must hold base->i_mutex.
  lookup_one - filesystem helper to lookup single pathname component
  @mnt_userns:	user namespace of the mount the lookup is performed from
  @name:	pathname component to lookup
  @base:	base directory to lookup from
  @len:	maximum length @len should be interpreted to
  Note that this routine is purely a helper for filesystem usage and should
  not be called by generic code.
  The caller must hold base->i_mutex.
  lookup_one_len_unlocked - filesystem helper to lookup single pathname component
  @name:	pathname component to lookup
  @base:	base directory to lookup from
  @len:	maximum length @len should be interpreted to
  Note that this routine is purely a helper for filesystem usage and should
  not be called by generic code.
  Unlike lookup_one_len, it should be called without the parent
  i_mutex held, and will take the i_mutex itself if necessary.
  Like lookup_one_len_unlocked(), except that it yields ERR_PTR(-ENOENT)
  on negatives.  Returns known positive or ERR_PTR(); that's what
  most of the users want.  Note that pinned negative with unlocked parent
  _can_ become positive at any time, so callers of lookup_one_len_unlocked()
  need to be very careful; pinned positives have ->d_inode stable, so
  this one avoids such problems.
	 Find something mounted on "pts" in the same directory as
	  the input path.
 	Check whether we can remove a link victim from directory dir, check
   whether the type of victim is right.
   1. We can't do it if dir is read-only (done in permission())
   2. We should have write and exec permissions on dir
   3. We can't remove anything from append-only dir
   4. We can't do anything with immutable dir (done in permission())
   5. If the sticky bit on dir is set we should either
 	a. be owner of dir, or
 	b. be owner of victim, or
 	c. have CAP_FOWNER capability
   6. If the victim is append-only or immutable we can't do antyhing with
      links pointing to it.
   7. If the victim has an unknown uid or gid we can't change the inode.
   8. If we were asked to remove a directory and victim isn't one - ENOTDIR.
   9. If we were asked to remove a non-directory and victim isn't one - EISDIR.
  10. We can't remove a root or mountpoint.
  11. We don't allow removal of NFS sillyrenamed files; it's handled by
      nfs_async_unlink().
 Inode writeback is not safe when the uid or gid are invalid. 
	Check whether we can create an object with dentry child in directory
   dir.
   1. We can't do it if child already exists (open has special treatment for
      this case, but since we are inlined it's OK)
   2. We can't do it if dir is read-only (done in permission())
   3. We can't do it if the fs can't represent the fsuid or fsgid.
   4. We should have write and exec permissions on dir
   5. We can't do it if dir is immutable (done in permission())
  p1 and p2 should be directories on the same fs.
  vfs_create - create new file
  @mnt_userns:	user namespace of the mount the inode was found from
  @dir:	inode of @dentry
  @dentry:	pointer to dentry of the base directory
  @mode:	mode of the new file
  @want_excl:	whether the file must not yet exist
  Create a new file.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
 shouldn't it be ENOSYS? 
	
	  An append-only file must be opened in append mode for writing.
 O_NOATIME can only be set by the owner or superuser 
  Attempt to atomically look up, create and open a file from a negative
  dentry.
  Returns 0 if successful.  The file will have been created and attached to
  @file by the filesystem calling finish_open().
  If the file was looked up only or didn't need creating, FMODE_OPENED won't
  be set.  The caller will need to perform the open themselves.  @path will
  have been updated to point to the new dentry.  This may be negative.
  Returns an error code otherwise.
  Look up and maybe create and open the last component.
  Must be called with parent locked (exclusive in O_CREAT case).
  Returns 0 on success, that is, if
   the file was successfully atomically created (if necessary) and opened, or
   the file was not completely opened at this time, though lookups and
   creations were performed.
  These case are distinguished by presence of FMODE_OPENED on file->f_mode.
  In the latter case dentry returned in @path might be negative if O_CREAT
  hadn't been specified.
  An error code is returned on failure.
 Cached positive dentry: will open in f_op->open 
	
	  Checking write permission is tricky, bacuse we don't know if we are
	  going to actually need it: O_CREAT opens should work as long as the
	  file exists.  But checking existence breaks atomicity.  The trick is
	  to check access and if not granted clear O_CREAT from the flags.
	 
	  Another problem is returing the "right" error value (e.g. for an
	  O_EXCL open we want to return EEXIST not EROFS).
 Negative dentry, just create the file 
 we _can_ be in RCU mode here 
 create side of things 
 trailing slashes? 
		
		  do _not_ fail yet - we might not need that or fail with
		  a different error; let lookup_open() decide; we'll be
		  dropping this one anyway.
  Handle the last step of open()
 Don't check for write permission, don't truncate 
  vfs_tmpfile - create tmpfile
  @mnt_userns:	user namespace of the mount the inode was found from
  @dentry:	pointer to dentry of the base directory
  @mode:	mode of the new tmpfile
  @open_flag:	flags
  Create a temporary file.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
 we want directory to be writable 
 Don't check for other permissions, the inode was just created 
	
	  Note that only LOOKUP_REVAL and LOOKUP_DIRECTORY matter here. Any
	  other flags passed in are ignored!
	
	  Yucky last component or no last component at all?
	  (foo., foo.., )
 don't fail immediately if it's ro, at least try to report other errors 
	
	  Do the final lookup.
	
	  Special case - lookup gave negative, but... we had foobar
	  From the vfs_mknod() POV we just have a negative dentry -
	  all is fine. Let's be bastards - you had  on the end, you've
	  been asking for (non-existent) directory. -ENOENT for you.
  vfs_mknod - create device node or file
  @mnt_userns:	user namespace of the mount the inode was found from
  @dir:	inode of @dentry
  @dentry:	pointer to dentry of the base directory
  @mode:	mode of the new device node or file
  @dev:	device number of device to create
  Create a device node or file.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
 zero mode translates to S_IFREG 
  vfs_mkdir - create directory
  @mnt_userns:	user namespace of the mount the inode was found from
  @dir:	inode of @dentry
  @dentry:	pointer to dentry of the base directory
  @mode:	mode of the new directory
  Create a directory.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
  vfs_rmdir - remove directory
  @mnt_userns:	user namespace of the mount the inode was found from
  @dir:	inode of @dentry
  @dentry:	pointer to dentry of the base directory
  Remove a directory.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
  vfs_unlink - unlink a filesystem object
  @mnt_userns:	user namespace of the mount the inode was found from
  @dir:	parent directory
  @dentry:	victim
  @delegated_inode: returns victim inode, if the inode is delegated.
  The caller must hold dir->i_mutex.
  If vfs_unlink discovers a delegation, it will return -EWOULDBLOCK and
  return a reference to the inode in delegated_inode.  The caller
  should then break the delegation on that inode and retry.  Because
  breaking a delegation may take a long time, the caller should drop
  dir->i_mutex before doing so.
  Alternatively, a caller may pass NULL for delegated_inode.  This may
  be appropriate for callers that expect the underlying filesystem not
  to be NFS exported.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
 We don't d_delete() NFS sillyrenamed files--they still exist. 
  Make sure that the actual truncation of the file will occur outside its
  directory's i_mutex.  Truncate can take a long time if there is a lot of
  writeout happening, and we don't want to prevent access to the directory
  while waiting on the IO.
 Why not before? Because we want correct error value 
 truncate the inode here 
  vfs_symlink - create symlink
  @mnt_userns:	user namespace of the mount the inode was found from
  @dir:	inode of @dentry
  @dentry:	pointer to dentry of the base directory
  @oldname:	name of the file to link to
  Create a symlink.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
  vfs_link - create a new link
  @old_dentry:	object to be linked
  @mnt_userns:	the user namespace of the mount
  @dir:	new parent
  @new_dentry:	where to create the new link
  @delegated_inode: returns inode needing a delegation break
  The caller must hold dir->i_mutex
  If vfs_link discovers a delegation on the to-be-linked file in need
  of breaking, it will return -EWOULDBLOCK and return a reference to the
  inode in delegated_inode.  The caller should then break the delegation
  and retry.  Because breaking a delegation may take a long time, the
  caller should drop the i_mutex before doing so.
  Alternatively, a caller may pass NULL for delegated_inode.  This may
  be appropriate for callers that expect the underlying filesystem not
  to be NFS exported.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
	
	  A link to an append-only or immutable file cannot be created.
	
	  Updating the link count will likely cause i_uid and i_gid to
	  be writen back improperly if their true value is unknown to
	  the vfs.
 Make sure we don't allow creating hardlink to an unlinked file 
  Hardlinks are often used in delicate situations.  We avoid
  security-related surprises by not following symlinks on the
  newname.  --KAB
  We don't follow them on the oldname either to be compatible
  with linux 2.0, and to avoid hard-linking to directories
  and other special files.  --ADM
	
	  To use null names we require CAP_DAC_READ_SEARCH
	  This ensures that not everyone will be able to create
	  handlink using the passed filedescriptor.
  vfs_rename - rename a filesystem object
  @rd:		pointer to &struct renamedata info
  The caller must hold multiple mutexes--see lock_rename()).
  If vfs_rename discovers a delegation in need of breaking at either
  the source or destination, it will return -EWOULDBLOCK and return a
  reference to the inode in delegated_inode.  The caller should then
  break the delegation and retry.  Because breaking a delegation may
  take a long time, the caller should drop all locks before doing
  so.
  Alternatively, a caller may pass NULL for delegated_inode.  This may
  be appropriate for callers that expect the underlying filesystem not
  to be NFS exported.
  The worst of all namespace operations - renaming directory. "Perverted"
  doesn't even start to describe it. Somebody in UCB had a heck of a trip...
  Problems:
 	a) we can get into loop creation.
 	b) race potential - two innocent renames can create a loop together.
 	   That's where 4.4 screws up. Current fix: serialization on
 	   sb->s_vfs_rename_mutex. We might be more accurate, but that's another
 	   story.
 	c) we have to lock _four_ objects - parents and victim (if it exists),
 	   and source (if it is not a directory).
 	   And that - after we got ->i_mutex on parents (until then we don't know
 	   whether the target exists).  Solution: try to be smart with locking
 	   order for inodes.  We rely on the fact that tree topology may change
 	   only under ->s_vfs_rename_mutex _and_ that parent of the object we
 	   move will be locked.  Thus we can rank directories by the tree
 	   (ancestors first) and rank all non-directories after them.
 	   That works since everybody except rename does "lock parent, lookup,
 	   lock child" and rename is under ->s_vfs_rename_mutex.
 	   HOWEVER, it relies on the assumption that any object with ->lookup()
 	   has no more than 1 dentry.  If "hybrid" objects will ever appear,
 	   we'd better make sure that there's no link(2) for them.
 	d) conversion from fhandle to dentry may come in the wrong moment - when
 	   we are removing the target. Solution: we will have to grab ->i_mutex
 	   in the fhandle_to_dentry code. [FIXME - current nfsfh.c relies on
 	   ->i_mutex on parents, which works but leads to some truly excessive
 	   locking].
	
	  If we are going to change the parent - check write permissions,
	  we'll need to flip '..'.
 source must exist 
 unless the source is a directory trailing slashes give -ENOTDIR 
 source should not be ancestor of target 
 target should not be an ancestor of source 
  vfs_readlink - copy symlink body into userspace buffer
  @dentry: dentry on which to get symbolic link
  @buffer: user memory pointer
  @buflen: size of buffer
  Does not touch atime.  That's up to the caller if necessary
  Does not call security hook.
  vfs_get_link - get symlink body
  @dentry: dentry on which to get symbolic link
  @done: caller needs to free returned data with this
  Calls security hook and i_op->get_link() on the supplied inode.
  It does not touch atime.  That's up to the caller if necessary.
  Does not work on "special" symlinks like proc$$fdN
 get the link contents into pagecache 
  The nofs argument instructs pagecache_write_begin to pass AOP_FLAG_NOFS
 SPDX-License-Identifier: GPL-2.0
   linuxfssuper.c
   Copyright (C) 1991, 1992  Linus Torvalds
   super.c contains code to handle: - mount structures
                                    - super-block tables
                                    - filesystem drivers list
                                    - mount system call
                                    - umount system call
                                    - ustat system call
  GK 2595  -  Changed to support mounting the root fs via NFS
   Added kerneld support: Jacques Gelinas and Bjorn Ekwall
   Added change_root: Werner Almesberger & Hans Lermen, Feb '96
   Added options to procmounts:
     Torbjrn Lindh (torbjorn.lindh@gopta.se), April 14, 1996.
   Added devfs support: Richard Gooch <rgooch@atnf.csiro.au>, 13-JAN-1998
   Heavily rewritten for 'one fs - one tree' dcache architecture. AV, Mar 2000
 for the emergency remount stuff 
  One thing we have to be careful of with a per-sb shrinker is that we don't
  drop the last active reference to the superblock from within the shrinker.
  If that happens we could trigger unregistering the shrinker from within the
  shrinker path and that leads to deadlock on the shrinker_rwsem. Hence we
  take a passive reference to the superblock to avoid this from occurring.
	
	  Deadlock avoidance.  We may hold various FS locks, and we don't want
	  to recurse into the FS that called us in clear_inode() and friends..
 proportion the scan between the caches 
	
	  prune the dcache first as the icache is pinned by it, then
	  prune the icache, followed by the filesystem specific caches
	 
	  Ensure that we always scan at least one object - memcg kmem
	  accounting uses this to fully empty the caches.
	
	  We don't call trylock_super() here as it is a scalability bottleneck,
	  so we're exposed to partial setup state. The shrinker rwsem does not
	  protect filesystem operations backing list_lru_shrink_count() or
	  s_op->nr_cached_objects(). Counts can change between
	  super_cache_count and super_cache_scan, so we really don't need locks
	  here.
	 
	  However, if we are currently mounting the superblock, the underlying
	  filesystem might be in a state of partial construction and hence it
	  is dangerous to access it.  trylock_super() uses a SB_BORN check to
	  avoid this situation, so do the same here. The memory barrier is
	  matched with the one in mount_fs() as we don't hold locks here.
 Free a superblock that has never been seen by anyone 
 no delays needed 
 	alloc_super	-	create new superblock
 	@type:	filesystem type superblock should belong to
 	@flags: the mount flags
 	@user_ns: User namespace for the super_block
 	Allocates and initializes a new &struct super_block.  alloc_super()
 	returns a pointer new superblock or %NULL if allocation had failed.
	
	  sget() can have s_umount recursion.
	 
	  When it cannot find a suitable sb, it allocates a new
	  one (this one), and tries again to find a suitable old
	  one.
	 
	  In case that succeeds, it will acquire the s_umount
	  lock of the old one. Since these are clearly distrinct
	  locks, and this object isn't exposed yet, there's no
	  risk of deadlocks.
	 
	  Annotate this by putting this lock in a different
	  subclass.
 Superblock refcounting  
  Drop a superblock's refcount.  The caller must hold sb_lock.
 	put_super	-	drop a temporary reference to superblock
 	@sb: superblock in question
 	Drops a temporary reference, frees superblock if there's no
 	references left.
 	deactivate_locked_super	-	drop an active reference to superblock
 	@s: superblock to deactivate
 	Drops an active reference to superblock, converting it into a temporary
 	one if there is no other active references left.  In that case we
 	tell fs driver to shut it down and drop the temporary reference we
 	had just acquired.
 	Caller holds exclusive lock on superblock; that lock is released.
		
		  Since list_lru_destroy() may sleep, we cannot call it from
		  put_super(), where we hold the sb_lock. Therefore we destroy
		  the lru lists right now.
 	deactivate_super	-	drop an active reference to superblock
 	@s: superblock to deactivate
 	Variant of deactivate_locked_super(), except that superblock is not
 	locked by caller.  If we are going to drop the final active reference,
 	lock will be acquired prior to that.
 	grab_super - acquire an active reference
 	@s: reference we are trying to make active
 	Tries to acquire an active reference.  grab_super() is used when we
  	had just found a superblock in super_blocks or fs_type->fs_supers
 	and want to turn it into a full-blown active reference.  grab_super()
 	is called with sb_lock held and drops it.  Returns 1 in case of
 	success, 0 if we had failed (superblock contents was already dead or
 	dying when grab_super() had been called).  Note that this is only
 	called for superblocks not in rundown mode (== ones still on ->fs_supers
 	of their type), so increment of ->s_count is OK here.
 	trylock_super - try to grab ->s_umount shared
 	@sb: reference we are trying to grab
 	Try to prevent fs shutdown.  This is used in places where we
 	cannot take an active reference but we need to ensure that the
 	filesystem is not shut down while we are working on it. It returns
 	false if we cannot acquire s_umount or if we lose the race and
 	filesystem already got into shutdown, and returns true with the s_umount
 	lock held in read mode in case of success. On successful return,
 	the caller must drop the s_umount lock when done.
 	Note that unlike get_super() et.al. this one does not bump ->s_count.
 	The reason why it's safe is that we are OK with doing trylock instead
 	of down_read().  There's a couple of places that are OK with that, but
 	it's very much not a general-purpose interface.
 	generic_shutdown_super	-	common helper for ->kill_sb()
 	@sb: superblock to kill
 	generic_shutdown_super() does all fs-independent work on superblock
 	shutdown.  Typical ->kill_sb() should pick all fs-specific objects
 	that need destruction out of superblock, call generic_shutdown_super()
 	and release aforementioned objects.  Note: dentries and inodes _are_
 	taken care of and do not need specific handling.
 	Upon calling this function, the filesystem may no longer alter or
 	rearrange the set of dentries belonging to this super_block, nor may it
 	change the attachments of dentries to inodes.
 evict all inodes with zero refcount 
 only nonzero refcount inodes can have marks 
 should be initialized for __put_super_and_need_restart() 
  sget_fc - Find or create a superblock
  @fc:	Filesystem context.
  @test: Comparison callback
  @set: Setup callback
  Find or create a superblock using the parameters stored in the filesystem
  context and the two callback functions.
  If an extant superblock is matched, then that will be returned with an
  elevated reference count that the caller must transfer or discard.
  If no match is made, a new superblock will be allocated and basic
  initialisation will be performed (s_type, s_fs_info and s_id will be set and
  the set() callback will be invoked), the superblock will be published and it
  will be returned in a partially constructed state with SB_BORN and SB_ACTIVE
  as yet unset.
 	sget	-	find or create a superblock
 	@type:	  filesystem type superblock should belong to
 	@test:	  comparison callback
 	@set:	  setup callback
 	@flags:	  mount flags
 	@data:	  argument to each of them
	 We don't yet pass the user namespace of the parent
	  mount through to here so always use &init_user_ns
	  until that changes.
 	iterate_supers - call function for all active superblocks
 	@f: function to call
 	@arg: argument to pass to it
 	Scans the superblock list and calls given function, passing it
 	locked superblock and given argument.
 	iterate_supers_type - call function for superblocks of given type
 	@type: fs type
 	@f: function to call
 	@arg: argument to pass to it
 	Scans the superblock list and calls given function, passing it
 	locked superblock and given argument.
  get_super - get the superblock of a device
  @bdev: device to get the superblock for
  Scans the superblock list and finds the superblock of the file system
  mounted on the device given. %NULL is returned if no match is found.
 still alive? 
 nope, got unmounted 
  get_active_super - get an active reference to the superblock of a device
  @bdev: device to get the superblock for
  Scans the superblock list and finds the superblock of the file system
  mounted on the device given.  Returns the superblock with an active
  reference or %NULL if none was found.
 still alive? 
 nope, got unmounted 
  reconfigure_super - asks filesystem to change superblock parameters
  @fc: The superblock and configuration
  Alters the configuration parameters of a live superblock.
	 If we are reconfiguring to RDONLY and current sb is readwrite,
	  make sure there are no files open for writing.
 If forced remount, go ahead despite any errors 
 Needs to be ordered wrt mnt_is_readonly() 
	
	  Some filesystems modify their metadata via some other path than the
	  bdev buffer cache (eg. use a private mapping, or directories in
	  pagecache, etc). Also file data modifications go via their own
	  mappings. So If we try to mount readonly then copy the filesystem
	  from bdev, we could get stale data, so invalidate it to give a best
	  effort at coherency.
  emergency_thaw_all -- forcibly thaw every frozen filesystem
  Used for emergency unfreeze of all filesystems via SysRq
  get_anon_bdev - Allocate a block device for filesystems which don't have one.
  @p: Pointer to a dev_t.
  Filesystems which don't use real block devices can call this function
  to allocate a virtual block device.
  Context: Any context.  Frequently called while holding sb_lock.
  Return: 0 on success, -EMFILE if there are no anonymous bdevs left
  or -ENOMEM if memory allocation failed.
	
	  Many userspace utilities consider an FSID of 0 invalid.
	  Always return at least 1 from get_anon_bdev.
  vfs_get_super - Get a superblock with a search key set in s_fs_info.
  @fc: The filesystem context holding the parameters
  @keying: How to distinguish superblocks
  @fill_super: Helper to initialise a new superblock
  Search for a superblock and create a new one if not found.  The search
  criterion is controlled by @keying.  If the search fails, a new superblock
  is created and @fill_super() is called to initialise it.
  @keying can take one of a number of values:
  (1) vfs_get_single_super - Only one superblock of this type may exist on the
      system.  This is typically used for special system filesystems.
  (2) vfs_get_keyed_super - Multiple superblocks may exist, but they must have
      distinct keys (where the key is in s_fs_info).  Searching for the same
      key again will turn up the superblock for that key.
  (3) vfs_get_independent_super - Multiple superblocks may exist and are
      unkeyed.  Each call will get a new superblock.
  A permissions check is made by sget_fc() unless we're getting a superblock
  for a kernel-internal mount or a submount.
  get_tree_bdev - Get a superblock based on a single block device
  @fc: The filesystem context holding the parameters
  @fill_super: Helper to initialise a new superblock
	 Once the superblock is inserted into the list by sget_fc(), s_umount
	  will protect the lockfs code from trying to start a snapshot while
	  we are mounting
 Don't summarily change the RORW state. 
		
		  s_umount nests inside open_mutex during
		  __invalidate_device().  blkdev_put() acquires
		  open_mutex and can't be called under s_umount.  Drop
		  s_umount temporarily.  This is safe as we're
		  holding an active reference.
	
	  once the super is inserted into the list by sget, s_umount
	  will protect the lockfs code from trying to start a snapshot
	  while we are mounting
		
		  s_umount nests inside open_mutex during
		  __invalidate_device().  blkdev_put() acquires
		  open_mutex and can't be called under s_umount.  Drop
		  s_umount temporarily.  This is safe as we're
		  holding an active reference.
	 The caller really need to be passing fc down into mount_single(),
	  then a chunk of this can be removed.  [Bollocks -- AV]
	  Better yet, reconfiguration shouldn't happen, but rather the second
	  mount should be rejected if the parameters are not compatible.
  vfs_get_tree - Get the mountable root
  @fc: The superblock configuration context.
  The filesystem is invoked to get or create a superblock which can then later
  be used for mounting.  The filesystem places a pointer to the root to be
  used for mounting in @fc->root.
	 Get the mountable root in fc->root, with a ref on the root and a ref
	  on the superblock.
		 We don't know what the locking state of the superblock is -
		  if there is a superblock.
	
	  Write barrier is for super_cache_count(). We place it before setting
	  SB_BORN as the data dependency between the two functions is the
	  superblock structure contents that we just set up, not the SB_BORN
	  flag.
	
	  filesystems should never set s_maxbytes larger than MAX_LFS_FILESIZE
	  but s_maxbytes was an unsigned long long for many releases. Throw
	  this warning for a little while to try and catch filesystems that
	  violate this rule.
  Setup private BDI for given superblock. It gets automatically cleaned up
  in generic_shutdown_super().
  Setup private BDI for given superblock. I gets automatically cleaned up
  in generic_shutdown_super().
  sb_wait_write - wait until all writers to given file system finish
  @sb: the super for which we wait
  @level: type of writers we wait for (normal vs page fault)
  This function waits until there are no writers of given type to given file
  system.
  We are going to return to userspace and forget about these locks, the
  ownership goes to the caller of thaw_super() which does unlock().
  Tell lockdep we are holding these locks before we call ->unfreeze_fs(sb).
  freeze_super - lock the filesystem and force it into a consistent state
  @sb: the super to lock
  Syncs the super to make sure the filesystem is consistent and calls the fs's
  freeze_fs.  Subsequent calls to this without first thawing the fs will return
  -EBUSY.
  During this function, sb->s_writers.frozen goes through these values:
  SB_UNFROZEN: File system is normal, all writes progress as usual.
  SB_FREEZE_WRITE: The file system is in the process of being frozen.  New
  writes should be blocked, though page faults are still allowed. We wait for
  all writes to complete and then proceed to the next stage.
  SB_FREEZE_PAGEFAULT: Freezing continues. Now also page faults are blocked
  but internal fs threads can still modify the filesystem (although they
  should not dirty new pages or inodes), writeback can run etc. After waiting
  for all running page faults we sync the filesystem which will clean all
  dirty pages and inodes (no new dirty pages or inodes can be created when
  sync is running).
  SB_FREEZE_FS: The file system is frozen. Now all internal sources of fs
  modification are blocked (e.g. XFS preallocation truncation on inode
  reclaim). This is usually implemented by blocking new transactions for
  filesystems that have them and need this additional guard. After all
  internal writers are finished we call ->freeze_fs() to finish filesystem
  freezing. Then we transition to SB_FREEZE_COMPLETE state. This state is
  mostly auxiliary for filesystems to verify they do not modify frozen fs.
  sb->s_writers.frozen is protected by sb->s_umount.
 sic - it's "nothing to do" 
 Nothing to do really... 
 Release s_umount to preserve sb_start_write -> s_umount ordering 
 Now we go and block page faults... 
 All writers are done so after syncing there won't be dirty data 
 Now wait for internal filesystem counter 
	
	  For debugging purposes so that fs can warn if it sees write activity
	  when frozen is set to SB_FREEZE_COMPLETE, and for thaw_super().
  thaw_super -- unlock filesystem
  @sb: the super to thaw
  Unlocks the filesystem and marks it writeable again after freeze_super().
 SPDX-License-Identifier: GPL-2.0-or-later
   fseventpoll.c (Efficient event retrieval implementation)
   Copyright (C) 2001,...,2009	 Davide Libenzi
   Davide Libenzi <davidel@xmailserver.org>
  LOCKING:
  There are three level of locking required by epoll :
  1) epmutex (mutex)
  2) ep->mtx (mutex)
  3) ep->lock (rwlock)
  The acquire order is the one listed above, from 1 to 3.
  We need a rwlock (ep->lock) because we manipulate objects
  from inside the poll callback, that might be triggered from
  a wake_up() that in turn might be called from IRQ context.
  So we can't sleep inside the poll callback and hence we need
  a spinlock. During the event transfer loop (from kernel to
  user space) we could end up sleeping due a copy_to_user(), so
  we need a lock that will allow us to sleep. This lock is a
  mutex (ep->mtx). It is acquired during the event transfer loop,
  during epoll_ctl(EPOLL_CTL_DEL) and during eventpoll_release_file().
  Then we also need a global mutex to serialize eventpoll_release_file()
  and ep_free().
  This mutex is acquired by ep_free() during the epoll file
  cleanup path and it is also acquired by eventpoll_release_file()
  if a file has been pushed inside an epoll set and it is then
  close()d without a previous call to epoll_ctl(EPOLL_CTL_DEL).
  It is also acquired when inserting an epoll fd onto another epoll
  fd. We do this so that we walk the epoll tree and ensure that this
  insertion does not create a cycle of epoll file descriptors, which
  could lead to deadlock. We need a global mutex to prevent two
  simultaneous inserts (A into B and B into A) from racing and
  constructing a cycle without either insert observing that it is
  going to.
  It is necessary to acquire multiple "ep->mtx"es at once in the
  case when one epoll fd is added to another. In this case, we
  always acquire the locks in the order of nesting (i.e. after
  epoll_ctl(e1, EPOLL_CTL_ADD, e2), e1->mtx will always be acquired
  before e2->mtx). Since we disallow cycles of epoll file
  descriptors, this ensures that the mutexes are well-ordered. In
  order to communicate this nesting to lockdep, when walking a tree
  of epoll file descriptors, we use the current recursion depth as
  the lockdep subkey.
  It is possible to drop the "ep->mtx" and to use the global
  mutex "epmutex" (together with "ep->lock") to have it working,
  but having "ep->mtx" will make the interface more scalable.
  Events that require holding "epmutex" are very rare, while for
  normal operations the epoll private "ep->mtx" will guarantee
  a better scalability.
 Epoll private bits inside the event mask 
 Maximum number of nesting allowed inside epoll sets 
 Wait structure used by the poll hooks 
 List header used to link this structure to the "struct epitem" 
 The "base" pointer is set to the container "struct epitem" 
	
	  Wait queue item that will be linked to the target file wait
	  queue head.
 The wait queue head that linked the "wait" wait queue item 
  Each file descriptor added to the eventpoll interface will
  have an entry of this type linked to the "rbr" RB tree.
  Avoid increasing the size of this struct, there can be many thousands
  of these on a server and we do not want this to take another cache line.
 RB tree node links this structure to the eventpoll RB tree 
 Used to free the struct epitem 
 List header used to link this structure to the eventpoll ready list 
	
	  Works together "struct eventpoll"->ovflist in keeping the
	  single linked chain of items.
 The file descriptor information this item refers to 
 List containing poll wait queues 
 The "container" of this item 
 List header used to link this item to the "struct file" items list 
 wakeup_source used when EPOLLWAKEUP is set 
 The structure that describe the interested events and the source fd 
  This structure is stored inside the "private_data" member of the file
  structure and represents the main data structure for the eventpoll
  interface.
	
	  This mutex is used to ensure that files are not removed
	  while epoll is using them. This is held during the event
	  collection loop, the file cleanup path, the epoll file exit
	  code and the ctl operations.
 Wait queue used by sys_epoll_wait() 
 Wait queue used by file->poll() 
 List of ready file descriptors 
 Lock which protects rdllist and ovflist 
 RB tree root used to store monitored fd structs 
	
	  This is a single linked list that chains all the "struct epitem" that
	  happened while transferring ready events to userspace wout
	  holding ->lock.
 wakeup_source used when ep_scan_ready_list is running 
 The user that created the eventpoll descriptor 
 used to optimize loop detection check 
 used to track busy poll napi_id 
 tracks wakeup nests for lockdep validation 
 Wrapper struct used by poll queueing 
  Configuration options available inside procsysfsepoll
 Maximum number of epoll watched descriptors, per user 
  This mutex is used to serialize ep_free() and eventpoll_release_file().
 Used to check for epoll file descriptor inclusion loops 
 Slab cache used to allocate "struct epitem" 
 Slab cache used to allocate "struct eppoll_entry" 
  List of files with newly added links, where we may need to limit the number
  of emanating paths. Protected by the epmutex.
 CONFIG_SYSCTL 
 Setup the structure that is used as key for the RB tree 
 Compare RB tree keys 
 Tells us if the item is currently linked 
 Get the "struct epitem" from a wait queue pointer 
  ep_events_available - Checks if ready events might be available.
  @ep: Pointer to the eventpoll context.
  Return: a value different than %zero if ready events are available,
           or %zero otherwise.
  Busy poll if globally on and supporting sockets found && no events,
  busy loop will return if need_resched or ep_events_available.
  we must do our busy polling with irqs enabled
		
		  Busy poll timed out.  Drop NAPI ID for now, we can add
		  it back in when we have moved a socket with a valid NAPI
		  ID onto the ready list.
  Set epoll busy poll NAPI ID from sk.
	 Non-NAPI IDs can be rejected
	 	or
	  Nothing to do if we already have this ID
 record NAPI ID for use in next busy poll 
 CONFIG_NET_RX_BUSY_POLL 
  As described in commit 0ccf831cb lockdep: annotate epoll
  the use of wait queues used by epoll is done in a very controlled
  manner. Wake ups can nest inside each other, but are never done
  with the same locking. For example:
    dfd = socket(...);
    efd1 = epoll_create();
    efd2 = epoll_create();
    epoll_ctl(efd1, EPOLL_CTL_ADD, dfd, ...);
    epoll_ctl(efd2, EPOLL_CTL_ADD, efd1, ...);
  When a packet arrives to the device underneath "dfd", the net code will
  issue a wake_up() on its poll wake list. Epoll (efd1) has installed a
  callback wakeup entry on that queue, and the wake_up() performed by the
  "dfd" net code will end up in ep_poll_callback(). At this point epoll
  (efd1) notices that it may have some event ready, so it needs to wake up
  the waiters on its poll wait list (efd2). So it calls ep_poll_safewake()
  that ends up in another wake_up(), after having checked about the
  recursion constraints. That are, no more than EP_MAX_POLLWAKE_NESTS, to
  avoid stack blasting.
  When CONFIG_DEBUG_LOCK_ALLOC is enabled, make sure lockdep can handle
  this special case of epoll.
	
	  To set the subclass or nesting level for spin_lock_irqsave_nested()
	  it might be natural to create a per-cpu nest count. However, since
	  we can recurse on ep->poll_wait.lock, and a non-raw spinlock can
	  schedule() in the -rt kernel, the per-cpu variable are no longer
	  protected. Thus, we are introducing a per eventpoll nest field.
	  If we are not being call from ep_poll_callback(), epi is NULL and
	  we are at the first level of nesting, 0. Otherwise, we are being
	  called from ep_poll_callback() and if a previous wakeup source is
	  not an epoll file itself, we are at depth 1 since the wakeup source
	  is depth 0. If the wakeup source is a previous epoll file in the
	  wakeup chain then we use its nests value and record ours as
	  nests + 1. The previous epoll file nests value is stable since its
	  already holding its own poll_wait.lock.
	
	  If it is cleared by POLLFREE, it should be rcu-safe.
	  If we read NULL we need a barrier paired with
	  smp_store_release() in ep_poll_callback(), otherwise
	  we rely on whead->lock.
  This function unregisters poll callbacks from the associated file
  descriptor.  Must be called with "mtx" held (or "epmutex" if called from
  ep_free).
 call only when ep->mtx is held 
 call only when ep->mtx is held 
 call when ep->mtx cannot be held (ep_poll_callback) 
  ep->mutex needs to be held because we could be hit by
  eventpoll_release_file() and epoll_ctl().
	
	  Steal the ready list, and re-init the original one to the
	  empty list. Also, set ep->ovflist to NULL so that events
	  happening while looping wout locks, are not lost. We cannot
	  have the poll callback to queue directly on ep->rdllist,
	  because we want the "sproc" callback to be able to do it
	  in a lockless way.
	
	  During the time we spent inside the "sproc" callback, some
	  other events might have been queued by the poll callback.
	  We re-insert them inside the main ready-list here.
		
		  We need to check if the item is already in the list.
		  During the "sproc" callback execution time, items are
		  queued into ->ovflist but the "txlist" might already
		  contain them, and the list_splice() below takes care of them.
			
			  ->ovflist is LIFO, so we have to reverse it in order
			  to keep in FIFO.
	
	  We need to set back ep->ovflist to EP_UNACTIVE_PTR, so that after
	  releasing the lock, events will be queued in the normal way inside
	  ep->rdllist.
	
	  Quickly re-inject items left on "txlist".
  Removes a "struct epitem" from the eventpoll RB tree and deallocates
  all the associated resources. Must be called with "mtx" held.
	
	  Removes poll wait queue hooks.
 Remove the current item from the list of epoll hooks 
	
	  At this point it is safe to free the eventpoll item. Use the union
	  field epi->rcu, since we are trying to minimize the size of
	  'struct epitem'. The 'rbn' field is no longer in use. Protected by
	  ep->mtx. The rcu read side, reverse_path_check_proc(), does not make
	  use of the rbn field.
 We need to release all tasks waiting for these file 
	
	  We need to lock this because we could be hit by
	  eventpoll_release_file() while we're freeing the "struct eventpoll".
	  We do not need to hold "ep->mtx" here because the epoll file
	  is on the way to be removed and no one has references to it
	  anymore. The only hit might come from eventpoll_release_file() but
	  holding "epmutex" is sufficient here.
	
	  Walks through the whole tree by unregistering poll callbacks.
	
	  Walks through the whole tree by freeing each "struct epitem". At this
	  point we are sure no poll callbacks will be lingering around, and also by
	  holding "epmutex" we can be sure that no file cleanup code will hit
	  us during this operation. So we can avoid the lock on "ep->lock".
	  We do not need to lock ep->mtx, either, we only do it to prevent
	  a lockdep warning.
 Insert inside our poll wait queue 
	
	  Proceed to find out if wanted events are really available inside
	  the ready list.
			
			  Item has been dropped into the ready list by the poll
			  callback, but it's not actually ready, as far as
			  caller requested events goes. We can remove it here.
  Differs from ep_eventpoll_poll() in that internal callers already have
  the ep->mtx so we need to start from depth=1, such that mutex_lock_nested()
  is correctly annotated.
 File callbacks that implement the eventpoll file behaviour 
  This is called from eventpoll_release() to unlink files from the eventpoll
  interface. We need to have this facility to cleanup correctly files that are
  closed without being removed from the eventpoll interface.
	
	  We don't want to get "file->f_lock" because it is not
	  necessary. It is not necessary because we're in the "struct file"
	  cleanup path, and this means that no one is using this file anymore.
	  So, for example, epoll_ctl() cannot hit here since if we reach this
	  point, the file counter already went to zero and fget() would fail.
	  The only hit might come from ep_free() but by holding the mutex
	  will correctly serialize the operation. We do need to acquire
	  "ep->mtx" after "epmutex" because ep_remove() requires it when called
	  from anywhere but ep_free().
	 
	  Besides, ep_remove() acquires the lock, so we can't hold it here.
  Search the file inside the eventpoll tree. The RB tree operations
  are protected by the "mtx" mutex, and ep_find() must be called with
  "mtx" held.
 CONFIG_KCMP 
  Adds a new entry to the tail of the list in a lockless way, i.e.
  multiple CPUs are allowed to call this function concurrently.
  Beware: it is necessary to prevent any other modifications of the
          existing list until all changes are completed, in other words
          concurrent list_add_tail_lockless() calls should be protected
          with a read lock, where write lock acts as a barrier which
          makes sure all list_add_tail_lockless() calls are fully
          completed.
         Also an element can be locklessly added to the list only in one
         direction i.e. either to the tail or to the head, otherwise
         concurrent access will corrupt the list.
  Return: %false if element has been already added to the list, %true
  otherwise.
	
	  This is simple 'new->next = head' operation, but cmpxchg()
	  is used in order to detect that same element has been just
	  added to the list from another CPU: the winner observes
	  new->next == new.
	
	  Initially ->next of a new element must be updated with the head
	  (we are inserting to the tail) and only then pointers are atomically
	  exchanged.  XCHG guarantees memory ordering, thus ->next should be
	  updated before pointers are actually swapped and pointers are
	  swapped before prev->next is updated.
	
	  It is safe to modify prev->next and new->prev, because a new element
	  is added only to the tail and new->next is updated before XCHG.
  Chains a new epi entry to the tail of the ep->ovflist in a lockless way,
  i.e. multiple CPUs are allowed to call this function concurrently.
  Return: %false if epi element has been already chained, %true otherwise.
 Fast preliminary check 
 Check that the same epi has not been just chained from another CPU 
 Atomically exchange tail 
  This is the callback that is passed to the wait queue wakeup
  mechanism. It is called by the stored file descriptors when they
  have events to report.
  This callback takes a read lock in order not to contend with concurrent
  events from another file descriptor, thus all modifications to ->rdllist
  or ->ovflist are lockless.  Read lock is paired with the write lock from
  ep_scan_ready_list(), which stops all list modifications and guarantees
  that lists state is seen correctly.
  Another thing worth to mention is that ep_poll_callback() can be called
  concurrently for the same @epi from different CPUs if poll table was inited
  with several wait queues entries.  Plural wakeup from different CPUs of a
  single wait queue is serialized by wq.lock, but the case when multiple wait
  queues are used should be detected accordingly.  This is detected using
  cmpxchg() operation.
	
	  If the event mask does not contain any poll(2) event, we consider the
	  descriptor to be disabled. This condition is likely the effect of the
	  EPOLLONESHOT bit that disables the descriptor when an event is received,
	  until the next EPOLL_CTL_MOD will be issued.
	
	  Check the events coming with the callback. At this stage, not
	  every device reports the events in the "key" parameter of the
	  callback. We need to be able to handle both cases here, hence the
	  test for "key" != NULL before the event match test.
	
	  If we are transferring events to userspace, we can hold no locks
	  (because we're accessing user memory, and because of linux f_op->poll()
	  semantics). All the events that happen during that period of time are
	  chained in ep->ovflist and requeued later on.
 In the usual case, add event to ready list. 
	
	  Wake up ( if active ) both the eventpoll wait list and the ->poll()
	  wait list.
 We have to call this outside the lock 
		
		  If we race with ep_remove_wait_queue() it can miss
		  ->whead = NULL and do another remove_wait_queue() after
		  us, so we can't use __remove_wait_queue().
		
		  ->whead != NULL protects us from the race with ep_free()
		  or ep_remove(), ep_remove_wait_queue() takes whead->lock
		  held by the caller. Once we nullify it, nothing protects
		  epepi or even wait.
  This is the callback that is used to add our wait queue to the
  target file wakeup lists.
 an earlier allocation has failed
  These are the number paths of length 1 to 5, that we are allowing to emanate
  from a single file of interest. For example, we allow 1000 paths of length
  1, to emanate from each file of interest. This essentially represents the
  potential wakeup paths, which need to be limited in order to avoid massive
  uncontrolled wakeup storms. The common use case should be a single ep which
  is connected to n file sources. In this case each file source has 1 path
  of length 1. Thus, the numbers below should be more than sufficient. These
  path limits are enforced during an EPOLL_CTL_ADD operation, since a modify
  and delete can't add additional paths. Protected by the epmutex.
 Allow an arbitrary number of depth 1 paths 
 too deep nesting 
 CTL_DEL can remove links here, but that can't increase our count 
  reverse_path_check - The tfile_check_list is list of epitem_head, which have
                       links that are proposed to be newly added. We need to
                       make sure that those added links don't add too many
                       paths such that we will spend all our time waking up
                       eventpoll objects.
  Return: %zero if the proposed links don't create too many paths,
 	    %-1 otherwise.
 rare code path, only used when EPOLL_CTL_MOD removes a wakeup source 
	
	  wait for ep_pm_stay_awake_rcu to finish, synchronize_rcu is
	  used internally by wakeup_source_remove, too (called by
	  wakeup_source_unregister), so we cannot use call_rcu
  Must be called with "mtx" held.
 Item initialization follow here ... 
 Add the current item to the list of active epoll hook for this file 
	
	  Add the current item to the RB tree. All RB tree operations are
	  protected by "mtx", and ep_insert() is called with "mtx" held.
 now check if we've created too many backpaths 
 Initialize the poll table using the queue callback 
	
	  Attach the item to the poll hooks and get current event bits.
	  We can safely use the file here because its usage count has
	  been increased by the caller of this function. Note that after
	  this operation completes, the poll callback can start hitting
	  the new item.
	
	  We have to check if something went wrong during the poll wait queue
	  install process. Namely an allocation for a wait queue failed due
	  high memory pressure.
 We have to drop the new item inside our item list to keep track of it 
 record NAPI ID of new item if present 
 If the file is already "ready" we drop it inside the ready list 
 Notify waiting tasks that events are available 
 We have to call this outside the lock 
  Modify the interest event mask by dropping an event if the new mask
  has a match in the current file status. Must be called with "mtx" held.
	
	  Set the new event interest mask before calling f_op->poll();
	  otherwise we might miss an event that happens between the
	  f_op->poll() call and the new event set registering.
 need barrier below 
 protected by mtx 
	
	  The following barrier has two effects:
	 
	  1) Flush epi changes above to other CPUs.  This ensures
	     we do not miss events from ep_poll_callback if an
	     event occurs immediately after we call f_op->poll().
	     We need this because we did not take ep->lock while
	     changing epi above (but ep_poll_callback does take
	     ep->lock).
	 
	  2) We also need to ensure we do not miss _past_ events
	     when calling f_op->poll().  This barrier also
	     pairs with the barrier in wq_has_sleeper (see
	     comments for wq_has_sleeper).
	 
	  This barrier will now guarantee ep_poll_callback or f_op->poll
	  (or both) will notice the readiness of an item.
	
	  Get current event bits. We can safely use the file here because
	  its usage count has been increased by the caller of this function.
	  If the item is "hot" and it is not registered inside the ready
	  list, push it inside.
 Notify waiting tasks that events are available 
 We have to call this outside the lock 
	
	  Always short-circuit for fatal signals to allow threads to make a
	  timely exit without the chance of finding more events available and
	  fetching repeatedly.
	
	  We can loop without lock because we are passed a task private list.
	  Items cannot vanish during the loop we are holding ep->mtx.
		
		  Activate ep->ws before deactivating epi->ws to prevent
		  triggering auto-suspend here (in case we reactive epi->ws
		  below).
		 
		  This could be rearranged to delay the deactivation of epi->ws
		  instead, but then epi->ws would temporarily be out of sync
		  with ep_is_linked().
		
		  If the event mask intersect the caller-requested one,
		  deliver the event to userspace. Again, we are holding ep->mtx,
		  so no operations coming from userspace can change the item.
			
			  If this file has been added with Level
			  Trigger mode, we need to insert back inside
			  the ready list, so that the next call to
			  epoll_wait() will check again the events
			  availability. At this point, no one can insert
			  into ep->rdllist besides us. The epoll_ctl()
			  callers are locked out by
			  ep_scan_ready_list() holding "mtx" and the
			  poll callback will queue them in ep->ovflist.
  ep_poll - Retrieves ready events, and delivers them to the caller-supplied
            event buffer.
  @ep: Pointer to the eventpoll context.
  @events: Pointer to the userspace buffer where the ready events should be
           stored.
  @maxevents: Size (in terms of number of events) of the caller event buffer.
  @timeout: Maximum timeout for the ready events fetch operation, in
            timespec. If the timeout is zero, the function will not block,
            while if the @timeout ptr is NULL, the function will block
            until at least one event has been retrieved (or an error
            occurred).
  Return: the number of ready events which have been fetched, or an
           error code, in case of error.
		
		  Avoid the unnecessary trip to the wait queue loop, if the
		  caller specified a non blocking operation.
	
	  This call is racy: We may or may not see events that are being added
	  to the ready list under the lock (e.g., in IRQ callbacks). For cases
	  with a non-zero timeout, this thread will check the ready list under
	  lock and will add to the wait queue.  For cases with a zero
	  timeout, the user by definition should not care and will have to
	  recheck again.
			
			  Try to transfer events to user space. In case we get
			  0 events and there's still timeout left over, we go
			  trying again in search of more luck.
		
		  Internally init_wait() uses autoremove_wake_function(),
		  thus wait entry is removed from the wait queue on each
		  wakeup. Why it is important? In case of several waiters
		  each new wakeup will hit the next waiter, giving it the
		  chance to harvest new event. Otherwise wakeup can be
		  lost. This is also good performance-wise, because on
		  normal wakeup path no need to call __remove_wait_queue()
		  explicitly, thus ep->lock is not taken, which halts the
		  event delivery.
		
		  Barrierless variant, waitqueue_active() is called under
		  the same lock on wakeup ep_poll_callback() side, so it
		  is safe to avoid an explicit barrier.
		
		  Do the final check under the lock. ep_scan_ready_list()
		  plays with two lists (->rdllist and ->ovflist) and there
		  is always a race when both lists are empty for short
		  period of time although events are pending, so lock is
		  important.
		
		  We were woken up, thus go and try to harvest some events.
		  If timed out and still on the wait queue, recheck eavail
		  carefully under lock, below.
			
			  If the thread timed out and is not on the wait queue,
			  it means that the thread was woken up after its
			  timeout expired before it could reacquire the lock.
			  Thus, when wait.entry is empty, it needs to harvest
			  events.
  ep_loop_check_proc - verify that adding an epoll file inside another
                       epoll structure does not violate the constraints, in
                       terms of closed loops, or too deep chains (which can
                       result in excessive stack usage).
  @ep: the &struct eventpoll to be currently checked.
  @depth: Current depth of the path being checked.
  Return: %zero if adding the epoll @file inside current epoll
           structure @ep does not violate the constraints, or %-1 otherwise.
			
			  If we've reached a file that is not associated with
			  an ep, then we need to check if the newly added
			  links are going to add too many wakeup paths. We do
			  this by adding it to the tfile_check_list, if it's
			  not already there, and calling reverse_path_check()
			  during ep_insert().
  ep_loop_check - Performs a check to verify that adding an epoll file (@to)
                  into another epoll file (represented by @ep) does not create
                  closed loops or too deep chains.
  @ep: Pointer to the epoll we are inserting into.
  @to: Pointer to the epoll to be inserted.
  Return: %zero if adding the epoll @to inside the epoll @from
  does not violate the constraints, or %-1 otherwise.
  Open an eventpoll file descriptor.
 Check the EPOLL_ constant for consistency.  
	
	  Create the internal data structure ("struct eventpoll").
	
	  Creates all the items needed to setup an eventpoll file. That is,
	  a file structure and a free file descriptor.
 Get the "struct file " for the target file 
 The target file descriptor must support poll 
 Check if EPOLLWAKEUP is allowed 
	
	  We have to check that the file structure underneath the file descriptor
	  the user passed to us _is_ an eventpoll file. And also we do not permit
	  adding an epoll file descriptor inside itself.
	
	  epoll adds to the wakeup queue at EPOLL_CTL_ADD time only,
	  so EPOLLEXCLUSIVE is not allowed for a EPOLL_CTL_MOD operation.
	  Also, we do not currently supported nested exclusive wakeups.
	
	  At this point it is safe to assume that the "private_data" contains
	  our own data structure.
	
	  When we insert an epoll file descriptor inside another epoll file
	  descriptor, there is the chance of creating closed loops, which are
	  better be handled here, than in more critical paths. While we are
	  checking for loops we also determine the list of files reachable
	  and hang them on the tfile_check_list, so we can check that we
	  haven't created too many possible wakeup paths.
	 
	  We do not need to take the global 'epumutex' on EPOLL_CTL_ADD when
	  the epoll file descriptor is attaching directly to a wakeup source,
	  unless the epoll file descriptor is nested. The purpose of taking the
	  'epmutex' on add is to prevent complex toplogies such as loops and
	  deep wakeup paths from forming in parallel through multiple
	  EPOLL_CTL_ADD operations.
	
	  Try to lookup the file inside our RB tree. Since we grabbed "mtx"
	  above, we can be sure to be able to use the item looked up by
	  ep_find() till we release the mutex.
  The following function implements the controller interface for
  the eventpoll file that enables the insertionremovalchange of
  file descriptors inside the interest set.
  Implement the event wait interface for the eventpoll file. It is the kernel
  part of the user space epoll_wait(2).
 The maximum number of event must be greater than zero 
 Verify that the area passed by the user is writeable 
 Get the "struct file " for the eventpoll file 
	
	  We have to check that the file structure underneath the fd
	  the user passed to us _is_ an eventpoll file.
	
	  At this point it is safe to assume that the "private_data" contains
	  our own data structure.
 Time to fish for events ... 
  Implement the event wait interface for the eventpoll file. It is the kernel
  part of the user space epoll_pwait(2).
	
	  If the caller wants a certain signal mask to be set during the wait,
	  we apply it here.
	
	  If the caller wants a certain signal mask to be set during the wait,
	  we apply it here.
	
	  Allows top 4% of lomem to be allocated for epoll watches (per user).
	
	  We can have many thousands of epitems, so prevent this from
	  using an extra cache line on 64-bit (and smaller) CPUs
 Allocates slab cache used to allocate "struct epitem" items 
 Allocates slab cache used to allocate "struct eppoll_entry" 
 SPDX-License-Identifier: GPL-2.0
  This file contains the procedures for the handling of select and poll
  Created for Linux based loosely upon Mathius Lattner's minix
  patches by Peter MacDonald. Heavily edited by Linus.
   4 February 1994
      COFFELF binary emulation. If the process has the STICKY_TIMEOUTS
      flag set in its personality we do not modify the given timeout
      parameter to reflect time remaining.
   24 January 2000
      Changed sys_poll()do_poll() to use PAGE_SIZE chunk-based allocation 
      of fds to overcome nfds < 16390 descriptors limit (Tigran Aivazian).
 for STICKY_TIMEOUTS 
  Estimate expected accuracy in ns from a timeval.
  After quite a bit of churning around, we've settled on
  a simple thing of taking 0.1% of the timeout as the
  slack, with a cap of 100 msec.
  "nice" tasks get a 0.5% slack instead.
  Consider this comment an open invitation to come up with even
  better solutions..
	
	  Realtime tasks get a slack of 0 for obvious reasons.
  Ok, Peter made a complicated, but straightforward multiple_wait() function.
  I have rewritten this, taking some shortcuts: This code may not be easy to
  follow, but it should be free of race-conditions, and it's practical. If you
  understand what I'm doing here, then you understand how the linux
  sleepwakeup mechanism works.
  Two very simple procedures, poll_wait() and poll_freewait() make all the
  work.  poll_wait() is an inline-function defined in <linuxpoll.h>,
  as all selectpoll functions have to call it to add an entry to the
  poll table.
	
	  Although this function is called under waitqueue lock, LOCK
	  doesn't imply write barrier and the users expect write
	  barrier semantics on wakeup functions.  The following
	  smp_wmb() is equivalent to smp_wmb() in try_to_wake_up()
	  and is paired with smp_store_mb() in poll_schedule_timeout.
	
	  Perform the default wake up operation using a dummy
	  waitqueue.
	 
	  TODO: This is hacky but there currently is no interface to
	  pass in @sync.  @sync is scheduled to be removed and once
	  that happens, wake_up_process() can be used directly.
 Add a new entry 
	
	  Prepare for the next iteration.
	 
	  The following smp_store_mb() serves two purposes.  First, it's
	  the counterpart rmb of the wmb in pollwake() such that data
	  written before wake up is always visible after wake up.
	  Second, the full barrier guarantees that triggered clearing
	  doesn't pass event check of the next iteration.  Note that
	  this problem doesn't exist for the first iteration as
	  add_wait_queue() has full barrier semantics.
  poll_select_set_timeout - helper function to setup the timeout value
  @to:		pointer to timespec64 variable for the final timeout
  @sec:	seconds (from user space)
  @nsec:	nanoseconds (from user space)
  Note, we do not use a timespec for the user space value here, That
  way we can use the function for timeval and compat interfaces as well.
  Returns -EINVAL if secnsec are not normalized. Otherwise 0.
 Optimize for the zero timeout value here 
 No update for zero timeout 
	
	  If an application puts its timeval in read-only memory, we
	  don't want the Linux-specific update to the timeval to
	  cause a fault after the select has completed
	  successfully. However, because we're not updating the
	  timeval, we can't restart the system call.
  Scalable version of the fd_set.
  How many longwords for "nr" bits?
  Use "unsigned long" accesses to let user-mode fd_set's be long-aligned.
 handle last in-complete long-word first 
 got something, stop busy polling 
					
					  only remember a returned
					  POLL_BUSY_LOOP if we asked for it
 only if found POLL_BUSY_LOOP sockets && not out of time 
		
		  If this is the first loop and we have a timeout
		  given, then we convert to ktime_t and set the to
		  pointer to the expiry value.
  We can actually return ERESTARTSYS instead of EINTR, but I'd
  like to be certain this leads to no problems. So I return
  EINTR just for safety.
  Update: ERESTARTSYS breaks at least the xview clock binary, so
  I'm trying ERESTARTNOHAND which restart only when you want to.
 Allocate small arguments on the stack to save memory and be faster 
 max_fds can increase, so grab it once to avoid race 
	
	  We need 6 bitmaps (inoutex for both incoming and outgoing),
	  since we used fdset we need to allocate memory in units of
	  long-words. 
 Not enough space in on-stack array; must use kmalloc 
  Most architectures can't handle 7-argument syscalls. So we provide a
  6-argument version where the sixth argument is a pointer to a structure
  which has a pointer to the sigset_t itself followed by a size_t containing
  the sigset size.
 the path is hot enough for overhead of copy_from_user() to matter
  Fish for pollable events on the pollfd->fd file descriptor. We're only
  interested in events matching the pollfd->events mask, and the result
  matching that mask is both recorded in pollfd->revents and returned. The
  pwait poll_table will be used by the fd-provided poll handler for waiting,
  if pwait->_qproc is non-NULL.
 userland u16 ->events contains POLL... bitmap 
 Mask out unneeded events. 
 ... and so does ->revents 
 Optimise the no-wait case 
				
				  Fish for events. If we found one, record it
				  and kill poll_table->_qproc, so we don't
				  needlessly register any other waiters after
				  this. They'll get immediately deregistered
				  when we break out and return.
 found something, stop busy polling 
		
		  All waiters have already been registered, so don't provide
		  a poll_table->_qproc to them on the next loop iteration.
 only if found POLL_BUSY_LOOP sockets && not out of time 
		
		  If this is the first loop and we have a timeout
		  given, then we convert to ktime_t and set the to
		  pointer to the expiry value.
	 Allocate small arguments on the stack to save memory and be
	   faster - use long to make sure the buffer is aligned properly
  Ooo, nasty.  We need here to frob 32-bit unsigned longs to
  64-bit unsigned longs.
  This is a virtual copy of sys_select from fsselect.c and probably
  should be compared to it from time to time
  We can actually return ERESTARTSYS instead of EINTR, but I'd
  like to be certain this leads to no problems. So I return
  EINTR just for safety.
  Update: ERESTARTSYS breaks at least the xview clock binary, so
  I'm trying ERESTARTNOHAND which restart only when you want to.
 max_fds can increase, so grab it once to avoid race 
	
	  We need 6 bitmaps (inoutex for both incoming and outgoing),
	  since we used fdset we need to allocate memory in units of
	  long-words.
 New compat syscall for 64 bit time_t
 SPDX-License-Identifier: GPL-2.0
   linuxfsbinfmt_flat.c
 	Copyright (C) 2000-2003 David McCullough <davidm@snapgear.com>
 	Copyright (C) 2002 Greg Ungerer <gerg@snapgear.com>
 	Copyright (C) 2002 SnapGear, by Paul Dale <pauli@snapgear.com>
 	Copyright (C) 2000, 2001 Lineo, by David McCullough <davidm@lineo.com>
   based heavily on:
   linuxfsbinfmt_aout.c:
       Copyright (C) 1991, 1992, 1996  Linus Torvalds
   linuxfsbinfmt_flat.c for 2.0 kernel
 	    Copyright (C) 1998  Kenneth Albanowski <kjahds@kjahds.com>
 	JAN99 -- coded full program relocation (gerg@snapgear.com)
  User data (data section and bss) needs to be aligned.
  We pick 0x20 here because it is the max value elf2flt has always
  used in producing FLAT files, and because it seems to be large
  enough to make all the gcc alignment related tests happy.
  User data (stack) also needs to be aligned.
  Here we can be a bit looser than the data sections since this
  needs to only meet arch ABI requirements.
 Relocation incorrect somewhere 
 Placeholder for unused library 
 Start of text segment 
 Start of data segment 
 End of data segment 
 Length of text segment 
 Start address for this module 
 When this one was compiled 
 Has this library been loaded? 
  Routine writes a core dump image in the current directory.
  Currently only a stub-function.
  create_flat_tables() parses the env- and arg-strings in new user
  memory and creates the pointer tables from them, and puts their
  addresses on the "stack", recording the new stack pointer value.
 argvp + envp 
 &argc 
 gzip flag byte 
 bit 0 set: file probably ASCII text 
 bit 1 set: continuation of multi-part gzip file 
 bit 2 set: extra field present 
 bit 3 set: original file name present 
 bit 4 set: file comment present 
 bit 5 set: file is encrypted 
 bit 6,7:   reserved 
 Read in first chunk of data and parse gzip header. 
 Check minimum size -- gzip header 
 Check gzip magic number 
 Check gzip method 
 Check gzip flags 
 CONFIG_BINFMT_ZFLAT 
 Relocs of 0 are always self referring 
 Find ID for this reloc 
 Trim ID off here 
 Check versioning information (i.e. time stamps) 
 In text segment 
 In data segment 
 Range checked already above so doing the range tests is redundant...
 CONFIG_BINFMT_FLAT_OLD 
 exec-header 
		
		  Previously, here was a printk to tell people
		    "BINFMT_FLAT: bad header magic".
		  But for the kernel which also use ELF FD-PIC format, this
		  error message is confusing.
		  because a lot of people do not manage to produce good
 Don't allow old format executables to use shared libraries 
	
	  fix up the flags for the older format,  there were all kinds
	  of endian hacks,  this only works for the simple cases
 CONFIG_BINFMT_FLAT_OLD 
 !CONFIG_BINFMT_FLAT_OLD 
	
	  Make sure the header params are sane.
	  28 bits (256 MB) is way more than reasonable in this case.
	  If some top bits are set we have probable binary corruption.
	
	  Check initial limits. This avoids letting people circumvent
	  size limits imposed on them by creating programs with large
	  arrays in the data or bss.
 Flush all traces of the currently running executable 
 OK, This is the point of no return 
	
	  calculate the extra space we need to map in
	
	  there are a couple of cases here,  the separate codedata
	  case,  and then the fully copied to RAM case which lumps
	  it all together.
		
		  this should give us a ROM ptr,  but if it doesn't we don't
		  really care
		
		  load it all in and treat it like a RAM load from now on
			
			  This is used on MMU systems mainly for testing.
			  Let's use a kernel buffer to simplify things.
 CONFIG_BINFMT_ZFLAT 
 the real code len 
 The main program needs a little extra setup in the task structure 
		
		  set up the brk stuff, uses any slack left in databssstack
		  allocation.  We put the brk after the bss (between the bss
		  and stack) like other platforms.
		  Userspace code relies on the stack pointer starting out at
		  an address right at the end of a page.
 Store the current module values into the global library structure 
	
	  We just load the allocations into some temporary memory to
	  help simplify all this mumbo jumbo
	 
	  We've got two different sections of relocation entries.
	  The first is the GOT which resides at the beginning of the data segment
	  and is terminated with a -1.  This one can be relocated in place.
	  The second is the extra relocation entries tacked after the image's
	  data segment. These require a little more processing as the entry is
	  really an offset into the image which contains an offset into the
	  image.
	
	  Now run through the relocation entries.
	  We've got to be careful here as C++ produces relocatable zero
	  entries in the constructor and destructor tables which are then
	  tested for being not zero (which will always occur unless we're
	  based from address zero).  This causes an endless loop as __start
	  is at zero.  The solution used is to not relocate zero addresses.
	  This has the negative side effect of not allowing a global data
	  reference to be statically initialised to _stext (I've moved
	  __start to address 4 so that is okay).
			
			  Get the address of the pointer to be
			  relocated (of course, the address has to be
			  relocated first).
 Get the pointer's value.  
				
				  Do the relocation.  PIC relocs in the data section are
				  already in target order
					
					  Meh, the same value can have a different
					  byte order based on a flag..
 Write back the relocated pointer.  
 CONFIG_BINFMT_FLAT_OLD 
 zero the BSS,  BRK and stack areas 
 end brk 
 start brk 
  Load a shared library into memory.  The library gets its own data
  segment (including bss) but not argvargcenviron.
	
	  This is a fake bprm struct; only the members "buf", "file" and
	  "filename" are actually used.
 Create the file name 
 Open the file up 
 CONFIG_BINFMT_SHARED_FLAT 
  These are the functions used to load flat style executables and shared
  libraries.  There is no binary dependent code anywhere else.
	
	  We have to add the size of our arguments to our stack size
	  otherwise it's too easy for users to create stack overflows
	  by passing in a huge argument list.  And yes,  we have to be
	  pedantic and include space for the argvenvp array as it may have
	  a lot of entries.
 the strings 
 the argv array 
 the envp array 
 Update data segment pointers for all libraries 
 Stash our initial stack pointer into the mm structure 
 copy the arg pages onto the stack 
	 Fake some return addresses to ensure the call chain will
	  initialise library in order for us.  We are required to call
	  lib 1 first, then 2, ... and finally the main program (id 0).
 Push previos first to call address 
 SPDX-License-Identifier: GPL-2.0
  fsproc_namespace.c - handling of proc<pid>{mounts,mountinfo,mountstats}
  In fact, that's a piece of procfs; it's almost isolated from
  the rest of fsproc, but has rather close relationships with
  fsnamespace.c, thus here instead of fsproc
 only for get_proc_task() in ->open() 
 mountpoints outside of chroot jail will give SEQ_SKIP on this 
 mountpoints outside of chroot jail will give SEQ_SKIP on this 
 Tagged fields ("foo:X" or "bar") 
 Filesystem specific data 
 device 
 mount point 
 mountpoints outside of chroot jail will give SEQ_SKIP on this 
 file system type 
 optional statistics 
 SPDX-License-Identifier: GPL-2.0
   fssignalfd.c
   Copyright (C) 2003  Linus Torvalds
   Mon Mar 5, 2007: Davide Libenzi <davidel@xmailserver.org>
       Changed ->read() to return a siginfo strcture instead of signal number.
       Fixed locking in ->poll().
       Added sighand-detach notification.
       Added fd re-use in sys_signalfd() syscall.
       Now using anonymous inode source.
       Thanks to Oleg Nesterov for useful code review and suggestions.
       More comments and suggestions from Arnd Bergmann.
   Sat May 19, 2007: Davi E. M. Arnaut <davi@haxent.com.br>
       Retrieve multiple signals with one read() call
   Sun Jul 15, 2007: Davide Libenzi <davidel@xmailserver.org>
       Attach to the sighand only during read() and poll().
	
	  The lockless check can race with remove_wait_queue() in progress,
	  but in this case its caller should run under rcu_read_lock() and
	  sighand_cachep is SLAB_TYPESAFE_BY_RCU, we can safely return.
 wait_queue_entry_t->func(POLLFREE) should do remove_wait_queue() 
  Copied from copy_siginfo_to_user() in kernelsignal.c
	
	  Unused members should be zero ...
	
	  If you change siginfo_t structure, please be sure
	  this code is fixed accordingly.
		
		  Fall through to the SIL_FAULT case.  SIL_FAULT_BNDERR,
		  SIL_FAULT_PKUERR, and SIL_FAULT_PERF_EVENT are only
		  generated by faults that deliver them synchronously to
		  userspace.  In case someone injects one of these signals
		  and signalfd catches it treat it as SIL_FAULT.
		
		  This case catches also the signals queued by sigqueue().
  Returns a multiple of the size of a "struct signalfd_siginfo", or a negative
  error code. The "count" parameter must be at least the size of a
  "struct signalfd_siginfo".
 Check the SFD_ constants for consistency.  
		
		  When we call this, the initialization must be complete, since
		  anon_inode_getfd() will install the fd.
 SPDX-License-Identifier: GPL-2.0-or-later
 Provide a way to create a superblock configuration context within the kernel
  that allows a superblock to be set up prior to mounting.
  Copyright (C) 2017 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
 Data page for legacy filesystems 
  Check for a common mount option that manipulates s_flags.
  vfs_parse_fs_param_source - Handle setting "source" via parameter
  @fc: The filesystem context to modify
  @param: The parameter
  This is a simple helper for filesystems to verify that the "source" they
  accept is sane.
  Returns 0 on success, -ENOPARAM if this is not  "source" parameter, and
  -EINVAL otherwise. In the event of failure, supplementary error information
   is logged.
  vfs_parse_fs_param - Add a single parameter to a superblock config
  @fc: The filesystem context to modify
  @param: The parameter
  A single mount option in string form is applied to the filesystem context
  being set up.  Certain standard options (for example "ro") are translated
  into flag bits without going to the filesystem.  The active security module
  is allowed to observe and poach options.  Any other options are passed over
  to the filesystem to parse.
  This may be called multiple times for a context.
  Returns 0 on success and a negative error code on failure.  In the event of
  failure, supplementary error information may have been set.
		 Param belongs to the LSM or is disallowed by the LSM; so
		  don't pass to the FS.
	 If the filesystem doesn't take any arguments, give it the
	  default handling of source.
  vfs_parse_fs_string - Convenience function to just parse a string.
  generic_parse_monolithic - Parse key[=val][,key[=val]] mount data
  @ctx: The superblock configuration to fill in.
  @data: The data to parse
  Parse a blob of data that's in key[=val][,key[=val]] form.  This can be
  called from the ->monolithic_mount_data() fs_context operation.
  Returns 0 on success or the error returned by the ->parse_option() fs_context
  operation on failure.
  alloc_fs_context - Create a filesystem context.
  @fs_type: The filesystem type.
  @reference: The dentry from which this one derives (or NULL)
  @sb_flags: Filesystemsuperblock flags (SB_)
  @sb_flags_mask: Applicable members of @sb_flags
  @purpose: The purpose that this configuration shall be used for.
  Open a filesystem and create a mount context.  The mount context is
  initialised with the supplied flags and, if a submountautomount from
  another superblock (referred to by @reference) is supplied, may have
  parameters such as namespaces copied across from that superblock.
 TODO: Make all filesystems support this unconditionally 
  vfs_dup_fc_config: Duplicate a filesystem context.
  @src_fc: The context to copy.
 Can't call put until we've called ->dup 
  logfc - Log a message to a filesystem context
  @fc: The filesystem context to log to.
  @fmt: The format of the buffer.
 The buffer is full, discard the oldest message 
  Free a logging structure.
  put_fs_context - Dispose of a superblock configuration context.
  @fc: The context to dispose of.
  Free the config for a filesystem that doesn't support fs_context.
  Duplicate a legacy config.
  Add a parameter to a legacy config.  We build up a comma-separated list of
  options.
  Add monolithic mount data.
  Get a mountable root with the legacy mount command.
  Handle remount.
  Initialise a legacy context for a filesystem that doesn't support
  fs_context.
  Clean up a context after performing an action on it and put it into a state
  from where it can be used to reconfigure a superblock.
  Note that here we do only the parts that can't fail; the rest is in
  finish_clean_context() below and in between those fs_context is marked
  FS_CONTEXT_AWAITING_RECONF.  The reason for splitup is that after
  successful mount or remount we need to report success to userland.
  Trying to do full reinit (for the sake of possible subsequent remount)
  and failing to allocate memory would've put us into a nasty situation.
  So here we only discard the old state and reinitialization is left
  until we actually try to reconfigure.
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2002,2003 by Andreas Gruenbacher <a.gruenbacher@computer.org>
  Fixes from William Schumacher incorporated on 15 March 2001.
     (Reported by Charles Bertsch, <CBertsch@microtest.com>).
   This file contains generic functions for manipulating
   POSIX 1003.1e draft standard 17 ACLs.
	
	  The sentinel is used to detect when another operation like
	  set_cached_acl() or forget_cached_acl() races with get_acl().
	  It is guaranteed that is_uncached_acl(sentinel) is true.
	
	  If the ACL isn't being read yet, set our sentinel.  Otherwise, the
	  current value of the ACL will not be ACL_NOT_CACHED and so our own
	  sentinel will not be set; another task will update the cache.  We
	  could wait for that other task to complete its job, but it's easier
	  to just call ->get_acl to fetch the ACL ourself.  (This is going to
	  be an unlikely race.)
	
	  Normally, the ACL returned by ->get_acl will be cached.
	  A filesystem can prevent that by calling
	  forget_cached_acl(inode, type) in ->get_acl.
	 
	  If the filesystem doesn't have a get_acl() function at all, we'll
	  just create the negative cache entry.
		
		  Remove our sentinel so that we don't block future attempts
		  to cache the ACL.
	
	  Cache the result, but only if our sentinel is still in place.
  Init a fresh posix_acl
  Allocate a new ACL with the specified number of entries.
  Clone an ACL.
  Check if an acl is valid. Returns 0 if it is, or -E... otherwise.
  Returns 0 if the acl can be exactly represented in the traditional
  file mode permission bits, or else 1. Returns -E... on error.
	
	  A null ACL can always be presented as mode bits.
  Create an ACL representing the file mode permission bits of an inode.
  Return 0 if current is granted want access to the inode
  by the acl. Returns -E... otherwise.
 (May have been checked already) 
  Modify acl when creating a new inode. The caller must ensure the acl is
  only referenced once.
  mode_p initially must contain the mode parameter to the open()  creat()
  system calls. All permissions that are not granted by the acl are removed.
  The permissions in the acl are changed to reflect the mode_p parameter.
 assert(atomic_read(acl->a_refcount) == 1); 
  Modify the ACL for the chmod syscall.
 assert(atomic_read(acl->a_refcount) == 1); 
  posix_acl_chmod - chmod a posix acl
  @mnt_userns:	user namespace of the mount @inode was found from
  @inode:	inode to check permissions on
  @mode:	the new mode of @inode
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then
  take care to map the inode according to @mnt_userns before checking
  permissions. On non-idmapped mounts or if permission checking is to be
  performed on the raw inode simply passs init_user_ns.
  posix_acl_update_mode  -  update mode in set_acl
  @mnt_userns:	user namespace of the mount @inode was found from
  @inode:	target inode
  @mode_p:	mode (pointer) for update
  @acl:	acl pointer
  Update the file mode when setting an ACL: compute the new file permission
  bits based on the ACL.  In addition, if the ACL is equivalent to the new
  file mode, set @acl to NULL to indicate that no ACL should be set.
  As with chmod, clear the setgid bit if the caller is not in the owning group
  or capable of CAP_FSETID (see inode_change_ok).
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then
  take care to map the inode according to @mnt_userns before checking
  permissions. On non-idmapped mounts or if permission checking is to be
  performed on the raw inode simply passs init_user_ns.
  Called from set_acl inode operations.
  Fix up the uids and gids in posix acl extended attributes in place.
  Convert from extended attribute to in-memory representation.
  Convert from in-memory to extended attribute representation.
 SPDX-License-Identifier: GPL-2.0-only
  File: fsxattr.c
  Extended attribute handling.
  Copyright (C) 2001 by Andreas Gruenbacher <a.gruenbacher@computer.org>
  Copyright (C) 2001 SGI - Silicon Graphics, Inc <linux-xfs@oss.sgi.com>
  Copyright (c) 2004 Red Hat, Inc., James Morris <jmorris@redhat.com>
  In order to implement different sets of xattr operations for each xattr
  prefix, a filesystem should create a null-terminated array of struct
  xattr_handler (one for each prefix) and hang a pointer to it off of the
  s_xattr field of the superblock.
  Find the xattr_handler with the matching prefix.
  Check permissions for extended attribute access.  This is a bit complicated
  because different namespaces have very different rules.
	
	  We can never set or remove an extended attribute on a read-only
	  filesystem  or on an immutable  append-only inode.
		
		  Updating an xattr will likely cause i_uid and i_gid
		  to be writen back improperly if their true value is
		  unknown to the vfs.
	
	  No restriction for security. and system. from the VFS.  Decision
	  on these is left to the underlying filesystem  security module.
	
	  The trusted. namespace can only be accessed by privileged users.
	
	  In the user. namespace, only regular files and directories can have
	  extended attributes. For sticky directories, only the owner and
	  privileged users can write attributes.
  Look for any handler that deals with the specified namespace.
 empty EA, do not remove 
   __vfs_setxattr_noperm - perform setxattr operation without performing
   permission checks.
   @mnt_userns: user namespace of the mount the inode was found from
   @dentry: object to perform setxattr on
   @name: xattr name to set
   @value: value to set @name to
   @size: size of @value
   @flags: flags to pass into filesystem operations
   returns the result of the internal setxattr or setsecurity operations.
   This function requires the caller to lock the inode's i_mutex before it
   is executed. It also assumes that the caller will make the appropriate
   permission checks.
  __vfs_setxattr_locked - set an extended attribute while holding the inode
  lock
   @mnt_userns: user namespace of the mount of the target inode
   @dentry: object to perform setxattr on
   @name: xattr name to set
   @value: value to set @name to
   @size: size of @value
   @flags: flags to pass into filesystem operations
   @delegated_inode: on return, will contain an inode pointer that
   a delegation was broken on, NULL if none.
  vfs_getxattr_alloc - allocate memory, if necessary, before calling getxattr
  Allocate memory, if not already allocated, or re-allocate correct size,
  before retrieving the extended attribute.
  Returns the result of alloc, if failed, or the getxattr operation.
		
		  Only overwrite the return value if a security module
		  is actually active.
  __vfs_removexattr_locked - set an extended attribute while holding the inode
  lock
   @mnt_userns: user namespace of the mount of the target inode
   @dentry: object to perform setxattr on
   @name: name of xattr to remove
   @delegated_inode: on return, will contain an inode pointer that
   a delegation was broken on, NULL if none.
  Extended attribute SET operations
  Extended attribute GET operations
		 The file system tried to returned a value bigger
  Extended attribute LIST operations
		 The file system tried to returned a list bigger
  Extended attribute REMOVE operations
  Combine the results of the list() operation from every xattr_handler in the
  list.
  xattr_full_name  -  Compute full attribute name from suffix
  @handler:	handler of the xattr_handler operation
  @name:	name passed to the xattr_handler operation
  The get and set xattr handler operations are called with the remainder of
  the attribute name after skipping the handler's prefix: for example, "foo"
  is passed to the get operation of a handler with prefix "user." to get
  attribute "user.foo".  The full name is still "there" in the name though.
  Note: the list xattr handler operation when called from the vfs is passed a
  NULL name; some file systems use this operation internally, with varying
  semantics.
  Allocate new xattr and copy in the value; but leave the name to callers.
 wrap around? 
  xattr GET operation for in-memorypseudo filesystems
  simple_xattr_set - xattr SET operation for in-memorypseudo filesystems
  @xattrs: target simple_xattr list
  @name: name of the extended attribute
  @value: value of the xattr. If %NULL, will remove the attribute.
  @size: size of the new xattr
  @flags: %XATTR_{CREATE|REPLACE}
  @removed_size: returns size of the removed xattr, -1 if none removed
  %XATTR_CREATE is set, the xattr shouldn't exist already; otherwise fails
  with -EEXIST.  If %XATTR_REPLACE is set, the xattr should exist;
  otherwise, fails with -ENODATA.
  Returns 0 on success, -errno on failure.
 value == NULL means remove 
  xattr LIST operation for in-memorypseudo filesystems
 skip "trusted." attributes for unprivileged callers 
  Adds an extended attribute to the list
 SPDX-License-Identifier: GPL-2.0-only
   fsanon_inodes.c
   Copyright (C) 2007  Davide Libenzi <davidel@xmailserver.org>
   Thanks to Arnd Bergmann for code review and suggestions.
   More changes for Thomas Gleixner suggestions.
  anon_inodefs_dname() is called from d_path().
		
		  We know the anon_inode inode count is always
		  greater than zero, so ihold() is safe.
  anon_inode_getfile - creates a new file instance by hooking it up to an
                       anonymous inode, and a dentry that describe the "class"
                       of the file
  @name:    [in]    name of the "class" of the new file
  @fops:    [in]    file operations for the new file
  @priv:    [in]    private data for the new file (will be file's private_data)
  @flags:   [in]    flags
  Creates a new file by hooking it on a single inode. This is useful for files
  that do not need to have a full-fledged inode in order to operate correctly.
  All the files created with anon_inode_getfile() will share a single inode,
  hence saving memory and avoiding code duplication for the fileinodedentry
  setup.  Returns the newly created file or an error pointer.
  anon_inode_getfile_secure - Like anon_inode_getfile(), but creates a new
                              !S_PRIVATE anon inode rather than reuse the
                              singleton anon inode and calls the
                              inode_init_security_anon() LSM hook.  This
                              allows for both the inode to have its own
                              security context and for the LSM to enforce
                              policy on the inode's creation.
  @name:    [in]    name of the "class" of the new file
  @fops:    [in]    file operations for the new file
  @priv:    [in]    private data for the new file (will be file's private_data)
  @flags:   [in]    flags
  @context_inode:
            [in]    the logical relationship with the new inode (optional)
  The LSM may use @context_inode in inode_init_security_anon(), but a
  reference to it is not held.  Returns the newly created file or an error
  pointer.  See the anon_inode_getfile() documentation for more information.
  anon_inode_getfd - creates a new file instance by hooking it up to
                     an anonymous inode and a dentry that describe
                     the "class" of the file
  @name:    [in]    name of the "class" of the new file
  @fops:    [in]    file operations for the new file
  @priv:    [in]    private data for the new file (will be file's private_data)
  @flags:   [in]    flags
  Creates a new file by hooking it on a single inode. This is
  useful for files that do not need to have a full-fledged inode in
  order to operate correctly.  All the files created with
  anon_inode_getfd() will use the same singleton inode, reducing
  memory use and avoiding code duplication for the fileinodedentry
  setup.  Returns a newly created file descriptor or an error code.
  anon_inode_getfd_secure - Like anon_inode_getfd(), but creates a new
  !S_PRIVATE anon inode rather than reuse the singleton anon inode, and calls
  the inode_init_security_anon() LSM hook. This allows the inode to have its
  own security context and for a LSM to reject creation of the inode.
  @name:    [in]    name of the "class" of the new file
  @fops:    [in]    file operations for the new file
  @priv:    [in]    private data for the new file (will be file's private_data)
  @flags:   [in]    flags
  @context_inode:
            [in]    the logical relationship with the new inode (optional)
  The LSM may use @context_inode in inode_init_security_anon(), but a
  reference to it is not held.
 SPDX-License-Identifier: GPL-2.0
	
	  We need to make sure whether the file system
	  support decoding of the file handle
 convert handle size to multiple of sizeof(u32) 
 we ask for a non connected handle 
 convert handle size to bytes 
		 As per old exportfs_encode_fh documentation
		  we could return ENOSPC to indicate overflow
		  But file system returned 255 always. So handle
		  both the values
		
		  set the handle size to zero so we copy only
		  non variable part of the file_handle
 copy the mount id 
  sys_name_to_handle_at: convert name to handle
  @dfd: directory relative to which name is interpreted if not absolute
  @name: name that should be converted to handle.
  @handle: resulting file handle
  @mnt_id: mount id of the file system containing the file
  @flag: flag value to indicate whether to follow symlink or not
  @handle->handle_size indicate the space available to store the
  variable part of the file handle in bytes. If there is not
  enough space, the field is updated to return the minimum
  value required.
 change the handle size to multiple of sizeof(u32) 
	
	  With handle we don't look at the execute bit on the
	  directory. Ideally we would like CAP_DAC_SEARCH.
	  But we don't have that
 copy the full handle 
  sys_open_by_handle_at: Open the file handle
  @mountdirfd: directory file descriptor
  @handle: file handle to be opened
  @flags: open flags.
  @mountdirfd indicate the directory file descriptor
  of the mount point. file handle is decoded relative
  to the vfsmount pointed by the @mountdirfd. @flags
  value is same as the open(2) flags.
  Exactly like fsopen.c:sys_open_by_handle_at(), except that it
  doesn't set the O_LARGEFILE flag.
 SPDX-License-Identifier: GPL-2.0-only
  (C) 1997 Linus Torvalds
  (C) 1999 Andrea Arcangeli <andrea@suse.de> (dynamic inode allocation)
 for inode_has_buffers 
  Inode locking rules:
  inode->i_lock protects:
    inode->i_state, inode->i_hash, __iget()
  Inode LRU list locks protect:
    inode->i_sb->s_inode_lru, inode->i_lru
  inode->i_sb->s_inode_list_lock protects:
    inode->i_sb->s_inodes, inode->i_sb_list
  bdi->wb.list_lock protects:
    bdi->wb.b_{dirty,io,more_io,dirty_time}, inode->i_io_list
  inode_hash_lock protects:
    inode_hashtable, inode->i_hash
  Lock ordering:
  inode->i_sb->s_inode_list_lock
    inode->i_lock
      Inode LRU list locks
  bdi->wb.list_lock
    inode->i_lock
  inode_hash_lock
    inode->i_sb->s_inode_list_lock
    inode->i_lock
  iunique_lock
    inode_hash_lock
  Empty aops. Can be used for the cases where the user does not
  define any of the address_space operations.
  Statistics gathering..
 not actually dirty inodes, but a wild approximation 
  Handle nr_inode sysctl
  inode_init_always - perform inode structure initialisation
  @sb: superblock inode belongs to
  @inode: inode to initialise
  These are initializations that need to be done on every inode
  allocation as the fields are not initialised by slab allocation.
 buggered by rcu freeing 
  drop_nlink - directly drop an inode's link count
  @inode: inode
  This is a low-level filesystem helper to replace any
  direct filesystem manipulation of i_nlink.  In cases
  where we are attempting to track writes to the
  filesystem, a decrement to zero means an imminent
  write when the file is truncated and actually unlinked
  on the filesystem.
  clear_nlink - directly zero an inode's link count
  @inode: inode
  This is a low-level filesystem helper to replace any
  direct filesystem manipulation of i_nlink.  See
  drop_nlink() for why we care about i_nlink hitting zero.
  set_nlink - directly set an inode's link count
  @inode: inode
  @nlink: new nlink (should be non-zero)
  This is a low-level filesystem helper to replace any
  direct filesystem manipulation of i_nlink.
 Yes, some filesystems do change nlink from zero to one 
  inc_nlink - directly increment an inode's link count
  @inode: inode
  This is a low-level filesystem helper to replace any
  direct filesystem manipulation of i_nlink.  Currently,
  it is only here for parity with dec_nlink().
  These are initializations that only need to be done
  once, because the fields are idempotent across use
  of the inode, so let the slab aware of that.
  inode->i_lock must be held
  get additional reference to inode; caller must already hold one.
  Add inode to LRU if needed (inode is unused and clean).
  Needs inode->i_lock held.
  inode_sb_list_add - add inode to the superblock list of inodes
  @inode: inode to add
 	__insert_inode_hash - hash an inode
 	@inode: unhashed inode
 	@hashval: unsigned long value used to locate this object in the
 		inode_hashtable.
 	Add an inode to the inode hash for this superblock.
 	__remove_inode_hash - remove an inode from the hash
 	@inode: inode to unhash
 	Remove an inode from the superblock.
	
	  We have to cycle the i_pages lock here because reclaim can be in the
	  process of removing the last page (in __delete_from_page_cache())
	  and we must not free the mapping under it.
	
	  Almost always, mapping_empty(&inode->i_data) here; but there are
	  two known and long-standing ways in which nodes may get left behind
	  (when deep radix-tree node allocation failed partway; or when THP
	  collapse_file() failed). Until those two known cases are cleaned up,
	  or a cleanup function is called here, do not BUG_ON(!mapping_empty),
	  nor even WARN_ON(!mapping_empty).
 don't need i_lock here, no concurrent mods to i_state 
  Free the inode passed in, removing it from the lists it is still connected
  to. We remove any pages still attached to the inode and wait for any IO that
  is still in progress before finally destroying the inode.
  An inode must already be marked I_FREEING so that we avoid the inode being
  moved back onto lists if we race with other code that manipulates the lists
  (e.g. writeback_single_inode). The caller is responsible for setting this.
  An inode must already be removed from the LRU list before being evicted from
  the cache. This should occur atomically with setting the I_FREEING state
  flag, so no inodes here should ever be on the LRU when being evicted.
	
	  Wait for flusher thread to be done with the inode so that filesystem
	  does not start destroying it while writeback is still running. Since
	  the inode has I_FREEING set, flusher thread won't start new work on
	  the inode.  We just have to wait for running writeback to finish.
  dispose_list - dispose of the contents of a local list
  @head: the head of the list to free
  Dispose-list gets a local list with local inodes in it, so it doesn't
  need to worry about list corruption and SMP locks.
  evict_inodes	- evict all evictable inodes for a superblock
  @sb:		superblock to operate on
  Make sure that no inodes with zero refcount are retained.  This is
  called by superblock shutdown after having SB_ACTIVE flag removed,
  so any inode reaching zero refcount during or after that call will
  be immediately evicted.
		
		  We can have a ton of inodes to evict at unmount time given
		  enough memory, check to see if we need to go to sleep for a
		  bit so we don't livelock.
  invalidate_inodes	- attempt to free all inodes on a superblock
  @sb:		superblock to operate on
  @kill_dirty: flag to guide handling of dirty inodes
  Attempts to free all inodes for a given superblock.  If there were any
  busy inodes return a non-zero value, else zero.
  If @kill_dirty is set, discard dirty inodes too, otherwise treat
  them as busy.
  Isolate the inode from the LRU in preparation for freeing it.
  If the inode has the I_REFERENCED flag set, then it means that it has been
  used recently - the flag is set in iput_final(). When we encounter such an
  inode, clear the flag and move it to the back of the LRU so it gets another
  pass through the LRU before it gets reclaimed. This is necessary because of
  the fact we are doing lazy LRU updates to minimise lock contention so the
  LRU does not have strict ordering. Hence we don't want to reclaim inodes
  with this flag set because they are the inodes that are out of order.
	
	  We are inverting the lru lockinode->i_lock here, so use a
	  trylock. If we fail to get the lock, just skip it.
	
	  Inodes can get referenced, redirtied, or repopulated while
	  they're already on the LRU, and this can make them
	  unreclaimable for a while. Remove them lazily here; iput,
	  sync, or the last page cache deletion will requeue them.
 Recently referenced inodes get one more pass 
	
	  On highmem systems, mapping_shrinkable() permits dropping
	  page cache in order to free up struct inodes: lowmem might
	  be under pressure before the cache inside the highmem zone.
  Walk the superblock inode LRU for freeable inodes and attempt to free them.
  This is called from the superblock shrinker function with a number of inodes
  to trim from the LRU. Inodes to be freed are moved to a temporary list and
  then are freed outside inode_lock by dispose_list().
  Called with the inode lock held.
  find_inode_fast is the fast path version of find_inode, see the comment at
  iget_locked for details.
  Each cpu owns a range of LAST_INO_BATCH numbers.
  'shared_last_ino' is dirtied only once out of LAST_INO_BATCH allocations,
  to renew the exhausted range.
  This does not significantly increase overflow rate because every CPU can
  consume at most LAST_INO_BATCH-1 unused inode numbers. So there is
  NR_CPUS(LAST_INO_BATCH-1) wastage. At 4096 and 1024, this is ~0.1% of the
  2^32 range, and is a worst-case. Even a 50% wastage would only increase
  overflow rate by 2x, which does not seem too significant.
  On a 32bit, non LFS stat() call, glibc will generate an EOVERFLOW
  error if st_ino won't fit in target struct field. Use 32bit counter
  here to attempt to avoid that.
 get_next_ino should not provide a 0 inode number 
 	new_inode_pseudo 	- obtain an inode
 	@sb: superblock
 	Allocates a new inode for given superblock.
 	Inode wont be chained in superblock s_inodes list
 	This means :
 	- fs can't be unmount
 	- quotas, fsnotify, writeback can't work
 	new_inode 	- obtain an inode
 	@sb: superblock
 	Allocates a new inode for given superblock. The default gfp_mask
 	for allocations related to inode->i_mapping is GFP_HIGHUSER_MOVABLE.
 	If HIGHMEM pages are unsuitable or it is known that pages allocated
 	for the page cache are not reclaimable or migratable,
 	mapping_set_gfp_mask() must be called with suitable flags on the
 	newly created inode's mapping
 Set new key only if filesystem hasn't already changed it 
			
			  ensure nobody is actually holding i_mutex
 mutex_destroy(&inode->i_mutex);
  unlock_new_inode - clear the I_NEW state and wake up any waiters
  @inode:	new inode to unlock
  Called when the inode is fully initialised to clear the new state of the
  inode and wake up anyone waiting for the inode to finish initialisation.
  lock_two_nondirectories - take two i_mutexes on non-directory objects
  Lock any non-NULL argument that is not a directory.
  Zero, one or two objects may be locked by this function.
  @inode1: first inode to lock
  @inode2: second inode to lock
  unlock_two_nondirectories - release locks from lock_two_nondirectories()
  @inode1: first inode to unlock
  @inode2: second inode to unlock
  inode_insert5 - obtain an inode from a mounted file system
  @inode:	pre-allocated inode to use for insert to cache
  @hashval:	hash value (usually inode number) to get
  @test:	callback used for comparisons between inodes
  @set:	callback used to initialize a new struct inode
  @data:	opaque data pointer to pass to @test and @set
  Search for the inode specified by @hashval and @data in the inode cache,
  and if present it is return it with an increased reference count. This is
  a variant of iget5_locked() for callers that don't want to fail on memory
  allocation of inode.
  If the inode is not in cache, insert the pre-allocated inode to cache and
  return it locked, hashed, and with the I_NEW flag set. The file system gets
  to fill it in before unlocking it via unlock_new_inode().
  Note both @test and @set are called with the inode_hash_lock held, so can't
  sleep.
		
		  Uhhuh, somebody else created the same inode under us.
		  Use the old inode instead of the preallocated one.
	
	  Return the locked inode with I_NEW set, the
	  caller is responsible for filling in the contents
  iget5_locked - obtain an inode from a mounted file system
  @sb:		super block of file system
  @hashval:	hash value (usually inode number) to get
  @test:	callback used for comparisons between inodes
  @set:	callback used to initialize a new struct inode
  @data:	opaque data pointer to pass to @test and @set
  Search for the inode specified by @hashval and @data in the inode cache,
  and if present it is return it with an increased reference count. This is
  a generalized version of iget_locked() for file systems where the inode
  number is not sufficient for unique identification of an inode.
  If the inode is not in cache, allocate a new inode and return it locked,
  hashed, and with the I_NEW flag set. The file system gets to fill it in
  before unlocking it via unlock_new_inode().
  Note both @test and @set are called with the inode_hash_lock held, so can't
  sleep.
  iget_locked - obtain an inode from a mounted file system
  @sb:		super block of file system
  @ino:	inode number to get
  Search for the inode specified by @ino in the inode cache and if present
  return it with an increased reference count. This is for file systems
  where the inode number is sufficient for unique identification of an inode.
  If the inode is not in cache, allocate a new inode and return it locked,
  hashed, and with the I_NEW flag set.  The file system gets to fill it in
  before unlocking it via unlock_new_inode().
 We released the lock, so.. 
			 Return the locked inode with I_NEW set, the
			  caller is responsible for filling in the contents
		
		  Uhhuh, somebody else created the same inode under
		  us. Use the old inode instead of the one we just
		  allocated.
  search the inode cache for a matching inode number.
  If we find one, then the inode number we are trying to
  allocate is not unique and so we should not use it.
  Returns 1 if the inode number is unique, 0 if it is not.
 	iunique - get a unique inode number
 	@sb: superblock
 	@max_reserved: highest reserved inode number
 	Obtain an inode number that is unique on the system for a given
 	superblock. This is used by file systems that have no natural
 	permanent inode numbering system. An inode number is returned that
 	is higher than the reserved limit but unique.
 	BUGS:
 	With a large number of inodes live on the file system this function
 	currently becomes quite slow.
	
	  On a 32bit, non LFS stat() call, glibc will generate an EOVERFLOW
	  error if st_ino won't fit in target struct field. Use 32bit counter
	  here to attempt to avoid that.
		
		  Handle the case where s_op->clear_inode is not been
		  called yet, and somebody is calling igrab
		  while the inode is getting freed.
  ilookup5_nowait - search for an inode in the inode cache
  @sb:		super block of file system to search
  @hashval:	hash value (usually inode number) to search for
  @test:	callback used for comparisons between inodes
  @data:	opaque data pointer to pass to @test
  Search for the inode specified by @hashval and @data in the inode cache.
  If the inode is in the cache, the inode is returned with an incremented
  reference count.
  Note: I_NEW is not waited upon so you have to be very careful what you do
  with the returned inode.  You probably should be using ilookup5() instead.
  Note2: @test is called with the inode_hash_lock held, so can't sleep.
  ilookup5 - search for an inode in the inode cache
  @sb:		super block of file system to search
  @hashval:	hash value (usually inode number) to search for
  @test:	callback used for comparisons between inodes
  @data:	opaque data pointer to pass to @test
  Search for the inode specified by @hashval and @data in the inode cache,
  and if the inode is in the cache, return the inode with an incremented
  reference count.  Waits on I_NEW before returning the inode.
  returned with an incremented reference count.
  This is a generalized version of ilookup() for file systems where the
  inode number is not sufficient for unique identification of an inode.
  Note: @test is called with the inode_hash_lock held, so can't sleep.
  ilookup - search for an inode in the inode cache
  @sb:		super block of file system to search
  @ino:	inode number to search for
  Search for the inode @ino in the inode cache, and if the inode is in the
  cache, the inode is returned with an incremented reference count.
  find_inode_nowait - find an inode in the inode cache
  @sb:		super block of file system to search
  @hashval:	hash value (usually inode number) to search for
  @match:	callback used for comparisons between inodes
  @data:	opaque data pointer to pass to @match
  Search for the inode specified by @hashval and @data in the inode
  cache, where the helper function @match will return 0 if the inode
  does not match, 1 if the inode does match, and -1 if the search
  should be stopped.  The @match function must be responsible for
  taking the i_lock spin_lock and checking i_state for an inode being
  freed or being initialized, and incrementing the reference count
  before returning 1.  It also must not sleep, since it is called with
  the inode_hash_lock spinlock held.
  This is a even more generalized version of ilookup5() when the
  function must never block --- find_inode() can block in
  __wait_on_freeing_inode() --- or when the caller can not increment
  the reference count because the resulting iput() might cause an
  inode eviction.  The tradeoff is that the @match funtion must be
  very carefully implemented.
  find_inode_rcu - find an inode in the inode cache
  @sb:		Super block of file system to search
  @hashval:	Key to hash
  @test:	Function to test match on an inode
  @data:	Data for test function
  Search for the inode specified by @hashval and @data in the inode cache,
  where the helper function @test will return 0 if the inode does not match
  and 1 if it does.  The @test function must be responsible for taking the
  i_lock spin_lock and checking i_state for an inode being freed or being
  initialized.
  If successful, this will return the inode for which the @test function
  returned 1 and NULL otherwise.
  The @test function is not permitted to take a ref on any inode presented.
  It is also not permitted to sleep.
  The caller must hold the RCU read lock.
  find_inode_by_ino_rcu - Find an inode in the inode cache
  @sb:		Super block of file system to search
  @ino:	The inode number to match
  Search for the inode specified by @hashval and @data in the inode cache,
  where the helper function @test will return 0 if the inode does not match
  and 1 if it does.  The @test function must be responsible for taking the
  i_lock spin_lock and checking i_state for an inode being freed or being
  initialized.
  If successful, this will return the inode for which the @test function
  returned 1 and NULL otherwise.
  The @test function is not permitted to take a ref on any inode presented.
  It is also not permitted to sleep.
  The caller must hold the RCU read lock.
  Called when we're dropping the last reference
  to an inode.
  Call the FS "drop_inode()" function, defaulting to
  the legacy UNIX filesystem behaviour.  If it tells
  us to evict inode, do so.  Otherwise, retain inode
  in cache if fs is alive, sync and evict if fs is
  shutting down.
 	iput	- put an inode
 	@inode: inode to put
 	Puts an inode, dropping its usage count. If the inode use count hits
 	zero, the inode is then freed and may also be destroyed.
 	Consequently, iput() can sleep.
 	bmap	- find a block number in a file
 	@inode:  inode owning the block number being requested
 	@block: pointer containing the block to find
 	Replaces the value in ``block`` with the block number on the device holding
 	corresponding to the requested block number in the file.
 	That is, asked for block 4 of inode 1 the function will replace the
 	4 in ``block``, with disk block relative to the disk start that holds that
 	block of the file.
 	Returns -EINVAL in case of error, 0 otherwise. If mapping falls into a
 	hole, returns 0 and ``block`` is also set to 0.
  With relative atime, only update atime if the previous atime is
  earlier than either the ctime or mtime or if at least a day has
  passed since the last atime update.
	
	  Is mtime younger than atime? If yes, update atime:
	
	  Is ctime younger than atime? If yes, update atime:
	
	  Is the previous atime value older than a day? If yes,
	  update atime:
	
	  Good, we can skip the atime update:
  This does the actual work of updating an inodes time or version.  Must have
  had called mnt_want_write() before calling this.
 	atime_needs_update	-	update the access time
 	@path: the &struct path to update
 	@inode: inode to update
 	Update the accessed time on an inode and mark it for writeback.
 	This function automatically handles read only file systems and media,
 	as well as the "noatime" flag and inode specific "noatime" markers.
	 Atime updates will likely cause i_uid and i_gid to be written
	  back improprely if their true value is unknown to the vfs.
	
	  File systems can error out when updating inodes if they need to
	  allocate new space to modify an inode (such is the case for
	  Btrfs), but since we touch atime while walking down the path we
	  really don't care if we failed to update the atime of the file,
	  so just ignore the return value.
	  We may also fail on filesystems that have the ability to make parts
	  of the fs read only, e.g. subvolumes in Btrfs.
  The logic we want is
 	if suid or (sgid and xgrp)
 		remove privs
 suid always must be killed 
	
	  sgid without any exec bits is just a mandatory locking mark; leave
	  it alone.  If some exec bits are set, it's a real sgid; kill it.
  Return mask of changes for notify_change() that need to be done as a
  response to write or truncate. Return 0 if nothing has to be changed.
  Negative value on error (change should be denied).
	
	  Note we call this on write, so notify_change will not
	  encounter any conflicting delegations:
  Remove special file priviledges (suid, capabilities) when file is written
  to or truncated.
	
	  Fast path for nothing security related.
	  As well for non-regular files, e.g. blkdev inodes.
	  For example, blkdev_write_iter() might get here
	  trying to remove privs which it is not allowed to.
 	file_update_time	-	update mtime and ctime time
 	@file: file accessed
 	Update the mtime and ctime members of an inode and mark the inode
 	for writeback.  Note that this function is meant exclusively for
 	usage in the file write path of filesystems, and filesystems may
 	choose to explicitly ignore update via this function with the
 	S_NOCMTIME inode flag, e.g. for network filesystem where these
 	timestamps are handled by the server.  This can return an error for
 	file systems who need to allocate space in order to update an inode.
 First try to exhaust all avenues to not sync 
 Finally allowed to write? Takes lock. 
 Caller must hold the file's inode lock 
	
	  Clear the security bits if the process is not being run by root.
	  This keeps people from modifying setuid and setgid binaries.
  If we try to find an inode in the inode hash while it is being
  deleted, we have to wait until the filesystem completes its
  deletion before reporting that it isn't found.  This function waits
  until the deletion _might_ have completed.  Callers are responsible
  to recheck inode state.
  It doesn't matter if I_NEW is not set initially, a call to
  wake_up_bit(&inode->i_state, __I_NEW) after removing from the hash list
  will DTRT.
  Initialize the waitqueues and inode hash table.
	 If hashes are distributed across NUMA nodes, defer
	  hash allocation until vmalloc space is available.
 inode slab cache 
 Hash may have been set up in inode_init_early 
 leave it no_open_fops 
  inode_init_owner - Init uid,gid,mode for new inode according to posix standards
  @mnt_userns:	User namespace of the mount the inode was created from
  @inode: New inode
  @dir: Directory inode
  @mode: mode of the new inode
  If the inode has been created through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions
  and initializing i_uid and i_gid. On non-idmapped mounts or if permission
  checking is to be performed on the raw inode simply passs init_user_ns.
 Directories are special, and always inherit S_ISGID 
  inode_owner_or_capable - check current task permissions to inode
  @mnt_userns:	user namespace of the mount the inode was found from
  @inode: inode being checked
  Return true if current either has CAP_FOWNER in a namespace with the
  inode owner uid mapped, or owns the file.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then take
  care to map the inode according to @mnt_userns before checking permissions.
  On non-idmapped mounts or if permission checking is to be performed on the
  raw inode simply passs init_user_ns.
  Direct io helper functions
  inode_dio_wait - wait for outstanding DIO requests to finish
  @inode: inode to wait for
  Waits for all pending direct IO requests to finish so that we can
  proceed with a truncate or equivalent operation.
  Must be called under a lock that serializes taking new references
  to i_dio_count, usually by inode->i_mutex.
  inode_set_flags - atomically set some inode flags
  Note: the caller should be holding i_mutex, or else be sure that
  they have exclusive access to the inode structure (i.e., while the
  inode is being instantiated).  The reason for the cmpxchg() loop
  --- which wouldn't be necessary if all code paths which modify
  i_flags actually followed this rule, is that there is at least one
  code path which doesn't today so we use cmpxchg() out of an abundance
  of caution.
  In the long run, i_mutex is overkill, and we should probably look
  at using the i_lock spinlock to protect i_flags, and then make sure
  it is so documented in includelinuxfs.h and that all code follows
  the locking convention!!
  timestamp_truncate - Truncate timespec to a granularity
  @t: Timespec
  @inode: inode being updated
  Truncate a timespec to the granularity supported by the fs
  containing the inode. Always rounds down. gran must
  not be 0 nor greater than a second (NSEC_PER_SEC, or 10^9 ns).
 Avoid division in the common cases 1 ns and 1 s. 
 nothing 
  current_time - Return FS time
  @inode: inode.
  Return the current time truncated to the time granularity supported by
  the fs.
  Note that inode and inode->sb cannot be NULL.
  Otherwise, the function warns and returns time without truncation.
 SPDX-License-Identifier: GPL-2.0-or-later
 Filesystem parameter parser.
  Copyright (C) 2018 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  lookup_constant - Look up a constant by name in an ordered table
  @tbl: The table of constants to search.
  @name: The name to look up.
  @not_found: The value to return if the name is not found.
  fs_parse - Parse a filesystem configuration parameter
  @fc: The filesystem context to log errors through.
  @desc: The parameter description to use.
  @param: The parameter.
  @result: Where to place the result of the parse
  Parse a filesystem configuration parameter and attempt a conversion for a
  simple parameter for which this is requested.  If successful, the determined
  parameter ID is placed into @result->key, the desired type is indicated in
  @result->t and any converted value is placed into an appropriate member of
  the union in @result.
  The function returns the parameter number if the parameter was matched,
  -ENOPARAM if it wasn't matched and @desc->ignore_unknown indicated that
  unknown parameters are okay and -EINVAL if there was a conversion issue or
  the parameter wasn't recognised and unknowns aren't okay.
	 Try to turn the type we were given into the type desired by the
	  parameter and give an error if we can't.
  fs_lookup_param - Look up a path referred to by a parameter
  @fc: The filesystem context to log errors through.
  @param: The parameter.
  @want_bdev: T if want a blockdev
  @_path: The result of the lookup
  validate_constant_table - Validate a constant table
  @tbl: The constant table to validate.
  @tbl_size: The size of the table.
  @low: The lowest permissible value.
  @high: The highest permissible value.
  @special: One special permissible value outside of the range.
  fs_validate_description - Validate a parameter description
  @name: The parameter name to search for.
  @desc: The parameter description to validate.
 Check for duplicate parameter names 
 CONFIG_VALIDATE_FS_PARSER 
 SPDX-License-Identifier: GPL-2.0
  fsmpage.c
  Copyright (C) 2002, Linus Torvalds.
  Contains functions related to preparing and submitting BIOs which contain
  multiple pagecache pages.
  15May2002	Andrew Morton
 		Initial version
  27Jun2002	axboe@suse.de
 		use bio_add_page() to build bio's just the right size
  IO completion handler for multipage BIOs.
  The mpage code never puts partial pages into a BIO (except for end-of-file).
  If a page does not map to a contiguous run of blocks then it simply falls
  back to block_read_full_page().
  Why is this?  If a page's completion depends on a number of different BIOs
  which can complete in any order (or at the same time) then determining the
  status of that page is hard.  See end_buffer_async_read() for the details.
  There is no point in duplicating all that complexity.
 Restrict the given (page cache) mask for slab allocations 
  support function for mpage_readahead.  The fs supplied get_block might
  return an up to date buffer.  This is used to map that buffer into
  the page, which allows readpage to avoid triggering a duplicate call
  to get_block.
  The idea is to avoid adding buffers to pages that don't already have
  them.  So when the buffer is up to date and the page size == block size,
  this marks the page up to date instead of adding new buffers.
		
		  don't make any buffers if there is only one buffer on
		  the page and the page just needs to be set up to date
  This is the worker routine which does all the work of mapping the disk
  blocks and constructs largest possible bios, submits them for IO if the
  blocks are not contiguous on the disk.
  We pass a buffer_head back and forth and use its buffer_mapped() flag to
  represent the validity of its disk mapping and to decide when to do the next
  get_block() call.
	
	  Map blocks using the result from the previous get_blocks call first.
	
	  Then do more get_blocks calls until we are done with this page.
		 some filesystems will copy data into the page during
		  the get_block call, in which case we don't want to
		  read it again.  map_buffer_to_page copies the data
		  we just collected from get_block into the page's buffers
		  so readpage doesn't have to repeat the get_block call
 hole -> non-hole 
 Contiguous blocks? 
	
	  This page will go to BIO.  Do we need to send this BIO off first?
  mpage_readahead - start reads against pages
  @rac: Describes which pages to read.
  @get_block: The filesystem's block mapper function.
  This function walks the pages and the blocks within each page, building and
  emitting large BIOs.
  If anything unusual happens, such as:
  - encountering a page which has buffers
  - encountering a page which has a non-hole after a hole
  - encountering a page with non-contiguous blocks
  then this code just gives up and calls the buffer_head-based read function.
  It does handle a page which has holes at the end - that is a common case:
  the end-of-file on blocksize < PAGE_SIZE setups.
  BH_Boundary explanation:
  There is a problem.  The mpage read code assembles several pages, gets all
  their disk mappings, and then submits them all.  That's fine, but obtaining
  the disk mappings may require IO.  Reads of indirect blocks, for example.
  So an mpage read of the first 16 blocks of an ext2 file will cause IO to be
  submitted in the following order:
  	12 0 1 2 3 4 5 6 7 8 9 10 11 13 14 15 16
  because the indirect block has to be read to get the mappings of blocks
  13,14,15,16.  Obviously, this impacts performance.
  So what we do it to allow the filesystem's get_block() function to set
  BH_Boundary when it maps block 11.  BH_Boundary says: mapping of the block
  after this one will require IO against a block which is probably close to
  this one.  So you should push what IO you have currently accumulated.
  This all causes the disk requests to be issued in the correct order.
  This isn't called much at all
  Writing is not so simple.
  If the page has buffers then they will be used for obtaining the disk
  mapping.  We only support pages which are fully mapped-and-dirty, with a
  special case for pages which are unmapped at the end: end-of-file.
  If the page has no buffers (preferred) then the page is mapped here.
  If all blocks are found to be contiguous then the page can go into the
  BIO.  Otherwise fall back to the mapping's writepage().
  FIXME: This code wants an estimate of how many pages are still to be
  written, so it can intelligently allocate a suitably-sized BIO.  For now,
  just allocate full-size (16-page) BIOs.
  We have our BIO, so we can now mark the buffers clean.  Make
  sure to only clean buffers which we know we'll be writing.
	
	  we cannot drop the bh if the page is not uptodate or a concurrent
	  readpage would fail to serialize with the bh and it would read from
	  disk before we reach the platter.
  For situations where we want to clean all buffers attached to a page.
  We don't need to calculate how many buffers are attached to the page,
  we just need to specify a number larger than the maximum number of buffers.
 If they're all mapped and dirty, do it 
				
				  unmapped dirty buffers are created by
				  __set_page_dirty_buffers -> mmapped data
 hole -> non-hole 
		
		  Page has buffers, but they are all unmapped. The page was
		  created by pagein or read over a hole which was handled by
		  block_read_full_page().  If this address_space is also
		  using mpage_readahead then this can rarely happen.
	
	  The page has no buffers: map it to disk
		
		  The page straddles i_size.  It must be zeroed out on each
		  and every writepage invocation because it may be mmapped.
		  "A file is mapped in multiples of the page size.  For a file
		  that is not a multiple of the page size, the remaining memory
		  is zeroed when mapped, and writes to that region are not
		  written out to the file."
	
	  This page will go to BIO.  Do we need to send this BIO off first?
	
	  Must try to add the page before marking the buffer clean or
	  the confused fail path above (OOM) will be very confused when
	  it finds all bh marked clean (i.e. it will not write anything)
	
	  The caller has a ref on the inode, so mapping is stable
  mpage_writepages - walk the list of dirty pages of the given address space & writepage() all of them
  @mapping: address space structure to write
  @wbc: subtract the number of written pages from @wbc->nr_to_write
  @get_block: the filesystem's block mapper function.
              If this is NULL then use a_ops->writepage.  Otherwise, go
              direct-to-BIO.
  This is a library function, which implements the writepages()
  address_space_operation.
  If a page is already under IO, generic_writepages() skips it, even
  if it's dirty.  This is desirable behaviour for memory-cleaning writeback,
  but it is INCORRECT for data-integrity system calls such as fsync().  fsync()
  and msync() need to guarantee that all the data which was dirty at the time
  the call was made get new IO started against them.  If wbc->sync_mode is
  WB_SYNC_ALL then we were called for data integrity and we must wait for
  existing IO to complete.
 SPDX-License-Identifier: GPL-2.0-only
   linuxfsfile_table.c
   Copyright (C) 1991, 1992  Linus Torvalds
   Copyright (C) 1997 David S. Miller (davem@caip.rutgers.edu)
 sysctl tunables... 
 SLAB cache for file structures 
  Return the total number of open files in the system
  Return the maximum number of open files in the system
  Handle nr_files sysctl
 f->f_version: 0 
 Find an unused file structure and return a pointer to it.
  Returns an error pointer if some error happend e.g. we over file
  structures limit, run out of memory or operation is not permitted.
  Be very careful using this.  You are responsible for
  getting write access to any mount that you might assign
  to this filp, if it is opened for write.  If this is not
  done, you will imbalance int the mount's writer count
  and a warning at __fput() time.
	
	  Privileged users can go above max_files
		
		  percpu_counters are inaccurate.  Do an expensive check before
		  we go and fail.
 Ran out of filps - report that 
  Variant of alloc_empty_file() that doesn't check and modify nr_files.
  Should not be used unless there's a very good reason to do so.
  alloc_file - allocate and initialize a 'struct file'
  @path: the (dentry, vfsmount) pair for the new file
  @flags: O_... flags with which the new file will be opened
  @fop: the 'struct file_operations' for the new file
 the real guts of fput() - releasing the last reference to file
	
	  The function eventpoll_release() should be the first called
	  in the file cleanup chain.
  If kernel thread really needs to have the final fput() it has done
  to complete, call this.  The only user right now is the boot - we
  do need to make sure our writes to binaries on initramfs has
  not left us with opened struct file waiting for __fput() - execve()
  won't work without that.  Please, don't add more callers without
  very good reasons; in particular, never call that with locks
  held and never call that from a thread that might need to do
  some work on any kind of umount.
			
			  After this task has run exit_task_work(),
			  task_work_add() will fail.  Fall through to delayed
			  fput to avoid leaking file.
  synchronous analog of fput(); for kernel threads that might be needed
  in some umount() (and thus can't use flush_delayed_fput() without
  risking deadlocks), need to wait for completion of __fput() and know
  for this specific struct file it won't involve anything that would
  need them.  Use only if you really need it - at the very least,
  don't blindly convert fput() by kernel thread to that.
  One file with associated inode and dcache is very roughly 1K. Per default
  do not use more than 10% of our memory for files.
 SPDX-License-Identifier: GPL-2.0-only
   linuxfsbuffer.c
   Copyright (C) 1991, 1992, 2002  Linus Torvalds
  Start bdflush() with kernel_thread not syscall - Paul Gortmaker, 1295
  Removed a lot of unnecessary code and simplified things now that
  the buffer cache isn't our primary cache - Andrew Tridgell 1296
  Speed up hash, lru, and free list operations.  Use gfp() for allocating
  hash table, use SLAB cache for buffer heads. SMP threading.  -DaveM
  Added 32k buffer block sizes - these are required older ARM systems. - RMK
  async buffer flushing, 1999 Andrea Arcangeli <andrea@suse.de>
  Returns if the page has dirty or writeback buffers. If all the buffers
  are unlocked and clean then the PageDirty information is stale. If
  any of the pages are locked, it is assumed they are locked for IO.
  Block until a buffer comes unlocked.  This doesn't stop it
  from becoming locked again - you have to lock it yourself
  if you want to preserve its state.
  End-of-IO handler helper function which does not touch the bh after
  unlocking it.
  Note: unlock_buffer() sort-of does touch the bh after unlocking it, but
  a race there is benign: unlock_buffer() only use the bh's address for
  hashing after unlocking the buffer, so it doesn't actually touch the bh
  itself.
 This happens, due to failed read-ahead attempts. 
  Default synchronous end-of-IO handler..  Just mark it up-to-date and
  unlock the buffer. This is what ll_rw_block uses too.
  Various filesystems appear to want __find_get_block to be non-blocking.
  But it's the page lock which protects the buffers.  To get around this,
  we get exclusion from try_to_free_buffers with the blockdev mapping's
  private_lock.
  Hack idea: for the blockdev mapping, private_lock contention
  may be quite high.  This code could TryLock the page, and if that
  succeeds, there is no need to take private_lock.
	 we might be here because some of the buffers on this page are
	  not mapped.  This is due to various races between
	  file io on the block device and getblk.  It gets dealt with
	  elsewhere, don't buffer_error if we had some unmapped buffers
	
	  Be _very_ careful from here on. Bad things can happen if
	  two buffer heads end IO at almost the same time and both
	  decide that the page is now completely done.
	
	  If none of the buffers had errors and they are all
	  uptodate then we can set the page uptodate.
  IO completion handler for block_read_full_page() - pages
  which come unlocked at the end of IO.
 Decrypt if needed 
  Completion handler for block_write_full_page() - pages which are unlocked
  during IO, and which have PageWriteback cleared upon IO completion.
  If a page's buffers are under async readin (end_buffer_async_read
  completion) then there is a possibility that another thread of
  control could lock one of the buffers after it has completed
  but while some of the other buffers have not completed.  This
  locked buffer would confuse end_buffer_async_read() into not unlocking
  the page.  So the absence of BH_Async_Read tells end_buffer_async_read()
  that this buffer is not under async IO.
  The page comes unlocked when it has no locked buffer_async buffers
  left.
  PageLocked prevents anyone starting new async IO reads any of
  the buffers.
  PageWriteback is used to prevent simultaneous writeout of the same
  page.
  PageLocked prevents anyone from starting writeback of a page which is
  under read IO (PageWriteback is only ever set against a locked page).
  fsbuffer.c contains helper functions for buffer-backed address space's
  fsync functions.  A common requirement for buffer-based filesystems is
  that certain data from the backing blockdev needs to be written out for
  a successful fsync().  For example, ext2 indirect blocks need to be
  written back and waited upon before fsync() returns.
  The functions mark_buffer_inode_dirty(), fsync_inode_buffers(),
  inode_has_buffers() and invalidate_inode_buffers() are provided for the
  management of a list of dependent buffers at ->i_mapping->private_list.
  Locking is a little subtle: try_to_free_buffers() will remove buffers
  from their controlling inode's queue when they are being freed.  But
  try_to_free_buffers() will be operating against the blockdev mapping
  at the time, not against the S_ISREG file which depends on those buffers.
  So the locking for private_list is via the private_lock in the address_space
  which backs the buffers.  Which is different from the address_space 
  against which the buffers are listed.  So for a particular address_space,
  mapping->private_lock does not protect mapping->private_list!  In fact,
  mapping->private_list will always be protected by the backing blockdev's
  ->private_lock.
  Which introduces a requirement: all buffers on an address_space's
  ->private_list must be from the same address_space: the blockdev's.
  address_spaces which do not place buffers at ->private_list via these
  utility functions are free to use private_lock and private_list for
  whatever they want.  The only requirement is that list_empty(private_list)
  be true at clear_inode() time.
  FIXME: clear_inode should not call invalidate_inode_buffers().  The
  filesystems should do that.  invalidate_inode_buffers() should just go
  BUG_ON(!list_empty).
  FIXME: mark_buffer_dirty_inode() is a data-plane operation.  It should
  take an address_space, not an inode.  And it should be called
  mark_buffer_dirty_fsync() to clearly define why those buffers are being
  queued up.
  FIXME: mark_buffer_dirty_inode() doesn't need to add the buffer to the
  list if it is already on a list.  Because if the buffer is on a list,
  it must already be on the right one.  If not, the filesystem is being
  silly.  This will save a ton of locking.  But first we have to ensure
  that buffers are taken off the old inode's list when they are freed
  (presumably in truncate).  That requires careful auditing of all
  filesystems (do it inside bforget()).  It could also be done by bringing
  b_inode back.
  The buffer's backing address_space's private_lock must be held
  osync is designed to support O_SYNC io.  It waits synchronously for
  all already-submitted IO to complete, but does not queue any new
  writes to the disk.
  To do O_SYNC writes, just queue the buffer writes with ll_rw_block as
  you dirty the buffers, and then use osync_inode_buffers to wait for
  completion.  Any other dirty buffers which are not yet queued for
  write will not be flushed to disk by the osync.
  sync_mapping_buffers - write out & wait upon a mapping's "associated" buffers
  @mapping: the mapping which wants those buffers written
  Starts IO against the buffers at mapping->private_list, and waits upon
  that IO.
  Basically, this is a convenience function for fsync().
  @mapping is a file or directory which needs those buffers to be written for
  a successful fsync().
  Called when we've recently written block `bblock', and it is known that
  `bblock' was for a buffer_boundary() buffer.  This means that the block at
  `bblock + 1' is probably a dirty indirect block.  Hunt it down and, if it's
  dirty, schedule it for IO.  So that indirects merge nicely with their data.
  Add a page to the dirty page list.
  It is a sad fact of life that this function is called from several places
  deeply under spinlocking.  It may not sleep.
  If the page has buffers, the uptodate buffers are set dirty, to preserve
  dirty-state coherency between the page and the buffers.  It the page does
  not have buffers then when they are later attached they will all be set
  dirty.
  The buffers are dirtied before the page is dirtied.  There's a small race
  window in which a writepage caller may see the page cleanness but not the
  buffer dirtiness.  That's fine.  If this code were to set the page dirty
  before the buffers, a concurrent writepage caller could clear the page dirty
  bit, see a bunch of clean buffers and we'd end up with dirty buffersclean
  page on the dirty page list.
  We use private_lock to lock against try_to_free_buffers while using the
  page's buffer list.  Also use this to protect against clean buffers being
  added to the page after it was set dirty.
  FIXME: may need to call ->reservepage here as well.  That's rather up to the
  address_space though.
	
	  Lock out page's memcg migration to keep PageDirty
	  synchronized with per-memcg dirty page counters.
  Write out and wait upon a list of buffers.
  We have conflicting pressures: we want to make sure that all
  initially dirty buffers get waited on, but that any subsequently
  dirtied buffers don't.  After all, we don't want fsync to last
  forever if somebody is actively writing to the file.
  Do this in two main stages: first we copy dirty buffers to a
  temporary inode list, queueing the writes as we go.  Then we clean
  up, waiting for those writes to complete.
  During this second stage, any subsequent updates to the file may end
  up refiling the buffer on the original inode's dirty list again, so
  there is a chance we will end up with a buffer queued for write but
  not yet completed on that list.  So, as a final cleanup we go through
  the osync code to catch these locked, dirty buffers without requeuing
  any newly dirty buffers for write.
		 Avoid race with mark_buffer_dirty_inode() which does
				
				  Ensure any pending IO completes so that
				  write_dirty_buffer() actually writes the
				  current contents - it is a noop if IO is
				  still in flight on potentially older
				  contents.
				
				  Kick off IO for the previous mapping. Note
				  that we will not run the very last mapping,
				  wait_on_buffer() will do that for us
				  through sync_buffer().
		 Avoid race with mark_buffer_dirty_inode() which does
  Invalidate any and all dirty buffers on a given inode.  We are
  probably unmounting the fs, but that doesn't mean we have already
  done a sync().  Just drop the buffers from the inode list.
  NOTE: we take the inode's blockdev's mapping's private_lock.  Which
  assumes that all the buffers are against the blockdev.  Not true
  for reiserfs.
  Remove any clean buffers from the inode's buffer list.  This is called
  when we're trying to free the inode itself.  Those buffers can pin it.
  Returns true if all buffers were removed.
  Create the appropriate buffers when given a page for data area and
  the size of each buffer.. Use the bh->b_this_page linked list to
  follow the buffers created.  Return NULL if unable to create more
  buffers.
  The retry flag is used to differentiate async IO (paging, swapping)
  which may not fail from ordinary buffer allocations.
 The page lock pins the memcg 
 Link the buffer to its page 
  In case anything failed, we just free everything we got.
  Initialise the state of a blockdev page's buffers.
	
	  Caller needs to validate requested block against end of device.
  Create the page-cache page that contains the requested block.
  This is used purely for blockdev mappings.
	
	  XXX: __getblk_slow() can not really deal with failure and
	  will endlessly loop on improvised global reclaim.  Prefer
	  looping in the allocator rather than here, at least that
	  code knows what it's doing.
	
	  Allocate some buffers for this page
	
	  Link the page to the buffers and initialise them.  Take the
	  lock to be atomic wrt __find_get_block(), which does not
	  run under the page lock.
  Create buffers for the specified block device block's page.  If
  that page was dirty, the buffers are set dirty also.
	
	  Check for a block which wants to lie outside our maximum possible
	  pagecache index.  (this comparison is done using sector_t types).
 Create a page with the proper size buffers.. 
 Size must be multiple of hard sectorsize 
  The relationship between dirty buffers and dirty pages:
  Whenever a page has any dirty buffers, the page's dirty bit is set, and
  the page is tagged dirty in the page cache.
  At all times, the dirtiness of the buffers represents the dirtiness of
  subsections of the page.  If the page has buffers, the page dirty bit is
  merely a hint about the true dirty state.
  When a page is set dirty in its entirety, all its buffers are marked dirty
  (if the page has buffers).
  When a buffer is marked dirty, its page is dirtied, but the page's other
  buffers are not.
  Also.  When blockdev buffers are explicitly read with bread(), they
  individually become uptodate.  But their backing page remains not
  uptodate - even if all of its buffers are uptodate.  A subsequent
  block_read_full_page() against that page will discover all the uptodate
  buffers, will set the page uptodate and will perform no IO.
  mark_buffer_dirty - mark a buffer_head as needing writeout
  @bh: the buffer_head to mark dirty
  mark_buffer_dirty() will set the dirty bit against the buffer, then set
  its backing page dirty, then tag the page as dirty in the page cache
  and then attach the address_space's inode to its superblock's dirty
  inode list.
  mark_buffer_dirty() is atomic.  It takes bh->b_page->mapping->private_lock,
  i_pages lock and mapping->host->i_lock.
	
	  Very carefully optimize the it-is-already-dirty case.
	 
	  Don't let the final "is it dirty" escape to before we
	  perhaps modified the buffer.
 FIXME: do we need to set this in both places? 
  Decrement a buffer_head's reference count.  If all buffers against a page
  have zero reference count, are clean and unlocked, and if the page is clean
  and unlocked then try_to_free_buffers() may strip the buffers from the page
  in preparation for freeing it (sometimes, rarely, buffers are removed from
  a page but it ends up not being freed, and buffers may later be reattached).
  bforget() is like brelse(), except it discards any
  potentially dirty data.
  Per-cpu buffer LRU implementation.  To reduce the cost of __find_get_block().
  The bhs[] array is sorted - newest buffer is at bhs[0].  Buffers have their
  refcount elevated by one when they're in an LRU.  A buffer can only appear
  once in a particular CPU's LRU.  A single buffer can be present in multiple
  CPU's LRUs at the same time.
  This is a transparent caching front-end to sb_bread(), sb_getblk() and
  sb_find_get_block().
  The LRUs themselves only need locking against invalidate_bh_lrus.  We use
  a local interrupt disable for that.
  Install a buffer_head into this cpu's LRU.  If not already in the LRU, it is
  inserted at the front, and the buffer_head at the back if any is evicted.
  Or, if already in the LRU it is moved to the front.
	
	  the refcount of buffer_head in bh_lru prevents dropping the
	  attached page(i.e., try_to_free_buffers) so it could cause
	  failing page migration.
	  Skip putting upcoming bh into bh_lru until migration is done.
  Look up the bh in this cpu's LRU.  If it's there, move it to the head.
  Perform a pagecache lookup for the matching buffer.  If it's there, refresh
  it in the LRU and mark it as accessed.  If it is not present then return
  NULL
 __find_get_block_slow will mark the page accessed 
  __getblk_gfp() will locate (and, if necessary, create) the buffer_head
  which corresponds to the passed block_device, block and size. The
  returned buffer has its reference count incremented.
  __getblk_gfp() will lock up the machine if grow_dev_page's
  try_to_free_buffers() attempt is failing.  FIXME, perhaps?
  Do async read-ahead on a buffer..
   __bread_gfp() - reads a specified block and returns the bh
   @bdev: the block_device to read from
   @block: number of block
   @size: size (in bytes) to read
   @gfp: page allocation flag
   Reads a specified block, and returns buffer head that contains it.
   The page cache can be allocated from non-movable area
   not to prevent page migration if you set gfp to zero.
   It returns NULL if the block was unreadable.
  invalidate_bh_lrus() is called rarely - but not only at unmount.
  This doesn't race because it runs in each cpu either in irq
  or with preempt disabled.
  It's called from workqueue context so we need a bh_lru_lock to close
  the race with preemptionirq.
		
		  This catches illegal uses and preserves the offset:
  Called when truncating a buffer on a page completely.
 Bits that are cleared during an invalidate 
  block_invalidatepage - invalidate part or all of a buffer-backed page
  @page: the page which is affected
  @offset: start of the range to invalidate
  @length: length of the range to invalidate
  block_invalidatepage() is called when all or part of the page has become
  invalidated by a truncate operation.
  block_invalidatepage() does not have to release all buffers, but it must
  ensure that no dirty buffer is left outside @offset and that no IO
  is underway against any of the blocks which are outside the truncation
  point.  Because the caller is about to free (and possibly reuse) those
  blocks on-disk.
	
	  Check for overflow
		
		  Are we still fully in range ?
		
		  is this block fully invalidated?
	
	  We release buffers only if the entire page is being invalidated.
	  The get_block cached value has been unconditionally invalidated,
	  so real IO is not possible anymore.
  We attach and possibly dirty the buffers atomically wrt
  __set_page_dirty_buffers() via private_lock.  try_to_free_buffers
  is already excluded via the page lock.
  clean_bdev_aliases: clean a range of buffers in block device
  @bdev: Block device to clean buffers in
  @block: Start of a range of blocks to clean
  @len: Number of blocks to clean
  We are taking a range of blocks for data and we don't want writeback of any
  buffer-cache aliases starting from return from this function and until the
  moment when something will explicitly mark the buffer dirty (hopefully that
  will not happen until we will free that block ;-) We don't even need to mark
  it not-uptodate - nobody can expect anything from a newly allocated buffer
  anyway. We used to use unmap_buffer() for such invalidation, but that was
  wrong. We definitely don't want to mark the alias unmapped, for example - it
  would confuse anyone who might pick it with bread() afterwards...
  Also..  Note that bforget() doesn't lock the buffer.  So there can be
  writeout IO going on against recently-freed buffers.  We don't wait on that
  IO in bforget() - it's more efficient to wait on the IO only if we really
  need to.  That happens here.
			
			  We use page lock instead of bd_mapping->private_lock
			  to pin buffers here since we can afford to sleep and
			  it scales better than a global spinlock lock.
 Recheck when the page is locked which pins bhs 
 End of range already reached? 
  Size is a power-of-two in the range 512..PAGE_SIZE,
  and the case we care about most is PAGE_SIZE.
  So this could possibly be written with those
  constraints in mind (relevant mostly if some
  architecture has a slow bit-scan instruction)
  NOTE! All mappeduptodate combinations are valid:
 	Mapped	Uptodate	Meaning
 	No	No		"unknown" - must do get_block()
 	No	Yes		"hole" - zero-filled
 	Yes	No		"allocated" - allocated on disk, not read in
 	Yes	Yes		"valid" - allocated and up-to-date in memory.
  "Dirty" is valid only with the last case (mapped+uptodate).
  While block_write_full_page is writing back the dirty buffers under
  the page lock, whoever dirtied the buffers may decide to clean them
  again at any time.  We handle that by only looking at the buffer
  state inside lock_buffer().
  If block_write_full_page() is called for regular writeback
  (wbc->sync_mode == WB_SYNC_NONE) then it will redirty a page which has a
  locked buffer.   This only can happen if someone has written the buffer
  directly, with submit_bh().  At the address_space level PageWriteback
  prevents this contention from occurring.
  If block_write_full_page() is called with wbc->sync_mode ==
  WB_SYNC_ALL, the writes are posted using REQ_SYNC; this
  causes the writes to be flagged as synchronous writes.
	
	  Be very careful.  We have no exclusion from __set_page_dirty_buffers
	  here, and the (potentially unmapped) buffers may become dirty at
	  any time.  If a buffer becomes dirty here after we've inspected it
	  then we just miss that fact, and the page stays dirty.
	 
	  Buffers outside i_size may be dirtied by __set_page_dirty_buffers;
	  handle that here by just cleaning them.
	
	  Get all the dirty buffers mapped to disk addresses and
	  handle any aliases from the underlying blockdev's mapping.
			
			  mapped buffers outside i_size will occur, because
			  this page can be outside i_size when there is a
			  truncate in progress.
			
			  The buffer was zeroed by block_write_full_page()
 blockdev mappings never come here 
		
		  If it's a fully non-blocking write attempt and we cannot
		  lock the buffer then redirty the page.  Note that this can
		  potentially cause a busy-wait loop from writeback threads
		  and kswapd activity, but those code paths have their own
		  higher-level throttling.
	
	  The page and its buffers are protected by PageWriteback(), so we can
	  drop the bh refcounts early.
		
		  The page was marked dirty, but the buffers were
		  clean.  Someone wrote them back by hand with
		  ll_rw_blocksubmit_bh.  A rare case.
		
		  The page and buffer_heads can be released at any time from
		  here on.
	
	  ENOSPC, or some other error.  We may already have added some
	  blocks to the file, so we need to write these out to avoid
	  exposing stale data.
	  The page is currently locked and not marked for writeback
 Recovery: lock and submit the mapped buffers 
			
			  The buffer may have been set dirty during
			  attachment to a dirty page.
  If a page has any new buffers, zero them out here, and mark them uptodate
  and dirty so they'll be written out (in order to prevent uninitialised
  block data from leaking). And clear the new bit.
	
	  Block points to offset in file we need to map, iomap contains
	  the offset at which the map starts. If the map ends before the
	  current block, then do not map the buffer and let the caller
	  handle it.
		
		  If the buffer is not up to date or beyond the current EOF,
		  we need to mark it as new to ensure sub-block zeroing is
		  executed if necessary.
		
		  For unwritten regions, we always need to ensure that regions
		  in the block we are not writing to are zeroed. Mark the
		  buffer as new to ensure this.
	
	  If we issued read requests - let them complete.
	
	  If this is a partial write which happened to make all buffers
	  uptodate then we can optimize away a bogus readpage() for
	  the next read(). Here we 'discover' whether the page went
	  uptodate as a result of this (potentially partial) write.
  block_write_begin takes care of the basic task of block allocation and
  bringing partial write blocks uptodate first.
  The filesystem needs to handle block truncation upon failure.
		
		  The buffers that were written will now be uptodate, so we
		  don't have to worry about a readpage reading them and
		  overwriting a partial write. However if we have encountered
		  a short write and only partially written into a buffer, it
		  will not be marked uptodate, so a readpage might come in and
		  destroy our partial write.
		 
		  Do the simplest thing, and just treat any short write to a
		  non uptodate page as a zero-length write, and force the
		  caller to redo the whole thing.
 This could be a short (even 0-length) commit 
	
	  No need to use i_size_read() here, the i_size cannot change under us
	  because we hold i_rwsem.
	 
	  But it's important to update i_size while still holding page lock:
	  page writeout could otherwise come in and zero beyond i_size.
	
	  Don't mark the inode dirty under page lock. First, it unnecessarily
	  makes the holding time of page lock longer. Second, it forces lock
	  ordering of page lock and transaction start for journaling
	  filesystems.
  block_is_partially_uptodate checks whether buffers within a page are
  uptodate or not.
  Returns true if all buffers which correspond to a file portion
  we want to read are uptodate.
  Generic "read page" function for block devices that have the normal
  get_block functionality. This is most of the block device filesystems.
  Reads the page asynchronously --- the unlock_buffer() and
  setclear_buffer_uptodate() functions propagate buffer state into the
  page struct once IO has completed.
			
			  get_block() might have updated the buffer
			  synchronously
		
		  All buffers are uptodate - we can set the page uptodate
		  as well. But not if get_block() returned an error.
 Stage two: lock the buffers 
	
	  Stage 3: start the IO.  Check for uptodateness
	  inside the buffer lock in case another process reading
	  the underlying blockdev brought it uptodate (the sct fix).
 utility function for filesystems that need to do work on expanding
  truncates.  Uses filesystem pagecache writes to allow the filesystem to
  deal with the hole.  
 page covers the boundary, find the boundary offset 
 if we will expand the thing last block will be filled 
  For moronic filesystems that do not allow holes in file.
  We may have to extend the file.
  block_page_mkwrite() is not allowed to change the file size as it gets
  called from a page fault handler when a page is first dirtied. Hence we must
  be careful to check for EOF conditions here. We set the page up correctly
  for a written page which means we get ENOSPC checking when writing into
  holes and correct delalloc and unwritten extent mapping on filesystems that
  support these features.
  We are not allowed to take the i_mutex here so we have to play games to
  protect against truncate races as the page could now be beyond EOF.  Because
  truncate writes the inode size before removing pages, once we have the
  page lock we can determine safely if the page is beyond EOF. If it is not
  beyond EOF, then the page is guaranteed safe against truncation until we
  unlock the page.
  Direct callers of this function should protect against filesystem freezing
  using sb_start_pagefault() - sb_end_pagefault() functions.
 We overload EFAULT to mean page got truncated 
 page is wholly or partially inside EOF 
  nobh_write_begin()'s prereads are special: the buffer_heads are freed
  immediately, while under the page lock.  So it needs a special end_io
  handler which does not touch the bh after unlocking it.
  Attach the singly-linked list of buffers created by nobh_write_begin, to
  the page (converting it to circular linked list and taking care of page
  dirty races).
  On entry, the page is fully not uptodate.
  On exit the page is fully uptodate in the areas outside (from,to)
  The filesystem needs to handle block truncation upon failure.
	
	  Allocate buffers so that we can keep track of state, and potentially
	  attach them to the page if an error occurs. In the common case of
	  no error, they will just be freed again without ever being attached
	  to the page (which is all OK, because we're under the page lock).
	 
	  Be careful: the buffer linked list is a NULL terminated one, rather
	  than the circular one we're used to.
	
	  We loop across all blocks in the page, whether or not they are
	  part of the affected region.  This is so we can discover if the
	  page is fully mapped-to-disk.
 reiserfs does this 
		
		  The page is locked, so these buffers are protected from
		  any VM or truncate activity.  Hence we don't need to care
		  for the buffer_head refcounts.
 to be released by nobh_write_end 
	
	  Error recovery is a bit difficult. We need to zero out blocks that
	  were newly allocated, and dirty them to ensure they get written out.
	  Buffers need to be attached to the page at this point, otherwise
	  the handling of potential IO errors during writeout would be hard
	  (could try doing synchronous writeout, but what if that fails too?)
  nobh_writepage() - based on block_full_write_page() except
  that it tries to operate without attaching bufferheads to
  the page.
 Is the page fully inside i_size? 
 Is the page fully outside i_size? (truncate in progress) 
 don't care 
	
	  The page straddles i_size.  It must be zeroed out on each and every
	  writepage invocation because it may be mmapped.  "A file is mapped
	  in multiples of the page size.  For a file that is not a multiple of
	  the  page size, the remaining memory is zeroed when mapped, and
	  writes to that region are not written out to the file."
 Block boundary? Nothing to do 
 Find the buffer that contains "offset" 
 unmapped? It's a hole - nothing to do 
 Ok, it's mapped. Make sure it's up-to-date 
 Block boundary? Nothing to do 
 Find the buffer that contains "offset" 
 unmapped? It's a hole - nothing to do 
 Ok, it's mapped. Make sure it's up-to-date 
 Uhhuh. Read error. Complain and punt. 
  The generic ->writepage function for buffer-backed address_spaces
 Is the page fully inside i_size? 
 Is the page fully outside i_size? (truncate in progress) 
 don't care 
	
	  The page straddles i_size.  It must be zeroed out on each and every
	  writepage invocation because it may be mmapped.  "A file is mapped
	  in multiples of the page size.  For a file that is not a multiple of
	  the  page size, the remaining memory is zeroed when mapped, and
	  writes to that region are not written out to the file."
	
	  Only clear out a write error when rewriting
 Take care of bh's that straddle the end of the device 
  ll_rw_block: low-level access to block devices (DEPRECATED)
  @op: whether to %READ or %WRITE
  @op_flags: req_flag_bits
  @nr: number of &struct buffer_heads in the array
  @bhs: array of pointers to &struct buffer_head
  ll_rw_block() takes an array of pointers to &struct buffer_heads, and
  requests an IO operation on them, either a %REQ_OP_READ or a %REQ_OP_WRITE.
  @op_flags contains flags modifying the detailed IO behavior, most notably
  %REQ_RAHEAD.
  This function drops any buffer that it cannot get a lock on (with the
  BH_Lock state bit), any buffer that appears to be clean when doing a write
  request, and any buffer that appears to be up-to-date when doing read
  request.  Further it marks as clean buffers that are processed for
  writing (the buffer cache won't assume that they are actually clean
  until the buffer gets unlocked).
  ll_rw_block sets b_end_io to simple completion handler that marks
  the buffer up-to-date (if appropriate), unlocks the buffer and wakes
  any waiters. 
  All of the buffers must be for the same device, and must also be a
  multiple of the current approved size for the device.
  For a data-integrity writeout, we need to wait upon any in-progress IO
  and then start new IO and then wait upon it.  The caller must have a ref on
  the buffer_head.
		
		  The bh should be mapped, but it might not be if the
		  device was hot-removed. Not much we can do but fail the IO.
  try_to_free_buffers() checks if all the buffers on this particular page
  are unused, and releases them if so.
  Exclusion against try_to_free_buffers may be obtained by either
  locking the page or by holding its mapping's private_lock.
  If the page is dirty but all the buffers are clean then we need to
  be sure to mark the page clean as well.  This is because the page
  may be against a block device, and a later reattachment of buffers
  to a dirty page will set all buffers dirty.  Which would corrupt
  filesystem data on the same device.
  The same applies to regular filesystem pages: if all the buffers are
  clean then we set the page clean and proceed.  To do that, we require
  total exclusion from __set_page_dirty_buffers().  That is obtained with
  private_lock.
  try_to_free_buffers() is non-blocking.
 can this still happen? 
	
	  If the filesystem writes its buffers by hand (eg ext3)
	  then we can have clean buffers against a dirty page.  We
	  clean the page here; otherwise the VM will never notice
	  that the filesystem did any IO at all.
	 
	  Also, during truncate, discard_buffer will have marked all
	  the page's buffers clean.  We discover that here and clean
	  the page also.
	 
	  private_lock must be held over this entire operation in order
	  to synchronise against __set_page_dirty_buffers and prevent the
	  dirty bit from being lost.
  Buffer-head allocation
  Once the number of bh's in the machine exceeds this level, we start
  stripping them in writeback.
 Number of live bh's 
 Limit cacheline bouncing 
  bh_uptodate_or_lock - Test whether the buffer is uptodate
  @bh: struct buffer_head
  Return true if the buffer is up-to-date and false,
  with the buffer locked, if not.
  bh_submit_read - Submit a locked buffer for reading
  @bh: struct buffer_head
  Returns zero on success and -EIO on error.
	
	  Limit the bh occupancy to 10% of ZONE_NORMAL
 SPDX-License-Identifier: GPL-2.0-only
   linuxfsbinfmt_script.c
   Copyright (C) 1996  Martin von Lwis
   original #!-checking implemented by tytso.
 Not ours to exec if we don't start with "#!". 
	
	  This section handles parsing the #! line into separate
	  interpreter path and argument strings. We must be careful
	  because bprm->buf is not yet guaranteed to be NUL-terminated
	  (though the buffer will have trailing NUL padding when the
	  file size was smaller than the buffer size).
	 
	  We do not want to exec a truncated interpreter path, so either
	  we find a newline (which indicates nothing is truncated), or
	  we find a spacetabNUL after the interpreter path (which
	  itself may be preceded by spacestabs). Truncating the
	  arguments is fine: the interpreter can re-read the script to
	  parse them on its own.
 Entire buf is spacestabs 
		
		  If there is no later spacetabNUL we must assume the
		  interpreter path is truncated.
 Trim any trailing spacestabs from i_end 
 Skip over leading spacestabs 
 No interpreter name found 
 Is there an optional argument? 
	
	  If the script filename will be inaccessible after exec, typically
	  because it is a "devfd<fd>.." path against an O_CLOEXEC fd, give
	  up now (on the assumption that the interpreter will want to load
	  this file).
	
	  OK, we've parsed out the interpreter name and
	  (optional) argument.
	  Splice in (1) the interpreter's name for argv[0]
	            (2) (optional) argument to interpreter
	            (3) filename of shell script (replace argv[0])
	 
	  This is done in reverse order, because of how the
	  user environment and arguments are stored.
	
	  OK, now restart the process with the interpreter's dentry.
 SPDX-License-Identifier: GPL-2.0-only
  fsdcache.c
  Complete reimplementation
  (C) 1997 Thomas Schoebel-Theuer,
  with heavy changes by Linus Torvalds
  Notes on the allocation strategy:
  The dcache is a master of the icache - whenever a dcache entry
  exists, the inode will always exist. "iput()" is done either when
  the dcache entry is deleted or garbage collected.
  Usage:
  dcache->d_inode->i_lock protects:
    - i_dentry, d_u.d_alias, d_inode of aliases
  dcache_hash_bucket lock protects:
    - the dcache hash table
  s_roots bl list spinlock protects:
    - the s_roots list (see __d_drop)
  dentry->d_sb->s_dentry_lru_lock protects:
    - the dcache lru lists and counters
  d_lock protects:
    - d_flags
    - d_name
    - d_lru
    - d_count
    - d_unhashed()
    - d_parent and d_subdirs
    - childrens' d_child and d_parent
    - d_u.d_alias, d_inode
  Ordering:
  dentry->d_inode->i_lock
    dentry->d_lock
      dentry->d_sb->s_dentry_lru_lock
      dcache_hash_bucket lock
      s_roots lock
  If there is an ancestor relationship:
  dentry->d_parent->...->d_parent->d_lock
    ...
      dentry->d_parent->d_lock
        dentry->d_lock
  If no ancestor relationship:
  arbitrary, since it's serialized on rename_lock
  This is the single most critical data structure when it comes
  to the dcache: the hashtable for lookups. Somebody should try
  to make this good - I've just made it work.
  This hash-function tries to avoid losing too many bits of hash
  information, yet avoid using a prime hash-size or similar.
 Statistics gathering. 
  Here we resort to our own counters instead of using generic per-cpu counters
  for consistency with what the vfs inode code does. We are expected to harvest
  better code and performance by having our own specialized counters.
  Please note that the loop is done over all possible CPUs, not over all online
  CPUs. The reason for this is that we don't want to play games with CPUs going
  on and off. If one of them goes off, we will just keep their counters.
  glommer: See cffbc8a for details, and if you ever intend to change this,
  please update all vfs counters to match.
  Compare 2 name strings, return 0 if they match, otherwise non-zero.
  The strings are both count bytes long, and count is non-zero.
  NOTE! 'cs' and 'scount' come from a dentry, so it has a
  aligned allocation for this particular component. We don't
  strictly need the load_unaligned_zeropad() safety, but it
  doesn't hurt either.
  In contrast, 'ct' and 'tcount' can be from a pathname, and do
  need the careful unaligned handling.
	
	  Be careful about RCU walk racing with rename:
	  use 'READ_ONCE' to fetch the name pointer.
	 
	  NOTE! Even if a rename will mean that the length
	  was not loaded atomically, we don't care. The
	  RCU walk will check the sequence count eventually,
	  and catch it. And we won't overrun the buffer,
	  because we're reading the name pointer atomically,
	  and a dentry name is guaranteed to be properly
	  terminated with a NUL byte.
	 
	  End result: even if 'len' is wrong, we'll exit
	  early because the data cannot match (there can
	  be no NUL in the cttcount data)
 if dentry was never visible to RCU, immediate free is OK 
  Release the dentry's inode, using the filesystem
  d_iput() operation if defined.
  The DCACHE_LRU_LIST bit is set whenever the 'd_lru' entry
  is in use - which includes both the "real" per-superblock
  LRU list _and_ the DCACHE_SHRINK_LIST use.
  The DCACHE_SHRINK_LIST bit is set whenever the dentry is
  on the shrink list (ie not on the superblock LRU list).
  The per-cpu "nr_dentry_unused" counters are updated with
  the DCACHE_LRU_LIST bit.
  The per-cpu "nr_dentry_negative" counters are only updated
  when deleted from or added to the per-superblock LRU list, not
  fromto the shrink list. That is to avoid an unneeded decinc
  pair when moving from LRU to shrink list in select_collect().
  These helper functions make sure we always follow the
  rules. d_lock must be held by the caller.
  These can only be called under the global LRU lock, ie during the
  callback for freeing the LRU list. "isolate" removes it from the
  LRU lists entirely, while shrink_move moves it to the indicated
  private list.
	
	  Hashed dentries are normally on the dentry hashtable,
	  with the exception of those newly allocated by
	  d_obtain_root, which are always IS_ROOT:
  d_drop - drop a dentry
  @dentry: dentry to drop
  d_drop() unhashes the entry from the parent dentry hashes, so that it won't
  be found through a VFS lookup any more. Note that this is different from
  deleting the dentry - d_delete will try to mark the dentry negative if
  possible, giving a successful _negative_ lookup, while d_drop will
  just make the cache lookup fail.
  d_drop() is used mainly for stuff that wants to invalidate a dentry for some
  reason (NFS timeouts or autofs deletes).
  __d_drop requires dentry->d_lock
  ___d_drop doesn't mark dentry as "unhashed"
  (dentry->d_hash.pprev will be LIST_POISON2, not NULL).
	
	  Inform d_walk() and shrink_dentry_list() that we are no longer
	  attached to the dentry tree
	
	  Cursors can move around the list of children.  While we'd been
	  a normal list member, it didn't matter - ->d_child.next would've
	  been updated.  However, from now on it won't be and for the
	  things like d_walk() it might end up with a nasty surprise.
	  Normally d_walk() doesn't care about cursors moving around -
	  ->d_lock on parent prevents that and since a cursor has no children
	  of its own, we get through it without ever unlocking the parent.
	  There is one exception, though - if we ascend from a child that
	  gets killed as soon as we unlock it, the next sibling is found
	  using the value left in its ->d_child.next.  And if _that_
	  pointed to a cursor, and cursor got moved (e.g. by lseek())
	  before d_walk() regains parent->d_lock, we'll end up skipping
	  everything the cursor had been moved past.
	 
	  Solution: make sure that the pointer left behind in ->d_child.next
	  points to something that won't be moving around.  I.e. skip the
	  cursors.
	
	  The dentry is now unrecoverably dead to the world.
	
	  inform the fs via d_prune that this dentry is about to be
	  unhashed and destroyed.
 if it was on the hash then remove it 
	
	  We can't blindly lock dentry until we are sure
	  that we won't violate the locking order.
	  Any changes of dentry->d_parent must have
	  been done with parent->d_lock held, so
	  spin_lock() above is enough of a barrier
	  for checking if it's still our child.
 Unreachable? Get rid of it 
 retain; LRU fodder 
  Finish off a dentry we've decided to kill.
  dentry->d_lock must be held, returns with it unlocked.
  Returns dentry requiring refcount drop, or NULL if we're done.
 negative that became positive 
 we are keeping it, after all 
  Try to do a lockless dput(), and return whether that was successful.
  If unsuccessful, we return false, having already taken the dentry lock.
  The caller needs to hold the RCU read lock, so that the dentry is
  guaranteed to stay around even if the refcount goes down to zero!
	
	  If we have a d_op->d_delete() operation, we sould not
	  let the dentry count go to zero, so use "put_or_lock".
	
	  .. otherwise, we can try to just decrement the
	  lockref optimistically.
	
	  If the lockref_put_return() failed due to the lock being held
	  by somebody else, the fast path has failed. We will need to
	  get the lock, and then check the count again.
	
	  If we weren't the last ref, we're done.
	
	  Careful, careful. The reference count went down
	  to zero, but we don't hold the dentry lock, so
	  somebody else could get it again, and do another
	  dput(), and we need to not race with that.
	 
	  However, there is a very special and common case
	  where we don't care, because there is nothing to
	  do: the dentry is still hashed, it does not have
	  a 'delete' op, and it's referenced and already on
	  the LRU list.
	 
	  NOTE! Since we aren't locked, these values are
	  not "stable". However, it is sufficient that at
	  some point after we dropped the reference the
	  dentry was hashed and the flags had the proper
	  value. Other dentry users may have re-gotten
	  a reference to the dentry and change that, but
	  our work is done - we can leave the dentry
	  around with a zero refcount.
	 
	  Nevertheless, there are two cases that we should kill
	  the dentry anyway.
	  1. free disconnected dentries as soon as their refcount
	     reached zero.
	  2. free dentries if they should not be cached.
 Nothing to do? Dropping the reference was all we needed? 
	
	  Not the fast normal case? Get the lock. We've already decremented
	  the refcount, but we'll need to re-check the situation after
	  getting the lock.
	
	  Did somebody else grab a reference to it in the meantime, and
	  we're no longer the last user after all? Alternatively, somebody
	  else could have killed it and marked it dead. Either way, we
	  don't need to do anything else.
	
	  Re-get the reference we optimistically dropped. We hold the
	  lock, and we just tested that it was zero, so we can just
	  set it to 1.
  This is dput
  This is complicated by the fact that we do not want to put
  dentries that are no longer on any hash chain on the unused
  list: we'd much rather just get rid of them immediately.
  However, that implies that we have to traverse the dentry
  tree upwards to the parents which might _also_ now be
  scheduled for deletion (it may have been only waiting for
  its last child to go away).
  This tail recursion is done by hand as we don't want to depend
  on the compiler to always get this right (gcc generally doesn't).
  Real recursion would eat up our stack space.
  dput - release a dentry
  @dentry: dentry to release 
  Release a dentry. This will drop the usage count and if appropriate
  call the dentry unlink method as well as removing it from the queues and
  releasing its resources. If the parent dentries were scheduled for release
  they too may now get deleted.
 Slow case: now with the dentry lock held 
 let the owner of the list it's on deal with it 
 This must be called with d_lock held 
	
	  Do optimistic parent lookup without any
	  locking.
	
	  Don't need rcu_dereference because we re-check it was correct under
	  the lock.
  d_find_any_alias - find any alias for a given inode
  @inode: inode to find an alias for
  If any aliases exist for the given inode, take and return a
  reference for one of them.  If no aliases exist, return %NULL.
  d_find_alias - grab a hashed alias of inode
  @inode: inode in question
  If inode has a hashed alias, or is a directory and has any alias,
  acquire the reference to alias and return it. Otherwise return NULL.
  Notice that if inode is a directory there can be only one alias and
  it can be unhashed only if it has no children, or if it is the root
  of a filesystem, or if the directory was renamed and d_revalidate
  was the first vfs operation to notice.
  If the inode has an IS_ROOT, DCACHE_DISCONNECTED alias, then prefer
  any other hashed alias over that one.
   Caller MUST be holding rcu_read_lock() and be guaranteed
   that inode won't get freed until rcu_read_unlock().
 ->i_dentry and ->i_rcu are colocated, but the latter won't be
 used without having I_FREEING set, which means no aliases left
 	Try to kill dentries associated with this inode.
  WARNING: you must own a reference to inode.
  Lock a dentry from shrink list.
  Called under rcu_read_lock() and dentry->d_lock; the former
  guarantees that nothing we access will be freed under us.
  Note that dentry is not protected from concurrent dentry_kill(),
  d_delete(), etc.
  Return false if dentry has been disrupted or grabbed, leaving
  the caller to kick it off-list.  Otherwise, return true and have
  that dentry's inode and parent both locked.
 changed inode means that somebody had grabbed it 
	
	  we are inverting the lru lockdentry->d_lock here,
	  so use a trylock. If we fail to get the lock, just skip
	  it
	
	  Referenced dentries are still in use. If they have active
	  counts, just remove them from the LRU. Otherwise give them
	  another pass through the LRU.
		
		  The list move itself will be made by the common LRU code. At
		  this point, we've dropped the dentry->d_lock but keep the
		  lru lock. This is safe to do, since every list movement is
		  protected by the lru lock even if both locks are held.
		 
		  This is guaranteed by the fact that all LRU management
		  functions are intermediated by the LRU API calls like
		  list_lru_add and list_lru_del. List movement in this file
		  only ever occur through this functions or through callbacks
		  like this one, that are called from the LRU API.
		 
		  The only exceptions to this are functions like
		  shrink_dentry_list, and code that first checks for the
		  DCACHE_SHRINK_LIST flag.  Those are guaranteed to be
		  operating only with stack provided lists after they are
		  properly isolated from the main list.  It is thus, always a
		  local access.
  prune_dcache_sb - shrink the dcache
  @sb: superblock
  @sc: shrink control, passed to list_lru_shrink_walk()
  Attempt to shrink the superblock dcache LRU by @sc->nr_to_scan entries. This
  is done when we need more memory and called from the superblock shrinker
  function.
  This function may fail to free any resources if all the dentries are in
  use.
	
	  we are inverting the lru lockdentry->d_lock here,
	  so use a trylock. If we fail to get the lock, just skip
	  it
  shrink_dcache_sb - shrink dcache for a superblock
  @sb: superblock
  Shrink the dcache for the specified super block. This is used to free
  the dcache before unmounting a file system.
  enum d_walk_ret - action to talke during tree walk
  @D_WALK_CONTINUE:	contrinue walk
  @D_WALK_QUIT:	quit walk
  @D_WALK_NORETRY:	quit when retry is needed
  @D_WALK_SKIP:	skip this dentry and its children
  d_walk - walk the dentry tree
  @parent:	start of walk
  @data:	data passed to @enter() and @finish()
  @enter:	callback when first entering the dentry
  The @enter() callbacks are called with d_lock held.
	
	  All done at this level ... ascend and resume the search.
 might go back up the wrong parent if we have had a rename. 
 go into the first sibling still alive 
  path_has_submounts - check for mounts over a dentry in the
                       current namespace.
  @parent: path to check.
  Return true if the parent or its subdirectories contain
  a mount point in the current namespace.
  Called by mount code to set a mountpoint and check if the mountpoint is
  reachable (e.g. NFS can unhash a directory dentry and then the complete
  subtree can become unreachable).
  Only one of d_invalidate() and d_set_mounted() must succeed.  For
  this reason take rename_lock and d_lock on dentry and ancestors.
 Need exclusion wrt. d_invalidate() 
  Search the dentry child list of the specified parent,
  and move any unused dentries to the end of the unused
  list for prune_dcache(). We descend to the next level
  whenever the d_subdirs list is non-empty and continue
  searching.
  It returns zero iff there are no unused children,
  otherwise  it returns the number of children moved to
  the end of the unused list. This may not be the total
  number of unused children, because select_parent can
  drop the lock and return early due to latency
  constraints.
	
	  We can return to the caller if we have found some (this
	  ensures forward progress). We'll be coming back to find
	  the rest.
	
	  We can return to the caller if we have found some (this
	  ensures forward progress). We'll be coming back to find
	  the rest.
  shrink_dcache_parent - prune dcache
  @parent: parent of entries to prune
  Prune the dcache to remove unused children of the parent dentry.
 it has busy descendents; complain about those instead 
 root with refcount 1 is fine 
  destroy the dentries attached to a superblock on unmounting
  d_invalidate - detach submounts, prune dcache, and drop
  @dentry: dentry to invalidate (aka detach, prune and drop)
 Negative dentries can be dropped without further checks 
  __d_alloc	-	allocate a dcache entry
  @sb: filesystem it will belong to
  @name: qstr of the name
  Allocates a dentry. It returns %NULL if there is insufficient memory
  available. On a success the dentry is returned. The name passed in is
  copied and the copy passed in may be reused after this call.
	
	  We guarantee that the inline name is always NUL-terminated.
	  This way the memcpy() done by the name switching in rename
	  will still always have a NUL at the end, even if we might
	  be overwriting an internal NUL character
 Make sure we always see the terminating NUL character 
 ^^^ 
  d_alloc	-	allocate a dcache entry
  @parent: parent of entry to allocate
  @name: qstr of the name
  Allocates a dentry. It returns %NULL if there is insufficient memory
  available. On a success the dentry is returned. The name passed in is
  copied and the copy passed in may be reused after this call.
	
	  don't need child lock because it is not subject
	  to concurrency here
  d_alloc_pseudo - allocate a dentry (for lookup-less filesystems)
  @sb: the superblock
  @name: qstr of the name
  For a filesystem that just pins its dentries in memory and never
  performs lookups at all, return an unhashed IS_ROOT dentry.
  This is used for pipes, sockets et.al. - the stuff that should
  never be anyone's children or parents.  Unlike all other
  dentries, these will not have RCU delay between dropping the
  last reference and freeing them.
  The only user is alloc_file_pseudo() and that's what should
  be considered a public interface.  Don't use directly.
  d_set_fallthru - Mark a dentry as falling through to a lower layer
  @dentry - The dentry to mark
  Mark a dentry as falling through to the lower layer (as set with
  d_pin_lower()).  This flag may be recorded on the medium.
	
	  Decrement negative dentry count if it was in the LRU list.
  d_instantiate - fill in inode information for a dentry
  @entry: dentry to complete
  @inode: inode to attach to this dentry
  Fill in inode information in the entry.
  This turns negative dentries into productive full members
  of society.
  NOTE! This assumes that the inode count has been incremented
  (or otherwise set) by the caller to indicate that it is now
  in use by the dcache.
  This should be equivalent to d_instantiate() + unlock_new_inode(),
  with lockdep-related part of unlock_new_inode() done before
  anything else.  Use that instead of open-coding d_instantiate()
  unlock_new_inode() combinations.
 attach a disconnected dentry 
  d_obtain_alias - find or allocate a DISCONNECTED dentry for a given inode
  @inode: inode to allocate the dentry for
  Obtain a dentry for an inode resulting from NFS filehandle conversion or
  similar open by handle operations.  The returned dentry may be anonymous,
  or may have a full name (if the inode was already in the cache).
  When called on a directory inode, we must ensure that the inode only ever
  has one dentry.  If a dentry is found, that is returned instead of
  allocating a new one.
  On successful return, the reference to the inode has been transferred
  to the dentry.  In case of an error the reference on the inode is released.
  To make it easier to use in export operations a %NULL or IS_ERR inode may
  be passed in and the error will be propagated to the return value,
  with a %NULL @inode replaced by ERR_PTR(-ESTALE).
  d_obtain_root - find or allocate a dentry for a given inode
  @inode: inode to allocate the dentry for
  Obtain an IS_ROOT dentry for the root of a filesystem.
  We must ensure that directory inodes only ever have one dentry.  If a
  dentry is found, that is returned instead of allocating a new one.
  On successful return, the reference to the inode has been transferred
  to the dentry.  In case of an error the reference on the inode is
  released.  A %NULL or IS_ERR inode may be passed in and will be the
  error will be propagate to the return value, with a %NULL @inode
  replaced by ERR_PTR(-ESTALE).
  d_add_ci - lookup or allocate new dentry with case-exact name
  @inode:  the inode case-insensitive lookup has found
  @dentry: the negative dentry that was passed to the parent's lookup func
  @name:   the case-exact name to be associated with the returned dentry
  This is to avoid filling the dcache with case-insensitive names to the
  same inode, only the actual correct case is stored in the dcache for
  case-insensitive filesystems.
  For a case-insensitive lookup match and if the case-exact dentry
  already exists in the dcache, use it and return it.
  If no entry exists with the exact case name, allocate new dentry with
  the exact case, and return the spliced entry.
	
	  First check if a dentry matching the name already exists,
	  if not go ahead and create it now.
  __d_lookup_rcu - search for a dentry (racy, store-free)
  @parent: parent dentry
  @name: qstr of name we wish to find
  @seqp: returns d_seq value at the point where the dentry was found
  Returns: dentry, or NULL
  __d_lookup_rcu is the dcache lookup function for rcu-walk name
  resolution (store-free path walking) design described in
  Documentationfilesystemspath-lookup.txt.
  This is not to be used outside core vfs.
  __d_lookup_rcu must only be used in rcu-walk mode, ie. with vfsmount lock
  held, and rcu_read_lock held. The returned dentry must not be stored into
  without taking d_lock and checking d_seq sequence count against @seq
  returned here.
  A refcount may be taken on the found dentry with the d_rcu_to_refcount
  function.
  Alternatively, __d_lookup_rcu may be called again to look up the child of
  the returned dentry, so long as its parent's seqlock is checked after the
  child is looked up. Thus, an interlocking stepping of sequence lock checks
  is formed, giving integrity down the path walk.
  NOTE! The caller has to check the resulting dentry against the sequence
  number we've returned before using any of the resulting dentry state!
	
	  Note: There is significant duplication with __d_lookup_rcu which is
	  required to prevent single threaded performance regressions
	  especially on architectures where smp_rmb (in seqcounts) are costly.
	  Keep the two functions in sync.
	
	  The hash list is protected using RCU.
	 
	  Carefully use d_seq when comparing a candidate dentry, to avoid
	  races with d_move().
	 
	  It is possible that concurrent renames can mess up our list
	  walk here and result in missing our dentry, resulting in the
	  false-negative result. d_lookup() protects against concurrent
	  renames using rename_lock seqlock.
	 
	  See Documentationfilesystemspath-lookup.txt for more details.
		
		  The dentry sequence count protects us from concurrent
		  renames, and thus protects parent and name fields.
		 
		  The caller must perform a seqcount check in order
		  to do anything useful with the returned dentry.
		 
		  NOTE! We do a "raw" seqcount_begin here. That means that
		  we don't wait for the sequence count to stabilize if it
		  is in the middle of a sequence change. If we do the slow
		  dentry compare, we will do seqretries until it is stable,
		  and if we end up with a successful lookup, we actually
		  want to exit RCU lookup anyway.
		 
		  Note that raw_seqcount_begin still does smp_rmb(), so
		  we are still guaranteed NUL-termination of ->d_name.name.
 we want a consistent (name,len) pair 
  d_lookup - search for a dentry
  @parent: parent dentry
  @name: qstr of name we wish to find
  Returns: dentry, or NULL
  d_lookup searches the children of the parent dentry for the name in
  question. If the dentry is found its reference count is incremented and the
  dentry is returned. The caller must use dput to free the entry when it has
  finished using it. %NULL is returned if the dentry does not exist.
  __d_lookup - search for a dentry (racy)
  @parent: parent dentry
  @name: qstr of name we wish to find
  Returns: dentry, or NULL
  __d_lookup is like d_lookup, however it may (rarely) return a
  false-negative result due to unrelated rename activity.
  __d_lookup is slightly faster by avoiding rename_lock read seqlock,
  however it must be used carefully, eg. with a following d_lookup in
  the case of failure.
  __d_lookup callers must be commented.
	
	  Note: There is significant duplication with __d_lookup_rcu which is
	  required to prevent single threaded performance regressions
	  especially on architectures where smp_rmb (in seqcounts) are costly.
	  Keep the two functions in sync.
	
	  The hash list is protected using RCU.
	 
	  Take d_lock when comparing a candidate dentry, to avoid races
	  with d_move().
	 
	  It is possible that concurrent renames can mess up our list
	  walk here and result in missing our dentry, resulting in the
	  false-negative result. d_lookup() protects against concurrent
	  renames using rename_lock seqlock.
	 
	  See Documentationfilesystemspath-lookup.txt for more details.
  d_hash_and_lookup - hash the qstr then search for a dentry
  @dir: Directory to search in
  @name: qstr of name we wish to find
  On lookup failure NULL is returned; on bad name - ERR_PTR(-error)
	
	  Check for a fs-specific hash function. Note that we must
	  calculate the standard hash first, as the d_op->d_hash()
	  routine may choose to leave the hash value unchanged.
  When a file is deleted, we have two options:
  - turn this dentry into a negative dentry
  - unhash this dentry and free it.
  Usually, we want to just turn this into
  a negative dentry, but if anybody else is
  currently using the dentry or the inode
  we can't do that and we fall back on removing
  it from the hash queues and waiting for
  it to be deleted later when it has no users
  d_delete - delete a dentry
  @dentry: The dentry to delete
  Turn the dentry into a negative dentry if possible, otherwise
  remove it from the hash queues so it can be deleted later
	
	  Are we the only user?
  d_rehash	- add an entry back to the hash
  @entry: dentry to add to the hash
  Adds a dentry to the hash according to its name.
	
	  No changes for the parent since the beginning of d_lookup().
	  Since all removals from the chain happen with hlist_bl_lock(),
	  any potential in-lookup matches are going to stay here until
	  we unlock the chain.  All fields are stable in everything
	  we encounter.
 now we can try to grab a reference 
		
		  somebody is likely to be still doing lookup for it;
		  wait for them to finish
		
		  it's not in-lookup anymore; in principle we should repeat
		  everything from dcache lookup, but it's likely to be what
		  d_lookup() would've found anyway.  If it is, just return it;
		  otherwise we really have to repeat the whole thing.
 OK, it is a hashed match; return it 
 we can't take ->d_lock here; it's OK, though. 
 inode->i_lock held if inode is non-NULL 
  d_add - add dentry to hash queues
  @entry: dentry to add
  @inode: The inode to attach to this dentry
  This adds the entry to the hash queues and initializes @inode.
  The entry was actually filled in earlier during d_alloc().
  d_exact_alias - find and hash an exact unhashed alias
  @entry: dentry to add
  @inode: The inode to go with this dentry
  If an unhashed dentry with the same nameparent and desired
  inode already exists, hash and return it.  Otherwise, return
  NULL.
  Parent directory should be locked.
		
		  Don't need alias->d_lock here, because aliases with
		  d_parent == entry->d_parent are not subject to name or
		  parent changes, because the parent inode i_mutex is held.
			
			  Both external: swap the pointers
			
			  dentry:internal, target:external.  Steal target's
			  storage and make target internal.
			
			  dentry:external, target:internal.  Give dentry's
			  storage to target and make dentry internal
			
			  Both are internal.
  __d_move - move a dentry
  @dentry: entry to move
  @target: new dentry
  @exchange: exchange the two dentries
  Update the dcache to reflect the move of a file name. Negative
  dcache entries should not be moved in this way. Caller must hold
  rename_lock, the i_mutex of the source and target directories,
  and the sb->s_vfs_rename_mutex if they differ. See lock_rename().
 target is not a descendent of dentry->d_parent 
 unhash both 
 ... and switch them in the tree 
 wasn't IS_ROOT 
  d_move - move a dentry
  @dentry: entry to move
  @target: new dentry
  Update the dcache to reflect the move of a file name. Negative
  dcache entries should not be moved in this way. See the locking
  requirements for __d_move.
  d_exchange - exchange two dentries
  @dentry1: first dentry
  @dentry2: second dentry
  d_ancestor - search for an ancestor
  @p1: ancestor dentry
  @p2: child dentry
  Returns the ancestor dentry of p2 which is a child of p1, if p1 is
  an ancestor of p2, else NULL.
  This helper attempts to cope with remotely renamed directories
  It assumes that the caller is already holding
  dentry->d_parent->d_inode->i_mutex, and rename_lock
  Note: If ever the locking in lock_rename() changes, then please
  remember to update this too...
 If alias and dentry share a parent, then no extra locks required 
 See lock_rename() 
  d_splice_alias - splice a disconnected dentry into the tree if one exists
  @inode:  the inode which may have a disconnected dentry
  @dentry: a negative dentry which we want to point to the inode.
  If inode is a directory and has an IS_ROOT alias, then d_move that in
  place of the given dentry and return it, else simply d_add the inode
  to the dentry and return NULL.
  If a non-IS_ROOT directory is found, the filesystem is corrupt, and
  we should error out: directories can't have multiple aliases.
  This is needed in the lookup routine of any filesystem that is exportable
  (via knfsd) so that we can build dcache paths to directories effectively.
  If a dentry was found and moved, then it is returned.  Otherwise NULL
  is returned.  This matches the expected return value of ->lookup.
  Cluster filesystems may call this function with a negative, hashed dentry.
  In that case, we know that the inode will be a regular file, and also this
  will only occur during atomic_open. So we need to check for the dentry
  being already hashed only in the final case.
 The reference to new ensures it remains an alias 
  Test whether new_dentry is a subdirectory of old_dentry.
  Trivially implemented using the dcache structure
  is_subdir - is new dentry a subdirectory of old_dentry
  @new_dentry: new dentry
  @old_dentry: old dentry
  Returns true if new_dentry is a subdirectory of the parent (at any depth).
  Returns false otherwise.
  Caller must ensure that "new_dentry" is pinned before calling is_subdir()
 for restarting inner loop in case of seq retry 
		
		  Need rcu_readlock to protect against the d_parent trashing
		  due to d_move
	 If hashes are distributed across NUMA nodes, defer
	  hash allocation until vmalloc space is available.
	
	  A constructor could be added for stable state like the lists,
	  but it is probably not worth it because of the cache nature
	  of the dcache.
 Hash may have been set up in dcache_init_early 
 SLAB cache for __getname() consumers 
 SPDX-License-Identifier: GPL-2.0
   linuxfsfcntl.c
   Copyright (C) 1991, 1992  Linus Torvalds
	
	  O_APPEND cannot be cleared if the file is marked as append-only
	  and the file is open for write.
 O_NOATIME can only be set by the owner or superuser 
 required for strict SunOS emulation 
 Pipe packetized mode is controlled by O_DIRECT flag 
	
	  ->fasync() is responsible for setting the FASYNC bit.
 avoid overflow below 
 32-bit arches must use fcntl64() 
 32-bit arches must use fcntl64() 
		
		  XXX If f_owner is a process group, the
		  negative return value will get converted
		  into an error.  Oops.  If we keep the
		  current syscall conventions, the only way
		  to fix this will be in libc.
 arg == 0 restores default behaviour. 
 careful - don't use anywhere else 
  GETLK was successful and we need to return the data, but it needs to fit in
  the compat structure.
  l_start shouldn't be too big, unless the original start + end is greater than
  COMPAT_OFF_T_MAX, in which case the app was asking for trouble, so we return
  -EOVERFLOW in that case.  l_len could be too big, in which case we just
  truncate it, and only allow the app to see that part of the conflicting lock
  that might make sense to it anyway
 Table to convert sigio signal codes into poll band bitmaps 
 POLL_IN 
 POLL_OUT 
 POLL_MSG 
 POLL_ERR 
 POLL_PRI 
 POLL_HUP 
	
	  F_SETSIG can change ->signum lockless in parallel, make
	  sure we read it once and use the same value throughout.
			 Queue a rt signal with the appropriate fd as its
			   value.  We use SI_SIGIO as the source, not 
			   SI_KERNEL, since kernel signals always get 
			   delivered even if we can't queue.  Failure to
			   queue in this case _should_ be reported; we fall
			
			  Posix definies POLL_IN and friends to be signal
			  specific si_codes for SIG_POLL.  Linux extended
			  these si_codes to other signals in a way that is
			  ambiguous if other signals also have signal
			  specific si_codes.  In that case use SI_SIGIO instead
			  to remove the ambiguity.
			 Make sure we are called with one of the POLL_
			   reasons, otherwise we could leak kernel stack into
 fall back on the old plain SIGIO signal 
  Remove a fasync entry. If successfully removed, return
  positive and clear the FASYNC flag. If no entry exists,
  do nothing and return 0.
  NOTE! It is very important that the FASYNC flag always
  match the state "is the filp on a fasync list".
  NOTE! This can be used only for unused fasync entries:
  entries that actually got inserted on the fasync list
  need to be released by rcu - see fasync_remove_entry.
  Insert a new entry into the fasync list.  Return the pointer to the
  old one if we didn't use the new one.
  NOTE! It is very important that the FASYNC flag always
  match the state "is the filp on a fasync list".
  Add a fasync entry. Return negative on error, positive if
  added, and zero if did nothing but change an existing one.
	
	  fasync_insert_entry() returns the old (update) entry if
	  it existed.
	 
	  So free the (unused) new entry and return 0 to let the
	  caller know that we didn't add any new fasync entries.
  fasync_helper() is used by almost all character device drivers
  to set up the fasync queue, and for regular files by the file
  lease code. It returns negative on error, 0 if it did no changes
  and positive if it addeddeleted the entry.
  rcu_read_lock() is held
			 Don't send SIGURG to processes which have not set a
			   queued signum: SIGURG has its own default signalling
	 First a quick test without locking: usually
	  the list is empty.
	
	  Please add new bits here to ensure allocation uniqueness.
	  Exceptions: O_NONBLOCK is a two bit define on parisc; O_NDELAY
	  is defined as O_NONBLOCK on some platforms and not on others.
 for O_RDONLY being 0  !=
 SPDX-License-Identifier: GPL-2.0-only
 	fslibfs.c
 	Library for filesystems writers.
 sync_mapping_buffers 
  Retaining negative dentries for an in-memory filesystem just wastes
  memory and lookup time: arrange for them to be deleted immediately.
  Lookup the data. This is trivial - if the dentry didn't already
  exist, we know it is negative.  Set d_op to delete negative dentries.
 parent is locked at least shared 
  Returns an element of siblings' list.
  We are looking for <count>th positive after <p>; if
  found, dentry is grabbed and returned to caller.
  If no such element exists, NULL is returned.
 we must at least skip cursors, to avoid livelocks
 Relationship between i_mode and the DT_xxx types 
  Directory is locked and all positive dentries in it are safe, since
  for ramfs-type trees they can't go away without unlink() or rmdir(),
  both impossible due to the lock on directory.
 kill and ascend
 update metadata while it's still locked
 avoid lost mounts
 unpin it
	
	  since this is the first inode, make it number 1. New inodes created
	  after this must take care not to collide with it (by passing
	  max_reserved of 1 to iunique).
  Common helper for pseudo-filesystems (sockfs, pipefs, bdev - stuff that
  will never be mountable)
  simple_setattr - setattr for simple filesystem
  @mnt_userns: user namespace of the target mount
  @dentry: dentry
  @iattr: iattr structure
  Returns 0 on success, -error on failure.
  simple_setattr is a simple ->setattr implementation without a proper
  implementation of size changes.
  It can either be used for in-memory filesystems or special files
  on simple regular filesystems.  Anything that needs to change on-disk
  or wire state on size changes needs its own setattr method.
  simple_write_end - .write_end helper for non-block-device FSes
  @file: See .write_end of address_space_operations
  @mapping: 		"
  @pos: 		"
  @len: 		"
  @copied: 		"
  @page: 		"
  @fsdata: 		"
  simple_write_end does the minimum needed for updating a page after writing is
  done. It has the same API signature as the .write_end of
  address_space_operations vector. So it can just be set onto .write_end for
  FSes that don't need any other processing. i_mutex is assumed to be held.
  Block based filesystems should use generic_write_end().
  NOTE: Even though i_size might get updated by this function, mark_inode_dirty
  is not called, so a filesystem that actually does store data in .write_inode
  should extend on what's done here with a call to mark_inode_dirty() in the
  case that i_size has changed.
  Use ONLY with simple_readpage()
 zero the stale part of the page if we did a short copy 
	
	  No need to use i_size_read() here, the i_size
	  cannot change under us because we hold the i_mutex.
  Provides ramfs-style behavior: data in the pagecache, but no writeback.
  the inodes created here are not hashed. If you use iunique to generate
  unique inode values later for this filesystem, then you must take care
  to pass it an appropriate max_reserved value to avoid collisions.
	
	  because the root inode is 1, the files array must not contain an
	  entry at index 1
 warn if it tries to conflict with the root inode 
  simple_read_from_buffer - copy data from the buffer to user space
  @to: the user space buffer to read to
  @count: the maximum number of bytes to read
  @ppos: the current position in the buffer
  @from: the buffer to read from
  @available: the size of the buffer
  The simple_read_from_buffer() function reads up to @count bytes from the
  buffer @from at offset @ppos into the user space address starting at @to.
  On success, the number of bytes read is returned and the offset @ppos is
  advanced by this number, or negative value is returned on error.
  simple_write_to_buffer - copy data from user space to the buffer
  @to: the buffer to write to
  @available: the size of the buffer
  @ppos: the current position in the buffer
  @from: the user space buffer to read from
  @count: the maximum number of bytes to read
  The simple_write_to_buffer() function reads up to @count bytes from the user
  space address starting at @from into the buffer @to at offset @ppos.
  On success, the number of bytes written is returned and the offset @ppos is
  advanced by this number, or negative value is returned on error.
  memory_read_from_buffer - copy data from the buffer
  @to: the kernel space buffer to read to
  @count: the maximum number of bytes to read
  @ppos: the current position in the buffer
  @from: the buffer to read from
  @available: the size of the buffer
  The memory_read_from_buffer() function reads up to @count bytes from the
  buffer @from at offset @ppos into the kernel space address starting at @to.
  On success, the number of bytes read is returned and the offset @ppos is
  advanced by this number, or negative value is returned on error.
  Transaction based IO.
  The file expects a single write which triggers the transaction, and then
  possibly a read which collects the result - which is stored in a
  file-local buffer.
	
	  The barrier ensures that ar->size will really remain zero until
	  ar->data is ready for reading.
 only one write allowed per open 
 Simple attribute files 
 enough to store a u64 and "\n\0" 
 format for read operation 
 protects access to these buffers 
 simple_attr_open is called by an actual attribute open file operation
 GPL-only?  This?  Really? 
 read from the buffer that is filled with the get function 
 continued read 
 first read 
 interpret the buffer as a number to call the set function with 
 on success, claim we got the whole input 
  generic_fh_to_dentry - generic helper for the fh_to_dentry export operation
  @sb:		filesystem to do the file handle conversion on
  @fid:	file handle to convert
  @fh_len:	length of the file handle in bytes
  @fh_type:	type of file handle
  @get_inode:	filesystem callback to retrieve inode
  This function decodes @fid as long as it has one of the well-known
  Linux filehandle types and calls @get_inode on it to retrieve the
  inode for the object specified in the file handle.
  generic_fh_to_parent - generic helper for the fh_to_parent export operation
  @sb:		filesystem to do the file handle conversion on
  @fid:	file handle to convert
  @fh_len:	length of the file handle in bytes
  @fh_type:	type of file handle
  @get_inode:	filesystem callback to retrieve inode
  This function decodes @fid as long as it has one of the well-known
  Linux filehandle types and calls @get_inode on it to retrieve the
  inode for the _parent_ object specified in the file handle if it
  is specified in the file handle, or NULL otherwise.
  __generic_file_fsync - generic fsync implementation for simple filesystems
  @file:	file to synchronize
  @start:	start offset in bytes
  @end:	end offset in bytes (inclusive)
  @datasync:	only synchronize essential metadata if true
  This is a generic implementation of the fsync method for simple
  filesystems which track all non-inode metadata in the buffers list
  hanging off the address_space structure.
 check and advance again to catch errors after syncing out buffers 
  generic_file_fsync - generic fsync implementation for simple filesystems
 			with flush
  @file:	file to synchronize
  @start:	start offset in bytes
  @end:	end offset in bytes (inclusive)
  @datasync:	only synchronize essential metadata if true
  generic_check_addressable - Check addressability of file system
  @blocksize_bits:	log of file system block size
  @num_blocks:		number of blocks in file system
  Determine whether a file system with @num_blocks blocks (and a
  block size of 2@blocksize_bits) is addressable by the sector_t
  and page cache of the system.  Return 0 if so and -EFBIG otherwise.
  No-op implementation of ->fsync for in-memory filesystems.
	
	  There is no page cache to invalidate in the dax case, however
	  we need this callback defined to prevent falling back to
	  block_invalidatepage() in do_invalidatepage().
	
	  iomap based filesystems support direct IO without need for
	  this callback. However, it still needs to be set in
	  inode->a_ops so that openfcntl know that direct IO is
	  generally supported.
 Because kfree isn't assignment-compatible with void(void) ;- 
	
	  Mark the inode dirty from the very beginning,
	  that way it will never be moved to the dirty
	  list because mark_inode_dirty() will think
	  that it already _is_ on the dirty list.
  simple_nosetlease - generic helper for prohibiting leases
  @filp: file pointer
  @arg: type of lease to obtain
  @flp: new lease supplied for insertion
  @priv: private data for lm_setup operation
  Generic helper for filesystems that do not wish to allow leases to be set.
  All arguments are ignored and it just returns -EINVAL.
  simple_get_link - generic helper to get the target of "fast" symlinks
  @dentry: not used here
  @inode: the symlink inode
  @done: not used here
  Generic helper for filesystems to use for symlink inodes where a pointer to
  the symlink target is stored in ->i_link.  NOTE: this isn't normally called,
  since as an optimization the path lookup code uses any non-NULL ->i_link
  directly, without calling ->get_link().  But ->get_link() still must be set,
  to mark the inode_operations as being for a symlink.
  Return: the symlink target
  Operations for a permanently empty directory.
 An empty directory has two entries . and .. at offsets 0 and 1 
  Determine if the name of a dentry should be casefolded.
  Return: if names will need casefolding
  generic_ci_d_compare - generic d_compare implementation for casefolding filesystems
  @dentry:	dentry whose name we are checking against
  @len:	len of name of dentry
  @str:	str pointer to name of dentry
  @name:	Name to compare against
  Return: 0 if names match, 1 if mismatch, or -ERRNO
	
	  If the dentry name is stored in-line, then it may be concurrently
	  modified by a rename.  If this happens, the VFS will eventually retry
	  the lookup, so it doesn't matter what ->d_compare() returns.
	  However, it's unsafe to call utf8_strncasecmp() with an unstable
	  string.  Therefore, we have to copy the name into a temporary buffer.
 prevent compiler from optimizing out the temporary buffer 
  generic_ci_d_hash - generic d_hash implementation for casefolding filesystems
  @dentry:	dentry of the parent directory
  @str:	qstr of name whose hash we should fill in
  Return: 0 if hash was successful or unchanged, and -EINVAL on error
  generic_set_encrypted_ci_d_ops - helper for setting d_ops for given dentry
  @dentry:	dentry to set ops on
  Casefolded directories need d_hash and d_compare set, so that the dentries
  contained in them are handled case-insensitively.  Note that these operations
  are needed on the parent directory rather than on the dentries in it, and
  while the casefolding flag can be toggled on and off on an empty directory,
  dentry_operations can't be changed later.  As a result, if the filesystem has
  casefolding support enabled at all, we have to give all dentries the
  casefolding operations even if their inode doesn't have the casefolding flag
  currently (and thus the casefolding ops would be no-ops for now).
  Encryption works differently in that the only dentry operation it needs is
  d_revalidate, which it only needs on dentries that have the no-key name flag.
  The no-key flag can't be set "later", so we don't have to worry about that.
  Finally, to maximize compatibility with overlayfs (which isn't compatible
  with certain dentry operations) and to avoid taking an unnecessary
  performance hit, we use custom dentry_operations for each possible
  combination rather than always installing all operations.
 SPDX-License-Identifier: GPL-2.0
   linuxfsattr.c
   Copyright (C) 1991, 1992  Linus Torvalds
   changes by Thomas Schoebel-Theuer
  chown_ok - verify permissions to chown inode
  @mnt_userns:	user namespace of the mount @inode was found from
  @inode:	inode to check permissions on
  @uid:	uid to chown @inode to
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then
  take care to map the inode according to @mnt_userns before checking
  permissions. On non-idmapped mounts or if permission checking is to be
  performed on the raw inode simply passs init_user_ns.
  chgrp_ok - verify permissions to chgrp inode
  @mnt_userns:	user namespace of the mount @inode was found from
  @inode:	inode to check permissions on
  @gid:	gid to chown @inode to
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then
  take care to map the inode according to @mnt_userns before checking
  permissions. On non-idmapped mounts or if permission checking is to be
  performed on the raw inode simply passs init_user_ns.
  setattr_prepare - check if attribute changes to a dentry are allowed
  @mnt_userns:	user namespace of the mount the inode was found from
  @dentry:	dentry to check
  @attr:	attributes to change
  Check if we are allowed to change the attributes contained in @attr
  in the given dentry.  This includes the normal unix access permission
  checks, as well as checks for rlimits and others. The function also clears
  SGID bit from mode if user is not allowed to set it. Also file capabilities
  and IMA extended attributes are cleared if ATTR_KILL_PRIV is set.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then
  take care to map the inode according to @mnt_userns before checking
  permissions. On non-idmapped mounts or if permission checking is to be
  performed on the raw inode simply passs init_user_ns.
  Should be called as the first thing in ->setattr implementations,
  possibly after taking additional locks.
	
	  First check size constraints.  These can't be overriden using
	  ATTR_FORCE.
 If force is set do it anyway. 
 Make sure a caller can chown. 
 Make sure caller can chgrp. 
 Make sure a caller can chmod. 
 Also check the setgid bit! 
 Check for setting the inode time. 
 User has permission for the change 
  inode_newsize_ok - may this inode be truncated to a given size
  @inode:	the inode to be truncated
  @offset:	the new size to assign to the inode
  inode_newsize_ok must be called with i_mutex held.
  inode_newsize_ok will check filesystem limits and ulimits to check that the
  new inode size is within limits. inode_newsize_ok will also send SIGXFSZ
  when necessary. Caller must not proceed with inode size change if failure is
  returned. @inode must be a file (not directory), with appropriate
  permissions to allow truncate (inode_newsize_ok does NOT check these
  conditions).
  Return: 0 on success, -ve errno on failure
		
		  truncation of in-use swapfiles is disallowed - it would
		  cause subsequent swapout to scribble on the now-freed
		  blocks.
  setattr_copy - copy simple metadata updates into the generic inode
  @mnt_userns:	user namespace of the mount the inode was found from
  @inode:	the inode to be updated
  @attr:	the new attributes
  setattr_copy must be called with i_mutex held.
  setattr_copy updates the inode's metadata with that specified
  in attr on idmapped mounts. If file ownership is changed setattr_copy
  doesn't map ia_uid and ia_gid. It will asssume the caller has already
  provided the intended values. Necessary permission checks to determine
  whether or not the S_ISGID property needs to be removed are performed with
  the correct idmapped mount permission helpers.
  Noticeably missing is inode size update, which is more complex
  as it requires pagecache updates.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then
  take care to map the inode according to @mnt_userns before checking
  permissions. On non-idmapped mounts or if permission checking is to be
  performed on the raw inode simply passs init_user_ns.
  The inode is not marked as dirty after this operation. The rationale is
  that for "simple" filesystems, the struct inode is the inode storage.
  The caller is free to mark the inode dirty afterwards if needed.
	
	  If utimes(2) and friends are called with times == NULL (or both
	  times are UTIME_NOW), then we need to check for write permission
  notify_change - modify attributes of a filesytem object
  @mnt_userns:	user namespace of the mount the inode was found from
  @dentry:	object affected
  @attr:	new attributes
  @delegated_inode: returns inode, if the inode is delegated
  The caller must hold the i_mutex on the affected object.
  If notify_change discovers a delegation in need of breaking,
  it will return -EWOULDBLOCK and return a reference to the inode in
  delegated_inode.  The caller should then break the delegation and
  retry.  Because breaking a delegation may take a long time, the
  caller should drop the i_mutex before doing so.
  If file ownership is changed notify_change() doesn't map ia_uid and
  ia_gid. It will asssume the caller has already provided the intended values.
  Alternatively, a caller may pass NULL for delegated_inode.  This may
  be appropriate for callers that expect the underlying filesystem not
  to be NFS exported.  Also, passing NULL is fine for callers holding
  the file open for write, as there can be no conflicting delegation in
  that case.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then
  take care to map the inode according to @mnt_userns before checking
  permissions. On non-idmapped mounts or if permission checking is to be
  performed on the raw inode simply passs init_user_ns.
 Flag setting protected by i_mutex 
	
	  We now pass ATTR_KILL_SID to the lower level setattr function so
	  that the function has the ability to reinterpret a mode change
	  that's due to these bits. This adds an implicit restriction that
	  no function will ever call notify_change with both ATTR_MODE and
	  ATTR_KILL_SID set.
	
	  Verify that uidgid changes are valid in the target
	  namespace of the superblock.
	 Don't allow modifications of files with invalid uids or
	  gids unless those uids & gids are being made valid.
 SPDX-License-Identifier: GPL-2.0-only
   linuxfsnamespace.c
  (C) Copyright Al Viro 2000, 2001
  Based on code from fssuper.c, copyright Linus Torvalds and others.
  Heavily rewritten.
 init_rootfs 
 get_fs_root et.al. 
 fsnotify_vfsmount_delete 
 Maximum number of mounts in a mount namespace 
 protected by namespace_sem 
 protected by namespace_sem 
 sysfs 
  vfsmount lock may be taken for read to prevent changes to the
  vfsmount hash, ie. during mountpoint lookups or walking back
  up the tree.
  It should be taken for write in all cases where the vfsmount
  tree or hash is modified or when a vfsmount structure is modified.
  Allocate a new peer group ID
  Release a peer group ID
  vfsmount lock must be held for read
  vfsmount lock must be held for write
  Most ro checks on a fs are for operations that take
  discrete amounts of time, like a write() or unlink().
  We must keep track of when those operations start
  (for permission checks) and when they end, so that
  we can determine when writes are able to occur to
  a filesystem.
  __mnt_is_readonly: check whether a mount is read-only
  @mnt: the mount to check for its write status
  This shouldn't be used directly ouside of the VFS.
  It does not guarantee that the filesystem will stay
  rw, just that it is right now.  This can not and
  should not be used in place of IS_RDONLY(inode).
  mnt_wantdrop_write() will _keep_ the filesystem
  rw.
 Order wrt setting s_flagss_readonly_remount in do_remount() 
  Most ro & frozen checks on a fs are for operations that take discrete
  amounts of time, like a write() or unlink().  We must keep track of when
  those operations start (for permission checks) and when they end, so that we
  can determine when writes are able to occur to a filesystem.
  __mnt_want_write - get write access to a mount without freeze protection
  @m: the mount on which to take a write
  This tells the low-level filesystem that a write is about to be performed to
  it, and makes sure that writes are allowed (mnt it read-write) before
  returning success. This operation does not protect against filesystem being
  frozen. When the write operation is finished, __mnt_drop_write() must be
  called. This is effectively a refcount.
	
	  The store to mnt_inc_writers must be visible before we pass
	  MNT_WRITE_HOLD loop below, so that the slowpath can see our
	  incremented count after it has set MNT_WRITE_HOLD.
	
	  After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will
	  be set to match its requirements. So we must not load that until
	  MNT_WRITE_HOLD is cleared.
  mnt_want_write - get write access to a mount
  @m: the mount on which to take a write
  This tells the low-level filesystem that a write is about to be performed to
  it, and makes sure that writes are allowed (mount is read-write, filesystem
  is not frozen) before returning success.  When the write operation is
  finished, mnt_drop_write() must be called.  This is effectively a refcount.
  __mnt_want_write_file - get write access to a file's mount
  @file: the file who's mount on which to take a write
  This is like __mnt_want_write, but if the file is already open for writing it
  skips incrementing mnt_writers (since the open file already has a reference)
  and instead only does the check for emergency ro remounts.  This must be
  paired with __mnt_drop_write_file.
		
		  Superblock may have become readonly while there are still
		  writable fd's, e.g. due to a fs error with errors=remount-ro
  mnt_want_write_file - get write access to a file's mount
  @file: the file who's mount on which to take a write
  This is like mnt_want_write, but if the file is already open for writing it
  skips incrementing mnt_writers (since the open file already has a reference)
  and instead only does the freeze protection and the check for emergency ro
  remounts.  This must be paired with mnt_drop_write_file.
  __mnt_drop_write - give up write access to a mount
  @mnt: the mount on which to give up write access
  Tells the low-level filesystem that we are done
  performing writes to it.  Must be matched with
  __mnt_want_write() call above.
  mnt_drop_write - give up write access to a mount
  @mnt: the mount on which to give up write access
  Tells the low-level filesystem that we are done performing writes to it and
  also allows filesystem to be frozen again.  Must be matched with
  mnt_want_write() call above.
	
	  After storing MNT_WRITE_HOLD, we'll read the counters. This store
	  should be visible before we do.
	
	  With writers on hold, if this value is zero, then there are
	  definitely no active writers (although held writers may subsequently
	  increment the count, they'll have to wait, and decrement it after
	  seeing MNT_READONLY).
	 
	  It is OK to have counter incremented on one CPU and decremented on
	  another: the sum will add up correctly. The danger would be when we
	  sum up each counter, if we read a counter before it is incremented,
	  but then read another CPU's count which it has been subsequently
	  decremented from -- we would see more decrements than we should.
	  MNT_WRITE_HOLD protects against this scenario, because
	  mnt_want_write first increments count, then smp_mb, then spins on
	  MNT_WRITE_HOLD, so it can't be decremented by another CPU while
	  we're counting up here.
	
	  MNT_READONLY must become visible before ~MNT_WRITE_HOLD, so writers
	  that become unheld will see MNT_READONLY.
 Racy optimization.  Recheck the counter under MNT_WRITE_HOLD 
 call under rcu_read_lock 
 see mntput_no_expire()
 caller will mntput() 
 call under rcu_read_lock 
  find the first mount at @dentry on vfsmount @mnt.
  call under rcu_read_lock()
  lookup_mnt - Return the first child mount mounted at path
  "First" means first mounted chronologically.  If you create the
  following mounts:
  mount devsda1 mnt
  mount devsda2 mnt
  mount devsda3 mnt
  Then lookup_mnt() on the base mnt dentry in the root mount will
  return successively the root dentry and vfsmount of devsda1, then
  devsda2, then devsda3, then NULL.
  lookup_mnt takes a reference to the found vfsmount.
  __is_local_mountpoint - Test to see if dentry is a mountpoint in the
                          current mount namespace.
  The common case is dentries are not mountpoints at all and that
  test is handled inline.  For the slow case when we are actually
  dealing with a mountpoint of some kind, walk through all of the
  mounts in the current mount namespace and test to see if the dentry
  is a mountpoint.
  The mount_hashtable is not usable in the context because we
  need to identify all mounts that may be in the current mount
  namespace not just a mount that happens to have some specified
  parent mount.
 might be worth a WARN_ON() 
 Exactly one processes may set d_mounted 
 Someone else set d_mounted? 
 The dentry is not available as a mountpoint? 
 Add the new mountpoint to the hash table 
  vfsmount lock must be held.  Additionally, the caller is responsible
  for serializing calls for given disposal list.
 called with namespace_lock and vfsmount lock 
  vfsmount lock must be held for write
  vfsmount lock must be held for write
  vfsmount lock must be held for write
  vfsmount lock must be held for write
  vfsmount lock must be held for write
 essentially, that's mntget 
  vfsmount lock must be held for write
  vfsmount lock must be held for write
  vfs_create_mount - Create a mount for a configured superblock
  @fc: The configuration context with the superblock attached
  Create a mount to an already configured superblock.  If necessary, the
  caller should invoke vfs_get_tree() before calling this.
  Note that this does not attach the mount to anything.
	 Until it is worked out how to pass the user namespace
	  through from the parent mount to the submount don't support
	  unprivileged mounts with submounts.
 not a peer of original 
	 stick the duplicate mount on the same expiry list
	
	  The warning here probably indicates that somebody messed
	  up a mnt_wantdrop_write() pair.  If this happens, the
	  filesystem was probably unable to make rw->ro transitions.
	  The locking used to deal with mnt_count decrement provides barriers,
	  so mnt_get_writers() below is safe.
		
		  Since we don't do lock_mount_hash() here,
		  ->mnt_ns can change under us.  However, if it's
		  non-NULL, then there's a reference that won't
		  be dropped until after an RCU delay done after
		  turning ->mnt_ns NULL.  So if we observe it
		  non-NULL under rcu_read_lock(), the reference
		  we are dropping is not the final one.
	
	  make sure that if __legitimize_mnt() has not seen us grab
	  mount_lock, we'll see their refcount increment here.
 avoid cacheline pingpong, hope gcc doesn't get "smart" 
  path_is_mountpoint() - Check if path is a mount in the current namespace.
  @path: path to check
   d_mountpoint() can only be used reliably to establish if a dentry is
   not mounted in any namespace and that common case is handled inline.
   d_mountpoint() isn't aware of the possibility there may be multiple
   mounts using a given dentry in a different namespace. This function
   checks if the passed in path is a mountpoint rather than the dentry
   alone.
 iterator; we want it to have access to namespace_sem, thus here... 
 Read after we'd reached the end? 
 CONFIG_PROC_FS 
  may_umount_tree - check if a mount tree is busy
  @m: root of mount tree
  This is called to check if a tree of mounts has any
  open files, pwds, chroots or sub mounts that are
  busy.
 write lock needed for mnt_get_count 
  may_umount - check if a mount point is busy
  @mnt: root of mount
  This is called to check if a mount point has any
  open files, pwds, chroots or sub mounts. If the
  mount has sub mounts this will return busy
  regardless of whether the sub mounts are busy.
  Doesn't take quota and stuff into account. IOW, in some cases it will
  give false negatives. The main reason why it's here is that we need
  a non-destructive way to look for easily umountable filesystems.
 Leaving mounts connected is only valid for lazy umounts 
 A mount without a parent has nothing to be connected to 
	 Because the reference counting rules change when mounts are
	  unmounted and connected, umounted mounts may not be
	  connected to mounted mounts.
 Has it been requested that the mount remain connected? 
 Is the mount locked such that it needs to remain connected? 
 By default disconnect the mount 
  mount_lock must be held
  namespace_sem must be held for write
 Gather the mounts to umount 
 Hide the mounts from mnt_mounts 
 Add propogated mounts to the tmp_list 
 Don't forget about p 
	
	  Allow userspace to request a mountpoint be expired rather than
	  unmounting unconditionally. Unmount only happens if:
	   (1) the mark is already set (the mark is cleared by mntput())
	   (2) the usage count == 1 [parent vfsmount] + 1 [sys_umount]
		
		  probably don't strictly need the lock here if we examined
		  all race cases, but it's a slowpath.
	
	  If we may have to abort operations to get out of this
	  mount, and they will themselves hold resources we must
	  allow the fs to do things. In the Unix tradition of
	  'Gee thats tricky lets do it in userspace' the umount_begin
	  might fail to complete on the first run through as other tasks
	  must return, and the like. Thats for the mount program to worry
	  about for the moment.
	
	  No sense to grab the lock for this test, but test itself looks
	  somewhat bogus. Suggestions for better replacement?
	  Ho-hum... In principle, we might treat that as umount + switch
	  to rootfs. GC would eventually take care of the old vfsmount.
	  Actually it makes sense, especially if rootfs would contain a
	  reboot - static binary that would close all descriptors and
	  call reboot(9). Then init(8) could umount root and exec reboot.
		
		  Special case for "unmounting" root ...
		  we just try to remount it readonly.
 Recheck MNT_LOCKED with the locks held 
  __detach_mounts - lazily unmount all mounts on the specified dentry
  During unlink, rmdir, and d_drop it is possible to loose the path
  to an existing mountpoint, and wind up leaking the mount.
  detach_mounts allows lazily unmounting those mounts instead of
  leaking them.
  The caller may hold dentry->d_inode->i_mutex.
  Is the caller allowed to modify his namespace?
 Check optimistically 
 caller is responsible for flags being sane
 we mustn't call path_put() as that would clear mnt_expiry_mark 
 basic validity checks done first
 	The 2.0 compatible umount. No flags.
 Is this a proxy for a mount namespace? 
	 Could bind mounting the mount namespace inode cause a
	  mount namespace loop?
 Both unbindable and locked. 
 Caller should check returned pointer for errors 
  clone_private_mount - create a private clone of a path
  @path: path to clone
  This creates a new vfsmount, which will be the clone of @path.  The new mount
  will not be attached anywhere in the namespace and will be private (i.e.
  changes to the originating mount won't be propagated into this).
  Release with mntput().
 Longterm mount to be removed by kern_unmount() 
 Don't allow unprivileged users to change mount flags 
 Don't allow unprivileged users to reveal what is under a mount 
   @source_mnt : mount tree to be attached
   @nd         : place the mount tree @source_mnt is attached
   @parent_nd  : if non-null, detach the source_mnt from its parent and
   		   store the parent mount and mountpoint dentry.
   		   (done when source_mnt is moved)
   NOTE: in the table below explains the semantics when a source mount
   of a given type is attached to a destination mount of a given type.
  ---------------------------------------------------------------------------
  |         BIND MOUNT OPERATION                                            |
  |
  | source-->| shared        |       private  |       slave    | unbindable |
  | dest     |               |                |                |            |
  |   |      |               |                |                |            |
  |   v      |               |                |                |            |
  |
  |  shared  | shared (++)   |     shared (+) |     shared(+++)|  invalid   |
  |          |               |                |                |            |
  |non-shared| shared (+)    |      private   |      slave () |  invalid   |
  A bind operation clones the source mount and mounts the clone on the
  destination mount.
  (++)  the cloned mount is propagated to all the mounts in the propagation
  	 tree of the destination mount and the cloned mount is added to
  	 the peer group of the source mount.
  (+)   the cloned mount is created under the destination mount and is marked
        as shared. The cloned mount is added to the peer group of the source
        mount.
  (+++) the mount is propagated to all the mounts in the propagation tree
        of the destination mount and the cloned mount is made slave
        of the same master as that of the source mount. The cloned mount
        is marked as 'shared and slave'.
  ()   the cloned mount is made a slave of the same master as that of the
  	 source mount.
  ---------------------------------------------------------------------------
  |         		MOVE MOUNT OPERATION                                 |
  |
  | source-->| shared        |       private  |       slave    | unbindable |
  | dest     |               |                |                |            |
  |   |      |               |                |                |            |
  |   v      |               |                |                |            |
  |
  |  shared  | shared (+)    |     shared (+) |    shared(+++) |  invalid   |
  |          |               |                |                |            |
  |non-shared| shared (+)   |      private   |    slave ()   | unbindable |
  (+)  the mount is moved to the destination. And is then propagated to
  	all the mounts in the propagation tree of the destination mount.
  (+)  the mount is moved to the destination.
  (+++)  the mount is moved to the destination and is then propagated to
  	all the mounts belonging to the destination mount's propagation tree.
  	the mount is marked as 'shared and slave'.
  ()	the mount continues to be a slave at the new location.
  if the source mount is a tree, the operations explained above is
  applied to each mount in the tree.
  Must be called without spinlocks held, since this function can sleep
  in allocations.
	 Preallocate a mountpoint in case the new mounts need
	  to be tucked under other mounts.
 Is there space to add these mounts to the mount namespace? 
 move from anon - the caller will destroy 
 Notice when we are propagating across user namespaces 
  Sanity check the flags to change_mnt_propagation.
 Fail if any non-propagation flags are set 
 Only one propagation flag should be set 
  recursively change the type of the mountpoint.
  do loopback mount.
  Don't allow locked mount flags to be cleared.
  No locks need to be held here while testing the various MNT_LOCK
  flags because those flags can never be cleared once they are set.
  Handle reconfiguration of the mountpoint only without alteration of the
  superblock it refers to.  This is triggered by specifying MS_REMOUNT|MS_BIND
  to mount(2).
	
	  We're only checking whether the superblock is read-only not
	  changing it, so only take down_read(&sb->s_umount).
  change filesystem flags. dir should be a physical root of filesystem.
  If you've mounted a non-root directory somewhere and want to do remount
  on it - tough luck.
  Check that there aren't references to earliersame mount namespaces in the
  specified subtree.  Such references can act as pins for mount namespaces
  that aren't checked by the mount-cycle checking code, thereby allowing
  cycles to be made.
 To and From must be mounted 
 We should be allowed to modify mount namespaces of both mounts 
 To and From paths should be mount roots 
 Setting sharing groups is only allowed across same superblock 
 From mount root should be wider than To mount root 
 From mount should not have locked children in place of To's root 
 Setting sharing groups is only allowed on private mounts 
 From should not be private 
 The mountpoint must be in our namespace. 
 The thing moved must be mounted... 
 ... and either ours or the root of anon namespace 
	
	  Don't move a mount residing in a shared parent.
	
	  Don't move a mount tree containing unbindable mounts to a destination
	  mount which is shared.
	 if the mount is moved, it should no longer be expire
  add a mount into a namespace's mount tree
 that's acceptable only for automounts done in private ns 
 ... and for those we'd better have mountpoint still alive 
 Refuse the same filesystem on the same mount point 
  Create a new mount using a superblock configuration and request it
  be added to the namespace tree.
  create a new mount for userspace and request it to be added into the
  namespace's tree
	 The new mount record should have at least 2 refs to prevent it being
	  expired before we get a chance to add it
	
	  we don't want to use lock_mount() - in this case finding something
	  that overmounts our mountpoint to be means "quitely drop what we've
	  got", not "try to mount it on top".
 remove m from any expiration list it may be on 
  mnt_set_expiry - Put a mount on an expiration list
  @mnt: The mount to list.
  @expiry_list: The list to add the mount to.
  process a list of expirable mountpoints with the intent of discarding any
  mountpoints that aren't in use and haven't been touched since last we came
  here
	 extract from the expiration list every vfsmount that matches the
	  following criteria:
	  - only referenced by its parent vfsmount
	  - still marked for expiry (marked on the last call here; marks are
	    cleared by mntput())
  Ripoff of 'select_parent()'
  search the list of submounts for a given mountpoint, and move any
  shrinkable submounts to the 'graveyard' list.
		
		  Descend a level if the d_mounts list is non-empty.
	
	  All done at this level ... ascend and resume the search
  process a list of expirable mountpoints with the intent of discarding any
  submounts of a specific parent mountpoint
  mount_lock must be held for write
 extract submounts of 'mountpoint' from the expiration list 
	
	  Not all architectures have an exact copy_from_user(). Resort to
	  byte at a time.
  Flags is a 32-bit value that allows up to 31 non-fs dependent flags to
  be given to the mount() call (ie: read-only, no-dev, no-suid etc).
  data is a (void ) that can point to any structure up to
  PAGE_SIZE-1 bytes, which can contain arbitrary fs-dependent
  information (or be NULL).
  Pre-0.97 versions of mount() didn't have a flags word.
  When the flags word was introduced its top half was required
  to have the magic value 0xC0ED, and this remained so until 2.4.0-test9.
  Therefore, if this magic number is present, it carries no information
  and must be discarded.
 Discard magic 
 Basic sanity checks 
 Default to relatime unless overriden 
 Separate the per-mountpoint flags 
 The default atime for remount is preservation 
  Assign a sequence number so we can detect when we attempt to bind
  mount a reference to an older mount namespace into the current
  mount namespace, preventing reference counting loops.  A 64bit
  number incrementing at 10Ghz will take 12,427 years to wrap which
  is effectively never, so we can ignore the possibility.
 First pass: copy the tree topology 
	
	  Second pass: switch the tsk->fs-> elements and mark new vfsmounts
	  as belonging to new namespace.  We have already acquired a private
	  fs_struct, so tsk->fs->lock is not needed.
 trade a vfsmount reference for active sb one 
 lock the sucker 
 ... and return the root of (sub)tree on it 
  Create a kernel mount representation for a new, prepared superblock
  (specified by fs_fd) and attach to an open_tree-like file descriptor.
 There must be a valid superblock or we can't mount it 
	 We've done the mount bit - now move the file context into more or
	  less the same state as if we'd done an fspick().  We don't want to
	  do any memory allocation or anything like that at this point as we
	  don't want to have to handle any errors incurred.
	 Attach to an apparent O_PATH fd with a note that we need to unmount
	  it, not just simply put it.
  Move a mount from one place to another.  In combination with
  fsopen()fsmount() this is used to install a new mount and in combination
  with open_tree(OPEN_TREE_CLONE [| AT_RECURSIVE]) it can be used to copy
  a mount subtree.
  Note the flags value is a combination of MOVE_MOUNT_ flags.
	 If someone gives a pathname, they aren't permitted to move
	  from an fd that requires unmount as we can't get at the flag
	  to clear it afterwards.
  Return true if path is reachable from root
  namespace_sem or mount_lock is held
  pivot_root Semantics:
  Moves the root file system of the current process to the directory put_old,
  makes new_root as the new root file system of the current process, and sets
  rootcwd of all processes which had them on the current root to new_root.
  Restrictions:
  The new_root and put_old must be directories, and  must not be on the
  same file  system as the current process root. The put_old  must  be
  underneath new_root,  i.e. adding a non-zero number of .. to the string
  pointed to by put_old must yield the same directory as new_root. No other
  file system may be mounted on put_old. After all, new_root is a mountpoint.
  Also, the current root cannot be on the 'rootfs' (initial ramfs) filesystem.
  See Documentationfilesystemsramfs-rootfs-initramfs.rst for alternatives
  in this situation.
  Notes:
   - we don't move rootcwd if they are not at the root (reason: if something
     cared enough to change them, it's probably wrong to force them elsewhere)
   - it's okay to pick a root that isn't the root of a file system, e.g.
     nfsmy_root where nfs is the mount point. It must be a mountpoint,
     though, so you may need to say mount --bind nfsmy_root nfsmy_root
     first.
 loop, on the same file system  
 not a mountpoint 
 not attached 
 not a mountpoint 
 not attached 
 make sure we can reach put_old from new_root 
 make certain new is below the root 
 we'll need its mountpoint 
 mount old root on put_old 
 mount new_root on  
 A moved mount should not expire automatically 
  flags to clear 
 flags to raise 
	
	  Once a mount has been idmapped we don't allow it to change its
	  mapping. It makes things simpler and callers can just create
	  another bind-mount they can idmap if they want to.
 The underlying filesystem doesn't support idmapped mounts yet. 
 Don't yet support filesystem mountable in user namespaces. 
 We're not controlling the superblock. 
 Mount has already been visible in the filesystem hierarchy. 
 Pairs with smp_load_acquire() in mnt_user_ns(). 
		
		  We either set MNT_READONLY above so make it visible
		  before ~MNT_WRITE_HOLD or we failed to recursively
		  apply mount options.
		
		  On failure, only cleanup until we found the first mount
		  we failed to handle.
		
		  Only take namespace_lock() if we're actually changing
		  propagation.
	
	  Get the mount tree in a shape where we can change mount
	  properties without failure.
 Commit all changes or revert to the old state. 
	
	  We currently do not support clearing an idmapped mount. If this ever
	  is a use-case we can revisit this but for now let's keep it simple
	  and not allow it.
	
	  The init_user_ns is used to indicate that a vfsmount is not idmapped.
	  This is simpler than just having to treat NULL as unmapped. Users
	  wanting to idmap a mount to init_user_ns can just use a namespace
	  with an identity mapping.
	
	  Since the MOUNT_ATTR_<atime> values are an enum, not a bitmap,
	  users wanting to transition to a different atime setting cannot
	  simply specify the atime setting in @attr_set, but must also
	  specify MOUNT_ATTR__ATIME in the @attr_clr field.
	  So ensure that MOUNT_ATTR__ATIME can't be partially set in
	  @attr_clr and that @attr_set can't have any atime bits set if
	  MOUNT_ATTR__ATIME isn't set in @attr_clr.
		
		  Clear all previous time settings as they are mutually
		  exclusive.
 Don't bother walking through the mounts if this is a nop. 
		
		  it is a longterm mount, don't release mnt until
		  we unmount before file sys is unregistered
 release long term mount so mount point can be released 
 yecchhh... 
 Does the current process have a non-standard root 
 Find the namespace root 
		 This mount is not fully visible if it's root directory
		  is not the root directory of the filesystem.
 A local view of the mount flags 
 Don't miss readonly hidden in the superblock flags 
		 Verify the mount flags are equal to or more permissive
		  than the proposed new mount.
		 This mount is not fully visible if there are any
		  locked child mounts that cover anything except for
		  empty directories.
 Only worry about locked mounts 
 Is the directory permanetly empty? 
 Preserve the locked attributes 
 Can this filesystem be too revealing? 
	
	  Foreign mounts (accessed via fchdir or through proc
	  symlinks) are always treated as if they are nosuid.  This
	  prevents namespaces from trusting potentially unsafe
	  suidsgid bits, file caps, or security labels that originate
	  in other namespaces.
 Find the root 
 revert to old namespace 
 Update the pwd and root 
 SPDX-License-Identifier: GPL-2.0
  Implement the manual drop-all-pagecache function
 A global variable is a bit ugly, but it keeps the code simple 
		
		  We must skip inodes in unusual state. We may also skip
		  inodes without pages but we deliberately won't in case
		  we need to reschedule to avoid softlockups.
 SPDX-License-Identifier: GPL-2.0 
  The source of the prepend data can be an optimistoc load
  of a dentry name and length. And because we don't hold any
  locks, the length and the pointer to the name may not be
  in sync if a concurrent rename happens, and the kernel
  copy might fault as a result.
  The end result will correct itself when we check the
  rename sequence count, but we need to be able to handle
  the fault gracefully.
 Already overflowed?
 Will overflow?
 Fill as much as possible from the end of the name
 Fits fully
  prepend_name - prepend a pathname in front of current buffer pointer
  @p: prepend buffer which contains buffer pointer and allocated length
  @name: name string and length qstr structure
  With RCU path tracing, it may race with d_move(). Use READ_ONCE() to
  make sure that either the old or the new name pointer and length are
  fetched. However, there may be mismatch between length and pointer.
  But since the length cannot be trusted, we need to copy the name very
  carefully when doing the prepend_copy(). It also prepends "" at
  the beginning of the name. The sequence number check at the caller will
  retry it again when a d_move() does happen. So any garbage in the buffer
  due to mismatched pointer and length will be discarded.
  Load acquire is needed to make sure that we see the new name data even
  if we might get the length wrong.
 ^^^ 
 Global root 
 open-coded is_mounted() to use local mnt_ns 
 absolute root
 detached or not attached yet
 Escaped? 
  prepend_path - Prepend path string to a buffer
  @path: the dentryvfsmount to report
  @root: root vfsmntdentry
  @p: prepend buffer which contains buffer pointer and allocated length
  The function will first try to write out the pathname without taking any
  lock other than the RCU read lock to make sure that dentries won't go away.
  It only checks the sequence number of the global rename_lock as any change
  in the dentry's d_seq will be preceded by changes in the rename_lock
  sequence number. If the sequence number had been changed, it will restart
  the whole pathname back-tracing sequence again by taking the rename_lock.
  In this case, there is no need to take the RCU read lock as the recursive
  parent pointer references will keep the dentry chain alive as long as no
  rename operation is performed.
  __d_path - return the path of a dentry
  @path: the dentryvfsmount to report
  @root: root vfsmntdentry
  @buf: buffer to return value in
  @buflen: buffer length
  Convert a dentry into an ASCII path name.
  Returns a pointer into the buffer or an error code if the
  path was too long.
  "buflen" should be positive.
  If the path is not reachable from the supplied root, return %NULL.
  d_path - return the path of a dentry
  @path: path to report
  @buf: buffer to return value in
  @buflen: buffer length
  Convert a dentry into an ASCII path name. If the entry has been deleted
  the string " (deleted)" is appended. Note that this is ambiguous.
  Returns a pointer into the buffer or an error code if the path was
  too long. Note: Callers should use the returned pointer, not the passed
  in buffer, to use the name! The implementation often starts at an offset
  into the buffer, and may leave 0 bytes at the start.
  "buflen" should be positive.
	
	  We have various synthetic filesystems that never get mounted.  On
	  these filesystems dentries are never used for lookup purposes, and
	  thus don't need to be hashed.  They also don't need a name until a
	  user wants to identify the object in procpidfd.  The little hack
	  below allows us to generate a name for these objects on demand:
	 
	  Some pseudo inodes are mountable.  When they are mounted
	  path->dentry == path->mnt->mnt_root.  In that case don't call d_dname
	  and instead have d_path return the mounted path.
  Helper function for dentry_operations.d_dname() members
 these dentries are never renamed, so d_lock is not needed 
  Write full pathname from the root of the filesystem into the buffer.
deleted", 10);
  NOTE! The user-level library version returns a
  character pointer. The kernel system call just
  returns the length of the buffer filled (which
  includes the ending '\0' character), or a negative
  error value. So libc would do something like
 	char getcwd(char  buf, size_t size)
 	{
 		int retval;
 		retval = sys_getcwd(buf, size);
 		if (retval >= 0)
 			return buf;
 		errno = -retval;
 		return NULL;
 	}
 SPDX-License-Identifier: GPL-2.0
  fs on-disk file type to dirent file type conversion
  fs_ftype_to_dtype() - fs on-disk file type to dirent type.
  @filetype: The on-disk file type to convert.
  This function converts the on-disk file type value (FT_) to the directory
  entry type (DT_).
  Context: Any context.
  Return:
   DT_UNKNOWN		- Unknown type
   DT_FIFO		- FIFO
   DT_CHR		- Character device
   DT_DIR		- Directory
   DT_BLK		- Block device
   DT_REG		- Regular file
   DT_LNK		- Symbolic link
   DT_SOCK		- Local-domain socket
  dirent file type to fs on-disk file type conversion
  Values not initialized explicitly are FT_UNKNOWN (0).
  fs_umode_to_ftype() - file mode to on-disk file type.
  @mode: The file mode to convert.
  This function converts the file mode value to the on-disk file type (FT_).
  Context: Any context.
  Return:
   FT_UNKNOWN		- Unknown type
   FT_REG_FILE	- Regular file
   FT_DIR		- Directory
   FT_CHRDEV		- Character device
   FT_BLKDEV		- Block device
   FT_FIFO		- FIFO
   FT_SOCK		- Local-domain socket
   FT_SYMLINK		- Symbolic link
  fs_umode_to_dtype() - file mode to dirent file type.
  @mode: The file mode to convert.
  This function converts the file mode value to the directory
  entry type (DT_).
  Context: Any context.
  Return:
   DT_UNKNOWN		- Unknown type
   DT_FIFO		- FIFO
   DT_CHR		- Character device
   DT_DIR		- Directory
   DT_BLK		- Block device
   DT_REG		- Regular file
   DT_LNK		- Symbolic link
   DT_SOCK		- Local-domain socket
 	An async IO implementation for Linux
 	Written by Benjamin LaHaise <bcrl@kvack.org>
 	Implements an efficient asynchronous io interface.
 	Copyright 2000, 2001, 2002 Red Hat, Inc.  All Rights Reserved.
 	Copyright 2018 Christoph Hellwig.
 	See ..COPYING for licensing terms.
 kernel internal index number 
 number of io_events 
	unsigned	head;	 Written to by userland or under ring_lock
 size of aio_ring 
 128 bytes + ring size 
  Plugging is meant to work with larger batches of IOs. If we don't
  have more than the below, then don't bother setting up a plug.
	
	  For percpu reqs_available, number of slots we move tofrom global
	  counter at a time:
	
	  This is what userspace passed to io_setup(), it's not used for
	  anything but counting against the global max_reqs quota.
	 
	  The real limit is nr_events - 1, which will be larger (see
	  aio_setup_ring())
 Size of ringbuffer, in units of struct io_event 
 see free_ioctx() 
	
	  signals when all in-flight requests are done
		
		  This counts the number of available slots in the ringbuffer,
		  so we avoid overflowing it: it's decremented (if positive)
		  when allocating a kiocb and incremented when the resulting
		  io_event is pulled off the ringbuffer.
		 
		  We batch accesses to it with a percpu version.
 used for cancellation 
  First field must be the file pointer in all the
  iocb unions! See also 'struct kiocb' in <linuxfs.h>
  NOTE! Each of the iocb union members has the file pointer
  as the first entry in their struct definition. So you can
  access the file pointer through any of the sub-structs,
  or directly as just 'ki_filp' in this struct.
	struct list_head	ki_list;	 the aio core uses this
	
	  If the aio_resfd field of the userspace iocb is not zero,
	  this is the underlying eventfd context to deliver events to.
------ sysctl variables----
 current system wide number of aio requests 
 system wide maximum number of aio requests 
----end sysctl variables---
 aio_setup
 	Creates the slab caches used by the aio routines, panic on
 	failure as this is done early during the boot sequence.
 Prevent further access to the kioctx from migratepages 
	 Disconnect the kiotx from the ring file.  This prevents future
	  accesses to the kioctx from page migration.
	
	  We cannot support the _NO_COPY case here, because copy needs to
	  happen under the ctx->completion_lock. That does not work with the
	  migration workflow of MIGRATE_SYNC_NO_COPY.
 mapping->private_lock here protects against the kioctx teardown.  
	 The ring_lock mutex.  The prevents aio_read_events() from writing
	  to the ring's head, and prevents page migration from mucking in
	  a partially initialized kiotx.
 Make sure the old page hasn't already been changed 
 Writeback must be complete 
	 Take completion_lock to prevent other writes to the ring buffer
	  while the old page is copied to the new.  This prevents new
	  events from being lost.
 The old page is no longer accessible. 
 Compensate for the ring buffer's headtail overlap entry 
 1 is required, 2 for good luck 
 trusted copy 
 user copy 
  free_ioctx() should be RCU delayed to synchronize against the RCU
  protected lookup_ioctx() and also needs process context to call
  aio_free_ring().  Use rcu_work.
 At this point we know that there are no any in-flight requests 
 Synchronize against RCU protected table->table[] dereferences 
  When this function runs, the kioctx has been removed from the "hash table"
  and ctx->users has dropped to 0, so we know no more kiocbs can be submitted -
  now it's safe to cancel any that need to be.
					 While kioctx setup is in progress,
					  we are protected from page migration
					  changes ring_pages by ->ring_lock.
 ioctx_alloc
 	Allocates and initializes an ioctx.  Returns an ERR_PTR if it failed.
	
	  Store the original nr_events -- what userspace passed to io_setup(),
	  for counting against the global limit -- before it changes.
	
	  We keep track of the number of available ringbuffer slots, to prevent
	  overflow (reqs_available), and we also use percpu counters for this.
	 
	  So since up to half the slots might be on other cpu's percpu counters
	  and unavailable, double nr_events so userspace sees what they
	  expected: additionally, we move req_batch slots tofrom percpu
	  counters at a time, so make sure that isn't 0:
 Prevent overflows 
	 Protect against page migration throughout kiotx setup by keeping
 limit the number of system wide aios 
 io_setup() will drop this ref 
 free_ioctx_users() will drop this 
 Release the ring_lock mutex now that all setup is complete. 
 kill_ioctx
 	Cancels all outstanding aio requests on an aio context.  Used
 	when the processes owning a context have all exited to encourage
 	the rapid destruction of the kioctx.
 free_ioctx_reqs() will do the necessary RCU synchronization 
	
	  It'd be more correct to do this in free_ioctx(), after all
	  the outstanding kiocbs have finished - but by then io_destroy
	  has already returned, so io_setup() could potentially return
	  -EAGAIN with no ioctxs actually in use (as far as userspace
	   could tell).
  exit_aio: called when the last user of mm goes away.  At this point, there is
  no way for any new requests to be submited or any of the io_ syscalls to be
  called on the context.
  There may be outstanding kiocbs, but free_ioctx() will explicitly wait on
  them.
		
		  We don't need to bother with munmap() here - exit_mmap(mm)
		  is coming and it'll unmap everything. And we simply can't,
		  this is not necessarily our ->mm.
		  Since kill_ioctx() uses non-zero ->mmap_size as indicator
		  that it needs to unmap the area, just set it to 0.
 Wait until all IO for the context are done. 
 refill_reqs_available
 	Updates the reqs_available reference counts used for tracking the
 	number of free slots in the completion ring.  This can be called
 	from aio_complete() (to optimistically update reqs_available) or
 	from aio_get_req() (the we're out of events case).  It must be
 	called holding ctx->completion_lock.
 Clamp head since userland can write to it. 
 user_refill_reqs_available
 	Called to refill reqs_available when aio_get_req() encounters an
 	out of space in the completion ring.
		 Access of ring->head may race with aio_read_events_ring()
		  here, but that's okay since whether we read the old version
		  or the new version, and either will be valid.  The important
		  part is that head cannot pass tail since we prevent
		  aio_complete() from updating tail by holding
		  ctx->completion_lock.  Even if head is invalid, the check
		  against ctx->completed_events below will make sure we do the
		  saferight thing.
 aio_get_req
 	Allocate a slot for an aio request.
  Returns NULL if no requests are free.
  The refcount is initialized to 2 - one for the async op completion,
  one for the synchronous code that does this.
 aio_complete
 	Called when the io request on the given iocb is complete.
	
	  Add a completion event to the ring buffer. Must be done holding
	  ctx->completion_lock to prevent other code from messing with the tail
	  pointer since we might be called from irq context.
	 after flagging the request as done, we
	  must never even look at it again
 make event visible before updating tail 
	
	  Check if the user asked us to deliver the result through an
	  eventfd. The eventfd_signal() function is safe to be called
	  from IRQ context.
	
	  We have to order our ring_info tail store above and test
	  of the wait list below outside the wait lock.  This is
	  like in wake_up_bit() where clearing a bit has to be
	  ordered with the unlocked test.
 aio_read_events_ring
 	Pull an event off of the ioctx's event ring.  Returns the number of
 	events fetched
	
	  The mutex can block and wake us up and that will cause
	  wait_event_interruptible_hrtimeout() to schedule without sleeping
	  and repeat. This should be rare enough that it doesn't cause
	  peformance issues. See the comment in read_events() for more detail.
 Access to ->ring_pages here is protected by ctx->ring_lock. 
	
	  Ensure that once we've read the current tail pointer, that
	  we also see the events that were stored up to the tail.
	
	  Note that aio_read_events() is being called as the conditional - i.e.
	  we're calling it after prepare_to_wait() has set task state to
	  TASK_INTERRUPTIBLE.
	 
	  But aio_read_events() can block, and if it blocks it's going to flip
	  the task state back to TASK_RUNNING.
	 
	  This should be ok, provided it doesn't flip the state back to
	  TASK_RUNNING and return 0 too much - that causes us to spin. That
	  will only happen if the mutex_lock() call blocks, and we then find
	  the ringbuffer empty. So in practice we should be ok, but it's
	  something to be aware of when touching this code.
 sys_io_setup:
 	Create an aio_context capable of receiving at least nr_events.
 	ctxp must not point to an aio_context that already exists, and
 	must be initialized to 0 prior to the call.  On successful
 	creation of the aio_context, ctxp is filled in with the resulting 
 	handle.  May fail with -EINVAL if ctxp is not initialized,
 	if the specified nr_events exceeds internal limits.  May fail 
 	with -EAGAIN if the specified nr_events exceeds the user's limit 
 	of available events.  May fail with -ENOMEM if insufficient kernel
 	resources are available.  May fail with -EFAULT if an invalid
 	pointer is passed for ctxp.  Will fail with -ENOSYS if not
 	implemented.
 truncating is ok because it's a user address 
 sys_io_destroy:
 	Destroy the aio_context specified.  May cancel any outstanding 
 	AIOs and block on completion.  Will fail with -ENOSYS if not
 	implemented.  May fail with -EINVAL if the context pointed to
 	is invalid.
		 Pass requests_done to kill_ioctx() where it can be set
		  in a thread-safe way. If we try to set it here then we have
		  a race condition if two io_destroy() called simultaneously.
		 Wait until all IO for the context are done. Otherwise kernel
		  keep using user-space buffers even if user thinks the context
		  is destroyed.
		
		  Tell lockdep we inherited freeze protection from submission
		  thread.
		
		  If the IOCB_FLAG_IOPRIO flag of aio_flags is set, then
		  aio_reqprio is interpreted as an IO scheduling
		  class and priority.
 no one is going to poll for this IO 
		
		  There's no easy way to restart the syscall since other AIO's
		  may be already running. Just fail this IO with EINTR.
		
		  Open-code file_start_write here to grab freeze protection,
		  which will be released by another thread in
		  aio_complete_rw().  Fool lockdep by telling it the lock got
		  released so that it doesn't complain about the held lock when
		  we return to userspace.
	
	  Note that ->ki_cancel callers also delete iocb from active_reqs after
	  calling ->ki_cancel.  We need the ctx_lock roundtrip here to
	  synchronize with them.  In the cancellation case the list_del_init
	  itself is not actually needed, but harmless so we keep it in to
	  avoid further branches in the fast path.
 assumes we are called with irqs disabled 
 for instances that support it check for an event match first: 
		
		  Try to complete the iocb inline if we can. Use
		  irqsaveirqrestore because not all filesystems (e.g. fuse)
		  call this function with IRQs disabled and because IRQs
		  have to be disabled before ctx_lock is obtained.
 multiple wait queues per file are not supported 
 reject any unknown events outside the normal event mask. 
 reject fields that are not defined for poll 
 same as no support for IOCB_CMD_POLL 
 initialized the list so that we can do list_empty checks 
 actually waiting for an event 
 no async, we'd stolen it 
		
		  If the IOCB_FLAG_RESFD flag of aio_flags is set, get an
		  instance of the file now. The file descriptor must be
		  an eventfd() fd, and will be signaled for each completed
		  event using the eventfd_signal() function.
 enforce forwards compatibility on users 
 prevent overflows 
 Done with the synchronous reference 
	
	  If err is 0, we'd either done aio_complete() ourselves or have
	  arranged for that to be done asynchronously.  Anything non-zero
	  means that we need to destroy req ourselves.
 sys_io_submit:
 	Queue the nr iocbs pointed to by iocbpp for processing.  Returns
 	the number of iocbs queued.  May return -EINVAL if the aio_context
 	specified by ctx_id is invalid, if nr is < 0, if the iocb at
 	iocbpp[0] is not properly initialized, if the operation specified
 	is invalid for the file descriptor in the iocb.  May fail with
 	-EFAULT if any of the data structures point to invalid data.  May
 	fail with -EBADF if the file descriptor specified in the first
 	iocb is invalid.  May fail with -EAGAIN if insufficient resources
 	are available to queue any iocbs.  Will return 0 if nr is 0.  Will
 	fail with -ENOSYS if not implemented.
 sys_io_cancel:
 	Attempts to cancel an iocb previously passed to io_submit.  If
 	the operation is successfully cancelled, the resulting event is
 	copied into the memory pointed to by result without being placed
 	into the completion queue and 0 is returned.  May fail with
 	-EFAULT if any of the data structures pointed to are invalid.
 	May fail with -EINVAL if aio_context specified by ctx_id is
 	invalid.  May fail with -EAGAIN if the iocb specified was not
 	cancelled.  Will fail with -ENOSYS if not implemented.
 TODO: use a hash or array, this sucks. 
		
		  The result argument is no longer used - the io_event is
		  always delivered via the ring buffer. -EINPROGRESS indicates
		  cancellation is progress:
 io_getevents:
 	Attempts to read at least min_nr events and up to nr events from
 	the completion queue for the aio_context specified by ctx_id. If
 	it succeeds, the number of read events is returned. May fail with
 	-EINVAL if ctx_id is invalid, if min_nr is out of range, if nr is
 	out of range, if timeout is out of range.  May fail with -EFAULT
 	if any of the memory specified is invalid.  May return 0 or
 	< min_nr if the timeout specified by timeout has elapsed
 	before sufficient events are available, where timeout == NULL
 	specifies an infinite timeout. Note that the timeout pointed to by
 	timeout is relative.  Will fail with -ENOSYS if not implemented.
 SPDX-License-Identifier: GPL-2.0-only
  "splice": joining two ropes together by interweaving their strands.
  This is the "extended pipe" functionality, where a pipe is used as
  an arbitrary in-memory buffer. Think of a pipe as a small kernel
  buffer that you can use to transfer data from one end to the other.
  The traditional unix readwrite is extended with a "splice()" operation
  that transfers data buffers to or from a pipe buffer.
  Named by Larry McVoy, original implementation from Linus, extended by
  Jens to support splicing to files, network, direct splicing, etc and
  fixing lots of bugs.
  Copyright (C) 2005-2006 Jens Axboe <axboe@kernel.dk>
  Copyright (C) 2005-2006 Linus Torvalds <torvalds@osdl.org>
  Copyright (C) 2006 Ingo Molnar <mingo@elte.hu>
  Attempt to steal a page from a pipe buffer. This should perhaps go into
  a vm helper function, it's already simplified quite a bit by the
  addition of remove_mapping(). If success is returned, the caller may
  attempt to reuse this page for another destination.
		
		  At least for ext2 with nobh option, we need to wait on
		  writeback completing on this page, since we'll remove it
		  from the pagecache.  Otherwise truncate wont wait on the
		  page, allowing the disk blocks to be reused by someone else
		  before we actually wrote our data to them. fs corruption
		  ensues.
		
		  If we succeeded in removing the mapping, set LRU flag
		  and return good.
	
	  Raced with truncate or failed to remove page from current
	  address space, unlock and return failure.
  Check whether the contents of buf is OK to access. Since the content
  is a page cache page, IO may be in flight.
		
		  Page got truncatedunhashed. This will cause a 0-byte
		  splice, if this is the first page.
		
		  Uh oh, read-error from disk.
		
		  Page is ok afterall, we are done.
  splice_to_pipe - fill passed data into a pipe
  @pipe:	pipe to fill
  @spd:	data to fill
  Description:
     @spd contains a map of pages and lenoffset tuples, along with
     the struct pipe_buf_operations associated with these pages. This
     function will link that data to the pipe.
  Check if we need to grow the arrays holding pages and partial page
  descriptions.
  generic_file_splice_read - splice data from file to a pipe
  @in:		file to splice from
  @ppos:	position in @in
  @pipe:	pipe to splice to
  @len:	number of bytes to splice
  @flags:	splice modifier flags
  Description:
     Will read pages from given file and fill them into a pipe. Can be
     used as long as it has more or less sane ->read_iter().
 to free what was emitted 
		
		  callers of ->splice_read() expect -EAGAIN on
		  "can't put anything in there", rather than -EFAULT.
 Pipe buffer operations for a socket and similar. 
  Send 'sd->len' bytes to socket from 'sd->file' at position 'sd->pos'
  using sendpage(). Return the number of bytes sent.
  splice_from_pipe_feed - feed available data from a pipe to a file
  @pipe:	pipe to splice from
  @sd:		information to @actor
  @actor:	handler that splices the data
  Description:
     This function loops over the pipe and calls @actor to do the
     actual moving of a single struct pipe_buffer to the desired
     destination.  It returns when there's no more buffers left in
     the pipe or if the requested number of bytes (@sd->total_len)
     have been copied.  It returns a positive number (one) if the
     pipe needs to be filled with more data, zero if the required
     number of bytes have been copied and -errno on error.
     This, together with splice_from_pipe_{begin,end,next}, may be
     used to implement the functionality of __splice_from_pipe() when
     locking is required around copying the pipe buffers to the
     destination.
 We know we have a pipe buffer, but maybe it's empty? 
  splice_from_pipe_next - wait for some data to splice from
  @pipe:	pipe to splice from
  @sd:		information about the splice operation
  Description:
     This function will wait for some data and return a positive
     value (one) if pipe buffers are available.  It will return zero
     or -errno if no more data needs to be spliced.
	
	  Check for signal early to make process killable when there are
	  always buffers available
  splice_from_pipe_begin - start splicing from pipe
  @sd:		information about the splice operation
  Description:
     This function should be called before a loop containing
     splice_from_pipe_next() and splice_from_pipe_feed() to
     initialize the necessary fields of @sd.
  splice_from_pipe_end - finish splicing from pipe
  @pipe:	pipe to splice from
  @sd:		information about the splice operation
  Description:
     This function will wake up pipe writers if necessary.  It should
     be called after a loop containing splice_from_pipe_next() and
     splice_from_pipe_feed().
  __splice_from_pipe - splice data from a pipe to given actor
  @pipe:	pipe to splice from
  @sd:		information to @actor
  @actor:	handler that splices the data
  Description:
     This function does little more than loop over the pipe and call
     @actor to do the actual moving of a single struct pipe_buffer to
     the desired destination. See pipe_to_file, pipe_to_sendpage, or
     pipe_to_user.
  splice_from_pipe - splice data from a pipe to a file
  @pipe:	pipe to splice from
  @out:	file to splice to
  @ppos:	position in @out
  @len:	how many bytes to splice
  @flags:	splice modifier flags
  @actor:	handler that splices the data
  Description:
     See __splice_from_pipe. This function locks the pipe inode,
     otherwise it's identical to __splice_from_pipe().
  iter_file_splice_write - splice data from a pipe to a file
  @pipe:	pipe info
  @out:	file to write to
  @ppos:	position in @out
  @len:	number of bytes to splice
  @flags:	splice modifier flags
  Description:
     Will either move or copy pages (determined by @flags options) from
     the given pipe inode to the given file.
     This one is ->write_iter-based.
 build the vector 
 zero-length bvecs are not supported, skip them 
 dismiss the fully eaten buffers, adjust the partial one 
  generic_splice_sendpage - splice data from a pipe to a socket
  @pipe:	pipe to splice from
  @out:	socket to write to
  @ppos:	position in @out
  @len:	number of bytes to splice
  @flags:	splice modifier flags
  Description:
     Will send @len bytes from the pipe to a network socket. No data copying
     is involved.
  Attempt to initiate a splice from pipe to file.
  Attempt to initiate a splice from a file to a pipe.
 Don't try to read more the pipe has space for. 
  splice_direct_to_actor - splices data directly between two non-pipes
  @in:		file to splice from
  @sd:		actor information on where to splice to
  @actor:	handles the data splicing
  Description:
     This is a special case helper to splice directly between two
     points, without requiring an explicit pipe. Internally an allocated
     pipe is cached in the process, and reused during the lifetime of
     that process.
	
	  We require the input being a regular file, as we don't want to
	  randomly drop data for eg socket -> socket splicing. Use the
	  piped splicing for that!
	
	  neither in nor out is a pipe, setup an internal pipe attached to
	  'out' and transfer the wanted data from 'in' to 'out' through that
		
		  We don't have an immediate reader, but we'll read the stuff
		  out of the pipe right after the splice_to_pipe(). So set
		  PIPE_READERS appropriately.
	
	  Do the splice.
	
	  Don't block on output, we have to drain the direct pipe.
		
		  If more data is pending, set SPLICE_F_MORE
		  If this is the last data and SPLICE_F_MORE was not set
		  initially, clears it.
		
		  NOTE: nonblocking mode only applies to the input. We
		  must not do the output in nonblocking mode as then we
		  could get stuck data in the internal pipe:
	
	  If we did an incomplete transfer we must release
	  the pipe buffers in question:
  do_splice_direct - splices data directly between two files
  @in:		file to splice from
  @ppos:	input file offset
  @out:	file to splice to
  @opos:	output file offset
  @len:	number of bytes to splice
  @flags:	splice modifier flags
  Description:
     For use by do_sendfile(). splice can easily emulate sendfile, but
     doing it in the application would incur an extra system call
     (splice in + splice out, as compared to just sendfile()). So this helper
     can splice directly through a process-private pipe.
  Determine where to splice tofrom.
 Splicing to self would be fun, but... 
  For lack of a better implementation, implement vmsplice() to userspace
  as a simple copy of the pipes pages to the user iov.
  vmsplice splices a user address range into a pipe. It can be thought of
  as splice-from-memory, where the regular splice is splice-from-file (or
  to file). In both cases the output is a pipe, naturally.
  Note that vmsplice only really supports true splicing _from_ user memory
  to a pipe, not the other way around. Splicing from user memory is a simple
  operation that can be supported without any funky alignment restrictions
  or nasty vm tricks. We simply map in the user memory and fill them into
  a pipe. The reverse isn't quite as easy, though. There are two possible
  solutions for that:
 	- memcpy() the data internally, at which point we might as well just
 	  do a regular read() on the buffer anyway.
 	- Lots of nasty vm tricks, that are neither fast nor flexible (it
 	  has restriction limitations on both ends of the pipe).
  Currently we punt and implement it as a normal copy, see pipe_to_user().
  Make sure there's data to read. Wait for input if we can, otherwise
  return an appropriate error.
	
	  Check the pipe occupancy without the inode lock first. This function
	  is speculative anyways, so missing one is ok.
  Make sure there's writeable room. Wait for room if we can, otherwise
  return an appropriate error.
	
	  Check pipe occupancy without the inode lock first. This function
	  is speculative anyways, so missing one is ok.
  Splice contents of ipipe to opipe.
	
	  Potential ABBA deadlock, work around it by ordering lock
	  grabbing by pipe info address. Otherwise two different processes
	  could deadlock (one doing tee from A -> B, the other from B -> A).
		
		  Cannot make any progress, because either the input
		  pipe is empty or the output pipe is full.
 Already processed some buffers, break 
			
			  We raced with another readerwriter and haven't
			  managed to process any buffers.  A zero return
			  value means EOF, so retry instead.
			
			  Simply move the whole buffer from ipipe to opipe
			
			  Get a reference to this pipe buffer,
			  so we can copy the contents over.
			
			  Don't inherit the gift and merge flags, we need to
			  prevent multiple steals of this page.
	
	  If we put data in the output pipe, wakeup any potential readers.
  Link contents of ipipe to opipe.
	
	  Potential ABBA deadlock, work around it by ordering lock
	  grabbing by pipe info address. Otherwise two different processes
	  could deadlock (one doing tee from A -> B, the other from B -> A).
		
		  If we have iterated all input buffers or run out of
		  output room, break.
		
		  Get a reference to this pipe buffer,
		  so we can copy the contents over.
		
		  Don't inherit the gift and merge flag, we need to prevent
		  multiple steals of this page.
	
	  If we put data in the output pipe, wakeup any potential readers.
  This is a tee(1) implementation that works on pipes. It doesn't copy
  any data, it simply references the 'in' pages on the 'out' pipe.
  The 'flags' used are the SPLICE_F_ variants, currently the only
  applicable one is SPLICE_F_NONBLOCK.
	
	  Duplicate the contents of ipipe to opipe without actually
	  copying the data.
		
		  Keep going, unless we encounter an error. The ipipeopipe
		  ordering doesn't really matter.
 SPDX-License-Identifier: GPL-2.0
 OK, we know p couldn't have been freed yet 
 SPDX-License-Identifier: GPL-2.0-only
  Mbcache is a simple key-value store. Keys need not be unique, however
  key-value pairs are expected to be unique (we use this fact in
  mb_cache_entry_delete()).
  Ext2 and ext4 use this cache for deduplication of extended attribute blocks.
  Ext4 also uses it for deduplication of xattr values stored in inodes.
  They use hash of data as a key and provide a value that may represent a
  block or inode number. That's why keys need not be unique (hash of different
  data may be the same). However user provided value always uniquely
  identifies a cache entry.
  We provide functions for creation and removal of entries, search by key,
  and a special "delete entry with given key-value pair" operation. Fixed
  size hash table is used for fast key lookups.
 Hash table of entries 
 log2 of hash table size 
 Maximum entries in cache to avoid degrading hash too much 
 Protects c_list, c_entry_count 
 Number of entries in cache 
 Work for shrinking when the cache has too many entries 
  Number of entries to reclaim synchronously when there are too many entries
  in cache
  mb_cache_entry_create - create entry in cache
  @cache - cache where the entry should be created
  @mask - gfp mask with which the entry should be allocated
  @key - key of the entry
  @value - value of the entry
  @reusable - is the entry reusable by others?
  Creates entry in @cache with key @key and value @value. The function returns
  -EBUSY if entry with the same key and value already exists in cache.
  Otherwise 0 is returned.
 Schedule background reclaim if there are too many entries 
 Do some sync reclaim if background reclaim cannot keep up 
 One ref for hash, one ref returned 
 Grab ref for LRU list 
  mb_cache_entry_find_first - find the first reusable entry with the given key
  @cache: cache where we should search
  @key: key to look for
  Search in @cache for a reusable entry with key @key. Grabs reference to the
  first reusable entry found and returns the entry.
  mb_cache_entry_find_next - find next reusable entry with the same key
  @cache: cache where we should search
  @entry: entry to start search from
  Finds next reusable entry in the hash chain which has the same key as @entry.
  If @entry is unhashed (which can happen when deletion of entry races with the
  search), finds the first reusable entry in the hash chain. The function drops
  reference to @entry and returns with a reference to the found entry.
  mb_cache_entry_get - get a cache entry by value (and key)
  @cache - cache we work with
  @key - key
  @value - value
 mb_cache_entry_delete - remove a cache entry
  @cache - cache we work with
  @key - key
  @value - value
  Remove entry from cache @cache with key @key and value @value.
 We keep hash list reference to keep entry alive 
 mb_cache_entry_touch - cache entry got used
  @cache - cache the entry belongs to
  @entry - entry that got used
  Marks entry as used to give hit higher chances of surviving in cache.
 Shrink number of entries in cache 
		
		  We keep LRU list reference so that entry doesn't go away
		  from under us.
 We shrink 1X of the cache when we have too many entries in it 
  mb_cache_create - create cache
  @bucket_bits: log2 of the hash table size
  Create cache for keys with 2^bucket_bits hash entries.
  mb_cache_destroy - destroy cache
  @cache: the cache to destroy
  Free all entries in cache and cache itself. Caller must make sure nobody
  (except shrinker) can reach @cache when calling this.
	
	  We don't bother with any locking. Cache must not be used at this
	  point.
 SPDX-License-Identifier: GPL-2.0-only
  kernel_read_file() - read file contents into a kernel buffer
  @file	file to read from
  @offset	where to start reading from (see below).
  @buf		pointer to a "void " buffer for reading into (if
 		@buf is NULL, a buffer will be allocated, and
 		@buf_size will be ignored)
  @buf_size	size of buf, if already allocated. If @buf not
 		allocated, this is the largest size to allocate.
  @file_size	if non-NULL, the full size of @file will be
 		written here.
  @id		the kernel_read_file_id identifying the type of
 		file contents being read (for LSMs to examine)
  @offset must be 0 unless both @buf and @file_size are non-NULL
  (i.e. the caller must be expecting to read partial file contents
  via an already-allocated @buf, in at most @buf_size chunks, and
  will be able to determine when the entire file was read by
  checking @file_size). This isn't a recommended way to read a
  file, though, since it is possible that the contents might
  change between calls to kernel_read_file().
  Returns number of bytes read (no single read will be bigger
  than INT_MAX), or negative on error.
 The file is too big for sane activities. 
 The entire file cannot be read in one buffer. 
 SPDX-License-Identifier: GPL-2.0-only
   linuxfsexec.c
   Copyright (C) 1991, 1992  Linus Torvalds
  #!-checking implemented by tytso.
  Demand-loading implemented 01.12.91 - no need to read anything but
  the header into memory. The inode of the executable is put into
  "current->executable", and page faults do the actual loading. Clean.
  Once more I can proudly say that linux stood up to being changed: it
  was less than 2 hours work to get demand-loading completely implemented.
  Demand loading changed July 1993 by Eric Youngdale.   Use mmap instead,
  current->executable is only used by the procfs.  This allows a dispatch
  table to check for several different types  of binary formats.  We keep
  trying until we recognize the file or we run out of supported binary
  formats.
  Note that a shared library must be both readable and executable due to
  security reasons.
  Also note that we take the address to load from from the file itself.
	
	  may_open() has already checked for this, so it should be
	  impossible to trip now. But we need to be extra cautious
	  and check again at the very end too.
 #ifdef CONFIG_USELIB 
  The nascent bprm->mm is not visible until exec_mmap() but it can
  use a lot of memory, account these pages in current->mm temporary
  for oom_badness()->get_mm_rss(). Once exec succeeds or fails, we
  change the counter back via acct_arg_size(0).
	
	  We are doing an exec().  'current' is the process
	  doing the exec and bprm->mm is the new process's mm.
	
	  Place the stack at the largest stack address the architecture
	  supports. Later, we'll move this to an appropriate place. We don't
	  use STACK_TOP because that can depend on attributes which aren't
	  configured yet.
 CONFIG_MMU 
  Create a new mm_struct and populate it with a temporary stack
  vm_area_struct.  We don't have enough context at this point to set the stack
  flags, permissions, and offset, so we use temporary values.  We'll update
  them later in setup_arg_pages().
 Save current stack limit for all calculations made during exec. 
  count() counts the number of strings in array ARGV.
	
	  Limit to 14 of the max stack size or 34 of _STK_LIM
	  (whichever is smaller) for the argv+env strings.
	  This ensures that:
	   - the remaining binfmt code will not run out of stack space,
	   - the program will have a reasonable amount of stack left
	     to work from.
	
	  We've historically supported up to 32 pages (ARG_MAX)
	  of argument strings even with small stacks
	
	  We must account for the size of all the argv and envp pointers to
	  the argv and envp strings, since they will also take up space in
	  the stack. They aren't stored until much later when we can't
	  signal to the parent that the child has run out of stack space.
	  Instead, calculate it here so it's possible to fail gracefully.
  'copy_strings()' copies argumentenvironment strings from the old
  processes's memory to the new process's stack.  The call to get_user_pages()
  ensures the destination page is created and not swapped out.
 We're going to work our way backwords. 
  Copy and argumentenvironment string from the kernel to the processes stack.
 terminating NUL ;
 We're going to work our way backwards. 
  During bprm_mm_init(), we create a temporary stack at STACK_TOP_MAX.  Once
  the binfmt code determines where the new stack should reside, we shift it to
  its final location.  The process proceeds as follows:
  1) Use shift to calculate the new vma endpoints.
  2) Extend vma to cover both the old and new ranges.  This ensures the
     arguments passed to subsequent functions are consistent.
  3) Move vma's page tables to the new range.
  4) Free up any cleared pgd range.
  5) Shrink the vma to cover only the new range.
	
	  ensure there are no vmas between where we want to go
	  and where we are
	
	  cover the whole range: [new_start, old_end)
	
	  move the page tables downwards, on failure we rely on
	  process cleanup to remove whatever mess we made.
		
		  when the old and new regions overlap clear from new_end.
		
		  otherwise, clean from old_start; this is done to not touch
		  the address space in [new_end, old_start) some architectures
		  have constraints on va-space that make this illegal (IA64) -
		  for the others its just a little faster.
	
	  Shrink the vma to just the new range.  Always succeeds.
  Finalizes the stack vm_area_struct. The flags and permissions are updated,
  the stack is optionally relocated, and some extra space is added.
 Limit stack size 
 Add space for stack randomization. 
 Make sure we didn't let the argument array grow too large. 
	
	  Adjust stack execute permissions; explicitly enable for
	  EXSTACK_ENABLE_X, disable for EXSTACK_DISABLE_X and leave alone
	  (arch default) otherwise.
 Move stack pages down in memory. 
 mprotect_fixup is overkill to remove the temporary stack flags 
 randomly 324k (or 264k) pages 
	
	  Align this down to a page boundary as expand_stack
	  will align it up.
  Transfer the program arguments and environment from the holding pages
  onto the stack. The provided stack pointer is adjusted accordingly.
 CONFIG_MMU 
	
	  may_open() has already checked for this, so it should be
	  impossible to trip now. But we need to be extra cautious
	  and check again at the very end too.
  Maps the mm_struct mm into the current task struct.
  On success, this function returns with exec_update_lock
  held for writing.
 Notify parent that we're no longer interested in the old VM 
		
		  If there is a pending fatal signal perhaps a signal
		  whose default action is to create a coredump get
		  out and die instead of going through with the exec.
	
	  This prevents preemption while active_mm is being loaded and
	  it and mm are being updated, which could cause problems for
	  lazy tlb mm refcounting when these are updated by context
	  switches. Not all architectures can handle irqs off over
	  activate_mm yet.
	
	  Kill all other threads in the thread group.
		
		  Another group action in progress, just
		  return so that the signal is processed.
	
	  At this point all other threads have exited, all we have to
	  do is to wait for the thread group leader to become inactive,
	  and to assume its PID:
			
			  Do this under tasklist_lock to ensure that
			  exit_notify() can't miss ->group_exit_task
		
		  The only record we have of the real-time age of a
		  process, regardless of execs it's done, is start_time.
		  All the past CPU time is accumulated in signal_struct
		  from sister threads now dead.  But in this non-leader
		  exec, nothing survives from the original leader thread,
		  whose birth marks the true age of this process now.
		  When we take on its identity by switching to its PID, we
		  also take its birthdate (always earlier than our own).
		
		  An exec() starts a new thread group with the
		  TGID of the previous thread group. Rehash the
		  two threads with a switched PID, and release
		  the former thread group leader:
		 Become a process group leader with the old leader's pid.
		  The old leader becomes a thread of the this thread group.
		
		  We are going to release_task()->ptrace_unlink() silently,
		  the tracer can sleep in do_wait(). EXIT_DEAD guarantees
		  the tracer wont't block again waiting for this thread.
 we have changed execution domain 
 protects against exit_notify() and __exit_signal() 
  This function makes sure the current process has its own signal table,
  so that flush_signal_handlers can later reset the handlers without
  disturbing other processes.  (Other processes might share the signal
  table via the CLONE_SIGHAND option to clone().)
		
		  This ->sighand is shared with the CLONE_SIGHAND
		  but not CLONE_THREAD task, switch to the new one.
  These functions flushes out all traces of the currently running executable
  so that a new one can be started
  Calling this is the point of no return. None of the failures will be
  seen by userspace since either the process is already taking a fatal
  signal (via de_thread() or coredump), or will have SEGV raised
  (after exec_mmap()) by search_binary_handler (see below).
 Once we are committed compute the creds 
	
	  Ensure all future errors are fatal.
	
	  Make this the only thread in the thread group.
	
	  Cancel any io_uring activity across execve
 Ensure the files table is not shared. 
	
	  Must be called _before_ exec_mmap() as bprm->mm is
	  not visibile until then. This also enables the update
	  to be lockless.
 If the binary is not readable then enforce mm->dumpable=0 
	
	  Release all of the old mmap stuff
	
	  Make the signal table private.
	
	  Ensure that the uaccess routines can actually operate on userspace
	  pointers:
	
	  We have to apply CLOEXEC before we change whether the process is
	  dumpable (in setup_new_exec) to avoid a race with a process in userspace
	  trying to access the should-be-closed file descriptors of a process
	  undergoing exec(2).
 Make sure parent cannot signal privileged process. 
		
		  For secureexec, reset the stack limit to sane default to
		  avoid bad behavior from the prior rlimits. This has to
		  happen before arch_pick_mmap_layout(), which examines
		  RLIMIT_STACK, but after the point of no return to avoid
		  needing to clean up the change on failure.
	
	  Figure out dumpability. Note that this checking only of current
	  is wrong, but userspace depends on it. This should be testing
	  bprm->secureexec instead.
	 An exec changes our domain. We are no longer part of the thread
	
	  install the new credentials for this executable
	
	  Disable monitoring for regular users
	  when executing setuid binaries. Must
	  wait until new credentials are committed
	  by commit_creds() above
	
	  cred_guard_mutex must be held at least to this point to prevent
	  ptrace_attach() from altering our determination of the task's
	  credentials; any time after this it may be unlocked.
 Pass the opened binary to the interpreter. 
 Ensure mm->user_ns contains the executable 
 Setup things that can depend upon the personality 
	 Set the new mm task size. We have to do that late because it may
	  depend on TIF_32BIT which is only updated in flush_thread() on
	  some architectures like powerpc
 Runs immediately before start_thread() takes over. 
 Store any stack rlimit changes before starting thread. 
  Prepare credentials and lock ->cred_guard_mutex.
  setup_new_exec() commits the new creds and drops the lock.
  Or, if exec fails before, free_bprm() should release ->cred
  and unlock.
 If a binfmt changed the interp, free it. 
 If a binfmt changed the interp, free it first. 
  determine how safe it is to execute the proposed program
  - the caller must hold ->cred_guard_mutex to protect against
    PTRACE_ATTACH or seccomp thread-sync
	
	  This isn't strictly necessary, but it makes it harder for LSMs to
	  mess up.
 Handle suid and sgid on files 
 Be careful if suidsgid is set 
 reload atomically modeuidgid now that lock held 
 We ignore suidsgid if there are no mappings for them in the ns 
  Compute brpm->cred based upon the final binary.
 Compute creds based on which file? 
  Fill the binprm structure from the inode.
  Read the first BINPRM_BUF_SIZE bytes
  This may be called multiple times for binary chains (scripts for example).
  Arguments are '\0' separated strings found at the location bprm->p
  points to; chop off the first by relocating brpm->p to right after
  the first '\0' encountered.
  cycle the list of binary formats handler, until one recognizes the image
 Need to fetch pid before load_binary changes it 
 This allows 4 levels of binfmt rewrites before failing hard. 
  sys_execve() executes a new program.
	
	  Record that a name derived from an O_CLOEXEC fd will be
	  inaccessible after exec.  This allows the code in exec to
	  choose to fail when the executable is not mmaped into the
	  interpreter and an open file descriptor is not passed to
	  the interpreter.  This makes for a better user experience
	  than having the interpreter start and then immediately fail
	  when it finds the executable is inaccessible.
 Set the unchanging part of bprm->cred 
 execve succeeded 
	
	  If past the point of no return ensure the code never
	  returns to the userspace process.  Use an existing fatal
	  signal if present otherwise terminate the process with
	  SIGSEGV.
	
	  We move the actual failure in case of RLIMIT_NPROC excess from
	  setuid() to execve() because too many poorly written programs
	  don't check setuid() return code.  Here we additionally recheck
	  whether NPROC limit is still exceeded.
	 We're below the limit (still or again), so we don't want to make
  set_dumpable stores three-value SUID_DUMP_ into mm->flags.
 SPDX-License-Identifier: GPL-2.0
  Routines that mimic syscalls, but don't use the user address space or file
  descriptors.  Only for init and related early init code.
 SPDX-License-Identifier: GPL-2.0-only
  linuxfsbinfmt_elf.c
  These are the functions used to load ELF format executables as used
  on SVr4 machines.  Information on the format may be found in the book
  "UNIX SYSTEM V RELEASE 4 Programmers Guide: Ansi C and Programming Support
  Tools".
  Copyright 1993, 1994: Eric Youngdale (ericy@cais.com).
 That's for binfmt_elf_fdpic to deal with 
  If we don't support core dumping, then supply a NULL so we
  don't even try.
		
		  Map the last of the bss segment.
		  If the header is requesting these pages to be
		  executable, honour that (ppc32 needs this).
 We need to explicitly zero any fractional pages
   after the data section (i.e. bss).  This would
   contain the junk from the file that should not
   be in memory
 Let's use some macros to make this stack manipulation a little clearer 
  AT_BASE_PLATFORM indicates the "real" hardwaremicroarchitecture.
  If the arch defines ELF_BASE_PLATFORM (in asmelf.h), the value
  will be copied to the user stack in the same manner as AT_PLATFORM.
	
	  In some cases (e.g. Hyper-Threading), we want to avoid L1
	  evictions by the processes running on the same package. One
	  thing we can do is to shuffle the initial stack for them.
	
	  If this architecture has a platform capability string, copy it
	  to userspace.  In some cases (Sparc), this info is impossible
	  for userspace to get any other way, in others (i386) it is
	  merely difficult.
	
	  If this architecture has a "base" platform capability
	  string, copy it to userspace.
	
	  Generate 16 random bytes for userspace PRNG seeding.
 Create the ELF interpreter info 
 update AT_VECTOR_SIZE_BASE if the number of NEW_AUX_ENT() changes 
	 
	  ARCH_DLINFO must come first so PPC can do its special alignment of
	  AUXV.
	  update AT_VECTOR_SIZE_ARCH if the number of NEW_AUX_ENT() in
	  ARCH_DLINFO changes
 AT_NULL is zero; clear the rest too 
 And advance past the AT_NULL entry.  
 Point sp at the lowest address on the stack 
 XXX: PARISC HACK 
	
	  Grow the stack manually; some architectures have a limit on how
	  far ahead a user-space access may be in order to grow the stack.
 Now, let's put argc (and argv, envp if appropriate) on the stack 
 Populate list of argv pointers back to argv strings. 
 Populate list of envp pointers back to envp strings. 
 Put the elf_info on the stack in the right place.  
	 mmap() will return -EINVAL if given a zero size, but a
	
	 total_size is the size of the ELF (interpreter) image.
	 The _first_ mmap needs to know the full size, otherwise
	 randomization might put this image into an overlapping
	 position with the ELF binary image. (since size < total_size)
	 So we first map the 'big' image - and unmap the remainder at
	 the end. (which unmap is needed for ELF images with holes.)
 skip non-power of two alignments as invalid 
 ensure we align to at least one page 
  load_elf_phdrs() - load ELF program headers
  @elf_ex:   ELF header of the binary whose program headers should be loaded
  @elf_file: the opened ELF binary file
  Loads ELF program headers from the binary file elf_file, which has the ELF
  header pointed to by elf_ex, into a newly allocated array. The caller is
  responsible for freeing the allocated data. Returns an ERR_PTR upon failure.
	
	  If the size of this structure has changed, then punt, since
	  we will be doing the wrong thing.
 Sanity check the number of program headers... 
 ...and their total size. 
 Read in the program headers 
 Success! 
  struct arch_elf_state - arch-specific ELF loading state
  This structure is used to preserve architecture specific data during
  the loading of an ELF file, throughout the checking of architecture
  specific ELF headers & through to the point where the ELF load is
  known to be proceeding (ie. SET_PERSONALITY).
  This implementation is a dummy for architectures which require no
  specific state.
  arch_elf_pt_proc() - check a PT_LOPROC..PT_HIPROC ELF program header
  @ehdr:	The main ELF header
  @phdr:	The program header to check
  @elf:	The open ELF file
  @is_interp:	True if the phdr is from the interpreter of the ELF being
 		loaded, else false.
  @state:	Architecture-specific state preserved throughout the process
 		of loading the ELF.
  Inspects the program header phdr to validate its correctness andor
  suitability for the system. Called once per ELF program header in the
  range PT_LOPROC to PT_HIPROC, for both the ELF being loaded and its
  interpreter.
  Return: Zero to proceed with the ELF load, non-zero to fail the ELF load
          with that return code.
 Dummy implementation, always proceed 
  arch_check_elf() - check an ELF executable
  @ehdr:	The main ELF header
  @has_interp:	True if the ELF has an interpreter, else false.
  @interp_ehdr: The interpreter's ELF header
  @state:	Architecture-specific state preserved throughout the process
 		of loading the ELF.
  Provides a final opportunity for architecture code to reject the loading
  of the ELF & cause an exec syscall to return an error. This is called after
  all program headers to be checked by arch_elf_pt_proc have been.
  Return: Zero to proceed with the ELF load, non-zero to fail the ELF load
          with that return code.
 Dummy implementation, always proceed 
 !CONFIG_ARCH_BINFMT_ELF_STATE 
 This is much more generalized than the library routine read function,
   so we keep this separate.  Technically the library read function
   is only provided so that we can read a.out libraries that have
 First of all, some simple consistency checks 
			
			  Check to see if the section's size will overflow the
			  allowed task size. Note that p_filesz must always be
			  <= p_memsize so it's only necessary to check p_memsz.
			
			  Find the end of the file mapping for this phdr, and
			  keep track of the largest address we see for this.
			
			  Do the same thing for the memory mapping - between
			  elf_bss and last_bss is the bss section.
	
	  Now fill out the bss section: first pad the last page from
	  the file up to the page boundary, and zero it from elf_bss
	  up to the end of the page.
	
	  Next, align both the file and mem bss up to the page size,
	  since this is where elf_bss was just zeroed up to, and where
	  last_bss will end after the vm_brk_flags() below.
 Finally, if there is still more bss to allocate, do it. 
  These are the functions used to load ELF style executables and shared
  libraries.  There is no binary dependent code anywhere else.
 Properties are supposed to be unique and sorted on pr_type: 
 load_elf_binary() shouldn't call us unless this is true... 
 If the properties are crazy large, that's too bad (for now): 
 to shut gcc up 
 First of all, some simple consistency checks 
		
		  This is the program interpreter used for shared libraries -
		  for now assume that this is an a.out format binary.
 make sure path is NULL terminated 
		
		  If the binary is not readable then enforce mm->dumpable = 0
		  regardless of the interpreter's permissions.
 Get the exec headers 
 Some simple consistency checks for the interpreter 
 Not an ELF interpreter 
 Verify the interpreter has a valid arch 
 Load the interpreter program headers 
 Pass PT_LOPROC..PT_HIPROC headers to arch code 
	
	  Allow arch code to reject the ELF at this point, whilst it's
	  still possible to return an error to the code that invoked
	  the exec syscall.
 Flush all traces of the currently running executable 
	 Do this immediately, since STACK_TOP as used in setup_arg_pages
	 Do this so that we can load the interpreter, if need be.  We will
	 Now we do a little grungy work by mmapping the ELF image into
			 There was a PT_LOAD segment with p_memsz > p_filesz
			   before this one. Map anonymous pages, if needed,
					
					  This bss-zeroing can fail if the ELF
					  file specifies odd protections. So
					  we don't check the return value
		
		  The first time through the loop, load_addr_set is false:
		  layout will be calculated. Once set, use MAP_FIXED since
		  we know we've already safely mapped the entire region with
		  MAP_FIXED_NOREPLACE in the once-per-binary logic following.
			
			  This logic is run once for the first LOAD Program
			  Header for ET_EXEC binaries. No special handling
			  is needed.
			
			  This logic is run once for the first LOAD Program
			  Header for ET_DYN binaries to calculate the
			  randomization (load_bias) for all the LOAD
			  Program Headers.
			 
			  There are effectively two types of ET_DYN
			  binaries: programs (i.e. PIE: ET_DYN with INTERP)
			  and loaders (ET_DYN without INTERP, since they
			  _are_ the ELF interpreter). The loaders must
			  be loaded away from programs since the program
			  may otherwise collide with the loader (especially
			  for ET_EXEC which does not have a randomized
			  position). For example to handle invocations of
			  ".ld.so someprog" to test out a new version of
			  the loader, the subsequent program that the
			  loader loads must avoid the loader itself, so
			  they cannot share the same load range. Sufficient
			  room for the brk must be allocated with the
			  loader as well, since brk must be available with
			  the loader.
			 
			  Therefore, programs are loaded offset from
			  ELF_ET_DYN_BASE and loaders are loaded into the
			  independently randomized mmap region (0 load_bias
			  without MAP_FIXED nor MAP_FIXED_NOREPLACE).
			
			  Since load_bias is used for all subsequent loading
			  calculations, we must lower it by the first vaddr
			  so that the remaining calculations based on the
			  ELF vaddrs will be correctly offset. The result
			  is then page aligned.
		
		  Calculate the entire size of the ELF mapping (total_size).
		  (Note that load_addr_set is set to true later once the
		  initial mapping is performed.)
		
		  Check to see if the section's size will overflow the
		  allowed task size. Note that p_filesz must always be
		  <= p_memsz so it is only necessary to check p_memsz.
 set_brk can never work. Avoid overflows. 
	 Calling set_brk effectively mmaps the pages that we need
	  for the bss and break sections.  We must do this before
	  mapping in the interpreter, to make sure it doesn't wind
	  up getting placed where the bss needs to go.
 Nobody gets to see this, but.. 
			
			  load_elf_interp() returns relocation
			  adjustment
 ARCH_HAS_SETUP_ADDITIONAL_PAGES 
		
		  For architectures with ELF randomization, when executing
		  a loader directly (i.e. no interpreter listed in ELF
		  headers), move the brk area out of the mmap region
		  (since it grows up, and may collide early with the stack
		  growing down), and into the unused ELF_ET_DYN_BASE region.
		 Why this, you ask???  Well SVr4 maps page 0 as read-only,
		   and some applications "depend" upon this behavior.
		   Since we do not have the power to recompile these, we
	
	  The ABI may specify that certain registers be set up in special
	  ways (on i386 %edx is the address of a DT_FINI function, for
	  example.  In addition, it may also specify (eg, PowerPC64 ELF)
	  that the e_entry field is the address of the function descriptor
	  for the startup routine, rather than the address of the startup
	  routine itself.  This macro performs whatever initialization to
	  the regs structure is required as well as any relocations to the
	  function descriptor entries when executing dynamically links apps.
 error cleanup 
 This is really simpleminded and specialized - we are loading an
 First of all, some simple consistency checks 
 Now read in all of the header information 
 j < ELF_MIN_ALIGN because elf_ex.e_phnum <= 2 
 Now use mmap to map the library into memory. 
 #ifdef CONFIG_USELIB 
  ELF core dumper
  Modelled on fsexec.c:aout_core_dump()
  Jeremy Fitzhardinge <jeremy@sw.oz.au>
 An ELF note in memory 
  fill up all the fields in prstatus from the given task struct, except
  registers which need to be filled up separately.
		
		  This is the record for the group leader.  It shows the
		  group-wide total, not its individual thread total.
 first copy the parameters from user space 
  Format of NT_FILE note:
  long count     -- how many files are mapped
  long page_size -- units for file_ofs
  array of [COUNT] elements of
    long start
    long end
    long file_ofs
  followed by COUNT filenames in ASCII: "FILE1" NUL "FILE2" NUL...
 Estimated file count and total data size needed 
 paranoia check 
	
	  "size" can be 0 here legitimately.
	  Let it ENOMEM and omit NT_FILE section which will be empty anyway.
 file_path() fills at the end, move name down 
 n = strlen(filename) + 1: 
 Now we know exact count of files, can store it 
	
	  Count usually is less than mm->map_count,
	  we need to move filenames down.
  When a regset has a writeback hook, we call it on each thread before
  dumping user memory.  On register window machines, this makes sure the
  user memory backing the register data is up to date before we read it.
	
	  NT_PRSTATUS is the one special case, because the regset data
	  goes into the pr_reg field inside the note contents, rather
	  than being the whole note contents.  We fill the reset in here.
	  We assume that regset 0 is NT_PRSTATUS.
	
	  Each other regset might generate a note too.  For each regset
	  that has no core_note_type or is inactive, we leave t->notes[i]
	  all zero and we'll know to skip writing it later.
 not for coredumps
 So we don't free this wrongly 
	
	  Figure out how many notes we're going to need for each thread.
	
	  Sanity check.  We rely on regset 0 being in NT_PRSTATUS,
	  since it is our one special case.
	
	  Initialize the ELF file header.
	
	  Allocate a structure for each thread.
			
			  Make sure to keep the original task at
			  the head of the list.
	
	  Now fill in each thread's information.
	
	  Fill in the two process-wide notes.
  Write all the notes for each thread.  When writing the first thread, the
  process-wide notes are interleaved after the first thread-specific note.
 Here is the structure in which status of each thread is captured. 
 NT_PRSTATUS 
 NT_PRFPREG 
  In order to add the specific thread information for the elf file format,
  we need to keep a linked list of every threads pr_status and then create
  a single section for them in the final core file.
 NT_PRSTATUS 
 NT_PRPSINFO 
 Allocate space for ELF notes 
 now collect the dump for the current 
 Set up header 
	
	  Set up the notes in similar form to SVR4 core dumps made
	  with info from their proc.
 Try to dump the FPU. 
 write out the thread status notes section 
 Free data possibly allocated by fill_files_note(): 
  Actual dumper
  This is a two-pass process; first we find the offsets of the bits,
  and then they are actually written out.  If we run out of core limit
  we just truncate.
	
	  The number of segs are recored into ELF header as 16bit value.
	  Please check DEFAULT_MAX_MAP_COUNT definition when you modify here.
 for notes section 
	 If segs > PN_XNUM(0xffff), then e_phnum overflows. To avoid
	  this, kernel supports extended numbering. Have a look at
	
	  Collect all the non-memory information about the process for the
	  notes.  This also sets up the file header.
 Elf header 
 Program headers 
 Write notes phdr entry 
 For cell spufs 
 Write program headers for segments dump 
 write out the notes section 
 For cell spufs 
 Align to page 
 CONFIG_ELF_CORE 
 Remove the COFF and ELF loaders. 
 SPDX-License-Identifier: GPL-2.0
   linuxfsfile.c
   Copyright (C) 1998-1999, Stephen Tweedie and Bill Hawes
   Manage the dynamic fd arrays in the process files_struct.
 our min() is unusable in constant expressions ;- 
  Copy 'count' fd bits from the old table to the new table and clear the extra
  space if any.  This does not copy the file pointers.  Called with the files
  spinlock held for write.
  Copy all file descriptors from the old table to the new, expanded table and
  clear the extra space.  Called with the files spinlock held for write.
	
	  Figure out how many fds we actually want to support in this fdtable.
	  Allocation steps are keyed to the size of the fdarray, since it
	  grows far faster than any of the other dynamic data. We try to fit
	  the fdarray into comfortable page-tuned chunks: starting at 1024B
	  and growing in powers of two from there on.
	
	  Note that this can drive nr below what we had passed if sysctl_nr_open
	  had been set lower between the check in expand_files() and here.  Deal
	  with that in caller, it's cheaper that way.
	 
	  We make sure that nr remains a multiple of BITS_PER_LONG - otherwise
	  bitmaps handling below becomes unpleasant, to put it mildly...
  Expand the file descriptor table.
  This function will allocate a new fdtable and both fd array and fdset, of
  the given size.
  Return <0 error code on error; 1 on successful completion.
  The files->file_lock should be held on entry, and will be held on exit.
	 make sure all fd_install() have seen resize_in_progress
	  or have finished their rcu_read_lock_sched() section.
	
	  extremely unlikely race - sysctl_nr_open decreased between the check in
	  caller and alloc_fdtable().  Cheaper to catch it here...
 coupled with smp_rmb() in fd_install() 
  Expand files.
  This function will expand the file structures, if the requested size exceeds
  the current capacity and there is room for expansion.
  Return <0 error code on error; 0 when nothing done; 1 when files were
  expanded and execution may have blocked.
  The files->file_lock should be held on entry, and will be held on exit.
 Do we need to expand? 
 Can we expand? 
 All good, so we try 
 Find the last open fd 
  Allocate a new files structure and copy contents from the
  passed in files structure.
  errorp will be valid only when the returned files_struct is NULL.
	
	  Check whether we need to allocate a larger fd array and fd set.
 beyond sysctl_nr_open; nothing to do 
		
		  Reacquire the oldf lock and a pointer to its fd table
		  who knows it may have a new bigger fd table. We need
		  the latest pointer.
			
			  The fd may be claimed in the fd bitmap but not yet
			  instantiated in the files array if a sibling thread
			  is partway through open().  So make sure that this
			  fd is available to the new process.
 clear the remainder 
	
	  It is safe to dereference the fd table without RCU or
	  ->file_lock because this is the last reference to the
	  files structure.
 free the arrays if they are not embedded 
  allocate a file descriptor, mark it busy.
	
	  N.B. For clone tasks sharing a files structure, this test
	  will limit the total number of files that can be opened.
	
	  If we needed to expand the fs array we
	  might have blocked - try again.
 Sanity check 
  Install a file pointer in the fd array.
  The VFS is full of places where we drop the files lock between
  setting the open_fds bitmap and installing the file in the file
  array.  At any such point, we are vulnerable to a dup2() race
  installing a file in the array before us.  We need to detect this and
  fput() the struct file we are about to overwrite in this case.
  It should never happen - if we allow dup2() do it, _really_ bad things
  will follow.
  This consumes the "file" refcount, so callers should treat it
  as if they had called fput(file).
 coupled with smp_wmb() in expand_fdtable() 
  pick_file - return file associatd with fd
  @files: file struct to retrieve file from
  @fd: file descriptor to retrieve file for
  If this functions returns an EINVAL error pointer the fd was beyond the
  current maximum number of file descriptors for that fdtable.
  Returns: The file associated with @fd, on error returns an error pointer.
 for ksys_close() 
  last_fd - return last valid index into fd table
  @cur_fds: files struct
  Context: Either rcu read lock or files_lock must be held.
  Returns: Last valid index into fdtable.
 make sure we're using the correct maximum value 
 found a valid file to close 
 beyond the last fd in that table 
  __close_range() - Close all file descriptors in a given range.
  @fd:     starting file descriptor to close
  @max_fd: last file descriptor to close
  This closes a range of file descriptors. All file descriptors
  from @fd up to and including @max_fd are closed.
		
		  If the caller requested all fds to be made cloexec we always
		  copy all of the file descriptors since they still want to
		  use them.
			
			  If the requested range is greater than the current
			  maximum, we're closing everything so only copy all
			  file descriptors beneath the lowest file descriptor.
		
		  We used to share our file descriptor table, and have now
		  created a private one, make sure we're using it below.
		
		  We're done closing the files we were supposed to. Time to install
		  the new file descriptor table and drop the old one.
  See close_fd_get_file() below, this variant assumes current->files->file_lock
  is held.
  variant of close_fd that gets a ref on the file for later fput.
  The caller must ensure that filp_close() called on the file, and then
  an fput().
 exec unshares first 
		 File object ref couldn't be taken.
		  dup2() atomicity guarantee is the reason
		  we loop to catch the new file (or NULL pointer)
 Must be called with rcu_read_lock held 
 Must be called with rcu_read_lock held 
  Lightweight file lookup - no refcnt increment if fd table isn't shared.
  You can use this instead of fget if you satisfy all of the following
  conditions:
  1) You must call fput_light before exiting the syscall and returning control
     to userspace (i.e. you cannot remember the returned struct file  after
     returning to userspace).
  2) You must not call filp_close on the returned struct file  in between
     calls to fget_light and fput_light.
  3) You must not clone the current task in between the calls to fget_light
     and fput_light.
  The fput_needed flag returned by fget_light should be passed to the
  corresponding fput_light.
  We only lock f_pos if we have threads or if the file might be
  shared with another process. In both cases we'll have an elevated
  file count (done either by fdget() or by fork()).
	
	  We need to detect attempts to do dup2() over allocated but still
	  not finished descriptor.  NB: OpenBSD avoids that at the price of
	  extra work in their equivalent of fget() - they insert struct
	  file immediately after grabbing descriptor, mark it larval if
	  more work (e.g. actual opening) is needed and make sure that
	  fget() treats larval files as absent.  Potentially interesting,
	  but while extra work in fget() is trivial, locking implications
	  and amount of surgery on open()-related paths in VFS are not.
	  FreeBSD fails with -EBADF in the same situation, NetBSD "solution"
	  deadlocks in rather amusing ways, AFAICS.  All of that is out of
	  scope of POSIX or SUS, since neither considers shared descriptor
	  tables and this condition does not arise without those.
  __receive_fd() - Install received file into file descriptor table
  @file: struct file that was received from another process
  @ufd: __user pointer to write new fd number to
  @o_flags: the O_ flags to apply to the new fd entry
  Installs a received file into the file descriptor table, with appropriate
  checks and count updates. Optionally writes the fd number to userspace, if
  @ufd is non-NULL.
  This helper handles its own reference counting of the incoming
  struct file.
  Returns newly install fd or -ve on error.
 corner case 
 SPDX-License-Identifier: GPL-2.0-only
  binfmt_misc.c
  Copyright (C) 1997 Richard Gnther
  binfmt_misc detects binaries via a magic or filename extension and invokes
  a specified wrapper. See Documentationadmin-guidebinfmt-misc.rst for more details.
 make it zero to save 400 bytes kernel memory 
 type, status, etc. 
 offset of magic 
 size of magicmask 
 magic or filename extension 
 mask, NULL for exact match 
 filename of interpreter 
  Max length of the register string.  Determined by:
   - 7 delimiters
   - name:   ~50 bytes
   - type:   1 byte
   - offset: 3 bytes (has to be smaller than BINPRM_BUF_SIZE)
   - magic:  128 bytes (512 in escaped form)
   - mask:   128 bytes (512 in escaped form)
   - interp: ~50 bytes
   - flags:  5 bytes
  Round that up a bit, and then back off to hold the internal data
  (like struct Node).
  Check if we support the binfmt
  if we do, return the node, else NULL
  locking is done in load_misc_binary
 Walk all the registered handlers. 
 Make sure this one is currently enabled. 
 Do matching based on extension if applicable. 
 Do matching based on magic & mask. 
  the loader itself
 to keep locking time low, we copy the interpreter string 
 Need to be able to load the file after exec 
 make argv[1] be the path to the binary 
 add the interp as argv[0] 
 Update interp in case binfmt_script needs it. 
 Command parsers 
  parses and copies one argument enclosed in del from sp to dp,
  recognising the \x special.
  returns pointer to the copied argument or NULL in case of an
  error (and sets err) or null argument length.
 special flags 
			 this flags also implies the
  This registers a new binary format, it recognises the syntax
  ':name:type:offset:magic:mask:interpreter:flags'
  where the ':' is the IFS, that can be chosen with the first char
 some sanity checks 
 delimeter 
 Pad the buffer with the delim to simplify parsing below. 
 Parse the 'name' field. 
 Parse the 'type' field. 
 Handle the 'M' (magic) format. 
 Parse the 'offset' field. 
 Parse the 'magic' field. 
 Parse the 'mask' field. 
		
		  Decode the magic & mask fields.
		  Note: while we might have accepted embedded NUL bytes from
		  above, the unescape helpers here will stop at the first one
		  it encounters.
 Handle the 'E' (extension) format. 
 Skip the 'offset' field. 
 Parse the 'magic' field. 
 Skip the 'mask' field. 
 Parse the 'interpreter' field. 
 Parse the 'flags' field. 
  Set status of entrybinfmt_misc:
  '1' enables, '0' disables and '-1' clears entrybinfmt_misc
 generic stuff 
 print the special flags 
 <entry> 
 Disable this handler. 
 Enable this handler. 
 Delete this handler. 
 register 
 status 
 Disable all handlers. 
 Enable all handlers. 
 Delete all handlers. 
 Superblock handling 
 last one  {""}
 SPDX-License-Identifier: GPL-2.0-only
   linuxfsopen.c
   Copyright (C) 1991, 1992  Linus Torvalds
 Not pretty: "inode->i_size" shouldn't really be signed. But it is. 
 Remove suid, sgid, and file capabilities on truncate too 
 Note any delegations or leases have already been broken: 
 For directories it's -EISDIR, for other non-regulars - -EINVAL 
	
	  Make sure that there are no leases.  get_write_access() protects
	  against the truncate racing with a lease-granting setlease().
 sorry, but loff_t says... 
 explicitly opened as large or we are on 64-bit box 
 Cannot ftruncate over 2^31 bytes without large file support 
 Check IS_APPEND on real upper inode 
 LFS versions of truncate are only needed on 32 bit machines 
 BITS_PER_LONG == 32 
 Return error if mode is not supported 
 Punch hole and zero range are mutually exclusive 
 Punch hole must have keep size set 
 Collapse range should only be used exclusively. 
 Insert range should only be used exclusively. 
 Unshare range should only be used with allocate mode. 
	
	  We can only allow pure fallocate on append only files
	
	  We cannot allow any fallocate operation on an active swapfile
	
	  Revalidate the write permissions, in case security policy has
	  changed since the files were opened.
 Check for wrap through zero too 
	
	  Create inotify and fanotify events.
	 
	  To keep the logic simple always create events if fallocate succeeds.
	  This implies that events are even created if the file size remains
	  unchanged, e.g. when using flag FALLOC_FL_KEEP_SIZE.
  access() needs to use the real uidgid, not the effective uidgid.
  We do this by temporarily clearing all FS-related capabilities and
  switching the fsuidfsgid around to the real ones.
 Clear the capabilities if we switch to a non-root user 
	
	  The new set of credentials can only be used in
	  task-synchronous circumstances, and does not need
	  RCU freeing, unless somebody then takes a separate
	  reference to it.
	 
	  NOTE! This is _only_ true because this credential
	  is used purely for override_creds() that installs
	  it as the subjective cred. Other threads will be
	  accessing ->real_cred, not the subjective cred.
	 
	  If somebody _does_ make a copy of this (using the
	  'get_current_cred()' function), that will clear the
	  non_rcu field, because now that other user may be
	  expecting RCU freeing. But normal thread-synchronous
	  cred accesses will keep things non-RCY.
 override_cred() gets its own ref 
 where's F_OK, X_OK, W_OK, R_OK? 
		
		  MAY_EXEC on regular files is denied if the fs is mounted
		  with the "noexec" flag.
 SuS v2 requires we report a read only fs too 
	
	  This is a rare case where using __mnt_is_readonly()
	  is OK without a mnt_wantdrop_write() pair.  Since
	  no actual write to the fs is performed here, we do
	  not need to telegraph to that to anyone.
	 
	  By doing this, we accept that this access is
	  inherently racy and know that the fs may change
	  state before we even see this result.
 POSIX.1-2008SUSv4 Section XSI 2.9.7 
 normally all 3 are set; ->open() can clear them if needed 
 NB: we're sure to have correct a_ops only after f_op->open 
	
	  XXX: Huge page cache doesn't support writing yet. Drop all page
	  cache for this file before processing writes.
		
		  Paired with smp_mb() in collapse_file() to ensure nr_thps
		  is up to date and the update to i_writecount by
		  get_write_access() is visible. Ensures subsequent insertion
		  of THPs into the page cache will fail.
			
			  unmap_mapping_range just need to be called once
			  here, because the private pages is not need to be
			  unmapped mapping (e.g. data segment of dynamic
			  shared libraries here).
  finish_open - finish opening a file
  @file: file pointer
  @dentry: pointer to dentry
  @open: open callback
  @opened: state of open
  This can be used to finish opening a file passed to i_op->atomic_open().
  If the open callback is set to NULL, then the standard f_op->open()
  filesystem callback is substituted.
  NB: the dentry reference is _not_ consumed.  If, for example, the dentry is
  the return value of d_splice_alias(), then the caller needs to perform dput()
  on it after finish_open().
  Returns zero on success or -errno if the open failed.
 once it's opened, it's opened 
  finish_no_open - finish ->atomic_open() without opening the file
  @file: file pointer
  @dentry: dentry or NULL (as returned from ->lookup())
  This can be used to set the result of a successful lookup in ->atomic_open().
  NB: unlike finish_open() this function does consume the dentry reference and
  the caller need not dput() it.
  Returns "0" which must be the return value of ->atomic_open() after having
  called this function.
  vfs_open - open the file at the given path
  @path: path to open
  @file: newly allocated file with f_flag initialized
  @cred: credentials to use
 We must always pass in a valid mount pointer. 
 O_PATH beats everything else. 
 Modes should only be set for create-like flags. 
	
	  Strip flags that either shouldn't be set by userspace like
	  FMODE_NONOTIFY or that aren't relevant in determining struct
	  open_flags like O_CLOEXEC.
	
	  Older syscalls implicitly clear all of the invalid flags or argument
	  values before calling build_open_flags(), but openat2(2) checks all
	  of its arguments.
 Scoping flags are mutually exclusive. 
 Deal with the mode. 
	
	  In order to ensure programs get explicit errors when trying to use
	  O_TMPFILE on old kernels, O_TMPFILE is implemented such that it
	  looks like (O_DIRECTORY|O_RDWR & ~O_CREAT) to old kernels. But we
	  have to require userspace to explicitly set it.
 O_PATH only permits certain other flags to be set. 
	
	  O_SYNC is implemented as __O_SYNC|O_DSYNC.  As many places only
	  check for O_DSYNC if the need any syncing at all we enforce it's
	  always set instead of having to deal with possibly weird behaviour
	  for malicious applications setting only __O_SYNC.
 O_TRUNC implies we need access checks for write permissions 
	 Allow the LSM permission hook to distinguish append
 Don't bother even trying for createtruncatetmpfile open 
  file_open_name - open file and return file pointer
  @name:	struct filename containing path to open
  @flags:	open flags as per the open(2) second argument
  @mode:	mode for the new file if O_CREAT is set, else ignored
  This is the helper to open a file from kernelspace if you really
  have to.  But in generally you should not do this, so please move
  along, nothing to see here..
  filp_open - open file and return file pointer
  @filename:	path to open
  @flags:	open flags as per the open(2) second argument
  @mode:	mode for the new file if O_CREAT is set, else ignored
  This is the helper to open a file from kernelspace if you really
  have to.  But in generally you should not do this, so please move
  along, nothing to see here..
 O_LARGEFILE is only allowed for non-O_PATH. 
  Exactly like sys_open(), except that it doesn't set the
  O_LARGEFILE flag.
  Exactly like sys_openat(), except that it doesn't set the
  O_LARGEFILE flag.
  For backward compatibility?  Maybe this should be moved
  into archi386 instead?
  "id" is the POSIX thread ID. We use the
  files pointer for this..
  Careful here! We test whether the file pointer is NULL before
  releasing the fd. This ensures that one clone task can't release
  an fd while another clone is opening it.
 can't restart close syscall because file table entry was cleared 
  close_range() - Close all file descriptors in a given range.
  @fd:     starting file descriptor to close
  @max_fd: last file descriptor to close
  @flags:  reserved for future extensions
  This closes a range of file descriptors. All file descriptors
  from @fd up to and including @max_fd are closed.
  Currently, errors to close a given file descriptor are ignored.
  This routine simulates a hangup on the tty, to arrange that users
  are given clean terminals at login time.
  Called when an inode is about to be open.
  We use this to disallow opening large files on 32bit systems if
  the caller didn't specify O_LARGEFILE.  On 64bit systems we force
  on this flag in sys_open.
  This is used by subsystems that don't want seekable
  file descriptors. The function is not supposed to ever fail, the only
  reason it returns an 'int' and not 'void' is so that it can be plugged
  directly into file_operations structure.
  stream_open is used by subsystems that want stream-like file descriptors.
  Such file descriptors are not seekable and don't have notion of position
  (file.f_pos is always 0 and ppos passed to .read().write() is always NULL).
  Contrary to file descriptors of other regular files, .read() and .write()
  can run simultaneously.
  stream_open never fails and is marked to return int so that it could be
  directly used as file_operations.open .
 SPDX-License-Identifier: GPL-2.0
		
		  Tell setattr_prepare(), that this is an explicit time
		  update, even if neither ATTR_ATIME_SET nor ATTR_MTIME_SET
		  were used.
  do_utimes - change times on filename or file descriptor
  @dfd: open file descriptor, -1 or AT_FDCWD
  @filename: path name or NULL
  @times: new times or NULL
  @flags: zero or more flags (only AT_SYMLINK_NOFOLLOW for the moment)
  If filename is NULL and dfd refers to an open file, then operate on
  the file.  Otherwise look up filename, possibly using dfd as a
  starting point.
  If times==NULL, set access and modification to current time,
  must be owner or have write permission.
  Else, update from times, must be owner or super user.
 Nothing to do, we must not even check the path.  
  futimesat(), utimes() and utime() are older versions of utimensat()
  that are provided for compatibility with traditional C libraries.
  On modern architectures, we always use libc wrappers around
  utimensat() instead.
		 This test is needed to catch all invalid values.  If we
		   would test only in do_utimes we would miss those invalid
		   values truncated by the multiplication with 1000.  Note
		   that we also catch UTIME_{NOW,OMIT} here which are only
  Not all architectures have sys_utime, so implement this in terms
  of sys_utimes.
 SPDX-License-Identifier: GPL-2.0-only
   fsuserfaultfd.c
   Copyright (C) 2007  Davide Libenzi <davidel@xmailserver.org>
   Copyright (C) 2008-2009 Red Hat, Inc.
   Copyright (C) 2015  Red Hat, Inc.
   Some part derived from fseventfd.c (anon inode setup) and
   mmksm.c (mm hashing).
  Start with fault_pending_wqh and fault_wqh so they're more likely
  to be in the same cacheline.
  Locking order:
 	fd_wqh.lock
 		fault_pending_wqh.lock
 			fault_wqh.lock
 		event_wqh.lock
  To avoid deadlocks, IRQs must be disabled when taking any of the above locks,
  since fd_wqh.lock is taken by aio_poll() while it's holding a lock that's
  also taken in IRQ context.
 waitqueue head for the pending (i.e. not read) userfaults 
 waitqueue head for the userfaults 
 waitqueue head for the pseudo fd to wakeup pollread 
 waitqueue head for events 
 a refile sequence protected by fault_pending_wqh lock 
 pseudo fd refcounting 
 userfaultfd syscall flags 
 features requested from the userspace 
 released 
 memory mappings are changing because of non-cooperative event 
 mm with one ore more vmas attached to this userfaultfd_ctx 
 internal indication that UFFD_API ioctl was successfully executed 
 len == 0 means wake all 
	
	  The Program-Order guarantees provided by the scheduler
	  ensure uwq->waken is visible before the task is woken.
		
		  Wake only once, autoremove behavior.
		 
		  After the effect of list_del_init is visible to the other
		  CPUs, the waitqueue may disappear from under us, see the
		  !list_empty_careful() in handle_userfault().
		 
		  try_to_wake_up() has an implicit smp_mb(), and the
		  wq->private is read before calling the extern function
		  "wake_up_state" (which in turns calls try_to_wake_up).
  userfaultfd_ctx_get - Acquires a reference to the internal userfaultfd
  context.
  @ctx: [in] Pointer to the userfaultfd context.
  userfaultfd_ctx_put - Releases a reference to the internal userfaultfd
  context.
  @ctx: [in] Pointer to userfaultfd context.
  The userfaultfd context reference must have been previously acquired either
  with userfaultfd_ctx_get() or userfaultfd_ctx_fdget().
	
	  Must use memset to zero out the paddings or kernel data is
	  leaked to userland.
	
	  These flags indicate why the userfault occurred:
	  - UFFD_PAGEFAULT_FLAG_WP indicates a write protect fault.
	  - UFFD_PAGEFAULT_FLAG_MINOR indicates a minor fault.
	  - Neither of these flags being set indicates a MISSING fault.
	 
	  Separately, UFFD_PAGEFAULT_FLAG_WRITE indicates it was a write
	  fault. Otherwise, it was a read fault.
  Same functionality as userfaultfd_must_wait below with modifications for
  hugepmd ranges.
	
	  Lockless access: we're in a wait_event so it's ok if it
	  changes under us.
 should never get here 
 CONFIG_HUGETLB_PAGE 
  Verify the pagetables are still not ok after having reigstered into
  the fault_pending_wqh to avoid userland having to UFFDIO_WAKE any
  userfault that has already been resolved, if userfaultfd_read and
  UFFDIO_COPY|ZEROPAGE are being run simultaneously on two different
  threads.
	
	  READ_ONCE must function as a barrier with narrower scope
	  and it must be equivalent to:
	 	_pmd = pmd; barrier();
	 
	  This is to deal with the instability (as in
	  pmd_trans_unstable) of the pmd.
	
	  the pmd is stable (as in !pmd_trans_unstable) so we can re-read it
	  and use the standard pte_offset_map() instead of parsing _pmd.
	
	  Lockless access: we're in a wait_event so it's ok if it
	  changes under us.
  The locking rules involved in returning VM_FAULT_RETRY depending on
  FAULT_FLAG_ALLOW_RETRY, FAULT_FLAG_RETRY_NOWAIT and
  FAULT_FLAG_KILLABLE are not straightforward. The "Caution"
  recommendation in __lock_page_or_retry is not an understatement.
  If FAULT_FLAG_ALLOW_RETRY is set, the mmap_lock must be released
  before returning VM_FAULT_RETRY only if FAULT_FLAG_RETRY_NOWAIT is
  not set.
  If FAULT_FLAG_ALLOW_RETRY is set but FAULT_FLAG_KILLABLE is not
  set, VM_FAULT_RETRY can still be returned if and only if there are
  fatal_signal_pending()s, and the mmap_lock must be released before
  returning it.
	
	  We don't do userfault handling for the final child pid update.
	 
	  We also don't do userfault handling during
	  coredumping. hugetlbfs has the special
	  follow_hugetlb_page() to skip missing pages in the
	  FOLL_DUMP case, anon memory also checks for FOLL_DUMP with
	  the no_page_table() helper in follow_page_mask(), but the
	  shmem_vm_ops->fault method is invoked even during
	  coredumping without mmap_lock and it ends up here.
	
	  Coredumping runs without mmap_lock so we can only check that
	  the mmap_lock is held, if PF_DUMPCORE was not set.
 Any unrecognized flag is a bug. 
 0 or > 1 flags set is a bug; we expect exactly 1. 
	
	  If it's already released don't get it. This avoids to loop
	  in __get_user_pages if userfaultfd_release waits on the
	  caller of handle_userfault to release the mmap_lock.
		
		  Don't return VM_FAULT_SIGBUS in this case, so a non
		  cooperative manager can close the uffd after the
		  last UFFDIO_COPY, without risking to trigger an
		  involuntary SIGBUS if the process was starting the
		  userfaultfd while the userfaultfd was still armed
		  (but after the last UFFDIO_COPY). If the uffd
		  wasn't already closed when the userfault reached
		  this point, that would normally be solved by
		  userfaultfd_must_wait returning 'false'.
		 
		  If we were to return VM_FAULT_SIGBUS here, the non
		  cooperative manager would be instead forced to
		  always call UFFDIO_UNREGISTER before it can safely
		  close the uffd.
	
	  Check that we can return VM_FAULT_RETRY.
	 
	  NOTE: it should become possible to return VM_FAULT_RETRY
	  even if FAULT_FLAG_TRIED is set without leading to gup()
	  -EBUSY failures, if the userfaultfd is to be extended for
	  VM_UFFD_WP tracking and we intend to arm the userfault
	  without first stopping userland access to the memory. For
	  VM_UFFD_MISSING userfaults this is enough for now.
		
		  Validate the invariant that nowait must allow retry
		  to be sure not to return SIGBUS erroneously on
		  nowait invocations.
	
	  Handle nowait, not much to do other than tell it to retry
	  and wait.
 take the reference before dropping the mmap_lock 
	
	  After the __add_wait_queue the uwq is visible to userland
	  through pollread().
	
	  The smp_mb() after __set_current_state prevents the reads
	  following the spin_unlock to happen before the list_add in
	  __add_wait_queue.
	
	  Here we race with the list_del; list_add in
	  userfaultfd_ctx_read(), however because we don't ever run
	  list_del_init() to refile across the two lists, the prev
	  and next pointers will never point to self. list_add also
	  would never let any of the two pointers to point to
	  self. So list_empty_careful won't risk to see both pointers
	  pointing to self at any time during the list refile. The
	  only case where list_del_init() is called is the full
	  removal in the wake function and there we don't re-list_add
	  and it's fine not to block on the spinlock. The uwq on this
	  kernel stack can be released after the list_del_init.
		
		  No need of list_del_init(), the uwq on the stack
		  will be freed shortly anyway.
	
	  ctx may go away after this if the userfault pseudo fd is
	  already released.
	
	  After the __add_wait_queue the uwq is visible to userland
	  through pollread().
			
			  &ewq->wq may be queued in fork_event, but
			  __remove_wait_queue ignores the head
			  parameter. It would be a problem if it
			  didn't.
 the various vma->vm_userfaultfd_ctx still points to it 
	
	  ctx may go away after this if the userfault pseudo fd is
	  already released.
 Drop uffd context if remap feature not enabled 
 len == 0 means wake all 
	
	  Flush page faults out of all CPUs. NOTE: all page faults
	  must be retried without returning VM_FAULT_SIGBUS if
	  userfaultfd_ctx_get() succeeds but vma->vma_userfault_ctx
	  changes while handle_userfault released the mmap_lock. So
	  it's critical that released is set to true (above), before
	  taking the mmap_lock for writing.
	
	  After no new page faults can wait on this fault_wqh, flush
	  the last page faults that may have been already waiting on
	  the fault_wqh.
 Flush pending events that may still wait on event_wqh 
 fault_pending_wqh.lock must be hold by the caller 
 walk in reverse to provide FIFO behavior to read userfaults 
	
	  poll() never guarantees that read won't block.
	  userfaults can be waken before they're read().
	
	  lockless access to see if there are pending faults
	  __pollwait last action is the add_wait_queue but
	  the spin_unlock would allow the waitqueue_active to
	  pass above the actual list_add inside
	  add_wait_queue critical section. So use a full
	  memory barrier to serialize the list_add write of
	  add_wait_queue() with the waitqueue_active read
	  below.
	
	  Handling fork event requires sleeping operations, so
	  we drop the event_wqh lock, then do these ops, then
	  lock it back and wake up the waiter. While the lock is
	  dropped the ewq may go away so we keep track of it
	  carefully.
 always take the fd_wqh lock before the fault_pending_wqh lock 
			
			  Use a seqcount to repeat the lockless check
			  in wake_userfault() to avoid missing
			  wakeups because during the refile both
			  waitqueue could become empty if this is the
			  only userfault.
			
			  The fault_pending_wqh.lock prevents the uwq
			  to disappear from under us.
			 
			  Refile this userfault from
			  fault_pending_wqh to fault_wqh, it's not
			  pending anymore after we read it.
			 
			  Use list_del() by hand (as
			  userfaultfd_wake_function also uses
			  list_del_init() by hand) to be sure nobody
			  changes __remove_wait_queue() to use
			  list_del_init() in turn breaking the
			  !list_empty_careful() check in
			  handle_userfault(). The uwq->wq.head list
			  must never be empty at any time during the
			  refile, or the waitqueue could disappear
			  from under us. The "wait_queue_head_t"
			  parameter of __remove_wait_queue() is unused
			  anyway.
 careful to always initialize msg if ret == 0 
				
				  fork_nctx can be freed as soon as
				  we drop the lock, unless we take a
				  reference on it.
			
			  The fork thread didn't abort, so we can
			  drop the temporary refcount.
			
			  If fork_event list wasn't empty and in turn
			  the event wasn't already released by fork
			  (the event is allocated on fork kernel
			  stack), put the event back to its place in
			  the event_wq. fork_event head will be freed
			  as soon as we return so the event cannot
			  stay queued there no matter the current
			  "ret" value.
			
			  Leave the event in the waitqueue and report
			  error to userland if we failed to resolve
			  the userfault fork.
			
			  Here the fork thread aborted and the
			  refcount from the fork thread on fork_nctx
			  has already been released. We still hold
			  the reference we took before releasing the
			  lock above. If resolve_userfault_fork
			  failed we've to drop it because the
			  fork_nctx has to be freed in such case. If
			  it succeeded we'll hold it because the new
			  uffd references it.
		
		  Allow to read more than one fault at time but only
		  block if waiting for the very first one.
 wake all in the range and autoremove 
	
	  To be sure waitqueue_active() is not reordered by the CPU
	  before the pagetable update, use an explicit SMP memory
	  barrier here. PT lock release or mmap_read_unlock(mm) still
	  have release semantics that can allow the
	  waitqueue_active() to be reordered before the pte update.
	
	  Use waitqueue_active because it's very frequent to
	  change the address space atomically even if there are no
	  userfaults yet. So we take the spinlock only when we're
	  sure we've userfaults to wake.
 FIXME: add WP support to hugetlbfs and shmem 
 check that there's at least one vma in the range 
	
	  If the first vma contains huge pages, make sure start address
	  is aligned to huge page size.
	
	  Search for not compatible vmas.
 check not compatible vmas 
		
		  UFFDIO_COPY will fill file holes even without
		  PROT_WRITE. This check enforces that if this is a
		  MAP_SHARED, the process has write permission to the backing
		  file. If VM_MAYWRITE is set it also enforces that on a
		  MAP_SHARED vma: there is no F_WRITE_SEAL and no further
		  F_WRITE_SEAL can be taken until the vma is destroyed.
		
		  If this vma contains ending address, and huge pages
		  check alignment.
		
		  Check that this vma isn't already owned by a
		  different userfaultfd. We can't allow more than one
		  userfaultfd to own a single vma simultaneously or we
		  wouldn't know which one to deliver the userfaults to.
		
		  Note vmas containing huge pages
		
		  Nothing to do: this vma is already registered into this
		  userfaultfd and with the right tracking mode too.
		
		  In the vma_merge() successful mprotect-like case 8:
		  the next vma was merged into the current one and
		  the current one has not been updated yet.
		
		  Declare the WP ioctl only if the WP mode is
		  specified and all checks passed with the range
 CONTINUE ioctl is only supported for MINOR ranges. 
		
		  Now that we scanned all vmas we can already tell
		  userland which ioctls methods are guaranteed to
		  succeed on this range.
 check that there's at least one vma in the range 
	
	  If the first vma contains huge pages, make sure start address
	  is aligned to huge page size.
	
	  Search for not compatible vmas.
		
		  Check not compatible vmas, not strictly required
		  here as not compatible vmas cannot have an
		  userfaultfd_ctx registered on them, but this
		  provides for more strict behavior to notice
		  unregistration errors.
		
		  Nothing to do: this vma is already registered into this
		  userfaultfd and with the right tracking mode too.
			
			  Wake any concurrent pending userfault while
			  we unregister, so they will not hang
			  permanently and it avoids userland to call
			  UFFDIO_WAKE explicitly.
		
		  In the vma_merge() successful mprotect-like case 8:
		  the next vma was merged into the current one and
		  the current one has not been updated yet.
  userfaultfd_wake may be used in combination with the
  UFFDIO__MODE_DONTWAKE to wakeup userfaults in batches.
	
	  len == 0 means wake all and we don't want to wake all here,
	  so check it again to be sure.
 don't copy "copy" last field 
	
	  double check for wraparound just in case. copy_from_user()
	  will later check uffdio_copy.src + uffdio_copy.len to fit
	  in the userland range.
 len == 0 would wake all 
 don't copy "zeropage" last field 
 len == 0 would wake all 
 don't copy the output fields 
 double check for wraparound just in case. 
 len == 0 would wake all 
	
	  For the current set of features the bits just coincide. Set
	  UFFD_FEATURE_INITIALIZED to mark the features as enabled.
  userland asks for a certain API version and we return which bits
  and ioctl commands are implemented in this kernel for such API
  version or -EINVAL if unknown.
 report all available features and ioctls to userland 
 only enable the requested features for this uffd context 
	
	  If more protocols will be added, there will be all shown
	  separated by a space. Like this:
	 	protocols: aa:... bb:...
 Check the UFFD_ constants for consistency.  
 prevent the mm struct to be freed 
 SPDX-License-Identifier: GPL-2.0
   linuxfsstat.c
   Copyright (C) 1991, 1992  Linus Torvalds
  generic_fillattr - Fill in the basic attributes from the inode struct
  @mnt_userns:	user namespace of the mount the inode was found from
  @inode:	Inode to use as the source
  @stat:	Where to fill in the attributes
  Fill in the basic attributes in the kstat structure from data that's to be
  found on the VFS inode structure.  This is the default if no getattr inode
  operation is supplied.
  If the inode has been found through an idmapped mount the user namespace of
  the vfsmount must be passed through @mnt_userns. This function will then
  take care to map the inode according to @mnt_userns before filling in the
  uid and gid filds. On non-idmapped mounts or if permission checking is to be
  performed on the raw inode simply passs init_user_ns.
  generic_fill_statx_attr - Fill in the statx attributes from the inode flags
  @inode:	Inode to use as the source
  @stat:	Where to fill in the attribute flags
  Fill in the STATX_ATTR_ flags in the kstat structure for properties of the
  inode that are published on i_flags and enforced by the VFS.
  vfs_getattr_nosec - getattr without security checks
  @path: file to get attributes from
  @stat: structure to return attributes in
  @request_mask: STATX_xxx flags indicating what the caller wants
  @query_flags: Query mode (AT_STATX_SYNC_TYPE)
  Get attributes without calling security_inode_getattr.
  Currently the only caller other than vfs_getattr is internal to the
  filehandle lookup code, which uses only the inode number and returns no
  attributes to any user.  Any other code probably wants vfs_getattr.
 allow the fs to override these if it really wants to 
 SB_NOATIME means filesystem supplies dummy atime value 
	
	  Note: If you add another clause to set an attribute flag, please
	  update attributes_mask below.
  vfs_getattr - Get the enhanced basic attributes of a file
  @path: The file of interest
  @stat: Where to return the statistics
  @request_mask: STATX_xxx flags indicating what the caller wants
  @query_flags: Query mode (AT_STATX_SYNC_TYPE)
  Ask the filesystem for a file's attributes.  The caller must indicate in
  request_mask and query_flags to indicate what they want.
  If the file is remote, the filesystem can be forced to update the attributes
  from the backing store by passing AT_STATX_FORCE_SYNC in query_flags or can
  suppress the update by passing AT_STATX_DONT_SYNC.
  Bits must have been set in request_mask to indicate which attributes the
  caller wants retrieving.  Any such attribute not requested may be returned
  anyway, but the value may be approximate, and, if remote, may not have been
  synchronised with the server.
  0 will be returned on success, and a -ve error code if unsuccessful.
  vfs_fstat - Get the basic attributes by file descriptor
  @fd: The file descriptor referring to the file of interest
  @stat: The result structure to fill in.
  This function is a wrapper around vfs_getattr().  The main difference is
  that it uses a file descriptor to determine the file location.
  0 will be returned on success, and a -ve error code if unsuccessful.
  vfs_statx - Get basic and extra attributes by filename
  @dfd: A file descriptor representing the base dir for a relative filename
  @filename: The name of the file of interest
  @flags: Flags to control the query
  @stat: The result structure to fill in.
  @request_mask: STATX_xxx flags indicating what the caller wants
  This function is a wrapper around vfs_getattr().  The main difference is
  that it uses a filename and base directory to determine the file location.
  Additionally, the use of AT_SYMLINK_NOFOLLOW in flags will prevent a symlink
  at the given name from being referenced.
  0 will be returned on success, and a -ve error code if unsuccessful.
  For backward compatibility?  Maybe this should be moved
  into archi386 instead?
 it's laughable, but... 
 __ARCH_WANT_OLD_STAT 
		
		  AFS mountpoints allow readlink(2) but are not symlinks
 ---------- LFS-64 ----------- 
 mips has weird padding, so we don't get 64 bits there 
 __ARCH_WANT_STAT64 || __ARCH_WANT_COMPAT_STAT64 
  sys_statx - System call to get enhanced stats
  @dfd: Base directory to pathwalk from or fd to stat.
  @filename: File to stat or "" with AT_EMPTY_PATH
  @flags: AT_ flags to control pathwalk.
  @mask: Parts of statx struct actually required.
  @buffer: Result buffer.
  Note that fstat() can be emulated by setting dfd to the fd of interest,
  supplying "" as the filename and setting AT_EMPTY_PATH in the flags.
 Caller is here responsible for sufficient locking (ie. inode->i_lock) 
	 Caller is here responsible for sufficient locking
 SPDX-License-Identifier: GPL-2.0-only
  Replace the fs->{rootmnt,root} with {mnt,dentry}. Put the old values.
  It can block.
  Replace the fs->{pwdmnt,pwd} with {mnt,dentry}. Put the old values.
  It can block.
 We don't need to lock fs - think why ;-) 
 to be mentioned only in INIT_TASK 
 SPDX-License-Identifier: GPL-2.0
   linuxfsfilesystems.c
   Copyright (C) 1991, 1992  Linus Torvalds
   table of configured filesystems
  Handling of filesystem drivers list.
  Rules:
 	Inclusion toremovals fromscanning of list are protected by spinlock.
 	During the unload module must call unregister_filesystem().
 	We can access the fields of list element if:
 		1) spinlock is held or
 		2) we hold the reference to the module.
 	The latter can be guaranteed by call of try_module_get(); if it
 	returned 0 we must skip the element, otherwise we got the reference.
 	Once the reference is obtained we can drop the spinlock.
 WARNING: This can be used only if we _already_ own a reference 
 	register_filesystem - register a new filesystem
 	@fs: the file system structure
 	Adds the file system passed to the list of file systems the kernel
 	is aware of for mount and other syscalls. Returns 0 on success,
 	or a negative errno code on an error.
 	The &struct file_system_type that is passed is linked into the kernel 
 	structures and must not be freed until the file system has been
 	unregistered.
 	unregister_filesystem - unregister a file system
 	@fs: filesystem to unregister
 	Remove a file system that was previously successfully registered
 	with the kernel. An error is returned if the file system is not found.
 	Zero is returned on a success.
 	
 	Once this function has returned the &struct file_system_type structure
 	may be freed or reused.
 OK, we got the reference, so we can safely block 
  Whee.. Weird sysv syscall. 
 SPDX-License-Identifier: GPL-2.0-only
  32-bit compatibility support for ELF format executables and core dumps.
  Copyright (C) 2007 Red Hat, Inc.  All rights reserved.
  Red Hat Author: Roland McGrath.
  This file is used in a 64-bit kernel that wants to support 32-bit ELF.
  asmelf.h is responsible for defining the compat_ and COMPAT_ macros
  used below, with definitions appropriate for 32-bit ABI compatibility.
  We use macros to rename the ABI types and machine-dependent
  functions used in binfmt_elf.c to compat versions.
  Rename the basic ELF layout types to refer to the 32-bit class of files.
  Some data types as stored in coredump.
  The machine-dependent core note format types are defined in elfcore-compat.h,
  which requires asmelf.h to define compat_elf_gregset_t et al.
  To use this file, asmelf.h must define compat_elf_check_arch.
  The other following macros can be defined if the compat versions
  differ from the native ones, or omitted when they match.
  Rename a few of the symbols that binfmt_elf.c will define.
  These are all local so the names don't really matter, but it
  might make some debugging less confusing not to duplicate them.
  We share all the actual code with the native (64-bit) version.
 SPDX-License-Identifier: GPL-2.0-only
  fsfs-writeback.c
  Copyright (C) 2002, Linus Torvalds.
  Contains all the functions related to writing back and waiting
  upon dirty inodes against superblocks, and writing back dirty
  pages against inodes.  ie: data writeback.  Writeout of the
  inode itself is not handled here.
  10Apr2002	Andrew Morton
 		Split out of fsinode.c
 		Additions for address_space-based writeback
  4MB minimal write chunk size
  Passed into wb_writeback(), essentially a subset of writeback_control
 sync(2) WB_SYNC_ALL writeback 
 free on completion 
 why was writeback initiated? 
 pending work list 
 set if the caller waits 
  If an inode is constantly having its pages dirtied, but then the
  updates stop dirtytime_expire_interval seconds in the past, it's
  possible for the worst case time between when an inode has its
  timestamps updated and when they finally get written out to be two
  dirtytime_expire_intervals.  We set the default to 12 hours (in
  seconds), which means most of the time inodes will have their
  timestamps written to disk after 12 hours, but in the worst case a
  few inodes might not their timestamps updated for 24 hours.
  Include the creation of the trace points after defining the
  wb_writeback_work structure and inline functions so that the definition
  remains local to this file.
  inode_io_list_move_locked - move an inode onto a bdi_writeback IO list
  @inode: inode to be moved
  @wb: target bdi_writeback
  @head: one of @wb->b_{dirty|io|more_io|dirty_time}
  Move @inode->i_io_list to @list of @wb and set %WB_has_dirty_io.
  Returns %true if @inode is the first occupant of the !dirty_time IO
  lists; otherwise, %false.
 dirty_time doesn't count as dirty_io until expiration 
 @done can't be accessed after the following dec 
  wb_wait_for_completion - wait for completion of bdi_writeback_works
  @done: target wb_completion
  Wait for one or more work items issued to @bdi with their ->done field
  set to @done, which should have been initialized with
  DEFINE_WB_COMPLETION().  This function returns after all such work items
  are completed.  Work items which are waited upon aren't freed
  automatically on completion.
 put down the initial count 
  Parameters for foreign inode detection, see wbc_detach_inode() to see
  how they're used.
  These paramters are inherently heuristical as the detection target
  itself is fuzzy.  All we want to do is detaching an inode from the
  current owner if it's being written to by some other cgroups too much.
  The current cgroup writeback is built on the assumption that multiple
  cgroups writing to the same inode concurrently is very rare and a mode
  of operation which isn't well supported.  As such, the goal is not
  taking too long when a different cgroup takes over an inode while
  avoiding too aggressive flip-flops from occasional foreign writes.
  We record, very roughly, 2s worth of IO time history and if more than
  half of that is foreign, trigger the switch.  The recording is quantized
  to 16 slots.  To avoid tiny writes from swinging the decision too much,
  writes smaller than 18 of avg size are ignored.
 1s = 2^13, upto 8 secs w 16bit 
 avg = avg  78 + new  18 
 ignore rounds < avg  8 
 2s 
 inode->i_wb_frn_history is 16bit 
 each slot's duration is 2s  16 
 if foreign slots >= 8, switch 
 one round can affect upto 5 slots 
 don't queue too many concurrently 
  Maximum inodes per isw.  A specific value has been chosen to make
  struct inode_switch_wbs_context fit into 1024 bytes kmalloc.
 must pin memcg_css, see wb_get_create() 
	
	  There may be multiple instances of this function racing to
	  update the same inode.  Use cmpxchg() to tell the winner.
  inode_cgwb_move_to_attached - put the inode onto wb->b_attached list
  @inode: inode of interest with i_lock held
  @wb: target bdi_writeback
  Remove the inode from wb's io lists and if necessarily put onto b_attached
  list.  Only inodes attached to cgwb's are kept on this list.
  locked_inode_to_wb_and_lock_list - determine a locked inode's wb and lock it
  @inode: inode of interest with i_lock held
  Returns @inode's wb with its list_lock held.  @inode->i_lock must be
  held on entry and is released on return.  The returned wb is guaranteed
  to stay @inode's associated wb until its list_lock is released.
		
		  inode_to_wb() association is protected by both
		  @inode->i_lock and @wb->list_lock but list_lock nests
		  outside i_lock.  Drop i_lock and verify that the
		  association hasn't changed after acquiring list_lock.
 i_wb may have changed inbetween, can't use inode_to_wb() 
 @inode already has ref 
  inode_to_wb_and_lock_list - determine an inode's wb and lock it
  @inode: inode of interest
  Same as locked_inode_to_wb_and_lock_list() but @inode->i_lock isn't held
  on entry.
	
	  Multiple inodes can be switched at once.  The switching procedure
	  consists of two parts, separated by a RCU grace period.  To make
	  sure that the second part is executed for each inode gone through
	  the first part, all inode pointers are placed into a NULL-terminated
	  array embedded into struct inode_switch_wbs_context.  Otherwise
	  an inode could be left in a non-consistent state.
	
	  Once I_FREEING or I_WILL_FREE are visible under i_lock, the eviction
	  path owns the inode and we shouldn't modify ->i_io_list.
	
	  Count and transfer stats.  Note that PAGECACHE_TAG_DIRTY points
	  to possibly dirty pages while PAGECACHE_TAG_WRITEBACK points to
	  pages actually under writeback.
	
	  Transfer to @new_wb's IO list if necessary.  If the @inode is dirty,
	  the specific list @inode was on is ignored and the @inode is put on
	  ->b_dirty which is always correct including from ->b_dirty_time.
	  The transfer preserves @inode->dirtied_when ordering.  If the @inode
	  was clean, it means it was on the b_attached list, so move it onto
	  the b_attached list of @new_wb.
 ->i_wb_frn updates may race wbc_detach_inode() but doesn't matter 
	
	  Paired with load_acquire in unlocked_inode_to_wb_begin() and
	  ensures that the new wb is visible if they see !I_WB_SWITCH.
	
	  If @inode switches cgwb membership while sync_inodes_sb() is
	  being issued, sync_inodes_sb() might miss it.  Synchronize.
	
	  By the time control reaches here, RCU grace period has passed
	  since I_WB_SWITCH assertion and all wb stat update transactions
	  between unlocked_inode_to_wb_beginend() are guaranteed to be
	  synchronizing against the i_pages lock.
	 
	  Grabbing old_wb->list_lock, inode->i_lock and the i_pages lock
	  gives us exclusion against all wb related operations on @inode
	  including IO list manipulations and stat updates.
	
	  Paired with smp_mb() in cgroup_writeback_umount().
	  isw_nr_in_flight must be increased before checking SB_ACTIVE and
	  grabbing an inode, otherwise isw_nr_in_flight can be observed as 0
	  in cgroup_writeback_umount() and the isw_wq will be not flushed.
 while holding I_WB_SWITCH, no one else can update the association 
  inode_switch_wbs - change the wb association of an inode
  @inode: target inode
  @new_wb_id: ID of the new wb
  Switch @inode's wb association to the wb identified by @new_wb_id.  The
  switching is performed asynchronously and may fail silently.
 noop if seems to be already in progress 
 avoid queueing a new switch if too many are already in flight 
 find and pin the new wb 
	
	  In addition to synchronizing among switchers, I_WB_SWITCH tells
	  the RCU protected stat update paths to grab the i_page
	  lock so that stat transfer can synchronize against them.
	  Let's continue after I_WB_SWITCH is guaranteed to be visible.
  cleanup_offline_cgwb - detach associated inodes
  @wb: target wb
  Switch all inodes attached to @wb to a nearest living ancestor's wb in order
  to eventually release the dying @wb.  Returns %true if not all inodes were
  switched and the function has to be restarted.
 wb_get() is noop for bdi's wb 
 no attached inodes? bail out 
	
	  In addition to synchronizing among switchers, I_WB_SWITCH tells
	  the RCU protected stat update paths to grab the i_page
	  lock so that stat transfer can synchronize against them.
	  Let's continue after I_WB_SWITCH is guaranteed to be visible.
  wbc_attach_and_unlock_inode - associate wbc with target inode and unlock it
  @wbc: writeback_control of interest
  @inode: target inode
  @inode is locked and about to be written back under the control of @wbc.
  Record @inode's writeback context into @wbc and unlock the i_lock.  On
  writeback completion, wbc_detach_inode() should be called.  This is used
  to track the cgroup writeback context.
	
	  A dying wb indicates that either the blkcg associated with the
	  memcg changed or the associated memcg is dying.  In the first
	  case, a replacement wb should already be available and we should
	  refresh the wb immediately.  In the second case, trying to
	  refresh will keep failing.
  wbc_detach_inode - disassociate wbc from inode and perform foreign detection
  @wbc: writeback_control of the just finished writeback
  To be called after a writeback attempt of an inode finishes and undoes
  wbc_attach_and_unlock_inode().  Can be called under any context.
  As concurrent write sharing of an inode is expected to be very rare and
  memcg only tracks page ownership on first-use basis severely confining
  the usefulness of such sharing, cgroup writeback tracks ownership
  per-inode.  While the support for concurrent write sharing of an inode
  is deemed unnecessary, an inode being written to by different cgroups at
  different points in time is a lot more common, and, more importantly,
  charging only by first-use can too readily lead to grossly incorrect
  behaviors (single foreign page can lead to gigabytes of writeback to be
  incorrectly attributed).
  To resolve this issue, cgroup writeback detects the majority dirtier of
  an inode and transfers the ownership to it.  To avoid unnnecessary
  oscillation, the detection mechanism keeps track of history and gives
  out the switch verdict only if the foreign usage pattern is stable over
  a certain amount of time andor writeback attempts.
  On each writeback attempt, @wbc tries to detect the majority writer
  using Boyer-Moore majority vote algorithm.  In addition to the byte
  count from the majority voting, it also counts the bytes written for the
  current wb and the last round's winner wb (max of last round's current
  wb, the winner from two rounds ago, and the last round's majority
  candidate).  Keeping track of the historical winner helps the algorithm
  to semi-reliably detect the most active writer even when it's not the
  absolute majority.
  Once the winner of the round is determined, whether the winner is
  foreign or not and how much IO time the round consumed is recorded in
  inode->i_wb_frn_history.  If the amount of recorded foreign IO time is
  over a certain threshold, the switch verdict is given.
 pick the winner of this round 
	
	  Calculate the amount of IO time the winner consumed and fold it
	  into the running average kept per inode.  If the consumed IO
	  time is lower than avag  WB_FRN_TIME_CUT_DIV, ignore it for
	  deciding whether to switch or not.  This is to prevent one-off
	  small dirtiers from skewing the verdict.
 immediate catch up on first run 
		
		  The switch verdict is reached if foreign wb's consume
		  more than a certain proportion of IO time in a
		  WB_FRN_TIME_PERIOD.  This is loosely tracked by 16 slot
		  history mask where each bit represents one sixteenth of
		  the period.  Determine the number of slots to shift into
		  history from @max_time.
		
		  Switch if the current wb isn't the consistent winner.
		  If there are multiple closely competing dirtiers, the
		  inode may switch across them repeatedly over time, which
		  is okay.  The main goal is avoiding keeping an inode on
		  the wrong wb for an extended period of time.
	
	  Multiple instances of this function may race to update the
	  following fields but we don't mind occassional inaccuracies.
  wbc_account_cgroup_owner - account writeback to update inode cgroup ownership
  @wbc: writeback_control of the writeback in progress
  @page: page being written out
  @bytes: number of bytes being written out
  @bytes from @page are about to written out during the writeback
  controlled by @wbc.  Keep the book for foreign inode detection.  See
  wbc_detach_inode().
	
	  pageout() path doesn't attach @wbc to the inode being written
	  out.  This is intentional as we don't want the function to block
	  behind a slow cgroup.  Ultimately, we want pageout() to kick off
	  regular writeback instead of writing things out itself.
 dead cgroups shouldn't contribute to inode ownership arbitration 
 Boyer-Moore majority vote algorithm 
  inode_congested - test whether an inode is congested
  @inode: inode to test for congestion (may be NULL)
  @cong_bits: mask of WB_[a]sync_congested bits to test
  Tests whether @inode is congested.  @cong_bits is the mask of congestion
  bits to test and the return value is the mask of set bits.
  If cgroup writeback is enabled for @inode, the congestion state is
  determined by whether the cgwb (cgroup bdi_writeback) for the blkcg
  associated with @inode is congested; otherwise, the root wb's congestion
  state is used.
  @inode is allowed to be NULL as this function is often called on
  mapping->host which is NULL for the swapper space.
	
	  Once set, ->i_wb never becomes NULL while the inode is alive.
	  Start transaction iff ->i_wb is visible.
  wb_split_bdi_pages - split nr_pages to write according to bandwidth
  @wb: target bdi_writeback to split @nr_pages to
  @nr_pages: number of pages to write for the whole bdi
  Split @wb's portion of @nr_pages according to @wb's write bandwidth in
  relation to the total write bandwidth of all wb's w dirty inodes on
  @wb->bdi.
	
	  This may be called on clean wb's and proportional distribution
	  may not make sense, just use the original @nr_pages in those
	  cases.  In general, we wanna err on the side of writing more.
  bdi_split_work_to_wbs - split a wb_writeback_work to all wb's of a bdi
  @bdi: target backing_dev_info
  @base_work: wb_writeback_work to issue
  @skip_if_busy: skip wb's which already have writeback in progress
  Split and issue @base_work to all wb's (bdi_writeback's) of @bdi which
  have dirty inodes.  If @base_work->nr_page isn't %LONG_MAX, it's
  distributed to the busy wbs according to each wb's proportion in the
  total active write bandwidth of @bdi.
 SYNC_ALL writes out I_DIRTY_TIME too 
 alloc failed, execute synchronously using on-stack fallback 
		
		  Pin @wb so that it stays on @bdi->wb_list.  This allows
		  continuing iteration from @wb after dropping and
		  regrabbing rcu read lock.
  cgroup_writeback_by_id - initiate cgroup writeback from bdi and memcg IDs
  @bdi_id: target bdi id
  @memcg_id: target memcg css id
  @reason: reason why some writeback work initiated
  @done: target wb_completion
  Initiate flush of the bdi_writeback identified by @bdi_id and @memcg_id
  with the specified parameters.
 lookup bdi and memcg 
	
	  And find the associated wb.  If the wb isn't there already
	  there's nothing to flush, don't create one.
	
	  The caller is attempting to write out most of
	  the currently dirty pages.  Let's take the current dirty page
	  count and inflate it by 25% which should be large enough to
	  flush out most dirty pages while avoiding getting livelocked by
	  concurrent dirtiers.
	 
	  BTW the memcg stats are flushed periodically and this is best-effort
	  estimation, so some potential error is ok.
 issue the writeback work 
  cgroup_writeback_umount - flush inode wb switches for umount
  This function is called when a super_block is about to be destroyed and
  flushes in-flight inode wb switches.  An inode wb switch goes through
  RCU and then workqueue, so the two need to be flushed in order to ensure
  that all previously scheduled switches are finished.  As wb switches are
  rare occurrences and synchronize_rcu() can take a while, perform
  flushing iff wb switches are in flight.
	
	  SB_ACTIVE should be reliably cleared before checking
	  isw_nr_in_flight, see generic_shutdown_super().
		
		  Use rcu_barrier() to wait for all pending callbacks to
		  ensure that all in-flight wb switches are in the workqueue.
 CONFIG_CGROUP_WRITEBACK 
 CONFIG_CGROUP_WRITEBACK 
  Add in the number of potentially dirty inodes, because each inode
  write can dirty pagecache in the underlying blockdev.
	
	  All callers of this function want to start writeback of all
	  dirty pages. Places like vmscan can call this at a very
	  high frequency, causing pointless allocations of tons of
	  work items and keeping the flusher threads busy retrieving
	  that work. Ensure that we only allow one of them pending and
	  inflight at the time.
  wb_start_background_writeback - start background writeback
  @wb: bdi_writback to write from
  Description:
    This makes sure WB_SYNC_NONE background writeback happens. When
    this function returns, it is only guaranteed that for given wb
    some IO is happening if we are over background dirty threshold.
    Caller need not hold sb s_umount semaphore.
	
	  We just wake up the flusher thread. It will perform background
	  writeback as soon as there is no other work to do.
  Remove the inode from the writeback list it is on.
  mark an inode as under writeback on the sb
  clear an inode as under writeback on the sb
  Redirty an inode: set its when-it-was dirtied timestamp and move it to the
  furthest end of its superblock's dirty-inode list.
  Before stamping the inode's ->dirtied_when, we check to see whether it is
  already the most-recently-dirtied inode on the b_dirty list.  If that is
  the case then the inode must have been redirtied while it was being written
  out and we don't reset its dirtied_when.
  requeue inode for re-scanning after bdi->b_io list is exhausted.
 If inode is clean an unused, put it into LRU now... 
 Waiters must see I_SYNC cleared before being woken up 
	
	  For inodes being constantly redirtied, dirtied_when can get stuck.
	  It _appears_ to be in the future, but is actually in distant past.
	  This test is necessary to prevent such wrapped-around relative times
	  from permanently stopping the whole bdi writeback.
  Move expired (dirtied before dirtied_before) dirty inodes from
  @delaying_queue to @dispatch_queue.
 just one sb in list, splice to dispatch_queue and we're done 
 Move inodes from one superblock together 
  Queue all expired dirty inodes for io, eldest first.
  Before
          newly dirtied     b_dirty    b_io    b_more_io
          =============>    gf         edc     BA
  After
          newly dirtied     b_dirty    b_io    b_more_io
          =============>    g          fBAedc
                                            |
                                            +--> dequeue for IO
  Wait for writeback on an inode to complete. Called with i_lock held.
  Caller must make sure inode cannot go away when we drop i_lock.
  Wait for writeback on an inode to complete. Caller must have inode pinned.
  Sleep until I_SYNC is cleared. This function must be called with i_lock
  held and drops it. It is aimed for callers not holding any inode reference
  so once i_lock is dropped, inode can go away.
  Find proper writeback list for the inode depending on its current state and
  possibly also change of its state while we were doing writeback.  Here we
  handle things such as livelock prevention or fairness of writeback among
  inodes. This function can be called only by flusher thread - noone else
  processes all inodes in writeback lists and requeueing inodes behind flusher
  thread's back can have unexpected consequences.
	
	  Sync livelock prevention. Each inode is tagged and synced in one
	  shot. If still dirty, it will be redirty_tail()'ed below.  Update
	  the dirty time to prevent enqueue and sync it again.
		
		  writeback is not making progress due to locked
		  buffers. Skip this inode for now.
		
		  We didn't write back all the pages.  nfs_writepages()
		  sometimes bales out without doing anything.
 Slice used up. Queue for next turn. 
			
			  Writeback blocked by something other than
			  congestion. Delay the inode for some time to
			  avoid spinning on the CPU (100% iowait)
			  retrying writeback of the dirty pageinode
			  that cannot be performed immediately.
		
		  Filesystems can dirty the inode during writeback operations,
		  such as delayed allocation during submission or metadata
		  updates after data IO completion.
 The inode is clean. Remove from writeback lists. 
  Write out an inode and its dirty pages (or some of its dirty pages, depending
  on @wbc->nr_to_write), and clear the relevant dirty flags from i_state.
  This doesn't remove the inode from the writeback list it is on, except
  potentially to move it from b_dirty_time to b_dirty due to timestamp
  expiration.  The caller is otherwise responsible for writeback list handling.
  The caller is also responsible for setting the I_SYNC flag beforehand and
  calling inode_sync_complete() to clear it afterwards.
	
	  Make sure to wait on the data before writing out the metadata.
	  This is important for filesystems that modify metadata on data
	  IO completion. We don't do it for sync(2) writeback because it has a
	  separate, external IO completion path and ->sync_fs for guaranteeing
	  inode metadata is written back correctly.
	
	  If the inode has dirty timestamps and we need to write them, call
	  mark_inode_dirty_sync() to notify the filesystem about it and to
	  change I_DIRTY_TIME into I_DIRTY_SYNC.
	
	  Get and clear the dirty flags from i_state.  This needs to be done
	  after calling writepages because some filesystems may redirty the
	  inode during writepages due to delalloc.  It also needs to be done
	  after handling timestamp expiration, as that may dirty the inode too.
	
	  Paired with smp_mb() in __mark_inode_dirty().  This allows
	  __mark_inode_dirty() to test i_state without grabbing i_lock -
	  either they see the I_DIRTY bits cleared or we see the dirtied
	  inode.
	 
	  I_DIRTY_PAGES is always cleared together above even if @mapping
	  still has dirty pages.  The flag is reinstated after smp_mb() if
	  necessary.  This guarantees that either __mark_inode_dirty()
	  sees clear I_DIRTY_PAGES or we see PAGECACHE_TAG_DIRTY.
 Don't write the inode if only I_DIRTY_PAGES was set 
  Write out an inode's dirty data and metadata on-demand, i.e. separately from
  the regular batched writeback done by the flusher threads in
  writeback_sb_inodes().  @wbc controls various aspects of the write, such as
  whether it is a data-integrity sync (%WB_SYNC_ALL) or not (%WB_SYNC_NONE).
  To prevent the inode from going away, either the caller must have a reference
  to the inode, or the inode must have I_WILL_FREE or I_FREEING set.
		
		  Writeback is already running on the inode.  For WB_SYNC_NONE,
		  that's enough and we can just return.  For WB_SYNC_ALL, we
		  must wait for the existing writeback to complete, then do
		  writeback again if there's anything left.
	
	  If the inode is already fully clean, then there's nothing to do.
	 
	  For data-integrity syncs we also need to check whether any pages are
	  still under writeback, e.g. due to prior WB_SYNC_NONE writeback.  If
	  there are any such pages, we'll need to wait for them.
	
	  If the inode is now fully clean, then it can be safely removed from
	  its writeback list (if any).  Otherwise the flusher threads are
	  responsible for the writeback lists.
	
	  WB_SYNC_ALL mode does livelock avoidance by syncing dirty
	  inodespages in one big loop. Setting wbc.nr_to_write=LONG_MAX
	  here avoids calling into writeback_inodes_wb() more than once.
	 
	  The intended call sequence for WB_SYNC_ALL writeback is:
	 
	       wb_writeback()
	           writeback_sb_inodes()       <== called only once
	               write_cache_pages()     <== called once for each inode
	                    (quickly) tag currently dirty pages
	                    (maybe slowly) sync all tagged pages
  Write a portion of b_io inodes which belong to @sb.
  Return the number of pages andor inodes written.
  NOTE! This is called with wb->list_lock held, and will
  unlock and relock that for each inode it ends up doing
  IO for.
 count both pages and inodes 
				
				  We only want to write back data for this
				  superblock, move all inodes not belonging
				  to it back onto the dirty list.
			
			  The inode belongs to a different superblock.
			  Bounce back to the caller to unpin this and
			  pin the next superblock.
		
		  Don't bother with new inodes or inodes being freed, first
		  kind does not need periodic writeout yet, and for the latter
		  kind writeout is handled by the freer.
			
			  If this inode is locked for writeback and we are not
			  doing writeback-for-data-integrity, move it to
			  b_more_io so that writeback can proceed with the
			  other inodes on s_io.
			 
			  We'll have another go at writing back this inode
			  when we completed a full scan of b_io.
		
		  We already requeued the inode if it had I_SYNC set and we
		  are doing WB_SYNC_NONE writeback. So this catches only the
		  WB_SYNC_ALL case.
 Wait for I_SYNC. This function drops i_lock... 
 Inode may be gone, start again 
		
		  We use I_SYNC to pin the inode in memory. While it is set
		  evict_inode() will wait so the inode cannot be freed.
			
			  We're trying to balance between building up a nice
			  long list of IOs to improve our merge rate, and
			  getting those IOs out quickly for anyone throttling
			  in balance_dirty_pages().  cond_resched() doesn't
			  unplug, so get our IOs out the door before we
			  give up the CPU.
		
		  Requeue @inode if still dirty.  Be careful as @inode may
		  have been switched to another wb in the meantime.
		
		  bail out to wb_writeback() often enough to check
		  background threshold and other termination conditions.
			
			  trylock_super() may fail consistently due to
			  s_umount being grabbed by someone else. Don't use
			  requeue_io() to avoid busy retrying the inodesb.
 refer to the same tests at the end of writeback_sb_inodes 
 Leave any unwritten inodes on b_io 
  Explicit flushing or periodic writeback of "old" data.
  Define "old": the first time one of an inode's pages is dirtied, we mark the
  dirtying-time in the inode's address_space.  So this periodic writeback code
  just walks the superblock inode list, writing back any inodes which are
  older than a specific point in time.
  Try to run once per dirty_writeback_interval.  But if a writeback event
  takes longer than a dirty_writeback_interval interval, then leave a
  one-second gap.
  dirtied_before takes precedence over nr_to_write.  So we'll only write back
  all dirty pages if they are all attached to "old" mappings.
		
		  Stop writeback when nr_pages has been consumed
		
		  Background writeout and kupdate-style writeback may
		  run forever. Stop them if there is other work to do
		  so that e.g. sync can proceed. They'll be restarted
		  after the other works are all done.
		
		  For background writeout, stop when we are below the
		  background dirty threshold
		
		  Kupdate and background works are special and we want to
		  include all inodes that need writing. Livelock avoidance is
		  handled by these works yielding to any other work so we are
		  safe.
		
		  Did we write something? Try for more
		 
		  Dirty inodes are moved to b_io for writeback in batches.
		  The completion of the current batch does not necessarily
		  mean the overall work is done. So we keep looping as long
		  as made some progress on cleaning pages or inodes.
		
		  No more inodes for IO, bail
		
		  Nothing written. Wait for some inode to
		  become available for writeback. Otherwise
		  we'll just busyloop.
 This function drops i_lock... 
  Return the next wb_writeback_work struct that hasn't been processed yet.
	
	  When set to zero, disable periodic writeback
  Retrieve work items and do the writeback they describe
	
	  Check for a flush-everything request
	
	  Check for periodic writeback, kupdated() style
  Handle writeback of dirty data for the device backed by this bdi. Also
  reschedules periodically and does kupdated style flushing.
		
		  The normal path.  Keep writing back @wb until its
		  work_list is empty.  Note that this path is also taken
		  if @wb is shutting down even when we're running off the
		  rescuer as work_list needs to be drained.
		
		  bdi_wq can't get enough workers and we're running off
		  the emergency worker.  Don't hog it.  Hopefully, 1024 is
		  enough for efficient IO.
  Start writeback of `nr_pages' pages on this bdi. If `nr_pages' is zero,
  write back the whole world.
  Wakeup the flusher threads to start writeback of all currently dirty pages
	
	  If we are expecting writeback progress we must submit plugged IO.
  Wake up bdi's periodically to make sure dirtytime inodes gets
  written back periodically.  We deliberately do not check the
  b_dirtytime list in wb_has_dirty_io(), since this would cause the
  kernel to be constantly waking up once there are any dirtytime
  inodes on the system.  So instead we define a separate delayed work
  function which gets called much more rarely.  (By default, only
  once every 12 hours.)
  If there is any other write activity going on in the file system,
  this function won't be necessary.  But if the only thing that has
  happened on the file system is a dirtytime inode caused by an atime
  update, we need this infrastructure below to make sure that inode
  eventually gets pushed out to disk.
  __mark_inode_dirty -	internal function to mark an inode dirty
  @inode: inode to mark
  @flags: what kind of dirty, e.g. I_DIRTY_SYNC.  This can be a combination of
 	   multiple I_DIRTY_ flags, except that I_DIRTY_TIME can't be combined
 	   with I_DIRTY_PAGES.
  Mark an inode as dirty.  We notify the filesystem, then update the inode's
  dirty flags.  Then, if needed we add the inode to the appropriate dirty list.
  Most callers should use mark_inode_dirty() or mark_inode_dirty_sync()
  instead of calling this directly.
  CAREFUL!  We only add the inode to the dirty list if it is hashed or if it
  refers to a blockdev.  Unhashed inodes will never be added to the dirty list
  even if they are later hashed, as they will have been marked dirty already.
  In short, ensure you hash any inodes _before_ you start marking them dirty.
  Note that for blockdevs, inode->dirtied_when represents the dirtying time of
  the block-special inode (devhda1) itself.  And the ->dirtied_when field of
  the kernel-internal blockdev inode represents the dirtying time of the
  blockdev's pages.  This is why for I_DIRTY_PAGES we always use
  page->mapping->host, so the page-dirtying time is recorded in the internal
  blockdev inode.
		
		  Notify the filesystem about the inode being dirtied, so that
		  (if needed) it can update on-disk fields and journal the
		  inode.  This is only needed when the inode itself is being
		  dirtied now.  I.e. it's only needed for I_DIRTY_INODE, not
		  for just I_DIRTY_PAGES or I_DIRTY_TIME.
 I_DIRTY_INODE supersedes I_DIRTY_TIME. 
		
		  Else it's either I_DIRTY_PAGES, I_DIRTY_TIME, or nothing.
		  (We don't support setting both I_DIRTY_PAGES and I_DIRTY_TIME
		  in one call to __mark_inode_dirty().)
	
	  Paired with smp_mb() in __writeback_single_inode() for the
	  following lockless i_state test.  See there for details.
 I_DIRTY_INODE supersedes I_DIRTY_TIME. 
		
		  If the inode is queued for writeback by flush worker, just
		  update its dirty state. Once the flush worker is done with
		  the inode it will place it on the appropriate superblock
		  list, based upon its state.
		
		  Only add valid (hashed) inodes to the superblock's
		  dirty list.  Add blockdev inodes as well.
		
		  If the inode was already on b_dirtyb_iob_more_io, don't
		  reposition it (that would break b_dirty time-ordering).
			
			  If this is the first dirty inode for this bdi,
			  we have to wake-up the corresponding bdi thread
			  to make sure background write-back happens
			  later.
  The @s_sync_lock is used to serialise concurrent sync operations
  to avoid lock contention problems with concurrent wait_sb_inodes() calls.
  Concurrent callers will block on the s_sync_lock rather than doing contending
  walks. The queueing maintains sync(2) required behaviour as all the IO that
  has been issued up to the time this function is enter is guaranteed to be
  completed by the time we have gained the lock and waited for all IO that is
  in progress regardless of the order callers are granted the lock.
	
	  We need to be protected against the filesystem going from
	  ro to rw or vice versa.
	
	  Splice the writeback list onto a temporary list to avoid waiting on
	  inodes that have started writeback after this point.
	 
	  Use rcu_read_lock() to keep the inodes around until we have a
	  reference. s_inode_wblist_lock protects sb->s_inodes_wb as well as
	  the local list because inodes can be dropped from either by writeback
	  completion.
	
	  Data integrity sync. Must wait for all pages under writeback, because
	  there may have been pages dirtied before our sync call, but which had
	  writeout started before we write it out.  In which case, the inode
	  may not be on the dirty list, but we still have to wait for that
	  writeout.
		
		  Move each inode back to the wb list before we drop the lock
		  to preserve consistency between i_wb_list and the mapping
		  writeback tag. Writeback completion is responsible to remove
		  the inode from either list once the writeback tag is cleared.
		
		  The mapping can appear untagged while still on-list since we
		  do not have the mapping lock. Skip it here, wb completion
		  will remove it.
		
		  We keep the error status of individual mapping so that
		  applications can catch the writeback error using fsync(2).
		  See filemap_fdatawait_keep_errors() for details.
  writeback_inodes_sb_nr -	writeback dirty inodes from given super_block
  @sb: the superblock
  @nr: the number of pages to write
  @reason: reason why some writeback work initiated
  Start writeback on some inodes on this super_block. No guarantees are made
  on how many (if any) will be written, and this function does not wait
  for IO completion of submitted IO.
  writeback_inodes_sb	-	writeback dirty inodes from given super_block
  @sb: the superblock
  @reason: reason why some writeback work was initiated
  Start writeback on some inodes on this super_block. No guarantees are made
  on how many (if any) will be written, and this function does not wait
  for IO completion of submitted IO.
  try_to_writeback_inodes_sb - try to start writeback if none underway
  @sb: the superblock
  @reason: reason why some writeback work was initiated
  Invoke __writeback_inodes_sb_nr if no writeback is currently underway.
  sync_inodes_sb	-	sync sb inode pages
  @sb: the superblock
  This function writes and waits on any dirty inode belonging to this
  super_block.
	
	  Can't skip on !bdi_has_dirty() because we should wait for !dirty
	  inodes under writeback and I_DIRTY_TIME inodes ignored by
	  bdi_has_dirty() need to be written out too.
 protect against inode wb switch, see inode_switch_wbs_work_fn() 
  write_inode_now	-	write an inode to disk
  @inode: inode to write to disk
  @sync: whether the write should be synchronous or not
  This function commits an inode to disk immediately if it is dirty. This is
  primarily needed by knfsd.
  The caller must either have a ref on the inode or must have set I_WILL_FREE.
  sync_inode_metadata - write an inode to disk
  @inode: the inode to sync
  @wait: wait for IO to complete.
  Write an inode to disk and adjust its dirty state after completion.
  Note: only writes the actual inode, no associated data or other metadata.
 metadata-only 
 SPDX-License-Identifier: GPL-2.0-only
 does _NOT_ require i_mutex to be held.
  This function cannot be inlined since i_size_{read,write} is rather
  heavy-weight on 32-bit systems
	
	  i_size_read() includes its own seqlocking and protection from
	  preemption (see includelinuxfs.h): we need nothing extra for
	  that here, and prefer to avoid nesting locks than attempt to keep
	  i_size and i_blocks in sync together.
	
	  But on 32-bit, we ought to make an effort to keep the two halves of
	  i_blocks in sync despite SMP or PREEMPTION - though stat's
	  generic_fillattr() doesn't bother, and we won't be applying quotas
	  (where i_blocks does become important) at the upper level.
	 
	  We don't actually know what locking is used at the lower level;
	  but if it's a filesystem that supports quotas, it will be using
	  i_lock as in inode_add_bytes().
	
	  If CONFIG_SMP or CONFIG_PREEMPTION on 32-bit, it's vital for
	  fsstack_copy_inode_size() to hold some lock around
	  i_size_write(), otherwise i_size_read() may spin forever (see
	  includelinuxfs.h).  We don't necessarily hold i_mutex when this
	  is called, so take i_lock for that case.
	 
	  And if on 32-bit, continue our effort to keep the two halves of
	  i_blocks in sync despite SMP or PREEMPTION: use i_lock for that case
	  too, and do both at once by combining the tests.
	 
	  There is none of this locking overhead in the 64-bit case.
 copy all attributes 
 SPDX-License-Identifier: GPL-2.0-only
   linuxfspnode.c
  (C) Copyright IBM Corporation 2005.
 	Author : Ram Pai (linuxram@us.ibm.com)
 return the next shared peer mount of @p 
 Check the namespace first for optimization 
  Get ID of closest dominating peer group having a representative
  under the given root.
  Caller must hold namespace_sem
		
		  slave 'mnt' to a peer mount that has the
		  same root dentry. If none is available then
		  slave it to anything that is available.
  vfsmount lock must be held for write
  get the next mount in the propagation tree.
  @m: the mount seen last
  @origin: the original mount from where the tree walk initiated
  Note that peer groups form contiguous segments of slave lists.
  We rely on that in get_source() to be able to find out if
  vfsmount found while iterating with propagation_next() is
  a peer of one we'd found earlier.
 are there any slaves of this mount? 
 back at master 
	
	  Advance m such that propagation_next will not return
	  the slaves of m.
 m is the last peer 
 all accesses are serialized by namespace_sem 
 skip ones added by this propagate_mnt() 
 skip if mountpoint isn't covered by it 
 beginning of peer group among the slaves? 
  mount 'source_mnt' under the destination 'dest_mnt' at
  dentry 'dest_dentry'. And propagate that mount to
  all the peer and slave mounts of 'dest_mnt'.
  Link all the new mounts into a propagation tree headed at
  source_mnt. Also link all the new mounts using ->mnt_list
  headed at source_mnt's ->mnt_list
  @dest_mnt: destination mount.
  @dest_dentry: destination dentry.
  @source_mnt: source mount.
  @tree_list : list of heads of trees to be attached.
	
	  we don't want to bother passing tons of arguments to
	  propagate_one(); everything is serialized by namespace_sem,
	  so globals will do just fine.
 all peers of dest_mnt, except dest_mnt itself 
 all slave groups 
 everything in that slave group 
 If there is exactly one mount covering mnt completely return it. 
  return true if the refcount is greater than count
  check if the mount 'mnt' can be unmounted successfully.
  @mnt: the mount to be checked for unmount
  NOTE: unmounting 'mnt' would naturally propagate to all
  other mounts its parent propagates to.
  Check if any of these mounts that do not have submounts
  have more references than 'refcnt'. If so return busy.
  vfsmount lock must be held for write
	
	  quickly check if the current mount can be unmounted.
	  If not, we don't have to go checking for all other
	  mounts
		 Is there exactly one mount on the child that covers
		  it completely whose reference should be ignored?
  Clear MNT_LOCKED when it can be shown to be safe.
  mount_lock lock must be held for write
  NOTE: unmounting 'mnt' naturally propagates to all other mounts its
  parent propagates to.
	
	  The state of the parent won't change if this mount is
	  already unmounted or marked as without children.
	 Verify topper is the only grandchild that has not been
	  speculatively unmounted.
 Found a mounted child 
 Mark mounts that can be unmounted if not locked 
 If a mount is without children and not locked umount it. 
 topper? 
 Restore mounts to a clean working state 
 Should this mount be reparented? 
  collect all mounts that receive propagation from the mount in @list,
  and return these additional mounts in the same list.
  @list: the list of mounts to be unmounted.
  vfsmount lock must be held for write
 Find candidates for unmounting 
		
		  If this mount has already been visited it is known that it's
		  entire peer group and all of their slaves in the propagation
		  tree for the mountpoint has already been visited and there is
		  no need to visit them again.
				
				  If the child has already been visited it is
				  know that it's entire peer group and all of
				  their slaves in the propgation tree for the
				  mountpoint has already been visited and there
				  is no need to visit this subtree again.
				
				  We have come accross an partially unmounted
				  mount in list that has not been visited yet.
				  Remember it has been visited and continue
				  about our merry way.
 Check the child and parents while progress is made 
 Is the parent a umount candidate? 
 SPDX-License-Identifier: GPL-2.0-only
   linuxfslocks.c
  We implement four types of file locks: BSD locks, posix locks, open
  file description locks, and leases.  For details about BSD locks,
  see the flock(2) man page; for details about the other three, see
  fcntl(2).
  Locking conflicts and dependencies:
  If multiple threads attempt to lock the same byte (or flock the same file)
  only one can be granted the lock, and other must wait their turn.
  The first lock has been "applied" or "granted", the others are "waiting"
  and are "blocked" by the "applied" lock..
  Waiting and applied locks are all kept in trees whose properties are:
 	- the root of a tree may be an applied or waiting lock.
 	- every other node in the tree is a waiting lock that
 	  conflicts with every ancestor of that node.
  Every such tree begins life as a waiting singleton which obviously
  satisfies the above properties.
  The only ways we modify trees preserve these properties:
 	1. We may add a new leaf node, but only after first verifying that it
 	   conflicts with all of its ancestors.
 	2. We may remove the root of a tree, creating a new singleton
 	   tree from the root and N new trees rooted in the immediate
 	   children.
 	3. If the root of a tree is not currently an applied lock, we may
 	   apply it (if possible).
 	4. We may upgrade the root of the tree (either extend its range,
 	   or upgrade its entire range from read to write).
  When an applied lock is modified in a way that reduces or downgrades any
  part of its range, we remove all its children (2 above).  This particularly
  happens when a lock is unlocked.
  For each of those child trees we "wake up" the thread which is
  waiting for the lock so it can continue handling as follows: if the
  root of the tree applies, we do so (3).  If it doesn't, it must
  conflict with some applied lock.  We remove (wake up) all of its children
  (2), and add it is a new leaf to the tree rooted in the applied
  lock (1).  We then repeat the process recursively with those
  children.
  The global file_lock_list is only used for displaying proclocks, so we
  keep a list on each CPU, with each list protected by its own spinlock.
  Global serialization is done using file_rwsem.
  Note that alterations to the list also require that the relevant flc_lock is
  held.
  The blocked_hash is used to find POSIX lock loops for deadlock detection.
  It is protected by blocked_lock_lock.
  We hash locks by lockowner in order to optimize searching for the lock a
  particular lockowner is waiting on.
  FIXME: make this value scale via some heuristic? We generally will want more
  buckets when we have more lockowners holding locks, but that's a little
  difficult to determine without knowing what the workload will look like.
  This lock protects the blocked_hash. Generally, if you're accessing it, you
  want to be holding this lock.
  In addition, it also protects the fl->fl_blocked_requests list, and the
  fl->fl_blocker pointer for file_lock structures that are acting as lock
  requests (in contrast to those that are acting as records of acquired locks).
  Note that when we acquire this lock in order to change the above fields,
  we often hold the flc_lock as well. In certain cases, when reading the fields
  protected by this lock, we can skip acquiring it iff we already hold the
  flc_lock.
 paired with cmpxchg() below 
	
	  Assign the pointer if it's not already assigned. If it is, then
	  free the context we just allocated.
 Allocate an empty lock structure. 
 Free a lock which is not in use. 
  Initialize a new lock from an existing file_lock structure.
 "new" must be a freshly-initialized lock 
	
	  As ctx->flc_lock is held, new requests cannot be added to
	  ->fl_blocked_requests, so we don't need a lock to check if it
	  is empty.
 Fill in a file_lock structure with an appropriate FLOCK lock. 
	 POSIX-1996 leaves the case l->l_len < 0 undefined;
 Verify a "struct flock" and copy it to a "struct file_lock" as a POSIX
  style lock.
 default lease lock manager operations 
	
	  fasync_insert_entry() returns the old entry if any. If there was no
	  old entry, then it used "priv" and inserted it into the fasync list.
	  Clear the pointer to indicate that it shouldn't be freed.
  Initialize a lease, use the default lock manager operations
 Allocate a file_lock initialised to this type of lease 
 Check if two locks overlap each other.
  Check whether two locks have the same owner.
 Must be called with the flc_lock held! 
 Must be called with the flc_lock held! 
	
	  Avoid taking lock if already unhashed. This is safe since this check
	  is done while holding the flc_lock, and new insertions into the list
	  also require that it be held.
 Remove waiter from blocker's block list.
  When blocker ends up pointing to itself then the list is empty.
  Must be called with blocked_lock_lock held.
		
		  The setting of fl_blocker to NULL marks the "done"
		  point in deleting a block. Paired with acquire at the top
		  of locks_delete_block().
 	locks_delete_block - stop waiting for a file lock
 	@waiter: the lock which was waiting
 	lockdnfsd need to disconnect the lock while working on it.
	
	  If fl_blocker is NULL, it won't be set again as this thread "owns"
	  the lock and is the only one that might try to claim the lock.
	 
	  We use acquirerelease to manage fl_blocker so that we can
	  optimize away taking the blocked_lock_lock in many cases.
	 
	  The smp_load_acquire guarantees two things:
	 
	  1 that fl_blocked_requests can be tested locklessly. If something
	  was recently added to that list it must have been in a locked region
	  before the locked region when fl_blocker was set to NULL.
	 
	  2 that no other thread is accessing 'waiter', so it is safe to free
	  it.  __locks_wake_up_blocks is careful not to touch waiter after
	  fl_blocker is released.
	 
	  If a lockless check of fl_blocker shows it to be NULL, we know that
	  no new locks can be inserted into its fl_blocked_requests list, and
	  can avoid doing anything further if the list is empty.
	
	  The setting of fl_blocker to NULL marks the "done" point in deleting
	  a block. Paired with acquire at the top of this function.
 Insert waiter into blocker's block list.
  We use a circular list so that processes can be easily woken up in
  the order they blocked. The documentation doesn't require this but
  it seems like the reasonable thing to do.
  Must be called with both the flc_lock and blocked_lock_lock held. The
  fl_blocked_requests list itself is protected by the blocked_lock_lock,
  but by ensuring that the flc_lock is also held on insertions we can avoid
  taking the blocked_lock_lock in some cases when we see that the
  fl_blocked_requests list is empty.
  Rather than just adding to the list, we check for conflicts with any existing
  waiters, and add beneath any waiter that blocks the new waiter.
  Thus wakeups don't happen until needed.
	 The requests in waiter->fl_blocked are known to conflict with
	  waiter, but might not conflict with blocker, or the requests
	  and lock which block it.  So they all need to be woken.
 Must be called with flc_lock held. 
  Wake up processes blocked waiting for blocker.
  Must be called with the inode->flc_lock held!
	
	  Avoid taking global lock if list is empty. This is safe since new
	  blocked requests are only added to the list under the flc_lock, and
	  the flc_lock is always held here. Note that removal from the
	  fl_blocked_requests list does not require the flc_lock, so we must
	  recheck list_empty() after acquiring the blocked_lock_lock.
 Determine if lock sys_fl blocks lock caller_fl. Common functionality
  checks for sharedexclusive status of overlapping locks.
 Determine if lock sys_fl blocks lock caller_fl. POSIX specific
  checking before calling the locks_conflict().
	 POSIX locks owned by the same process do not conflict with
	  each other.
 Check whether they overlap 
 Determine if lock sys_fl blocks lock caller_fl. FLOCK specific
  checking before calling the locks_conflict().
	 FLOCK locks referring to the same filp do not conflict with
	  each other.
  Deadlock detection:
  We attempt to detect deadlocks that are due purely to posix file
  locks.
  We assume that a task can be waiting for at most one lock at a time.
  So for any acquired lock, the process holding that lock may be
  waiting on at most one other lock.  That lock in turns may be held by
  someone waiting for at most one other lock.  Given a requested lock
  caller_fl which is about to wait for a conflicting lock block_fl, we
  follow this chain of waiters to ensure we are not about to create a
  cycle.
  Since we do this before we ever put a process to sleep on a lock, we
  are ensured that there is never a cycle; that is what guarantees that
  the while() loop in posix_locks_deadlock() eventually completes.
  Note: the above assumption may not be true when handling lock
  requests from a broken NFS client. It may also fail in the presence
  of tasks (such as posix threads) sharing the same open file table.
  To handle those cases, we just bail out after a few iterations.
  For FL_OFDLCK locks, the owner is the filp, not the files_struct.
  Because the owner is not even nominally tied to a thread of
  execution, the deadlock detection below can't reasonably work well. Just
  skip it for those.
  In principle, we could do a more limited deadlock detection on FL_OFDLCK
  locks that just checks for the case where two tasks are attempting to
  upgrade from read to write locks on the same inode.
 Find a lock that the owner of the given block_fl is blocking on. 
 Must be called with the blocked_lock_lock held! 
	
	  This deadlock detector can't reasonably detect deadlocks with
	  FL_OFDLCK locks, since they aren't owned by a process, per-se.
 Try to create a FLOCK lock on filp. We always insert new FLOCK locks
  after any leases, but before any posix locks.
  Note that if called with an FL_EXISTS argument, the caller may determine
  whether or not a lock was successfully freed by testing the return
  value for -ENOENT.
	
	  We may need two file_lock structures for this operation,
	  so we get them in advance to avoid races.
	 
	  In some cases we can be sure, that no new locks will be needed
	
	  New lock request. Walk all POSIX locks and look for conflicts. If
	  there are any, either return error or put the request on the
	  blocker's list of waiters and the global blocked_hash.
			
			  Deadlock detection and insertion into the blocked
			  locks list must be done while holding the same lock!
			
			  Ensure that we don't find any locks blocked on this
			  request during deadlock detection.
 If we're just looking for a conflict, we're done. 
 Find the first old lock with the same owner as the new lock 
 Process locks with this owner. 
 Detect adjacent or overlapping regions (if same lock type) 
			 In all comparisons of start vs end, use
			  "start - 1" rather than "end + 1". If end
			  is OFFSET_MAX, end + 1 will become negative.
			 If the next lock in the list has entirely bigger
			  addresses than the new one, insert the lock here.
			 If we come here, the new and old lock are of the
			  same type and adjacent or overlapping. Make one
			  lock yielding from the lower start address of both
			  locks to the higher end address.
			 Processing for different lock types is a bit
			  more complex.
			 If the next lock in the list has a higher end
			  address than the new one, insert the new one here.
				 The new lock completely replaces an old
				  one (This may happen several times).
				
				  Replace the old lock with new_fl, and
				  remove the old one. It's safe to do the
				  insert here since we know that we won't be
				  using new_fl later, and that the lock is
				  just replacing an existing lock.
	
	  The above code only modifies existing locks in case of merging or
	  replacing. If new lock(s) need to be inserted all modifications are
	  done below this, so it's safe yet to bail out.
 "no luck" 
			 The new lock breaks the old one in two pieces,
			  so we have to use the second new lock.
	
	  Free any unused locks.
  posix_lock_file - Apply a POSIX-style lock to a file
  @filp: The file to apply the lock to
  @fl: The lock to be applied
  @conflock: Place to return a copy of the conflicting lock, if found.
  Add a POSIX style lock to a file.
  We merge adjacent & overlapping locks whenever possible.
  POSIX locks are sorted by owner task, then by starting address
  Note that if called with an FL_EXISTS argument, the caller may determine
  whether or not a lock was successfully freed by testing the return
  value for -ENOENT.
  posix_lock_inode_wait - Apply a POSIX-style lock to a file
  @inode: inode of file to which lock request should be applied
  @fl: The lock to be applied
  Apply a POSIX style lock request to an inode.
 We already had a lease on this file; just change its type 
 0 is a special value meaning "this never expires": 
 	__break_lease	-	revoke all outstanding leases on file
 	@inode: the inode of the file to return
 	@mode: O_RDONLY: break only write leases; O_WRONLY or O_RDWR:
 	    break all leases
 	@type: FL_LEASE: break leases and delegations; FL_DELEG: break
 	    only delegations
 	break_lease (inlined for speed) has checked there already is at least
 	some kind of lock (maybe a lease) on this file.  Leases are broken on
 	a call to open() or truncate().  This function can sleep unless you
 	specified %O_NONBLOCK to your open().
 typically we will check that ctx is non-NULL before calling 
 so that 0 means no break time 
		
		  Wait for the next conflicting lease that has not been
		  broken yet
 	lease_get_mtime - update modified time of an inode with exclusive lease
 	@inode: the inode
       @time:  pointer to a timespec which contains the last modified time
  This is to force NFS clients to flush their caches for files with
  exclusive leases.  The justification is that if someone has an
  exclusive lease, then they could be modifying it.
 	fcntl_getlease - Enquire what lease is currently active
 	@filp: the file
 	The value returned by this function will be one of
 	(if no lease break is pending):
 	%F_RDLCK to indicate a shared lease is held.
 	%F_WRLCK to indicate an exclusive lease is held.
 	%F_UNLCK to indicate no lease is held.
 	(if a lease break is pending):
 	%F_RDLCK to indicate an exclusive lease needs to be
 		changed to a shared lease (or removed).
 	%F_UNLCK to indicate the lease needs to be removed.
 	XXX: sfr & willy disagree over whether F_INPROGRESS
 	should be returned to userspace.
  check_conflicting_open - see if the given file points to an inode that has
 			    an existing open that would conflict with the
 			    desired lease.
  @filp:	file to check
  @arg:	type of lease that we're trying to acquire
  @flags:	current lock flags
  Check to see if there's an existing open fd on this file that would
  conflict with the lease we're trying to set.
 We leave these checks to the caller 
	
	  Make sure that only readwrite count is from lease requestor.
	  Note that this will result in denying write leases when i_writecount
	  is negative, which is what we want.  (We shouldn't grant write leases
	  on files open for execution.)
 Note that arg is never F_UNLCK here 
	
	  In the delegation case we need mutual exclusion with
	  a number of operations that take the i_mutex.  We trylock
	  because delegations are an optional optimization, and if
	  there's some chance of a conflict--we'd rather not
	  bother, maybe that's a sign this just isn't a good file to
	  hand out a delegation on.
 Write delegations are not currently supported: 
	
	  At this point, we know that if there is an exclusive
	  lease on this file, then we hold it on this filp
	  (otherwise our open of this file would have blocked).
	  And if we are trying to acquire an exclusive lease,
	  then the file is not open by anyone (including us)
	  except for this filp.
		
		  No exclusive leases if someone else has a lease on
		  this file:
		
		  Modifying our existing lease is OK, but no getting a
		  new lease if someone else is opening for write:
	
	  The check in break_lease() is lockless. It's possible for another
	  open to race in after we did the earlier check for a conflicting
	  open but before the lease was inserted. Check again for a
	  conflicting open and cancel the lease if there is one.
	 
	  We also add a barrier here to ensure that the insertion of the lock
	  precedes these checks.
 	generic_setlease	-	sets a lease on an open file
 	@filp:	file pointer
 	@arg:	type of lease to obtain
 	@flp:	input - file_lock to use, output - file_lock inserted
 	@priv:	private data for lm_setup (may be NULL if lm_setup
 		doesn't require it)
 	The (input) flp->fl_lmops->lm_break function is required
 	by break_lease().
  Kernel subsystems can register to be notified on any attempt to set
  a new lease with the lease_notifier_chain. This is used by (e.g.) nfsd
  to close files that it may have cached when there is an attempt to set a
  conflicting lease.
 !IS_ENABLED(CONFIG_SRCU) 
 IS_ENABLED(CONFIG_SRCU) 
  vfs_setlease        -       sets a lease on an open file
  @filp:	file pointer
  @arg:	type of lease to obtain
  @lease:	file_lock to use when adding a lease
  @priv:	private info for lm_setup when adding a lease (may be
 		NULL if lm_setup doesn't require it)
  Call this to establish a lease on the file. The "lease" argument is not
  used for F_UNLCK requests and may be NULL. For commands that set or alter
  an existing lease, the ``(lease)->fl_lmops->lm_break`` operation must be
  set; if not, this function will return -ENOLCK (and generate a scary-looking
  stack trace).
  The "priv" pointer is passed directly to the lm_setup function as-is. It
  may be NULL if the lm_setup operation doesn't require it.
 	fcntl_setlease	-	sets a lease on an open file
 	@fd: open file descriptor
 	@filp: file pointer
 	@arg: type of lease to obtain
 	Call this fcntl to establish a lease on the file.
 	Note that you also need to call %F_SETSIG to
 	receive a signal when the lease is broken.
  flock_lock_inode_wait - Apply a FLOCK-style lock to a file
  @inode: inode of the file to apply to
  @fl: The lock to be applied
  Apply a FLOCK style lock request to an inode.
  locks_lock_inode_wait - Apply a lock to an inode
  @inode: inode of the file to apply to
  @fl: The lock to be applied
  Apply a POSIX or FLOCK style lock request to an inode.
 	sys_flock: - flock() system call.
 	@fd: the file descriptor to lock.
 	@cmd: the type of lock to apply.
 	Apply a %FL_FLOCK style lock to an open file descriptor.
 	The @cmd can be one of:
 	- %LOCK_SH -- a shared lock.
 	- %LOCK_EX -- an exclusive lock.
 	- %LOCK_UN -- remove an existing lock.
 	- %LOCK_MAND -- a 'mandatory' flock. (DEPRECATED)
 	%LOCK_MAND support has been removed from the kernel.
	
	  LOCK_MAND locks were broken for a long time in that they never
	  conflicted with one another and didn't prevent any sort of open,
	  read or write activity.
	 
	  Just ignore these requests now, to preserve legacy behavior, but
	  throw a warning to let people know that they don't actually work.
  vfs_test_lock - test file byte range lock
  @filp: The file to test lock for
  @fl: The lock to test; also used to hold result
  Returns -ERRNO on failure.  Indicates presence of conflicting lock by
  setting conf->fl_type to something other than F_UNLCK.
  locks_translate_pid - translate a file_lock's fl_pid number into a namespace
  @fl: The file_lock who's fl_pid should be translated
  @ns: The namespace into which the pid should be translated
  Used to tranlate a fl_pid into a namespace virtual pid number
	
	  If the flock owner process is dead and its pid has been already
	  freed, the translation below won't work, but we still want to show
	  flock owner pid number in init pidns.
	
	  Make sure we can represent the posix lock via
	  legacy 32bit flock.
 Report the first existing lock that would conflict with l.
  This implements the F_GETLK command of fcntl().
  vfs_lock_file - file byte range lock
  @filp: The file to apply the lock to
  @cmd: type of locking operation (F_SETLK, F_GETLK, etc.)
  @fl: The lock to be applied
  @conf: Place to return a copy of the conflicting lock, if found.
  A caller that doesn't care about the conflicting lock may pass NULL
  as the final argument.
  If the filesystem defines a private ->lock() method, then @conf will
  be left unchanged; so a caller that cares should initialize it to
  some acceptable default.
  To avoid blocking kernel daemons, such as lockd, that need to acquire POSIX
  locks, the ->lock() interface may return asynchronously, before the lock has
  been granted or denied by the underlying filesystem, if (and only if)
  lm_grant is set. Callers expecting ->lock() to return asynchronously
  will only use F_SETLK, not F_SETLKW; they will set FL_SLEEP if (and only if)
  the request is for a blocking lock. When ->lock() does return asynchronously,
  it must return FILE_LOCK_DEFERRED, and call ->lm_grant() when the lock
  request completes.
  If the request is for non-blocking lock the file system should return
  FILE_LOCK_DEFERRED then try to get the lock and call the callback routine
  with the result. If the request timed out the callback routine will return a
  nonzero return code and the file system should release the lock. The file
  system is also responsible to keep a corresponding posix lock when it
  grants a lock so the VFS can find out which locks are locally held and do
  the correct lock cleanup when required.
  The underlying filesystem must not drop the kernel lock or call
  ->lm_grant() before returning to the caller with a FILE_LOCK_DEFERRED
  return code.
 Ensure that fl->fl_file has compatible f_mode for F_SETLK calls 
 Apply the lock described by l to an open file descriptor.
  This implements both the F_SETLK and F_SETLKW commands of fcntl().
	
	  If the cmd is requesting file-private locks, then set the
	  FL_OFDLCK flag and override the owner.
	
	  Attempt to detect a closefcntl race and recover by releasing the
	  lock that was just acquired. There is no need to do that when we're
	  unlocking though, or for OFD locks.
		
		  We need that spin_lock here - it prevents reordering between
		  update of i_flctx->flc_posix and check for it done in
		  close(). rcu_read_lock() wouldn't do.
 Report the first existing lock that would conflict with l.
  This implements the F_GETLK command of fcntl().
 Apply the lock described by l to an open file descriptor.
  This implements both the F_SETLK and F_SETLKW commands of fcntl().
	
	  If the cmd is requesting file-private locks, then set the
	  FL_OFDLCK flag and override the owner.
	
	  Attempt to detect a closefcntl race and recover by releasing the
	  lock that was just acquired. There is no need to do that when we're
	  unlocking though, or for OFD locks.
		
		  We need that spin_lock here - it prevents reordering between
		  update of i_flctx->flc_posix and check for it done in
		  close(). rcu_read_lock() wouldn't do.
 BITS_PER_LONG == 32 
  This function is called when the file is being removed
  from the task's fd array.  POSIX locks belonging to this task
  are deleted at this time.
	
	  If there are no locks held on this file, we don't need to call
	  posix_lock_file().  Another process could be setting a lock on this
	  file at the same time, but we wouldn't remove that lock anyway.
 The i_flctx must be valid when calling into here 
 The i_flctx must be valid when calling into here 
  This function is called on the last close of an open file.
 remove any OFD locks 
 remove flock locks 
 remove any leases 
  vfs_cancel_lock - file byte range unblock lock
  @filp: The file to apply the unblock to
  @fl: The lock to be unblocked
  Used by lock managers to cancel blocked requests
	
	  If lock owner is dead (and pid is freed) or not visible in current
	  pidns, zero is shown as a pid value. Check lock info from
	  init_pid_ns to get saved lock pid value.
 userspace relies on this representation of dev_t 
 NULL node or root node 
 Next member in the linked list could be itself 
	 View this crossed linked list as a binary tree, the first member of fl_blocked_requests
	  is the left child of current node, the next silibing in fl_blocked_member is the
	  right child, we can alse get the parent of current node from fl_blocker, so this
	  question becomes traversal of a binary tree
 Turn left 
 Turn right 
 Fall back to parent node 
 SPDX-License-Identifier: GPL-2.0
   linuxfsbad_inode.c
   Copyright (C) 1997, Stephen Tweedie
   Provide stub functions for unreadable inodes
   Fabian Frederick : August 2003 - All file operations assigned to EIO
  When a filesystem is unable to read an inode due to an IO error in
  its read_inode() function, it can call make_bad_inode() to return a
  set of stubs which will return EIO errors as required. 
  We only need to do limited initialisation: all other fields are
  preinitialised to zero automatically.
 	make_bad_inode - mark an inode bad due to an IO error
 	@inode: Inode to mark bad
 	When an inode cannot be read due to a media or remote network
 	failure this function makes the inode "bad" and causes IO operations
 	on it to fail from this point on.
  This tests whether an inode has been flagged as bad. The test uses
  &bad_inode_ops to cover the case of invalidated inodes as well as
  those created by make_bad_inode() above.
 	is_bad_inode - is an inode errored
 	@inode: inode to test
 	Returns true if the inode in question has been marked as bad.
  iget_failed - Mark an under-construction inode as dead and release it
  @inode: The inode to discard
  Mark an under-construction inode as dead and release it.
 SPDX-License-Identifier: GPL-2.0-or-later
 Filesystem access-by-fd.
  Copyright (C) 2017 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Allow the user to read back any error, warning or informational messages.
  Attach a filesystem context to a file and an fd.
  Open a filesystem by name so that it can be configured for mounting.
  We are allowed to specify a container in which the filesystem will be
  opened, thereby indicating which namespaces will be used (notably, which
  network namespace will be used for network filesystems).
  Pick a superblock into a context for reconfiguration.
  Check the state and apply the configuration.  Note that this function is
  allowed to 'steal' the value by setting param->xxx to NULL before returning.
  sys_fsconfig - Set parameters and trigger actions on a context
  @fd: The filesystem context to act upon
  @cmd: The action to take
  @_key: Where appropriate, the parameter key to set
  @_value: Where appropriate, the parameter value to set
  @aux: Additional information for the value
  This system call is used to set parameters on a context, including
  superblock settings, data source and security labelling.
  Actions include triggering the creation of a superblock and the
  reconfiguration of the superblock attached to the specified context.
  When setting a parameter, @cmd indicates the type of value being proposed
  and @_key indicates the parameter to be altered.
  @_value and @aux are used to specify the value, should a value be required:
  () fsconfig_set_flag: No value is specified.  The parameter must be boolean
      in nature.  The key may be prefixed with "no" to invert the
      setting. @_value must be NULL and @aux must be 0.
  () fsconfig_set_string: A string value is specified.  The parameter can be
      expecting boolean, integer, string or take a path.  A conversion to an
      appropriate type will be attempted (which may include looking up as a
      path).  @_value points to a NUL-terminated string and @aux must be 0.
  () fsconfig_set_binary: A binary blob is specified.  @_value points to the
      blob and @aux indicates its size.  The parameter must be expecting a
      blob.
  () fsconfig_set_path: A non-empty path is specified.  The parameter must be
      expecting a path object.  @_value points to a NUL-terminated string that
      is the path and @aux is a file descriptor at which to start a relative
      lookup or AT_FDCWD.
  () fsconfig_set_path_empty: As fsconfig_set_path, but with AT_EMPTY_PATH
      implied.
  () fsconfig_set_fd: An open file descriptor is specified.  @_value must be
      NULL and @aux indicates the file descriptor.
	 Clean up the our record of any value that we obtained from
	  userspace.  Note that the value may have been stolen by the LSM or
	  filesystem, in which case the value pointer will have been cleared.
 SPDX-License-Identifier: GPL-2.0
   fstimerfd.c
   Copyright (C) 2007  Davide Libenzi <davidel@xmailserver.org>
   Thanks to Thomas Gleixner for code reviews and useful comments.
 to show in fdinfo 
  This gets called when the timer event triggers. We set the "expired"
  flag, but we do not re-arm the timer (in case it's necessary,
  tintv != 0) until the timer is accessed.
  Called when the clock was set to cancel the timers in the cancel
  list. This will wake up processes waiting on these timers. The
  wake-up requires ctx->ticks to be non zero, therefore we increment
  it before calling wake_up_locked().
  Invoked from timekeeping_resume(). Defer the actual update to work so
  timerfd_clock_was_set() runs in task context.
	
	  If clock has changed, we do not care about the
	  ticks and we do not rearm the timer. Userspace must
	  reevaluate anyway.
			
			  If tintv != 0, this is a periodic timer that
			  needs to be re-armed. We avoid doing it in the timer
			  callback to avoid DoS attacks specifying a very
			  short timer period.
 Check the TFD_ constants for consistency.  
	
	  We need to stop the existing timer before reprogramming
	  it to the new values.
	
	  If the timer is expired and it's periodic, we need to advance it
	  because the caller may want to know the previous expiration time.
	  We do not update "ticks" and "expired" since the timer will be
	  re-programmed again in the following timerfd_setup() call.
	
	  Re-program the timer to the new value ...
 SPDX-License-Identifier: GPL-2.0
   linuxfspipe.c
   Copyright (C) 1991, 1992, 1999  Linus Torvalds
  New pipe buffers will be restricted to this size while the user is exceeding
  their pipe buffer quota. The general pipe use case needs at least two
  buffers: one for data yet to be read, and one for new data. If this is less
  than two, then a write to a non-empty pipe may block even if the pipe is not
  full. This can occur with GNU make jobserver or similar uses of pipes as
  semaphores: multiple processes may be waiting to write tokens back to the
  pipe before reading tokens: https:lore.kernel.orglkml1628086770.5rn8p04n6j.none@localhost.
  Users can reduce their pipe buffers with F_SETPIPE_SZ below this at their
  own risk, namely: pipe writes to non-full pipes may block until the pipe is
  emptied.
  The max size that a non-root user is allowed to grow the pipe. Can
  be set by root in procsysfspipe-max-size
 Maximum allocatable pages per user. Hard limit is unset by default, soft
  matches default values.
  We use head and tail indices that aren't masked off, except at the point of
  dereference, but rather they're allowed to wrap naturally.  This means there
  isn't a dead spot in the buffer, but the ring has to be a power of two and
  <= 2^31.
  -- David Howells 2019-09-23.
  Reads with count = 0 should always return 0.
  -- Julian Bradfield 1999-06-07.
  FIFOs and Pipes now generate SIGIO for both readers and writers.
  -- Jeremy Elson <jelson@circlemud.org> 2001-08-16
  pipe_read & write cleanup
  -- Manfred Spraul <manfred@colorfullife.com> 2002-05-09
	
	  pipe_lock() nests non-pipe inode locks (for writing to a file)
	
	  If nobody else uses this page, and we don't already have a
	  temporary page, let's keep track of it as a one-deep
	  allocation cache. (Otherwise just release our reference to it)
  generic_pipe_buf_try_steal - attempt to take ownership of a &pipe_buffer
  @pipe:	the pipe that the buffer belongs to
  @buf:	the buffer to attempt to steal
  Description:
 	This function attempts to steal the &struct page attached to
 	@buf. If successful, this function returns 0 and returns with
 	the page locked. The caller may then reuse the page for whatever
 	he wishes; the typical use is insertion into a different file
 	page cache.
	
	  A reference of one is golden, that means that the owner of this
	  page is the only one holding a reference to it. lock the page
	  and return OK.
  generic_pipe_buf_get - get a reference to a &struct pipe_buffer
  @pipe:	the pipe that the buffer belongs to
  @buf:	the buffer to get a reference to
  Description:
 	This function grabs an extra reference to @buf. It's used in
 	the tee() system call, when we duplicate the buffers in one
 	pipe into another.
  generic_pipe_buf_release - put a reference to a &struct pipe_buffer
  @pipe:	the pipe that the buffer belongs to
  @buf:	the buffer to put a reference to
  Description:
 	This function releases a reference to @buf.
 Done while waiting without holding the pipe lock - thus the READ_ONCE() 
 Null read succeeds. 
	
	  We only wake up writers if the pipe was full when we started
	  reading in order to avoid unnecessary wakeups.
	 
	  But when we do wake up writers, we do so using a sync wakeup
	  (WF_SYNC), because we want them to get going and generate more
	  data for us.
 Was it a packet buffer? Clean up and exit 
 common path: read succeeded 
 More to do? 
		
		  We only get here if we didn't actually read anything.
		 
		  However, we could have seen (and removed) a zero-sized
		  pipe buffer, and might have made space in the buffers
		  that way.
		 
		  You can't make zero-sized pipe buffers by doing an empty
		  write (not even in packet mode), but they can happen if
		  the writer gets an EFAULT when trying to fill a buffer
		  that already got allocated and inserted in the buffer
		  array.
		 
		  So we still need to wake up any pending writers in the
		  _very_ unlikely case that the pipe was full, but we got
		  no data.
		
		  But because we didn't read anything, at this point we can
		  just return directly with -ERESTARTSYS if we're interrupted,
		  since we've done any required wakeups and there's no need
		  to mark anything accessed. And we've dropped the lock.
 Done while waiting without holding the pipe lock - thus the READ_ONCE() 
 Null write succeeds. 
	
	  If it wasn't empty we try to merge new data into
	  the last buffer.
	 
	  That naturally merges small writes, but it also
	  page-aligns the rest of the writes for large writes
	  spanning multiple pages.
			 Allocate a slot in the ring in advance and attach an
			  empty buffer.  If we fault or otherwise fail to use
			  it, either the reader will consume it or it'll still
			  be there for the next write.
 Insert it into the buffer array 
 Wait for buffer space to become available. 
		
		  We're going to release the pipe lock and wait for more
		  space. We wake up any readers if necessary, and then
		  after waiting we need to re-check whether the pipe
		  become empty while we dropped the lock.
	
	  If we do do a wakeup event, we do a 'sync' wakeup, because we
	  want the reader to start processing things asap, rather than
	  leave the data pending.
	 
	  This is particularly important for small writes, because of
	  how (for example) the GNU make jobserver uses small writes to
	  wake up pending jobs
	 
	  Epoll nonsensically wants a wakeup whether the pipe
	  was already empty or not.
 No kernel lock held - fine 
 Epoll has some historical nasty semantics, this enables them 
	
	  Reading pipe state only -- no need for acquiring the semaphore.
	 
	  But because this is racy, the code has to add the
	  entry to the poll table _first_ ..
	
	  .. and only then can you do the racy tests. That way,
	  if something changes and you got it wrong, the poll
	  table entry will wake you up and fix it.
		
		  Most Unices do not set EPOLLERR for FIFOs but on Linux they
		  behave exactly like pipes for poll().
 Was that the last reader or writer, but not the other side? 
 this can happen only if on == T 
  pipefs_dname() is called from d_path().
	
	  Mark the inode dirty from the very beginning,
	  that way it will never be moved to the dirty
	  list because "mark_inode_dirty()" will think
	  that it already _is_ on the dirty list.
  sys_pipe() is the normal C calling standard for creating
  a pipe. It's not the way Unix traditionally does this, though.
  This is the stupid "wait for pipe to be readable or writable"
  model.
  See pipe_readwrite() for the proper kind of exclusive wait,
  but that requires that we wake up any other readerswriters
  if we then do not end up reading everything (ie the whole
  "wake_next_readerwriter" logic in pipe_readwrite()).
  This depends on both the wait (here) and the wakeup (wake_up_partner)
  holding the pipe lock, so "cnt" is stable and we know a wakeup cannot
  race with the count check and waitqueue prep.
  Normally in order to avoid races, you'd do the prepare_to_wait() first,
  then check the condition you're waiting for, and only then sleep. But
  because of the pipe lock, we can check the condition before being on
  the wait queue.
  We use the 'rd_wait' waitqueue for pipe partner waiting.
 OK, we have a pipe and it's pinned down 
 We can only do regular readwrite on fifos 
	
	   O_RDONLY
	   POSIX.1 says that O_NONBLOCK means return with the FIFO
	   opened, even when there is no process writing the FIFO.
				 suppress EPOLLHUP until we have
	
	   O_WRONLY
	   POSIX.1 says that O_NONBLOCK means return -1 with
	   errno=ENXIO when there is no process reading the FIFO.
	
	   O_RDWR
	   POSIX.1 leaves this case "undefined" when O_NONBLOCK is set.
	   This implementation will NEVER block on a O_RDWR open, since
	   the process can at least talk to itself.
 Ok! 
  Currently we rely on the pipe array holding a power-of-2 number
  of pages. Returns 0 on error.
 Minimum pipe size, as required by POSIX 
  Resize the pipe ring to a number of slots.
	
	  We can shrink the pipe, if arg is greater than the ring occupancy.
	  Since we don't expect a lot of shrink+grow operations, just free and
	  allocate again like we would do for growing.  If the pipe currently
	  contains more buffers than arg, then return busy.
	
	  The pipe array wraps around, so just start the new one at zero
	  and adjust the indices.
 This might have made more room for writers 
  Allocate a new array of pipe buffers and copy the info over. Returns the
  pipe size if successful, or return -ERROR on error.
	
	  If trying to increase the pipe capacity, check that an
	  unprivileged user is not trying to exceed various limits
	  (soft limit check here, hard limit check just below).
	  Decreasing the pipe capacity is always permitted, even
	  if the user is currently over a limit.
  Note that i_pipe and i_cdev share the same location, so checking ->i_pipe is
  not enough to verify that this is a pipe.
  pipefs should _never_ be mounted by userland - too much of security hassle,
  no real gain from having the whole whorehouse mounted. So we don't need
  any operations on the root directory. However, we need a non-trivial
  d_name - pipe: will go nicely and kill the special-casing in procfs.
 SPDX-License-Identifier: GPL-2.0-or-later
 binfmt_elf_fdpic.c: FDPIC ELF binary format
  Copyright (C) 2003, 2004, 2006 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Derived from binfmt_elf.c
  read the program headers table into memory
 determine stack size for this binary 
  load an fdpic binary into various bits of memory
 to shut gcc up 
 check that this is a binary we know how to deal with 
 binfmt_elf handles non-fdpic elf except on nommu 
 nommu can only load ET_DYN (PIE) ELF 
 read the program header table 
 scan for a program header that specifies an interpreter 
 read the name of the interpreter into memory 
 replace the program with the interpreter 
			
			  If the binary is not readable then enforce
			  mm->dumpable = 0 regardless of the interpreter's
			  permissions.
 perform insanity checks on the interpreter 
 read the interpreter's program header table 
 same as exec.c's default commit 
 flush all traces of the currently running executable 
	 there's now no turning back... the old userspace image is dead,
	  defunct, deceased, etc.
 load the executable and interpreter into memory 
 create a stack area and zero-size brk area 
	
	  The ABI may specify that certain registers be set up in special
	  ways (on i386 %edx is the address of a DT_FINI function, for
	  example.  This macro performs whatever initialization to
	  the regs structure is required.
 everything is now ready... get the userspace context ready to roll 
  AT_BASE_PLATFORM indicates the "real" hardwaremicroarchitecture.
  If the arch defines ELF_BASE_PLATFORM (in asmelf.h), the value
  will be copied to the user stack in the same manner as AT_PLATFORM.
  present useful information to the program by shovelling it onto the new
  process's stack
 reset for each csp adjustment 
	 In some cases (e.g. Hyper-Threading), we want to avoid L1 evictions
	  by the processes running on the same package. One thing we can do is
	  to shuffle the initial stack for them, so we give the architecture
	  an opportunity to do so here.
 stack the program arguments and environment 
	
	  If this architecture has a platform capability string, copy it
	  to userspace.  In some cases (Sparc), this info is impossible
	  for userspace to get any other way, in others (i386) it is
	  merely difficult.
	
	  If this architecture has a "base" platform capability
	  string, copy it to userspace.
 stack the load map(s) 
 force 16 byte _final_ alignment here for generality 
 envv[] 
 argv[] 
 argc 
 put the ELF interpreter info on the stack 
	 ARCH_DLINFO must come last so platform specific code can enforce
	  special alignment requirements on the AUXV if necessary (eg. PPC).
 allocate room for argv[] and envv[] 
 stack argc 
 fill in the argv[] array 
 fill in the envv[] array 
  load the appropriate binary image (executable or interpreter) into memory
  - we assume no MMU is available
  - if no other PIC bits are set in params->hdr->e_flags
    - we assume that the LOADable segments in the binary are independently relocatable
    - we assume RO executable segments are shareable
  - else
    - we assume the loadable parts of the image to require fixed displacement
    - the image is not shareable
 allocate a load map table 
 map the requested LOADs into the memory space 
 map the entry point 
 determine where the program header table has wound up if mapped 
 determine where the dynamic section has wound up if there is one 
				 check the dynamic section contains at least
				  one item, and that the last item is a NULL
	 now elide adjacent segments in the load map on MMU linux
	  - on uClinux the holes between may actually be filled with system
	    stuff or stuff from other processes
 see if we have a candidate for merging 
  map a file with constant displacement under uClinux
	 determine the bounds of the contiguous overall allocation we must
 allocate one big anon block for everything 
 and then load the file segments into it 
 map the ELF header address if in this segment 
 clear any space allocated but not loaded 
  map a binary by direct mmap() of the individual PT_LOAD segments
 deal with each load segment separately 
 determine the mapping parameters 
 PT_LOADs are independently locatable 
 the specified virtual address must be honoured 
			 constant displacement
			  - can be mapped anywhere, but must be mapped as a
			    unit
 contiguity handled later 
 create the mapping 
 map the ELF header address if in this segment 
		 clear the bit between beginning of mapping and beginning of
		 clear any space allocated but not loaded
		  - on uClinux we can just clear the lot
		  - on MMU linux we'll get a SIGBUS beyond the last page
		    extant in the file
  ELF-FDPIC core dumper
  Modelled on fsexec.c:aout_core_dump()
  Jeremy Fitzhardinge <jeremy@sw.oz.au>
  Modelled on fsbinfmt_elf.c core dumper
 GP registers 
	 When using FDPIC, the loadmap addresses need to be communicated
	  to GDB in order for GDB to do the necessary relocations.  The
	  fields (below) used to communicate this information are placed
	  immediately after ``pr_reg'', so that the loadmap addresses may
	  be viewed as part of the register set if so desired.
 True if math co-processor being used.  
 An ELF note in memory 
 #define DEBUG 
  fill up all the fields in prstatus from the given task struct, except
  registers which need to be filled up separately.
		
		  This is the record for the group leader.  It shows the
		  group-wide total, not its individual thread total.
 first copy the parameters from user space 
 Here is the structure in which status of each thread is captured. 
 NT_PRSTATUS 
 NT_PRFPREG 
  In order to add the specific thread information for the elf file format,
  we need to keep a linked list of every thread's pr_status and then create
  a single section for them in the final core file.
  dump the segments for an MMU process
  Actual dumper
  This is a two-pass process; first we find the offsets of the bits,
  and then they are actually written out.  If we run out of core limit
  we just truncate.
 NT_PRPSINFO 
 alloc memory for large data structures: too large to be on stack 
 now collect the dump for the current 
 for notes section 
	 If segs > PN_XNUM(0xffff), then e_phnum overflows. To avoid
	  this, kernel supports extended numbering. Have a look at
 Set up header 
	
	  Set up the notes in similar form to SVR4 core dumps made
	  with info from their proc.
 Elf header 
 Program headers 
 Write notes phdr entry 
 Page-align dumped data 
 write program headers for segments dump 
 write out the notes section 
 write out the thread status notes section 
 Sanity check 
 CONFIG_ELF_CORE 
 SPDX-License-Identifier: GPL-2.0-or-later
 no-block.c: implementation of routines required for non-BLOCK configuration
  Copyright (C) 2006 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
 SPDX-License-Identifier: GPL-2.0-only
   fseventfd.c
   Copyright (C) 2007  Davide Libenzi <davidel@xmailserver.org>
	
	  Every time that a write(2) is performed on an eventfd, the
	  value of the __u64 being written is added to "count" and a
	  wakeup is performed on "wqh". A read(2) will return the "count"
	  value to userspace, and will reset "count" to zero. The kernel
	  side eventfd_signal() also, adds to the "count" counter and
	  issue a wakeup.
  eventfd_signal - Adds @n to the eventfd counter.
  @ctx: [in] Pointer to the eventfd context.
  @n: [in] Value of the counter to be added to the eventfd internal counter.
           The value cannot be negative.
  This function is supposed to be called by the kernel in paths that do not
  allow sleeping. In this function we allow the counter to reach the ULLONG_MAX
  value, and we signal this as overflow condition by returning a EPOLLERR
  to poll(2).
  Returns the amount by which the counter was incremented.  This will be less
  than @n if the counter has overflowed.
	
	  Deadlock or stack overflow issues can happen if we recurse here
	  through waitqueue wakeup handlers. If the caller users potentially
	  nested waitqueues with custom wakeup handlers, then it should
	  check eventfd_signal_allowed() before calling this function. If
	  it returns false, the eventfd_signal() call should be deferred to a
	  safe context.
  eventfd_ctx_put - Releases a reference to the internal eventfd context.
  @ctx: [in] Pointer to eventfd context.
  The eventfd context reference must have been previously acquired either
  with eventfd_ctx_fdget() or eventfd_ctx_fileget().
	
	  All writes to ctx->count occur within ctx->wqh.lock.  This read
	  can be done outside ctx->wqh.lock because we know that poll_wait
	  takes that lock (through add_wait_queue) if our caller will sleep.
	 
	  The read _can_ therefore seep into add_wait_queue's critical
	  section, but cannot move above it!  add_wait_queue's spin_lock acts
	  as an acquire barrier and ensures that the read be ordered properly
	  against the writes.  The following CAN happen and is safe:
	 
	      poll                               write
	      -----------------                  ------------
	      lock ctx->wqh.lock (in poll_wait)
	      count = ctx->count
	      __add_wait_queue
	      unlock ctx->wqh.lock
	                                         lock ctx->qwh.lock
	                                         ctx->count += n
	                                         if (waitqueue_active)
	                                           wake_up_locked_poll
	                                         unlock ctx->qwh.lock
	      eventfd_poll returns 0
	 
	  but the following, which would miss a wakeup, cannot happen:
	 
	      poll                               write
	      -----------------                  ------------
	      count = ctx->count (INVALID!)
	                                         lock ctx->qwh.lock
	                                         ctx->count += n
	                                         waitqueue_active is false
	                                         no wake_up_locked_poll!
	                                         unlock ctx->qwh.lock
	      lock ctx->wqh.lock (in poll_wait)
	      __add_wait_queue
	      unlock ctx->wqh.lock
	      eventfd_poll returns 0
  eventfd_ctx_remove_wait_queue - Read the current counter and removes wait queue.
  @ctx: [in] Pointer to eventfd context.
  @wait: [in] Wait queue to be removed.
  @cnt: [out] Pointer to the 64-bit counter value.
  Returns %0 if successful, or the following error codes:
  -EAGAIN      : The operation would have blocked.
  This is used to atomically remove a wait queue entry from the eventfd wait
  queue head, and readreset the counter value.
  eventfd_fget - Acquire a reference of an eventfd file descriptor.
  @fd: [in] Eventfd file descriptor.
  Returns a pointer to the eventfd file structure in case of success, or the
  following error pointer:
  -EBADF    : Invalid @fd file descriptor.
  -EINVAL   : The @fd file descriptor is not an eventfd file.
  eventfd_ctx_fdget - Acquires a reference to the internal eventfd context.
  @fd: [in] Eventfd file descriptor.
  Returns a pointer to the internal eventfd context, otherwise the error
  pointers returned by the following functions:
  eventfd_fget
  eventfd_ctx_fileget - Acquires a reference to the internal eventfd context.
  @file: [in] Eventfd file pointer.
  Returns a pointer to the internal eventfd context, otherwise the error
  pointer:
  -EINVAL   : The @fd file descriptor is not an eventfd file.
 Check the EFD_ constants for consistency.  
 SPDX-License-Identifier: GPL-2.0
  Basic worker thread pool for io_uring
  Copyright (C) 2019 Jens Axboe
 up and active 
 account as running 
 worker on free list 
 is doing bounded work 
 wq exiting 
 stalled on hash 
  One for each thread in a wqe pool
  Per-node worker thread pool
  Per io_wq state
  Check head of free list for an available worker. If one isn't available,
  caller must create one.
	
	  Iterate free_list and see if we can find an idle worker to
	  activate. If a given worker is on the free_list but in the process
	  of exiting, keep trying.
  We need a worker. If we find a free one, we're good. If not, and we're
  below the max number of workers, create one.
	
	  Most likely an attempt to queue unbounded work on an io_wq that
	  wasn't setup with any unbounded workers.
 raced with exit, just ignore create call 
	
	  create_state manages ownership of create_workindex. We should
	  only need one entry per worker, as the worker going to sleep
	  will trigger the condition, and waking will clear it once it
	  runs the task_work.
  Worker will start processing some work. Move it to the busy list, if
  it's currently on the freelist
  No work, worker going to sleep. Move to freelist, and unuse mm if we
  have one attached. Dropping the mm may potentially sleep, so we drop
  the lock in that case and return success. Since the caller has to
  retry the loop in that case (we changed task state), we don't regrab
  the lock if we return success.
 not hashed, can run anytime 
 all items with this hash lie in [work, tail] 
 hashed, can run if not already running 
 fast forward to a next hash, for-each will fix up @prev 
		
		  Set this before dropping the lock to avoid racing with new
		  work being added and clearing the stalled bit.
		
		  If we got some work, mark us as busy. If we didn't, but
		  the list isn't empty, it means we stalled on hashed work.
		  Mark us stalled so we don't keep looking for work when we
		  can't make progress, any work completion or insertion will
		  clear the stalled flag.
 handle a whole dependent link 
 serialize hash clear with wake_up() 
 skip unnecessary unlock-lock wqe->lock 
 timed out, exit unless we're the last worker 
  Called when a worker is scheduled in. Mark us as currently running.
  Called when worker is going to sleep. If there are no workers currently
  running and we have work pending, wake up a free one or create a new one.
 re-create attempts grab a new worker ref, drop the existing one 
  Iterate the passed in list and call the specific function for each
  worker that isn't exiting
 no task if node iswas offline 
	
	  If io-wq is exiting for this task, or if the request has explicitly
	  been marked as one that should not get executed, cancel it here.
 fatal condition, failed to create the first worker 
  Work items that hash to the same value will not be done in parallel.
  Used to limit concurrent writes, generally hashed by inode.
	
	  Hold the lock to avoid ->cur_work going out of scope, caller
	  may dereference the passed in work.
 not safe to continue after unlock 
	
	  First check pending list, if we're lucky we can just remove it
	  from there. CANCEL_OK means that the work is returned as-new,
	  no completion will be posted for it.
	
	  Now check if a free (going busy) or busy worker has the work
	  currently running. If we find it there, we'll return CANCEL_RUNNING
	  as an indication that we attempt to signal cancellation. The
	  completion will run normally in this case.
  Set max number of unbounded workers, returns old value. If new_count is 0,
  then just return the old value.
 SPDX-License-Identifier: GPL-2.0
   linuxfschar_dev.c
   Copyright (C) 1991, 1992  Linus Torvalds
 will die 
 index in the above 
 CONFIG_PROC_FS 
  Register a single major with a specified minor range.
  If major == 0 this function will dynamically allocate an unused major.
  If major > 0 this function will attempt to reserve the range of minors
  with given major.
  register_chrdev_region() - register a range of device numbers
  @from: the first in the desired range of device numbers; must include
         the major number.
  @count: the number of consecutive device numbers required
  @name: the name of the device or driver.
  Return value is zero on success, a negative error code on failure.
  alloc_chrdev_region() - register a range of char device numbers
  @dev: output parameter for first assigned number
  @baseminor: first of the requested range of minor numbers
  @count: the number of minor numbers required
  @name: the name of the associated device or driver
  Allocates a range of char device numbers.  The major number will be
  chosen dynamically, and returned (along with the first minor number)
  in @dev.  Returns zero or a negative error code.
  __register_chrdev() - create and register a cdev occupying a range of minors
  @major: major device number or 0 for dynamic allocation
  @baseminor: first of the requested range of minor numbers
  @count: the number of minor numbers required
  @name: name of this range of devices
  @fops: file operations associated with this devices
  If @major == 0 this functions will dynamically allocate a major and return
  its number.
  If @major > 0 this function will attempt to reserve a device with the given
  major number and will return zero on success.
  Returns a -ve errno on failure.
  The name of this device has nothing to do with the name of the device in
  dev. It only helps to keep track of the different owners of devices. If
  your module name has only one type of devices it's ok to use e.g. the name
  of the module here.
  unregister_chrdev_region() - unregister a range of device numbers
  @from: the first in the range of numbers to unregister
  @count: the number of device numbers to unregister
  This function will unregister a range of @count device numbers,
  starting with @from.  The caller should normally be the one who
  allocated those numbers in the first place...
  __unregister_chrdev - unregister and destroy a cdev
  @major: major device number
  @baseminor: first of the range of minor numbers
  @count: the number of minor numbers this cdev is occupying
  @name: name of this range of devices
  Unregister and destroy the cdev occupying the region described by
  @major, @baseminor and @count.  This function undoes what
  __register_chrdev() did.
  Called every time a character special file is opened
		 Check i_cdev again in case somebody beat us to it while
  Dummy default file-operations: the only thing this does
  is contain the open that then fills in the correct operations
  depending on the special file...
  cdev_add() - add a char device to the system
  @p: the cdev structure for the device
  @dev: the first device number for which this device is responsible
  @count: the number of consecutive minor numbers corresponding to this
          device
  cdev_add() adds the device represented by @p to the system, making it
  live immediately.  A negative error code is returned on failure.
  cdev_set_parent() - set the parent kobject for a char device
  @p: the cdev structure
  @kobj: the kobject to take a reference to
  cdev_set_parent() sets a parent kobject which will be referenced
  appropriately so the parent is not freed before the cdev. This
  should be called before cdev_add.
  cdev_device_add() - add a char device and it's corresponding
 	struct device, linkink
  @dev: the device structure
  @cdev: the cdev structure
  cdev_device_add() adds the char device represented by @cdev to the system,
  just as cdev_add does. It then adds @dev to the system using device_add
  The dev_t for the char device will be taken from the struct device which
  needs to be initialized first. This helper function correctly takes a
  reference to the parent device so the parent will not get released until
  all references to the cdev are released.
  This helper uses dev->devt for the device number. If it is not set
  it will not add the cdev and it will be equivalent to device_add.
  This function should be used whenever the struct cdev and the
  struct device are members of the same structure whose lifetime is
  managed by the struct device.
  NOTE: Callers must assume that userspace was able to open the cdev and
  can call cdev fops callbacks at any time, even if this function fails.
  cdev_device_del() - inverse of cdev_device_add
  @dev: the device structure
  @cdev: the cdev structure
  cdev_device_del() is a helper function to call cdev_del and device_del.
  It should be used whenever cdev_device_add is used.
  If dev->devt is not set it will not remove the cdev and will be equivalent
  to device_del.
  NOTE: This guarantees that associated sysfs callbacks are not running
  or runnable, however any cdevs already open will remain and their fops
  will still be callable even after this function returns.
  cdev_del() - remove a cdev from the system
  @p: the cdev structure to be removed
  cdev_del() removes @p from the system, possibly freeing the structure
  itself.
  NOTE: This guarantees that cdev device will no longer be able to be
  opened, however any cdevs already open will remain and their fops will
  still be callable even after cdev_del returns.
  cdev_alloc() - allocate a cdev structure
  Allocates and returns a cdev structure, or NULL on failure.
  cdev_init() - initialize a cdev structure
  @cdev: the structure to initialize
  @fops: the file_operations for this device
  Initializes @cdev, remembering @fops, making it ready to add to the
  system with cdev_add().
 Make old-style 2.4 aliases work 
 Let modules do char dev stuff 
 SPDX-License-Identifier: GPL-2.0-only
   linuxfsbinfmt_aout.c
   Copyright (C) 1991, 1992, 1996  Linus Torvalds
  create_aout_tables() parses the env- and arg-strings in new user
  memory and creates the pointer tables from them, and puts their
  addresses on the "stack", returning the new stack pointer value.
 whee.. test-programs are so much fun. 
  These are the functions used to load a.out style executables and shared
  libraries.  There is no binary dependent code anywhere else.
 exec-header 
	
	  Requires a mmap handler. This prevents people from using a.out
	  as part of an exploit attack against proc-related vulnerabilities.
	 Check initial limits. This avoids letting people circumvent
	  size limits imposed on them by creating programs with large
	  arrays in the data or bss.
 Flush all traces of the currently running executable 
 OK, This is the point of no return 
 We come in here for the regular a.out style of shared libraries 
	
	  Requires a mmap handler. This prevents people from using a.out
	  as part of an exploit attack against proc-related vulnerabilities.
	 For  QMAGIC, the starting address is 0x20 into the page.  We mask
 Now use mmap to map the library into memory. 
 SPDX-License-Identifier: GPL-2.0
  linuxfsseq_file.c
  helper functions for making synthetic files from sequences of records.
  initial implementation -- AV, Oct 2001.
 	seq_open -	initialize sequential file
 	@file: file we initialize
 	@op: method table describing the sequence
 	seq_open() sets @file, associating it with a sequence described
 	by @op.  @op->start() sets the iterator up and returns the first
 	element of sequence. @op->stop() shuts it down.  @op->next()
 	returns the next element of sequence.  @op->show() prints element
 	into the buffer.  In case of error ->start() and ->next() return
 	ERR_PTR(error).  In the end of sequence they return %NULL. ->show()
 	returns 0 in case of success and negative number in case of error.
 	Returning SEQ_SKIP means "discard this element and move on".
 	Note: seq_open() will allocate a struct seq_file and store its
 	pointer in @file->private_data. This pointer should not be modified.
 No refcounting: the lifetime of 'p' is constrained
 to the lifetime of the file.
	
	  seq_files support lseek() and pread().  They do not implement
	  write() at all, but we clear FMODE_PWRITE here for historical
	  reasons.
	 
	  If a client of seq_files a) implements file.write() and b) wishes to
	  support pwrite() then that client will need to implement its own
	  file.open() which calls seq_open() and then sets FMODE_PWRITE.
 	seq_read -	->read() method for sequential files.
 	@file: the file to read from
 	@buf: the buffer to read to
 	@size: the maximum number of bytes to read
 	@ppos: the current position in the file
 	Ready-made ->f_op->read()
  Ready-made ->f_op->read_iter()
	
	  if request is to read from zero offset, reset iterator to first
	  record as it might have been already advanced by previous requests
 Don't assume ki_pos is where we left it 
 With prejudice... 
 grab buffer if we didn't have one 
 something left in the buffer - copy it out first
 hadn't managed to copy everything
 get a non-empty record in the buffer
 EOF or an error
 hard error
 ->show() says "skip it"
 empty record
 got it
 need a bigger buffer
 EOF or an error
 one non-empty record is in the buffer; if they want more,
 try to fit more in, but in any case we need to advance
 the iterator once for every record shown.
 no next record for us
 ->show() says "skip it"
 	seq_lseek -	->llseek() method for sequential files.
 	@file: the file in question
 	@offset: new position
 	@whence: 0 for absolute, 1 for relative position
 	Ready-made ->f_op->llseek()
 with extreme prejudice... 
 	seq_release -	free the structures associated with sequential file.
 	@file: file in question
 	@inode: its inode
 	Frees the structures associated with sequential file; can be used
 	as ->f_op->release() if you don't have private data to destroy.
  seq_escape_mem - print data into buffer, escaping some characters
  @m: target buffer
  @src: source buffer
  @len: size of source buffer
  @flags: flags to pass to string_escape_mem()
  @esc: set of characters that need escaping
  Puts data into buffer, replacing each occurrence of character from
  given class (defined by @flags and @esc) with printable escaped sequence.
  Use seq_has_overflowed() to check for errors.
 CONFIG_BINARY_PRINTF 
 	mangle_path -	mangle and copy path to buffer beginning
 	@s: buffer start
 	@p: beginning of path in above buffer
 	@esc: set of characters that need escaping
       Copy the path from @p to @s, replacing each occurrence of character from
       @esc with usual octal escape.
       Returns pointer past last written character in @s, or NULL in case of
       failure.
  seq_path - seq_file interface to print a pathname
  @m: the seq_file handle
  @path: the struct path to print
  @esc: set of characters to escape in the output
  return the absolute path of 'path', as represented by the
  dentry  mnt pair in the path parameter.
  seq_file_path - seq_file interface to print a pathname of a file
  @m: the seq_file handle
  @file: the struct file to print
  @esc: set of characters to escape in the output
  return the absolute path to the file.
  Same as seq_path, but relative to supplied root.
  returns the path of the 'dentry' from the root of its filesystem.
  seq_put_decimal_ull_width - A helper routine for putting decimal numbers
  			       without rich format of printf().
  only 'unsigned long long' is supported.
  @m: seq_file identifying the buffer to which data should be written
  @delimiter: a string which is printed before the number
  @num: the number
  @width: a minimum field width
  This routine will put strlen(delimiter) + number into seq_filed.
  This routine is very quick when you show lots of numbers.
  In usual cases, it will be better to use seq_printf(). It's easier to read.
 we'll write 2 bytes at least 
  seq_put_hex_ll - put a number in hexadecimal notation
  @m: seq_file identifying the buffer to which data should be written
  @delimiter: a string which is printed before the number
  @v: the number
  @width: a minimum field width
  seq_put_hex_ll(m, "", v, 8) is equal to seq_printf(m, "%08llx", v)
  This routine is very quick when you show lots of numbers.
  In usual cases, it will be better to use seq_printf(). It's easier to read.
 If x is 0, the result of __builtin_clzll is undefined 
 we'll write 2 bytes at least 
  seq_write - write arbitrary data to buffer
  @seq: seq_file identifying the buffer to which data should be written
  @data: data address
  @len: number of bytes
  Return 0 on success, non-zero otherwise.
  seq_pad - write padding spaces to buffer
  @m: seq_file identifying the buffer to which data should be written
  @c: the byte to append after padding if non-zero
 A complete analogue of print_hex_dump() 
  seq_hlist_start - start an iteration of a hlist
  @head: the head of the hlist
  @pos:  the start position of the sequence
  Called at seq_file->op->start().
  seq_hlist_start_head - start an iteration of a hlist
  @head: the head of the hlist
  @pos:  the start position of the sequence
  Called at seq_file->op->start(). Call this function if you want to
  print a header at the top of the output.
  seq_hlist_next - move to the next position of the hlist
  @v:    the current iterator
  @head: the head of the hlist
  @ppos: the current position
  Called at seq_file->op->next().
  seq_hlist_start_rcu - start an iteration of a hlist protected by RCU
  @head: the head of the hlist
  @pos:  the start position of the sequence
  Called at seq_file->op->start().
  This list-traversal primitive may safely run concurrently with
  the _rcu list-mutation primitives such as hlist_add_head_rcu()
  as long as the traversal is guarded by rcu_read_lock().
  seq_hlist_start_head_rcu - start an iteration of a hlist protected by RCU
  @head: the head of the hlist
  @pos:  the start position of the sequence
  Called at seq_file->op->start(). Call this function if you want to
  print a header at the top of the output.
  This list-traversal primitive may safely run concurrently with
  the _rcu list-mutation primitives such as hlist_add_head_rcu()
  as long as the traversal is guarded by rcu_read_lock().
  seq_hlist_next_rcu - move to the next position of the hlist protected by RCU
  @v:    the current iterator
  @head: the head of the hlist
  @ppos: the current position
  Called at seq_file->op->next().
  This list-traversal primitive may safely run concurrently with
  the _rcu list-mutation primitives such as hlist_add_head_rcu()
  as long as the traversal is guarded by rcu_read_lock().
  seq_hlist_start_percpu - start an iteration of a percpu hlist array
  @head: pointer to percpu array of struct hlist_heads
  @cpu:  pointer to cpu "cursor"
  @pos:  start position of sequence
  Called at seq_file->op->start().
  seq_hlist_next_percpu - move to the next position of the percpu hlist array
  @v:    pointer to current hlist_node
  @head: pointer to percpu array of struct hlist_heads
  @cpu:  pointer to cpu "cursor"
  @pos:  start position of sequence
  Called at seq_file->op->next().
 SPDX-License-Identifier: GPL-2.0
   linuxfsreaddir.c
   Copyright (C) 1995  Linus Torvalds
  Note the "unsafe_put_user() semantics: we goto a
  label for errors.
  POSIX says that a dirent name cannot contain NULL or a ''.
  It's not 100% clear what we should really do in this case.
  The filesystem is clearly corrupted, but returning a hard
  error means that you now don't see any of the other names
  either, so that isn't a perfect alternative.
  And if you return an error, what error do you use? Several
  filesystems seem to have decided on EUCLEAN being the error
  code for EFSCORRUPTED, and that may be the error to use. Or
  just EIO, which is perhaps more obvious to users.
  In order to see the other file names in the directory, the
  caller might want to make this a "soft" error: skip the
  entry, and return the error at the end instead.
  Note that this should likely do a "memchr(name, 0, len)"
  check too, since that would be filesystem corruption as
  well. However, that case can't actually confuse user space,
  which has to do a strlen() on the name anyway to find the
  filename length, and the above "soft error" worry means
  that it's probably better left alone until we have that
  issue clarified.
  Note the PATH_MAX check - it's arbitrary but the real
  kernel limit on a possible path component, not NAME_MAX,
  which is the technical standard limit.
  Traditional linux readdir() handling..
  "count=1" is a special case, meaning that the buffer is one
  dirent-structure in size and that the code can't handle more
  anyway. Thus the special "fillonedir()" function for that
  case (the low-level handlers don't need to care about this).
 __ARCH_WANT_OLD_READDIR 
  New, all-improved, singing, dancing, iBCS2-compliant getdents()
  interface. 
 only used if we fail.. 
 This might be 'dirent->d_off', but if so it will get overwritten 
 only used if we fail.. 
 This might be 'dirent->d_off', but if so it will get overwritten 
 only used if we fail.. 
 SPDX-License-Identifier: GPL-2.0
			
			  f_files and f_ffree may be -1; it's okay to stuff
			  that into 32 bits
		 f_files and f_ffree may be -1; it's okay
  The following statfs calls are copies of code from fsstatfs.c and
  should be checked against those from time to time
  This is a copy of sys_ustat, just dealing with a structure layout.
  Given how simple this syscall is that apporach is more maintainable
  than the various conversion hacks.
 SPDX-License-Identifier: GPL-2.0-only
  fsdirect-io.c
  Copyright (C) 2002, Linus Torvalds.
  O_DIRECT
  04Jul2002	Andrew Morton
 		Initial version
  11Sep2002	janetinc@us.ibm.com
  		added readvwritev support.
  29Oct2002	Andrew Morton
 		rewrote bio_add_page() support.
  30Oct2002	pbadari@us.ibm.com
 		added support for non-aligned IO.
  06Nov2002	pbadari@us.ibm.com
 		added asynchronous IO support.
  21Jul2003	nathans@sgi.com
 		added IO completion notifier.
  How many user pages to map in one call to get_user_pages().  This determines
  the size of a structure in the slab cache
  Flags for dio_complete()
 This is async IO 
 Can invalidate pages 
  This code generally works in units of "dio_blocks".  A dio_block is
  somewhere between the hard sector size and the filesystem block size.  it
  is determined on a per-invocation basis.   When talking to the filesystem
  we need to convert dio_blocks to fs_blocks by scaling the dio_block quantity
  down by dio->blkfactor.  Similarly, fs-blocksize quantities are converted
  to bio_block quantities by shifting left by blkfactor.
  If blkfactor is zero then the user's request was aligned to the filesystem's
  blocksize.
 dio_state only used in the submission path 
 bio under assembly 
 doesn't change 
	unsigned blkfactor;		 When we're using an alignment which
					   is finer than the filesystem's soft
					   blocksize, this specifies how much
					   finer.  blkfactor=2 means 14-block
	unsigned start_zero_done;	 flag: sub-blocksize zeroing has
					   been performed at the start of a
 approximate total IO pages 
	sector_t block_in_file;		 Current offset into the underlying
 At block_in_file.  changes 
 rate limit reaping 
 doesn't change 
 prev block is at a boundary 
 block mapping function 
 IO submition function 
 current first logical block in bio 
 current final block in bio + 1 
	sector_t next_block_for_io;	 next block to be put under IO,
	
	  Deferred addition of a page to the dio.  These variables are
	  private to dio_send_cur_page(), submit_page_section() and
	  dio_bio_add_page().
 The page 
 Offset into it, in bytes 
 Nr of bytes at cur_page_offset 
 Where it starts 
 Offset in file 
	
	  Page queue.  These variables belong to dio_refill_pages() and
	  dio_get_page().
 next page to process 
 last valid page + 1 
 dio_state communicated between submission path and end_io 
 doesn't change 
 i_size when submitted 
 IO completion function 
 copy from map_bh.b_private 
 BIO completion state 
 protects BIO fields below 
 errno from get_user_pages() 
 is IO async ? 
 defer AIO completion to workqueue? 
 if pages should be dirtied 
 IO error in completion path 
 direct_io_worker() and bios 
 singly linked via bi_private 
 waiting task (NULL if none) 
 AIO related stuff 
 kiocb 
 IO result 
	
	  pages[] (and any fields placed after it) are not zeroed out at
	  allocation time.  Don't add new fields after pages[] unless you
	  wish that they not be zeroed.
 page buffer 
 deferred AIO completion 
  How many pages are in the queue?
  Go grab and pin some userspace pages.   Typically we'll get 64 at a time.
		
		  A memory fault, but the filesystem has some outstanding
		  mapped blocks.  We need to use those blocks up to avoid
		  leaking stale data in the file.
  Get another userspace page.  Returns an ERR_PTR on error.  Pages are
  buffered inside the dio so that we can call get_user_pages() against a
  decent number of pages, less frequently.  To provide nicer use of the
  L1 cache.
  dio_complete() - called when all DIO BIO IO has been completed
  This drops i_dio_count, lets interested parties know that a DIO operation
  has completed, and calculates the resulting return code for the operation.
  It lets the filesystem know if it registered an interest earlier via
  get_block.  Pass the private field of the map buffer_head so that
  filesystems can use it to hold additional state between get_block calls and
  dio_complete.
	
	  AIO submission can race with bio completion to get here while
	  expecting to have the last io completed by bio completion.
	  In that case -EIOCBQUEUED is in fact not an error we want
	  to preserve through this call.
 Check for short read case 
 ignore EFAULT if some IO has been done 
 XXX: ki_pos??
	
	  Try again to invalidate clean pages which might have been cached by
	  non-direct readahead, or faulted in by get_user_pages() if the source
	  of the write was an mmap'ed region of the file we're writing.  Either
	  one is a pretty crazy thing to do, so we don't support it 100%.  If
	  this invalidation fails, tough, the write still worked...
	 
	  And this page cache invalidation has to be after dio->end_io(), as
	  some filesystems convert unwritten extents to real allocations in
	  end_io() when necessary, otherwise a racing buffer read would cache
	  zeros from unwritten extents.
		
		  generic_write_sync expects ki_pos to have been updated
		  already, but the submission path only does this for
		  synchronous IO.
  Asynchronous IO callback. 
 cleanup the bio 
		
		  Defer completion when defer_completion is set or
		  when the inode has pages mapped and this is AIO write.
		  We need to invalidate those pages because there is a
		  chance they contain stale data in the case buffered IO
		  went in between AIO submission and completion into the
		  same region.
  The BIO completion handler simply queues the BIO up for the process-context
  handler.
  During IO bi_private points at the dio.  After IO, bi_private is used to
  implement a singly-linked list of completed BIOs, at dio->bio_list.
	
	  bio_alloc() is guaranteed to return a bio when allowed to sleep and
	  we request a valid number of vectors.
  In the AIO read case we speculatively dirty the pages before starting IO.
  During IO completion, any of these pages which happen to have been written
  back will be redirtied by bio_check_pages_dirty().
  bios hold a dio reference between submit_bio and ->end_io.
 don't account direct IO as memory stall 
  Release any resources in case of a failure
  Wait for the next BIO to complete.  Remove it and return it.  NULL is
  returned once all BIOs have been completed.  This must only be called once
  all bios have been issued so that dio->refcount can only decrease.  This
  requires that the caller hold a reference on the dio.
	
	  Wait as long as the list is empty and there are bios in flight.  bio
	  completion drops the count, maybe adds to the list, and wakes while
	  holding the bio_lock so we don't need set_current_state()'s barrier
	  and can call it after testing our condition.
 wake up sets us TASK_RUNNING 
  Process one completed BIO.  No locks are held.
 transfers ownership 
  Wait on and process all in-flight BIOs.  This must only be called once
  all bios have been issued so that the refcount can only decrease.
  This just waits for all bios to make it through dio_bio_complete.  IO
  errors are propagated through dio->io_error and should be propagated via
  dio_complete().
  A really large O_DIRECT read or write can generate a lot of BIOs.  So
  to keep the memory consumption sane we periodically reap any completed BIOs
  during the BIO generation phase.
  This also helps to limit the peak amount of pinned userspace memory.
  Create workqueue for deferred direct IO completions. We allocate the
  workqueue when it's first needed. This avoids creating workqueue for
  filesystems that don't need it and also allows us to create the workqueue
  late enough so the we can include s_id in the name of the workqueue.
	
	  This has to be atomic as more DIOs can race to create the workqueue
 Someone created workqueue before us? Free ours... 
  Call into the fs to map some more disk blocks.  We record the current number
  of available blocks at sdio->blocks_available.  These are in units of the
  fs blocksize, i_blocksize(inode).
  The fs is allowed to map lots of blocks at once.  If it wants to do that,
  it uses the passed inode-relative block number as the file offset, as usual.
  get_block() is passed the number of i_blkbits-sized blocks which direct_io
  has remaining to do.  The fs should not map more than this number of blocks.
  If the fs has mapped a lot of blocks, it should populate bh->b_size to
  indicate how much contiguous disk space has been made available at
  bh->b_blocknr.
  If any of the mapped blocks are new, then the fs must set buffer_new().
  This isn't very efficient...
  In the case of filesystem holes: the fs may return an arbitrarily-large
  hole by returning an appropriate value in b_size and by clearing
  buffer_mapped().  However the direct-io code will only process holes one
  block at a time - it will repeatedly call get_block() as it walks the hole.
 Into file, in filesystem-sized blocks 
 Into file, in filesystem-sized blocks 
 Number of filesystem-sized blocks 
	
	  If there was a memory error and we've overwritten all the
	  mapped blocks then we can now return that memory error
		
		  For writes that could fill holes inside i_size on a
		  DIO_SKIP_HOLES filesystem we forbid block creations: only
		  overwrites are permitted. We will return early to the caller
		  once we see an unmapped buffer head returned, and the caller
		  will fall back to buffered IO.
		 
		  Otherwise the decision is left to the get_blocks method,
		  which may decide to handle it or also return an unmapped
		  buffer head.
 Store for completion 
  There is no bio.  Make one now.
  Attempt to put the current chunk of 'cur_page' into the current BIO.  If
  that was successful then update final_block_in_bio and take a ref against
  the just-added page.
  Return zero on success.  Non-zero means the caller needs to start a new BIO.
		
		  Decrement count only, if we are done with this page
  Put cur_page under IO.  The section of cur_page which is described by
  cur_page_offset,cur_page_len is put into a BIO.  The section of cur_page
  starts on-disk at cur_page_block.
  We take a ref against the page here (on behalf of its presence in the bio).
  The caller of this function is responsible for removing cur_page from the
  dio, and for dropping the refcount which came from that presence.
		
		  See whether this new request is contiguous with the old.
		 
		  Btrfs cannot handle having logically non-contiguous requests
		  submitted.  For example if you have
		 
		  Logical:  [0-4095][HOLE][8192-12287]
		  Physical: [0-4095]      [4096-8191]
		 
		  We cannot submit those pages together as one BIO.  So if our
		  current logical offset in the file does not equal what would
		  be the next logical offset in the bio, submit the bio we
		  have.
  An autonomous function to put a chunk of a page under deferred IO.
  The caller doesn't actually know (or care) whether this piece of page is in
  a BIO, or is under IO or whatever.  We just take care of all possible 
  situations here.  The separation between the logic of do_direct_IO() and
  that of submit_page_section() is important for clarity.  Please don't break.
  The chunk of page starts on-disk at blocknr.
  We perform deferred IO, by recording the last-submitted page inside our
  private part of the dio structure.  If possible, we just expand the IO
  across that page here.
  If that doesn't work out then we put the old page into the bio and add this
  page to the dio instead.
 dio_send_cur_page may clear it 
		
		  Read accounting is performed in submit_bio()
	
	  Can we just grow the current page's presence in the dio?
	
	  If there's a deferred page already there then send it.
 It is in dio 
	
	  If boundary then we want to schedule the IO now to
	  avoid metadata seeks.
  If we are not writing the entire block and get_block() allocated
  the block for us, we need to fill-in the unused portion of the
  block with zeros. This happens only if user-buffer, fileoffset or
  io length is not filesystem block-size multiple.
  `end' is zero if we're doing the start of the IO, 1 at the end of the
  IO.
 In dio_blocks 
	
	  We need to zero out part of an fs block.  It is either at the
	  beginning or the end of the fs block.
  Walk the user pages, and the file, mapping blocks to disk and generating
  a sequence of (page,offset,len,block) mappings.  These mappings are injected
  into submit_page_section(), which takes care of the next stage of submission
  Direct IO against a blockdev is different from a file.  Because we can
  happily perform page-sized but 512-byte aligned IOs.  It is important that
  blockdev IO be able to have fine alignment and large sizes.
  So what we do is to permit the ->get_block function to populate bh.b_size
  with the size of IO which is permitted at this offset and this i_blkbits.
  For best results, the blockdev should be set up with 512-byte i_blkbits and
  it should set b_size to PAGE_SIZE or more inside get_block().  This gives
  fine alignment but still allows this function to work in PAGE_SIZE units.
 # of bytes mapped 
 # of blocks 
				
				  Need to go and map some more disk
				
				  If we are at the start of IO and that IO
				  starts partway into a fs-block,
				  dio_remainder will be non-zero.  If the IO
				  is a read then we can simply advance the IO
				  cursor to the first block which is to be
				  read.  But if the IO is a write and the
				  block was newly allocated we cannot do that;
				  the start of the fs block must be zeroed out
				  on-disk
 Handle holes 
 AKPM: eargh, -ENOTBLK is a hack 
				
				  Be sure to account for a partial block as the
				  last block in the file
 We hit eof 
			
			  If we're performing IO which has an alignment which
			  is finer than the underlying fs, go check to see if
			  we must zero out the start of this block.
			
			  Work out, in this_chunk_blocks, how much disk we
			  can add to this page
 Drop the ref which was taken in get_user_pages() 
	
	  Sync will always be dropping the final ref and completing the
	  operation.  AIO can if it was a broken operation described above or
	  in fact if all the bios race to complete before we get here.  In
	  that case dio_complete() translates the EIOCBQUEUED into the proper
	  return code that the caller will hand to ->complete().
	 
	  This is managed by the bio_lock instead of being an atomic_t so that
	  completion paths can drop their ref and use the remaining count to
	  decide to wake the submission path atomically.
  This is a library function for use by filesystem drivers.
  The locking rules are governed by the flags parameter:
   - if the flags value contains DIO_LOCKING we use a fancy locking
     scheme for dumb filesystems.
     For writes this function is called under i_mutex and returns with
     i_mutex held, for reads, i_mutex is not held on entry, but it is
     taken and dropped again before returning.
   - if the flags value does NOT contain DIO_LOCKING we don't use any
     internal locking but rather rely on the filesystem to synchronize
     direct IO readswrites versus each other and truncate.
  To help with locking against truncate we incremented the i_dio_count
  counter before starting direct IO, and decrement it once we are done.
  Truncate can wait for it to reach zero to provide exclusion.  It is
  expected that filesystem provide exclusion between new direct IO
  and truncates.  For DIO_LOCKING filesystems this is done by i_mutex,
  but other filesystems need to take care of this on their own.
  NOTE: if you pass "sdio" to anything by pointer make sure that function
  is always inlined. Otherwise gcc is unable to split the structure into
  individual fields and will generate much worse code. This is important
  for the whole file.
	
	  Avoid references to bdev if not absolutely needed to give
	  the early prefetch in the caller enough time.
 watch out for a 0 len io from a tricksy fs 
	
	  Believe it or not, zeroing out the page array caused a .5%
	  performance regression in a database benchmark.  So, we take
	  care to only zero out what's needed.
 will be released by direct_io_worker 
 Once we sampled i_size check for reads beyond EOF 
	
	  For file extending writes updating i_size before data writeouts
	  complete can expose uninitialized blocks in dumb filesystems.
	  In that case we need to wait for IO completion even if asked
	  for an asynchronous write.
	
	  For AIO O_(D)SYNC writes we need to defer completions to a workqueue
	  so that we can call ->fsync.
			
			  In case of AIO write racing with buffered read we
			  need to defer completion. We can't decide this now,
			  however the workqueue needs to be initialized here.
	
	  Will be decremented at IO completion time.
	
	  In case of non-aligned buffers, we may need 2 more
	  pages since we need to zero out first and last block.
		
		  The remaining part of the request will be
		  handled by buffered IO when we return
	
	  There may be some unwritten disk at the end of a part-written
	  fs-block-sized block.  Go zero that now.
	
	  It is possible that, we return short IO due to end of file.
	  In that case, we need to release all the pages we got hold on.
	
	  All block lookups have been performed. For READ requests
	  we can let i_mutex go now that its achieved its purpose
	  of protecting us from looking up uninitialized blocks.
	
	  The only time we want to leave bios in flight is when a successful
	  partial aio read or full aio write have been setup.  In that case
	  bio completion will call aio_complete.  The only time it's safe to
	  call aio_complete is when we return -EIOCBQUEUED, so we key on that.
	  This had better be the only place that raises -EIOCBQUEUED.
	
	  The block device state is needed in the end to finally
	  submit everything.  Since it's likely to be cache cold
	  prefetch it here as first thing to hide some of the
	  latency.
	 
	  Attempt to prefetch the pieces we likely need later.
 SPDX-License-Identifier: GPL-2.0
   linuxfsread_write.c
   Copyright (C) 1991, 1992  Linus Torvalds
  vfs_setpos - update the file offset for lseek
  @file:	file structure in question
  @offset:	file offset to seek to
  @maxsize:	maximum file size
  This is a low-level filesystem helper for updating the file offset to
  the value specified by @offset if the given offset is valid and it is
  not equal to the current file offset.
  Return the specified offset on success and -EINVAL on invalid offset.
  generic_file_llseek_size - generic llseek implementation for regular files
  @file:	file structure to seek on
  @offset:	file offset to seek to
  @whence:	type of seek
  @size:	max size of this file in file system
  @eof:	offset used for SEEK_END position
  This is a variant of generic_file_llseek that allows passing in a custom
  maximum file size and a custom EOF position, for e.g. hashed directories
  Synchronization:
  SEEK_SET and SEEK_END are unsynchronized (but atomic on 64bit platforms)
  SEEK_CUR is synchronized against other SEEK_CURs, but not readwrites.
  readwrites behave like SEEK_SET against seeks.
		
		  Here we special-case the lseek(fd, 0, SEEK_CUR)
		  position-querying operation.  Avoid rewriting the "same"
		  f_pos value back to the file because a concurrent read(),
		  write() or lseek() might have altered it
		
		  f_lock protects against readmodifywrite race with other
		  SEEK_CURs. Note that parallel writes and reads behave
		  like SEEK_SET.
		
		  In the generic case the entire file is data, so as long as
		  offset isn't at the end of the file then the offset is data.
		
		  There is a virtual hole at the end of the file, so as long as
		  offset isn't i_size or larger, return i_size.
  generic_file_llseek - generic llseek implementation for regular files
  @file:	file structure to seek on
  @offset:	file offset to seek to
  @whence:	type of seek
  This is a generic implemenation of ->llseek useable for all normal local
  filesystems.  It just updates the file offset to the value specified by
  @offset and @whence.
  fixed_size_llseek - llseek implementation for fixed-sized devices
  @file:	file structure to seek on
  @offset:	file offset to seek to
  @whence:	type of seek
  @size:	size of the file
  no_seek_end_llseek - llseek implementation for fixed-sized devices
  @file:	file structure to seek on
  @offset:	file offset to seek to
  @whence:	type of seek
  no_seek_end_llseek_size - llseek implementation for fixed-sized devices
  @file:	file structure to seek on
  @offset:	file offset to seek to
  @whence:	type of seek
  @size:	maximal offset allowed
  noop_llseek - No Operation Performed llseek implementation
  @file:	file structure to seek on
  @offset:	file offset to seek to
  @whence:	type of seek
  This is an implementation of ->llseek useable for the rare special case when
  userspace expects the seek to succeed but the (device) file is actually not
  able to perform the seek. In this case you use noop_llseek() instead of
  falling back to the default implementation of ->llseek.
			
			  In the generic case the entire file is data, so as
			  long as offset isn't at the end of the file then the
			  offset is data.
			
			  There is a virtual hole at the end of the file, so
			  as long as offset isn't i_size or larger, return
			  i_size.
 LFS: should only happen on 32 bit platforms 
 both values are in 0..LLONG_MAX 
	
	  Also fail if ->read_iter and ->read are both wired up as that
	  implies very convoluted semantics.
 caller is responsible for file_start_writefile_end_write 
	
	  Also fail if ->write_iter and ->write are both wired up as that
	  implies very convoluted semantics.
  This "EXPORT_SYMBOL_GPL()" is more of a "EXPORT_SYMBOL_DONTUSE()",
  but autofs is one of the few internal kernel users that actually
  wants this _and_ can be built as a module. So we need to export
  this symbol for autofs, even though it really isn't appropriate
  for any other kernel modules.
 file_ppos returns &file->f_pos or NULL if file is stream 
 Do it by hand, with file-ops 
  Various compat syscalls.  Note that they all pretend to take a native
  iovec - import_iovec will properly treat those as compat_iovecs based on
  in_compat_syscall().
 CONFIG_COMPAT 
	
	  Get input file, and verify that it is ok..
	
	  Get output file, and verify that it is ok..
	
	  We need to debate whether we can enable this or not. The
	  man page documents EAGAIN return for the output at least,
	  and the application is arguably buggy if it doesn't expect
	  EAGAIN on a non-blocking file descriptor.
  generic_copy_file_range - copy data between two files
  @file_in:	file structure to read from
  @pos_in:	file offset to read from
  @file_out:	file structure to write data to
  @pos_out:	file offset to write data to
  @len:	amount of data to copy
  @flags:	copy flags
  This is a generic filesystem helper to copy data from one file to another.
  It has no constraints on the source or destination file owners - the files
  can belong to different superblocks and different filesystem types. Short
  copies are allowed.
  This should be called from the @file_out filesystem, as per the
  ->copy_file_range() method.
  Returns the number of bytes copied or a negative error indicating the
  failure.
	
	  Although we now allow filesystems to handle cross sb copy, passing
	  a file of the wrong filesystem type to filesystem driver can result
	  in an attempt to dereference the wrong type of ->private_data, so
	  avoid doing that until we really have a good reason.  NFS defines
	  several different file_system_type structures, but they all end up
	  using the same ->copy_file_range() function pointer.
  Performs necessary checks before doing a file copy
  Can adjust amount of bytes to copy via @req_count argument.
  Returns appropriate error code that caller should return or
  zero in case the copy should be allowed.
 Don't touch certain kinds of inodes 
 Ensure offsets don't wrap. 
 Shorten the copy to EOF 
 Don't allow overlapped copying within the same file. 
  copy_file_range() differs from regular file read and write in that it
  specifically allows return partial success.  When it does so is up to
  the copy_file_range method.
	
	  Try cloning first, this is supported by more file systems, and
	  more efficient if both clone and copy are supported (e.g. NFS).
  Don't operate on ranges the page cache doesn't support, and don't exceed the
  LFS limits.  If pos is under the limit it becomes a short access.  If it
  exceeds the limit we return -EFBIG.
  Performs necessary checks before doing a write
  Can adjust writing position or amount of bytes to write.
  Returns appropriate error code that caller should return or
  zero in case that write should be allowed.
 FIXME: this is for backwards compatibility with 2.4 
  Performs common checks before doing a file copyclone
  from @file_in to @file_out.
 Don't copy dirs, pipes, sockets... 
 SPDX-License-Identifier: GPL-2.0-only
  fsdax.c - Direct Access filesystem code
  Copyright (c) 2013-2014 Intel Corporation
  Author: Matthew Wilcox <matthew.r.wilcox@intel.com>
  Author: Ross Zwisler <ross.zwisler@linux.intel.com>
 We choose 4096 entries - same as per-zone page wait tables 
 The 'colour' (ie low bits) within a PMD of a page offset.  
 The order of a PMD entry 
  DAX pagecache entries use XArray value entries so they can't be mistaken
  for pages.  We use one bit for locking, one bit for the entry size (PMD)
  and two more to tell us if the entry is a zero page or an empty entry that
  is just used for locking.  In total four special bits.
  If the PMD bit isn't set the entry has size PAGE_SIZE, and if the ZERO_PAGE
  and EMPTY bits aren't set the entry is a normal DAX entry with a filesystem
  block allocation.
  true if the entry that was found is of a smaller order than the entry
  we were looking for
  DAX page cache entry locking
  enum dax_wake_mode: waitqueue wakeup behaviour
  @WAKE_ALL: wake all waiters in the waitqueue
  @WAKE_NEXT: wake only the first waiter in the waitqueue
	
	  If 'entry' is a PMD, align the 'index' that we use for the wait
	  queue to the start of that PMD.  This ensures that all offsets in
	  the range covered by the PMD map to the same bit lock.
  @entry may no longer be the entry at the index in the mapping.
  The important information it's conveying is whether the entry at
  this index used to be a PMD entry.
	
	  Checking for locked entry and prepare_to_wait_exclusive() happens
	  under the i_pages lock, ditto for entry handling in our callers.
	  So at this point all tasks that could have seen our entry locked
	  must be in the waitqueue and the following check will see them.
  Look up entry in page cache, wait for it to become unlocked if it
  is a DAX entry and return it.  The caller must subsequently call
  put_unlocked_entry() if it did not lock the entry or dax_unlock_entry()
  if it did.  The entry returned may have a larger order than @order.
  If @order is larger than the order of the entry found in i_pages, this
  function returns a dax_is_conflict entry.
  Must be called with the i_pages lock held.
  The only thing keeping the address space around is the i_pages lock
  (it's cycled in clear_inode() after removing the entries from i_pages)
  After we call xas_unlock_irq(), we cannot touch xas->xa.
	
	  Unlike get_unlocked_entry() there is no guarantee that this
	  path ever successfully retrieves an unlocked entry before an
	  inode dies. Perform a non-exclusive wait in case this path
	  never successfully performs its own wake up.
  We used the xa_state to get the entry, but then we locked the entry and
  dropped the xa_lock, so we know the xa_state is stale and must be reset
  before use.
  Return: The entry stored at this location before it was locked.
  Iterate through all mapped pfns represented by an entry, i.e. skip
  'empty' and 'zero' entries.
  TODO: for reflink+dax we need a way to associate a single page with
  multiple address_space instances at different linear_page_index()
  offsets.
  dax_lock_mapping_entry - Lock the DAX entry corresponding to a page
  @page: The page whose entry we want to lock
  Context: Process context.
  Return: A cookie to pass to dax_unlock_page() or 0 if the entry could
  not be locked.
 Ensure page->mapping isn't freed while we look at it 
		
		  In the device-dax case there's no need to lock, a
		  struct dev_pagemap pin is sufficient to keep the
		  inode alive, and we assume we have dev_pagemap pin
		  otherwise we would not have a valid pfn_to_page()
		  translation.
  Find page cache entry at given index. If it is a DAX entry, return it
  with the entry locked. If the page cache doesn't contain an entry at
  that index, add a locked empty entry.
  When requesting an entry with size DAX_PMD, grab_mapping_entry() will
  either return that locked entry or will return VM_FAULT_FALLBACK.
  This will happen if there are any PTE entries within the PMD range
  that we are requesting.
  We always favor PTE entries over PMD entries. There isn't a flow where we
  evict PTE entries in order to 'upgrade' them to a PMD entry.  A PMD
  insertion will fail if it finds any PTE entries already in the tree, and a
  PTE insertion will cause an existing PMD entry to be unmapped and
  downgraded to PTE entries.  This happens for both PMD zero pages as
  well as PMD empty entries.
  The exception to this downgrade path is for PMD entries that have
  real storage backing them.  We will leave these real PMD entries in
  the tree, and PTE writes will simply dirty the entire PMD entry.
  Note: Unlike filemap_fault() we don't honor FAULT_FLAG_RETRY flags. For
  persistent memory the benefit is doubtful. We can add that later if we can
  show it helps.
  On error, this function does not return an ERR_PTR.  Instead it returns
  a VM_FAULT code, encoded as an xarray internal entry.  The ERR_PTR values
  overlap with xarray value entries.
 splitting PMD entry into PTE entries? 
		
		  Make sure 'entry' remains valid while we drop
		  the i_pages lock.
		
		  Besides huge zero pages the only other thing that gets
		  downgraded are empty entries which don't need to be
		  unmapped.
 undo the PMD join 
  dax_layout_busy_page_range - find first pinned page in @mapping
  @mapping: address space to scan for a page with ref count > 1
  @start: Starting offset. Page containing 'start' is included.
  @end: End offset. Page containing 'end' is included. If 'end' is LLONG_MAX,
        pages from 'start' till the end of file are included.
  DAX requires ZONE_DEVICE mapped pages. These pages are never
  'onlined' to the page allocator so they are considered idle when
  page->count == 1. A filesystem uses this interface to determine if
  any page in the mapping is busy, i.e. for DMA, or other
  get_user_pages() usages.
  It is expected that the filesystem is holding locks to block the
  establishment of new mappings in this address_space. I.e. it expects
  to be able to run unmap_mapping_range() and subsequently not race
  mapping_mapped() becoming true.
	
	  In the 'limited' case get_user_pages() for dax is disabled.
 If end == LLONG_MAX, all pages from start to till end of file 
	
	  If we race get_user_pages_fast() here either we'll see the
	  elevated page count in the iteration and wait, or
	  get_user_pages_fast() will see that the page it took a reference
	  against is no longer mapped in the page tables and bail to the
	  get_user_pages() slow path.  The slow path is protected by
	  pte_lock() and pmd_lock(). New references are not taken without
	  holding those locks, and unmap_mapping_pages() will not zero the
	  pte or pmd without holding the respective lock, so we are
	  guaranteed to either see new references or prevent new
	  references from being established.
  Delete DAX entry at @index from @mapping.  Wait for it
  to be unlocked before deleting it.
	
	  This gets called from truncate  punch_hole path. As such, the caller
	  must hold locks protecting against concurrent modifications of the
	  page cache (usually fs-private i_mmap_sem for writing). Since the
	  caller has seen a DAX entry for this index, we better find it
	  at that index as well...
  Invalidate DAX entry if it is clean.
  By this point grab_mapping_entry() has ensured that we have a locked entry
  of the appropriate size so we don't have to worry about downgrading PMDs to
  PTEs.  If we happen to be trying to insert a PTE and there is a PMD
  already in the tree, we will skip the insertion and just dirty the PMD as
  appropriate.
 we are replacing a zero page with block mapping 
 pte entry 
		
		  Only swap our new entry into the page cache if the current
		  entry is a zero page or an empty entry.  If a normal PTE or
		  PMD entry is already in the cache, we leave it alone.  This
		  means that if we are trying to insert a PTE and the
		  existing entry is a PMD, we will just leave the PMD in the
		  tree and dirty it if necessary.
 Walk the xa_state 
 Walk all mappings of a given index of a file and writeprotect them 
		
		  follow_invalidate_pte() will use the range to call
		  mmu_notifier_invalidate_range_start() on our behalf before
		  taking any lock.
		
		  No need to call mmu_notifier_invalidate_range() as we are
		  downgrading page table protection not changing it to point
		  to a new page.
		 
		  See Documentationvmmmu_notifier.rst
	
	  A page got tagged dirty in DAX mapping? Something is seriously
	  wrong.
 Entry got punched out  reallocated? 
		
		  Entry got reallocated elsewhere? No need to writeback.
		  We have to compare pfns as we must not bail out due to
		  difference in lockbit or entry type.
 Another fsync thread may have already done this entry 
 Lock the entry to serialize with page faults 
	
	  We can clear the tag now but we have to be careful so that concurrent
	  dax_writeback_one() calls for the same index cannot finish before we
	  actually flush the caches. This is achieved as the calls will look
	  at the entry only under the i_pages lock and once they do that
	  they will see the entry locked and wait for it to unlock.
	
	  If dax_writeback_mapping_range() was given a wbc->range_start
	  in the middle of a PMD, the 'index' we use needs to be
	  aligned to the start of the PMD.
	  This allows us to flush for PMD_SIZE and not have to worry about
	  partial PMD writebacks.
	
	  After we have flushed the cache, we can clear the dirty tag. There
	  cannot be new dirty data in the pfn after the flush has completed as
	  the pfn mappings are writeprotected and fault waits for mapping
	  entry lock.
  Flush the mapping to the persistent domain within the byte range of [start,
  end]. This is required by data integrity operations to ensure file data is
  on persistent storage prior to completion of the operation.
 For larger pages we need devmap 
  The user has performed a load from a hole in the file.  Allocating a new
  page in the file would cause excessive storage usage for workloads with
  sparse files.  Instead we insert a read-only mapping of the 4k zero page.
  If this page is ever written to we will re-fault and change the mapping to
  point to real DAX storage instead.
 CONFIG_FS_DAX_PMD 
	
	  Write can allocate block for an area which has a hole page mapped
	  into page tables. We have to tear down these mappings so that data
	  written by write(2) is visible in mmap.
		
		  The userspace address for the memory copy has already been
		  validated via access_ok() in either vfs_read() or
		  vfs_write(), depending on which operation we are doing.
  dax_iomap_rw - Perform IO to a DAX file
  @iocb:	The control block for this IO
  @iter:	The addresses to do IO from or to
  @ops:	iomap ops passed from the file system
  This function performs read and write operations to directly mapped
  persistent memory.  The callers needs to take care of readwrite exclusion
  and evicting any page cache pages in the region under IO.
  MAP_SYNC on a dax mapping guarantees dirty metadata is
  flushed on write-faults (non-cow), but not read-faults.
  When handling a synchronous page fault and the inode need a fsync, we can
  insert the PTEPMD into page tables only after that fsync happened. Skip
  insertion for now and return the pfn so that caller can insert it after the
  fsync is done.
  dax_fault_iter - Common actor to handle pfn insertion in PTEPMD fault.
  @vmf:	vm fault instance
  @iter:	iomap iter
  @pfnp:	pfn to be returned
  @xas:	the dax mapping tree of a file
  @entry:	an unlocked dax entry to be inserted
  @pmd:	distinguish whether it is a pmd fault
 if we are reading UNWRITTEN and HOLE, return a hole. 
 insert PMD pfn 
 insert PTE pfn 
	
	  Check whether offset isn't beyond end of file now. Caller is supposed
	  to hold locks serializing us with truncate  punch hole so this is
	  a reliable test.
	
	  It is possible, particularly with mixed reads & writes to private
	  mappings, that we have raced with a PMD fault that overlaps with
	  the PTE we need to set up.  If so just return and the fault will be
	  retried.
 fs corruption? 
	
	  Make sure that the faulting address's PMD offset (color) matches
	  the PMD offset from the start of the file.  This is necessary so
	  that a PMD range in the page table overlaps exactly with a PMD
	  range in the page cache.
 Fall back to PTEs if we're going to COW 
 If the PMD would extend outside the VMA 
 If the PMD would extend beyond the file size 
	
	  Check whether offset isn't beyond end of file now. Caller is
	  supposed to hold locks serializing us with truncate  punch hole so
	  this is a reliable test.
	
	  grab_mapping_entry() will make sure we get an empty PMD entry,
	  a zero PMD entry or a DAX PMD.  If it can't (because a PTE
	  entry is already in the array, for instance), it will return
	  VM_FAULT_FALLBACK.
	
	  It is possible, particularly with mixed reads & writes to private
	  mappings, that we have raced with a PTE fault that overlaps with
	  the PMD we need to set up.  If so just return and the fault will be
	  retried.
 actually breaks out of the loop 
 CONFIG_FS_DAX_PMD 
  dax_iomap_fault - handle a page fault on a DAX file
  @vmf: The description of the fault
  @pe_size: Size of the page to fault in
  @pfnp: PFN to insert for synchronous faults if fsync is required
  @iomap_errp: Storage for detailed error code in case of error
  @ops: Iomap ops passed from the file system
  When a page fault occurs, filesystems may call this helper in
  their fault handler for DAX files. dax_iomap_fault() assumes the caller
  has done all the necessary locking for page fault to proceed
  successfully.
  dax_insert_pfn_mkwrite - insert PTE or PMD entry into page tables
  @vmf: The description of the fault
  @pfn: PFN to insert
  @order: Order of entry to insert.
  This function inserts a writeable PTE or PMD entry into the page tables
  for an mmaped DAX file.  It also marks the page cache entry as dirty.
 Did we race with someone splitting entry or so? 
  dax_finish_sync_fault - finish synchronous page fault
  @vmf: The description of the fault
  @pe_size: Size of entry to be inserted
  @pfn: PFN to insert
  This function ensures that the file range touched by the page fault is
  stored persistently on the media and handles inserting of appropriate page
  table entry.
 SPDX-License-Identifier: GPL-2.0
 make sure ->d_prune() does nothing 
  ns_match() - Returns true if current namespace matches devino provided.
  @ns_common: current ns
  @dev: dev_t from nsfs that will be matched against current nsfs
  @ino: ino_t from nsfs that will be matched against current nsfs
  Return: true if dev and ino matches the current nsfs.
 SPDX-License-Identifier: GPL-2.0
  Shared applicationkernel submission and completion ring pairs, for
  supporting fastefficient IO.
  A note on the readwrite ordering memory barriers that are matched between
  the application and kernel side.
  After the application reads the CQ ring tail, it must use an
  appropriate smp_rmb() to pair with the smp_wmb() the kernel uses
  before writing the tail (using smp_load_acquire to read the tail will
  do). It also needs a smp_mb() before updating CQ head (ordering the
  entry load(s) with the head store), pairing with an implicit barrier
  through a control-dependency in io_get_cqe (smp_store_release to
  store head will do). Failure to do so could lead to reading invalid
  CQ entries.
  Likewise, the application must use an appropriate smp_wmb() before
  writing the SQ tail (ordering SQ entry stores with the tail store),
  which pairs with smp_load_acquire in io_get_sqring (smp_store_release
  to store the tail will do). And it needs a barrier ordering the SQ
  head load before writing new SQ entries (smp_load_acquire to read
  head will do).
  When using the SQ poll thread (IORING_SETUP_SQPOLL), the application
  needs to check the SQ flags for IORING_SQ_NEED_WAKEUP after
  updating the SQ tail; a full memory barrier smp_mb() is needed
  between.
  Also see the examples in the liburing library:
 	git:git.kernel.dkliburing
  io_uring also uses READWRITE_ONCE() for _any_ store or load that happens
  from data shared between the kernel and application. This is done both
  for ordering purposes, but also to ensure that once a value is loaded from
  data that the application could potentially modify, it remains stable.
  Copyright (C) 2018-2019 Jens Axboe
  Copyright (c) 2018-2019 Christoph Hellwig
 only define max 
  This data is shared with the application through the mmap at offsets
  IORING_OFF_SQ_RING and IORING_OFF_CQ_RING.
  The offsets to the member fields are published through struct
  io_sqring_offsets when calling io_uring_setup.
	
	  Head and tail offsets into the ring; the offsets need to be
	  masked to get valid indices.
	 
	  The kernel controls head of the sq ring and the tail of the cq ring,
	  and the application controls tail of the sq ring and the head of the
	  cq ring.
	
	  Bitmasks to apply to head and tail offsets (constant, equals
	  ring_entries - 1)
 Ring sizes (constant, power of 2) 
	
	  Number of invalid entries dropped by the kernel due to
	  invalid index stored in array
	 
	  Written by the kernel, shouldn't be modified by the
	  application (i.e. get number of "new events" by comparing to
	  cached value).
	 
	  After a new SQ head value was read by the application this
	  counter includes all submissions that were dropped reaching
	  the new SQ head (and possibly more).
	
	  Runtime SQ flags
	 
	  Written by the kernel, shouldn't be modified by the
	  application.
	 
	  The application needs a full memory barrier before checking
	  for IORING_SQ_NEED_WAKEUP after updating the sq tail.
	
	  Runtime CQ flags
	 
	  Written by the application, shouldn't be modified by the
	  kernel.
	
	  Number of completion events lost because the queue was full;
	  this should be avoided by the application by making sure
	  there are not more requests pending than there is space in
	  the completion queue.
	 
	  Written by the kernel, shouldn't be modified by the
	  application (i.e. get number of "new events" by comparing to
	  cached value).
	 
	  As completion events come in out of order this counter is not
	  ordered with any other data.
	
	  Ring buffer of completion events.
	 
	  The kernel writes completion events fresh every time they are
	  produced, so the application is allowed to modify pending
	  entries.
 int's last bit, sign checks are usually faster than a bit test 
 file  with additional FFS_ flags 
 ctx's that are using this sqd 
 inlinetask_work completion list, under ->uring_lock 
 batch completion logic 
 const or read-mostly hot data 
 submission data 
		
		  Ring buffer of indices into array of io_uring_sqe, which is
		  mmapped by the application using the IORING_OFF_SQES offset.
		 
		  This indirection could e.g. be used to assign fixed
		  io_uring_sqe entries to operations and only submit them to
		  the queue when needed.
		 
		  The kernel modifies neither the indices array nor the entries
		  array.
		
		  Fixed resources fast path, should be accessed only under
		  uring_lock, and updated through io_uring_register(2)
 IRQ completion list, under ->completion_lock 
 cred used for __io_sq_thread() 
 if using sq thread polling 
		
		  ->iopoll_list is protected by the ctx->uring_lock for
		  io_uring instances that don't use IORING_SETUP_SQPOLL.
		  For SQPOLL, only the single threaded io_sq_thread() will
		  manipulate the list, hence no extra locking is needed there.
 slow path rsrc auxilary data, used by updateregister 
 Keep this last, we don't need it for the fast path 
 hashed buffered write serialization 
 Only used for accounting purposes 
 ctx exit and cancelation 
 submission side 
  First field must be the file pointer in all the
  iocb unions! See also 'struct kiocb' in <linuxfs.h>
 head of the link, used by linked timeouts only 
 for linked completions 
 timeout update 
 NOTE: kiocb has the file as the first member, so don't do it here 
 points to an allocated iov, if NULL we use fast_iov instead 
 first byte is taken by user flags, shift it to not overlap 
 keep async readwrite and isreg together and in order 
 not a real bit, just to check we're not overflowing the space 
 ctx owns file 
 drain existing IO first 
 linked sqes 
 doesn't sever on completion < 0 
 IOSQE_ASYNC 
 IOSQE_BUFFER_SELECT 
 fail rest of links 
 on inflight list, should be cancelled and waited on exit reliably 
 readwrite uses file position 
 must not punt to workers 
 has or had linked timeout 
 needs cleanup 
 already went through poll handler 
 buffer already selected 
 completion is deferred through io_comp_state 
 caller should reissue async 
 supports async readswrites 
 regular file 
 has creds assigned 
 skip refcounting if not set 
 there is a linked timeout that has to be armed 
 ->async_data allocated 
  NOTE! Each of the iocb union members has the file pointer
  as the first entry in their struct definition. So you can
  access the file pointer through any of the sub-structs,
  or directly as just 'ki_filp' in this struct.
 polled IO has completed 
 store used ubuf, so we can prevent reloading 
 used by request caches, completion batching and iopoll 
 for polled requests, i.e. IORING_OP_POLL_ADD and async armed poll 
 internal polling, see IORING_FEAT_FAST_POLL 
 opcode allocated if it needs to store data for async defer 
 custom credentials, valid IFF REQ_F_CREDS is set 
 stores selected buf, valid IFF REQ_F_BUFFER_SELECTED is set 
 needs req->file assigned 
 should block plug 
 hash wq insertion if file is a regular file 
 unbound wq insertion if file is a non-regular file 
 set if opcode supports polled "wait" 
 op supports buffer selection 
 do prep async if is going to be punted 
 opcode is not supported by this kernel 
 skip auditing 
 size of async data needed, if any 
 used by timeout updates' prep() 
 requests with any of those set should undergo io_disarm_next() 
  Shamelessly stolen from the mm implementation of page reference checking,
  see commit f958d7b528b1 for details.
 already at zero, wait for ->release() 
	
	  Use 5 bits less than the max cq entries, that should give us around
	  32 entries per hash list if totally full and uniformly spread.
 set invalid range, so io_import_fixed() fails meeting it 
 linked timeouts should have two refs once prep'ed 
 init ->work of the whole link before punting 
	
	  Not expected to happen, but if we do have a bug where this _can_
	  happen, catch it here and ensure the request is marked as
	  canceled. That will make io-wq go through the usual work cancel
	  procedure rather than attempt to run this request (or create a new
	  worker for it).
		
		  Since seq can easily wrap around over time, subtract
		  the last seq at which timeouts were flushed before comparing.
		  Assuming not more than 2^31-1 events have happened since,
		  these subtractions won't have wrapped, so we can check if
		  target is in [last_seq, current_seq] by comparing the two.
 order cqe stores with ring update 
	
	  writes to the cq entry need to come after reading head; the
	  control dependency is enough as we're using WRITE_ONCE to
	  fill the cq entry
  This should only get called when at least one event has been posted.
  Some applications rely on the eventfd notification count only changing
  IFF a new CQE has been added to the CQ ring. There's no depedency on
  1:1 relationship between how many times this function is called (and
  hence the eventfd count) and number of CQEs posted to the CQ ring.
	
	  wake_up_all() may seem excessive, but io_wake_function() and
	  io_should_wake() handle the termination of the loop and only
	  wake as many waiters as we need to.
 see waitqueue_active() comment 
 Returns true if there are no backlogged entries after the flush 
 iopoll syncs against uring_lock, not completion_lock 
 must to be called somewhat shortly after putting a request 
		
		  If we're in ring overflow flush mode, or in task cancel mode,
		  or cannot allocate an overflow entry, then we need to drop it
		  on the floor.
	
	  If we can't get a cq entry, userspace overflowed the
	  submission (by quite a lot). Increment the overflow count in
	  the ring.
 not as hot to bloat with inlining 
	
	  If we're the last reference to this request, add to our locked
	  free_list cache.
	
	  We don't submit, fail them all, for that replace hardlinks with
	  normal links. Extra REQ_F_LINK is tolerated.
  Don't initialise the fields below on every allocation, but do that in
  advance and keep them valid across allocations.
 not necessary, but safer to zero 
 Returns true IFF there are requests in the cache 
	
	  If we have more than a batch's worth of requests in our IRQ side
	  locked cache, grab the lock and move them over to our submission
	  side cache.
  A request might get retired back into the request caches even before opcode
  handlers and io_issue_sqe() are done with it, e.g. inline completion path.
  Because of that, io_alloc_req() should be called only under ->uring_lock
  and with extra caution to not get a request that is still worked on.
	
	  Bulk alloc is all-or-nothing. If we fail to get a batch,
	  retry single alloc to be on the safe side.
	
	  If LINK is set, we have dependent requests in this chain. If we
	  didn't fail this request, queue the first one up, moving any other
	  dependencies to the next request. In case of failure, fail the rest
	  of the chain.
 if not contended, grab and improve batching 
 task_work already pending, we're done 
	
	  SQPOLL kernel thread doesn't need notification, just a wakeup. For
	  all other cases, use TWA_SIGNAL unconditionally to ensure we're
	  processing task_work. There's no reliable way to tell if TWA_RESUME
	  will do the job.
 not needed for normal modes, but SQPOLL depends on it 
 req->task == current here, checking PF_EXITING is safe 
  Drop reference to request, return next in chain (if there is one) if this
  was the last reference to this request.
 See comment at the top of this file 
 make sure SQ entry isn't read before tail 
	
	  Only spin for completions if we don't have multiple devices hanging
	  off our complete list.
		
		  Move completed and retryable entries to our local lists.
		  If we find a request that requires polling, break out
		  and complete those lists first, if we have entries there.
 iopoll may have completed current req 
 order with io_complete_rw_iopoll(), e.g. ->result updates 
  We can't just wait for polled events to come to us, we have to actively
  find and complete them.
 let it sleep and repeat later if can't complete a request 
		
		  Ensure we allow local-to-the-cpu processing to take place,
		  in this case we need to ensure that we reap all events.
		  Also let task_work, etc. to progress by releasing the mutex
	
	  We disallow the app entering submitcomplete with polling, but we
	  still need to lock the ring to prevent racing with polled issue
	  that got punted to a workqueue.
	
	  Don't enter poll loop if we already have events pending.
	  If we do, we can potentially be spinning for commands that
	  already triggered a CQE (eg in error).
		
		  If a submit got punted to a workqueue, we can have the
		  application entering polling for a command before it gets
		  issued. That app will hold the uring_lock for the duration
		  of the poll right here, so we need to take a breather every
		  now and then to ensure that the issue has a chance to add
		  the poll to the issued list. Otherwise we can spin here
		  forever, while the workqueue is stuck trying to acquire the
		  very same mutex.
 some requests don't go through iopoll_list 
	
	  Tell lockdep we inherited freeze protection from submission
	  thread.
	
	  If ref is dying, we might be running poll reap from the exit work.
	  Don't attempt to reissue from that path, just let it fail with
	  -EAGAIN.
	
	  Play it safe and assume not safe to re-import and reissue if we're
	  not in the original thread group (or in task context).
 order with io_iopoll_complete() checking ->iopoll_completed 
  After the iocb has been issued, it's safe to be found on the poll list.
  Adding the kiocb to the list AFTER submission ensures that we don't
  find it from a io_do_iopoll() thread before the issuer is done
  accessing the kiocb cookie.
 workqueue context doesn't hold uring_lock, grab it now 
	
	  Track whether we have multiple files in our lists. This will impact
	  how we do polling eventually, not spinning if we're on potentially
	  different devices.
	
	  For fast devices, IO may have already completed. If it has, add
	  it to the front so we find it first.
		
		  If IORING_SETUP_SQPOLL is enabled, sqes are either handle
		  in sq thread task context or in io worker task context. If
		  current task context is sq thread, we don't need to check
		  whether should wake up sq thread.
  If we tracked the file through the SCM inflight mechanism, we could support
  any file. For now, just ensure that anything potentially problematic is done
  inline.
 any ->readwrite should understand O_NONBLOCK 
  If we tracked the file through the SCM inflight mechanism, we could support
  any file. For now, just ensure that anything potentially problematic is done
  inline.
	
	  If the file is marked O_NONBLOCK, still allow retry for it if it
	  supports async. Otherwise it's impossible to use O_NONBLOCK files
	  reliably. If not, or it IOCB_NOWAIT is set, don't retry.
		
		  We can't just restart the syscall, since previously
		  submitted sqes may already be in progress. Just fail this
		  IO with EINTR.
 add previously done IO, if any 
 not inside the mapped region 
	
	  May not be a start of buffer, set size appropriately
	  and advance us to the beginning.
		
		  Don't use iov_iter_advance() here, as it's really slow for
		  using the latter parts of a big fixed buffer - it iterates
		  over each segment manually. We can cheat a bit here, because
		  we know that:
		 
		  1) it's a BVEC iter, we set it up
		  2) all bvecs are PAGE_SIZE in size, except potentially the
		     first and last bvec
		 
		  So just find our index, and adjust the iterator afterwards.
		  If the offset is within the first bvec (or the whole first
		  bvec, just use iov_iter_advance(). This makes it easier
		  since we can just skip the first segment, which may not
		  be PAGE_SIZE aligned.
 skip first vec 
	
	  "Normal" inline submissions always hold the uring_lock, since we
	  grab it from the system call. Same is true for the SQPOLL offload.
	  The only exception is when we've detached the request and issue it
	  from an async worker thread, grab the lock for that case.
 buffer index only valid with fixed readwrite, or buffer select  
  For files that don't have ->read_iter() and ->write_iter(), handle them
  by looping over ->read() or ->write() manually.
	
	  Don't support polled IO through this interface, and we can't
	  support non-blocking either. For the latter, this just causes
	  the kiocb to be handled from an async context.
 can only be fixed buffers, no need to do anything 
 we've copied and mapped the iter, ensure state is saved 
 submission path, ->uring_lock should already be taken 
  This is our waitqueue callback handler, registered through __folio_lock_async()
  when we initially tried to do the IO with the iocb armed our waitqueue.
  This gets called when the page is unlocked, and we generally expect that to
  happen when the page IO is completed and the page is now uptodate. This will
  queue a task_work based retry of the operation, attempting to copy the data
  again. If the latter fails because the page was NOT uptodate, then we will
  do a thread based blocking retry of the operation. That's the unexpected
  slow path.
  This controls whether a given IO request should be armed for async page
  based retry. If we return false here, the request is handed to the async
  worker threads for retry. If we're doing buffered reads on a regular file,
  we prepare a private wait_page_queue entry and retry the operation. This
  will either succeed because the page is now uptodate and unlocked, or it
  will register a callback when the page is unlocked at IO completion. Through
  that callback, io_uring uses task_work to setup a retry of the operation.
  That retry will attempt the buffered read again. The retry will generally
  succeed, or in rare cases where it fails, we then fall back to using the
  async worker threads for a blocking retry.
 never retry for NOWAIT, we just complete with -EAGAIN 
 Only for buffered IO 
	
	  just use poll if we can, and don't attempt if the fs doesn't
	  support callback based unlocks
		
		  We come here from an earlier attempt, restore our state to
		  match in case it doesn't. It's cheap enough that we don't
		  need to make this conditional.
 If the file doesn't support async, just async punt 
 Ensure we clear previously set non-block flag 
 IOPOLL retry should happen for io-wq threads 
 no retry on NONBLOCK nor RWF_NOWAIT 
 read all, failed, already did sync or don't want to retry 
	
	  Don't depend on the iter state matching what was consumed, or being
	  untouched in case of error. Restore it and we'll advance it
	  manually if we need to.
	
	  Now use our persistent iterator and state, if we aren't already.
	  We've restored and mapped the iter to match.
		
		  We end up here because of a partial read, either from
		  above or inside this loop. Advance the iter by the bytes
		  that were consumed.
 if we can retry, do so with the callbacks armed 
		
		  Now retry read with the IOCB_WAITQ parts set in the iocb. If
		  we get -EIOCBQUEUED, then we'll get a notification when the
		  desired page gets unlocked. We can also get a partial read
		  here, and if we do, then just retry at the new offset.
 we got some bytes, but not all. retry. 
 it's faster to check here then delegate to kfree 
 If the file doesn't support async, just async punt 
 file path doesn't support NOWAIT for non-direct_IO 
 Ensure we clear previously set non-block flag 
	
	  Open-code file_start_write here to grab freeze protection,
	  which will be released by another thread in
	  io_complete_rw().  Fool lockdep by telling it the lock got
	  released so that it doesn't complain about the held lock when
	  we return to userspace.
	
	  Raw bdev writes will return -EOPNOTSUPP for IOCB_NOWAIT. Just
	  retry them without IOCB_NOWAIT.
 no retry on NONBLOCK nor RWF_NOWAIT 
 IOPOLL retry should happen for io-wq threads 
 it's reportedly faster than delegating the null check to kfree() 
  IORING_OP_NOP just posts a completion event, nothing else.
 fsync always requires a blocking context 
 fallocate always requiring blocking context 
 open.how should be already initialised 
		
		  Don't bother trying for O_TRUNC, O_CREAT, or O_TMPFILE open,
		  it'll always -EAGAIN
		
		  We could hang on to this 'fd' on retrying, but seems like
		  marginal gain for something that is now known to be a slower
		  path. So just put it, and we'll get a new one when we retry.
 only retry if RESOLVE_CACHED wasn't already set by application 
 shouldn't happen 
 the head kbuf is the list itself 
 complete before unlock, IOPOLL may need the lock 
 complete before unlock, IOPOLL may need the lock 
 if the file has a flush method, be safe and punt to async 
 No ->flush() or already async, safely close from here 
 sync_file_range always requires a blocking context 
 if were using fast_iov, set it to the new one 
 fast path, check for non-NULL to avoid function call 
 fast path, check for non-NULL to avoid function call 
 !CONFIG_NET 
 CONFIG_NET 
 for instances that support it check for an event match first: 
	
	  If this fails, then the task is exiting. When a task exits, the
	  work gets canceled, so just cancel this request as well instead
	  of executing it. We can't safely execute it anyway, as we may not
	  have the needed state needed for it anyway.
 req->task == current here, checking PF_EXITING is safe 
 pure poll stashes this in ->async_data, poll driven retry elsewhere 
 for instances that support it check for an event match first: 
 make sure double remove sees this as being gone 
 use wait func handler, so it matches the rq type 
 mask in events that we always wantneed 
	
	  The file being polled uses multiple waitqueues for poll handling
	  (e.g. one for read, one for write). Setup a separate io_poll_iocb
	  if this happens.
 double add on the same waitqueue head, ignore 
 already have a 2nd entry, fail a third attempt 
		
		  Can't handle multishot for double wait for now, turn it
		  into one-shot mode.
 actually waiting for an event 
 If reading from MSG_ERRQUEUE using recvmsg, ignore POLLIN 
  Returns true if we found and killed one or more poll requests
 meaningless without update 
 no async, we'd stolen it 
	
	  Don't allow racy completion with singleshot, as we cannot safely
	  update those. For multishot, if we're racing with completion, just
	  let completion re-add it.
 we now have a detached poll request. reissue. 
 only mask one event flags, keep behavior flags 
 complete update request, we're done with it 
 can't happen, vetted at prep time 
 noseq 
 timeout removal doesn't support flags 
  Remove or update an existing timeout command
 more than one clock specified is invalid, obviously 
	
	  sqe->off holds how many events that need to occur for this
	  timeout event to be satisfied. If it isn't set, then this is
	  a pure timeout request, sequence isn't used.
	 Update the last seq here in case io_flush_timeouts() hasn't.
	  This is safe because ->completion_lock is held, and submissions
	  and completions are never mixed in the same ->completion_lock section.
	
	  Insertion sort, ensuring the first entry in the list is always
	  the one we need first.
 nxt.seq is behind @tail, otherwise would've been completed 
 slow path, try all io-wq's 
 need original cached_sq_head, but it was increased for each req 
 Still need defer if there is pending req in defer list. 
 If the op doesn't have a file, we're not polling for it 
 one will be dropped by ->io_free_work() after returning to io-wq 
 either cancelled or io-wq is dying, so don't touch tctx->iowq 
		
		  We can get EAGAIN for iopolled IO even though we're
		  forcing a sync submission from here, since we can't
		  wait for request slots on the block side.
 aborted or ready, in either case retry blocking 
 avoid locking problems by failing it from a clean context 
 mask in overlapping REQ_F and FFS bits 
 we don't allow fixed io_uring files 
	
	  We don't expect the list to be empty, that will only happen if we
	  race with the completion of the linked work.
	
	  If the back reference is NULL, then our linked request finished
	  before we got a chance to setup the timer
 drop submission reference 
		
		  Queued up for async execution, worker will release
		  submit reference when the iocb is actually submitted.
	
	  We async punt it if the file wasn't marked NOWAIT, or if the file
	  doesn't support non-blocking readwrite attempts
  Check SQE restrictions (opcode and flags).
  Returns 'true' if SQE is allowed, 'false' otherwise.
		
		  If we need to drain a request in the middle of a link, drain
		  the head request and the next requestlink after the current
		  link. Considering sequential execution of links,
		  IOSQE_IO_DRAIN will be maintained for every request of our
		  link.
 req is partially pre-initialised, see io_preinit_req() 
 same numerical values with corresponding REQ_F_, safe to copy 
 enforce forwards compatibility on users 
 knock it to the slow queue path, will be drained there 
 if there is no link, we're at "next" request and need to drain 
		
		  Plug now if we have more than 2 IO left after this, and the
		  target is potentially a readwrite to block based storage.
 fail even hard links since we don't submit 
			
			  we can judge a link req is failed or cancelled by if
			  REQ_F_FAIL is set, but the head is an exception since
			  it may be set REQ_F_FAIL because of other req's failure
			  so let's leverage req->result to distinguish if a head
			  is set REQ_F_FAIL because of its failure or other req's
			  failure so that we can set the correct ret code for it.
			  init result here to avoid affecting the normal path.
			
			  the current req is a normal req, we should return
			  error and thus break the submittion loop.
 don't need @sqe from now on 
	
	  If we already have a head request, queue this one for async
	  submittal once the head completes. If we don't have a head but
	  IOSQE_IO_LINK is set in the sqe, start a new head. This one will be
	  submitted sync once the chain is complete. If none of those
	  conditions are true (normal request), then just queue it.
 last request of a link, enqueue the link 
  Batched submission is done, ensure local IO is flushed out.
 flush only after queuing links as they can generate completions 
  Start submission side cache.
 set only head, no need to init link_last in advance 
	
	  Ensure any loads from the SQEs are done at this point,
	  since once we write the new head, the application could
	  write new data to them.
  Fetch an sqe, if one is available. Note this returns a pointer to memory
  that is mapped by userspace. This means that care needs to be taken to
  ensure that reads are stable, as we cannot rely on userspace always
  being a good citizen. If members of the sqe are validated and then later
  used, it's important that those reads are done through READ_ONCE() to
  prevent a re-load down the line.
	
	  The cached sq head (or cq tail) serves two purposes:
	 
	  1) allows us to batch the cost of updating the user visible
	     head updates.
	  2) allows the kernel side to track the head on its own, even
	     though the application is the one updating it.
 drop invalid entries 
 make sure SQ entry isn't read before tail 
 will complete beyond this point, count as submitted 
 Commit SQ ring head once we've consumed and submitted all SQEs 
 Tell userspace we may need a wakeup call 
 if we're handling multiple rings, cap submit size for fairness 
		
		  Don't submit if refs are dying, good for io_uring_register(),
		  but also it is relied upon by io_ring_exit_work()
	
	  Wake up if we have enough events, or if a timeout occurred since we
	  started waiting. For timeouts, we always want to return to userspace,
	  regardless of event count.
	
	  Cannot safely flush overflowed CQEs from here, ensure we wake up
	  the task, and the next invocation will do it.
 when returns >0, the caller should retry 
 make sure we run task_work before checking for signals 
 let the caller flush overflows, retry 
  Wait until events become available, if we don't already have some. The
  application must reap them itself, as they reside on the shared cq ring.
 if we can't even flush overflow, don't wait for more 
 recycle ref nodes in order 
 As we may drop ->uring_lock, other task may have started quiesce 
 kill initial ref, already quiesced if zero 
 wait for all works potentially completing data->done 
	
	  Do the dance but not conditional clear_bit() because it'd race with
	  other threads incrementing park_pending and setting the bit.
 fall through for EPERM case, setup new sqdtask 
  Ensure the UNIX gc is aware of our file set, so we are certain that
  the io_uring can be safely unregistered on process exit, even if we have
  loops in the file referencing.
  If UNIX sockets are enabled, fd passing can cause a reference cycle which
  causes regular reference counting to break down. We rely on the UNIX
  garbage collection to take care of this problem for us.
	
	  Find the skb that holds this file in its SCM_RIGHTS. When found,
	  remove this entry and rearrange the file array.
 allow sparse sets 
		
		  Don't allow io_uring instances to be registered. If UNIX
		  isn't enabled, then this causes a reference cycle and this
		  instance can never get freed. If UNIX is enabled we'll
		  handle it just fine, but there's still no point in allowing
		  a ring fd as it doesn't support regular readwrite anyway.
	
	  See if we can merge this file into an existing skb SCM_RIGHTS
	  file set. If there's no room, fall back to allocating a new skb
	  and filling it in.
			
			  Don't allow io_uring instances to be registered. If
			  UNIX isn't enabled, then this causes a reference
			  cycle and this instance can never get freed. If UNIX
			  is enabled we'll handle it just fine, but there's
			  still no point in allowing a ring fd as it doesn't
			  support regular readwrite anyway.
 Do QD, or 4  CPUS, whatever is smallest 
 Retain compatibility with failing for an invalid attach attempt 
 don't attach to a dying SQPOLL thread, would be racy 
 Can't have SQ_AFF without SQPOLL 
 Don't allow more pages than we can safely lock 
  Not super efficient, but this is just a registration time. And we do cache
  the last compound head, so generally we'll only do a full search if we don't
  match that one.
  We check if the given compound head page has already been accounted, to
  avoid double accounting it. This allows us to account the full size of the
  page, not just the constituent pages of a huge page.
 check current page array 
 check previously registered pages 
 don't support file backed memory 
		
		  if we did partial map, or found file backed vmas,
		  release any pages we did get
 store original address for later verification 
	
	  Don't impose further limits on the size and buffer
	  constraints here, we'll -EINVAL later when IO is
	  submitted if they are wrong.
 arbitrary limit, but we need something 
 __io_rsrc_put_work() may need uring_lock to progress, wait wo it 
 there are no registered resources left, nobody uses it 
 so that iput() is called 
	
	  synchronizes with barrier from wq_has_sleeper call in
	  io_commit_cqring
	
	  Don't flush cqring overflow list here, just do a simple check.
	  Otherwise there could possible be ABBA deadlock:
	       CPU0                    CPU1
	       ----                    ----
	  lock(&ctx->uring_lock);
	                               lock(&ep->mtx);
	                               lock(&ctx->uring_lock);
	  lock(&ep->mtx);
	 
	  Users may get EPOLLIN meanwhile seeing nothing in cqring, this
	  pushs them to do the flush.
	
	  When @in_idle, we're in cancellation and it's racy to remove the
	  node. It'll be removed by the end of cancellation, just ignore it.
	
	  If we're doing polled IO and end up having requests being
	  submitted async (out-of-line), then completions can come in while
	  we're waiting for refs to drop. We need to reap these manually,
	  as nobody else will be looking for them.
 there is little hope left, don't run it too often 
	
	  Some may use context even when all refs and requests have been put,
	  and they are free to do so while still holding uring_lock or
	  completion_lock, see io_req_task_submit(). Apart from other work,
	  this lockunlock section also waits them to finish.
 don't spin on a single task if cancellation failed 
 Returns true if we found and killed one or more timeouts 
 if we failed setting up the ctx, we might not have any rings 
	
	  Use system_unbound_wq to avoid spawning tons of event kworkers
	  if we're exiting a ton of rings at the same time. It just adds
	  noise and overhead, there's no discernable change in runtime
	  over using system_wq.
 protect against races with linked timeouts 
		
		  io_wq will stay alive while we hold uring_lock, because it's
		  killed after ctx nodes, which requires to take the lock.
			
			  Cancels requests of all rings, not only @ctx, but
			  it's fine as the task is in exitexec.
 SQPOLL thread does its own polling 
  Note that this task has used io_uring. We use it for cancelation purposes.
  Remove this io_uring_file -> task mapping.
		
		  Must be after io_uring_del_task_file() (removes nodes under
		  uring_lock) to avoid race with io_uring_try_cancel_iowq().
  Find any io_uring ctx that this task has registered or done IO on, and cancel
  requests. @sqd should be not-null IIF it's an SQPOLL thread cancellation.
 read completions before cancelations 
 sqpoll task will cancel all its requests 
		
		  If we've seen completions, retry without waiting. This
		  avoids a race where a completion comes in before we did
		  prepare_to_wait().
 for exec all current's requests should be gone, kill tctx 
 !CONFIG_MMU 
 !CONFIG_MMU 
	
	  If EXT_ARG isn't set, then we have no timespec and the argp pointer
	  is just a pointer to the sigset_t.
	
	  EXT_ARG is set - ensure we agree on the size of it and copy in our
	  timespec and sigset_t pointers if good.
	
	  For SQ polling, the thread will do all submissions and completions.
	  Just return the requested submit count, and wake the thread if
	  we were asked to.
		
		  When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user
		  space applications don't need to do io completion events
		  polling again, they can rely on io_sq_thread to do polling
		  work, which can reduce cpu usage and uring_lock contention.
	
	  we may get imprecise sqe and cqe info if uring is actively running
	  since we get cached_sq_head and cached_cq_tail without uring_lock
	  and sq_tail and cq_head are changed by userspace. But it's ok since
	  we usually use these info when it is stuck.
	
	  Avoid ABBA deadlock between the seq lock and the io_uring mutex,
	  since fdinfo case grabs it in the opposite direction of normal use
	  cases. If we fail to get the lock, we just don't iterate any
	  structures that could be going away outside the io_uring mutex.
 make sure these are sane, as we already accounted them 
  Allocate an anonymous fd, this is what constitutes the application
  visible backing of an io_uring instance. The application mmaps this
  fd to gain access to the SQCQ ring details. If UNIX sockets are enabled,
  we have to tie this fd to a socket for file garbage collection purposes.
	
	  Use twice as many entries for the CQ ring. It's possible for the
	  application to drive a higher depth than the size of the SQ ring,
	  since the sqes are only used at submission time. This allows for
	  some flexibility in overcommitting a bit. If the application has
	  set IORING_SETUP_CQSIZE, it will have passed in the desired number
	  of CQ ring entries manually.
		
		  If IORING_SETUP_CQSIZE is set, we do the same roundup
		  to a power-of-two, if it isn't already. We do NOT impose
		  any cq vs sq ring sizing.
	
	  This is just grabbed for accounting purposes. When a process exits,
	  the mm is exited and dropped before the files, hence we need to hang
	  on to this mm purely for the purposes of being able to unaccount
	  memory (lockedpinned vm). It's not used for anything else.
 always set a rsrc node 
	
	  Install ring fd as the very last thing, so we don't risk someone
	  having closed it before we finish setup
 fput will clean it up 
  Sets up an aio uring context, and returns the fd. Applications asks for a
  ring size, we return the actual sqcq ring sizes (among other things) in the
  params structure passed in.
 Restrictions allowed only if rings started disabled 
 We allow only a single restrictions registration 
 Reset all restrictions if an error happened 
 keep it extendible 
			
			  Observe the correct sqd->lock -> ctx->uring_lock
			  ordering. Fine to drop uring_lock here, we hold
			  a ref to the ctx.
 that's it for SQPOLL, only the SQPOLL task creates requests 
 now propagate the restriction to all registered users 
 ignore errors, it always returns zero anyway 
	
	  Drop uring mutex before waiting for references to exit. If another
	  thread is currently inside io_uring_enter() it might need to grab the
	  uring_lock to make progress. If we hold it here across the drain
	  wait, then we can deadlock. It's safe to drop the mutex here, since
	  no new references will come in after we've killed the percpu ref.
	
	  We're inside the ring mutex, if the ref is already dying, then
	  someone else killed the ctx or is already going through
	  io_uring_register().
 bring the ctx back to life 
 compat    int, rw_flags);
 compat  __u32, rw_flags);
 compat  __u16,  poll_events);
 ->buf_index is u16 
 should fit into one byte 
  Copyright (c) 2000-2001 Christoph Hellwig.
  All rights reserved.
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions
  are met:
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions, and the following disclaimer,
     without modification.
  2. The name of the author may not be used to endorse or promote products
     derived from this software without specific prior written permission.
  Alternatively, this software may be distributed under the terms of the
  GNU General Public License ("GPL").
  THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
  ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
  OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  SUCH DAMAGE.
  Veritas filesystem driver - shared subroutines.
  vxfs_get_page - read a page into memory.
  @ip:		inode to read from
  @n:		page number
  Description:
    vxfs_get_page reads the @n th page of @ip into the pagecache.
  Returns:
    The wanted page on success, else a NULL pointer.
 if (!PageChecked(pp)) 
 vxfs_check_page(pp); 
  vxfs_bread - read buffer for a give inode,block tuple
  @ip:		inode
  @block:	logical block
  Description:
    The vxfs_bread function reads block no @block  of
    @ip into the buffercache.
  Returns:
    The resulting &struct buffer_head.
  vxfs_get_block - locate buffer for given inode,block tuple 
  @ip:		inode
  @iblock:	logical block
  @bp:		buffer skeleton
  @create:	%TRUE if blocks may be newly allocated.
  Description:
    The vxfs_get_block function fills @bp with the right physical
    block and device number to perform a lowlevel readwrite on
    it.
  Returns:
    Zero on success, else a negativ error code (-EIO).
  vxfs_readpage - read one page synchronously into the pagecache
  @file:	file context (unused)
  @page:	page frame to fill in.
  Description:
    The vxfs_readpage routine reads @page synchronously into the
    pagecache.
  Returns:
    Zero on success, else a negative error code.
  Locking status:
    @page is locked and will be unlocked.
  vxfs_bmap - perform logical to physical block mapping
  @mapping:	logical to physical mapping to use
  @block:	logical block (relative to @mapping).
  Description:
    Vxfs_bmap find out the corresponding phsical block to the
    @mapping, @block pair.
  Returns:
    Physical block number on success, else Zero.
  Locking status:
    We are under the bkl.
  Copyright (c) 2000-2001 Christoph Hellwig.
  Copyright (c) 2016 Krzysztof Blaszkowski
  All rights reserved.
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions
  are met:
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions, and the following disclaimer,
     without modification.
  2. The name of the author may not be used to endorse or promote products
     derived from this software without specific prior written permission.
  Alternatively, this software may be distributed under the terms of the
  GNU General Public License ("GPL").
  THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
  ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
  OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  SUCH DAMAGE.
  Veritas filesystem driver - lookup and other directory related code.
  Number of VxFS blocks per page.
  vxfs_find_entry - find a mathing directory entry for a dentry
  @ip:		directory inode
  @dp:		dentry for which we want to find a direct
  @ppp:	gets filled with the page the return value sits in
  Description:
    vxfs_find_entry finds a &struct vxfs_direct for the VFS directory
    cache entry @dp.  @ppp will be filled with the page the return
    value resides in.
  Returns:
    The wanted direct on success, else a NULL pointer.
  vxfs_inode_by_name - find inode number for dentry
  @dip:	directory to search in
  @dp:		dentry we search for
  Description:
    vxfs_inode_by_name finds out the inode number of
    the path component described by @dp in @dip.
  Returns:
    The wanted inode number on success, else Zero.
  vxfs_lookup - lookup pathname component
  @dip:	dir in which we lookup
  @dp:		dentry we lookup
  @flags:	lookup flags
  Description:
    vxfs_lookup tries to lookup the pathname component described
    by @dp in @dip.
  Returns:
    A NULL-pointer on success, else a negative error code encoded
    in the return pointer.
  vxfs_readdir - read a directory
  @fp:		the directory to read
  @retp:	return buffer
  @filler:	filldir callback
  Description:
    vxfs_readdir fills @retp with directory entries from @fp
    using the VFS supplied callback @filler.
  Returns:
    Zero.
 the dir entry was not read, fix pos. 
  Copyright (c) 2000-2001 Christoph Hellwig.
  All rights reserved.
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions
  are met:
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions, and the following disclaimer,
     without modification.
  2. The name of the author may not be used to endorse or promote products
     derived from this software without specific prior written permission.
  Alternatively, this software may be distributed under the terms of the
  GNU General Public License ("GPL").
  THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
  ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
  OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  SUCH DAMAGE.
  Veritas filesystem driver - filesystem to disk block mapping.
  vxfs_bmap_ext4 - do bmap for ext4 extents
  @ip:		pointer to the inode we do bmap for
  @iblock:	logical block.
  Description:
    vxfs_bmap_ext4 performs the bmap operation for inodes with
    ext4-style extents (which are much like the traditional UNIX
    inode organisation).
  Returns:
    The physical block number on success, else Zero.
  vxfs_bmap_indir - recursion for vxfs_bmap_typed
  @ip:		pointer to the inode we do bmap for
  @indir:	indirect block we start reading at
  @size:	size of the typed area to search
  @block:	partially result from further searches
  Description:
    vxfs_bmap_indir reads a &struct vxfs_typed at @indir
    and performs the type-defined action.
  Return Value:
    The physical block number on success, else Zero.
  Note:
    Kernelstack is rare.  Unrecurse?
  vxfs_bmap_typed - bmap for typed extents
  @ip:		pointer to the inode we do bmap for
  @iblock:	logical block
  Description:
    Performs the bmap operation for typed extents.
  Return Value:
    The physical block number on success, else Zero.
  vxfs_bmap1 - vxfs-internal bmap operation
  @ip:			pointer to the inode we do bmap for
  @iblock:		logical block
  Description:
    vxfs_bmap1 perfoms a logical to physical block mapping
    for vxfs-internal purposes.
  Return Value:
    The physical block number on success, else Zero.
  Copyright (c) 2000-2001 Christoph Hellwig.
  Copyright (c) 2016 Krzysztof Blaszkowski
  All rights reserved.
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions
  are met:
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions, and the following disclaimer,
     without modification.
  2. The name of the author may not be used to endorse or promote products
     derived from this software without specific prior written permission.
  Alternatively, this software may be distributed under the terms of the
  GNU General Public License ("GPL").
  THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
  ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
  OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  SUCH DAMAGE.
  Veritas filesystem driver - superblock related routines.
  vxfs_put_super - free superblock resources
  @sbp:	VFS superblock.
  Description:
    vxfs_put_super frees all resources allocated for @sbp
    after the last instance of the filesystem is unmounted.
  vxfs_statfs - get filesystem information
  @dentry:	VFS dentry to locate superblock
  @bufp:	output buffer
  Description:
    vxfs_statfs fills the statfs buffer @bufp with information
    about the filesystem described by @dentry.
  Returns:
    Zero.
  Locking:
    No locks held.
  Notes:
    This is everything but complete...
  vxfs_read_super - read superblock into memory and initialize filesystem
  @sbp:		VFS superblock (to fill)
  @dp:			fs private mount data
  @silent:		do not complain loudly when sth is wrong
  Description:
    We are called on the first mount of a filesystem to read the
    superblock into memory and do some basic setup.
  Returns:
    The superblock on success, else %NULL.
  Locking:
    We are under @sbp->s_lock.
 Unixware, x86 
 HP-UX, parisc 
  The usual module blurb.
 makes mount -t vxfs autoload the module 
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
  Copyright (c) 2000-2001 Christoph Hellwig.
  Copyright (c) 2016 Krzysztof Blaszkowski
  All rights reserved.
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions
  are met:
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions, and the following disclaimer,
     without modification.
  2. The name of the author may not be used to endorse or promote products
     derived from this software without specific prior written permission.
  Alternatively, this software may be distributed under the terms of the
  GNU General Public License ("GPL").
  THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
  ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
  OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  SUCH DAMAGE.
  Veritas filesystem driver - fileset header routines.
  vxfs_getfsh - read fileset header into memory
  @ip:		the (fake) fileset header inode
  @which:	0 for the structural, 1 for the primary fsh.
  Description:
    vxfs_getfsh reads either the structural or primary fileset header
    described by @ip into memory.
  Returns:
    The fileset header structure on success, else Zero.
  vxfs_read_fshead - read the fileset headers
  @sbp:	superblock to which the fileset belongs
  Description:
    vxfs_read_fshead will fill the inode and structural inode list in @sb.
  Returns:
    Zero on success, else a negative error code (-EINVAL).
  Copyright (c) 2000-2001 Christoph Hellwig.
  All rights reserved.
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions
  are met:
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions, and the following disclaimer,
     without modification.
  2. The name of the author may not be used to endorse or promote products
     derived from this software without specific prior written permission.
  Alternatively, this software may be distributed under the terms of the
  GNU General Public License ("GPL").
  THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
  ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
  OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  SUCH DAMAGE.
  Veritas filesystem driver - support for 'immed' inodes.
  Address space operations for immed files and directories.
  vxfs_immed_readpage - read part of an immed inode into pagecache
  @file:	file context (unused)
  @page:	page frame to fill in.
  Description:
    vxfs_immed_readpage reads a part of the immed area of the
    file that hosts @pp into the pagecache.
  Returns:
    Zero on success, else a negative error code.
  Locking status:
    @page is locked and will be unlocked.
  Copyright (c) 2000-2001 Christoph Hellwig.
  All rights reserved.
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions
  are met:
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions, and the following disclaimer,
     without modification.
  2. The name of the author may not be used to endorse or promote products
     derived from this software without specific prior written permission.
  Alternatively, this software may be distributed under the terms of the
  GNU General Public License ("GPL").
  THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
  ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
  OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  SUCH DAMAGE.
  Veritas filesystem driver - object location table support.
  vxfs_read_olt - read olt
  @sbp:	superblock of the filesystem
  @bsize:	blocksize of the filesystem
  Description:
    vxfs_read_olt reads the olt of the filesystem described by @sbp
    into main memory and does some basic setup.
  Returns:
    Zero on success, else a negative error code.
	
	  It is in theory possible that vsi_oltsize is > 1.
	  I've not seen any such filesystem yet and I'm lazy..  --hch
  Copyright (c) 2000-2001 Christoph Hellwig.
  Copyright (c) 2016 Krzysztof Blaszkowski
  All rights reserved.
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions
  are met:
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions, and the following disclaimer,
     without modification.
  2. The name of the author may not be used to endorse or promote products
     derived from this software without specific prior written permission.
  Alternatively, this software may be distributed under the terms of the
  GNU General Public License ("GPL").
  THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
  ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
  OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  SUCH DAMAGE.
  Veritas filesystem driver - inode routines.
  Dump inode contents (partially).
  vxfs_transmod - mode for a VxFS inode
  @vip:	VxFS inode
  Description:
   vxfs_transmod returns a Linux mode_t for a given
   VxFS inode structure.
 don't endian swap the fields that differ by orgtype 
  vxfs_blkiget - find inode based on extent #
  @sbp:	superblock of the filesystem we search in
  @extent:	number of the extent to search
  @ino:	inode number to search
  Description:
   vxfs_blkiget searches inode @ino in the filesystem described by
   @sbp in the extent @extent.
   Returns the matching VxFS inode on success, else a NULL pointer.
  NOTE:
   While __vxfs_iget uses the pagecache vxfs_blkiget uses the
   buffercache.  This function should not be used outside the
   read_super() method, otherwise the data may be incoherent.
  __vxfs_iget - generic find inode facility
  @ilistp:		inode list
  @vip:		VxFS inode to fill in
  @ino:		inode number
  Description:
   Search the for inode number @ino in the filesystem
   described by @sbp.  Use the specified inode table (@ilistp).
   Returns the matching inode on success, else an error code.
  vxfs_stiget - find inode using the structural inode list
  @sbp:	VFS superblock
  @ino:	inode #
  Description:
   Find inode @ino in the filesystem described by @sbp using
   the structural inode list.
   Returns the matching inode on success, else a NULL pointer.
  vxfs_iget - get an inode
  @sbp:	the superblock to get the inode for
  @ino:	the number of the inode to get
  Description:
   vxfs_read_inode creates an inode, reads the disk inode for @ino and fills
   in all relevant fields in the new inode.
  vxfs_evict_inode - remove inode from main memory
  @ip:		inode to discard.
  Description:
   vxfs_evict_inode() is called on the final iput and frees the private
   inode area.
 SPDX-License-Identifier: GPL-2.0-only
  linuxfscephacl.c
  Copyright (C) 2013 Guangliang Zhao, <lucienchao@gmail.com>
 SPDX-License-Identifier: GPL-2.0
  ioctls
  get and set the file layout
 validate striping parameters 
 make sure it's a valid data pool 
 validate changed params against current layout 
 this is obsolete, and always -1 
  Set a layout policy on a directory inode. All items in the tree
  rooted at this inode will inherit this layout on creation,
  (It doesn't apply retroactively )
  unless a subdirectory has its own layout policy.
 copy and validate 
  Return object name, sizeoffset information, and location (OSD
  number, network address) for a given file offset.
 copy and validate 
 block_offset = object_offset % block_size 
 send result back to user 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2016 Trond Myklebust
  Copyright (c) 2019 Jeff Layton
  IO and data path helper functionality.
  Heavily borrowed from equivalent code in fsnfsio.c
 Call with exclusively locked inode->i_rwsem 
  ceph_start_io_read - declare the file is being used for buffered reads
  @inode: file inode
  Declare that a buffered read operation is about to start, and ensure
  that we block all direct IO.
  On exit, the function ensures that the CEPH_I_ODIRECT flag is unset,
  and holds a shared lock on inode->i_rwsem to ensure that the flag
  cannot be changed.
  In practice, this means that buffered read operations are allowed to
  execute in parallel, thanks to the shared lock, whereas direct IO
  operations need to wait to grab an exclusive lock in order to set
  CEPH_I_ODIRECT.
  Note that buffered writes and truncates both take a write lock on
  inode->i_rwsem, meaning that those are serialised w.r.t. the reads.
 Be an optimist! 
 Slow path.... 
  ceph_end_io_read - declare that the buffered read operation is done
  @inode: file inode
  Declare that a buffered read operation is done, and release the shared
  lock on inode->i_rwsem.
  ceph_start_io_write - declare the file is being used for buffered writes
  @inode: file inode
  Declare that a buffered write operation is about to start, and ensure
  that we block all direct IO.
  ceph_end_io_write - declare that the buffered write operation is done
  @inode: file inode
  Declare that a buffered write operation is done, and release the
  lock on inode->i_rwsem.
 Call with exclusively locked inode->i_rwsem 
 FIXME: unmap_mapping_range? 
  ceph_start_io_direct - declare the file is being used for direct io
  @inode: file inode
  Declare that a direct IO operation is about to start, and ensure
  that we block all buffered IO.
  On exit, the function ensures that the CEPH_I_ODIRECT flag is set,
  and holds a shared lock on inode->i_rwsem to ensure that the flag
  cannot be changed.
  In practice, this means that direct IO operations are allowed to
  execute in parallel, thanks to the shared lock, whereas buffered IO
  operations need to wait to grab an exclusive lock in order to clear
  CEPH_I_ODIRECT.
  Note that buffered writes and truncates both take a write lock on
  inode->i_rwsem, meaning that those are serialised w.r.t. O_DIRECT.
 Be an optimist! 
 Slow path.... 
  ceph_end_io_direct - declare that the direct io operation is done
  @inode: file inode
  Declare that a direct IO operation is done, and release the shared
  lock on inode->i_rwsem.
 SPDX-License-Identifier: GPL-2.0-only
  Ceph superblock operations
  Handle the basics of mounting, unmounting.
  super ops
 fill in kstatfs 
 ?? 
	
	  express utilization in terms of large blocks to avoid
	  overflow on 32-bit machines.
	 
	  NOTE: for the time being, we make bsize == frsize to humor
	  not-yet-ancient versions of glibc that are broken.
	  Someday, we will probably want to report a real block
	  size...  whatever that may mean for a network file system!
	
	  By default use root quota for stats; fallback to overall filesystem
	  usage if using 'noquotadf' mount option or if the root dir doesn't
	  have max_bytes quota set.
 Must convert the fsid, for consistent values across arches 
 fold the fs_cluster_id into the upper bits 
  mount options
 int args above 
 string args above 
 fsc|nofsc
 fsc=...
  Remove adjacent slashes and then the trailing slash, unless it is
  the only remaining character.
  E.g. "dir1dir2" --> "dir1dir2", "" --> "".
  Parse the source parameter.  Distinguish the server list from the path.
  The source will look like:
      <server_spec>[,<server_spec>...]:[<path>]
  where
      <server_spec> is <ip>[:<port>]
      <path> is optional, but if present must begin with ''
		
		  The server_path will include the whole chars from userland
		  including the leading ''.
 back up to ':' separator 
 at least 1M 
  ceph_show_options - Show mount options in procmounts
  @m: seq_file to write to
  @root: root of that (sub)tree
 a comma between MNTMS and client options 
 retract our comma if no client options 
  handle any mon messages the standard library doesn't understand.
  return error if we don't either.
  create a new fs client
  Success or not, this function consumes @fsopt and @opt.
 fsc->client now owns this 
	
	  The number of concurrent works can be high but they don't need
	  to be processed in parallel, limit concurrency.
  caches
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 invalidate open files
  ceph_umount_begin - initiate forced umount.  Tear down the
  mount, skipping steps that may hang while waiting for server(s).
  Bootstrap mount by opening the root directory.  Note the mount
  @started time from caller, and time out if this takes too long.
 open dir 
  mount: join the ceph cluster, and open root directory.
 note the start time 
 setup fscache 
 temp value until we get mdsmap 
  share superblock if same fs AND options
  construct our own bdi so we can control readahead, etc.
 set ra_pages based on rasize mount option? 
 set io_pages based on max osd read size 
 create client (which we maymay not use) 
  Set up the filesystem mount context.
	 Make sure all page caches get invalidated.
	 In case that we were blocklisted. This also reset
 wake up all the mds clients
 SPDX-License-Identifier: GPL-2.0
  These define virtual xattrs exposing the recursive directory
  statistics and layout metadata.
 strlen(name) + 1 (for '\0') 
 layouts 
  The convention with strings in xattrs is that they should not be NULL
  terminated, since we're returning the length with them. snprintf always
  NULL terminates however, so call it on a temporary buffer and then memcpy
  the result into place.
 NB: reevaluate size if new vxattrs are added 
 Sanity check 
 directories 
 dir pin 
 quotas 
 snapshots 
 Required table terminator 
 files 
 Required table terminator 
 Required table terminator 
 already built 
 updated internal xattr rb tree 
 lost a race, retry 
	
	  4 bytes for the length, and additional 4 bytes per each xattr name,
	  4 bytes per each value
  If there are dirty xattrs, reencode xattrs into the prealloc_blob
  and swap into place.  It returns the old i_xattrs.blob (or NULL) so
  that it can be freed by the caller as the i_ceph_lock is likely to be
  held.
 adjust buffer len; it may be larger than we need 
 let's see if a virtual xattr was requested 
 security module gets xattr while filling trace 
 get xattrs from mds (if we don't already have them) 
 == ENOATTR 
 add 1 byte for each xattr due to the null termination 
 copy value into pagelist 
 do request 
 pass any unhandled ceph. xattrs through to the MDS 
 preallocate memory for xattr name, value, index node 
 Shouldn't be required 
 prealloc_blob can't be released while holding i_ceph_lock 
 security module set xattr while filling trace 
 check if snaprealm was created for quota inode 
 match any name => handlers called with full name 
 do nothing 
	
	  FIXME: Make security_dentry_init_security() generic. Currently
	  It only supports single security module and only selinux has
	  dentry_init_security hook.
 update count of KV pairs 
 CONFIG_CEPH_FS_SECURITY_LABEL 
 CONFIG_SECURITY 
  List of handlers for synthetic system. attributes. Other
  attributes are handled directly.
 SPDX-License-Identifier: GPL-2.0
  Ceph inode operations
  Implement basic inode helpers (get, alloc) and inode ops (getattr,
  setattr, etc.), xattr helpers, and helpers for assimilating
  metadata returned by the MDS into our cache.
  Also define helpers for doing asynchronous writeback, invalidation,
  and truncation for the benefit of those who can't afford to block
  (typically because they are in the message handler path).
  find or create an inode, given the ceph ino number
  getconstuct snapdir inode for a given directory
 so we can open 
  We use a 'frag tree' to keep track of the MDS's directory fragments
  for a given inode (usually there is just a single fragment).  We
  need to know when a child frag is delegated to a new MDS, or when
  it is flagged as replicated, so we can direct our requests
  accordingly.
  findcreate a frag in the tree
  find a specific frag @f
  Choose frag containing the given value @v.  If @pfrag is
  specified, copy the frag delegation info to the caller if
  it is present.
 t is a leaf 
 choose child 
  Process dirfrag (delegation) info from the mds.  Include leaf
  fragment in tree ONLY if ndist > 0.  Otherwise, only
  branchessplits are included in i_fragtree)
 CDIR_AUTH_PARENT 
 no delegation info needed. 
 tree leaf, remove 
 tree branch, keep and clear 
 findadd this frag to store mds delegation info 
		 this is not the end of the world; we can continue
 delete stale splitleaf node 
 delete stale splitleaf node 
  initialize a newly allocated inode.
	
	  we may still have a snap_realm reference if there are stray
	  caps in i_snap_caps.
  Helpers to fill in size, ctime, mtime, and atime.  We have to be
  careful because either the client or MDS may have more up to date
  info, depending on which capabilities are held, and whether
  time_warp_seq or truncate_seq have increased.  (Ordinarily, mtime
  and size are monotonically increasing, except when utimes() or
  truncate() increments the corresponding _seq values.)
 the MDS should have revoked these caps 
			
			  If we hold relevant caps, or in the case where we're
			  not the only client referencing this file and we
			  don't hold those caps, then we need to check whether
			  the file is either opened or mmaped
 the MDS did a utimes() 
 nobody did utimes(); take the max 
 we did a utimes(); ignore mds values 
 we have no write|excl caps; whatever the MDS says is true 
 time_warp_seq shouldn't go backwards 
  Populate an inode based on info from mds.  May be called on new or
  existing inodes.
 Once I_NEW is cleared, we can't change type or dev numbers 
 prealloc new cap struct 
	
	  prealloc xattr data, if it looks like we'll need it.  only
	  if len > 4 (meaning there are actually xattrs; the first 4
	  bytes are the xattr count).
	
	  provided version will be odd if inode value is projected,
	  even if stable.  skip the update if we have newer stable
	  info (ours>=theirs, e.g. due to racing mds replies), unless
	  we are getting projected (unstable) info (in which case the
	  version is odd, and we want ours>theirs).
	    us   them
	    2    2     skip
	    3    2     skip
	    3    3     update
 Update change_attribute 
 directories have fl_stripe_unit set to zero 
 be careful with mtime, atime, size 
 only update max_size on auth cap 
	 layout and rstat are not tracked by capability, update them if
 xattrs 
 note that if i_xattrs.len <= 4, i_xattrs.data will still be NULL. 
 finally update i_version 
 lost a race 
 were we issued a capability? 
 set dir completion flag? 
 queue truncate if we saw i_size decrease 
 populate frag tree 
 update delegation info? 
  caller should hold session s_mutex and dentry->d_lock.
 only track leases on regular dentries 
 we already have a newer lease. 
  update dentry lease without having parent inode locked
 make sure dentry's name matches target 
 make sure parent matches dvino 
	 make sure dentry's inode matches target. NULL ptvino means that
  splice a dentry to an inode.
  caller must hold directory i_mutex for this to be safe.
		 If inode is directory, d_splice_alias() below will remove
		  'realdn' from its origin parent. We need to ensure that
		  origin parent's readdir cache will not reference 'realdn'
 dn must be unhashed 
  Incorporate results into the local cache.  This is either just
  one inode, or a directory, dentry, and possibly linked-to inode (e.g.,
  after a lookup).
  A reply may contain
          a directory inode along with a dentry.
   andor a target inode
  Called with snap_rwsem (read).
 Should be filled in by handle_reply 
	
	  ignore null leasebinding on snapdir ENOENT, or else we
	  will have trouble splicing in the virtual snapdir later
		
		  lookup link rename   : null -> possibly existing inode
		  mknod symlink mkdir  : null -> new inode
		  unlink               : linked -> null
 do we have a lease on the whole dir? 
 do we have a dn lease? 
 rename? 
 d_move screws up sibling dentries' offsets 
			 ensure target dentry is invalidated, despite
			 swap r_dentry and r_old_dentry in case that
			  splice_dentry() gets called later. This is safe
 null dentry? 
 attach proper inode 
 may have spliced 
 fill out a snapdir LOOKUPSNAP dentry 
 parent inode is not locked, be carefull 
  Prepopulate our cache with readdir results, leases, etc.
		 readingfilling the cache are serialized by
 mds understands offset_hash 
			 note dir version at start of readdir so we can
 FIXME: release capsleases if error occurs 
 inode 
  Make sure any pending truncation is applied before doing anything
  that may depend on it.
	
	  make sure any dirty snapped pages are flushed before we
	  possibly truncate them.. so write AND block!
 there should be no reader or writer 
  symlinks
 these do nothing 
			
			  if kernel wants to dirty ctime but nothing else,
			  we need to choose a cap to dirty under, or do
			  a almost-no-op setattr
  setattr
  Verify that we have a lease on the given mask.  If not,
  do a getattr against an mds.
 the reply is supposed to contain inline data 
  Check inode permissions.  We verify we have a valid value for
  the AUTH cap, then call the generic handler.
 Craft a mask of needed caps given a set of requested statx attrs. 
		
		  The link count for directories depends on inode->i_subdirs,
		  and that is only updated when Fs caps are held.
  Get all the attributes. If we have sufficient caps for the requested attrs,
  then we can avoid talking to the MDS at all.
 Skip the getattr altogether if we're asked not to sync 
	
	  btime on newly-allocated inodes is 0, so if this is still set to
	  that, then assume that it's not valid.
		
		  Some applications rely on the number of st_nlink
		  value on directories to be either 0 (if unlinked)
		  or 2 + number of subdirectories.
 '.' + '..' + subdirs 
 SPDX-License-Identifier: GPL-2.0
  Directory operations: readdir, lookup, create, link, unlink,
  rename, etc.
  Ceph MDS operations are specified in terms of a base ino and
  relative path.  Thus, the client can specify an operation on a
  specific inode (e.g., a getattr due to fstat(2)), or as a path
  relative to, say, the root directory.
  Normally, we limit ourselves to strict inode ops (no path component)
  or dentry operations (a single path component relative to an ino).  The
  exception to this is open_root_dentry(), which will open the mount
  point by name.
  Initialize ceph dentry state.
 oh well 
  for f_pos for readdir:
  - hash order:
 	(0xff << 52) | ((24 bits hash) << 28) |
 	(the nth entry has hash collision);
  - frag+name order;
 	((frag value) << 28) | (the nth entry in frag);
  make note of the last dentry we read, so we can
  continue at the same lexicographical point,
  regardless of what dir changes take place on the
  server.
		 readingfilling the cache are serialized by
	 check i_size again here, because empty directory can be
  When possible, we try to satisfy a readdir by peeking at the
  dcache.  We make this work by carefully ordering dentries on
  d_child when we initially get results back from the MDS, and
  falling back to a "normal" sync readdir if any dentries in the dir
  are dropped.
  Complete dir indicates that we have all dentries in the dir.  It is
  defined IFF we hold CEPH_CAP_FILE_SHARED (which will be revoked by
  the MDS ifwhen the directory is modified).
 search start position 
 use linar search 
 last_name no longer match cache index 
 always start with . and .. 
	 request Fx cap. if have Fx, we don't need to release Fs cap
 can we use the dcache? 
 proceed with a normal readdir 
 do we have the correct frag content buffered? 
 discard old result, if any 
			 fragtree isn't always accurate. choose frag
 hints to request -> mds selection code 
 adjust ctx->pos to beginning of frag 
 preclude from marking dir ordered 
				 note dir version at start of readdir so
 disable readdir cache 
 preclude from marking dir complete 
 note next offset and last dentry name 
 keep last name 
 search start position 
 more frags? 
 keep last_name 
	
	  if dir_release_count still matches the dir, no dentries
	  were released during the whole readdir, and we should have
	  the complete dir contents in our cache.
			 use i_size to track number of entries in
 compensate for . and .. 
  discard buffered readdir content on seekdir(0), or seek to new frag,
  or seek prior to current chunk
		 no need to reset last_name for a forward seek when
			 for hash offset, we don't know if a forward seek
  Handle lookups for the hidden .snap directory.
 we hold i_mutex 
 .snap dir? 
  Figure out final result of a lookupopen request.
  Mainly, make sure we return the final req->r_dentry (if it already
  existed) in place of the original VFS-provided dentry when they
  differ.
  Gracefully handle the case where the MDS replies with -ENOENT and
  no trace (which it may do, at its discretion, e.g., if it doesn't
  care to issue a lease on the negative dentry).
 no trace? 
 we got spliced 
  Look up a single dir entry.  If there is a lookup intent, inform
  the MDS so that it gets our 'caps wanted' value in a single op.
 can we conclude ENOENT locally? 
 will dput(dentry) 
  If we do a create but get no trace back from the MDS, follow up with
  a lookup (the VFS expects us to link up the provided dentry).
		
		  We created the item, then did a lookup, and found
		  it was already linked to another inode we already
		  had in our cache (and thus got spliced). To not
		  confuse VFS (especially when inode is a directory),
		  we don't link our dentry to that inode, return an
		  error instead.
		 
		  This event should be rare and it happens only when
		  we talk to old MDS. Recent MDS does not send traceless
		  reply for request that creates new inode.
 mkdir .snapfoo is a MKSNAP 
 release LINK_SHARED on source inode (mds will lock it) 
 If op failed, mark everyone involved for errors 
 mark error on parent + clear complete 
 drop the dentry -- we don't know its status 
 mark inode itself for an error (since metadata is bogus) 
 If we didn't get anything, return 0 
	
	  - We are holding Fx, which implies Fs caps.
	  - Only support async unlink for primary linkage
 Do we still want what we've got? 
  rmdir and unlink are differ only by the metadata op code
 rmdir .snapfoo is RMSNAP 
			
			  We have enough caps, so we assume that the unlink
			  will succeed. Fix up the target inode and dcache.
 don't allow cross-quota renames 
 release LINK_RDCACHE on source inode (mds will lock it) 
		
		  Normally d_move() is done by fill_trace (called by
		  do_request, above).  If there is no trace, we need
		  to do it here.
  Move dentry to tail of mdsc->dentry_leases list when lease is updated.
  Leases at front of the list will expire first. (Assume all leases have
  similar duration)
  Called under dentry->d_lock.
  When dir lease is used, add dentry to tail of mdsc->dentry_dir_leases
  list if it's not in the list, otherwise set 'referenced' flag.
  Called under dentry->d_lock.
			 don't remove dentry from dentry lease list
 move it into tail of dir lease list 
 stale lease 
				 update_dentry_lease() will re-add
				  it to lease list, or
				  ceph_d_delete() will return 1 when
 ceph_d_delete() does the trick 
		 Move dentry to tail of dir lease list if we don't want
		  to delete it. So dentries in the list are checked in a
 invalidate dir lease 
 more invalid leases 
 more to check 
  Ensure a dentry lease will no longer revalidate.
  Check if dentry lease is valid.  If not, delete the lease.  Try to
  renew if the least is more than half up.
			
			  We should renew. If we're in RCU walk mode
			  though, we can't do that so just return
			  -ECHILD.
  Called under dentry->d_lock.
  Check if directory-wide content leasecap is valid.
  Check if cached dentry can be trusted.
 always trust cached snapped dentries, snapdir dentry 
  Delete unused dentry that doesn't have valid lease
  Called under dentry->d_lock.
 won't release caps 
 vaild lease? 
  Release our ceph_dentry_info.
  When the VFS prunes a dentry from the cache, we need to clear the
  complete flag on the parent directory.
  Called under dentry->d_lock.
 do we have a valid parent? 
 we hold d_lock, so d_parent is stable 
 who calls d_delete() should also disable dcache readdir 
 d_fsdata does not get cleared until d_release 
	 Disable dcache readdir just in case that someone called d_drop()
	  or d_invalidate(), but MDS didn't revoke CEPH_CAP_FILE_SHARED
  read() on a dir.  This weird interface hack only works if mounted
  with '-o dirstat'.
  Return name hash for a given dentry.  This is dependent on
  the parent directory's hash function.
 for backward compat 
 SPDX-License-Identifier: GPL-2.0
  Ceph fs string constants
 down and out 
 up and out 
 up and in 
 SPDX-License-Identifier: GPL-2.0
 count 
 pick 
  choose a random mds that is "up" (i.e. has a state > 0), or -1.
 compat, ro_compat, incompat
 mask 
 names (map<u64, string>) 
  Decode an MDS map
  Ignore any fields we don't care about (there are quite a few of
  them).
 mdsmap_cv 
	
	  pick out the active nodes as the m_num_active_mds, the
	  m_num_active_mds maybe larger than m_max_mds when decreasing
	  the max_mds in cluster side, in other case it should less
	  than or equal to m_max_mds.
	
	  the possible max rank, it maybe larger than the m_num_active_mds,
	  for example if the mds_max == 2 in the cluster, when the MDS(0)
	  was laggy and being replaced by a new MDS, we will temporarily
	  receive a new mds map with n_num_mds == 1 and the active MDS(1),
	  and the mds rank >= m_num_active_mds.
 pick out active nodes from mds_info (state > 0) 
 info_cv 
 skip mds name 
 state_seq 
 pg_pools 
 metadata_pool 
 created + modified + tableserver 
 in 
 inc 
 up 
 failed 
 stopped 
 last_failure_osd_epoch 
 ever_allowed_snaps 
 explicitly_allowed_snaps 
 inline_data_enabled 
 enabled 
 damaged 
 SPDX-License-Identifier: GPL-2.0
  mdsc debugfs
 skip 'metadata' as it doesn't use the size metric 
 The 'num' portion of an 'entity name' 
 The -o name mount argument 
 The list of MDS session rank+state 
  debugfs
 CONFIG_DEBUG_FS 
 CONFIG_DEBUG_FS 
 SPDX-License-Identifier: GPL-2.0
  Ceph file operations
  Implement basic openclose functionality, and implement
  readwrite.
  We implement three modes of file IO:
   - buffered uses the generic_file_aio_{read,write} helpers
   - synchronous is used when there is multi-client readwrite
     sharing, avoids the page cache, and synchronously waits for an
     ack from the OSD.
   - direct io takes the variant of the sync path that references
     user pages directly.
  fsync() flushes and waits on dirty pages, but just queues metadata
  for writeback: since the MDS can recover size and mtime there is no
  need to wait for MDS acknowledgement.
  How many pages to get in one call to iov_iter_get_pages().  This
  determines the size of the on-stack array used as a buffer.
  iov_iter_get_pages() only considers one iov_iter segment, no matter
  what maxsize or maxpages are given.  For ITER_BVEC that is a single
  page.
  Attempt to get up to @maxsize bytes worth of pages from @iter.
  Return the number of bytes in the created bio_vec array, or an error.
	
	  __iter_get_bvecs() may populate only part of the array -- zero it
	  out.
		
		  No pages were pinned -- just free the array.
  Prepare an open request.  Preallocate ceph_cap to avoid an
  inopportune ENOMEM later.
  initialize private struct file data.
  if we fail, clean up by dropping fmode reference on the ceph_inode
		
		  we need to drop the open ref now, since we don't
		  have .release set to ceph_release.
 call the proper open fop 
  try renew caps after session gets killed.
  If we already have the requisite capabilities, we can satisfy
  the open request locally (no need to request new caps from the
  MDS).  We do, however, need to inform the MDS (asynchronously)
  if our wanted caps set expands.
 filter out O_CREAT|O_EXCL; vfs did that already.  yuck. 
 mds likes to know 
 snapped files are read-only 
 trivially open snapdir 
	
	  No need to block if we have caps on the auth MDS (for
	  write) or any MDS (for read).  Update wanted set
	  asynchronously.
 adjust wanted? 
 Clone the layout from a synchronous create, if the dir now has Dc caps 
  Try to set up an async create. We need caps, a file layout, and inode number,
  and either a lease on the dentry or complete dir info. If any of those
  criteria are not satisfied, then return false and the caller can go
  synchronous.
 No auth cap means no chance for Dc caps 
 Any delegated inos? 
 ???
			
			  If it's not I_NEW, then someone created this before
			  we got here. Assume the server is aware of it at
			  that point and don't worry about setting
			  CEPH_I_ASYNC_CREATE.
  Do a lookup + open with a single request.  If we get a non-existent
  file or symlink, return 1 so the VFS can retry.
 If it's not being looked up, it's negative 
 do the open 
 we were given a hashed negative dentry 
 make vfs retry on splice, ENOENT, or symlink 
 wake up anyone waiting for caps on this inode 
  Completely synchronous read and write methods.  Direct from __user
  buffer to osd, or directly to user pages (if O_DIRECT).
  If the read spans object boundary, just do multiple reads.  (That's not
  atomic, but good enough for now.)
  If we get a short result from the OSD, check against i_size; we need to
  only return a short read to the caller if we hit EOF.
	
	  flush any page cache pages in this range.  this
	  will make concurrent normal and sync io slow,
	  but it will at least behave sensibly when they are
	  in sequence.
			
			  If read is satisfied by single OSD request,
			  it can pass EOF. Otherwise read is within
			  i_size.
 r_start_latency == 0 means the request was not submitted 
 CEPH_OSD_FLAG_ORDERSNAP |  CEPH_OSD_FLAG_WRITE;
 CEPH_OSD_FLAG_ORDERSNAP |  CEPH_OSD_FLAG_WRITE;
		
		  To simplify error handling, allow AIO when IO within i_size
		  or IO can be satisfied by single OSD request.
 ignore error 
			
			  throw out any page cache pages in this range. this
			  may block.
  Synchronous write, straight from __user pointer or user pages.
  If write spans object boundary, just do multiple writes.  (For a
  correct atomic write, we should e.g. take write locks on all
  objects, rollback on failure, etc.)
 CEPH_OSD_FLAG_ORDERSNAP |  CEPH_OSD_FLAG_WRITE;
		
		  write from beginning of first page,
		  regardless of io alignment
  Wrap generic_file_aio_read with checks for cap bits on the inode.
  Atomically grab references, so that those bits are not released
  back to the MDS mid-read.
  Hmm, the sync read case isn't actually async... should it be?
 hit EOF or hole? 
  Take cap references to avoid releasing caps to MDS mid-write.
  If we are synchronous, and write with an old snap context, the OSD
  may return EOLDSNAPC.  In that case, retry the write.. _after_
  dropping our cap refs and allowing the pending snap to logically
  complete _before_ this write occurs.
  If we are near ENOSPC, write synchronously.
 We can write back this queue in page reclaim 
 we might need to revert back to that point 
		
		  No need to acquire the i_truncate_mutex. Because
		  the MDS revokes Fwb caps before sending truncate
		  message to us. We can't get Fwb cap while there
		  are pending vmtruncate. So write and vmtruncate
		  can not run at the same time
  llseek.  be sure to verify file size on SEEK_END.
		
		  Here we special-case the lseek(fd, 0, SEEK_CUR)
		  position-querying operation.  Avoid rewriting the "same"
		  f_pos value back to the file because a concurrent read(),
		  write() or lseek() might have altered it
 round offset up to next period boundary 
 Are we punching a hole beyond EOF? 
  This function tries to get FILE_WR capabilities for dst_ci and FILE_RD for
  src_ci.  Two attempts are made to obtain both caps, and an error is return if
  this fails; zero is returned on success.
	
	  Since we're already holding the FILE_WR capability for the dst file,
	  we would risk a deadlock by using ceph_get_caps.  Thus, we'll do some
	  retry dance instead to try to get both capabilities.
 Start by dropping dst_ci caps and getting src_ci caps 
 ceph_try_get_caps masks EAGAIN 
... drop src_ci caps too, and retry 
  This function does several size-related checks, returning an error if:
   - source file is smaller than off+len
   - destination file size is not OK (inode_newsize_ok())
   - max bytes quotas is exceeded
	
	  Don't copy beyond source file EOF.  Instead of simply setting length
	  to (size - src_off), just drop to VFS default implementation, as the
	  local i_size may be stale due to other clients writing to the source
	  inode.
 Do an object remote copy 
	
	  Some of the checks below will return -EOPNOTSUPP, which will force a
	  fallback to the default VFS copy_file_range implementation.  This is
	  desirable in several cases (for ex, the 'len' is smaller than the
	  size of the objects, or in cases where that would be more
	  efficient).
	
	  Striped file layouts require that we copy partial objects, but the
	  OSD copy-from operation only supports full-object copies.  Limit
	  this to non-striped file layouts for now.
 no remote copy will be done 
 Start by sync'ing the source and destination files 
	
	  We need FILE_WR caps for dst_ci and FILE_RD for src_ci as other
	  clients may have dirty data in their caches.  And OSDs know nothing
	  about caps, so they can't safely do the remote object copies.
 Drop dst file cached pages 
 XXX 
 object-level offsets need to the same 
	
	  Do a manual copy if the object offset isn't object aligned.
	  'src_objlen' contains the bytes left until the end of the object,
	  starting at the src_off
		
		  we need to temporarily drop all caps as we'll be calling
		  {read,write}_iter, which will get caps again.
 Abort on short copies or on error 
 Let the MDS know about dst file size change 
 Mark Fw dirty 
	
	  Do the final manual copy if we still have some bytes left, unless
	  there were errors in remote object copies (len >= object_size).
 SPDX-License-Identifier: GPL-2.0
  Some non-inline ceph helpers
  return true if @layout appears to be valid
 stripe unit, object size must be non-zero, 64k increment 
 object size must be a multiple of stripe unit 
 stripe count must be non-zero 
 fixme 
 this is what the VFS does 
 SPDX-License-Identifier: GPL-2.0
  quota.c - CephFS quota
  Copyright (C) 2017-2018 SUSE
 if root is the real CephFS root, we don't have quota realms 
 otherwise, we can't know for sure 
 increment msg sequence number 
 lookup inode 
 Not found, create a new one and insert it 
  This function will try to lookup a realm inode which isn't visible in the
  filesystem mountpoint.  A list of these kind of inodes (not visible) is
  maintained in the mdsc and freed only when the filesystem is umounted.
  Note that these inodes are kept in this list even if the lookup fails, which
  allows to prevent useless lookup requests.
 A request has already returned the inode 
 Check if this inode lookup has failed recently 
 get caps 
 XXX 
	
	  It should now be safe to clean quotarealms_inode tree without holding
	  mdsc->quotarealms_inodes_mutex...
  This function walks through the snaprealm for an inode and returns the
  ceph_snap_realm for the first snaprealm that has quotas set (either max_files
  or max_bytes).  If the root is reached, return the root ceph_snap_realm
  instead.
  Note that the caller is responsible for calling ceph_put_snap_realm() on the
  returned realm.
  Callers of this function need to hold mdsc->snap_rwsem.  However, if there's
  a need to do an inode lookup, this rwsem will be temporarily dropped.  Hence
  the 'retry' argument: if rwsem needs to be dropped and 'retry' is 'false'
  this function will return -EAGAIN; otherwise, the snaprealms walk-through
  will be restarted.
	
	  We need to lookup 2 quota realms atomically, i.e. with snap_rwsem.
	  However, get_quota_realm may drop it temporarily.  By setting the
	  'retry' parameter to 'false', we'll get -EAGAIN if the rwsem was
	  dropped and we can then restart the whole operation.
 check quota max_files limit 
 check quota max_files limit 
	QUOTA_CHECK_MAX_BYTES_APPROACHING_OP	 check if quota max_files
  check_quota_exceeded() will walk up the snaprealm hierarchy and, for each
  realm, it will execute quota check operation defined by the 'op' parameter.
  The snaprealm walk is interrupted if the quota check detects that the quota
  is exceeded or if the root inode is reached.
					
					  when we're writing more that 116th
					  of the available space
 Shouldn't happen 
 Just break the loop 
  ceph_quota_is_max_files_exceeded - check if we can create a new file
  @inode:	directory where a new file is being created
  This functions returns true is max_files quota allows a new file to be
  created.  It is necessary to walk through the snaprealm hierarchy (until the
  FS root) to check all realms with quotas set.
  ceph_quota_is_max_bytes_exceeded - check if we can write to a file
  @inode:	inode being written
  @newsize:	new size if write succeeds
  This functions returns true is max_bytes quota allows a file size to reach
  @newsize; it returns false otherwise.
 return immediately if we're decreasing file size 
  ceph_quota_is_max_bytes_approaching - check if we're reaching max_bytes
  @inode:	inode being written
  @newsize:	new size if write succeeds
  This function returns true if the new file size @newsize will be consuming
  more than 116th of the available quota space; it returns false otherwise.
 return immediately if we're decreasing file size 
  ceph_quota_update_statfs - if root has quota update statfs with quota status
  @fsc:	filesystem client instance
  @buf:	statfs to update
  If the mounted filesystem root has max_bytes quota set, update the filesystem
  statistics with the quota status.
  This function returns true if the stats have been updated, false otherwise.
			 It is possible for a quota to be exceeded.
			  Report 'zero' in that case
 SPDX-License-Identifier: GPL-2.0 
 encode the cap metric 
 encode the read latency metric 
 encode the write latency metric 
 encode the metadata latency metric 
 encode the dentry lease metric 
 encode the opened files metric 
 encode the pinned icaps metric 
 encode the opened inodes metric 
 encode the read io size metric 
 encode the write io size metric 
		
		  Skip it if MDS doesn't support the metric collection,
		  or the MDS will close the session's socket connection
		  directly when it get this message.
 the sq is (lat - old_avg)  (lat - new_avg) 
 SPDX-License-Identifier: GPL-2.0
  A cluster of MDS (metadata server) daemons is responsible for
  managing the file system namespace (the directory hierarchy and
  inodes) and for coordinating shared access to storage.  Metadata is
  partitioning hierarchically across a number of servers, and that
  partition varies over time as the cluster adjusts the distribution
  in order to balance load.
  The MDS client is primarily responsible to managing synchronous
  metadata requests for operations like open, unlink, and so forth.
  If there is a MDS failure, we find out about it when we (possibly
  request and) receive a new MDS map, and can resubmit affected
  requests.
  For the most part, though, we take advantage of a lossless
  communications channel to the MDS, and do not need to worry about
  timing out or resubmitting requests.
  We maintain a stateful "session" with each MDS we interact with.
  Within each session, we sent periodic heartbeat messages to ensure
  any capabilities or leases we have been issues remain valid.  If
  the session times out and goes stale, our leases and capabilities
  are no longer valid.
  mds reply parsing
	 struct_v is expected to be >= 1. we only
  parse individual inode info
		 struct_v is expected to be >= 1. we only understand
 inline data 
 quota 
 pool namespace 
 btime 
 change attribute 
 dir pin 
 snapshot birth time, remains zero for v<=2 
 snapshot count, remains zero for v<=3 
 info->snap_btime and info->rsnaps remain zero 
		 struct_v is expected to be >= 1. we only understand
		 struct_v is expected to be >= 1. we only understand
  parse a normal reply, which may contain a (dir+)dentry andor a
  target inode.
  parse readdir results
 dentry 
 dentry lease 
 inode 
 ceph_readdir_prepopulate() will update it 
 Skip over any unrecognized fields 
  parse fcntl F_GETLK results
 Skip over any unrecognized fields 
 Don't accept a delegation of system inodes 
 BITS_PER_LONG == 64 
  FIXME: xarrays can't handle 64-bit indexes on a 32-bit arch. For now, just
  ignore delegated_inos on 32 bit arch. Maybe eventually add xarrays for top
  and bottom words?
 BITS_PER_LONG == 64 
  parse create results
 Malformed reply? 
 struct_v, struct_compat, and len 
 legacy 
 Skip over any unrecognized fields 
  parse extra results
  parse entire mds reply
 trace 
 extra 
 snap blob 
  sessions
  called under mdsc->mutex
  create+register a new session for given mds.
  called under mdsc->mutex.
 one ref to sessions[], one to caller 
  called under mdsc->mutex
  drop session refs in request.
  should be last request ref, or hold mdsc->mutex
		
		  track (and drop pins for) r_old_dentry_dir
		  separately, since r_old_dentry's d_parent may have
		  changed between the dir mutex being dropped and
		  this request being freed.
  lookup session, bump ref if found.
  called under mdsc->mutex.
  Register an in-flight request, and assign a tid.  Link to directory
  are modifying (if any).
  Called under mdsc->mutex.
 set req->r_err to fail early from __do_request 
 Never leave an unregistered request on an unsafe list! 
  Walk back up the dentry tree until we hit a dentry representing a
  non-snapshot inode. We do this using the rcu_read_lock (which must be held
  when calling this) to ensure that the objects won't disappear while we're
  working with them. Once we hit a candidate dentry, we attempt to take a
  reference to it, and return that as the result.
  Choose mds to send request to next.  If there is a hint set in the
  request (e.g., due to a prior forward hint from the mds), use that.
  Otherwise, consult frag tree andor caps to identify the
  appropriate mds.  If all else fails, choose randomly.
  Called under mdsc->mutex.
	
	  is there a specific mds we should try?  ignore hint if we have
	  no session and the mds is not up (active or recovering).
 req->r_dentry is non-null for LSSNAP request 
 ignore race with rename; old or new d_parent is okay 
  not this fs or parent went negative 
			 direct snappedvirtual snapdir requests
 dentry target 
 dir + name 
 choose a random replica 
			 since this filedir wasn't known to be
			  replicated, then we want to look for the
 choose auth mds 
  session messages
 header 
 version 
 compat 
 metric spec info length 
 metric spec 
 metric spec info length 
 metric spec 
  session message, specialization for CEPH_SESSION_REQUEST_OPEN
  to include additional client metadata fields.
 Calculate serialized length of metadata 
 map length 
 supported feature 
 metric spec 
 Allocate the message 
	
	  Serialize client metadata into waiting buffer space, using
	  the format that userspace expects for map<string, string>
	 
	  ClientSession messages with metadata are v4
 The write pointer, following the session_head structure 
 Number of entries in the map 
 Two length-prefixed strings for each entry in the map 
  send session open request.
  called under mdsc->mutex
 wait for mds to go active? 
 send connect message 
  open sessions for any export targets for the given mds
  called under mdsc->mutex
  session caps
 zero out the in-progress message 
 zero r_attempts, so kick_requests() will re-send requests 
  Helper to safely iterate over all caps associated with a session, with
  special care taken to handle a racing __ceph_remove_cap().
  Caller must hold session s_mutex.
 put_cap it wo locks held 
  caller must hold session s_mutex
		
		  iterate_session_caps() skips inodes that are being
		  deleted, we need to wait until deletions are complete.
		  __wait_on_freeing_inode() is designed for the job,
		  but it is not exported, so use lookup inode function
		  to access it.
 drop cap expires and unlock s_cap_lock
  wake up any threads waiting on this session's caps.  if the cap is
  old (didn't get renewed on the client reconnect), remove it now.
  caller must hold s_mutex.
 mds did not re-issue stale cap 
  Send periodic message to MDS renewing all currently held caps.  The
  ack will reset the expiration for all caps from this session.
  caller holds s_mutex
	 do not try to renew caps until a recovering mds has reconnected
  Note new cap ttl, and any transition from stale -> not stale (fresh?).
  Called under session->s_mutex
  send a session close request
  Called with s_mutex held.
  Trim old(er) caps.
  Because we can't cache an inode without one or more caps, we do
  this indirectly: if a cap is unused, we prune its aliases, at which
  point the inode will hopefully get dropped to.
  Yes, this is a bit sloppy.  Our only real goal here is to respond to
  memory pressure from the MDS, though, so it needn't be perfect.
		 Note: it's possible that i_filelock_ref becomes non-zero
		  after dropping auth caps. It doesn't hurt because reply
	 The inode has cached pages, but it's no longer used.
 we need these caps 
 we aren't the only cap.. just remove us 
 try dropping referring dentries 
  Trim session cap count down to some max number.
  flush all dirty inode data to disk.
  returns true if we've flushed through want_flush_tid
  called under s_mutex
 Append cap_barrier field
 Append cap_barrier field
  caller holds session->s_cap_lock
  requests
  Create an mds request.
  return oldest (lowest) request, tid in request tree, 0 if none.
  called under mdsc->mutex.
  Build a dentry's path.  Allocate on heap; caller must kfree.  Based
  on build_path_from_dentry in fscifsdir.c.
  If @stop_on_nosnap, generate path relative to the first non-snapped
  inode.
  Encode hidden .snap dirs as a double , i.e.
    foo.snapbar -> foobar
 get rid of any prepended '' 
 Are we at the root? 
 Are we out of buffer? 
		
		  A rename didn't occur, but somehow we didn't end up where
		  we thought we would. Throw a warning and try again.
  request arguments may be specified via an inode , a dentry , or
  an explicit ino+path.
 gid_list 
  called under mdsc->mutex
 If r_old_dentry is set, then assume that its parent is locked 
 calculate (max) length for cap releases 
	
	  The old ceph_mds_request_head didn't contain a version field, and
	  one was added when we moved the message version from 3->4.
 make note of release offset, in case we need to replay 
 cap releases 
  called under mdsc->mutex if error, under no mutex if
  success.
  called under mdsc->mutex
		
		  Replay.  Do not regenerate message (and rebuild
		  paths, etc.); just use the original message.
		  Rebuilding paths will break for renames because
		  d_move mangles the src name.
 remove capdentry releases from message 
  called under mdsc->mutex
  send request, or put it on the appropriate wait list.
 get, open session 
		
		  We cannot queue async requests since the caps and delegated
		  inodes are bound to the session. Just return -EJUKEBOX and
		  let the caller retry a sync request in that case.
		
		  If the session has been REJECTED, then return a hard error,
		  unless it's a CLEANRECOVER mount, in which case we'll queue
		  it to the mdsc queue.
 retry the same mds later 
 send request 
 forget any previous mds hint 
 note request start time 
  called under mdsc->mutex
  Wake up threads with requests pending for @mds, so that they can
  resubmit their requests to a possibly different mds.
 only new requests 
 take CAP_PIN refs for r_inode, r_parent, r_old_dentry 
 wait 
 timed out 
 killed 
 only abort if we didn't race with a real reply 
		
		  ensure we aren't running concurrently with
		  ceph_fill_trace or ceph_readdir_prepopulate, which
		  rely on locks (dir mutex) held by our caller.
  Synchrously perform an mds request.  Take care of all of the
  session setup, forwarding, retry details.
 issue 
  Invalidate dir's completeness, dentry lease state on an aborted MDS
  namespace request.
  Handle mds reply.
  We take the session mutex and parse and process the reply immediately.
  This preserves the logical ordering of replies, capabilities, etc., sent
  by the MDS as they are applied to our local cache.
 parsed reply info 
 get request, session 
 correct session? 
 dup? 
	
	  Handle an ESTALE
	  if we're not talking to the authority, send to them
	  if the authority has changed while we weren't looking,
	  send to new authority
	  Otherwise we just have to return an ESTALE
 last request during umount? 
			
			  We already handled the unsafe response, now do the
			  cleanup.  No need to examine the response; the MDS
			  doesn't include any result info in the safe
			  response.  And even if it did, there is nothing
			  useful we could do with a revised return value.
 Must find target inode outside of mutexes to avoid deadlocks 
 snap trace 
 insert trace into our cache 
 kick calling process 
  handle mds notification that our request has been forwarded.
 dup reply? 
 resend. forward race not possible; mds would drop 
 map<string,string> 
		
		  Match "blocklisted (blacklisted)" from newer MDSes,
		  or "blacklisted" from older MDSes.
  handle a mds session control message
 decode 
		 version >= 2 and < 5, decode metadata, skip otherwise
		  as it's handled via flags.
 version >= 3, feature bits 
 version >= 4, struct_v, struct_cv, len, metric_spec 
 version >= 5, flags   
 FIXME: this ttl calculation is generous 
 for good measure 
 for good measure 
  called under session->mutex.
	
	  also re-send old requests when MDS enters reconnect stage. So that MDS
	  can process completed request in clientreplay stage.
 only old requests 
 can't handle message that contains both caps and realm 
 pre-allocate new pagelist 
 placeholder for nr_caps 
 currently encoding caps 
 placeholder for nr_realms (currently encoding relams) 
 currently encoding caps 
 currently encoding relams 
  Encode information about a cap for a reconnect with the MDS.
 set pathbase to parent dir when msg_version >= 2 
 reset cap seq 
 and issue_seq 
 and migrate_seq 
 These are lost when the session goes away 
 version, compat_version and struct_len 
		
		  number of encoded locks is stable, so copy to pagelist
 snap_follows 
	
	  snaprealms.  we provide mds with the ino, seq (version), and
	  parent for all of our realms.  If the mds has any newer info,
	  it will tell us.
  If an MDS fails and recovers, clients need to reconnect in order to
  reestablish shared state.  This includes all caps issued through
  this session _and_ the snap_realm hierarchy.  Because it's not
  clear which snap realms the mds cares about, we send everything we
  know about.. that ensures we'll then get any new info the
  recovering MDS might have.
  This is a relatively heavyweight operation, but it's rare.
 don't know if session is readonly 
	
	  notify __ceph_remove_cap() that we are composing cap reconnect.
	  If a cap get released before being added to the cap reconnect,
	  __ceph_remove_cap() should skip queuing cap release.
 drop old cap expires; we're about to reestablish that state 
 trim unused caps to reduce MDS's cache rejoin time 
 replay unsafe requests 
 placeholder for nr_caps 
 trsaverse this session's caps 
 check if all realms can be encoded into current message 
 number of realms 
 version, compat_version and struct_len 
  compare old and new mdsmaps, kicking requests
  and closing out old connections as necessary
  called under mdsc->mutex.
 force close session for stopped mds 
 just close it 
 nothing new with this mds 
		
		  send reconnect?
		
		  kick request on any mds that has gone active.
	
	  Only open and reconnect sessions that don't exist yet.
		
		  In case the import MDS is crashed just after
		  the EImportStart journal is flushed, so when
		  a standby MDS takes over it and is replaying
		  the EImportStart journal the new MDS daemon
		  will wait the client to reconnect it, but the
		  client may never registeropen the session yet.
		 
		  Will try to reconnect that MDS daemon if the
		  rank number is in the export targets array and
		  is the up:reconnect state.
		
		  The session maybe registered and opened by some
		  requests which were choosing random MDSes during
		  the mdsc->mutex's unlocklock gap below in rare
		  case. But the related MDS daemon will just queue
		  that requests and be still waiting for the client's
		  reconnection request in up:reconnect state.
  leases
  caller must hold session s_mutex, dentry->d_lock
 decode 
 lookup inode 
 dentry 
 hrm... 
 let's just reuse the same message 
	
	  if this is a preemptive lease RELEASE, no need to
	  flush request stream, since the actual request will
	  soon follow.
  lock unlock the session, to wait ongoing session activities
 Should never reach this when not force unmounting 
  If the sequence is incremented while we're waiting on a REQUEST_CLOSE reply,
  then we need to retransmit that request.
  delayed work -- periodically trim expired leases, renew caps with mds.  If
  the @delay parameter is set to 0 or if it's more than 5 secs, the default
  workqueue delay value of 5 secs will be used.
 5 secs default delay 
  Wait for safe replies on open mds requests.  If we time out, drop
  all requests from the tree to avoid dangling dentry refs.
 tear down remaining requests 
	
	  Pre-luminous MDS crashes when it sees an unknown session request
  called before mount is ro, and before dentries are torn down.
  (hmm, does this still race with new lookups?)
	
	  wait for reply handlers to drop their request refs and
	  their inodedcache refs
  wait for all write mds requests to flush.
 find next request 
 write op 
 next dne before, so we're done! 
 next request was removed from tree 
 won't go away 
  true if all sessions are closed, or we force unmount
  called after sb is ro.
 close sessions 
 tear down remaining sessions 
 cancel timer 
	
	  Make sure the delayed work stopped before releasing
	  the resources.
	 
	  Because the cancel_delayed_work_sync() will only
	  guarantee that the work finishes executing. But the
	  delayed work will re-arm itself again after that.
 flush out any connection work with references to us 
 struct_v, struct_cv, map_len, epoch, legacy_client_fscid 
 info_v, info_cv
  handle mds map update.
 do we need it? 
 swap into place 
 first mds map 
  if the client is unresponsive for long enough, the mds will kill
  the session entirely.
  authentication
  Note: returned pointer is the address of a structure that's
  managed separately.  Caller must not attempt to free it.
 eof 
 SPDX-License-Identifier: GPL-2.0-only
  Ceph cache definitions.
   Copyright (C) 2013 by Adfin Solutions, Inc. All Rights Reserved.
   Written by Milosz Tanski (milosz@adfin.com)
 The following members must be last 
 all other fs ignore this error 
 No caching for filesystem 
 Only cache for regular files that are read only 
 SPDX-License-Identifier: GPL-2.0
 unused map expires after 5 minutes 
  Snapshots in ceph are driven in large part by cooperation from the
  client.  In contrast to local file systems or file servers that
  implement snapshots at a single point in the system, ceph's
  distributed access to storage requires clients to help decide
  whether a write logically occurs before or after a recently created
  snapshot.
  This provides a perfect instantanous client-wide snapshot.  Between
  clients, however, snapshots may appear to be applied at slightly
  different points in time, depending on delays in delivering the
  snapshot notification.
  Snapshots are _not_ file system-wide.  Instead, each snapshot
  applies to the subdirectory nested beneath some directory.  This
  effectively divides the hierarchy into multiple "realms," where all
  of the files contained by each realm share the same set of
  snapshots.  An individual realm's snap set contains snapshots
  explicitly created on that realm, as well as any snaps in its
  parent's snap set _after_ the point at which the parent became it's
  parent (due to, say, a rename).  Similarly, snaps from prior parents
  during the time intervals during which they were the parent are included.
  The client is spared most of this detail, fortunately... it must only
  maintains a hierarchy of realms reflecting the current parentchild
  realm relationship, and for each realm has an explicit list of snaps
  inherited from prior parents.
  A snap_realm struct is maintained for realms containing every inode
  with an open cap in the system.  (The needed snap realm information is
  provided by the MDS whenever a cap is issued, i.e., on open.)  A 'seq'
  version number is used to ensure that as realm parameters change (new
  snapshot, new parent, etc.) the client's realm hierarchy is updated.
  The realm hierarchy drives the generation of a 'snap context' for each
  realm, which simply lists the resulting set of snaps for the realm.  This
  is attached to any writes sent to OSDs.
  Unfortunately error handling is a bit mixed here.  If we get a snap
  update, but don't have enough memory to update our realm hierarchy,
  it's not clear what we can do about it (besides complaining to the
  console).
  increase ref count for the realm
  caller must hold snap_rwsem.
	
	  The 0->1 and 1->0 transitions must take the snap_empty_lock
	  atomically with the refcount change. Go ahead and bump the
	  nref here, unless it's 0, in which case we take the spinlock
	  and then do the increment and remove it from the list.
  create and get the realm rooted at @ino and bump its ref count.
  caller must hold snap_rwsem for write.
 for caller 
  lookup the realm rooted at @ino.
  caller must hold snap_rwsem.
  called with snap_rwsem (write)
  caller holds snap_rwsem (write)
	
	  We do not require the snap_empty_lock here, as any caller that
	  increments the value must hold the snap_rwsem.
  See comments in ceph_get_snap_realm. Caller needn't hold any locks.
  Clean up any realms whose ref counts have dropped to zero.  Note
  that this does not include realms who were created but not yet
  used.
  Called under snap_rwsem (write)
  adjust the parent realm of a given @realm.  adjust child list, and parent
  pointers, and ref counts appropriately.
  return true if parent was changed, 0 if unchanged, <0 on error.
  caller must hold snap_rwsem for write.
  build the snap context for a given realm.
	
	  build parent context, if it hasn't been built.
	  conservatively estimate that all parent snaps might be
	  included by us.
	 do i actually need to update?  not if my context seq
	   matches realm seq, and my parents' does to.  (this works
	   because we rebuild_snap_realms() works _downward_ in
 alloc new snap context 
 build (reverse sorted) snap vector 
		 include any of parent's snaps occurring _after_ my
 queue realm for cap_snap creation 
	
	  if we fail, clear old (incorrect) cached_context... hopefully
	  we'll have better luck building it later
  rebuild snap context for the given realm and all of its children.
  helper to allocate and decode an array of snapids.  free prior
  instance, if any.
 snaps are in descending order 
  When a snapshot is applied, the sizemtime inode metadata is queued
  in a ceph_cap_snap (one for each snapshot) until writeback
  completes and the metadata can be flushed back to the MDS.
  However, if a (sync) write is currently in-progress when we apply
  the snapshot, we have to wait until the write succeeds or fails
  (and a final sizemtime is known).  In this case the
  cap_snap->writing = 1, and is said to be "pending."  When the write
  finishes, we __ceph_finish_cap_snap().
  Caller must hold snap_rwsem for read (i.e., the realm topology won't
  change).
	
	  If there is a write in progress, treat that as a dirty Fw,
	  even though it hasn't completed yet; by the time we finish
	  up this capsnap it will be.
		 there is no point in queuing multiple "pending" cap_snaps,
		   as no new writes are allowed to start when pending, so any
		   writes in progress now were started before the previous
	
	  There is no need to send FLUSHSNAP message to MDS if there is
	  no new snapshot. But when there is dirty pages or on-going
	  writes, we still need to create cap_snap. cap_snap is needed
	  by the write path and page writeback path.
	 
	  also see ceph_try_drop_cap_snap()
	 dirty page count moved from _head to this cap_snap;
	   all subsequent writes page dirties occur _after_ this
 note mtime, size NOW. 
  Finalize the size, mtime for a cap_snap.. that is, settle on final values
  to be used for the snapshot, to be flushed back to the mds.
  If capsnap can now be flushed, add to snap_flush list, and return 1.
  Caller must hold i_ceph_lock.
 Fb cap still in use, delay it 
 caller may want to ceph_flush_snaps 
  Queue cap_snaps for snap writeback for this realm and its children.
  Called under snap_rwsem, so realm topology won't change.
  Parse and apply a snapblob "snap trace" from the MDS.  This specifies
  the snap realm parameters from a given realm and all of its ancestors,
  up to the root.
  Caller must hold snap_rwsem for write.
 encoded 
 encoded 
 encoded 
 ensure the parent is correct 
 update realm parameters, snap lists 
 invalidate when we reach the _end_ (root) of the trace 
	
	  queue cap snaps _after_ we've built the new snap contexts,
	  so that i_head_snapc can be set appropriately.
  Send any cap_snaps that are queued for flush.  Try to carry
  s_mutex across multiple snap flushes to avoid locking overhead.
  Caller holds no locks.
  ceph_change_snap_realm - change the snap_realm for an inode
  @inode: inode to move to new snap realm
  @realm: new realm to move inode into (may be NULL)
  Detach an inode from its old snaprealm (if any) and attach it to
  the new snaprealm (if any). The old snap realm reference held by
  the inode is put. If realm is non-NULL, then the caller's reference
  to it is taken over by the inode.
  Handle a snap notification from the MDS.
  This can take two basic forms: the simplest is just a snap creation
  or deletion notification on an existing realm.  This should update the
  realm and its children.
  The more difficult case is realm creation, due to snap creation at a
  new point in the file hierarchy, or due to a rename that moves a file or
  directory into another realm.
 decode 
	split = le64_to_cpu(h->split);    non-zero if we are splitting an
		
		  A "split" breaks part of an existing realm off into
		  a new realm.  The MDS provides a list of inodes
		  (with caps) and child realms that belong to the new
		  child.
		 we will peek at realm info here, but will _not_
		  advance p, as the realm update will occur below in
			
			  If this inode belongs to a realm that was
			  created after our new realm, we experienced
			  a race (due to another split notifications
			  arriving from a different MDS).  So skip
			  this inode.
 we may have taken some of the old realm's children. 
	
	  update using the provided snap trace. if we are deleting a
	  snap, we can avoid queueing cap_snaps.
 we took a reference when we created the realm, above 
			 already cleaned up by
 SPDX-License-Identifier: GPL-2.0
	
	  Set the most significant bit, so that MDS knows the 'owner'
	  is sufficient to identify the owner of lock. (old code uses
	  both 'owner' and 'pid')
 clear error when all locks are released 
  Implement fcntl and flock locking functions.
		
		  increasing i_filelock_ref closes race window between
		  handling request reply and adding file_lock struct to
		  inode. Otherwise, auth caps may get trimmed in the
		  window. Caller function will decrease the counter.
 mds requires start and length rather than start and end 
		
		  ensure we aren't running concurrently with
		  ceph_fill_trace or ceph_readdir_prepopulate, which
		  rely on locks (dir mutex) held by our caller.
 haven't sent the request
  Attempt to set an fcntl lock.
  For now, this just goes away to the server. Later it may be more awesome.
 set wait bit as appropriate, then make command as Ceph expects it
				 undo! This should only happen if
				  the kernel detects local
  Fills in the passed counter variables, so you can prepare pagelist metadata
  before calling ceph_encode_locks.
  Given a pointer to a lock, convert it to a ceph filelock
  Encode the flock and fcntl locks for the given inode into the ceph_filelock
  array. Must be called with inode->i_lock already held.
  If we encounter more of a specific lock type than expected, return -ENOSPC.
  Copy the encoded flock and fcntl locks into the pagelist.
  Format is: #fcntl locks, sequential fcntl locks, #flock locks,
  sequential flock locks.
  Returns zero on success.
 SPDX-License-Identifier: GPL-2.0
  Basic fh
  Larger fh that includes parent ino.
  fh for snapped inode
 We need LINK caps to reliably check i_nlink 
 -ESTALE if inode as been unlinked and no file is open 
 mds does not support lookup snapped inode 
 see comments in ceph_get_parent() 
  convert regular fh to dentry
 do not support non-directory 
		 There can be multiple paths to access snapped inode.
		 If directory has already been deleted, futher get_parent
		  will fail. Do not mark snapdir dentry as disconnected,
  convert regular fh to parent
 SPDX-License-Identifier: GPL-2.0
 generic_writepages 
  Ceph address space ops.
  There are a few funny things going on here.
  The page->private field is used to reference a struct
  ceph_snap_context for _every_ dirty page.  This indicates which
  snapshot the page was logically dirtied in, and thus which snap
  context needs to be associated with the osd write during writeback.
  Similarly, struct ceph_inode_info maintains a set of counters to
  count dirty pages on the inode.  In the absence of snapshots,
  i_wrbuffer_ref == i_wrbuffer_ref_head == the dirty page count.
  When a snapshot is taken (that is, when the client receives
  notification that a snapshot was taken), each inode with caps and
  with dirty pages (dirty pages implies there is a cap) gets a new
  ceph_cap_snap in the i_cap_snaps list (which is sorted in ascending
  order, new snaps go to the tail).  The i_wrbuffer_ref_head count is
  moved to capsnap->dirty. (Unless a sync write is currently in
  progress.  In that case, the capsnap is said to be "pending", new
  writes cannot start, and the capsnap isn't "finalized" until the
  write completes (or fails) and a final sizemtime for the inode for
  that snap can be settled upon.)  i_wrbuffer_ref_head is reset to 0.
  On writeback, we must submit writes to the osd IN SNAP ORDER.  So,
  we look for the first capsnap in i_cap_snaps and write out pages in
  that snap context _only_.  Then we move on to the next capsnap,
  eventually reaching the "live" or "head" context (i.e., pages that
  are not yet snapped) and are writing the most recently dirtied
  pages.
  Invalidate and so forth must take care to ensure the dirty page
  accounting is preserved.
  Dirty a page.  Optimistically adjust accounting, on the assumption
  that we won't race with invalidate.  If we do, readjust.
 dirty the head 
 caller should hold Fw reference
	
	  Reference snap context in page->private.  Also set
	  PagePrivate so that we get invalidatepage callback.
  If we are truncating the full page (i.e. offset == 0), adjust the
  dirty page counters appropriately.  Only called if there is private
  data on the page.
 Expand the start downward 
 Now, round up the length to the next block 
 Truncate the extent at the end of the current block 
 no object means success but no data 
 should always give us a page-aligned read 
 read a single page, without unlocking it. 
		
		  Uptodate inline data should have been added
		  into page cache while getting Fcr caps.
		
		  readahead callers do not necessarily hold Fcb caps
		  (e.g. fadvise, madvise).
  Get ref for the oldest snapc for an inode with dirty data... that is, the
  only snap context we are allowed to write back.
 get i_size, truncate_{seq,size} for page_snapc? 
  Write a single page, but leave the page locked.
  If we get a write error, mark the mapping for error, but still adjust the
  dirty page accounting (i.e., page is no longer dirty).
 verify this is a writeable snap context 
 we should only noop if called by kswapd 
 is this a partial page at end of file? 
 it may be a short write due to an object boundary 
 killed by SIGKILL 
 vfs expects us to return 0 
 page's reference 
		 direct memory reclaimer was killed by SIGKILL. return 0
  async writeback completion handler.
  If we get an error, set the mapping error bit, but not the individual
  page error bits.
	
	  We lost the cache cap, need to truncate the page before
	  it is unlocked, otherwise we'd truncate it later in the
	  page truncation thread, possibly losing some data that
	  raced its way in
 clean all pages 
  initiate async writeback
 we're in a forced umount, don't write! 
 find oldest snap context with dirty data 
		 hmm, why does writepages get called when there
 where to startend? 
		 Do not respect wbc->range_{start,end}. Dirty pages
		  in that range can be associated with newer snapc.
		  They are not writeable until we write all dirty pages
 first page 
 only dirty pages, or our accounting breaks 
 only if matching snap context 
			
			  We have something to write.  If this is
			  the first locked page this time through,
			  calculate max possinle write size and
			  allocate a page array
 prepare async write request 
 note position of first page in pvec 
 did we get anything? 
 shift unused page to beginning of pvec 
 Format the osd request message and submit the write 
			 writepages_finish() clears writeback pages
			  according to the data length, so make sure
 allocate new pages array for next request 
 request message now owns the pages array 
		
		  We stop writing back only if we are not doing
		  integrity sync. In case of integrity sync we have to
		  keep going until we have written all the pages
		  we tagged for writeback prior to entering this loop.
 more to do; loop back to beginning of file 
 OK even when start_index == 0 
		 to write dirty pages associated with next snapc,
 all dirty pages were checked 
  See if a given @snapc is either writeable, or already written.
  ceph_find_incompatible - find an incompatible context and return it
  @page: page being dirtied
  We are only allowed to write intodirty a page if the page is
  clean, or already dirty within the same snap context. Returns a
  conflicting context if there is one, NULL if there isn't, or a
  negative error code on other errors.
  Must be called with page lock held.
		
		  this page is already dirty in another (older) snap
		  context!  is it writeable now?
 not writeable -- return it for the caller to deal with 
 yay, writeable, do it now (without dropping page lock) 
  We are only allowed to write intodirty the page if the page is
  clean, or already dirty within the same snap context.
	
	  Uninlining should have already been done and everything updated, EXCEPT
	  for inline_version sent to the MDS.
		
		  The inline_version on a new inode is set to 1. If that's the
		  case, then the folio is brand new and isn't yet Uptodate.
  we don't do anything in here that simple_write_end doesn't do
  except adjust dirty page accounting
 just return that nothing was copied on a short copy 
 did file size increase? 
  vm ops
 read inline data 
 does not support inline data > PAGE_SIZE 
 Update time before taking page lock 
 success.  we'll keep the page locked. 
 initial version, no data 
 no inline data 
 one page should be large enough for STAT data 
 Only need to do this for regular files 
		
		  Pool permission check needs to write to the first object.
		  But for snapshot, head of the first object may have alread
		  been deleted. Skip check to avoid creating orphan object.
 SPDX-License-Identifier: GPL-2.0
  Ceph 'frag' type
 SPDX-License-Identifier: GPL-2.0
  Capability management
  The Ceph metadata servers control client access to inode metadata
  and file data by issuing capabilities, granting clients permission
  to read andor write both inode field and file data to OSDs
  (storage nodes).  Each capability consists of a set of bits
  indicating which operations are allowed.
  If the client holds a _SHARED cap, the client has a coherent value
  that can be safely read from the cached inode.
  In the case of a _EXCL (exclusive) or FILE_WR capabilities, the
  client is allowed to change inode attributes (e.g., file size,
  mtime), note its dirty state in the ceph_cap, and asynchronously
  flush that metadata change to the MDS.
  In the event of a conflicting operation (perhaps by another
  client), the MDS will revoke the conflicting client capabilities.
  In order for a client to cache an inode, it must hold a capability
  with at least one MDS server.  When inodes are released, release
  notifications are batched and periodically sent en masse to the MDS
  cluster to release server state.
  Generate readable cap strings for debugging output.
  Called under mdsc->mutex.
 first reserve any caps that are already allocated 
 temporary, until we do something about cap importexport 
	
	  Keep some preallocated caps around (ceph_min_count), to
	  avoid lots of freealloc churn.
  Find ceph_cap for given mds, if any.
  Called with i_ceph_lock held.
  Called under i_ceph_lock.
  (re)set cap hold timeouts, which control the delayed release
  of unused caps back to the MDS.  Should be called on cap use.
  (Re)queue cap at the end of the delayed cap release list.
  If I_FLUSH is set, leave the inode at the front of the list.
  Caller holds i_ceph_lock
     -> we take mdsc->cap_delay_lock
  Queue an inode for immediate writeback.  Mark inode with I_FLUSH,
  indicating we should send a cap message to flush dirty metadata
  asap, and move to the front of the delayed cap list.
  Cancel delayed work on cap.
  Caller must hold i_ceph_lock.
 Common issue checks for add_cap, handle_cap_grant. 
	
	  Each time we receive FILE_CACHE anew, we increment
	  i_rdcache_gen.
	
	  If FILE_SHARED is newly issued, mark dir not complete. We don't
	  know what happened to this directory while we didn't have the cap.
	  If FILE_SHARED is being revoked, also mark dir not complete. It
	  stops on-going cached readdir.
 Wipe saved layout if we're losing DIR_CREATE caps 
  change_auth_cap_ses - move inode to appropriate lists when auth caps change
  @ci: inode to be moved
  @session: new auth caps session
  Add a capability under the given MDS session.
  Caller should hold session snap_rwsem (read) and ci->i_ceph_lock
  @fmode is the open file mode, if we are opening a file, otherwise
  it is < 0.  (This is so we can atomically add the cap and add an
  open file reference to it.)
 add to session cap list 
		
		  auth mds of the inode changed. we received the cap export
		  message, but still haven't received the cap import message.
		  handle_cap_export() updated the new auth MDS' cap.
		 
		  "ceph_seq_cmp(seq, cap->seq) <= 0" means we are processing
		  a message that was send before the cap import message. So
		  don't remove caps.
		
		  add this inode to the appropriate snap realm
	
	  If we are issued caps we don't want, or the mds' wanted
	  value appears to be off, queue a check so we'll release
	  later andor update the mds wanted value.
  Return true if cap has not timed out and belongs to the current
  generation of the MDS session (i.e. has not gone 'stale' due to
  us losing touch with the mds).
  Return set of valid cap bits issued to us.  Note that caps time
  out, and may be invalidated in bulk if the client session times out
  and session->s_cap_gen is bumped.
	
	  exclude caps issued by non-auth MDS, but are been revoking
	  by the auth MDS. The non-auth MDS should be revokingexporting
	  these caps, but the message is delayed.
  Get cap bits issued by caps other than @ocap
  Move a cap to the end of the LRU (oldest caps at list head, newest
  at list tail).
  Check if we hold the given mask.  If so, move the cap(s) to the
  front of their respective LRUs.  (This is the preferred way for
  callers to check for caps they want.)
 does a combination of caps satisfy mask? 
 touch this + preceding caps 
  Return true if mask caps are currently being revoked by an MDS.
  wanted, by virtue of open file modes
 use used_cutoff here, to keep dir's wanted caps longer 
 check lazyio only when readwrite is wanted 
  wanted, by virtue of open file modes AND cap refs (bufferedcached data)
 we want EXCL if holding caps of dir ops 
 we want EXCL if dirty data 
  Return caps we have registered with the MDS(s) as 'wanted'.
  Remove a cap.  Take steps to deal with a racing iterate_session_caps.
  caller should hold i_ceph_lock.
  caller will not hold session s_mutex if called from destroy_inode.
 'ci' being NULL means the remove have already occurred 
 remove from inode's cap rbtree, and clear auth cap 
 remove from session list 
 not yet, we are iterating over this very cap 
 protect backpointer with s_cap_lock: see iterate_session_caps 
	
	  s_cap_reconnect is protected by s_cap_lock. no one changes
	  s_cap_gen while session is in the reconnect state.
		 when reconnect denied, we remove session caps forcibly,
		  i_wr_ref can be non-zero. If there are ongoing write,
		  keep i_snap_realm.
 'ci' being NULL means the remove have already occurred 
  cap struct size + flock buffer size + inline version + inline data size +
  osd_epoch_barrier + oldest_flush_tid
 Marshal up the cap msg to the MDS 
 flock buffer size (version 2) 
 inline version (version 4) 
 inline data size 
	
	  osd_epoch_barrier (version 5)
	  The epoch_barrier is protected osdc->lock, so READ_ONCE here in
	  case it was recently changed
 oldest_flush_tid (version 6) 
	
	  caller_uidcaller_gid (version 7)
	 
	  Currently, we don't properly track which caller dirtied the caps
	  last, and force a flush of them when there is a conflict. For now,
	  just set this to 0:0, to emulate how the MDS has worked up to now.
 pool namespace (version 8) (mds always ignores this) 
 btime and change_attr (version 9) 
 Advisory flags (version 10) 
  Queue cap releases when an inode is dropped from our cache.
	 lock i_ceph_lock, because ceph_d_revalidate(..., LOOKUP_RCU)
  Prepare to send a cap message to an MDS. Update the cap state, and populate
  the arg struct with the parameters that will need to be sent. This should
  be done under the i_ceph_lock to guard against changes to cap state.
  Make note of max_size reportedrequested from mds, revoked caps
  that have now been implemented.
 drop bits we don't want 
	
	  Wake up any waiters on wanted -> needed transition. This is due to
	  the weird transition from buffered to sync IO... we need to flush
	  dirty pages _before_ allowing sync writes to avoid reordering.
  Send a cap msg on the given inode.
  Caller should hold snap_rwsem (read), s_mutex.
  When a snapshot is taken, clients accumulate dirty metadata on
  inodes with capabilities in ceph_cap_snaps to describe the file
  state at the time the snapshot was taken.  This must be flushed
  asynchronously back to the MDS once sync writes complete and dirty
  data is written out.
  Called under i_ceph_lock.
		
		  we need to wait for sync writes to complete and for dirty
		  pages to be written out.
 should be removed by ceph_try_drop_cap_snap() 
 only flush each capsnap once 
 make sure flushsnap messages are sent in proper order.
 we flushed them all; remove this inode from the queue 
  Mark caps dirty.  If inode is newly dirty, return the dirty flags.
  Caller is then responsible for calling __mark_inode_dirty with the
  returned flags value.
  Remove cap_flush from the mdsc's or inode's flushing cap list.
  Return true if caller needs to wake up flush waiters.
  Add dirty inode to the flushing list.  Assigned a seq number so we
  can wait for caps to flush without starving.
  Called under i_ceph_lock. Returns the flush tid.
  try to invalidate mapping pages without blocking.
 success. 
 save any racing async invalidate some trouble 
 mds will adjust max size according to the reported size 
 half of previous max_size increment has been used 
  Swiss army knife function to examine currently used and wanted
  versus held caps.  Release, flush, ack revoked caps to mds as
  appropriate.
   CHECK_CAPS_AUTHONLY - we should only check the auth cap
   CHECK_CAPS_FLUSH - we should flush any dirty caps immediately, without
     further delay.
	int mds = -1;    keep track of how far we've gone through i_caps list
 Caps wanted by virtue of active open files. 
 Caps which have active references against them 
	
	  "issued" represents the current caps that the MDS wants us to have.
	  "implemented" is the set that we have been granted, and includes the
	  ones that have not yet been returned to the MDS (the "revoking" set,
	  usually because they have outstanding references).
 The ones we currently want to retain (may be adjusted below) 
 be greedy 
			
			  If a directory is complete, we want to keep
			  the exclusive cap. So that MDS does not end up
			  revoking the shared cap on every createunlink
			  operation.
			
			  keep RD only if we didn't have the file open RW,
			  because then the mds would revoke it anyway to
			  journal max_size=0.
	
	  If we no longer need to hold onto old our caps, and we may
	  have cached pages, but don't want them, then try to invalidate.
	  If we fail, it's because pages are locked.... try again later.
 no dirty pages... 
 have cached pages 
  or revoking cache 
 avoid looping forever 
		
		  If we have an auth cap, we don't need to consider any
		  overlapping caps as used.
 request larger max_size from MDS? 
 approaching file_max? 
 flush anything dirty? 
 completed revocation? going down and there are no caps? 
 want more caps from mds? 
 things we might delay 
 nope, all good 
		 kick flushing and flush snaps before sending normal
 remember mds, so we don't repeat 
 retake i_ceph_lock and restart our cap scan. 
 periodically re-calculate caps wanted by open files 
  Try to flush dirty caps back to the auth mds.
  Return true if we've flushed caps through the given flush_tid.
  wait for any unsafe requests to complete.
	
	  Trigger to flush the journal logs in all the relevant MDSes
	  manually, or in the worst case we must wait at most 5 seconds
	  to wait the journal logs to be flushed by the MDSes periodically.
		
		  The mdsc->max_sessions is unlikely to be changed
		  mostly, here we will retry it by reallocating the
		  sessions arrary memory to get rid of the mdsc->mutex
		  lock.
 the auth MDS 
 send flush mdlog request to MDSes 
	
	  only wait on non-file metadata writeback (the mds
	  can recover size and mtime, so we don't need to
	  wait for that)
  Flush any dirty caps back to the mds.  If we aren't asked to wait,
  queue inode for flush but don't do so immediately, because we can
  get by with fewer MDS messages if we wait for data writeback to
  complete first.
		
		  if flushing caps were revoked, we re-send the cap flush
		  in client reconnect stage. This guarantees MDS  processes
		  the cap flush message before issuing the flushing caps to
		  other client.
			 encode_caps_cb() also will reset these sequence
			  numbers. make sure sequence numbers in cap flush
  Take references to capabilities we hold, so that we don't release
  them to the MDS prematurely.
  Try to grab cap references.  Specify those refs we @want, and the
  minimal set we @need.  Also include the larger offset we are writing
  to (when applicable), and check against max_size here as well.
  Note that caller is responsible for ensuring max_size increases are
  requested from the MDS.
  Returns 0 if caps were not able to be acquired (yet), 1 if succeed,
  or a negative error code. There are 3 speical error codes:
   -EAGAIN:  need to sleep but non-blocking is specified
   -EFBIG:   ask caller to call check_max_size() and try again.
   -EUCLEAN: ask caller to call ceph_renew_caps() and try again.
 first 8 bits are reserved for CEPH_FILE_MODE_FOO 
 finish pending truncate 
		
		  If a sync write is in progress, we must wait, so that we
		  can get a final snapshot value for size+mtime.
		
		  Look at (implemented & ~have & not) so that we keep waiting
		  on transition from wanted -> needed caps.  This is needed
		  for WRBUFFER|WR -> WR to avoid a new WR sync write from
		  going before a prior buffered writeback happens.
					
					  we can not call down_read() when
					  task isn't in TASK_RUNNING state
  Check the offset we are writing up to against our current
  max_size.  If necessary, tell the MDS we want to write to
  a larger offset.
 do we need to explicitly request a larger max_size? 
 duplicate ceph_check_caps()'s logic 
 three special error codes 
  Wait for caps, and take cap references.  If we can't get a WR cap
  due to a small max_size, make sure we check_max_size (and possibly
  ask the mds) so we don't get hung up indefinitely.
 make sure used fmode not timeout 
 session was killed, try renew caps 
			
			  drop cap refs first because getattr while
			  holding  caps refs can cause deadlock.
			
			  getattr request will bring inline data into
			  page cache
  Take cap refs.  Caller must already know we hold at least one ref
  on the caps in question or we don't know this is safe.
  drop cap_snap that is not associated with any snapshot.
  we don't need to send FLUSHSNAP message for it.
  Release cap refs.
  If we released the last ref on any given cap, call ceph_check_caps
  to release (or schedule a release).
  If we are releasing a WR cap (from a sync write), finalize any affected
  cap_snap, and wake up any waiters.
 put the ref held by ceph_take_cap_refs() 
 see comment in __ceph_remove_cap() 
 put the ref held by ceph_queue_cap_snap() 
  Release @nr WRBUFFER refs on dirty pages for the given @snapc snap
  context.  Adjust per-snap dirty page accounting as appropriate.
  Once all dirty data for a cap_snap is flushed, flush snapped file
  metadata back to the MDS.  If we dropped the last ref, call
  ceph_check_caps.
			
			  The capsnap should already be removed when removing
			  auth cap in the case of a forced unmount.
  Invalidate unlinked inode's aliases, so we can drop the inode ASAP.
	
	  For non-directory inode, d_find_alias() only returns
	  hashed dentry. After calling d_invalidate(), the
	  dentry becomes unhashed.
	 
	  For directory inode, d_find_alias() can return
	  unhashed dentry. But directory inode should have
	  one alias at most.
 inline data 
 dirstat 
 currently issued 
  Handle a cap GRANT message from the MDS.  (Note that a GRANT may
  actually be a revocation if it specifies a smaller cap set.)
  caller holds s_mutex and i_ceph_lock, we drop both.
	
	  If CACHE is being revoked, and we have no dirty buffers,
	  try to invalidate (once).  (If there are dirty buffers, we
	  will invalidate _after_ writeback.)
 don't invalidate readdir cache 
			 there were locked pages.. invalidate later
	
	  auth mds of the inode changed. we received the cap export message,
	  but still haven't received the cap import message. handle_cap_export
	  updated the new auth MDS' cap.
	 
	  "ceph_seq_cmp(seq, cap->seq) <= 0" means we are processing a message
	  that was sent before the cap import message. So don't remove caps.
 side effects now are allowed 
 ctimemtimeatime? 
 file layout may have changed 
 sizetruncate_seq? 
 reset 
 check cap bits 
		
		  If mds is importing cap, prior cap messages that update
		  'wanted' may get dropped by mds (migrate seq mismatch).
		 
		  We don't send cap message to update 'wanted' if what we
		  want are already issued. If mds revokes caps, cap message
		  that releases caps also tells mds what we want. But if
		  caps got revoked by mds forcedly (session stale). We may
		  haven't told mds what we want.
 revocation, grant, or no-op? 
 initiate writeback; will delay ack 
 do nothing yet, invalidation will be queued 
 check auth cap only 
 check all caps 
 non-auth MDS is revoking the newly grant caps ? 
		cap->implemented |= newcaps;  add bits only, to
					       avoid stepping on a
 re-request max_size if necessary 
		
		  queue inode for writeback: we can't actually call
		  filemap_write_and_wait, etc. from message handler
		  context.
  Handle FLUSH_ACK from MDS, indicating that metadata we sent to the
  MDS has been safely committed.
 Is this the one that was flushed? 
 Is this a capsnap? 
			
			  An earlier or current tid. The FLUSH_ACK should
			  represent a superset of this flush's caps.
			
			  This is a later one. Any caps in it are still dirty
			  so don't count them as cleaned.
  Handle FLUSHSNAP_ACK.  MDS has flushed snap data to disk and we can
  throw away our cap_snap.
  Caller hold s_mutex.
  Handle TRUNC from MDS, indicating file truncation.
  caller hold s_mutex.
  Handle EXPORT from MDS.  Cap is being migrated _from_ this mds to a
  different one.  If we are the most recent migration we've seen (as
  indicated by mseq), make note of the migrating cap bits for the
  duration (until we see the corresponding IMPORT).
  caller holds s_mutex
	
	  now we know we haven't received the cap import message yet
	  because the exported cap still exist.
 already have caps from the target 
 add placeholder for the export tagert 
 open target session 
  Handle cap IMPORT.
  caller holds s_mutex. acquires i_ceph_lock
  Handle a caps message from the MDS.
  Identify the appropriate session, inode, and call the right handler
  based on the cap op.
 decode 
 recorded in unused fields 
 version >= 6 
 flush_tid
 version >= 7 
 caller_uid
 caller_gid
 version >= 8 
 version >= 10 
 flags
 version >= 11 
 lookup ino 
 these will work even if we don't have a cap yet 
 the rest require a cap 
 note that each of these drops i_ceph_lock for us 
	
	  send any cap release message to try to move things
	  along for the mds (who clearly thinks we still have this
	  cap).
  Delayed work handler to process end of delayed cap release LRU list.
  If new caps are added to the list while processing it, these won't get
  processed in this run.  In this case, the ci->i_hold_caps_max will be
  returned so that the work can be scheduled accordingly.
  Flush all dirty caps to the mds
 queue periodic check 
		
		  If any of the mode ref is larger than 1,
		  that means it has been already opened by
		  others. Just skip checking the PIN ref.
  Drop open file reference.  If we were the last open file,
  we may need to release capabilities to the MDS (or schedule
  their delayed release).
		
		  If any of the mode ref is not 0 after
		  decreased, that means it is still opened
		  by others. Just skip checking the PIN ref.
  For a soon-to-be unlinked file, drop the LINK caps. If it
  looks like the link count will hit 0, drop any other caps (other
  than PIN) we don't specifically want (due to the file still being
  open).
  Helpers for embedding cap and dentry lease releases into mds
  requests.
  @force is used by dentry_release (below) to force inclusion of a
  record for the directory inode, even when there aren't any caps to
  drop.
 only drop unused, clean caps 
	
	  force an record for the directory caps if we have a dentry lease.
	  this is racy (can't take i_ceph_lock and d_lock together), but it
	  doesn't have to be perfect; the mds will revoke anything we don't
	  release.
 trash all of the cap flushes for this inode 
 make further file lock syscall return -EIO 
 SPDX-License-Identifier: GPL-2.0
  QNX4 file system, Linux implementation.
  Version : 0.2.1
  Using parts of the xiafs filesystem.
  History :
  01-06-1998 by Richard Frowijn : first release.
  21-06-1998 by Frank Denis : dcache support, fixed error codes.
  04-07-1998 by Frank Denis : first step for rmdirunlink.
  check if the filename is correct. For some obscure reason, qnx writes a
  new file twice in the directory entry, first with all possible options at 0
  and for a second time the way it is, they want us not to access the qnx
  filesystem when whe are using linux.
 The entry is linked, let's get the real info 
 SPDX-License-Identifier: GPL-2.0-only
  QNX4 file system, Linux implementation.
  Version : 0.2.1
  Using parts of the xiafs filesystem.
  History :
  01-06-1998 by Richard Frowijn : first release.
  20-06-1998 by Frank Denis : Linux 2.1.99+ support, boot signature, misc.
  30-06-1998 by Frank Denis : first step to write inodes.
 logical block is before EOF
 iblock is in the first extent. This is easy.
 iblock is beyond first extent. We have to follow the extent chain.
 read next xtnt block.
 got it!
  Check the root directory of the filesystem to make sure
  it really _is_ a qnx4 filesystem, and to check the size
  of the directory entry.
 root dir, first block 
 keep bitmap inode known 
 Yup, read-only yet 
	 Check the superblock signature. Since the qnx4 code is
	   dangerous, we should leave as quickly as possible
 check before allocating dentries, inodes, .. 
 does root not have inode number QNX4_ROOT_INO ?? 
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 SPDX-License-Identifier: GPL-2.0
  QNX4 file system, Linux implementation.
  Version : 0.2.1
  Using parts of the xiafs filesystem.
  History :
  28-05-1998 by Richard Frowijn : first release.
  20-06-1998 by Frank Denis : Linux 2.1.99+ & dcache support.
  A qnx4 directory entry is an inode entry or link info
  depending on the status field in the last byte. The
  first byte is where the name start either way, and a
  zero means it's empty.
  Also, due to a bug in gcc, we don't want to use the
  real (differently sized) name arrays in the inode and
  link entries, but always the 'de_name[]' one in the
  fake struct entry.
  See
    https:gcc.gnu.orgbugzillashow_bug.cgi?id=99578#c6
  for details, but basically gcc will take the size of the
  'name' array from one of the used union entries randomly.
  This use of 'de_name[]' (48 bytes) avoids the false positive
  warnings that would happen if gcc decides to use 'inode.di_name'
  (16 bytes) even when the pointer and size were to come from
  'link.dl_name' (48 bytes).
  In all cases the actual name pointer itself is the same, it's
  only the gcc internal 'what is the size of this field' logic
  that can get confused.
 SPDX-License-Identifier: GPL-2.0
  QNX4 file system, Linux implementation.
  Version : 0.2.1
  Using parts of the xiafs filesystem.
  History :
  28-05-1998 by Richard Frowijn : first release.
  20-06-1998 by Frank Denis : basic optimisations.
  25-06-1998 by Frank Denis : qnx4_is_free, qnx4_set_bitmap, qnx4_bmap .
  28-06-1998 by Frank Denis : qnx4_free_inode (to be fixed) .
 SPDX-License-Identifier: GPL-2.0-or-later
 FS-Cache object state machine handler
  Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  See Documentationfilesystemscachingobject.rst for a description of the
  object state machine and the in-kernel representations.
  Define a work state.  Work states are execution states.  No event processing
  is performed by them.  The function attached to a work state returns a
  pointer indicating the next state to which the state machine should
  transition.  Returning NO_TRANSIT repeats the current state, but goes back
  to the scheduler first.
  Returns from work states.
  Define a wait state.  Wait states are event processing states.  No execution
  is performed by them.  Wait states are just tables of "if event X occurs,
  clear it and transition to state Y".  The dispatcher returns to the
  scheduler if none of the events in which the wait state has an interest are
  currently pending.
  The object state machine.
  Out-of-band event transition tables.  These are for handling unexpected
  events, such as an IO error.  If an OOB event occurs, the state machine
  clears and disables the event and forces a transition to the nominated work
  state (acurrently executing work states will complete first).
  In such a situation, object->state remembers the state the machine should
  have been ingone to and returning NO_TRANSIT returns to that.
  we need to notify the parent when an op completes that we had outstanding
  upon it
  Object state machine dispatcher.
 Mask normal event handling 
 Handle any out-of-band events (typically an error) 
 Wait states are just transition tables 
 The event mask didn't include all the tabled bits 
 Randomly woke up 
 Transited to wait state 
  execute an object
  fscache_object_init - Initialise a cache object description
  @object: Object description
  @cookie: Cookie object will be attached to
  @cache: Cache in which backing object will be found
  Initialise a cache object description to its basic values.
  See Documentationfilesystemscachingbackend-api.rst for a complete
  description.
  Mark the object as no longer being live, making sure that we synchronise
  against op submission.
  Abort object initialisation before we start it.
  initialise an object
  - check the specified object's parent to see if we can make use of it
    immediately to do a creation
  - we may need to start the process of creating a parent and we need to wait
    for the parent's lookup and creation to complete if it's not there yet
	 fscache_acquire_non_index_cookie() uses this
  Once the parent object is ready, we should kick off our lookup op.
  look an object up in the cache from which it was allocated
  - we hold an "access lock" on the parent object, so the parent object cannot
    be withdrawn by either party till we've finished
 make sure the parent is still available 
		 probably stuck behind another object, so move this one to
  fscache_object_lookup_negative - Note negative cookie lookup
  @object: Object pointing to cookie to mark
  Note negative lookup, permitting those waiting to read data from an already
  existing backing object to continue as there's no data for them to read.
		 Allow write requests to begin stacking up and read requests to begin
		  returning ENODATA.
  fscache_obtained_object - Note successful object lookup or creation
  @object: Object pointing to cookie to mark
  Note successful lookup andor creation, permitting those waiting to write
  data to a backing object to continue.
  Note that after calling this, an object's cookie may be relinquished by the
  netfs, and so must be accessed with object lock held.
	 if we were still looking up, then we must have a positive lookup
 We do (presumably) have data 
		 Allow write requests to begin stacking up and read requests
		  to begin shovelling data.
  handle an object that has just become available
  Wake up this object's dependent objects now that we've become available.
 Not finished; requeue 
  Handle lookup or creation failute.
  Wait for completion of all active operations on this object and the death of
  all child objects of this object.
 Reject any new readwrite ops and abort any that are pending. 
  Kill dependent objects.
 Not finished 
  Drop an object's attachments
	 Make sure the cookie no longer points here and that the netfs isn't
	  waiting for us.
	 Prevent a race with our last child, which has to signal EV_CLEARED
	  before dropping our spinlock.
 Discard from the cache's collection of objects 
 The parent object wants to know when all it dependents have gone 
 this just shifts the object release to the work processor 
  get a ref on an object
  Discard a ref on an object
  fscache_object_destroy - Note that a cache object is about to be destroyed
  @object: The object to be destroyed
  Note the imminent destruction and deallocation of a cache object record.
 We can get rid of the cookie now 
  enqueue an object for metadata-type processing
  fscache_object_sleep_till_congested - Sleep until object wq is congested
  @timeoutp: Scheduler sleep timeout
  Allow an object handler to sleep until the object workqueue is congested.
  The caller must set up a wake up event before calling this and must have set
  the appropriate sleep mode (such as TASK_UNINTERRUPTIBLE) and tested its own
  condition before calling this function as no test is made here.
  %true is returned if the object wq is congested, %false otherwise.
  Enqueue the dependents of an object for metadata-type processing.
  If we don't manage to finish the list before the scheduler wants to run
  again then return false immediately.  We return true if the list was
  cleared.
  remove an object from whatever queue it's waiting on
  fscache_check_aux - Ask the netfs whether an object on disk is still valid
  @object: The object to ask about
  @data: The auxiliary data for the object
  @datalen: The size of the auxiliary data
  @object_size: The size of the object according to the server.
  This function consults the netfs about the coherency state of an object.
  The caller must be holding a ref on cookie->n_active (held by
  fscache_look_up_object() on behalf of the cache backend during object lookup
  and creation).
 entry okay as is 
 entry requires update 
 entry requires deletion 
  Asynchronously invalidate an object.
	 We're going to need the cookie.  If the cookie is not available then
	  retire the object instead.
 Reject any new readwrite ops and abort any that are pending. 
 Now we have to wait for in-progress reads and writes 
	 Once we've completed the invalidation, we know there will be no data
	  stored in the cache and thus we can reinstate the data-check-skip
	  optimisation.
	 We can allow read and write requests to come in once again.  They'll
	  queue up behind our exclusive invalidation operation.
  Update auxiliary data.
  Asynchronously update an object.
  fscache_object_retrying_stale - Note retrying stale object
  @object: The object that will be retried
  Note that an object lookup found an on-disk object that was adjudged to be
  stale and has been deleted.  The lookup will be retried.
  fscache_object_mark_killed - Note that an object was killed
  @object: The object that was culled
  @why: The reason the object was killed.
  Note that an object was killed.  Returns true if the object was
  already marked killed, false if it wasn't.
  The object is dead.  We can get here if an object gets queued by an event
  that would lead to its death (such as EV_KILL) when the dispatcher is
  already running (and so can be requeued) but hasn't yet cleared the event
  mask.
 SPDX-License-Identifier: GPL-2.0-or-later
 Cache data IO routines
  Copyright (C) 2021 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  Start a cache read operation.
  - we return:
    -ENOMEM	- out of memory, some pages may be being read
    -ERESTARTSYS - interrupted, some pages may be being read
    -ENOBUFS	- no backing object or space available in which to cache any
                 pages not being read
    -ENODATA	- no data available in the backing object for some or all of
                 the pages
    0		- dispatched a read on all pages
	 we wait for the operation to become active, and then process it
 ask the cache to honour the operation 
 SPDX-License-Identifier: GPL-2.0-or-later
 FS-Cache netfs (client) registration
  Copyright (C) 2008 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  register a network filesystem for caching
 allocate a cookie for the primary index 
 check the netfs type is not already present 
  unregister a network filesystem from the cache
  - all cookies must have been released first
 SPDX-License-Identifier: GPL-2.0-or-later
 FS-Cache worker operation management routines
  Copyright (C) 2008 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  See Documentationfilesystemscachingoperations.rst
  fscache_operation_init - Do basic initialisation of an operation
  @cookie: The cookie to operate on
  @op: The operation to initialise
  @processor: The function to perform the operation
  @cancel: A function to handle operation cancellation
  @release: The release function to assign
  Do basic initialisation of an operation.  The caller must still set flags,
  object and processor if needed.
  fscache_enqueue_operation - Enqueue an operation for processing
  @op: The operation to enqueue
  Enqueue an operation for processing by the FS-Cache thread pool.
  This will get its own ref on the object.
  start an op running
  report an unexpected submission
  submit an exclusive operation for an object
  - other ops are excluded from running simultaneously with this one
  - this gets any extra refs it needs on an op
 reads and writes must wait 
 need to issue a new write op after this 
 reads and writes must wait 
  submit an operation for an object
  - objects may be submitted only in the following states:
    - during object creation (write ops may be submitted)
    - whilst the object is active
    - after an IO error incurred in one of the two above states (op rejected)
  - this gets any extra refs it needs on an op
  queue an object for withdrawal on error, aborting all following asynchronous
  operations
  Jump start the operation processing on an object.  The caller must hold
  object->lock.
 the pending queue was holding a ref on the object 
  cancel an operation that's pending on an object
  Cancel all pending operations on an object
  Record the completion or cancellation of an in-progress operation.
  release an operation
  - queues pending ops if this is the last in-progress op
		 now... we may get called with the object spinlock held, so we
		  complete the cleanup here only if we can immediately acquire the
  garbage collect operations that have had their release deferred
  execute an operation using fs_op_wq to provide processing context -
  the caller holds a ref to this object, so we don't need to hold one
 SPDX-License-Identifier: GPL-2.0-or-later
 Cache page management and data IO routines
  Copyright (C) 2004-2008 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  check to see if a page is being written to the cache
  wait for a page to finish being written to the cache
  wait for a page to finish being written to the cache. Put a timeout here
  since we might be called recursively via parent fs.
  decide whether a page can be released, possibly by cancelling a store to it
  - we're allowed to sleep if __GFP_DIRECT_RECLAIM is flagged
	 see if the page is actually undergoing storage - if so we can't get
	 the page is pending storage, so we attempt to cancel the store and
		 the page started to undergo storage whilst we were looking,
	 We will wait here if we're allowed to, but that could deadlock the
	  allocator as the work threads writing to the cache may all end up
	  sleeping on memory allocation, so we may need to impose a timeout
  note that a page has finished being written to the cache
		 delete the page from the tree if it is now no longer
  actually apply the changed attributes to a cache object
  notification that the attributes on an object have changed
  Handle cancellation of a pending retrieval op
  release a retrieval op reference
  allocate a retrieval op
 allocate a retrieval operation and attempt to submit it 
	 Pin the netfs read context in case we need to do the actual netfs
	  read because we've encountered a cache read failure.
  wait for a deferred lookup to complete
  wait for an object to become active (or dead)
		 it's been removed from the pending queue by another party,
  read a page from the cache or allocate a block in which to store it
  - we return:
    -ENOMEM	- out of memory, nothing done
    -ERESTARTSYS - interrupted
    -ENOBUFS	- no backing object available in which to cache the block
    -ENODATA	- no data available in the backing object for this block
    0		- dispatched a read - it'll call end_io_func() when finished
	 we wait for the operation to become active, and then process it
 ask the cache to honour the operation 
  read a list of page from the cache or allocate a block in which to store
  them
  - we return:
    -ENOMEM	- out of memory, some pages may be being read
    -ERESTARTSYS - interrupted, some pages may be being read
    -ENOBUFS	- no backing object or space available in which to cache any
                 pages not being read
    -ENODATA	- no data available in the backing object for some or all of
                 the pages
    0		- dispatched a read on all pages
  end_io_func() will be called for each page read from the cache as it is
  finishes being read
  any pages for which a read is dispatched will be removed from pages and
  nr_pages
	 we wait for the operation to become active, and then process it
 ask the cache to honour the operation 
  allocate a block in the cache on which to store a page
  - we return:
    -ENOMEM	- out of memory, nothing done
    -ERESTARTSYS - interrupted
    -ENOBUFS	- no backing object available in which to cache the block
    0		- block allocated
 ask the cache to honour the operation 
  Unmark pages allocate in the readahead code path (via:
  fscache_readpages_or_alloc) after delegating to the base filesystem
  release a write op reference
  perform the background storage of a page into the cache
		 If we get here, then the on-disk cache object likely no
		  longer exists, so we should just cancel this write
		  operation.
		 If we get here, then the cookie belonging to the object was
		  detached, probably by the cookie being withdrawn due to
		  memory pressure, which means that the pages we might write
		  to the cache from no longer exist - therefore, we can just
		  cancel this write operation.
 find a page to store 
	 this writer is going away and there aren't any more things to
  Clear the pages pending writing for invalidation
  request a page be stored in the cache
  - returns:
    -ENOMEM	- out of memory, nothing done
    -ENOBUFS	- no backing object available in which to cache the page
    0		- dispatched a write - it'll call end_io_func() when finished
  if the cookie still has a backing object at this point, that object can be
  in one of a few states with respect to storage processing:
   (1) negative lookup, object not yet created (FSCACHE_COOKIE_CREATING is
       set)
 	(a) no writes yet
 	(b) writes deferred till post-creation (mark page for writing and
 	    return immediately)
   (2) negative lookup, object created, initial fill being made from netfs
 	(a) fill point not yet reached this page (mark page for writing and
           return)
 	(b) fill point passed this page (queue op to store this page)
   (3) object extant (queue op to store this page)
  any other state is invalid
	 add the page to the pending-storage radix tree on the backing
	 we only want one writer at a time, but we do need to queue new
 the work queue now carries its own ref on the object 
  remove a page from the cache
 cache withdrawal may beat us to it 
 get the object 
 there might now be stuff on disk we could read 
	 only invoke the cache backend if we managed to mark the page
 the cache backend releases the cookie lock 
  fscache_mark_page_cached - Mark a page as being cached
  @op: The retrieval op pages are being marked for
  @page: The page to be marked
  Mark a netfs page as being cached.  After this is called, the netfs
  must call fscache_uncache_page() to remove the mark.
  fscache_mark_pages_cached - Mark pages as being cached
  @op: The retrieval op pages are being marked for
  @pagevec: The pages to be marked
  Mark a bunch of netfs pages as being cached.  After this is called,
  the netfs must call fscache_uncache_page() to remove the mark.
  Uncache all the pages in an inode that are marked PG_fscache, assuming them
  to be associated with the given cookie.
 SPDX-License-Identifier: GPL-2.0-or-later
 netfs cookie management
  Copyright (C) 2004-2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  See Documentationfilesystemscachingnetfs-api.rst for more information on
  the netfs API.
  Set the index key in a cookie.  The cookie struct has space for a 16-byte
  key plus length and hash, but if that's not big enough, it's instead a
  pointer to a buffer containing 3 bytes of hash, 1 byte of length and then
  the key data.
  Allocate a cookie.
 allocate and initialise a cookie 
	 We keep the active count elevated until relinquishment to prevent an
	  attempt to wake up every time the object operations queue quiesces.
	 radix tree insertion won't use the preallocation pool unless it's
  Attempt to insert the new cookie into the hash.  If there's a collision, we
  return the old cookie if it's not in use and an error otherwise.
  request a cookie to represent an object (index, datafile, xattr, etc)
  - parent specifies the parent object
    - the top level index cookie for each netfs is stored in the fscache_netfs
      struct upon registration
  - def points to the definition
  - the netfs_data will be passed to the functions pointed to in def
  - all attached caches will be searched to see if they contain this object
  - index objects aren't stored on disk until there's a dependent file that
    needs storing
  - other objects are stored in a selected cache immediately, and all the
    indices forming the path to it are instantiated if necessary
  - we never let on to the netfs about errors
    - we may set a negative cookie pointer, but that's okay
 if there's no parent cookie, then we don't create one here either 
 validate the definition 
		 if the object is an index then we need do nothing more here
		  - we create indices on disk when we need them as an index
  Enable a cookie to permit it to accept new operations.
 The netfs decided it didn't want to enable after all 
 Wait for outstanding disablement to complete 
  acquire a non-index cookie
  - this must make sure the index chain is instantiated and instantiate the
    object representation too
	 now we need to see whether the backing objects for this cookie yet
 select a cache in which to store the object 
	 ask the cache to allocate objects for this cookie and its parent
	 initiate the process of looking up all the objects in the chain
 we may be required to wait for lookup to complete at this point 
  recursively allocate cache object records for a cookiecache combination
  - caller must be holding the addremove sem
	 ask the cache to allocate an object (we may end up with duplicate
	 only attach if we managed to allocate all we needed, otherwise
	  discard the object we just allocated and instead use the one
  attach a cache object to a cookie
	 there may be multiple initial creations of this object, but we only
 pin the parent object 
 attach to the cache's object list 
 Attach to the cookie.  The object already has a ref on it. 
  Invalidate an object.  Callable with spinlocks held.
	 Only permit invalidation of data files.  Invalidating an index will
	  require the caller to release all its attachments to the tree rooted
	  there, and if it's doing that, it may as well just retire the
	  cookie.
	 If there's an object, we tell the object state machine to handle the
	  invalidation on our behalf, otherwise there's nothing to do.
  Wait for object invalidation to complete.
  update the index entries backing a cookie
		 update the index entry on disk in each cache backing this
		  cookie.
  Disable a cookie to stop it from accepting new requests from the netfs.
	 If the cookie is being invalidated, wait for that to complete first
	  so that we can reuse the flag.
 Dispose of the backing objects 
	 Wait for cessation of activity requiring access to the netfs (when
	  n_active reaches 0).  This makes sure outstanding reads and writes
	  have completed.
 Make sure any pending writes are cancelled. 
 Reset the cookie state if it wasn't relinquished 
  release a cookie back to the cache
  - the object will be marked as recyclable on disk if retire is true
  - all dependents of this cookie must have already been unregistered
    (indicesfilespages)
 No further netfs-accessing operations on this cookie permitted 
 Clear pointers back to the netfs 
 Dispose of the netfs's link to the cookie 
  Remove a cookie from the hash table.
  Drop a reference to a cookie.
  Get a reference to a cookie.
  check the consistency between the netfs inode and the backing cache
  NOTE: it only serves no-index type
 the work queue now carries its own ref on the object 
 ask the cache to honour the operation 
  Generate a list of extant cookies in procfsfscachecookies
 SPDX-License-Identifier: GPL-2.0-or-later
 FS-Cache cache handling
  Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  look up a cache tag
 firstly check for the existence of the tag under read lock 
 the tag does not exist - create a candidate 
 return a dummy tag if out of memory 
 write lock, search again and add if still not present 
  release a reference to a cache tag
  select a cache in which to store an object
  - the cache addremove semaphore must be at least read-locked by the caller
  - the object will never be an index
 we check the parent to determine the cache to use 
	 the first in the parent's backing list should be the preferred
 the parent is unbacked 
 cookie not an index and is unbacked 
 ask the netfs for its preference 
 netfs has no preference - just select first cache 
  fscache_init_cache - Initialise a cache record
  @cache: The cache record to be initialised
  @ops: The cache operations to be installed in that record
  @idfmt: Format string to define identifier
  @...: sprintf-style arguments
  Initialise a record of a cache and fill in the name.
  See Documentationfilesystemscachingbackend-api.rst for a complete
  description.
  fscache_add_cache - Declare a cache as being open for business
  @cache: The record describing the cache
  @ifsdef: The record of the cache object describing the top-level index
  @tagname: The tag describing this cache
  Add a cache to the system, making it available for netfs's to use.
  See Documentationfilesystemscachingbackend-api.rst for a complete
  description.
 we use the cache tag to uniquely identify caches 
 add the cache to the list 
	 add the cache's netfs definition index object to the cache's
	 add the cache's netfs definition index object to the top level index
 done 
  fscache_io_error - Note a cache IO error
  @cache: The record describing the cache
  Note that an IO error occurred in a cache and that it should no longer be
  used for anything.  This also reports the error into the kernel log.
  See Documentationfilesystemscachingbackend-api.rst for a complete
  description.
  request withdrawal of all the objects in a cache
  - all the objects being withdrawn are moved onto the supplied list
			 This must be done under object_list_lock to prevent
			  a race with fscache_drop_object().
  fscache_withdraw_cache - Withdraw a cache from the active service
  @cache: The record describing the cache
  Withdraw a cache from service, unbinding all its cache objects from the
  netfs cookies they're currently representing.
  See Documentationfilesystemscachingbackend-api.rst for a complete
  description.
 make the cache unavailable for cookie acquisition 
	 make sure all pages pinned by operations on behalf of the netfs are
	 dissociate all the netfs pages backed by this cache from the block
	 we now have to destroy all the active objects pertaining to this
	  cache - which we do by passing them off to thread pool to be
	 wait for all extant objects to finish their outstanding operations
 SPDX-License-Identifier: GPL-2.0-or-later
 FS-Cache statistics
  Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  operation counters
  display the general statistics
 SPDX-License-Identifier: GPL-2.0-or-later
 FS-Cache statistics viewing interface
  Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  initialise the procfsfscache directory
  clean up the procfsfscache directory
 SPDX-License-Identifier: GPL-2.0-or-later
 Filesystem index definition
  Copyright (C) 2004-2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
  The root index is owned by FS-Cache itself.
  When a netfs requests caching facilities, FS-Cache will, if one doesn't
  already exist, create an entry in the root index with the key being the name
  of the netfs ("AFS" for example), and the auxiliary data holding the index
  structure version supplied by the netfs:
 				     FSDEF
 				       |
 				 +-----------+
 				 |           |
 				NFS         AFS
 			       [v=1]       [v=1]
  If an entry with the appropriate name does already exist, the version is
  compared.  If the version is different, the entire subtree from that entry
  will be discarded and a new entry created.
  The new entry will be an index, and a cookie referring to it will be passed
  to the netfs.  This is then the root handle by which the netfs accesses the
  cache.  It can create whatever objects it likes in that index, including
  further indices.
  Definition of an entry in the root index.  Each entry is an index, keyed to
  a specific netfs and only applicable to a particular version of the index
  structure used by that netfs.
  check that the index structure version number stored in the auxiliary data
  matches the one the netfs gave us
 SPDX-License-Identifier: GPL-2.0-or-later
 General filesystem local caching manager
  Copyright (C) 2004-2007 Red Hat, Inc. All Rights Reserved.
  Written by David Howells (dhowells@redhat.com)
 these values serve as lower bounds, will be adjusted in fscache_init() 
  Mixing scores (in bits) for (7,20):
  Input delta: 1-bit      2-bit
  1 round:     330.3     9201.6
  2 rounds:   1246.4    25475.4
  3 rounds:   1907.1    31295.1
  4 rounds:   2042.3    31718.6
  Perfect:    2048      31744
             (3264)   (32312  64)
 Use arch-optimized multiply if one exists 
  Generate a hash.  This is derived from full_name_hash(), but we want to be
  sure it is arch independent and that it doesn't change as bits of the
  computed hash value might appear on disk.  The caller also guarantees that
  the hashed data will be a series of aligned 32-bit words.
  initialise the fs caching module
  clean up on module removal
 SPDX-License-Identifier: GPL-2.0
   linuxfssysvnamei.c
   minixnamei.c
   Copyright (C) 1991, 1992  Linus Torvalds
   cohnamei.c
   Copyright (C) 1993  Pascal Haible, Bruno Haible
   sysvnamei.c
   Copyright (C) 1993  Bruno Haible
   Copyright (C) 1997, 1998  Krzysztof G. Baranowski
  Anybody can rename anything with this: the permission checks are left to the
  higher-level routines.
  directories can handle most operations...
 SPDX-License-Identifier: GPL-2.0-only
   linuxfssysvinode.c
   minixinode.c
   Copyright (C) 1991, 1992  Linus Torvalds
   xenixinode.c
   Copyright (C) 1992  Doug Evans
   cohinode.c
   Copyright (C) 1993  Pascal Haible, Bruno Haible
   sysvinode.c
   Copyright (C) 1993  Paul B. Monday
   sysvinode.c
   Copyright (C) 1993  Bruno Haible
   Copyright (C) 1997, 1998  Krzysztof G. Baranowski
   This file contains code for readparsing the superblock.
  The following functions try to recognize specific filesystems.
  We recognize:
  - Xenix FS by its magic number.
  - SystemV FS by its magic number.
  - Coherent FS by its funny fnamefpack field.
  - SCO AFS by s_nfree == 0xffff
  - V7 FS has no distinguishing features.
  We discriminate among SystemV4 and SystemV2 FS by the assumption that
  the time stamp is not < 01-01-1980.
 block size = 512, so bh1 != bh2 
 All relevant fields are at the same offsets in R2 and R4 
 this is likely to happen on SystemV2 FS 
	 On Interactive Unix (ISC) Version 4.03.x s_type field = 0x10,
	   0x20 or 0x30 indicates that symbolic links and the 14-character
	   filename limit is gone. Due to lack of information about this
 set up enough so that it can read an inode 
 plausibility check on superblock 
	 plausibility check on root inode: it is a directory,
 Try PDP-11 UNIX 
 Try PCIX, v7x86 
 Every kernel module contains stuff like this. 
 SPDX-License-Identifier: GPL-2.0
   linuxfssysvialloc.c
   minixbitmap.c
   Copyright (C) 1991, 1992  Linus Torvalds
   extfreelists.c
   Copyright (C) 1992  Remy Card (card@masi.ibp.fr)
   xenixalloc.c
   Copyright (C) 1992  Doug Evans
   cohalloc.c
   Copyright (C) 1993  Pascal Haible, Bruno Haible
   sysvialloc.c
   Copyright (C) 1993  Bruno Haible
   This file contains code for allocatingfreeing inodes.
 We don't trust the value of
   sb->sv_sbd2->s_tinode = sb->sv_sb_total_free_inodes
 An inode on disk is considered free if both i_mode == 0 and i_nlink == 0. 
 return &sb->sv_sb_fic_inodes[i] = &sbd->s_inode[i]; 
 512 byte Xenix FS 
 Now count > 0. 
 ensure inode not allocated again 
 cleared by sysv_write_inode() 
 That's it. 
 this causes a lot of disk traffic ... 
 SPDX-License-Identifier: GPL-2.0
   linuxfssysvinode.c
   minixinode.c
   Copyright (C) 1991, 1992  Linus Torvalds
   xenixinode.c
   Copyright (C) 1992  Doug Evans
   cohinode.c
   Copyright (C) 1993  Pascal Haible, Bruno Haible
   sysvinode.c
   Copyright (C) 1993  Paul B. Monday
   sysvinode.c
   Copyright (C) 1993  Bruno Haible
   Copyright (C) 1997, 1998  Krzysztof G. Baranowski
   This file contains code for allocatingfreeing inodes and for readwriting
   the superblock.
	
	  If we are going to write out the super block,
	  then attach current time stamp.
	  But if the filesystem was marked clean, keep it clean.
 XXX ext2 also updates the state here 
  NXI <-> N0XI for PDP, XIN <-> XIN0 for le32, NIX <-> 0NIX for be32
 SystemV FS: kludge permissions if ino==SYSV_ROOT_INO ?? 
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 SPDX-License-Identifier: GPL-2.0
   linuxfssysvdir.c
   minixdir.c
   Copyright (C) 1991, 1992  Linus Torvalds
   cohdir.c
   Copyright (C) 1993  Pascal Haible, Bruno Haible
   sysvdir.c
   Copyright (C) 1993  Bruno Haible
   SystemVCoherent directory handling functions
 compare strings: name[0..len-1] (not zero-terminated) and
  buffer[0..] (filled with zeroes up to buffer[0..maxlen-1])
 	sysv_find_entry()
  finds an entry in the specified directory with the wanted name. It
  returns the cache buffer in which the entry was found, and the entry
  itself (as a parameter - res_dir). It does NOT read the inode of the
  entry - you'll have to do that yourself if you want to.
 We take care of directory expansion in the same loop 
  routine to check that the specified directory is empty (for rmdir)
 check for . and .. 
 Releases the page 
 SPDX-License-Identifier: GPL-2.0
   linuxfssysvballoc.c
   minixbitmap.c
   Copyright (C) 1991, 1992  Linus Torvalds
   extfreelists.c
   Copyright (C) 1992  Remy Card (card@masi.ibp.fr)
   xenixalloc.c
   Copyright (C) 1992  Doug Evans
   cohalloc.c
   Copyright (C) 1993  Pascal Haible, Bruno Haible
   sysvballoc.c
   Copyright (C) 1993  Bruno Haible
   This file contains code for allocatingfreeing blocks.
 We don't trust the value of
   sb->sv_sbd2->s_tfree = sb->sv_free_blocks
 NOTE NOTE NOTE: nr is a block number _as_ _stored_ _on_ _disk_ 
	
	  This code does not work at all for AFS (it has a bitmap
	  free list).  As AFS is supposed to be read-only no one
	  should call this for an AFS filesystem anyway...
	 If the free list head in super-block is full, it is copied
	  into this block being freed, ditto if it's completely empty
	  (applies only on Coherent).
 Applies only to Coherent FS 
 Applies only to Xenix FS, SystemV FS 
 the last block continues the free list 
 retry this same block next time 
 Now the free list head in the superblock is valid again. 
	
	  This code does not work at all for AFS (it has a bitmap
	  free list).  As AFS is supposed to be read-only we just
	  lie and say it has no free block at all.
 this causes a lot of disk traffic ... 
 SPDX-License-Identifier: GPL-2.0
   linuxfssysvitree.c
   Handling of indirect blocks' trees.
   AV, Sep--Dec 2000
 Have triple indirect 
 nothing ;
  Requires read_lock(&pointers_lock) or write_lock(&pointers_lock)
 Allocate the next block 
		
		  Get buffer_head for parent block, zero it out and set 
		  the pointer to new one, then send parent to disk.
 Allocation failed, free what we already allocated 
 Verify that place we are splicing to is still there and vacant 
 had we spliced it onto indirect block? 
 Simplest case - block found, no allocation needed 
 Clean up and exit 
 the whole chain 
 Next simple case - plain lookup or failed read of indirect block 
	
	  Indirect block might be removed by truncate while we were
	  reading it. Handling of that case (forget what we've got and
	  reread) is taken out of the main path.
	
	  If the branch acquired continuation since we've looked at it -
	  fine, it should all survive and (new) top doesn't belong to us.
	
	  OK, we've found the last block that must survive. The rest of our
	  branch should be detached before unlocking. However, if that rest
	  of branch is all ours and does not grow immediately from the inode
	  it's easier to cheat and just decrement partial->p.
 Kill the top of shared branch (already detached) 
 Clear the ends of indirect blocks on the shared branch 
 Kill the remaining (whole) subtrees (== subtrees deeper than...) 
 SPDX-License-Identifier: GPL-2.0
   linuxfssysvfile.c
   minixfile.c
   Copyright (C) 1991, 1992  Linus Torvalds
   cohfile.c
   Copyright (C) 1993  Pascal Haible, Bruno Haible
   sysvfile.c
   Copyright (C) 1993  Bruno Haible
   SystemVCoherent regular file handling primitives
  We have mostly NULLs here: the current defaults are OK for
  the coh filesystem.
  namei.c
  PURPOSE
       Inode name handling routines for the OSTA-UDF(tm) filesystem.
  COPYRIGHT
       This file is distributed under the terms of the GNU General Public
       License (GPL). Copies of the GPL can be obtained from:
               ftp:prep.ai.mit.edupubgnuGPL
       Each contributing author retains all rights to their own work.
   (C) 1998-2004 Ben Fennema
   (C) 1999-2000 Stelias Computing Inc
  HISTORY
   121298 blf  Created. Split out the lookup code from dir.c
   041999 blf  link, mknod, symlink support
  udf_find_entry - find entry in given directory.
  @dir:	directory inode to search in
  @child:	qstr of the name
  @fibh:	buffer head  inode with file identifier descriptor we found
  @cfi:	found file identifier descriptor with given name
  This function searches in the directory @dir for a file name @child. When
  found, @fibh points to the buffer head(s) (bh is NULL for in ICB
  directories) containing the file identifier descriptor (FID). In that case
  the function returns pointer to the FID in the buffer or inode - but note
  that FID may be split among two buffers (blocks) so accessing it via that
  pointer isn't easily possible. This pointer can be used only as an iterator
  for other directory manipulation functions. For inspection of the FID @cfi
  can be used - the found FID is copied there.
  Returns pointer to FID, NULL when nothing found, or error code.
 Unpaded ending offset 
 Load extent udf_expand_dir_adinicb() has created 
 Entry fits into current block? 
 Round up last extent in the file 
 Extents could have been merged, invalidate our position 
 Find the freshly allocated block 
 Find the last extent and truncate it to proper size 
 Anybody can rename anything with this: the permission checks are left to the
  higher-level routines.
	
	  Like most other Unix systems, set the ctime for inodes on a
	  rename.
	
	  ok, that's it
 The old fid may have moved - find it again 
  partition.c
  PURPOSE
       Partition handling routines for the OSTA-UDF(tm) filesystem.
  COPYRIGHT
       This file is distributed under the terms of the GNU General Public
       License (GPL). Copies of the GPL can be obtained from:
               ftp:prep.ai.mit.edupubgnuGPL
       Each contributing author retains all rights to their own work.
   (C) 1998-2001 Ben Fennema
  HISTORY
  120698 blf  Created file.
 if old_block 
 outside of partitions 
 for now, fail =) 
 map to sparablephysical partition desc 
  super.c
  PURPOSE
   Super block routines for the OSTA-UDF(tm) filesystem.
  DESCRIPTION
   OSTA-UDF(tm) = Optical Storage Technology Association
   Universal Disk Format.
   This code is based on version 2.00 of the UDF specification,
   and revision 3 of the ECMA 167 standard [equivalent to ISO 13346].
     http:www.osta.org
     https:www.ecma.ch
     https:www.iso.org
  COPYRIGHT
   This file is distributed under the terms of the GNU General Public
   License (GPL). Copies of the GPL can be obtained from:
     ftp:prep.ai.mit.edupubgnuGPL
   Each contributing author retains all rights to their own work.
   (C) 1998 Dave Boynton
   (C) 1998-2004 Ben Fennema
   (C) 2000 Stelias Computing Inc
  HISTORY
   092498 dgb  changed to allow compiling outside of kernel, and
                 added some debugging.
   100198 dgb  updated to allow (some) possibility of compiling w2.0.34
   101698      attempting some multi-session support
   101798      added freespace count for "df"
   111198 gr   added novrs option
   112698 dgb  added fileset,anchor mount options
   120698 blf  really hosed things royally. vatsparing support. sequenced
                 vol descs. rewrote option handling based on isofs
   122098      find the free space bitmap (if it exists)
  Maximum number of Terminating Descriptor  Logical Volume Integrity
  Descriptor redirections. The chosen numbers are arbitrary - just that we
  hopefully don't limit any real use of rewritten inode on write-once media
  but avoid looping for too long on corrupted media.
 These are the "meat" - everything else is stuffing 
 The offset is to skip freeSpaceTable and sizeTable arrays 
 UDF filesystem type 
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 Superblock operations 
  udf_parse_options
  PURPOSE
 	Parse mount options.
  DESCRIPTION
 	The following mount options are supported:
 	gid=		Set the default group.
 	umask=		Set the default umask.
 	mode=		Set the default file permissions.
 	dmode=		Set the default directory permissions.
 	uid=		Set the default user.
 	bs=		Set the block size.
 	unhide		Show otherwise hidden files.
 	undelete	Show deleted files in lists.
 	adinicb		Embed data in the inode (default)
 	noadinicb	Don't embed data in the inode
 	shortad		Use short ad's
 	longad		Use long ad's (default)
 	nostrict	Unset strict conformance
 	iocharset=	Set the NLS character set
 	The remaining are for debugging and disaster recovery:
 	novrs		Skip volume sequence recognition
 	The following expect a offset from 0.
 	session=	Set the CDROM session (default= last session)
 	anchor=		Override standard anchor location. (default= 256)
 	volume=		Override the VolumeDesc location. (unused)
 	partition=	Override the PartitionDesc location. (unused)
 	lastblock=	Set the last block of the filesystem
 	The following expect a offset from the partition root.
 	fileset=	Override the fileset block location. (unused)
 	rootdir=	Override the root directory location. (unused)
 		WARNING: overriding the rootdir to a non-directory may
 		yield highly unpredictable results.
  PRE-CONDITIONS
 	options		Pointer to mount options string.
 	uopts		Pointer to mount options variable.
  POST-CONDITIONS
 	<return>	1	Mount options parsed okay.
 	<return>	0	Error parsing mount options.
  HISTORY
 	July 1, 1997 - Andrew E. Mileski
 	Written, tested, and released.
 Ignored (never implemented properly) 
 When nls_map is not loaded then UTF-8 is used 
 These options are superseeded by uid=<number> 
  Check VSD descriptor. Returns -1 in case we are at the end of volume
  recognition area, 0 if the descriptor is valid but non-interesting, 1 if
  we found one of NSR descriptors we are looking for.
 ret = 0 
 ret = 0 
 ret = 0 
 TEA01 or invalid id : end of volume recognition area 
  Check Volume Structure Descriptors (ECMA 167 29.1)
  We also check any "CD-ROM Volume Descriptor Set" (ECMA 167 28.3.1)
  @return   1 if NSR02 or NSR03 found,
 	    -1 if first sector read error, 0 otherwise
	 Process the sequence (if applicable). The hard limit on the sector
	  offset is arbitrary, hopefully large enough so that all valid UDF
	  filesystems will be recognised. There is no mention of an upper
	  bound to the size of the volume recognition area in the standard.
	   The limit will prevent the code to read all the sectors of a
	  specially crafted image (like a bluray disc full of CD001 sectors),
	  potentially causing minutes or even hours of uninterruptible IO
	  activity. This actually happened with uninitialised SSD partitions
	  (all 0xFF) before the check for the limit and all valid IDs were
 Read a block 
 Found NSR or end? 
		
		  Special handling for improperly formatted VRS (e.g., Win10)
		  where components are separated by 2048 bytes even though
		  sectors are 4K
 Ignore unknown IDs... 
  Load primary Volume Descriptor Sequence
  Return <0 on error, 0 on success. -EAGAIN is special meaning next sequence
  should be tried.
 metadata address 
 mirror file entry 
	
	  bitmap file entry
	  Note:
	  Load only if bitmap file location differs from 0xFFFFFFFF (DCN-5102)
 No Partition Header Descriptor? 
 No allocation info? 
 We don't support blocks that require erasing before overwrite 
 UDF 2.60: 2.3.3 - no mixing of tables & bitmaps, no VAT. 
 blocks 
	
	  Skip loading allocation info it we cannot ever write to the fs.
	  This is a correctness thing as we may have decided to force ro mount
	  to avoid allocation info we don't support.
	
	  VAT file entry is in the last recorded block. Some broken disks have
	  it a few blocks before so try a bit harder...
  Load partition descriptor block
  Returns <0 on error, 0 on success, -EAGAIN is special - try next descriptor
  sequence.
 First scan for TYPE1 and SPARABLE partitions 
	
	  Now rescan for VIRTUAL or METADATA partitions when SPARABLE and
	  PHYSICAL partitions are already set up
 supress 'maybe used uninitialized' warning 
		
		  If we have a partition with virtual map, we don't handle
		  writing to it (we overwrite blocks instead of relocating
		  them).
 In case loading failed, we handle cleanup in udf_fill_super 
 We can't generate unique IDs without a valid LVID 
  Find the prevailing Logical Volume Integrity Descriptor.
  Step for reallocation of table of partition descriptor sequence numbers.
  Must be power of 2.
 ISO 13346 310.1 
 ISO 13346 310.4 
 ISO 13346 310.6 
 ISO 13346 310.8 
 ISO 13346 310.5 
  Process a mainreserve volume descriptor sequence.
    @block		First block of first extent of the sequence.
    @lastblock		Lastblock of first extent of the sequence.
    @fileset		There we store extent containing root fileset
  Returns <0 on error, 0 on success. -EAGAIN is special - try next descriptor
  sequence
	
	  Read the main descriptor sequence and find which descriptors
	  are in it.
 Process each descriptor (ISO 13346 38.3-8.4) 
 ISO 13346 310.3 
 For loop is going to increment 'block' again 
 ISO 13346 310.1 
 ISO 13346 310.4 
 ISO 13346 310.6 
 ISO 13346 310.8 
 ISO 13346 310.5 
 Descriptor we don't care about? 
 ISO 13346 310.9 
	
	  Now read interesting descriptors again and process them
	  in a suitable order
 Now handle prevailing Partition Descriptors 
  Load Volume Descriptor Sequence described by anchor in bh
  Returns <0 on error, 0 on success
 Locate the main sequence 
 Locate the reserve sequence 
 Process the main & reserve sequences 
 responsible for finding the PartitionDesc(s) 
 No sequence was OK, return -EIO 
  Check whether there is an anchor block in the given block and
  load Volume Descriptor Sequence if so.
  Returns <0 on error, 0 on success, -EAGAIN is special - try next anchor
  block
  Search for an anchor volume descriptor pointer.
  Returns < 0 on error, 0 on success. -EAGAIN is special - try next set
  of anchors.
 First try user provided anchor 
	
	  according to spec, anchor is in either:
	      block 256
	      lastblock-256
	      lastblock
	   however, if the disc isn't closed, it could be 512.
	
	  The trouble is which block is the last one. Drives often misreport
	  this so we try various possibilities.
 Finally try block 512 in case media is open 
  Find an anchor volume descriptor and load Volume Descriptor Sequence from
  area specified by it. The function expects sbi->s_lastblock to be the last
  block on the media.
  Return <0 on error, 0 if anchor found. -EAGAIN is special meaning anchor
  was not found.
 No anchor found? Try VARCONV conversion of block numbers 
 Firstly, we try to not convert number of the last block 
 Secondly, we try with converted number of the last block 
 VARCONV didn't help. Clear it. 
  Check Volume Structure Descriptor, find Anchor block and load Volume
  Descriptor Sequence.
  Returns < 0 on error, 0 on success. -EAGAIN is special meaning anchor
  block was not found.
 Check that it is NSR02 compliant 
 Look for anchor block and load Volume Descriptor Sequence 
 Make opening of filesystem visible on the media immediately 
	
	  We set buffer uptodate unconditionally here to avoid spurious
	  warnings from mark_buffer_dirty() when previous EIO has marked
	  the buffer as !uptodate
 Make closing of filesystem visible on the media immediately 
 By default we'll use overflow[ug]id when UDF inode [ug]id == -1 
 Fill in the rest of the superblock 
				
				  EACCES is special - we want to propagate to
				  upper layers that we cannot handle RW mount.
 Assign the root inode 
 assign inodes by physical block number 
 perhaps it's not extensible enough, but for now ... 
 Allocate a dentry for the root inode 
		
		  Blockdevice will be synced later so we don't have to submit
		  the buffer for IO
	
	  Let's pretend each free block is also a free 'inode' since UDF does
	  not have separate preallocated table of inodes.
 offset in first block only 
		
		  Filesystems with VAT are append-only and we cannot write to
 		  them. Let's just report 0 here.
  ialloc.c
  PURPOSE
 	Inode allocation handling routines for the OSTA-UDF(tm) filesystem.
  COPYRIGHT
 	This file is distributed under the terms of the GNU General Public
 	License (GPL). Copies of the GPL can be obtained from:
 		ftp:prep.ai.mit.edupubgnuGPL
 	Each contributing author retains all rights to their own work.
   (C) 1998-2001 Ben Fennema
  HISTORY
   022499 blf  Created.
  truncate.c
  PURPOSE
 	Truncate handling routines for the OSTA-UDF(tm) filesystem.
  COPYRIGHT
 	This file is distributed under the terms of the GNU General Public
 	License (GPL). Copies of the GPL can be obtained from:
 		ftp:prep.ai.mit.edupubgnuGPL
 	Each contributing author retains all rights to their own work.
   (C) 1999-2004 Ben Fennema
   (C) 1999 Stelias Computing Inc
  HISTORY
   022499 blf  Created.
  Truncate the last extent to match i_size. This function assumes
  that preallocation extent is already truncated.
 Are we going to delete the file anyway? 
 Find the last extent in the file 
	 This inode entry is in-memory only and thus we don't have to mark
 Find the last extent in the file 
	 This inode entry is in-memory only and thus we don't have to mark
  Truncate extents of inode to inode->i_size. This function can be used only
  for making file shorter. For making file longer, udf_extend_file() has to
  be used.
 We should extend the file? 
				 We managed to free all extents in the
 Error reading indirect block? 
  inode.c
  PURPOSE
   Inode handling routines for the OSTA-UDF(tm) filesystem.
  COPYRIGHT
   This file is distributed under the terms of the GNU General Public
   License (GPL). Copies of the GPL can be obtained from:
     ftp:prep.ai.mit.edupubgnuGPL
   Each contributing author retains all rights to their own work.
   (C) 1998 Dave Boynton
   (C) 1998-2004 Ben Fennema
   (C) 1999-2000 Stelias Computing Inc
  HISTORY
   100498 dgb  Added rudimentary directory functions
   100798      Fully working udf_block_map! It works!
   112598      bmap altered to better support extents
   120698 blf  partition support in udf_iget, udf_block_map
                 and udf_read_inode
   121298      rewrote udf_block_map to handle next extents and descs across
                 block boundaries (which is not actually allowed)
   122098      added support for strategy 4096
   030799      rewrote udf_block_map (again)
                 New funcs, inode_bmap, udf_next_aext
   041999      Support for writing device EA's for majorminor #
 Invalidate extent cache 
 Return contents of extent cache 
 Cache hit 
 Add extent to extent cache 
 Invalidate previously cached extent 
  Expand file stored in ICB to a normal one-block-file
  This function requires i_data_sem for writing and releases it.
  This function requires i_mutex held
 from now on we have normal address_space methods 
	
	  Release i_data_sem so that we can lock a page - page lock ranks
	  above i_data_sem. i_mutex still protects us against file changes.
 from now on we have normal address_space methods 
 Restore everything back so that we don't lose data... 
 alloc block, and copy data to it 
 UniqueID stuff 
 Extend the file with new blocks totaling 'new_block_bytes',
  return the number of extents added
	 The previous extent is fake and we should not extend by anything
 Round the last extent up to a multiple of block size 
 Last extent are just preallocated blocks? 
 Save the extent so that we can reattach it to the end 
 Mark the extent as a hole 
 Can we merge with the previous extent? 
		
		  We've rewritten the last extent. If we are going to add
		  more extents, we may need to enter possible following
		  empty indirect extent.
 Managed to do everything necessary? 
 All further extents will be NOT_RECORDED_NOT_ALLOCATED 
 Create enough extents to cover the whole hole 
 Do we have some preallocated blocks saved? 
 last_pos should point to the last written extent... 
 Extend the final block of the file to final_block_len bytes 
		 File has no extents at all or has empty last
	 File has extent covering the new size (could happen when extending
	  inside a block)?
 Extending file within the last file block 
	 find the extent which contains the block we are looking for.
	   alternate between laarr[0] and laarr[1] for locations of the
	
	  Move prev_epos and cur_epos into indirect extent if we are at
	  the pointer to it
	 if the extent is allocated and recorded, return the block
 Are we beyond EOF? 
 Create a fake extent when there's not one 
			 Will udf_do_extend_file() create real extent from
 Create extents for the hole between EOF and offset 
 We are not covered by a preallocated extent? 
			 Is there any real extent? - otherwise we overwrite
		 if the current extent is in position 0,
		 if the current block is located in an extent,
	 if the current extent is not recorded but allocated, get the
 otherwise, allocate a new block 
 XXX: what was intended here? 
	 if the extent the requsted block is located in contains multiple
	  blocks, split the extent into at most three extents. blocks prior
	  to requested block, requested block, and blocks after requested
	 We preallocate blocks only for regular files. It also makes sense
	  for directories but there's a problem when to drop the
	  preallocation. We might use some delayed work for that but I feel
 merge any continuous blocks in laarr 
	 write back the new extents, inserting new extents if the new number
	  of extents is greater than the old number, and deleting extents if
l[i] = &laarr[i];
l[i plus 1] = &laarr[i + 1];
  Maximum length of linked list formed by ICB hierarchy. The chosen number is
  arbitrary - just that we hopefully don't limit any real use of rewritten
  inode on write-once media but avoid looping for too long on corrupted media.
	
	  Set defaults, but the inode is still incomplete!
	  Note: get_new_inode() sets the following on a new inode:
	       i_sb = sb
	       i_no = ino
	       i_flags = sb->s_flags
	       i_state = 0
	  clean_inode(): zero fills and sets
	       i_count = 1
	       i_nlink = 1
	       i_op = NULL;
 if (fe->icbTag.strategyType == cpu_to_le16(4096)) 
 Named streams 
	
	  Sanity check length of allocation descriptors and extended attrs to
	  avoid integer overflows
 Now do exact checks 
 Sanity checks for files in ICB so that we don't get confused later 
		
		  For file in ICB data is stored in allocation descriptor
		  so sizes should match
 File in ICB has to fit in there... 
 Developer ID ??? 
	
	  UDF 2.01 sec. 3.3.3.3 Note 2:
	  In Unix, delete permission tracks write
 No extents => no blocks! 
 write the data blocks 
	
	  Do we have to copy current last extent to make space for indirect
	  one?
  Append extent at the given position - should be the first free one in inode
   indirect extent. This function assumes there is enough space in the inode
  or indirect extent. Use udf_add_aext() if you didn't check for this before.
  Append extent at given position - should be the first free one in inode
   indirect extent. Takes care of allocating and linking indirect blocks.
  Only 1 indirect extent in a row really makes sense but allow upto 16 in case
  someone does some weird stuff.
 update extent cache 
  dir.c
  PURPOSE
   Directory handling routines for the OSTA-UDF(tm) filesystem.
  COPYRIGHT
 	This file is distributed under the terms of the GNU General Public
 	License (GPL). Copies of the GPL can be obtained from:
 		ftp:prep.ai.mit.edupubgnuGPL
 	Each contributing author retains all rights to their own work.
   (C) 1998-2004 Ben Fennema
  HISTORY
   100598 dgb  Split directory operations into its own file
                 Implemented directory reads via do_udf_readdir
   100698      Made directory operations work!
   111798      Rewrote directory to support ICBTAG_FLAG_AD_LONG
   112598 blf  Rewrote directory handling (readdir+lookup) to support reading
                 across blocks.
   121298      Split out the lookup code to namei.c. bulk of directory
                 code now in directory.c:udf_fileident_read.
	
	  Something changed since last readdir (either lseek was called or dir
	  changed)?  We need to verify the position correctly points at the
	  beginning of some dir entry so that the directory parsing code does
	  not get confused. Since UDF does not have any reliable way of
	  identifying beginning of dir entry (names are under user control),
	  we need to scan the directory from the beginning.
 Update file position only if we got past the current one 
 Still not at offset where user asked us to read from? 
 Unpaded ending offset 
 end while 
 readdir and lookup functions 
 Copyright (C) 1993, 1994, 1995, 1996, 1997 Free Software Foundation, Inc.
   This file is part of the GNU C Library.
   Contributed by Paul Eggert (eggert@twinsun.com).
   The GNU C Library is free software; you can redistribute it andor
   modify it under the terms of the GNU Library General Public License as
   published by the Free Software Foundation; either version 2 of the
   License, or (at your option) any later version.
   The GNU C Library is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   Library General Public License for more details.
   You should have received a copy of the GNU Library General Public
   License along with the GNU C Library; see the file COPYING.LIB.  If not,
   write to the Free Software Foundation, Inc., 59 Temple Place - Suite 330,
  dgb 100298: ripped this from glibc source to help convert timestamps
                to unix time
      100498: added new table-based lookup after seeing how ugly
                the gnu code is
  blf 092799: ripped out all the old code and inserted new table from
 		 John Brockmeyer (without leap second corrections)
 		 rewrote udf_stamp_to_time and fixed timezone accounting in
 		 udf_time_to_stamp.
  We don't take into account leap seconds. This may be correct or incorrect.
  For more NIST information (especially dealing with leap seconds), see:
  http:www.boulder.nist.govtimefreqpubsbulletinleapsecond.htm
 sign extent offset 
 unspecified offset 
	
	  Sanitize nanosecond field since reportedly some filesystems are
	  recorded with bogus sub-second values.
 EOF 
  balloc.c
  PURPOSE
 	Block allocation handling routines for the OSTA-UDF(tm) filesystem.
  COPYRIGHT
 	This file is distributed under the terms of the GNU General Public
 	License (GPL). Copies of the GPL can be obtained from:
 		ftp:prep.ai.mit.edupubgnuGPL
 	Each contributing author retains all rights to their own work.
   (C) 1999-2001 Ben Fennema
   (C) 1999 Stelias Computing Inc
  HISTORY
   022499 blf  Created.
		
		 Check to see if we are freeing blocks across a group boundary.
		
		  Ran off the end of the bitmap, and bits following are
		  non-compliant (not all zero)
		
		  NOTE: we CANNOT use udf_add_aext here, as it can try to
		  allocate a new block, and since we hold the super block
		  lock already very bad things would happen :)
		 
		  We copy the behavior of udf_add_aext, but instead of
		  trying to allocate a new block close to the existing one,
		  we just steal a block from the extent we are trying to add.
		 
		  It would be nice if the blocks were close together, but it
		  isn't required.
 Steal a block from the extent being free'd 
 It's possible that stealing the block emptied the extent 
 empty loop body 
	 We search for the closest matching block to goal. If we find
	   a exact hit, we stop. Otherwise we keep going till we run out
	   of extents. We store the buffer_head, bloc, and extoffset
	   of the current closest match and use that when we are done.
	 Only allocate blocks from the beginning of the extent.
	   That way, we only delete (empty) extents, never have to insert an
 This works, but very poorly.... 
  directory.c
  PURPOSE
 	Directory related functions
  COPYRIGHT
 	This file is distributed under the terms of the GNU General Public
 	License (GPL). Copies of the GPL can be obtained from:
 		ftp:prep.ai.mit.edupubgnuGPL
 	Each contributing author retains all rights to their own work.
 Got last entry outside of dir size - fs is corrupted! 
 we need to figure padding, too! 
  misc.c
  PURPOSE
 	Miscellaneous routines for the OSTA-UDF(tm) filesystem.
  COPYRIGHT
 	This file is distributed under the terms of the GNU General Public
 	License (GPL). Copies of the GPL can be obtained from:
 		ftp:prep.ai.mit.edupubgnuGPL
 	Each contributing author retains all rights to their own work.
   (C) 1998 Dave Boynton
   (C) 1998-2004 Ben Fennema
   (C) 1999-2000 Stelias Computing Inc
  HISTORY
   041999 blf  partial support for readingwriting specific EA's
 TODO - Check for FreeEASpace 
 check checksumcrc 
 rewrite CRC + checksum of eahd 
 check checksumcrc 
 Detect undersized elements and buffer overflows 
  udf_read_tagged
  PURPOSE
 	Read the first block of a tagged descriptor.
  HISTORY
 	July 1, 1997 - Andrew E. Mileski
 	Written, tested, and released.
 Read the block 
 Verify the tag checksum 
 Verify the tag version 
 Verify the descriptor CRC 
 position of checksum 
  lowlevel.c
  PURPOSE
   Low Level Device Routines for the UDF filesystem
  COPYRIGHT
 	This file is distributed under the terms of the GNU General Public
 	License (GPL). Copies of the GPL can be obtained from:
 		ftp:prep.ai.mit.edupubgnuGPL
 	Each contributing author retains all rights to their own work.
   (C) 1999-2001 Ben Fennema
  HISTORY
   032699 blf  Created.
 necessary for a valid ms_info.addr 
	
	  The cdrom layer call failed or returned obviously bogus value?
	  Try using the device size...
  file.c
  PURPOSE
   File handling routines for the OSTA-UDF(tm) filesystem.
  COPYRIGHT
   This file is distributed under the terms of the GNU General Public
   License (GPL). Copies of the GPL can be obtained from:
     ftp:prep.ai.mit.edupubgnuGPL
   Each contributing author retains all rights to their own work.
   (C) 1998-1999 Dave Boynton
   (C) 1998-2004 Ben Fennema
   (C) 1999-2000 Stelias Computing Inc
  HISTORY
   100298 dgb  Attempt to integrate into udf.o
   100798      Switched to using generic_readpage, etc., like isofs
                 And it works!
   120698 blf  Added udf_file_read. uses generic_file_read for all cases but
                 ICBTAG_FLAG_AD_IN_ICB.
   040699      64 bit file handling on 32 bit systems taken from ext2 file.c
   051299      Preliminary file write support
 memset 
	
	  We have to be careful here as truncate can change i_size under us.
	  So just sample it once and use the same value everywhere.
 Fallback to buffered IO. 
		
		  Grab i_mutex to avoid races with writes changing i_size
		  while we are running.
  unicode.c
  PURPOSE
 	Routines for converting between UTF-8 and OSTA Compressed Unicode.
       Also handles filename mangling
  DESCRIPTION
 	OSTA Compressed Unicode is explained in the OSTA UDF specification.
 		http:www.osta.org
 	UTF-8 is explained in the IETF RFC XXXX.
 		ftp:ftp.internic.netrfcrfcxxxx.txt
  COPYRIGHT
 	This file is distributed under the terms of the GNU General Public
 	License (GPL). Copies of the GPL can be obtained from:
 		ftp:prep.ai.mit.edupubgnuGPL
 	Each contributing author retains all rights to their own work.
 for memset 
 Number of chars we need to store generated CRC to make filename unique 
 Expand OSTA compressed Unicode to Unicode 
 Trailing surrogate char 
 Low surrogate must follow the high one... 
 These chars cannot be converted. Replace them. 
 Valid character? 
 Length of resulting output 
 Extension output length 
 Extension output length if used with CRC 
 Extension position in input buffer 
 Rightmost possible output pos for CRC+ext 
 Look for extension 
 Convert extension 
 Name didn't fit? 
 Invalid character, deal with it 
			
			  Use UTF-16 encoding for chars outside we
			  cannot encode directly.
  Convert CS0 dstring to output charset. Warning: This function may truncate
  input string if it is too long as it is used for informational strings only
  and it is better to truncate the string than to refuse mounting a media.
 2-byte encoding? Need to round properly... 
 Zero length filename isn't valid... 
  symlink.c
  PURPOSE
 	Symlink handling routines for the OSTA-UDF(tm) filesystem.
  COPYRIGHT
 	This file is distributed under the terms of the GNU General Public
 	License (GPL). Copies of the GPL can be obtained from:
 		ftp:prep.ai.mit.edupubgnuGPL
 	Each contributing author retains all rights to their own work.
   (C) 1998-2001 Ben Fennema
   (C) 1999 Stelias Computing Inc
  HISTORY
   041699 blf  Created.
 Reserve one byte for terminating \0 
			
			  Symlink points to some place which should be agreed
 			  upon between originator and receiver of the media. Ignore.
 that would be . - just ignore 
 We don't support symlinks longer than one block 
	
	  UDF uses non-trivial encoding of symlinks so i_size does not match
	  number of characters reported by readlink(2) which apparently some
	  applications expect. Also POSIX says that "The value returned in the
	  st_size field shall be the length of the contents of the symbolic
	  link, and shall not count a trailing null if one is present." So
	  let's report the length of string returned by readlink(2) for
	  st_size.
  symlinks can't do much...
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines  Corp., 2002-2004
    Copyright (C) Andreas Gruenbacher, 2001
    Copyright (C) Linus Torvalds, 1991, 1992
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2005
 	jfs_xtree.c: extent allocation descriptor B+-tree manager
  xtree local flag
 	xtree keyentry comparison: extent offset
  return:
 	-1: k < start of extent
 	 0: start_of_extent <= k <= end_of_extent
 	 1: k > end_of_extent
 write a xad entry 
 get page buffer for specified block address 
 ToDo: Replace this ugly macro with a function 
 for consistency 
 xtree entry parameter descriptor 
 	statistics
  forward references
  _STILL_TO_PORT 
 	xtLookup()
  function: map a single page into a physical extent;
 is lookup offset beyond eof ? 
	
	  search for the xad entry covering the logical extent
search:
	
	 	compute the physical extent covering logical extent
	 
	  N.B. search may have failed (e.g., hole in sparse file),
	  and returned the index of the next entry.
 retrieve search result 
	 is xad found covering start of logical extent ?
	  lstart is a page start address,
	  i.e., lstart cannot start in a hole;
	
	  lxd covered by xad
 initialize new pxd 
 a page must be fully covered by an xad 
 	xtSearch()
  function:	search for the xad entry covering specified offset.
  parameters:
 	ip	- file object;
 	xoff	- extent offset;
 	nextp	- address of next extent (if any) for search miss
 	cmpp	- comparison result:
 	btstack - traverse stack;
 	flag	- search process flag (XT_INSERT);
  returns:
 	btstack contains (bn, index) of search path traversed to the entry.
 	cmpp is set to result of comparison with the entry returned.
 	the page containing the entry is pinned at exit.
 init for empty page 
 block number 
 page buffer 
 page 
 number of pages to split 
	
	 	search down tree from root:
	 
	  between two consecutive entries of <Ki, Pi> and <Kj, Pj> of
	  internal page, child page Pi contains entry with k, Ki <= K < Kj.
	 
	  if entry with search key K is not found
	  internal page search find the entry with largest key Ki
	  less than K which point to the child page to search;
	  leaf page search find the entry with smallest key Kj
	  greater than K so that the returned index is the position of
	  the entry to be shifted right for insertion of new entry.
	  for empty tree, search key is greater than any key of the tree.
	 
	  by convention, root bn = 0.
 getpin the page to search 
		 try sequential access heuristics with the previous
		  access entry in target leaf page:
		  once search narrowed down into the target leaf,
		  key must either match an entry in the leaf or
		  key entry does not exist in the tree;
fastSearch:
 stop sequential access heuristics 
 (t64 + lengthXAD(xad)) <= xoff 
 try next sequential entry 
						 miss: key falls between
						  previous and this entry
					 (xoff >= t64 + lengthXAD(xad));
					  matching entry may be further out:
					  stop heuristic search
 stop sequential access heuristics 
				 (index == p->header.nextindex);
				  miss: key entry does not exist in
				  the target leaftree
			
			  if hit, return index of the entry found, and
			  if miss, where new entry with search key is
			  to be inserted;
 compute number of pages to split 
 little-endian 
 save search result 
 update sequential access heuristics 
 well, ... full search now 
		
		  binary search with search key K on the current page
				
				 	search hit
				 search hit - leaf page:
				  return the entry found
 compute number of pages to split 
 save search result 
 init sequential access heuristics 
				 search hit - internal page:
				  descendsearch its child page
		
		 	search miss
		 
		  base is the smallest index with key (Kj) greater than
		  search key (K) and may be zero or maxentry index.
		
		  search miss - leaf page:
		 
		  return location of entry (base) where new entry with
		  search key K is to be inserted.
 compute number of pages to split 
 save search result 
 init sequential access heuristics 
		
		  search miss - non-leaf page:
		 
		  if base is non-zero, decrement base by one to get the parent
		  entry of the child page to search.
		
		  go down to child page
 update number of pages to split 
 push (bn, index) of the parent pageentry 
 get the child page block number 
 unpin the parent page 
 	xtInsert()
  function:
  parameter:
 	tid	- transaction id;
 	ip	- file object;
 	xflag	- extent flag (XAD_NOTRECORDED):
 	xoff	- extent offset;
 	xlen	- extent length;
 	xaddrp	- extent address pointer (inout):
 		if (xaddrp)
 			caller allocated data extent at xaddrp;
 		else
 			allocate data extent and return its xaddr;
 	flag	-
  return:
 transaction id 
 meta-page buffer 
 base B+-tree index page 
 traverse stack 
 split information 
	
	 	search for the entry location at which to insert:
	 
	  xtFastSearch() and xtSearch() both returns (leaf page
	  pinned, index at which to insert).
	  n.b. xtSearch() may return index of maxentry of
	  the full page.
 retrieve search result 
	 This test must follow XT_GETSEARCH since mp must be valid if
	
	  allocate data extent requested
	 
	  allocation hint: last xad
	
	 	insert entry for new extent
	
	 	if the leaf page is full, split the page and
	 	propagate up the router entry for the new page from split
	 
	  The xtSplitUp() will insert the entry and unpin the leaf page.
 undo data extent allocation 
	
	 	insert the new entry into the leaf page
	
	  acquire a transaction lock on the leaf page;
	 
	  action: xad insertionextension;
 if insert into middle, shift right remaining entries. 
 insert the new entry: mark the entry NEW 
 advance next available entry index 
 Don't log it if there are no links to the file 
 unpin the leaf page 
 	xtSplitUp()
  function:
 	split full pages as propagating insertion up the tree
  parameter:
 	tid	- transaction id;
 	ip	- file object;
 	split	- entry parameter descriptor;
 	btstack - traverse stack from xtSearch()
  return:
 split page 
 new right page block number 
 right child page 
 right child page block number 
 index of entry of insertion 
 next available entry index of p 
 parent page entry on traverse stack 
 number of pages split 
 is inode xtree root extensioninline EA area free ? 
		
		  acquire a transaction lock on the leaf page;
		 
		  action: xad insertionextension;
 if insert into middle, shift right remaining entries. 
 insert the new entry: mark the entry NEW 
 advance next available entry index 
 Don't log it if there are no links to the file 
	
	  allocate new index blocks to cover index page split(s)
	 
	  allocation hint: ?
 undo allocation 
	
	  Split leaf page <sp> into <sp> and a new right page <rp>.
	 
	  The split routines insert the new entry into the leaf page,
	  and acquire txLock as appropriate.
	  return <rp> pinned and its block number <rpbn>.
	
	  propagate up the router entry for the leaf page just split
	 
	  insert a router entry for the new page into the parent page,
	  propagate the insertsplit up the tree by walking back the stack
	  of (bn of parent page, index of child page entry in parent page)
	  that were traversed during the search for the page that split.
	 
	  the propagation of insertsplit up the tree stops if the root
	  splits or the page inserted into doesn't have to split to hold
	  the new entry.
	 
	  the parent entry for the split page remains the same, and
	  a new entry is inserted at its right with the first key and
	  block number of the new right page.
	 
	  There are a maximum of 3 pages pinned at any time:
	  right child, left parent and right parent (when the parent splits)
	  to keep the child page pinned while working on the parent.
	  make sure that all pins are released at exit.
 parent page specified by stack frame <parent> 
 keep current child pages <rcp> pinned 
		
		  insert router entry in parent for new right child page <rp>
 getpin the parent page <sp> 
		
		  The new key entry goes ONE AFTER the index of parent entry,
		  because the split was to the right.
		
		  split or shift right remaining entries of the parent page
		
		  parent page is full - split the parent page
 init for parent page split 
 index at insert 
 unpin previous right child page 
			 The split routines insert the new entry,
			  and acquire txLock as appropriate.
			  return <rp> pinned and its block number <rpbn>.
 keep new child page <rp> pinned 
		
		  parent page is not full - insert in parent page
			
			  insert router entry in parent for the right child
			  page from the first entry of the right child page:
			
			  acquire a transaction lock on the parent page;
			 
			  action: router xad insertion;
			
			  if insert into middle, shift right remaining entries
 insert the router entry 
 advance next available entry index. 
 Don't log it if there are no links to the file 
 unpin parent page 
 exit propagate up 
 unpin current right page 
 	xtSplitPage()
  function:
 	split a full non-root page into
 	originalsplitleft page and new right page
 	i.e., the originalsplit page remains as left page.
  parameter:
 	int		tid,
 	struct inode	ip,
 	struct xtsplit	split,
 	struct metapage	rmpp,
 	u64		rbnp,
  return:
 	Pointer to page in which to insert or NULL on error.
 new right page allocated 
 new right page block number 
 Allocate blocks to quota. 
	
	  allocate the new right page for the split
	
	  action: new page;
 little-endian 
 Don't log it if there are no links to the file 
		
		  acquire a transaction lock on the new right page;
		
		  acquire a transaction lock on the split page
	
	  initializeupdate sibling pointers of <sp> and <rp>
	
	 	sequential append at tail (after last entry of last page)
	 
	  if splitting the last page on a level because of appending
	  a entry to it (skip is maxentry), it's likely that the access is
	  sequential. adding an empty page on the side of the level is less
	  work and can push the fill factor much higher than normal.
	  if we're wrong it's no big deal -  we will do the split the right
	  way next time.
	  (it may look like it's equally easy to do a similar hack for
	  reverse sorted data, that is, split the tree left, but it's not.
	  Be my guest.)
		
		  acquire a transaction lock on the newright page;
		 
		  action: xad insertion;
 insert entry at the first entry of the new right page 
 rxtlck->lwm.offset = XTENTRYSTART; 
	
	 	non-sequential insert (at possibly middle page)
	
	  update previous pointer of old nextright page of <sp>
		
		  acquire a transaction lock on the next page;
		 
		  action:sibling pointer update;
		 sibling page may have been updated previously, or
		  it may be updated later;
	
	  split the data between the split and newright pages
	
	  skip index in old splitleft page - insert into left page:
 move right half of split page to the new right page 
 shift right tail of left half to make room for new entry 
 insert new entry 
 update page header 
	
	  skip index in new right page - insert into right page:
 move left head of right half to right page 
 insert new entry 
 move right tail of right half to right page 
 update page header 
 rxtlck->lwm.offset = XTENTRYSTART; 
 Rollback quota allocation. 
 	xtSplitRoot()
  function:
 	split the full root page into originalrootsplit page and new
 	right page
 	i.e., root remains fixed in tree anchor (inode) and the root is
 	copied to a single new right child page since root page <<
 	non-root page, and the split root page contains a single entry
 	for the new right child page.
  parameter:
 	int		tid,
 	struct inode	ip,
 	struct xtsplit	split,
 	struct metapage	rmpp)
  return:
 	Pointer to page in which to insert or NULL on error.
	
	 	allocate a single (right) child page
 Allocate blocks to quota. 
	
	  acquire a transaction lock on the new right page;
	 
	  action: new page;
 initialize sibling pointers 
	
	  copy the in-line root page into new right page extent
	
	  insert the new entry into the new rightchild page
	  (skip index in the new right page will not change)
 if insert into middle, shift right remaining entries 
 update page header 
	
	 	reset the root
	 
	  init root with the single entry for the new right page
	  set the 1st entry offset to 0, which force the left-most key
	  at any level of the tree to be less than any search key.
	
	  acquire a transaction lock on the root page (in-memory inode);
	 
	  action: root split;
 update page header of root 
 	xtExtend()
  function: extend in-place;
  note: existing extent may or may not have been committed.
  caller is responsible for pager buffer cache update, and
  working block allocation map update;
  update pmap: alloc whole extended extent;
 transaction id 
 delta extent offset 
 delta extent length 
 meta-page buffer 
 base B+-tree index page 
 traverse stack 
 split information 
 there must exist extent to be extended 
 retrieve search result 
 extension must be contiguous 
	
	  acquire a transaction lock on the leaf page;
	 
	  action: xad insertionextension;
 extend will overflow extent ? 
	
	 	extent overflow: insert entry for new extent
insertNew:
	
	 	if the leaf page is full, insert the new entry and
	 	propagate up the router entry for the new page from split
	 
	  The xtSplitUp() will insert the entry and unpin the leaf page.
 xtSpliUp() unpins leaf pages 
 split offset 
 get back old page 
		
		  if leaf root has been split, original root has been
		  copied to new child page, i.e., original entry now
		  resides on the new child page;
 get new child page 
	
	 	insert the new entry into the leaf page
 insert the new entry: mark the entry NEW 
 advance next available entry index 
 get back old entry 
	
	  extend old extent
 unpin the leaf page 
 	xtTailgate()
  function: split existing 'tail' extent
 	(split offset >= start offset of tail extent), and
 	relocate and extend the split tail half;
  note: existing extent may or may not have been committed.
  caller is responsible for pager buffer cache update, and
  working block allocation map update;
  update pmap: free old split tail extent, alloc new extent;
 transaction id 
 splitnew extent offset 
 new extent length 
 new extent address 
 meta-page buffer 
 base B+-tree index page 
 traverse stack 
 split information 
printf("xtTailgate: nxoff:0x%lx nxlen:0x%x nxaddr:0x%lx\n",
	(ulong)xoff, xlen, (ulong)xaddr);
 there must exist extent to be tailgated 
 retrieve search result 
 entry found must be last entry 
	
	  acquire tlock of the leaf page containing original entry
 completely replace extent ? 
printf("xtTailgate: xoff:0x%lx xlen:0x%x xaddr:0x%lx\n",
	(ulong)offsetXAD(xad), lengthXAD(xad), (ulong)addressXAD(xad));
	
	 	partially replace extent: insert entry for new extent
insertNew:
	
	 	if the leaf page is full, insert the new entry and
	 	propagate up the router entry for the new page from split
	 
	  The xtSplitUp() will insert the entry and unpin the leaf page.
 xtSpliUp() unpins leaf pages 
 split offset 
 get back old page 
		
		  if leaf root has been split, original root has been
		  copied to new child page, i.e., original entry now
		  resides on the new child page;
 get new child page 
	
	 	insert the new entry into the leaf page
 insert the new entry: mark the entry NEW 
 advance next available entry index 
 get back old XAD 
	
	  truncaterelocate old extent at split offset
 update dmap for oldcommittedtruncated extent 
 free from PWMAP at commit 
 free from WMAP 
 truncate 
 replace 
 unpin the leaf page 
 _NOTYET 
 	xtUpdate()
  function: update XAD;
 	update extent for allocated_but_not_recorded or
 	compressed extent;
  parameter:
 	nxad	- new XAD;
 		logical extent of the specified XAD must be completely
 		contained by an existing XAD;
 new XAD 
 meta-page buffer 
 base B+-tree index page 
 traverse stack 
 split information 
 there must exist extent to be tailgated 
 retrieve search result 
	
	  acquire tlock of the leaf page containing original entry
 nXAD must be completely contained within XAD 
	
	  replace XAD with nXAD
 (nxoff == xoff) 
 replace XAD with nXAD:recorded 
 (nxlen < xlen) 
 _JFS_WIP_NOCOALESCE 
 #ifdef _JFS_WIP_COALESCE 
	
	  coalesce with left XAD
coalesceLeft:  (xoff == nxoff) 
 is XAD first entry of page ? 
 is nXAD logically and physically contiguous with lXAD ? 
 extend right lXAD 
		 If we just merged two extents together, need to make sure the
		  right extent gets logged.  If the left one is marked XAD_NEW,
		  then we know it will be logged.  Otherwise, mark as
		  XAD_EXTENDED
 truncate XAD 
 (xlen == nxlen) 
 remove XAD 
	
	  replace XAD with nXAD
 (nxoff == xoff) 
 replace XAD with nXAD:recorded 
 (nxlen < xlen) 
	
	  coalesce with right XAD
 (xoff <= nxoff) 
 is XAD last entry of page ? 
 is nXAD logically and physically contiguous with rXAD ? 
 extend left rXAD 
		 If we just merged two extents together, need to make sure
		  the left extent gets logged.  If the right one is marked
		  XAD_NEW, then we know it will be logged.  Otherwise, mark as
		  XAD_EXTENDED
 truncate XAD 
 (xlen == nxlen) 
 remove XAD 
 #endif _JFS_WIP_COALESCE 
	
	  split XAD into (lXAD, nXAD):
	 
	           |---nXAD--->
	  --|----------XAD----------|--
	    |-lXAD-|
 (xoff < nxoff) 
 truncate old XAD as lXAD:not_recorded 
 insert nXAD:recorded 
 xtSpliUp() unpins leaf pages 
 get back old page 
		
		  if leaf root has been split, original root has been
		  copied to new child page, i.e., original entry now
		  resides on the new child page;
 get new child page 
 is nXAD on new page ? 
 if insert into middle, shift right remaining entries 
 insert the entry 
 advance next available entry index. 
	
	  does nXAD force 3-way split ?
	 
	           |---nXAD--->|
	  --|----------XAD-------------|--
	    |-lXAD-|           |-rXAD -|
 reorient nXAD as XAD for further split XAD into (nXAD, rXAD) 
 close out old page 
 get new right page 
 recompute split pages 
 retrieve search result 
	
	  split XAD into (nXAD, rXAD)
	 
	           ---nXAD---|
	  --|----------XAD----------|--
	                     |-rXAD-|
 (nxoff == xoff) && (nxlen < xlen) 
 update old XAD with nXAD:recorded 
 insert rXAD:not_recorded 
printf("xtUpdate.updateLeft.split p:0x%p\n", p);
 xtSpliUp() unpins leaf pages 
 get back old page 
		
		  if leaf root has been split, original root has been
		  copied to new child page, i.e., original entry now
		  resides on the new child page;
 get new child page 
 if insert into middle, shift right remaining entries 
 insert the entry 
 advance next available entry index. 
 unpin the leaf page 
 	xtAppend()
  function: grow in append mode from contiguous region specified ;
  parameter:
 	tid		- transaction id;
 	ip		- file object;
 	xflag		- extent flag:
 	xoff		- extent offset;
 	maxblocks	- max extent length;
 	xlen		- extent length (inout);
 	xaddrp		- extent address pointer (inout):
 	flag		-
  return:
 transaction id 
 (inout) 
 (inout) 
 meta-page buffer 
 base B+-tree index page 
 traverse stack 
 split information 
	
	 	search for the entry location at which to insert:
	 
	  xtFastSearch() and xtSearch() both returns (leaf page
	  pinned, index at which to insert).
	  n.b. xtSearch() may return index of maxentry of
	  the full page.
 retrieve search result 
insert:
	
	 	insert entry for new extent
	
	 	if the leaf page is full, split the page and
	 	propagate up the router entry for the new page from split
	 
	  The xtSplitUp() will insert the entry and unpin the leaf page.
	
	  allocate new index blocks to cover index page split(s)
 undo allocation 
	
	  allocate data extent requested
 undo data extent allocation 
	
	 	insert the new entry into the leaf page
	
	  allocate data extent requested
	
	  acquire a transaction lock on the leaf page;
	 
	  action: xad insertionextension;
 insert the new entry: mark the entry NEW 
 advance next available entry index 
 unpin the leaf page 
 - TBD for defragmentaionreorganization -
 	xtDelete()
  function:
 	delete the entry with the specified key.
 	N.B.: whole extent of the entry is assumed to be deleted.
  parameter:
  return:
 	ENOENT: if the entry is not found.
  exception:
	
	  find the matching entry; xtSearch() pins the page
 unpin the leaf page 
	
	  delete the entry from the leaf page
	
	  if the leaf page bocome empty, free the page
	
	  acquire a transaction lock on the leaf page;
	 
	  action:xad deletion;
 if delete from middle, shift leftcompact the remaining entries 
 - TBD for defragmentaionreorganization -
 	xtDeleteUp()
  function:
 	free empty pages as propagating deletion up the tree
  parameter:
  return:
	
	  keep root leaf page which has become empty
 keep the root page 
 XT_PUTPAGE(fmp); 
	
	  free non-root leaf page
 free the page extent 
 free the buffer page 
	
	  propagate page deletion up the index tree
	 
	  If the delete from the parent page makes it empty,
	  continue all the way up the tree.
	  stop if the root page is reached (which is never deleted) or
	  if the entry deletion does not empty the page.
 getpin the parent page <sp> 
		 delete the entry for the freed child page from parent.
		
		  the parent has the single entry being deleted:
		  free the parent page which has become empty.
 keep the root page 
 XT_PUTPAGE(mp); 
 free the parent page 
 free the page extent 
 unpinfree the buffer page 
 propagate up 
		
		  the parent has other entries remaining:
		  delete the router entry from the parent page.
			
			  acquire a transaction lock on the leaf page;
			 
			  action:xad deletion;
			 if delete from middle,
			  shift leftcompact the remaining entries in the page
 unpin the parent page 
 exit propagation up 
  NAME:	xtRelocate()
  FUNCTION:	relocate xtpage or data extent of regular file;
 		This function is mainly used by defragfs utility.
  NOTE:	This routine does not have the logic to handle
 		uncommitted allocated extent. The caller should call
 		txCommit() to commit all the allocation before call
 		this routine.
 old XAD 
 new xaddr 
 extent type: XTPAGE or DATAEXT 
 meta-page buffer 
 base B+-tree index page 
 traverse stack 
 validate extent offset 
 stale extent 
	
	 	1. get and validate the parent xtpagexad entry
	 	covering the source extent to be relocated;
 search in leaf entry 
 retrieve search result 
 validate for exact match with a single entry 
 (xtype == XTPAGE) 
 search in internal entry 
 retrieve search result 
		 xtSearchNode() validated for exact match with a single entry
	
	 	2. relocate the extent
		 if the extent is allocated-but-not-recorded
		  there is no real data to be moved in this extent,
 release xtpage for cmRead()xtLookup() 
		
		 	cmRelocate()
		 
		  copy target data pages to be relocated;
		 
		  data extent must start at page boundary and
		  multiple of page size (except the last data extent);
		  read in each page of the source data extent into cbuf,
		  update the cbuf extent descriptor of the page to be
		  homeward bound to new dst data extent
		  copy the data from the old extent to new extent.
		  copy is essential for compressed files to avoid problems
		  that can arise if there was a change in compression
		  algorithms.
		  it is a good strategy because it may disrupt cache
		  policy to keep the pages in memory afterwards.
		npages = ((offset + nbytes - 1) >> CM_L2BSIZE) -
			  (offset >> CM_L2BSIZE) + 1;
 process the request one cache buffer at a time 
 compute page size 
 get the cache buffer of the page 
 bind buffer with the new extent address 
 release the cbuf, mark it as modified 
 get back parent page 
 (xtype == XTPAGE) 
		
		  read in the target xtpage from the source extent;
		
		  read in sibling pages if any to update sibling pointers;
 at this point, all xtpages to be updated are in memory 
		
		  update sibling pointers of sibling xtpages if any;
		
		  update the target xtpage to be relocated
		 
		  update the self address of the target page
		  and write to destination extent;
		  redo image covers the whole xtpage since it is new page
		  to the destination extent;
		  update of bmap for the free of source extent
		  of the target xtpage itself:
		  update of bmap for the allocation of destination extent
		  of the target xtpage itself:
		  update of bmap for the extents covered by xad entries in
		  the target xtpage is not necessary since they are not
		  updated;
		  if not committed before this relocation,
		  target page may contain XAD_NEW entries which must
		  be scanned for bmap update (logredo() always
		  scan xtpage REDOPAGE image for bmap update);
		  if committed before this relocation (tlckRELOCATE),
		  scan may be skipped by commit() and logredo();
 tlckNEW init xtlck->lwm.offset = XTENTRYSTART; 
 update the self address in the xtpage header 
 linelock for the after image of the whole page 
 update the buffer extent descriptor of target xtpage 
 unpin the target page to new homeward bound 
	
	 	3. acquire maplock for the source extent to be freed;
	 
	  acquire a maplock saving the src relocated extent address;
	  to free of the extent at commit time;
	 if DATAEXT relocation, write a LOG_UPDATEMAP record for
	  free PXD of the source data extent (logredo() will update
	  bmap for free of source data extent), and update bmap for
	  free of the source data extent;
	 if XTPAGE relocation, write a LOG_NOREDOPAGE record
	  for the source xtpage (logredo() will init NoRedoPage
	  filter and will also update bmap for free of the source
	  xtpage), and update bmap for free of the source xtpage;
	  N.B. We use tlckMAP instead of tlkcXTREE because there
	       is no buffer associated with this lock since the buffer
	       has been redirected to the target location.
 (xtype == XTPAGE) 
	
	 	4. update the parent xad entry for relocation;
	 
	  acquire tlck for the parent entry with XAD_NEW as entry
	  update which will write LOG_REDOPAGE and update bmap for
	  allocation of XAD_NEW destination extent;
 update the XAD with the new destination extent; 
 unpin the parent xtpage 
 	xtSearchNode()
  function:	search for the internal xad entry covering specified extent.
 		This function is mainly used by defragfs utility.
  parameters:
 	ip	- file object;
 	xad	- extent to find;
 	cmpp	- comparison result:
 	btstack - traverse stack;
 	flag	- search process flag;
  returns:
 	btstack contains (bn, index) of search path traversed to the entry.
 	cmpp is set to result of comparison with the entry returned.
 	the page containing the entry is pinned at exit.
 required XAD entry 
 init for empty page 
 block number 
 meta-page buffer 
 page 
	
	 	search down tree from root:
	 
	  between two consecutive entries of <Ki, Pi> and <Kj, Pj> of
	  internal page, child page Pi contains entry with k, Ki <= K < Kj.
	 
	  if entry with search key K is not found
	  internal page search find the entry with largest key Ki
	  less than K which point to the child page to search;
	  leaf page search find the entry with smallest key Kj
	  greater than K so that the returned index is the position of
	  the entry to be shifted right for insertion of new entry.
	  for empty tree, search key is greater than any key of the tree.
	 
	  by convention, root bn = 0.
 getpin the page to search 
		
		  binary search with search key K on the current page
				
				 	search hit
				 
				  verify for exact match;
 save search result 
 descendsearch its child page 
		
		 	search miss - non-leaf page:
		 
		  base is the smallest index with key (Kj) greater than
		  search key (K) and may be zero or maxentry index.
		  if base is non-zero, decrement base by one to get the parent
		  entry of the child page to search.
		
		  go down to child page
 get the child page block number 
 unpin the parent page 
 	xtRelink()
  function:
 	link around a freed page.
  Parameter:
 	int		tid,
 	struct inode	ip,
 	xtpage_t	p)
  returns:
 update prev pointer of the next page 
		
		  acquire a transaction lock on the page;
		 
		  action: update prev pointer;
 the page may already have been tlock'd 
 update next pointer of the previous page 
		
		  acquire a transaction lock on the page;
		 
		  action: update next pointer;
 the page may already have been tlock'd 
  _STILL_TO_PORT 
 	xtInitRoot()
  initialize file root (inline in inode)
	
	  acquire a transaction lock on the root
	 
	  action:
  We can run into a deadlock truncating a file with a large number of
  xtree pages (large fragmented file).  A robust fix would entail a
  reservation system where we would reserve a number of metadata pages
  and tlocks which we would be guaranteed without a deadlock.  Without
  this, a partial fix is to limit number of metadata pages we will lock
  in a single transaction.  Currently we will truncate the file so that
  no more than 50 leaf pages will be locked.  The caller of xtTruncate
  will be responsible for ensuring that the current transaction gets
  committed, and that subsequent transactions are created to truncate
  the file further if needed.
 	xtTruncate()
  function:
 	traverse for truncation logging backward bottom up;
 	terminate at the last extent entry at the current subtree
 	root page covering new down size.
 	truncation may occur within the last extent entry.
  parameter:
 	int		tid,
 	struct inode	ip,
 	s64		newsize,
 	int		type)	{PWMAP, PMAP, WMAP; DELETE, TRUNCATE}
  return:
  note:
 	PWMAP:
 	 1. truncate (non-COMMIT_NOLINK file)
 	    by jfs_truncate() or jfs_open(O_TRUNC):
 	    xtree is updated;
 	 2. truncate index table of directory when last entry removed
 	map update via tlock at commit time;
 	PMAP:
 	 Call xtTruncate_pmap instead
 	WMAP:
 	 1. remove (free zero link count) on last reference release
 	    (pmap has been freed at commit zero link count);
 	 2. truncate (COMMIT_NOLINK file, i.e., tmp file):
 	    xtree is updated;
 	 map update directly at truncation time;
 	if (DELETE)
 		no LOG_NOREDOPAGE is required (NOREDOFILE is sufficient);
 	else if (TRUNCATE)
 		must write LOG_NOREDOPAGE for deleted index page;
  pages may already have been tlocked by anonymous transactions
  during file growth (i.e., write) before truncation;
  except last truncated entry, deleted entries remains as is
  in the page (nextindex is updated) for other use
  (e.g., logupdate allocation map): this avoid copying the page
  info but delay free of pages;
 maplock for COMMIT_WMAP 
 maplock for COMMIT_WMAP 
 save object truncation type 
	
	  if the newsize is not an integral number of pages,
	  the file between newsize and next page boundary will
	  be cleared.
	  if truncating into a file hole, it will cause
	  a full block to be allocated for the logical block.
	
	  release page blocks of truncated region <teof, eof>
	 
	  free the data blocks from the leaf index blocks.
	  delete the parent index entries corresponding to
	  the freed child dataindex blocks.
	  free the index blocks themselves which aren't needed
	  in new sized file.
	 
	  index blocks are updated only if the blocks are to be
	  retained in the new sized file.
	  if type is PMAP, the data and index pages are NOT
	  freed, and the data and index blocks are NOT freed
	  from working map.
	  (this will allow continued access of dataindex of
	  temporary file (zerolink count file truncated to zero-length)).
 clear stack 
	
	  start with root
	 
	  root resides in the inode
	
	  first access of each page:
 process entries backward from last index 
	 Since this is the rightmost page at this level, and we may have
	  already freed a page that was formerly to the right, let's make
	  sure that the next pointer is zero.
			
			  Make sure this change to the header is logged.
			  If we really truncate this leaf, the flag
			  will be changed to tlckTRUNCATE
	
	 	leaf page
 does region covered by leaf page precede Teof ? 
 (re)acquire tlock of the leaf page 
			
			  We need to limit the size of the transaction
			  to avoid exhausting pagecache & tlocks
	
	  scan backward leaf page entries
		
		  The "data" for a directory is indexed by the block
		  device's address space.  This metadata must be invalidated
		  here
		
		  entry beyond eof: continue scan of current page
		           xad
		  ---|---=======------->
		    eof
		
		  (xoff <= teof): last entry to be deleted from page;
		  If other entries remain in page: keep and update the page.
		
		  eof == entry_start: delete the entry
		            xad
		  -------|=======------->
		        eof
		 
		
		  eof within the entry: truncate the entry.
		           xad
		  -------===|===------->
		           eof
 update truncated entry 
 save pxd of truncated extent in tlck 
 COMMIT_PWMAP 
 free truncated extent 
 COMMIT_WMAP 
 reset map lock 
 current entry is new last entry; 
		
		  eof beyond the entry:
		           xad
		  -------=======---|--->
		                  eof
 (xoff + xlen < teof) 
 COMMIT_WAMP 
 assert(freed == 0); 
 end scan of leaf page entries 
	
	  leaf page become empty: free the page if type != PMAP
 COMMIT_PWMAP 
		 txCommit() with tlckFREE:
		  free data extents covered by leaf [XTENTRYSTART:hwm);
		  invalidate leaf if COMMIT_PWMAP;
		  if (TRUNCATE), will write LOG_NOREDOPAGE;
 COMMIT_WAMP 
 free data extents covered by leaf 
 debug 
 COMMIT_PWMAP 
			 page will be invalidated at tx completion
 COMMIT_WMAP 
 invalidate empty leaf page 
	
	  the leaf page become empty: delete the parent entry
	  for the leaf page if the parent page is to be kept
	  in the new sized file.
	
	  go back up to the parent page
 poprestore parent entry for the current child page 
 current page must have been root 
 get back the parent page 
	
	  child page was not empty:
 has any entry deleted from parent ? 
 (re)acquire tlock on the parent page 
 COMMIT_PWMAP 
				 txCommit() with tlckTRUNCATE:
				  free child extents covered by parent [);
 COMMIT_WMAP 
 free child extents covered by parent 
	
	  child page was empty:
	
	  During working map update, child page's tlock must be handled
	  before parent's.  This is because the parent's tlock will cause
	  the child's disk space to be marked available in the wmap, so
	  it's important that the child page be released by that time.
	 
	  ToDo:  tlocks should be on doubly-linked list, so we can
	  quickly remove it and add it to the end.
	
	  Move parent page's tlock to the end of the tid's tlock list
	
	  parent page become empty: free the page
 COMMIT_PWMAP 
			 txCommit() with tlckFREE:
			  free child extents covered by parent;
			  invalidate parent if COMMIT_PWMAP;
 COMMIT_WMAP 
 free child extents covered by parent 
				
				  Shrink root down to allow inline
				  EA (otherwise fsck complains)
 debug 
 COMMIT_PWMAP 
				 page will be invalidated at tx completion
 COMMIT_WMAP 
 invalidate parent page 
			 parent has become empty and freed:
			  go back up to its parent page
 freed = 1; 
	
	  parent page still has entries for front region;
		 try truncate region covered by preceding entry
		  (process backward)
		 go back down to the child page corresponding
		  to the entry
	
	 	internal page: go down to child page of current entry
 save current parent entry for the child page 
 get child page 
	
	  first access of each internal entry:
 release parent page 
 process the child page 
	
	  update file resource stat
	 set size
 fsck hates zero-length directories 
 update quota allocation to reflect freed blocks 
	
	  free tlock of invalidated pages
 	xtTruncate_pmap()
  function:
 	Perform truncate to zero length for deleted file, leaving the
 	xtree and working map untouched.  This allows the file to
 	be accessed via open file handles, while the delete of the file
 	is committed to disk.
  parameter:
 	tid_t		tid,
 	struct inode	ip,
 	s64		committed_size)
  return: new committed size
  note:
 	To avoid deadlock by holding too many transaction locks, the
 	truncation may be broken up into multiple transactions.
 	The committed_size keeps track of part of the file has been
 	freed from the pmaps.
 save object truncation type 
 clear stack 
		
		  start with root
		 
		  root resides in the inode
		
		  first access of each page:
 process entries backward from last index 
	
	 	leaf page
		
		  We need to limit the size of the transaction
		  to avoid exhausting pagecache & tlocks
	
	  go back up to the parent page
 poprestore parent entry for the current child page 
 current page must have been root 
 get back the parent page 
	
	  parent page become empty: free the page
		 txCommit() with tlckFREE:
		  free child extents covered by parent;
		  invalidate parent if COMMIT_PWMAP;
	
	  parent page still has entries for front region;
	
	 	internal page: go down to child page of current entry
 save current parent entry for the child page 
 get child page 
	
	  first access of each internal entry:
 release parent page 
 process the child page 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2005
    Portions Copyright (C) Christoph Hellwig, 2001-2002
 # of page allocations 
 # of page frees 
 # of sleeping lock_metapage() calls 
  Must have mp->page locked
 log2 blocks per metapage 
	
	  Allocate the metapage structures
  Metapage address space operations
 else no mapping 
  This can race.  Recheck that log hasn't been set to null, and after
  acquiring logsync lock, recheck lsn
		
		  I'd like to call drop_metapage here, but I don't think it's
		  safe unless I have the page locked
 block offset of mp within page 
			
			  Make sure this page isn't blocked indefinitely.
			  If the journal isn't undergoing IO, push it
 Contiguous, in memory & on disk 
 Not contiguous 
			
			  Increment counter before submitting io to keep
			  count from hitting zero before we're through
			
			  We already called inc_io(), but can't cancel it
			  with dec_io() until we're done with the page
 Don't call bio_add_page yet, we may add to this vec 
 We should never reach here, since we're only adding one vec 
 address of page in fs blocks 
		
		  If an nfs client tries to read an inode that is larger
		  than any existing inodes, we may try to read past the
		  end of the inode map
 Someone else will release this 
 write_one_page unlocks the page 
 discard_metapage doesn't remove it 
 Try to keep metapages from using up too much memory 
 All callers are interested in block device's mapping 
	
	  Mark metapages to discard.  They will eventually be
	  released, but should not be written.
 SPDX-License-Identifier: GPL-2.0
  linuxfsjfsioctl.c
  Copyright (C) 2006 Herbert Poetzl
  adapted from Remy Card's ext2ioctl.c
 Is it quota file? Do not allow user to mess with it 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
    Portions Copyright (C) Christoph Hellwig, 2001-2002
  forward references
  NAME:	free_ea_wmap(inode)
  FUNCTION:	free uncommitted extended attributes from working map
 free EA pages from cache 
  NAME:	jfs_create(dip, dentry, mode)
  FUNCTION:	create a regular file in the parent directory <dip>
 		with name = <from dentry> and mode = <mode>
  PARAMETER:	dip	- parent directory vnode
 		dentry	- dentry of new file
 		mode	- create mode (rwxrwxrwx).
 		nd- nd struct
  RETURN:	Errors from subroutines
 transaction id 
 child directory inode 
 child directory name 
	
	  search parent directory for entryfreespace
	  (dtSearch() returns parent directory page pinned)
	
	  Either iAlloc() or txBegin() may block.  Deadlock can occur if we
	  block there while holding dtree page, so we allocate the inode &
	  begin the transaction before we search the directory.
	
	  initialize the child XAD tree root in-line in inode
	
	  create entry in parent directory for child directory
	  (dtInsert() releases parent directory page)
 Marks Filesystem dirty 
 Filesystem full 
  NAME:	jfs_mkdir(dip, dentry, mode)
  FUNCTION:	create a child directory in the parent directory <dip>
 		with name = <from dentry> and mode = <mode>
  PARAMETER:	dip	- parent directory vnode
 		dentry	- dentry of child directory
 		mode	- create mode (rwxrwxrwx).
  RETURN:	Errors from subroutines
  note:
  EACCES: user needs search+write permission on the parent directory
 transaction id 
 child directory inode 
 child directory name 
	
	  search parent directory for entryfreespace
	  (dtSearch() returns parent directory page pinned)
	
	  Either iAlloc() or txBegin() may block.  Deadlock can occur if we
	  block there while holding dtree page, so we allocate the inode &
	  begin the transaction before we search the directory.
	
	  initialize the child directory in-line in inode
	
	  create entry in parent directory for child directory
	  (dtInsert() releases parent directory page)
 Marks Filesystem dirty 
 Filesystem full 
 for '.' 
 update parent directory inode 
 for '..' from child directory 
  NAME:	jfs_rmdir(dip, dentry)
  FUNCTION:	remove a link to child directory
  PARAMETER:	dip	- parent inode
 		dentry	- child directory dentry
  RETURN:	-EINVAL	- if name is . or ..
 		-EINVAL - if . or .. exist but are invalid.
 		errors from subroutines
  note:
  if other threads have the directory open when the last link
  is removed, the "." and ".." entries, if present, are removed before
  rmdir() returns and no new entries may be created in the directory,
  but the directory is not removed until the last reference to
  the directory is released (cf.unlink() of regular file).
 transaction id 
 Init inode for quota operations. 
 directory must be empty to be removed 
	
	  delete the entry of target directory from parent directory
	 update parent directory's link count corresponding
	  to ".." entry of the target directory deleted
	
	  OS2 could have created EA andor ACL
 free EA from both persistent and working map 
 free EA pages 
 free ACL from both persistent and working map 
 free ACL pages 
 mark the target directory as deleted 
	
	  Truncating the directory index table is not guaranteed.  It
	  may need to be done iteratively
  NAME:	jfs_unlink(dip, dentry)
  FUNCTION:	remove a link to object <vp> named by <name>
 		from parent directory <dvp>
  PARAMETER:	dip	- inode of parent directory
 		dentry	- dentry of object to be removed
  RETURN:	errors from subroutines
  note:
  temporary file: if one or more processes have the file open
  when the last link is removed, the link will be removed before
  unlink() returns, but the removal of the file contents will be
  postponed until all references to the files are closed.
  JFS does NOT support unlink() on directories.
 transaction id 
 object name 
 Init inode for quota operations. 
	
	  delete the entry of target file from parent directory
 Marks FS Dirty 
 update target's inode 
	
	 	commit zero link count object
 free block resources 
 Marks FS Dirty 
	
	  Incomplete truncate of file data can
	  result in timing problems unless we synchronously commit the
	  transaction.
	
	  If xtTruncate was incomplete, commit synchronously to avoid
	  timing complications
 Marks FS Dirty 
	
	  Truncating the directory index table is not guaranteed.  It
	  may need to be done iteratively
  NAME:	commitZeroLink()
  FUNCTION:	for non-directory, called by jfs_remove(),
 		truncate a regular file, directory or symbolic
 		link to zero length. return 0 if type is not
 		one of these.
 		if the file is currently associated with a VM segment
 		only permanent disk and inode map resources are freed,
 		and neither the inode nor indirect blocks are modified
 		so that the resources can be later freed in the work
 		map by ctrunc1.
 		if there is no VM segment on entry, the resources are
 		freed in both work and permanent map.
 		(? for temporary file - memory object is cached even
 		after no reference:
 		reference count > 0 -   )
  PARAMETERS:	cd	- pointer to commit data structure.
 			  current inode is the one to truncate.
  RETURN:	Errors from subroutines
 fast symbolic link 
 mark transaction of block map update type 
	
	  free EA
 acquire maplock on EA to be freed from block map 
	
	  free ACL
 acquire maplock on EA to be freed from block map 
	
	  free xtreedata (truncate to zero length):
	  free xtreedata pages from cache if COMMIT_PWMAP,
	  free xtreedata blocks from persistent block map, and
	  free xtreedata blocks from working block map if COMMIT_PWMAP;
  NAME:	jfs_free_zero_link()
  FUNCTION:	for non-directory, called by iClose(),
 		free resources of a file from cache and WORKING map
 		for a file previously committed with zero link count
 		while associated with a pager object,
  PARAMETER:	ip	- pointer to inode of file.
	 return if not reg or symbolic link or if size is
	  already ok.
 if its contained in inode nothing to do 
	
	  free EA
 maplock for COMMIT_WMAP 
 maplock for COMMIT_WMAP 
 free EA pages from cache 
 free EA extent from working block map 
	
	  free ACL
 maplock for COMMIT_WMAP 
 maplock for COMMIT_WMAP 
 free ACL extent from working block map 
	
	  free xtreedata (truncate to zero length):
	  free xtreedata pages from cache, and
	  free xtreedata blocks from working block map;
  NAME:	jfs_link(vp, dvp, name, crp)
  FUNCTION:	create a link to <vp> by the name = <name>
 		in the parent directory <dvp>
  PARAMETER:	vp	- target object
 		dvp	- parent directory of new link
 		name	- name of new link to target object
 		crp	- credential
  RETURN:	Errors from subroutines
  note:
  JFS does NOT support link() on directories (to prevent circular
  path in the directory hierarchy);
  EPERM: the target object is a directory, and either the caller
  does not have appropriate privileges or the implementation prohibits
  using link() on directories [XPG4.2].
  JFS does NOT support links between file systems:
  EXDEV: target object and new link are on different file systems and
  implementation does not support links between file systems [XPG4.2].
	
	  scan parent directory for entryfreespace
	
	  create entry for new link in parent directory
 update object inode 
 for new link 
 never instantiated 
  NAME:	jfs_symlink(dip, dentry, name)
  FUNCTION:	creates a symbolic link to <symlink> by name <name>
 			in directory <dip>
  PARAMETER:	dip	- parent directory vnode
 		dentry	- dentry of symbolic link
 		name	- the path name of the existing object
 			  that will be the source of the link
  RETURN:	errors from subroutines
  note:
  ENAMETOOLONG: pathname resolution of a symbolic link produced
  an intermediate result whose length exceeds PATH_MAX [XPG4.2]
 source pathname size 
	
	  search parent directory for entryfreespace
	  (dtSearch() returns parent directory page pinned)
	
	  allocate on-diskin-memory inode for symbolic link:
	  (iAlloc() returns new, locked inode)
	 fix symlink access permission
	  (dir_create() ANDs in the u.u_cmask,
	  but symlinks really need to be 777 access)
	
	  write symbolic link target path name
	
	  write source path name inline in on-disk inode (fast symbolic link)
		
		  if symlink is > 128 bytes, we don't have the space to
		  store inline extended attributes
	
	  write source path name in a single extent
		
		  even though the data of symlink object (source
		  path name) is treated as non-journaled user data,
		  it is readwritten thru buffer cache for performance.
 This is kind of silly since PATH_MAX == 4K 
	
	  create entry for symbolic link in parent directory
 discard new inode 
	
	  commit update of parent directory and link object
  NAME:	jfs_rename
  FUNCTION:	rename a file or directory
	
	  Make sure source inode number is what we think it is
	
	  Make sure dest inode number (if any) is what we think it is
 no entry exists, but one was expected 
 Init inode for quota operations. 
	
	  The real work starts here
	
	  How do we know the locking is safe from deadlocks?
	  The vfs does the hard part for us.  Any time we are taking nested
	  commit_mutexes, the vfs already has i_mutex held on the parent.
	  Here, the vfs has already taken i_mutex on both old_dir and new_dir.
		
		  Change existing directory entry to new inode number
 free block resources 
 Marks FS Dirty 
		
		  Add new directory entry
	
	  Remove old directory entry
 Marks Filesystem dirty 
			
			  Change inode number of parent for moved directory
 Linelock header of dtree 
	
	  Update ctime on changedmoved inodes & mark dirty
 Build list of inodes modified by this transaction 
	
	  Incomplete truncate of file data can
	  result in timing problems unless we synchronously commit the
	  transaction.
	
	  Truncating the directory index table is not guaranteed.  It
	  may need to be done iteratively
  NAME:	jfs_mknod
  FUNCTION:	Create a special file (device)
	
	  This is not negative dentry. Always valid.
	 
	  Note, rename() to existing directory entry will have ->d_inode,
	  and will use existing name which isn't specified name by user.
	 
	  We may be able to drop this positive dentry here. But dropping
	  positive dentry isn't good idea. So it's unsupported like
	  rename("filename", "FILENAME") for now.
	
	  This may be nfsd (or something), anyway, we can't see the
	  intent of this. So, since this can be for creation, drop it.
	
	  Drop the negative dentry, in order to make sure to use the
	  case sensitive name which is specified by user if this is
	  for creation.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
    Portions Copyright (C) Christoph Hellwig, 2001-2002
 nothing is done for continue beyond marking the superblock dirty 
	
	  If we really return the number of allocated & free inodes, some
	  applications will fail because they won't see enough free inodes.
	  We'll try to calculate some guess as to how many inodes we can
	  really allocate
	 
	  buf->f_files = atomic_read(&imap->im_numinos);
	  buf->f_ffree = atomic_read(&imap->im_numfree);
 -1: no change;  NULL: none 
 Silently ignore the quota options 
 Don't do anything ;-) 
			 if set to 1, even copying files will cause
			  trimming :O
			  -> user has more control over the online trimming
 Discard old (if remount) 
		
		  Invalidate any previously read metadata.  fsck may have
		  changed the on-disk data since we mounted ro
 mark the fs rw for quota activity 
 initialize the mount flag and determine the default error handler 
	
	  Initialize blocksize to 4K.
	
	  Set method vectors.
	
	  Initialize direct-mapping inodeaddress-space
	 logical blocks are represented by 40 bits in pxd_t, etc.
	  and page cache is indexed by long
 let operations fail rather than hang 
			
			  Don't fail here. Everything succeeded except
			  marking the superblock clean, so there's really
			  no harm in leaving it frozen for now.
 log == NULL indicates read-only mount 
		
		  Write quota structures to quota file, sync_blockdev() will
		  write them to disk later
 Read data from quotafile - avoid pagecache and such because we cannot afford
  acquiring the locks... As quota files are never truncated and quota code
  itself serializes the operations (and no one else should touch the files)
 A hole? 
 Write to quotafile 
	
	  Metapage initialization
	
	  Transaction Manager initialization
	
	  IO completion thread (endio)
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
  Module: jfs_mount.c
  note: file system in transition to aggregatefileset:
  file system mount is interpreted as the mount of aggregate,
  if not already mounted, and mount of the singleonly fileset in
  the aggregate;
  a file systemaggregate is represented by an internal inode
  (aka mount inode) initialized with aggregate superblock;
  each vfs represents a fileset, and points to its "fileset inode
  allocation map inode" (aka fileset inode):
  (an aggregate itself is structured recursively as a filset:
  an internal vfs is constructed and points to its "fileset inode
  allocation map inode" (aka aggregate inode) where each inode
  represents a fileset inode) so that inode number is mapped to
  on-disk inode in uniform way at both aggregate and fileset level;
  each vnodeinode of a fileset is linked to its vfs (to facilitate
  per fileset inode operations, e.g., unmount of a fileset, etc.);
  each inode points to the mount inode (to facilitate access to
  per aggregate information, e.g., block size, etc.) as well as
  its file set inode.
    aggregate
    ipmnt
    mntvfs -> fileset ipimap+ -> aggregate ipbmap -> aggregate ipaimap;
              fileset vfs     -> vp(1) <-> ... <-> vp(n) <->vproot;
  forward references
  NAME:	jfs_mount(sb)
  FUNCTION:	vfs_mount()
  PARAMETER:	sb	- super block
  RETURN:	-EBUSY	- device already mounted or open for write
 		-EBUSY	- cvrdvp already mounted;
 		-EBUSY	- mount table full
 		-ENOTDIR- cvrdvp not directory on a device mount
 		-ENXIO	- device open failure
 Return code 
	
	  readvalidate superblock
	  (initialize mount inode from the superblock)
	
	  initialize aggregate inode allocation map
	
	  open aggregate block allocation map
	
	  initialize aggregate block allocation map
	
	  open the secondary aggregate inode allocation map
	 
	  This is a duplicate of the aggregate inode allocation map.
	 
	  hand craft a vfs in the same fashion as we did to read ipaimap.
	  By adding INOSPEREXT (32) to the inode number, we are telling
	  diReadSpecial that we are reading from the secondary aggregate
	  inode table.  This also creates a unique entry in the inode hash
	  table.
		
		  initialize secondary aggregate inode allocation map
 Secondary aggregate inode table is not valid 
	
	 	mount (the onlysingle) fileset
	
	  open fileset inode allocation map (aka fileset inode)
 open fileset secondary inode allocation map 
 map further access of per fileset inodes by the fileset inode 
 initialize fileset inode allocation map 
	
	 	unwind on error
 close fileset inode allocation map inode 
 close secondary aggregate inode allocation map 
 close aggregate inodes 
 close aggregate block allocation map 
 close aggregate inodes 
 close aggregate inode allocation map 
 close aggregate inodes 
  NAME:	jfs_mount_rw(sb, remount)
  FUNCTION:	Completes read-write mount, or remounts read-only volume
 		as read-write
	
	  If we are re-mounting a previously read-only volume, we want to
	  re-read the inode and block maps, since fsck.jfs may have updated
	  them.
	
	  openinitialize log
	
	  update file system superblock;
	
	  write MOUNT log record of the file system
 	chkSuper()
  validate the superblock of the file system to be mounted and
  get the file system parameters.
  returns
 	0 with fragsize set if check successful
 	error code if not successful
	
	  validate superblock
 validate fs signature 
 _JFS_4K 
 validate the descriptors for Secondary AIM and AIT 
 validate fs state 
	
	  JFS always does IO by 4K pages.  Don't tell the buffer cache
	  that we use anything else (leave s_blocksize alone).
 check some fields for possible corruption 
	
	  For now, ignore s_pbsize, l2bfactor.  All IO going through buffer
	  cache.
 	updateSuper()
  update synchronously superblock if it is mounted read-write.
 record log's dev_t and mount serial number 
		
		  If this volume is shared with OS2, OS2 will need to
		  recalculate DASD usage, since we don't deal with it.
 	readSuper()
  read superblock by raw sector address
 read in primary superblock 
 read in secondaryreplicated superblock 
 	logMOUNT()
  function: write a MOUNT log record for file system.
  MOUNT record keeps logredo() from processing log records
  for this file system past this point in log.
  it is harmless if mount fails.
  note: MOUNT record is at aggregate level, not at fileset level,
  since log records of previous mounts of a fileset
  (e.g., AFTER record of extent allocation) have to be processed
  to update block allocation map at aggregate level.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines  Corp., 2000-2004
    Copyright (C) Christoph Hellwig, 2002
 	jfs_xattr.c: extended attribute service
  Overall design --
  Format:
    Extended attribute lists (jfs_ea_list) consist of an overall size (32 bit
    value) and a variable (0 or more) number of extended attribute
    entries.  Each extended attribute entry (jfs_ea) is a <name,value> double
    where <name> is constructed from a null-terminated ascii string
    (1 ... 255 bytes in the name) and <value> is arbitrary 8 bit data
    (1 ... 65535 bytes).  The in-memory format is
    0       1        2        4                4 + namelen + 1
    +-------+--------+--------+----------------+-------------------+
    | Flags | Name   | Value  | Name String \0 | Data . . . .      |
    |       | Length | Length |                |                   |
    +-------+--------+--------+----------------+-------------------+
    A jfs_ea_list then is structured as
    0            4                   4 + EA_SIZE(ea1)
    +------------+-------------------+--------------------+-----
    | Overall EA | First FEA Element | Second FEA Element | .....
    | List Size  |                   |                    |
    +------------+-------------------+--------------------+-----
    On-disk:
 	FEALISTs are stored on disk using blocks allocated by dbAlloc() and
 	written directly. An EA list may be in-lined in the inode if there is
 	sufficient room available.
 Indicates what storage xattr points to 
 largest xattr that fits in current buffer 
 dxd to replace ea when modifying xattr 
 metapage containing ea list 
 buffer containing ea list 
  ea_buffer.flag values
  Mapping of on-disk attribute names: for on-disk attribute names with an
  unknown prefix (not "system.", "user.", "security.", or "trusted."), the
  prefix "os2." is prepended.  On the way back to disk, "os2." prefixes are
  stripped and we make sure that the remaining name does not start with one
  of the know prefixes.
 Forward references 
  NAME: ea_write_inline
  FUNCTION: Attempt to write an EA inline if area is available
  PRE CONDITIONS:
 	Already verified that the specified EA is small enough to fit inline
  PARAMETERS:
 	ip	- Inode pointer
 	ealist	- EA list pointer
 	size	- size of ealist in bytes
 	ea	- dxd_t structure to be filled in with necessary EA information
 		  if we successfully copy the EA inline
  NOTES:
 	Checks if the inode's inline area is available.  If so, copies EA inline
 	and sets <ea> fields appropriately.  Otherwise, returns failure, EA will
 	have to be put into an extent.
  RETURNS: 0 for successful copy to inline area; -1 if area not available
	
	  Make sure we have an EA -- the NULL EA list is valid, but you
	  can't copy it!
		
		  See if the space is available or if it is already being
		  used for an inline EA.
 Free up INLINE area 
  NAME: ea_write
  FUNCTION: Write an EA for an inode
  PRE CONDITIONS: EA has been verified
  PARAMETERS:
 	ip	- Inode pointer
 	ealist	- EA list pointer
 	size	- size of ealist in bytes
 	ea	- dxd_t structure to be filled in appropriately with where the
 		  EA was copied
  NOTES: Will write EA inline if able to, otherwise allocates blocks for an
 	extent and synchronously writes it to those blocks.
  RETURNS: 0 for success; Anything else indicates failure
	
	  Quick check to see if this is an in-linable EA.  Short EAs
	  and empty EAs are all in-linable, provided the space exists.
 figure out how many blocks we need 
 Allocate new blocks to quota. 
Rollback quota allocation. 
	
	  Now have nblocks worth of storage to stuff into the FEALIST.
	  loop over the FEALIST copying data into the buffer one page at
	  a time.
		
		  Determine how many bytes for this request, and round up to
		  the nearest aggregate block size
		
		  We really need a way to propagate errors for
		  forced writes like this one.  --hch
		 
		  (__write_metapage => release_metapage => flush_metapage)
			
			  the write failed -- this means that the buffer
			  is still assigned and the blocks are not being
			  used.  this seems like the best error recovery
			  we can get ...
 Free up INLINE area 
 Rollback quota allocation. 
  NAME: ea_read_inline
  FUNCTION: Read an inlined EA into user's buffer
  PARAMETERS:
 	ip	- Inode pointer
 	ealist	- Pointer to buffer to fill in with EA
  RETURNS: 0
 Sanity Check 
  NAME: ea_read
  FUNCTION: copy EA data into user's buffer
  PARAMETERS:
 	ip	- Inode pointer
 	ealist	- Pointer to buffer to fill in with EA
  NOTES:  If EA is inline calls ea_read_inline() to copy EA.
  RETURNS: 0 for success; other indicates failure
 quick check for in-line EA 
	
	  Figure out how many blocks were allocated when this EA list was
	  originally written to disk.
	
	  I have found the disk blocks which were originally used to store
	  the FEALIST.  now i loop over each contiguous block copying the
	  data into the buffer.
		
		  Determine how many bytes for this request, and round up to
		  the nearest aggregate block size
  NAME: ea_get
  FUNCTION: Returns buffer containing existing extended attributes.
 	     The size of the buffer will be the larger of the existing
 	     attributes size, or min_size.
 	     The buffer, which may be inlined in the inode or in the
 	     page cache must be release by calling ea_release or ea_put
  PARAMETERS:
 	inode	- Inode pointer
 	ea_buf	- Structure to be populated with ealist and its metadata
 	min_size- minimum size of buffer to be returned
  RETURNS: 0 for success; Other indicates failure
 When fsck.jfs clears a bad ea, it doesn't clear the size 
		
		  To keep the rest of the code simple.  Allocate a
		  contiguous buffer to work with. Make the buffer large
		  enough to make use of the whole extent.
 Allocate new blocks to quota. 
 Rollback quota allocation 
 We have already allocated a new dxd 
 ->xattr must point to original ea's metapage 
 If old blocks exist, they must be removed from quota allocation. 
		
		  We need to allocate more space for merged ea list.
		  We should only have loop to again: once.
 Remove old ea of the same name 
 number of bytes following target EA 
 Add new entry to the end 
 Completely new ea list 
		
		  The size of EA value is limitted by on-disk format up to
		   __le16, there would be an overflow if the size is equal
		  to XATTR_SIZE_MAX (65536).  In order to avoid this issue,
		  we can pre-checkup the value size against USHRT_MAX, and
		  return -E2BIG in this case, which is consistent with the
		  VFS setxattr interface.
 DEBUG - If we did this right, these number match 
	
	  If we're left with an empty list, there's no ea
 Find the named attribute 
 Found it 
  No special permissions are needed to list attributes except for trusted.
 compute required size of list 
 Copy attribute names to buffer 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
    Portions Copyright (C) Christoph Hellwig, 2001-2002
			
			  The inline data should be null-terminated, but
			  don't let on-disk corruption crash the kernel
  Workhorse of both fsync & write_inode
	
	  Don't commit if inode has been committed since last being
	  marked dirty, or if it has been deleted.
		 kernel allows writes to devices on read-only
		  partitions and may think inode is dirty
	
	  Retest inode state after taking commit_mutex
	
	  If COMMIT_DIRTY is not set, the inode isn't really dirty.
	  It has been committed since the last change, but was still
	  on the dirty inode list.
 Make sure committed changes hit the disk 
			
			  Free the inode from the quota allocation.
			 kernel allows writes to devices on read-only
			  partitions and may try to mark inode dirty
	
	  Take appropriate lock on inode
				
				  Allocated but not recorded, read treats
				  this as a hole
 _JFS_4K 
			
			  As long as block size = 4K, this isn't a problem.
			  We should mark the whole page not ABNR, but how
			  will we know to mark the other blocks BH_New?
 _JFS_4K 
	
	  Allocate a new block
 _JFS_4K 
	
	  We need to do whatever it takes to keep all but the last buffers
	  in 4K pages - see jfs_write.c
 _JFS_4K 
	
	  Release lock on inode
	
	  In case of error extending write may have instantiated a few
	  blocks outside i_size. Trim these off again.
  Guts of jfs_truncate.  Called with locks already held.  Can be called
  with directory for truncating directory index table.
		
		  The commit_mutex cannot be taken before txBegin.
		  txBegin may block and there is a chance the inode
		  could be marked dirty and need to be committed
		  before txBegin unblocks
 Truncate isn't always atomic 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
 	jfs_imap.c: inode allocation map manager
  Serialization:
    Each AG has a simple lock which is used to control the serialization of
 	the AG level lists.  This lock should be taken first whenever an AG
 	level list will be modified or accessed.
    Each IAG is locked by obtaining the buffer for the IAG page.
    There is also a inode lock for the inode map inode.  A read lock needs to
 	be taken whenever an IAG is read from the map or the global level
 	information is read.  A write lock needs to be taken whenever the global
 	level information is modified or an atomic operation needs to be used.
 	If more than one IAG is read at one time, the read lock may not
 	be given up until all of the IAG's are read.  Otherwise, a deadlock
 	may occur when trying to obtain the read lock while another thread
 	holding the read lock is waiting on the IAG already being held.
    The control page of the inode map is read into memory by diMount().
 	Thereafter it should only be modified in memory and then it will be
 	written out when the filesystem is unmounted by diUnmount().
  imap locks
 iag free list lock 
 per ag iag list locks 
  forward references
  NAME:	diMount()
  FUNCTION:	initialize the incore inode map control structures for
 		a fileset or aggregate init time.
 		the inode map's control structure (dinomap) is
 		brought in from disk and placed in virtual memory.
  PARAMETERS:
 	ipimap	- pointer to inode map inode for the aggregate or fileset.
  RETURN VALUES:
 	0	- success
 	-ENOMEM	- insufficient free virtual memory.
 	-EIO	- io error.
	
	  allocateinitialize the in-memory inode map control structure
 allocate the in-memory inode map control structure. 
 read the on-disk inode map control structure. 
 copy the on-disk version to the in-memory version. 
 release the buffer. 
	
	  allocateinitialize inode allocation map locks
 allocate and init iag free list lock 
 allocate and init ag list locks 
	 bind the inode map inode and inode map control structure
	  to each other.
  NAME:	diUnmount()
  FUNCTION:	write to disk the incore inode map control structures for
 		a fileset or aggregate at unmount time.
  PARAMETERS:
 	ipimap	- pointer to inode map inode for the aggregate or fileset.
  RETURN VALUES:
 	0	- success
 	-ENOMEM	- insufficient free virtual memory.
 	-EIO	- io error.
	
	  update the on-disk inode map control structure
	
	  Invalidate the page cache buffers
	
	  free in-memory control structure
 	diSync()
	
	  write imap global conrol page
 read the on-disk inode map control structure 
 copy the in-memory version to the on-disk version 
 write out the control structure 
	
	  write out dirty pages of imap
  NAME:	diRead()
  FUNCTION:	initialize an incore inode from disk.
 		on entry, the specifed incore inode should itself
 		specify the disk inode number corresponding to the
 		incore inode (i.e. i_number should be initialized).
 		this routine handles incore inode initialization for
 		both "special" and "regular" inodes.  special inodes
 		are those required early in the mount process and
 		require special handling since much of the file system
 		is not yet initialized.  these "special" inodes are
 		identified by a NULL inode map inode pointer and are
 		actually initialized by a call to diReadSpecial().
 		for regular inodes, the iag describing the disk inode
 		is read from disk to determine the inode extent address
 		for the disk inode.  with the inode extent address in
 		hand, the page of the extent that contains the disk
 		inode is read and the disk inode is copied to the
 		incore inode.
  PARAMETERS:
 	ip	-  pointer to incore inode to be initialized from disk.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error.
 	-ENOMEM	- insufficient memory
 determine the iag number for this inode (number) 
 read the iag 
 determine inode extent that holds the disk inode 
	 get disk block number of the page within the inode extent
	  that holds the disk inode.
 get the ag for the iag 
		
		  OS2 didn't always align inode extents on page boundaries
 read the page of disk inode 
 locate the disk inode requested 
 copy the disk inode to the in-memory inode 
 set the ag for the inode 
  NAME:	diReadSpecial()
  FUNCTION:	initialize a 'special' inode from disk.
 		this routines handles aggregate level inodes.  The
 		inode cache cannot differentiate between the
 		aggregate inodes and the filesystem inodes, so we
 		handle these here.  We don't actually use the aggregate
 		inode map, since these inodes are at a fixed location
 		and in some cases the aggregate inode map isn't initialized
 		yet.
  PARAMETERS:
 	sb - filesystem superblock
 	inum - aggregate inode number
 	secondary - 1 if secondary aggregate inode table
  RETURN VALUES:
 	new inode	- success
 	NULL		- io error.
 8 inodes per 4K page 
 read the page of fixed disk inode (AIT) in raw mode 
 Don't want iput() deleting it 
 get the pointer to the disk inode of interest 
 8 inodes per 4K page 
 copy on-disk inode to in-memory inode 
 handle bad return by returning NULL for ip 
 Don't want iput() deleting it 
 release the page 
 Allocations to metadata inodes should not affect quotas 
 release the page 
  NAME:	diWriteSpecial()
  FUNCTION:	Write the special inode to disk
  PARAMETERS:
 	ip - special inode
 	secondary - 1 if secondary aggregate inode table
  RETURN VALUES: none
 8 inodes per 4K page 
 read the page of fixed disk inode (AIT) in raw mode 
 get the pointer to the disk inode of interest 
 8 inodes per 4K page 
 copy on-disk inode to in-memory inode 
 write the page 
  NAME:	diFreeSpecial()
  FUNCTION:	Free allocated space for special inode
  NAME:	diWrite()
  FUNCTION:	write the on-disk inode portion of the in-memory inode
 		to its corresponding on-disk inode.
 		on entry, the specifed incore inode should itself
 		specify the disk inode number corresponding to the
 		incore inode (i.e. i_number should be initialized).
 		the inode contains the inode extent address for the disk
 		inode.  with the inode extent address in hand, the
 		page of the extent that contains the disk inode is
 		read and the disk inode portion of the incore inode
 		is copied to the disk inode.
  PARAMETERS:
 	tid -  transacation id
 	ip  -  pointer to incore inode to be written to the inode extent.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error.
	
	  read the page of disk inode containing the specified inode:
 compute the block address of the page 
		
		  OS2 didn't always align inode extents on page boundaries
 read the page of disk inode 
 get the pointer to the disk inode 
	
	  acquire transaction lock on the on-disk inode;
	  N.B. tlock is acquired on ipimap not ip;
	
	  copy btree root from in-memory inode to on-disk inode
	 
	  (tlock is taken from inline B+-tree root in in-memory
	  inode when the B+-tree root is updated, which is pointed
	  by jfs_ip->blid as well as being on tx tlock list)
	 
	  further processing of btree root is based on the copy
	  in in-memory inode, where txLog() will log from, and,
	  for xtree root, txUpdateMap() will update map and reset
	  XAD_NEW bit;
		
		  This is the special xtree inside the directory for storing
		  the directory table
		
		  copy xtree root from inode to dinode:
 reset on-disk (metadata page) xtree XAD_NEW bit 
	
	 	regular file: 16 byte (XAD slot) granularity
		
		  copy xtree root from inode to dinode:
 reset on-disk (metadata page) xtree XAD_NEW bit 
	
	 	directory: 32 byte (directory entry slot) granularity
		
		  copy dtree root from inode to dinode:
	
	  copy inline symlink from in-memory inode to on-disk inode
	
	  copy inline data from in-memory inode to on-disk inode:
	  128 byte slot granularity
	
	 	lockcopy inode base: 128 byte slot granularity
	 release the buffer holding the updated on-disk inode.
	  the buffer will be later written by commit processing.
  NAME:	diFree(ip)
  FUNCTION:	free a specified inode from the inode working map
 		for a fileset or aggregate.
 		if the inode to be freed represents the first (only)
 		free inode within the iag, the iag will be placed on
 		the ag free inode list.
 		freeing the inode will cause the inode extent to be
 		freed if the inode is the only allocated inode within
 		the extent.  in this case all the disk resource backing
 		up the inode extent will be freed. in addition, the iag
 		will be placed on the ag extent free list if the extent
 		is the first free extent in the iag.  if freeing the
 		extent also means that no free inodes will exist for
 		the iag, the iag will also be removed from the ag free
 		inode list.
 		the iag describing the inode will be freed if the extent
 		is to be freed and it is the only backed extent within
 		the iag.  in this case, the iag will be removed from the
 		ag free extent list and ag free inode list and placed on
 		the inode map's free iag list.
 		a careful update approach is used to provide consistency
 		in the face of updates to multiple buffers.  under this
 		approach, all required buffers are obtained before making
 		any updates and are held until all updates are complete.
  PARAMETERS:
 	ip	- inode to be freed.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error.
	
	  This is just to suppress compiler warnings.  The same logic that
	  references these variables is used to initialize them.
	 get the iag number containing the inode.
	 make sure that the iag is contained within
	  the map.
	 get the allocation group for this ino.
	 Lock the AG specific inode map information
	 Obtain read lock in imap inode.  Don't release it until we have
	  read all of the IAG's that we are going to.
	 read the iag.
	 get the inode number and extent number of the inode within
	  the iag and the inode number within the extent.
	 compute the bitmap for the extent reflecting the freed inode.
	
	 	inode extent still has some inodes or below low water mark:
	 	keep the inode extent;
		 if the iag currently has no free inodes (i.e.,
		  the inode being freed is the first free inode of iag),
		  insert the iag at head of the inode free list for the ag.
			 check if there are any iags on the ag inode
			  free list.  if so, read the first one so that
			  we can link the current iag onto the list at
			  the head.
				 read the iag that currently is the head
				  of the list.
				 make current head point back to the iag.
			 iag points forward to current head and iag
			  becomes the new head of the list.
		 update the free inode summary map for the extent if
		  freeing the inode means the extent will now have free
		  inodes (i.e., the inode being freed is the first free
		  inode of extent),
		 update the bitmap.
		 update the free inode counts at the iag, ag and
		  map level.
		 release the AG inode map lock
 write the iag 
	
	 	inode extent has become free and above low water mark:
	 	free the inode extent;
	
	 	prepare to update iag list(s) (careful update step 1)
	 check if the iag currently has no free extents.  if so,
	  it will be placed on the head of the ag extent free list.
		 check if the ag extent free list has any iags.
		  if so, read the iag at the head of the list now.
		  this (head) iag will be updated later to reflect
		  the addition of the current iag at the head of
		  the list.
		 iag has free extents. check if the addition of a free
		  extent will cause all extents to be free within this
		  iag.  if so, the iag will be removed from the ag extent
		  free list and placed on the inode map's free iag list.
			 in preparation for removing the iag from the
			  ag extent free list, read the iags preceding
			  and following the iag on the ag extent free
			  list.
	 remove the iag from the ag inode free list if freeing
	  this extent cause the iag to have no free inodes.
		 in preparation for removing the iag from the
		  ag inode free list, read the iags preceding
		  and following the iag on the ag inode free
		  list.  before reading these iags, we must make
		  sure that we already don't have them in hand
		  from up above, since re-reading an iag (buffer)
		  we are currently holding would cause a deadlock.
	
	  invalidate any page of the inode extent freed from buffer cache;
	
	 	update iag list(s) (careful update step 2)
	 add the iag to the ag extent free list if this is the
	  first free extent for the iag.
		 remove the iag from the ag extent list if all extents
		  are now free and place it on the inode map iag free list.
	 remove the iag from the ag inode free list if freeing
	  this extent causes the iag to have no free inodes.
	 update the inode extent address and working map
	  to reflect the free extent.
	  the permanent map should have been updated already
	  for the inode being freed.
	 update the free extent and free inode summary maps
	  to reflect the freed extent.
	  the inode summary map is marked to indicate no inodes
	  available for the freed extent.
	 update the number of free inodes and number of free extents
	  for the iag.
	 update the number of free inodes and backed inodes
	  at the ag and inode map level.
	
	  start transaction to update block allocation map
	  for the inode extent freed;
	 
	  N.B. AG_LOCK is released and iag will be released below, and
	  other thread may allocate inode fromreusing the ixad freed
	  BUT with newdifferent backing inode extent from the extent
	  to be freed by the transaction;
	 acquire tlock of the iag page of the freed ixad
	  to force the page NOHOMEOK (even though no data is
	  logged from the iag page) until NOREDOPAGE|FREEXTENT log
	  for the free of the extent is committed;
	  write FREEXTENT|NOREDOPAGE log record
	  N.B. linelock is overlaid as freed extent descriptor;
	
	  logredo needs the IAG number and IAG extent index in order
	  to ensure that the IMap is consistent.  The least disruptive
	  way to pass these values through  to the transaction manager
	  is in the iplist array.
	 
	  It's not pretty, but it works.
 unlock the AG inode map information 
  There are several places in the diAlloc routines where we initialize
  the inode.
  NAME:	diAlloc(pip,dir,ip)
  FUNCTION:	allocate a disk inode from the inode working map
 		for a fileset or aggregate.
  PARAMETERS:
 	pip	- pointer to incore inode for the parent inode.
 	dir	- 'true' if the new disk inode is for a directory.
 	ip	- pointer to a new inode
  RETURN VALUES:
 	0	- success.
 	-ENOSPC	- insufficient disk resources.
 	-EIO	- io error.
	 get the pointers to the inode map inode and the
	  corresponding imap control structure.
	 for a directory, the allocation policy is to start
	  at the ag level using the preferred ag.
	 for files, the policy starts off by trying to allocate from
	  the same iag containing the parent disk inode:
	  try to allocate the new disk inode close to the parent disk
	  inode, using parent disk inode number + 1 as the allocation
	  hint.  (we use a left-to-right policy to attempt to avoid
	  moving backward on the disk.)  compute the hint within the
	  file system and the iag.
 get the ag number of this iag 
		
		  There is an open file actively growing.  We want to
		  allocate new inodes from a different ag to avoid
		  fragmentation problems.
 back off the hint if it is outside of the iag 
 lock the AG inode map information 
 Get read lock on imap inode 
 get the iag number and read the iag 
	 determine if new inode extent is allowed to be added to the iag.
	  new inode extent can be added to the iag if the ag
	  has less than 32 free disk inodes and the iag has free extents.
	
	 	try to allocate from the IAG
	 check if the inode may be allocated from the iag
	  (i.e. the inode has free inodes or new extent can be added).
		 determine the extent number of the hint.
		 check if the extent containing the hint has backed
		  inodes.  if so, try to allocate within this extent.
				 a free inode (bit) was found within this
				  extent, so allocate it.
					 set the results of the allocation
					  and write the iag.
				 free the AG lock and return.
		
		  no free inodes within the extent containing the hint.
		 
		  try to allocate from the backed extents following
		  hint or, if appropriate (i.e. addext is true), allocate
		  an extent of free inodes at or following the extent
		  containing the hint.
		 
		  the free inode and free extent summary maps are used
		  here, so determine the starting summary map position
		  and the number of words we'll have to examine.  again,
		  the approach is to allocate following the hint, so we
		  might have to initially ignore prior bits of the summary
		  map that represent extents prior to the extent containing
		  the hint and later revisit these bits.
		 mask any prior bits for the starting words of the
		  summary map.
		 scan the free inode and free extent summary maps for
		  free resources.
			 check if this word of the free inode summary
			  map describes an extent with free inodes.
				 an extent with free inodes has been
				  found. determine the extent number
				  and the inode number within the extent.
				 determine the inode number within the
				  iag and allocate the inode from the
				  map.
					 set the results of the allocation
					  and write the iag.
				 free the AG lock and return.
			 check if we may allocate an extent of free
			  inodes and whether this word of the free
			  extents summary map describes a free extent.
				 a free extent has been found.  determine
				  the extent number.
				 allocate an extent of free inodes.
					 if there is no disk space for a
					  new extent, try to allocate the
					  disk inode from somewhere else.
					 set the results of the allocation
					  and write the iag.
				 free the imap inode & the AG lock & return.
			 move on to the next set of summary map words.
 unlock imap inode 
 nothing doing in this iag, so release it. 
	
	  try to allocate anywhere within the same AG as the parent inode.
	
	  try to allocate in any AG.
  NAME:	diAllocAG(imap,agno,dir,ip)
  FUNCTION:	allocate a disk inode from the allocation group.
 		this routine first determines if a new extent of free
 		inodes should be added for the allocation group, with
 		the current request satisfied from this extent. if this
 		is the case, an attempt will be made to do just that.  if
 		this attempt fails or it has been determined that a new
 		extent should not be added, an attempt is made to satisfy
 		the request by allocating an existing (backed) free inode
 		from the allocation group.
  PRE CONDITION: Already have the AG lock for this AG.
  PARAMETERS:
 	imap	- pointer to inode map control structure.
 	agno	- allocation group to allocate from.
 	dir	- 'true' if the new disk inode is for a directory.
 	ip	- pointer to the new inode to be filled in on successful return
 		  with the disk inode number allocated, its extent address
 		  and the start of the ag.
  RETURN VALUES:
 	0	- success.
 	-ENOSPC	- insufficient disk resources.
 	-EIO	- io error.
	 get the number of free and the number of backed disk
	  inodes currently within the ag.
	 determine if we should allocate a new extent of free inodes
	  within the ag: for directory inodes, add a new extent
	  if there are a small number of free inodes or number of free
	  inodes is a small percentage of the number of backed inodes.
	
	  try to allocate a new extent of free inodes.
		 if free space is not available for this new extent, try
		  below to allocate a free and existing (already backed)
		  inode from the ag.
	
	  try to allocate an existing free inode from the ag.
  NAME:	diAllocAny(imap,agno,dir,iap)
  FUNCTION:	allocate a disk inode from any other allocation group.
 		this routine is called when an allocation attempt within
 		the primary allocation group has failed. if attempts to
 		allocate an inode from any allocation group other than the
 		specified primary group.
  PARAMETERS:
 	imap	- pointer to inode map control structure.
 	agno	- primary allocation group (to avoid).
 	dir	- 'true' if the new disk inode is for a directory.
 	ip	- pointer to a new inode to be filled in on successful return
 		  with the disk inode number allocated, its extent address
 		  and the start of the ag.
  RETURN VALUES:
 	0	- success.
 	-ENOSPC	- insufficient disk resources.
 	-EIO	- io error.
	 try to allocate from the ags following agno up to
	  the maximum ag number.
	 try to allocate from the ags in front of agno.
	 no free disk inodes.
  NAME:	diAllocIno(imap,agno,ip)
  FUNCTION:	allocate a disk inode from the allocation group's free
 		inode list, returning an error if this free list is
 		empty (i.e. no iags on the list).
 		allocation occurs from the first iag on the list using
 		the iag's free inode summary map to find the leftmost
 		free inode in the iag.
  PRE CONDITION: Already have AG lock for this AG.
  PARAMETERS:
 	imap	- pointer to inode map control structure.
 	agno	- allocation group.
 	ip	- pointer to new inode to be filled in on successful return
 		  with the disk inode number allocated, its extent address
 		  and the start of the ag.
  RETURN VALUES:
 	0	- success.
 	-ENOSPC	- insufficient disk resources.
 	-EIO	- io error.
	 check if there are iags on the ag's free inode list.
 obtain read lock on imap inode 
	 read the iag at the head of the list.
	 better be free inodes in this iag if it is on the
	  list.
	 scan the free inode summary map to find an extent
	  with free inodes.
	 found a extent with free inodes. determine
	  the extent number.
	 find the first free inode in the extent.
	 compute the inode number within the iag.
	 allocate the inode.
	 set the results of the allocation and write the iag.
  NAME:	diAllocExt(imap,agno,ip)
  FUNCTION:	add a new extent of free inodes to an iag, allocating
 		an inode from this extent to satisfy the current allocation
 		request.
 		this routine first tries to find an existing iag with free
 		extents through the ag free extent list.  if list is not
 		empty, the head of the list will be selected as the home
 		of the new extent of free inodes.  otherwise (the list is
 		empty), a new iag will be allocated for the ag to contain
 		the extent.
 		once an iag has been selected, the free extent summary map
 		is used to locate a free extent within the iag and diNewExt()
 		is called to initialize the extent, with initialization
 		including the allocation of the first inode of the extent
 		for the purpose of satisfying this request.
  PARAMETERS:
 	imap	- pointer to inode map control structure.
 	agno	- allocation group number.
 	ip	- pointer to new inode to be filled in on successful return
 		  with the disk inode number allocated, its extent address
 		  and the start of the ag.
  RETURN VALUES:
 	0	- success.
 	-ENOSPC	- insufficient disk resources.
 	-EIO	- io error.
	 check if the ag has any iags with free extents.  if not,
	  allocate a new iag for the ag.
		 If successful, diNewIAG will obtain the read lock on the
		  imap inode.
		 set the ag number if this a brand new iag
		 read the iag.
	 using the free extent summary map, find a free extent.
	 determine the extent number of the free extent.
	 initialize the new extent.
		 something bad happened.  if a new iag was allocated,
		  place it back on the inode map's iag free list, and
		  clear the ag number information.
	 set the results of the allocation and write the iag.
  NAME:	diAllocBit(imap,iagp,ino)
  FUNCTION:	allocate a backed inode from an iag.
 		this routine performs the mechanics of allocating a
 		specified inode from a backed extent.
 		if the inode to be allocated represents the last free
 		inode within the iag, the iag will be removed from the
 		ag free inode list.
 		a careful update approach is used to provide consistency
 		in the face of updates to multiple buffers.  under this
 		approach, all required buffers are obtained before making
 		any updates and are held all are updates are complete.
  PRE CONDITION: Already have buffer lock on iagp.  Already have AG lock on
 	this AG.  Must have read lock on imap inode.
  PARAMETERS:
 	imap	- pointer to inode map control structure.
 	iagp	- pointer to iag.
 	ino	- inode number to be allocated within the iag.
  RETURN VALUES:
 	0	- success.
 	-ENOSPC	- insufficient disk resources.
 	-EIO	- io error.
	 check if this is the last free inode within the iag.
	  if so, it will have to be removed from the ag free
	  inode list, so get the iags preceding and following
	  it on the list.
	 get the ag number, extent number, inode number within
	  the extent.
	 compute the mask for setting the map.
	 the inode should be free and backed.
	 mark the inode as allocated in the working map.
	 check if all inodes within the extent are now
	  allocated.  if so, update the free inode summary
	  map to reflect this.
	 if this was the last free inode in the iag, remove the
	  iag from the ag free inode list.
	 update the free inode count at the iag, ag, inode
	  map levels.
  NAME:	diNewExt(imap,iagp,extno)
  FUNCTION:	initialize a new extent of inodes for an iag, allocating
 		the first inode of the extent for use for the current
 		allocation request.
 		disk resources are allocated for the new extent of inodes
 		and the inodes themselves are initialized to reflect their
 		existence within the extent (i.e. their inode numbers and
 		inode extent addresses are set) and their initial state
 		(mode and link count are set to zero).
 		if the iag is new, it is not yet on an ag extent free list
 		but will now be placed on this list.
 		if the allocation of the new extent causes the iag to
 		have no free extent, the iag will be removed from the
 		ag extent free list.
 		if the iag has no free backed inodes, it will be placed
 		on the ag free inode list, since the addition of the new
 		extent will now cause it to have free inodes.
 		a careful update approach is used to provide consistency
 		(i.e. list consistency) in the face of updates to multiple
 		buffers.  under this approach, all required buffers are
 		obtained before making any updates and are held until all
 		updates are complete.
  PRE CONDITION: Already have buffer lock on iagp.  Already have AG lock on
 	this AG.  Must have read lock on imap inode.
  PARAMETERS:
 	imap	- pointer to inode map control structure.
 	iagp	- pointer to iag.
 	extno	- extent number.
  RETURN VALUES:
 	0	- success.
 	-ENOSPC	- insufficient disk resources.
 	-EIO	- io error.
	 better have free extents.
	 get the inode map inode.
	 get the ag and iag numbers for this iag.
	 check if this is the last free extent within the
	  iag.  if so, the iag must be removed from the ag
	  free extent list, so get the iags preceding and
	  following the iag on this list.
		 the iag has free extents.  if all extents are free
		  (as is the case for a newly allocated iag), the iag
		  must be added to the ag free extent list, so get
		  the iag at the head of the list in preparation for
		  adding this iag to this list.
	 check if the iag has no free inodes.  if so, the iag
	  will have to be added to the ag free inode list, so get
	  the iag at the head of the list in preparation for
	  adding this iag to this list.  in doing this, we must
	  check if we already have the iag at the head of
	  the list in hand.
	 allocate disk space for the inode extent.
	 compute the inode number of the first inode within the
	  extent.
	 initialize the inodes within the newly allocated extent a
	  page at a time.
		 get a buffer for this page of disk inodes.
		 initialize the inode number, mode, link count and
		  inode extent address.
	 if this is the last free extent within the iag, remove the
	  iag from the ag free extent list.
		 if the iag has all free extents (newly allocated iag),
		  add the iag to the ag free extent list.
	 if the iag has no free inodes, add the iag to the
	  ag free inode list.
 initialize the extent descriptor of the extent. 
	 initialize the working and persistent map of the extent.
	  the working map will be initialized such that
	  it indicates the first inode of the extent is allocated.
	 update the free inode and free extent summary maps
	  for the extent to indicate the extent has free inodes
	  and no longer represents a free extent.
	 update the free inode and free extent counts for the
	  iag.
	 update the free and backed inode counts for the ag.
	 update the free and backed inode counts for the inode map.
	 write the iags.
	 release the iags.
  NAME:	diNewIAG(imap,iagnop,agno)
  FUNCTION:	allocate a new iag for an allocation group.
 		first tries to allocate the iag from the inode map
 		iagfree list:
 		if the list has free iags, the head of the list is removed
 		and returned to satisfy the request.
 		if the inode map's iag free list is empty, the inode map
 		is extended to hold a new iag. this new iag is initialized
 		and returned to satisfy the request.
  PARAMETERS:
 	imap	- pointer to inode map control structure.
 	iagnop	- pointer to an iag number set with the number of the
 		  newly allocated iag upon successful return.
 	agno	- allocation group number.
 	bpp	- Buffer pointer to be filled in with new IAG's buffer
  RETURN VALUES:
 	0	- success.
 	-ENOSPC	- insufficient disk resources.
 	-EIO	- io error.
  serialization:
 	AG lock held on entryexit;
 	write lock on the map is held inside;
 	read lock on the map is held on successful completion;
  note: new iag transaction:
  . synchronously write iag;
  . write log of xtree and inode of imap;
  . commit;
  . synchronous write of xtree (right to left, bottom to top);
  . at start of logredo(): init in-memory imap with one additional iag page;
  . at end of logredo(): re-read imap inode to determine
    new imap size;
 pick up pointers to the inode map and mount inodes 
 acquire the free iag lock 
	 if there are any iags on the inode map free iag list,
	  allocate the iag from the head of the list.
 pick up the iag number at the head of the list 
 determine the logical block number of the iag 
		 no free iags. the inode map will have to be extented
		  to include a new iag.
 acquire inode map lock 
 get the next available iag number 
		 make sure that we have not exceeded the maximum inode
		  number limit.
 release the inode map lock 
		
		  synchronously append new iag page.
 determine the logical address of iag page to append 
 Allocate extent for new iag page 
 release the inode map lock 
		
		  start transaction of update of the inode map
		  addressing structure pointing to the new iag page;
 update the inode map addressing structure to point to it 
			 Free the blocks allocated for the iag since it was
			  not successfully added to the inode map
 release the inode map lock 
 update the inode map's inode to reflect the extension 
 assign a buffer for the page 
			
			  This is very unlikely since we just created the
			  extent, but let's try to handle it correctly
 release the inode map lock 
 init the iag 
		 initialize the free inode summary map (free extent
		  summary map initialization handled by bzero).
		
		  Write and sync the metapage
		
		  txCommit(COMMIT_FORCE) will synchronously write address
		  index pages and inode after commit in careful update order
		  of address index pages (right to left, bottom up);
 update the next available iag number 
		 Add the iag to the iag free list so we don't lose the iag
		  if a failure happens now.
		 Until we have logredo working, we want the imap inode &
		  control page to be up to date.
 release the inode map lock 
 obtain read lock on map 
 read the iag 
 remove the iag from the iag free list 
 set the return iag number and buffer pointer 
 release the iag free lock 
  NAME:	diIAGRead()
  FUNCTION:	get the buffer for the specified iag within a fileset
 		or aggregate inode map.
  PARAMETERS:
 	imap	- pointer to inode map control structure.
 	iagno	- iag number.
 	bpp	- point to buffer pointer to be filled in on successful
 		  exit.
  SERIALIZATION:
 	must have read lock on imap inode
 	(When called by diExtendFS, the filesystem is quiesced, therefore
 	 the read lock is unnecessary.)
  RETURN VALUES:
 	0	- success.
 	-EIO	- io error.
 compute the logical block number of the iag. 
 read the iag. 
  NAME:	diFindFree()
  FUNCTION:	find the first free bit in a word starting at
 		the specified bit position.
  PARAMETERS:
 	word	- word to be examined.
 	start	- starting bit position.
  RETURN VALUES:
 	bit position of first free bit in the word or 32 if
 	no free bits were found.
 scan the word for the first free bit. 
  NAME:	diUpdatePMap()
  FUNCTION: Update the persistent map in an IAG for the allocation or
 	freeing of the specified inode.
  PRE CONDITIONS: Working map has already been updated for allocate.
  PARAMETERS:
 	ipimap	- Incore inode map inode
 	inum	- Number of inode to mark in permanent map
 	is_free	- If 'true' indicates inode should be marked freed, otherwise
 		  indicates inode should be marked allocated.
  RETURN VALUES:
 		0 for success
 get the iag number containing the inode 
 make sure that the iag is contained within the map 
 read the iag 
	 get the inode number and extent number of the inode within
	  the iag and the inode number within the extent.
	
	  mark the inode free in persistent map:
		 The inode should have been allocated both in working
		  map and in persistent map;
		  the inode will be freed from working map at the release
		  of last reference release;
 update the bitmap for the extent of the freed inode 
	
	  mark the inode allocated in persistent map:
		 The inode should be already allocated in the working map
		  and should be free in persistent map;
 update the bitmap for the extent of the allocated inode 
	
	  update iag lsn
 inherit oldersmaller lsn 
 move mp after tblock in logsync list 
 inherit youngerlarger clsn 
 insert mp after tblock in logsync list 
 	diExtendFS()
  function: update imap for extendfs();
  note: AG size has been increased s.t. each k old contiguous AGs are
  coalesced into a new AG;
	
	 	reconstruct imap
	 
	  coalesce contiguous k (newAGSizeoldAGSize) AGs;
	  i.e., (AGi, ..., AGj) where i = kn and j = k(n+1) - 1 to AGn;
	  note: new AG size = old AG size  (2x).
 init per AG control information im_agctl[] 
 number of backed inodes 
 number of free backed inodes 
	
	 	process each iag page of the map.
	 
	  rebuild AG Free Inode List, AG Free Inode Extent List;
 leave free iag in the free iag list 
 compute backed inodes 
 merge AG backed inodes 
 if any backed free inodes, insert at AG free inode list 
 merge AG backed free inodes 
 if any free extents, insert at AG free extent list 
 	duplicateIXtree()
  serialization: IWRITE_LOCK held on entryexit
  note: shadow page with regular inode (rel.2);
 if AIT2 ipmap2 is bad, do not try to update it 
 s_flag 
 start transaction 
 update the inode map addressing structure to point to it 
 update the inode map's inode to reflect the extension 
  NAME:	copy_from_dinode()
  FUNCTION:	Copies inode info from disk inode to in-memory inode
  RETURN VALUES:
 	0	- success
 	-ENOMEM	- insufficient memory
 For directories, add x permission if r is allowed by umask 
 in-memory pxd's are little-endian 
 as are dxd's 
 Zero the in-memory-only stuff 
  NAME:	copy_to_dinode()
  FUNCTION:	Copies inode info from in-memory inode to disk inode
	
	  mode2 is only needed for storing the higher order bits.
	  Trust i_mode for the lower order ones
 Leave the original permissions alone 
 in-memory pxd's are little-endian 
 as are dxd's 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2005
    Portions Copyright (C) Christoph Hellwig, 2001-2002
 	jfs_txnmgr.c: transaction manager
  notes:
  transaction starts with txBegin() and ends with txCommit()
  or txAbort().
  tlock is acquired at the time of update;
  (obviate scan at commit time for xtree and dtree)
  tlock and mp points to each other;
  (no hashlist for mp -> tlock).
  special cases:
  tlock on in-memory inode:
  in-place tlock in the in-memory inode itself;
  converted to page lock by iWrite() at commit time.
  tlock during write()mmap() under anonymous transaction (tid = 0):
  transferred (?) to transaction at commit time.
  use the page itself to update allocation maps
  (obviate intermediate replication of allocationdeallocation data)
  hold on to mp+lock thru update of maps
 	transaction management structures
 index of a free tid structure 
 index first free lock word 
 eventlist of free tblock 
 eventlist of free tlock 
 eventlist of ample tlocks 
 Number of tlocks in use 
 synchronize sync_queue & unlock_queue 
	struct tblock sync_queue;  Transactions waiting for data sync 
 Txns waiting to be released 
 inodes having anonymous txns 
	struct list_head anon_list2;	 inodes having anonymous txns
 Indicates low number of available tlocks 
 number of transaction blocks 
 number of transaction locks 
 transaction block table 
 Low water mark for number of txLocks used 
 High water mark for number of txLocks used 
 Very High water mark 
 transaction lock table 
 	transaction management lock
  Retry logic exist outside these macros to protect from spurrious wakeups.
 	statistics
 4: biggest tid ever used 
 4: biggest lid ever used 
 4: # of transactions performed 
 4: # of tlocks acquired 
 4: # of tlock wait 
  forward references
 		transaction blocklock management
 		---------------------------------
  Get a transaction lock from the free list.  If the number in use is
  greater than the high water mark, wake up the sync daemon.  This should
  free some anonymous transaction locks.  (TXN_LOCK must be held.)
  NAME:	txInit()
  FUNCTION:	initialize transaction management structures
  RETURN:
  serialization: single thread at jfs_init()
 Set defaults for nTxLock and nTxBlock if unset 
 Base default on memory size 
 1 GB 
 Verify tunable parameters 
 No one should set it this low 
 No one should set it this low 
	
	  initialize transaction block (tblock) table
	 
	  transaction id (tid) = tblock index
	  tid = 0 is reserved.
 statistics 
	
	  initialize transaction lock (tlock) table
	 
	  transaction lock id = tlock index
	  tlock id = 0 is reserved.
 initialize tlock table 
 statistics 
  NAME:	txExit()
  FUNCTION:	clean up when module is unloaded
  NAME:	txBegin()
  FUNCTION:	start a transaction.
  PARAMETER:	sb	- superblock
 		flag	- force for nested tx;
  RETURN:	tid	- transaction id
  note: flag force allows to start tx for nested tx
  to prevent deadlock on logsync barrier;
		
		  synchronize with logsync barrier
		
		  Don't begin transaction if we're getting starved for tlocks
		  unless COMMIT_FORCE or COMMIT_INODE (which may ultimately
		  free tlocks)
	
	  allocate transaction idblock
 Don't let a non-forced transaction take the last tblk 
	
	  initialize transaction
	
	  We can't zero the whole thing or we screw up another thread being
	  awakened after sleeping on tblk->waitor
	 
	  memset(tblk, 0, sizeof(struct tblock));
 statistics 
 statistics 
  NAME:	txBeginAnon()
  FUNCTION:	start an anonymous transaction.
 		Blocks if logsync or available tlocks are low to prevent
 		anonymous tlocks from depleting supply.
  PARAMETER:	sb	- superblock
  RETURN:	none
	
	  synchronize with logsync barrier
	
	  Don't begin transaction if we're getting starved for tlocks
 	txEnd()
  function: free specified transaction block.
 	logsync barrier processing:
  serialization:
	
	  wakeup transactions waiting on the page locked
	  by the current transaction
	
	  Lazy commit thread can't free this guy until we mark it UNLOCKED,
	  otherwise, we would be left with a transaction that may have been
	  reused.
	 
	  Lazy commit thread will turn off tblkGC_LAZY before calling this
	  routine.
 LOGGC_LOCK
 LOGGC_UNLOCK
	
	  insert tblock back on freelist
	
	  mark the tblock not active
		
		  synchronize with logsync barrier
 write dirty metadata & forward log syncpt 
 enable new transactions start 
 wakeup all waitors for logsync barrier 
	
	  wakeup all waitors for a free tblock
 	txLock()
  function: acquire a transaction lock on the specified <mp>
  parameter:
  return:	transaction lock id
  serialization:
		
		  Directory inode is special.  It can have both an xtree tlock
		  and a dtree tlock associated with it.
 is page not locked by a transaction ? 
 is page locked by the requester transaction ? 
	
	  is page locked by anonymous transactionlock ?
	 
	  (page update without transaction (i.e., file write) is
	  locked under anonymous transaction tid = 0:
	  anonymous tlocks maintained on anonymous tlock list of
	  the inode of the page and available to all anonymous
	  transactions until txCommit() time at which point
	  they are transferred to the transaction tlock list of
	  the committing transaction of the inode)
		
		  The order of the tlocks in the transaction is important
		  (during truncate, child xtree pages must be freed before
		  parent's tlocks change the working map).
		  Take tlock off anonymous list and add to tail of
		  transaction list
		 
		  Note:  We really need to get rid of the tid & lid and
		  use list_head's.  This code is getting UGLY!
				 only anonymous txn.
				  Remove from anon_list
 insert the tlock at tail of transaction tlock list 
	
	  allocate a tlock
	
	  initialize tlock
 mark tlock for meta-data page 
 mark the page dirty and nohomeok 
		 if anonymous transaction, and buffer is on the group
		  commit synclist, mark inode to show this.  This will
		  prevent the buffer from being marked nohomeok for too
		  long a time.
 mark tlock for in-memory inode 
 bind the tlock and the page 
	
	  enqueue transaction lock to transactioninode
 insert the tlock at tail of transaction tlock list 
	 anonymous transaction:
	  insert the tlock at head of inode anonymous tlock list
 This inode's first anonymous transaction 
 initialize type dependent area for linelock 
 ! 
	
	  update tlock vector
	
	  page is being locked by another transaction:
 Only locks on ipimap or ipaimap should reach here 
 assert(jfs_ip->fileset == AGGREGATE_I); 
 statistics 
 reacquire after dropping TXN_LOCK 
 Recheck everything since dropping TXN_LOCK 
  NAME:	txRelease()
  FUNCTION:	Release buffers associated with transaction locks, but don't
 		mark homeok yet.  The allows other transactions to modify
 		buffers, but won't let them go to disk until commit record
 		actually gets written.
  PARAMETER:
 		tblk	-
  RETURN:	Errors from subroutines.
	
	  wakeup transactions waiting on a page locked
	  by the current transaction
  NAME:	txUnlock()
  FUNCTION:	Initiates pageout of pages modified by tid in journalled
 		objects and frees their lockwords.
	
	  mark page under tlock homeok (its log has been written):
 unbind page from tlock 
			 hold buffer
 inherit youngerlarger clsn 
		 insert tlock, and linelock(s) of the tlock if any,
		  at head of freelist
	
	  remove tblock from logsynclist
	  (allocation map pages inherited lsn of tblk and
	  has been inserted in logsync list at txUpdateMap())
 	txMaplock()
  function: allocate a transaction lock for freed pageentry;
 	for freed page, maplock is used as xtlockdtlock type;
	
	  allocate a tlock
	
	  initialize tlock
 bind the tlock and the object 
	
	  enqueue transaction lock to transactioninode
 insert the tlock at tail of transaction tlock list 
	 anonymous transaction:
	  insert the tlock at head of inode anonymous tlock list
 This inode's first anonymous transaction 
 initialize type dependent area for maplock 
 	txLinelock()
  function: allocate a transaction lock for log vector list
 allocate a TxLock structure 
 initialize linelock 
 append linelock after tlock 
 		transaction commit management
 		-----------------------------
  NAME:	txCommit()
  FUNCTION:	commit the changes to the objects specified in
 		clist.  For journalled segments only the
 		changes of the caller are committed, ie by tid.
 		for non-journalled segments the data are flushed to
 		disk and then the change to the disk inode and indirect
 		blocks committed (so blocks newly allocated to the
 		segment will be made a part of the segment atomically).
 		all of the segments specified in clist must be in
 		one file system. no more than 6 segments are needed
 		to handle all unix svcs.
 		if the i_nlink field (i.e. disk inode link count)
 		is zero, and the type of inode is a regular file or
 		directory, or symbolic link , the inode is truncated
 		to zero length. the truncation is committed but the
 		VM resources are unaffected until it is closed (see
 		iput and iclose).
  PARAMETER:
  RETURN:
  serialization:
 		on entry the inode lock on each segment is assumed
 		to be held.
  io error:
 transaction identifier 
 number of inodes to commit 
 list of inode to commit 
 is read-only file system ? 
	
	  initialize commit structure
 initialize log record descriptor in commit 
	
	 	prepare non-journaled objects for commit
	 
	  flush data pages of non-journaled file
	  to prevent the file getting non-initialized disk blocks
	  in case of crash.
	  (new blocks - )
	
	 	acquire transaction lock on (on-disk) inodes
	 
	  update on-disk inode from in-memory inode
	  acquiring transaction locks for AFTER records
	  on the on-disk inode of file object
	 
	  sort the inodes array by inode number in descending order
	  to prevent deadlock when acquiring transaction lock
	  of on-disk inodes on multiple on-disk inode pages by
	  multiple concurrent transactions
		
		  BUGBUG - This code has temporarily been removed.  The
		  intent is to ensure that any file data is written before
		  the metadata is committed to the journal.  This prevents
		  uninitialized data from appearing in a file after the
		  journal has been replayed.  (The uninitialized data
		  could be sensitive data removed by another user.)
		 
		  The problem now is that we are holding the IWRITELOCK
		  on the inode, and calling filemap_fdatawrite on an
		  unmapped page will cause a deadlock in jfs_get_block.
		 
		  The long term solution is to pare down the use of
		  IWRITELOCK.  We are currently holding it too long.
		  We could also be smarter about which data pages need
		  to be written before the transaction is committed and
		  when we don't need to worry about it at all.
		 
		  if ((!S_ISDIR(ip->i_mode))
		     && (tblk->flag & COMMIT_DELETE) == 0)
		 	filemap_write_and_wait(ip->i_mapping);
		
		  Mark inode as not dirty.  It will still be on the dirty
		  inode list, but we'll know not to commit it again unless
		  it gets marked dirty again
 inherit anonymous tlock(s) of inode 
		
		  acquire transaction lock on on-disk inode page
		  (become first tlock of the tblk's tlock list)
	
	 	write log records from transaction locks
	 
	  txUpdateMap() resets XAD_NEW in XAD.
	
	  Ensure that inode isn't reused before
	  lazy commit thread finishes processing
		
		  Avoid a rare deadlock
		 
		  If the inode is locked, we may be blocked in
		  jfs_commit_inode.  If so, we don't want the
		  lazy_commit thread doing the last iput() on the inode
		  since that may block on the locked inode.  Instead,
		  commit the transaction synchronously, so the last iput
		  will be done by the calling thread (or later)
		
		  I believe this code is no longer needed.  Splitting I_LOCK
		  into two bits, I_NEW and I_SYNC should prevent this
		  deadlock as well.  But since I don't have a JFS testload
		  to verify this, only a trivial sI_LOCKI_SYNC was done.
		  Joern
	
	 	write COMMIT log record
	
	 	- transaction is now committed -
	
	  force pages in careful update
	  (imap addressing structure update)
	
	 	update allocation map.
	 
	  update inode allocation map and inode:
	  free pager lock on memory object of inode if any.
	  update block allocation map.
	 
	  txUpdateMap() resets XAD_NEW in XAD.
	
	 	free transaction locks and pageoutfree pages
	
	 	reset in-memory object state
		
		  reset in-memory inode state
  NAME:	txLog()
  FUNCTION:	Writes AFTER log records for all lines modified
 		by tid for segments specified by inodes in comdata.
 		Code assumes only WRITELOCKS are recorded in lockwords.
  PARAMETERS:
  RETURN :
	
	  write log record(s) for each tlock of transaction,
 initialize lrd common 
 write log record of page from the tlock 
 	diLog()
  function:	log inode tlock and format maplock to update bmap;
 initialize as REDOPAGE record format 
	
	 	inode after image
 log after-image for logredo(): 
 mark page as homeward bound 
		
		 	free inode extent
		 
		  (pages of the freed inode extent have been invalidated and
		  a maplock for free of the extent has been formatted at
		  txLock() time);
		 
		  the tlock had been acquired on the inode allocation map page
		  (iag) that specifies the freed extent, even though the map
		  page is not itself logged, to prevent pageout of the map
		  page before the log;
		 log LOG_NOREDOINOEXT of the freed inode extent for
		  logredo() to start NoRedoPage filters, and to update
		  imap and bmap for free of the extent;
		
		  For the LOG_NOREDOINOEXT record, we need
		  to pass the IAG number and inode extent
		  index (within that IAG) from which the
		  extent is being released.  These have been
		  passed to us in the iplist[1] and iplist[2].
 update bmap 
 mark page as homeward bound 
	
	 	allocfree external EA extent
	 
	  a maplock for txUpdateMap() to update bPWMAP for allocfree
	  of the extent has been formatted at txLock() time;
		 log LOG_UPDATEMAP for logredo() to update bmap for
		  alloc of new (and free of old) external EA extent;
 update bmap 
 _JFS_WIP 
 	dataLog()
  function:	log data tlock
 initialize as REDOPAGE record format 
 log after-image for logredo(): 
		
		  The table has been truncated, we've must have deleted
		  the last entry, so don't bother logging this
 mark page as homeward bound 
 	dtLog()
  function:	log dtree tlock and format maplock to update bmap;
 initialize as REDOPAGENOREDOPAGE record format 
	
	 	page extension via relocation: entry insertion;
	 	page extension in-place: entry insertion;
	 	new right page from page split, reinitialized in-line
	 	root from root page split: entry insertion;
		 log after-image of the new page for logredo():
		  mark log (LOG_NEW) for logredo() to initialize
		  freelist and update bmap for alloc of the new page;
		 format a maplock for txUpdateMap() to update bPMAP for
		  alloc of the new page;
 mark page as homeward bound 
	
	 	entry insertiondeletion,
	 	sibling page link update (old right page before split);
 log after-image for logredo(): 
 mark page as homeward bound 
	
	 	page deletion: page has been invalidated
	 	page relocation: source extent
	 
	 	a maplock for free of the page has been formatted
	 	at txLock() time);
		 log LOG_NOREDOPAGE of the deleted page for logredo()
		  to start NoRedoPage filter and to update bmap for free
		  of the deletd page
		 a maplock for txUpdateMap() for free of the page
		  has been formatted at txLock() time;
 	xtLog()
  function:	log xtree tlock and format maplock to update bmap;
 initialize as REDOPAGENOREDOPAGE record format 
	
	 	entry insertionextension;
	 	sibling page link update (old right page before split);
		 log after-image for logredo():
		  logredo() will update bmap for alloc of newextended
		  extents (XAD_NEW|XAD_EXTEND) of XAD[lwm:next) from
		  after-image of XADlist;
		  logredo() resets (XAD_NEW|XAD_EXTEND) flag when
		  applying the after-image to the meta-data page.
		 format a maplock for txUpdateMap() to update bPMAP
		  for alloc of newextended extents of XAD[lwm:next)
		  from the page itself;
		  txUpdateMap() resets (XAD_NEW|XAD_EXTEND) flag.
			
			  Lazy commit may allow xtree to be modified before
			  txUpdateMap runs.  Copy xad into linelock to
			  preserve correct data.
			 
			  We can fit twice as may pxd's as xads in the lock
			
			  xdlist will point to into inode's xtree, ensure
			  that transaction is not committed lazily.
 mark page as homeward bound 
	
	 	page deletion: file deletiontruncation (ref. xtTruncate())
	 
	  (page will be invalidated after log is written and bmap
	  is updated from the page);
		 LOG_NOREDOPAGE log for NoRedoPage filter:
		  if page free from file delete, NoRedoFile filter from
		  inode image of zero link count will subsume NoRedoPage
		  filters for each page;
		  if page free from file truncattion, write NoRedoPage
		  filter;
		 
		  upadte of block allocation map for the page itself:
		  if page free from deletion and truncation, LOG_UPDATEMAP
		  log for the page itself is generated from processing
		  its parent page xad entries;
		 if page free from file truncation, log LOG_NOREDOPAGE
		  of the deleted page for logredo() to start NoRedoPage
		  filter for the page;
 write NOREDOPAGE for the page 
 Empty xtree must be logged 
		 init LOG_UPDATEMAP of the freed extents
		  XAD[XTENTRYSTART:hwm) from the deleted page itself
		  for logredo() to update bmap;
 reformat linelock for lmLog() 
		 format a maplock for txUpdateMap() to update bmap
		  to free extents of XAD[XTENTRYSTART:hwm) from the
		  deleted page itself;
			
			  Lazy commit may allow xtree to be modified before
			  txUpdateMap runs.  Copy xad into linelock to
			  preserve correct data.
			 
			  We can fit twice as may pxd's as xads in the lock
			
			  xdlist will point to into inode's xtree, ensure
			  that transaction is not committed lazily.
 mark page as invalid 
		
		   else (tblk->xflag & COMMIT_PMAP)
		   ? release the page;
	
	 	pageentry truncation: file truncation (ref. xtTruncate())
	 
	 	|----------+------+------+---------------|
	 		   |      |      |
	 		   |      |     hwm - hwm before truncation
	 		   |     next - truncation point
	 		  lwm - lwm before truncation
	  header ?
 truncated extent of xad 
		
		  For truncation the entire linelock may be used, so it would
		  be difficult to store xad list in linelock itself.
		  Therefore, we'll just force transaction to be committed
		  synchronously, so that xtree pages won't be changed before
		  txUpdateMap runs.
		
		 	write log records
		 log after-image for logredo():
		 
		  logredo() will update bmap for alloc of newextended
		  extents (XAD_NEW|XAD_EXTEND) of XAD[lwm:next) from
		  after-image of XADlist;
		  logredo() resets (XAD_NEW|XAD_EXTEND) flag when
		  applying the after-image to the meta-data page.
		
		  truncate entry XAD[twm == next - 1]:
			 init LOG_UPDATEMAP for logredo() to update bmap for
			  free of truncated delta extent of the truncated
			  entry XAD[next - 1]:
			  (xtlck->pxdlock = truncated delta extent);
 assert(pxdlock->type & tlckTRUNCATE); 
 save to format maplock 
		
		  free entries XAD[next:hwm]:
			 init LOG_UPDATEMAP of the freed extents
			  XAD[next:hwm] from the deleted page itself
			  for logredo() to update bmap;
 reformat linelock for lmLog() 
		
		 	format maplock(s) for txUpdateMap() to update bmap
		
		  allocate entries XAD[lwm:next):
			 format a maplock for txUpdateMap() to update bPMAP
			  for alloc of newextended extents of XAD[lwm:next)
			  from the page itself;
			  txUpdateMap() resets (XAD_NEW|XAD_EXTEND) flag.
		
		  truncate entry XAD[twm == next - 1]:
			 format a maplock for txUpdateMap() to update bmap
			  to free truncated delta extent of the truncated
			  entry XAD[next - 1];
			  (xtlck->pxdlock = truncated delta extent);
		
		  free entries XAD[next:hwm]:
			 format a maplock for txUpdateMap() to update bmap
			  to free extents of XAD[next:hwm] from thedeleted
			  page itself;
 mark page as homeward bound 
 	mapLog()
  function:	log from maplock of freed data extents;
	
	 	page relocation: free the source page extent
	 
	  a maplock for txUpdateMap() for free of the page
	  has been formatted at txLock() time saving the src
	  relocated page address;
		 log LOG_NOREDOPAGE of the old relocated page
		  for logredo() to start NoRedoPage filter;
		 (N.B. currently, logredo() does NOT update bmap
		  for free of the page itself for (LOG_XTREE|LOG_NOREDOPAGE);
		  if page free from relocation, LOG_UPDATEMAP log is
		  specifically generated now for logredo()
		  to update bmap for free of src relocated page;
		  (new flag LOG_RELOCATE may be introduced which will
		  inform logredo() to start NORedoPage filter and also
		  update block allocation map at the same time, thus
		  avoiding an extra log write);
		 a maplock for txUpdateMap() for free of the page
		  has been formatted at txLock() time;
	
	  Otherwise it's not a relocate request
	 
		 log LOG_UPDATEMAP for logredo() to update bmap for
		  free of truncatedrelocated delta extent of the data;
		  e.g.: external EA extent, relocatedtruncated extent
		  from xtTailgate();
 update bmap 
 	txEA()
  function:	acquire maplock for EAACL extents or
 		set COMMIT_INLINE flag;
	
	  format maplock for alloc of new EA extent
		 Since the newea could be a completely zeroed entry we need to
		  check for the two flags which indicate we should actually
		  commit new EA data
	
	  format maplock for free of old EA extent
 	txForce()
  function: synchronously write pages locked by transaction
 	     after txLog() but before txUpdateMap();
	
	  reverse the order of transaction tlocks in
	  careful update order of address index pages
	  (right to left, bottom up)
	
	  synchronously write the page, and
	  hold the page for txUpdateMap();
 do not release page to freelist 
				
				  The "right" thing to do here is to
				  synchronously write the metadata.
				  With the current implementation this
				  is hard since write_metapage requires
				  us to kunmap & remap the page.  If we
				  have tlocks pointing into the metadata
				  pages, we don't want to do this.  I think
				  we can get by with synchronously writing
				  the pages when they are released.
 	txUpdateMap()
  function:	update persistent allocation map (and working map
 		if appropriate);
  parameter:
	
	 	update block allocation map
	 
	  update allocation state in pmap (and wmap) and
	  update lsn of the pmap page;
	
	  scan each tlockpage of transaction for block allocationfree:
	 
	  for each tlockpage of transaction, update map.
	   ? are there tlock for pmap and pwmap at the same time ?
			
			  Another thread may attempt to reuse freed space
			  immediately, so we want to get rid of the metapage
			  before anyone else has a chance to get it.
			  Lock metapage, update maps, then invalidate
			  the metapage.
		
		  extent list:
		  . in-line PXD list:
		  . out-of-line XAD list:
			
			  allocate blocks in persistent map:
			 
			  blocks have been allocated from wmap at alloc time;
			
			  free blocks in persistent and working map:
			  blocks will be freed in pmap and then in wmap;
			 
			  ? tblock specifies the PMAPPWMAP based upon
			  transaction
			 
			  free blocks in persistent map:
			  blocks will be freed from wmap at last reference
			  release of the object for regular files;
			 
			  Alway free blocks from both persistent & working
			  maps for directories
 (maplock->flag & mlckFREE) 
 This is equivalent to txRelease 
	
	 	update inode allocation map
	 
	  update allocation state in pmap and
	  update lsn of the pmap page;
	  update in-memory inode flagstate
	 
	  unlock mapperwrite lock
		 update persistent block allocation map
		  for the allocation of inode extent;
 	txAllocPMap()
  function: allocate from persistent map;
  parameter:
 	ipbmap	-
 	malock	-
 		xad list:
 		pxd:
 	maptype -
 		allocate from persistent map;
 		free from persistent map;
 		(e.g., tmp file - free from working map at releae
 		 of last reference);
 		free from persistent and working map;
 	lsn	- log sequence number;
	
	  allocate from persistent map;
 (maplock->flag & mlckALLOCPXDLIST) 
 	txFreeMap()
  function:	free from persistent andor working map;
  todo: optimization
	
	  free from persistent map;
 (maplock->flag & mlckALLOCPXDLIST) 
	
	  free from working map;
 (maplock->flag & mlckFREEPXDLIST) 
 	txFreelock()
  function:	remove tlock from inode anonymous locklist
		
		  If inode was on anon_list, remove it
 	txAbort()
  function: abort tx before commit;
  frees line-locks and segment locks for all
  segments in comdata structure.
  Optionally sets state of file-system to FM_DIRTY in super-block.
  log age of page-frames in memory for which caller has
  are reset to 0 (to avoid logwarap).
	
	  free tlocks of the transaction
			
			  reset lsn of page to avoid logwarap:
			 
			  (page may have been previously committed by another
			  transaction(s) but has not been paged, i.e.,
			  it may be on logsync list even though it has not
			  been logged for the current tx.)
 insert tlock at head of freelist 
 caller will free the transaction block 
	
	  mark filesystem dirty
 	txLazyCommit(void)
 	All transactions except those changing ipimap (COMMIT_FORCE) are
 	processed by this routine.  This insures that the inode and block
 	allocation maps are updated in order.  For synchronous transactions,
 	let the user thread finish processing after txUpdateMap() is called.
		 We must have gotten ahead of the user thread
 LOGGC_LOCK
 LOGGC_WAKEUP
	
	  Can't release log->gclock until we've tested tblk->flag
 LOGGC_UNLOCK
 Convert back to tid 
 LOGGC_UNLOCK
 	jfs_lazycommit(void)
 	To be run as a kernel daemon.  If lbmIODone is called in an interrupt
 	context, or where blocking is not wanted, this routine will process
 	committed transactions from the unlock queue.
 OK to wake another thread 
				
				  For each volume, the transactions must be
				  handled in order.  If another commit thread
				  is handling a tblk for this superblock,
				  skip it
				
				  Remove transaction from queue
				
				  Don't continue in the for loop.  (We can't
				  anyway, it's unsafe!)  We want to go back to
				  the beginning of the list.
 If there was nothing to do, don't continue 
 In case a wakeup came while all threads were active 
	
	  Don't wake up a commit thread if there is already one servicing
	  this superblock, or if the last one we woke up hasn't started yet.
 	txQuiesce
 	Block all new transactions and push anonymous transactions to
 	completion
 	This does almost the same thing as jfs_sync below.  We don't
 	worry about deadlocking when jfs_tlocks_low is set, since we would
 	expect jfs_sync to get us out of that jam.
		
		  inode will be removed from anonymous list
		  when it is committed
		
		  Just to be safe.  I don't know how
		  long we can run without blocking
	
	  If jfs_sync is running in parallel, there could be some inodes
	  on anon_list2.  Let's check.
	
	  We may need to kick off the group commit
  txResume()
  Allows transactions to start again following txQuiesce
 	jfs_sync(void)
 	To be run as a kernel daemon.  This is awakened when tlocks run low.
 	We write any inodes that have anonymous tlocks so they will become
 	available.
		
		  write each inode on the anonymous inode list
				
				  Inode is being freed
				
				  inode will be removed from anonymous list
				  when it is committed
				
				  Just to be safe.  I don't know how
				  long we can run without blocking
				 We can't get the commit mutex.  It may
				  be held by a thread waiting for tlock's
				  so let's not block here.  Save it to
				  put back on the anon_list.
 Move from anon_list to anon_list2 
 Add anon_list2 back to anon_list 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
 	jfs_dtree.c: directory B+-tree manager
  B+-tree with variable length key directory:
  each directory page is structured as an array of 32-byte
  directory entry slots initialized as a freelist
  to avoid searchcompaction of free space at insertion.
  when an entry is inserted, a number of slots are allocated
  from the freelist as required to store variable length data
  of the entry; when the entry is deleted, slots of the entry
  are returned to freelist.
  leaf entry stores full name as key and file serial number
  (aka inode number) as data.
  internalrouter entry stores sufffix compressed name
  as key and simple extent descriptor as data.
  each directory page maintains a sorted entry index table
  which stores the start slot index of sorted entries
  to allow binary search on the table.
  directory starts as a rootleaf page in on-disk inode
  inline data area.
  when it becomes full, it starts a leaf of a external extent
  of length of 1 block. each time the first leaf becomes full,
  it is extended rather than split (its size is doubled),
  until its length becoms 4 KBytes, from then the extent is split
  with new 4 Kbyte extent when it becomes full
  to reduce external fragmentation of small directories.
  blah, blah, blah, for linear scan of directory in pieces by
  readdir().
 	case-insensitive directory file system
  names are stored in case-sensitive way in leaf entry.
  but stored, searched and compared in case-insensitive (uppercase) order
  (i.e., both search key and entry key are folded for searchcompare):
  (note that case-sensitive order is BROKEN in storage, e.g.,
   sensitive: Ad, aB, aC, aD -> insensitive: aB, aC, aD, Ad
   entries which folds to the same key makes up a equivalent class
   whose members are stored as contiguous cluster (may cross page boundary)
   but whose order is arbitrary and acts as duplicate, e.g.,
   abc, Abc, aBc, abC)
  once match is found at leaf, requires scan forwardbackward
  either for, in case-insensitive search, duplicate
  or for, in case-sensitive search, for exact match
  router entry must be createdstored in case-insensitive way
  in internal entry:
  (right most key of left page and left most key of right page
  are folded, and its suffix compression is propagated as router
  key in parent)
  (e.g., if split occurs <abc> and <aBd>, <ABD> trather than <aB>
  should be made the router key for the split)
  case-insensitive search:
 	fold search key;
 	case-insensitive search of B-tree:
 	for internal entry, router key is already folded;
 	for leaf entry, fold the entry key before comparison.
 	if (leaf entry case-insensitive match found)
 		if (next entry satisfies case-insensitive match)
 			return EDUPLICATE;
 		if (prev entry satisfies case-insensitive match)
 			return EDUPLICATE;
 		return match;
 	else
 		return no match;
 	serialization:
  target directory inode lock is being held on entryexit
  of all main directory service routines.
 	log based recovery:
 dtree split parameter 
 get page buffer for specified block address 
 for consistency 
  forward references
 	read_index_page()
 	Reads a page of a directory's index table.
 	Having metadata mapped into the directory inode's address space
 	presents a multitude of problems.  We avoid this by mapping to
 	the absolute address space outside of the _metapage routines
 	get_index_page()
 	Same as get_index_page(), but get's a new page without reading
 	find_index()
 	Returns dtree page containing directory table entry for specified
 	index and pointer to its entry.
 	mp must be released by caller.
		
		  Inline directory table
	
	 	Linelock slot size is twice the size of directory table
	 	slot size.  512 entries per page.
 	add_index()
 	Adds an entry to the directory index table.  This is used to provide
 	each directory entry with a persistent index in which to resume
 	directory traversals
		
		  i_size reflects size of index table, or 8 bytes per entry.
		
		  dir table fits inline within inode
		
		  It's time to move the inline table to an external
		  page and begin to build the xtree
		
		  Save the table, we're going to overwrite it with the
		  xtree root
		
		  Initialize empty x-tree
		
		  Add the first block to the xtree
 This really shouldn't fail 
 tlckDATA slot size is 16 bytes 
		
		  Logging is now directed by xtree tlocks
		
		  This will be the beginning of a new page
 Just looks better 
 	free_index()
 	Marks an entry to the directory index table as free.
 	modify_index()
 	Changes an entry in the directory index table
 	read_index()
 	reads a directory table slot
 	dtSearch()
  function:
 	Search for the entry with specified key
  parameter:
  return: 0 - search result on stack, leaf page pinned;
 	   errno - IO error
 init for empty page 
 initial in-line directory 
 uppercase search key for c-i directory 
 only uppercase if case-insensitive support is on 
 reset stack 
 init level count for max pages to split 
	
	 	search down tree from root:
	 
	  between two consecutive entries of <Ki, Pi> and <Kj, Pj> of
	  internal page, child page Pi contains entry with k, Ki <= K < Kj.
	 
	  if entry with search key K is not found
	  internal page search find the entry with largest key Ki
	  less than K which point to the child page to search;
	  leaf page search find the entry with smallest key Kj
	  greater than K so that the returned index is the position of
	  the entry to be shifted right for insertion of new entry.
	  for empty tree, search key is greater than any key of the tree.
	 
	  by convention, root bn = 0.
 getpin the page to search 
 get sorted entry table of the page 
		
		  binary search with search key K on the current page.
 uppercase leaf name to compare 
 router key is in uppercase 
				
				 	search hit
				 search hit - leaf page:
				  return the entry found
					
					  search for JFS_LOOKUP
					
					  search for JFS_CREATE
					
					  search for JFS_REMOVE or JFS_RENAME
					
					  JFS_REMOVE|JFS_FINDDIR|JFS_RENAME
 save search result 
				 search hit - internal page:
				  descendsearch its child page
		
		 	search miss
		 
		  base is the smallest index with key (Kj) greater than
		  search key (K) and may be zero or (maxindex + 1) index.
		
		  search miss - leaf page
		 
		  return location of entry (base) where new entry with
		  search key K is to be inserted.
			
			  search for JFS_LOOKUP, JFS_REMOVE, or JFS_RENAME
			
			  search for JFS_CREATE|JFS_FINDDIR:
			 
			  save search result
		
		  search miss - internal page
		 
		  if base is non-zero, decrement base by one to get the parent
		  entry of the child page to search.
		
		  go down to child page
 update max. number of pages to split 
			 Something's corrupted, mark filesystem dirty so
			  chkdsk will fix it.
 push (bn, index) of the parent pageentry 
 get the child page block number 
 unpin the parent page 
 	dtInsert()
  function: insert an entry to directory tree
  parameter:
  return: 0 - success;
 	   errno - failure;
 meta-page buffer 
 base B+-tree index page 
 split information 
	
	 	retrieve search result
	 
	  dtSearch() returns (leaf page pinned, index at which to insert).
	  n.b. dtSearch() may return index of (maxindex + 1) of
	  the full page.
	
	 	insert entry for new key
 signifies legacy directory format 
	
	 	leaf page does not have enough room for new entry:
	 
	 	extendsplit the leaf page;
	 
	  dtSplitUp() will insert the entry and unpin the leaf page.
	
	 	leaf page does have enough room for new entry:
	 
	 	insert the new data entry into the leaf page;
	
	  acquire a transaction lock on the leaf page
 linelock header 
 linelock stbl of non-root leaf page 
 unpin the leaf page 
 	dtSplitUp()
  function: propagate insertion bottom up;
  parameter:
  return: 0 - success;
 	   errno - failure;
 	leaf page unpinned;
 split page 
 new right page split from sp 
 new right page extent descriptor 
 left child page 
 index of entry of insertion 
 parent page entry on traverse stack 
 get split page 
	
	 	split leaf page
	 
	  The split routines insert the new entry, and
	  acquire txLock as appropriate.
	
	 	split root leaf page:
		
		  allocate a single extent child page
 stbl size 
 header + entries 
	
	 	extend first leaf page
	 
	  extend the 1st extent if less than buffer page size
	  (dtExtendPage() reurns leaf page unpinned)
 stbl size 
 Allocate blocks to quota. 
 free relocated extent 
 free extended delta 
	
	 	split leaf page <sp> into <sp> and a new right page <rp>.
	 
	  return <rp> pinned and its extent descriptor <rpxd>
	
	  allocate new directory page extent and
	  new index page(s) to cover page split(s)
	 
	  allocation hint: ?
 undo allocation 
 undo allocation 
	
	  propagate up the router entry for the leaf page just split
	 
	  insert a router entry for the new page into the parent page,
	  propagate the insertsplit up the tree by walking back the stack
	  of (bn of parent page, index of child page entry in parent page)
	  that were traversed during the search for the page that split.
	 
	  the propagation of insertsplit up the tree stops if the root
	  splits or the page inserted into doesn't have to split to hold
	  the new entry.
	 
	  the parent entry for the split page remains the same, and
	  a new entry is inserted at its right with the first key and
	  block number of the new right page.
	 
	  There are a maximum of 4 pages pinned at any time:
	  two children, left parent and right parent (when the parent splits).
	  keep the child pages pinned while working on the parent.
	  make sure that all pins are released at exit.
 parent page specified by stack frame <parent> 
 keep current child pages (<lp>, <rp>) pinned 
		
		  insert router entry in parent for new right child page <rp>
 get the parent page <sp> 
		
		  The new key entry goes ONE AFTER the index of parent entry,
		  because the split was to the right.
		
		  compute the key for the router entry
		 
		  key suffix compression:
		  for internal pages that have leaf pages as children,
		  retain only what's needed to distinguish between
		  the new entry and the entry on the page to its left.
		  If the keys compare equal, retain the entire key.
		 
		  note that compression is performed only at computing
		  router key at the lowest internal level.
		  further compression of the key between pairs of higher
		  level internal pages loses too much information and
		  the search may fail.
		  (e.g., two adjacent leaf pages of {a, ..., x} {xx, ...,}
		  results in two adjacent parent entries (a)(xx).
		  if split occurs between these two entries, and
		  if compression is applied, the router key of parent entry
		  of right page (x) will divert search for x into right
		  subtree and miss x in the left subtree.)
		 
		  the entire key must be retained for the next-to-leftmost
		  internal key at any level of the tree, or search may fail
		  (e.g., ?)
			
			  compute the length of prefix for suffix compression
			  between last entry of left page and first entry
			  of right page
 compute uppercase router prefix key 
				 next to leftmost entry of
 compute uppercase router key 
 unpin left child page 
		
		  compute the data for the router entry
 child page xd 
		
		  parent page is full - split the parent page
 init for parent page split 
 index at insert 
 split->data = data; 
 unpin right child page 
			 The split routines insert the new entry,
			  acquire txLock as appropriate.
			  return <rp> pinned and its block number <rbn>.
 smp and rmp are pinned 
		
		  parent page is not full - insert router entry in parent page
			
			  acquire a transaction lock on the parent page
 linelock header 
 linelock stbl of non-root parent page 
 exit propagate up 
 unpin current split and its right page 
	
	  free remaining extents allocated for split
 Rollback quota allocation 
 	dtSplitPage()
  function: Split a non-root page of a btree.
  parameter:
  return: 0 - success;
 	   errno - failure;
 	return split and new page pinned;
 new right page allocated 
 new right page block number 
 get split page 
	
	  allocate the new right page for the split
 Allocate blocks to quota. 
	
	  acquire a transaction lock on the new right page
	
	  acquire a transaction lock on the split page
	 
	  action:
 linelock header of split page 
	
	  initializeupdate sibling pointers between sp and rp
	
	  initialize new right page
 compute sorted entry table at start of extent data area 
 in unit of slot 
 init freelist 
	
	 	sequential append at tail: append without split
	 
	  If splitting the last page on a level because of appending
	  a entry to it (skip is maxentry), it's likely that the access is
	  sequential. Adding an empty page on the side of the level is less
	  work and can push the fill factor much higher than normal.
	  If we're wrong it's no big deal, we'll just do the split the right
	  way next time.
	  (It may look like it's equally easy to do a similar hack for
	  reverse sorted data, that is, split the tree left,
	  but it's not. Be my guest.)
 linelock header + stbl (first slot) of new page 
		
		  initialize freelist of new right page
 insert entry at the first entry of the new right page 
	
	 	non-sequential insert (at possibly middle page)
	
	  update prev pointer of previous right sibling page;
		
		  acquire a transaction lock on the next page
 linelock header of previous right sibling page 
	
	  split the data between the split and right pages.
 swag 
	
	 	compute fill factor for split pages
	 
	  <nxt> traces the next entry to move to rp
	  <off> traces the next entry to stay in sp
 check for fill factor with new entry size 
 advance to next entry to move in sp 
 <nxt> poins to the 1st entry to move 
	
	 	move entries to right page
	 
	  dtMoveEntry() initializes rp and reserves entry for insertion
	 
	  split page moved out entries are linelocked;
	  newright page moved in entries are linelocked;
 linelock header + stbl of new right page 
	
	  finalize freelist of new right page
	
	  Update directory index table for entries now in right page
	
	  the skipped index was on the left page,
 insert the new entry in the split page 
 linelock stbl of split page 
	
	  the skipped index was on the right page,
 adjust the skip index to reflect the new position 
 insert the new entry in the right page 
 	dtExtendPage()
  function: extend 1stonly directory leaf page
  parameter:
  return: 0 - success;
 	   errno - failure;
 	return extended page pinned;
 get page to extend 
 get parentroot page 
	
	 	extend the extent
 in-place extension 
 relocation 
 save moved extent descriptor for later free 
		
		  Update directory index table to reflect new page address
	
	 	extend the page
	
	  acquire a transaction lock on the extendedleaf page
 update buffer extent descriptor of extended page 
	
	  copy old stbl to new stbl at start of extended area
	
	  in-line extension: linelock old area of extended page
 linelock header 
 linelock new stbl of extended page 
	
	  relocation: linelock whole relocated area
 sp->header.nextindex remains the same 
	
	  add old stbl region at head of freelist
	
	  append free region of newly extended area at tail of freelist
 init free region of newly extended area 
 append new free region at tail of old freelist 
	
	  insert the new entry
	
	  linelock any freeslots residing in old extent
	
	 	update parent entry on the parentroot page
	
	  acquire a transaction lock on the parentroot page
 linelock parent entry - 1st slot 
 update the parent pxd for page extension 
 	dtSplitRoot()
  function:
 	split the full root page into
 	originalrootsplit page and new right page
 	i.e., root remains fixed in tree anchor (inode) and
 	the root is copied to a single new right child page
 	since root page << non-root page, and
 	the split root page contains a single entry for the
 	new right child page.
  parameter:
  return: 0 - success;
 	   errno - failure;
 	return new page pinned;
 get split root page 
	
	 	allocateinitialize a single (right) child page
	 
	  N.B. at first split, a one (or two) block to fit new entry
	  is allocated; at subsequent split, a full page is allocated;
 Allocate blocks to quota. 
	
	  acquire a transaction lock on the new right page
 initialize sibling pointers 
	
	 	move in-line root page into new right page extent
 linelock header + copied entries + new stbl (1st slot) in new page 
 1 + 8 + 1 
 copy old stbl to new stbl at start of extended area 
 copy old data area to start of new data area 
	
	  append free region of newly extended area at tail of freelist
 init free region of newly extended area 
 append new free region at tail of old freelist 
	
	  Update directory index table for entries now in right page
	
	  insert the new entry into the new rightchild page
	  (skip index in the new right page will not change)
	
	 	reset parentroot page
	 
	  set the 1st entry offset to 0, which force the left-most key
	  at any level of the tree to be less than any search key.
	 
	  The btree comparison code guarantees that the left-most key on any
	  level of the tree is never used, so it doesn't need to be filled in.
	
	  acquire a transaction lock on the root page (in-memory inode)
 linelock root 
 update page header of root 
 init the first entry 
 init freelist 
 init free region of remaining area 
 	dtDelete()
  function: delete the entry(s) referenced by a key.
  parameter:
  return:
	
	 	search for the entry to delete:
	 
	  dtSearch() returns (leaf page pinned, index at which to delete).
 retrieve search result 
	
	  We need to find put the index of the next entry into the
	  directory index table in order to resume a readdir from this
	  entry.
			
			  Last entry in this leaf page
 Read next leaf page 
	
	  the leaf page becomes empty, delete the page
 delete empty page 
	
	  the leaf page has other entries remaining:
	 
	  delete the entry from the leaf page.
		
		  acquire a transaction lock on the leaf page
		
		  Do not assume that dtlck->index will be zero.  During a
		  rename within a directory, this transaction may have
		  modified this page already when adding the new entry.
 linelock header 
 linelock stbl of non-root leaf page 
 free the leaf entry 
		
		  Update directory index table for entries moved in stbl
 	dtDeleteUp()
  function:
 	free empty pages as propagating deletion up the tree
  parameter:
  return:
	
	 	keep the root leaf page which has become empty
		
		  reset the root
		 
		  dtInitRoot() acquires txlock on the root
	
	 	free the non-root leaf page
	
	  acquire a transaction lock on the page
	 
	  write FREEXTENT|NOREDOPAGE log record
	  N.B. linelock is overlaid as freed extent descriptor, and
	  the buffer page is freed;
 update sibling pointers 
 Free quota allocation. 
 freeinvalidate its buffer page 
	
	 	propagate page deletion up the directory tree
	 
	  If the delete from the parent page makes it empty,
	  continue all the way up the tree.
	  stop if the root page is reached (which is never deleted) or
	  if the entry deletion does not empty the page.
 pin the parent page <sp> 
		
		  free the extent of the child page deleted
		
		  delete the entry for the child page from parent
		
		  the parent has the single entry being deleted:
		 
		  free the parent page which has become empty.
			
			  keep the root internal page which has become empty
				
				  reset the root
				 
				  dtInitRoot() acquires txlock on the root
			
			  free the parent page
				
				  acquire a transaction lock on the page
				 
				  write FREEXTENT|NOREDOPAGE log record
 update sibling pointers 
 Free quota allocation 
 freeinvalidate its buffer page 
 propagate up 
		
		  the parent has other entries remaining:
		 
		  delete the router entry from the parent page.
		
		  acquire a transaction lock on the page
		 
		  action: router entry deletion
 linelock header 
 linelock stbl of non-root leaf page 
 free the router entry 
 reset key of new leftmost entry of level (for consistency) 
 unpin the parent page 
 exit propagation up 
  NAME:	dtRelocate()
  FUNCTION:	relocate dtpage (internal or leaf) of directory;
 		This function is mainly used by defragfs utility.
	
	 	1. get the internal parent dtpage covering
	 	router entry for the tartget page to be relocated;
 retrieve search result 
	
	 	2. relocate the target dtpage
 read in the target page from src extent 
 release the pinned parent page 
	
	  read in sibling pages if any to update sibling pointers;
 at this point, all xtpages to be updated are in memory 
	
	  update sibling pointers of sibling dtpages if any;
 linelock header 
 linelock header 
	
	  update the target dtpage to be relocated
	 
	  write LOG_REDOPAGE of LOG_NEW type for dst page
	  for the whole target page (logredo() will apply
	  after image and update bmap for allocation of the
	  dst extent), and update bmap for allocation of
	  the dst extent;
 linelock header 
 update the self address in the dtpage header 
	 the dst page is the same as the src page, i.e.,
	  linelock for afterimage of the whole page;
 update the buffer extent descriptor of the dtpage 
 unpin the relocated page 
	 the moved extent is dtpage, then a LOG_NOREDOPAGE log rec
	  needs to be written (in logredo(), the LOG_NOREDOPAGE log rec
	  will also force a bmap update ).
	
	 	3. acquire maplock for the source extent to be freed;
	 for dtpage relocation, write a LOG_NOREDOPAGE record
	  for the source dtpage (logredo() will init NoRedoPage
	  filter and will also update bmap for free of the source
	  dtpage), and upadte bmap for free of the source dtpage;
	
	 	4. update the parent router entry for relocation;
	 
	  acquire tlck for the parent entry covering the target dtpage;
	  write LOG_REDOPAGE to apply after image only;
 update the PXD with the new address 
 unpin the parent dtpage 
  NAME:	dtSearchNode()
  FUNCTION:	Search for an dtpage containing a specified address
 		This function is mainly used by defragfs utility.
  NOTE:	Search result on stack, the found page is pinned at exit.
 		The result page must be an internal dtpage.
 		lmxaddr give the address of the left most page of the
 		dtree level, in which the required dtpage resides.
 initial in-line directory 
 reset stack 
	
	 	descend tree to the level with specified leftmost page
	 
	   by convention, root bn = 0.
 getpin the page to search 
		 does the xaddr of leftmost page of the levevl
		  matches levevl search key ?
		
		  descend down to leftmost child page
 get the leftmost entry 
 get the child page block address 
 unpin the parent page 
	
	 	search each page at the current levevl
 found the specified router entry 
 get the right sibling page if any 
 unpin current page 
 get the right sibling page 
 _NOTYET 
 	dtRelink()
  function:
 	link around a freed page.
  parameter:
 	fp:	page to be freed
  return:
 update prev pointer of the next page 
		
		  acquire a transaction lock on the next page
		 
		  action: update prev pointer;
 linelock header 
 update next pointer of the previous page 
		
		  acquire a transaction lock on the prev page
		 
		  action: update next pointer;
 linelock header 
 	dtInitRoot()
  initialize directory root (inline in inode)
	
	  If this was previously an non-empty directory, we need to remove
	  the old directory table.
			
			  We're playing games with the tid's xflag.  If
			  we're removing a regular file, the file's xtree
			  is committed with COMMIT_PMAP, but we always
			  commit the directories xtree with COMMIT_PWMAP.
			
			  xtTruncate isn't guaranteed to fully truncate
			  the xtree.  The caller needs to check i_size
			  after committing the transaction to see if
			  additional truncation is needed.  The
			  COMMIT_Stale flag tells caller that we
			  initiated the truncation.
	
	  acquire a transaction lock on the root
	 
	  action: directory initialization;
 linelock root 
 init freelist 
 init data area of root 
 init '..' entry 
 	add_missing_indices()
  function: Fix dtree page in which one or more entries has an invalid index.
 	     fsck.jfs should really fix this, but it currently does not.
 	     Called from jfs_readdir when bad index is detected.
  Buffer to hold directory entry info while traversing a dtree page
  before being fed to the filldir function
  function to determine next variable-sized jfs_dirent in buffer
 	jfs_readdir()
  function: read directory entries sequentially
 	from the specified entry offset
  parameter:
  return: offset = (pn, index) of start entry
 	of next jfs_readdir()dtRead()
 legacy OS2 style position 
 If we can't fix broken index 
		
		  persistent index is stored in directory entries.
		  Special cases:	 0 = .
		 			 1 = ..
		 			-1 = End of directory
		
		  NFSv4 reserves cookies 1 and 2 for . and .. so the value
		  we return to the vfs is one greater than the one we use
		  internally.
 Stale position.  Directory has shrunk 
				
				  self "."
			
			  parent ".."
			
			  Find first entry of left-most leaf
		
		  Legacy filesystem - OS2 & Linux JFS < 0.3.6
		 
		  pn = 0; index = 1:	First entry "."
		  pn = 0; index = 2:	Second entry ".."
		  pn > 0:		Real entries, pn=1 -> leftmost page
		  pn = index = -1:	No more entries
 build "." entry 
 build ".." entry 
 get start leaf page and index 
 offset beyond directory eof ? 
 DBCS codepages could overrun dirent_buf 
				
				  d->index should always be valid, but it
				  isn't.  fsck.jfs doesn't create the
				  directory index for the lost+found
				  directory.  Rather than let it go,
				  we can try to fix it.
						
						  setting overflow and setting
						  index to i will cause the
						  same page to be processed
						  again starting here
				
				  We add 1 to the index because we may
				  use a value of 2 internally, and NFSv4
				  doesn't like that.
 copy the name of headonly segment 
 copy name in the additional segment(s) 
 Sanity Check 
 Point to next leaf page 
 update offset (pn:index) for new page 
 unpin previous leaf page 
 	dtReadFirst()
  function: get the leftmost page of the directory
 initial in-line directory 
 reset stack 
	
	 	descend leftmost path of the tree
	 
	  by convention, root bn = 0.
		
		  leftmost leaf page
 return leftmost entry 
		
		  descend down to leftmost child page
 push (bn, index) of the parent pageentry 
 get the leftmost entry 
 get the child page block address 
 unpin the parent page 
 	dtReadNext()
  function: get the page of the specified offset (pn:index)
  return: if (offset > eof), bn = -1;
  note: if index > nextindex of the target leaf page,
  start with 1st entry of next leaf page;
	
	  get leftmost leaf page pinned
 get leaf page 
 get the start offset (pn:index) 
 Now pn = 0 represents leftmost leaf 
 start at leftmost page ? 
 offset beyond eof ? 
 start with 1st entry of next leaf page 
 start at non-leftmost page: scan parent pages for large pn 
 start after next leaf page ? 
 get leaf page pn = 1 
 unpin leaf page 
 offset beyond eof ? 
	
	  scan last internal page level to get target leaf page
 unpin leftmost leaf page 
 get left most parent page 
 scan parent pages at last internal page level 
 get next parent page address 
 unpin current parent page 
 offset beyond eof ? 
 get next parent page 
 update parent page stack frame 
 get leaf page address 
 unpin parent page 
	
	  get target leaf page
	
	  leaf page has been completed:
	  start with 1st entry of next leaf page
 unpin leaf page 
 offset beyond eof ? 
 get next leaf page 
 start with 1st entry of next leaf page 
 return target leaf page pinned 
 	dtCompare()
  function: compare search key with an internal entry
  return:
 	< 0 if k is < record
 	= 0 if k is = record
 	> 0 if k is > record
 search key 
 directory page 
 entry slot index 
	
	  force the left-most key on internal pages, at any level of
	  the tree, to be less than any search key.
	  this obviates having to update the leftmost key on an internal
	  page when the user inserts a new key in the tree smaller than
	  anything that has been stored.
	 
	  (? ifwhen dtSearch() narrows down to 1st entry (index = 0),
	  at any internal page at any level of the tree,
	  it descends to child of the entry anyway -
	  ? make the entry as min size dummy entry)
	 
	  if (e->index == 0 && h->prevpg == P_INVALID && !(h->flags & BT_LEAF))
	  return (1);
 compare with headonly segment 
 compare with additional segment(s) 
 compare with next name segment 
 	ciCompare()
  function: compare search key with an (leafinternal) entry
  return:
 	< 0 if k is < record
 	= 0 if k is = record
 	> 0 if k is > record
 search key 
 directory page 
 entry slot index 
	
	  force the left-most key on internal pages, at any level of
	  the tree, to be less than any search key.
	  this obviates having to update the leftmost key on an internal
	  page when the user inserts a new key in the tree smaller than
	  anything that has been stored.
	 
	  (? ifwhen dtSearch() narrows down to 1st entry (index = 0),
	  at any internal page at any level of the tree,
	  it descends to child of the entry anyway -
	  ? make the entry as min size dummy entry)
	 
	  if (e->index == 0 && h->prevpg == P_INVALID && !(h->flags & BT_LEAF))
	  return (1);
	
	  leaf page entry
	
	  internal page entry
 compare with headonly segment 
 only uppercase if case-insensitive support is on 
 compare with additional segment(s) 
 compare with next name segment 
 only uppercase if case-insensitive support is on 
 	ciGetLeafPrefixKey()
  function: compute prefix of suffix compression
 	     from two adjacent leaf entries
 	     across page boundary
  return: non-zero on error
 get left and right key 
 compute prefix 
 l->namlen <= r->namlen since l <= r 
 l->namelen == r->namelen 
 	dtGetKey()
  function: get key of the entry
 entry index 
 get entry 
	
	  move headonly segment
	
	  move additional segment(s)
 get next segment 
 	dtInsertEntry()
  function: allocate free slot(s) and
 	     write a leafinternal entry
  return: entry slot index
 allocate a free slot 
 open new linelock 
 write headonly segment 
 write additional segment(s) 
 get free slot 
 is next slot contiguous ? 
 close current linelock 
 open new linelock 
 close current linelock 
 terminate lastonly segment 
 single segment entry 
 multi-segment entry 
 if insert into middle, shift right succeeding entries in stbl 
			
			  Need to update slot number for entries that moved
			  in the stbl
 advance next available entry index of stbl 
 	dtMoveEntry()
  function: move entries from splitleft page to newright page
 	nextindex of dst page and freelistfreecnt of both pages
 	are updated.
 src slot index 
 dst entry index 
 dst slot index 
 sorted entry table 
 first (whole page) free slot 
 linelock destination entry slot 
 linelock source entry slot 
	
	  move entries
 is next slot contiguous ? 
 close current linelock 
 open new linelock 
		
		  move headonly segment of an entry
 get dst slot 
 get src slot and move 
 get source entry 
 little-endian 
 update dst headonly segment next field 
 free src headonly segment 
		
		  move additional segment(s) of the entry
 is next slot contiguous ? 
 close current linelock 
 open new linelock 
 get next source segment 
 get next destination free slot 
 free source segment 
 end while 
 terminate dst lastonly segment 
 single segment entry 
 multi-segment entry 
 end for 
 close current linelock 
 update source header 
 update destination header 
 	dtDeleteEntry()
  function: free a (leafinternal) entry
  log freelist header, stbl, and each segment slot of entry
  (even though lastonly segment next field is modified,
  physical image logging requires all segment slots of
  the entry logged to avoid applying previous updates
  to the same slots)
 free entry slot index 
 get free entry slot index 
 open new linelock 
 get the headonly segment 
 find the lastonly segment 
 is next slot contiguous ? 
 close current linelock 
 open new linelock 
 close current linelock 
 update freelist 
	 if delete from middle,
	  shift left the succedding entries in the stbl
 	dtTruncateEntry()
  function: truncate a (leafinternal) entry
  log freelist header, stbl, and each segment slot of entry
  (even though lastonly segment next field is modified,
  physical image logging requires all segment slots of
  the entry logged to avoid applying previous updates
  to the same slots)
 truncate entry slot index 
 get free entry slot index 
 open new linelock 
 get the headonly segment 
 find the lastonly segment 
 is next slot contiguous ? 
 close current linelock 
 open new linelock 
 close current linelock 
 update freelist 
 	dtLinelockFreelist()
 directory page 
 max slot index 
 free entry slot index 
 get free entry slot index 
 open new linelock 
 find the lastonly segment 
 is next slot contiguous ? 
 close current linelock 
 open new linelock 
 close current linelock 
  NAME: dtModify
  FUNCTION: Modify the inode number part of a directory entry
  PARAMETERS:
 	tid	- Transaction id
 	ip	- Inode of parent directory
 	key	- Name of entry to be modified
 	orig_ino	- Original inode number expected in entry
 	new_ino	- New inode number to put into entry
 	flag	- JFS_RENAME
  RETURNS:
 	-ESTALE	- If entry found does not match orig_ino passed in
 	-ENOENT	- If no entry can be found to match key
 	0	- If successfully modified entry
 entry slot index 
	
	 	search for the entry to modify:
	 
	  dtSearch() returns (leaf page pinned, index at which to modify).
 retrieve search result 
	
	  acquire a transaction lock on the leaf page of named entry
 get slot index of the entry 
 linelock entry 
 get the headonly segment 
 substitute the inode number of the entry 
 unpin the leaf page 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines  Corp., 2000-2004
 convert block number to bmap file page number 
 	jfs_extendfs()
  function: extend file system;
    |-------------------------------|----------|----------|
    file system space               fsck       inline log
                                    workspace  space
  input:
 	new LVSize: in LV blocks (required)
 	new LogSize: in LV blocks (optional)
 	new FSSize: in LV blocks (optional)
  new configuration:
  1. set new LogSize as specified or default from new LVSize;
  2. compute new FSCKSize from new LVSize;
  3. set new FSSize as MIN(FSSize, LVSize-(LogSize+FSCKSize)) where
     assert(new FSSize >= old FSSize),
     i.e., file system must not be shrunk;
 If the volume hasn't grown, get out now 
 check the device 
 Can't extend write-protected drive 
	
	 	reconfigure LV spaces
	 	---------------------
	 
	  validate new size, or, if not specified, determine new size
	
	  reconfigure inline log space:
			
			  no size specified: default to 1256 of aggregate
			  size; rounded up to a megabyte boundary;
			
			  convert the newLogSize to fs blocks.
			 
			  Since this is given in megabytes, it will always be
			  an even number of pages.
	
	  reconfigure fsck work space:
	 
	  configure it to the end of the logical volume regardless of
	  whether file system extends to the end of the aggregate;
	  Need enough 4k pages to cover:
	   - 1 bit per block in aggregate rounded up to BPERDMAP boundary
	   - 1 extra page to handle control page and intermediate level pages
	   - 50 extra pages for the chkdsk service log
	
	  compute new file system space;
 file system cannot be shrunk 
	
	  If we're expanding enough that the inline log does not overlap
	  the old one, we can format the new log before we quiesce the
	  filesystem.
	
	 	quiesce file system
	 
	  (prepare to move the inline log and to prevent map update)
	 
	  block any new transactions and wait for completion of
	  all wip transactions and flush modified pages s.t.
	  on-disk file system is in consistent state and
	  log is not required for recovery.
 Reset size of direct inode 
		
		  deactivate old inline log
		
		  mark on-disk super block for fs in transition;
		 
		  update on-disk superblock for the new space configuration
		  of inline log space and fsck work space descriptors:
		  N.B. FS descriptor is NOT updated;
		 
		  crash recovery:
		  logredo(): if FM_EXTENDFS, return to fsck() for cleanup;
		  fsck(): if FM_EXTENDFS, reformat inline log and fsck
		  workspace from superblock inline log descriptor and fsck
		  workspace descriptor;
 read in superblock 
 mark extendfs() in progress 
 synchronously update superblock 
		
		  format new inline log synchronously;
		 
		  crash recovery: if log move in progress,
		  reformat log and exit success;
		
		  activate new log
	
	 	extend block allocation map
	 	---------------------------
	 
	  extendfs() for new extension, retry after crash recovery;
	 
	  note: both logredo() and fsck() rebuild map from
	  the bitmap and configuration parameter from superblock
	  (disregarding all other control information in the map);
	 
	  superblock:
	   s_size: aggregate size in physical blocks;
	
	 	compute the new block allocation map configuration
	 
	  map dinode:
	   di_size: map file size in byte;
	   di_nblocks: number of blocks allocated for map file;
	   di_mapsize: number of blocks in aggregate (covered by map);
	  map control page:
	   db_mapsize: number of blocks in aggregate (covered by map);
	 number of data pages of new bmap file:
	  roundup new size to full dmap page boundary and
	  add 1 extra dmap page for next extendfs()
	
	 	extend map from current map (WITHOUT growing mapfile)
	 
	  map new extension with unmapped part of the last partial
	  dmap page, if applicable, and extra page(s) allocated
	  at end of bmap by mkfs() or previous extendfs();
 compute number of blocks requested to extend 
 eXtension Address 
 eXtension Size 
 We need to know if this changes 
 compute number of blocks that can be extended by current mapfile 
	
	  update map pages for new extension:
	 
	  updateinit dmap and bubble up the control hierarchy
	  incrementally fold up dmaps into upper levels;
	  update bmap control page;
	
	  the map now has extended to cover additional nblocks:
	  dn_mapsize = oldMapsize + nblocks;
 ipbmap->i_mapsize += nblocks; 
	
	 	grow map file to cover remaining extension
	 	andor one extra dmap page for next extendfs();
	 
	  allocate new map pages and its backing blocks, and
	  update map file xtree
 compute number of data pages of current bmap file 
 need to grow map file ? 
	
	  grow bmap file for the new map pages required:
	 
	  allocate growth at the start of newly extended region;
	  bmap file only grows sequentially, i.e., both data pages
	  and possibly xtree index pages may grow in append mode,
	  s.t. logredo() can reconstruct pre-extension state
	  by washing away bmap file of pages outside s_size boundary;
	
	  journal map file growth as if a regular file growth:
	  (note: bmap is created with di_mode = IFJOURNAL|IFREG);
	 
	  journaling of bmap file growth is not required since
	  logredo() docan not use log records of bmap file growth
	  but it provides careful write semantics, pmap update, etc.;
	 synchronous write of data pages: bmap data pages are
	  cached in meta-data cache, and not written out
	  by txCommit();
 first new page number 
 update bmap file size 
	
	  map file has been grown now to cover extension to further out;
	  di_size = new map file size;
	 
	  if huge extension, the previous extension based on previous
	  map file size may not have been sufficient to cover whole extension
	  (it could have been used up for new map pages),
	  but the newly grown map file now covers lot bigger new free space
	  available for further extension of map;
 any more blocks to extend ? 
 finalize bmap 
	
	 	update inode allocation map
	 	---------------------------
	 
	  move iag lists from old to new iag;
	  agstart field is not updated for logredo() to reconstruct
	  iag lists if system crash occurs.
	  (computation of ag number from agstart based on agsize
	  will correctly identify the new ag);
 if new AG size the same as old AG size, done! 
 finalize imap 
	
	 	finalize
	 	--------
	 
	  extension is committed when on-disk super block is
	  updated with new descriptors: logredo will recover
	  crash before it to pre-extension state;
 sync log to skip log replay of bmap file growth transaction; 
 lmLogSync(log, 1); 
	
	  synchronous write bmap global control page;
	  for crash before completion of write
	  logredo() will recover to pre-extendfs state;
	  for crash after completion of write,
	  logredo() will recover post-extendfs state;
	
	  copy primary bmap inode to secondary bmap inode
	
	 	update superblock
 mark extendfs() completion 
 update inline log space descriptor 
 record log's mount serial number 
 update fsck work space descriptor 
 sb->s_fsckloglen remains the same 
 Update secondary superblock 
 write primary superblock 
	
	 	resume file system transactions
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) Tino Reichardt, 2012
  NAME:	jfs_issue_discard()
  FUNCTION:	TRIM the specified block range on device, if supported
  PARAMETERS:
 	ip	- pointer to in-core inode
 	blkno	- starting block number to be trimmed (0..N)
 	nblocks	- number of blocks to be trimmed
  RETURN VALUES:
 	none
  serialization: IREAD_LOCK(ipbmap) held on entryexit;
  NAME:	jfs_ioc_trim()
  FUNCTION:	attempt to discard (TRIM) all free blocks from the
               filesystem.
  PARAMETERS:
 	ip	- pointer to in-core inode;
 	range	- the range, given by user space
  RETURN VALUES:
 	0	- success
 	-EIO	- io error
	
	  convert byte values to block size of filesystem:
	  start:	First Byte to trim
	  len:		number of Bytes to trim from start
	  minlen:	minimum extent length in Bytes
	
	  we trim all ag's within the range
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2002
    Portions Copyright (C) Christoph Hellwig, 2001-2002
 Make sure committed changes hit the disk 
	
	  We attempt to allow only one "active" file open per aggregate
	  group.  Otherwise, appending to files in parallel can cause
	  fragmentation within the files.
	 
	  If the file is empty, it was probably just created and going
	  to be written to.  If it has a size, we'll hold off until the
	  file is actually grown.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
    Portions Copyright (C) Tino Reichardt, 2012
 	SERIALIZATION of the Block Allocation Map.
 	the working state of the block allocation map is accessed in
 	two directions:
 	1) allocation and free requests that start at the dmap
 	   level and move up through the dmap control pages (i.e.
 	   the vast majority of requests).
 	2) allocation requests that start at dmap control page
 	   level and work down towards the dmaps.
 	the serialization scheme used here is as follows.
 	requests which start at the bottom are serialized against each
 	other through buffers and each requests holds onto its buffers
 	as it works it way up from a single dmap to the required level
 	of dmap control page.
 	requests that start at the top are serialized against each other
 	and request that start from the bottom by the multiple readsingle
 	write inode lock of the bmap inode. requests starting at the top
 	take this lock in write mode while request starting at the bottom
 	take the lock in read mode.  a single top-down request may proceed
 	exclusively while multiple bottoms-up requests may proceed
 	simultaneously (under the protection of busy buffers).
 	in addition to information found in dmaps and dmap control pages,
 	the working state of the block allocation map also includes read
 	write information maintained in the bmap descriptor (i.e. total
 	free block count, allocation group level free block counts).
 	a single exclusive lock (BMAP_LOCK) is used to guard this information
 	in the face of multiple-bottoms up requests.
 	(lock ordering: IREAD_LOCK, BMAP_LOCK);
 	accesses to the persistent state of the block allocation map (limited
 	to the persistent bitmaps in dmaps) is guarded by (busy) buffers.
  forward references
 	buddy table
  table used for determining buddy sizes within characters of
  dmap bitmap words.  the characters themselves serve as indexes
  into the table, with the table elements yielding the maximum
  binary buddy of free bits within the character.
  NAME:	dbMount()
  FUNCTION:	initializate the block allocation map.
 		memory is allocated for the in-core bmap descriptor and
 		the in-core descriptor is initialized from disk.
  PARAMETERS:
 	ipbmap	- pointer to in-core inode for the block map.
  RETURN VALUES:
 	0	- success
 	-ENOMEM	- insufficient memory
 	-EIO	- io error
	
	  allocateinitialize the in-memory bmap descriptor
 allocate memory for the in-memory bmap descriptor 
 read the on-disk bmap descriptor. 
 copy the on-disk bmap descriptor to its in-memory version. 
 release the buffer. 
 bind the bmap inode and the bmap descriptor to each other. 
	
	  allocateinitialize the bmap lock
  NAME:	dbUnmount()
  FUNCTION:	terminate the block allocation map in preparation for
 		file system unmount.
 		the in-core bmap descriptor is written to disk and
 		the memory for this descriptor is freed.
  PARAMETERS:
 	ipbmap	- pointer to in-core inode for the block map.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error
	
	  Invalidate the page cache buffers
 free the memory for the in-memory bmap. 
 	dbSync()
	
	  write bmap global control page
 get the buffer for the on-disk bmap descriptor. 
 copy the in-memory version of the bmap to the on-disk version 
 write the buffer 
	
	  write out dirty pages of bmap
  NAME:	dbFree()
  FUNCTION:	free the specified block range from the working block
 		allocation map.
 		the blocks will be free from the working map one dmap
 		at a time.
  PARAMETERS:
 	ip	- pointer to in-core inode;
 	blkno	- starting block number to be freed.
 	nblocks	- number of blocks to be freed.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error
 block to be freed better be within the mapsize. 
	
	  TRIM the blocks, when mounted with discard option
	
	  free the blocks a dmap at a time.
 release previous dmap if any 
 get the buffer for the current dmap. 
		 determine the number of blocks to be freed from
		  this dmap.
 free the blocks. 
 write the last buffer. 
  NAME:	dbUpdatePMap()
  FUNCTION:	update the allocation state (free or allocate) of the
 		specified block range in the persistent block allocation map.
 		the blocks will be updated in the persistent map one
 		dmap at a time.
  PARAMETERS:
 	ipbmap	- pointer to in-core inode for the block map.
 	free	- 'true' if block range is to be freed from the persistent
 		  map; 'false' if it is to be allocated.
 	blkno	- starting block number of the range.
 	nblocks	- number of contiguous blocks in the range.
 	tblk	- transaction block;
  RETURN VALUES:
 	0	- success
 	-EIO	- io error
 the blocks better be within the mapsize. 
 compute delta of transaction lsn from log syncpt 
	
	  update the block state a dmap at a time.
 get the buffer for the current dmap. 
		 determine the bit number and word within the dmap of
		  the starting block.  also determine how many blocks
		  are to be updated within this dmap.
		 update the bits of the dmap words. the first and last
		  words may only have a subset of their bits updated. if
		  this is the case, we'll work against that word (i.e.
		  partial first andor last) only in a single pass.  a
		  single pass will also be used to update all words that
		  are to have all their bits updated.
			 determine the bit number within the word and
			  the number of bits within the word.
 check if only part of the word is to be updated. 
				 update (free or allocate) the bits
				  in this word.
				 one or more words are to have all
				  their bits updated.  determine how
				  many words and how many bits.
				 update (free or allocate) the bits
				  in these words.
		
		  update dmap lsn
 inherit oldersmaller lsn 
 move bp after tblock in logsync list 
 inherit youngerlarger clsn 
 insert bp after tblock in logsync list 
 write the last buffer. 
  NAME:	dbNextAG()
  FUNCTION:	find the preferred allocation group for new allocations.
 		Within the allocation groups, we maintain a preferred
 		allocation group which consists of a group with at least
 		average free space.  It is the preferred group that we target
 		new inode allocation towards.  The tie-in between inode
 		allocation and block allocation occurs as we allocate the
 		first (data) block of an inode and specify the inode (block)
 		as the allocation hint for this block.
 		We try to avoid having more than one open file growing in
 		an allocation group, as this will lead to fragmentation.
 		This differs from the old OS2 method of trying to keep
 		empty ags around for large allocations.
  PARAMETERS:
 	ipbmap	- pointer to in-core inode for the block map.
  RETURN VALUES:
 	the preferred allocation group number.
 determine the average number of free blocks within the ags. 
	
	  if the current preferred ag does not have an active allocator
	  and has at least average freespace, return it
	 From the last preferred ag, find the next one with at least
	  average free space.
 open file is currently growing in this ag 
 Return this one 
 Less than avg. freespace, but best so far 
	
	  If no inactive ag was found with average freespace, use the
	  next best
 else leave db_agpref unchanged 
	 return the preferred group.
  NAME:	dbAlloc()
  FUNCTION:	attempt to allocate a specified number of contiguous free
 		blocks from the working allocation block map.
 		the block allocation policy uses hints and a multi-step
 		approach.
 		for allocation requests smaller than the number of blocks
 		per dmap, we first try to allocate the new blocks
 		immediately following the hint.  if these blocks are not
 		available, we try to allocate blocks near the hint.  if
 		no blocks near the hint are available, we next try to
 		allocate within the same dmap as contains the hint.
 		if no blocks are available in the dmap or the allocation
 		request is larger than the dmap size, we try to allocate
 		within the same allocation group as contains the hint. if
 		this does not succeed, we finally try to allocate anywhere
 		within the aggregate.
 		we also try to allocate anywhere within the aggregate
 		for allocation requests larger than the allocation group
 		size or requests that specify no hint value.
  PARAMETERS:
 	ip	- pointer to in-core inode;
 	hint	- allocation hint.
 	nblocks	- number of contiguous blocks in the range.
 	results	- on successful return, set to the starting block number
 		  of the newly allocated contiguous range.
  RETURN VALUES:
 	0	- success
 	-ENOSPC	- insufficient disk resources
 	-EIO	- io error
 assert that nblocks is valid 
	 get the log2 number of blocks to be allocated.
	  if the number of blocks is not a log2 multiple,
	  it will be rounded up to the next log2 multiple.
 the hint should be within the map 
	 if the number of blocks to be allocated is greater than the
	  allocation group size, try to allocate anywhere.
	
	  If no hint, let dbNextAG recommend an allocation group
	 we would like to allocate close to the hint.  adjust the
	  hint to the block following the hint since the allocators
	  will start looking for free space starting at this point.
	 check if blkno crosses over into a new allocation group.
	  if so, check if we should allow allocations within this
	  allocation group.
		 check if the AG is currently being written to.
		  if so, call dbNextAG() to find a non-busy
		  AG with sufficient free space.
	 check if the allocation request size can be satisfied from a
	  single dmap.  if so, try to allocate from the dmap containing
	  the hint using a tiered strategy.
		 get the buffer for the dmap containing the hint.
		 first, try to satisfy the allocation request with the
		  blocks beginning at the hint.
			
			  Someone else is writing in this allocation
			  group.  To avoid fragmenting, try another ag
		 next, try to satisfy the allocation request with blocks
		  near the hint.
		 try to satisfy the allocation request with blocks within
		  the same dmap as the hint.
	 try to satisfy the allocation request with blocks within
	  the same allocation group as the hint.
	
	  Let dbNextAG recommend a preferred allocation group
	 Try to allocate within this allocation group.  if that fails, try to
	  allocate anywhere in the map.
  NAME:	dbAllocExact()
  FUNCTION:	try to allocate the requested extent;
  PARAMETERS:
 	ip	- pointer to in-core inode;
 	blkno	- extent address;
 	nblocks	- extent length;
  RETURN VALUES:
 	0	- success
 	-ENOSPC	- insufficient disk resources
 	-EIO	- io error
	
	  validate extent request:
	 
	  note: defragfs policy:
	   max 64 blocks will be moved.
	   allocation request size must be satisfied from a single dmap.
 the free space is no longer available 
 read in the dmap covering the extent 
 try to allocate the requested extent 
 _NOTYET 
  NAME:	dbReAlloc()
  FUNCTION:	attempt to extend a current allocation by a specified
 		number of blocks.
 		this routine attempts to satisfy the allocation request
 		by first trying to extend the existing allocation in
 		place by allocating the additional blocks as the blocks
 		immediately following the current allocation.  if these
 		blocks are not available, this routine will attempt to
 		allocate a new set of contiguous blocks large enough
 		to cover the existing allocation plus the additional
 		number of blocks required.
  PARAMETERS:
 	ip	    -  pointer to in-core inode requiring allocation.
 	blkno	    -  starting block of the current allocation.
 	nblocks	    -  number of contiguous blocks within the current
 		       allocation.
 	addnblocks  -  number of blocks to add to the allocation.
 	results	-      on successful return, set to the starting block number
 		       of the existing allocation if the existing allocation
 		       was extended in place or to a newly allocated contiguous
 		       range if the existing allocation could not be extended
 		       in place.
  RETURN VALUES:
 	0	- success
 	-ENOSPC	- insufficient disk resources
 	-EIO	- io error
	 try to extend the allocation in place.
	 could not extend the allocation in place, so allocate a
	  new set of blocks for the entire request (i.e. try to get
	  a range of contiguous blocks large enough to cover the
	  existing allocation plus the additional blocks.)
  NAME:	dbExtend()
  FUNCTION:	attempt to extend a current allocation by a specified
 		number of blocks.
 		this routine attempts to satisfy the allocation request
 		by first trying to extend the existing allocation in
 		place by allocating the additional blocks as the blocks
 		immediately following the current allocation.
  PARAMETERS:
 	ip	    -  pointer to in-core inode requiring allocation.
 	blkno	    -  starting block of the current allocation.
 	nblocks	    -  number of contiguous blocks within the current
 		       allocation.
 	addnblocks  -  number of blocks to add to the allocation.
  RETURN VALUES:
 	0	- success
 	-ENOSPC	- insufficient disk resources
 	-EIO	- io error
	
	  We don't want a non-aligned extent to cross a page boundary
 get the last block of the current allocation 
	 determine the block number of the block following
	  the existing allocation.
 better be within the file system 
	 we'll attempt to extend the current allocation in place by
	  allocating the additional blocks as the blocks immediately
	  following the current allocation.  we only try to extend the
	  current allocation in place if the number of additional blocks
	  can fit into a dmap, the last block of the current allocation
	  is not the last block of the file system, and the start of the
	  inplace extension is not on an allocation group boundary.
	 get the buffer for the dmap containing the first block
	  of the extension.
	 try to allocate the blocks immediately following the
	  current allocation.
 were we successful ? 
 we were not successful 
  NAME:	dbAllocNext()
  FUNCTION:	attempt to allocate the blocks of the specified block
 		range within a dmap.
  PARAMETERS:
 	bmp	-  pointer to bmap descriptor
 	dp	-  pointer to dmap.
 	blkno	-  starting block number of the range.
 	nblocks	-  number of contiguous free blocks of the range.
  RETURN VALUES:
 	0	- success
 	-ENOSPC	- insufficient disk resources
 	-EIO	- io error
  serialization: IREAD_LOCK(ipbmap) held on entryexit;
	 pick up a pointer to the leaves of the dmap tree.
	 determine the bit number and word within the dmap of the
	  starting block.
	 check if the specified block range is contained within
	  this dmap.
	 check if the starting leaf indicates that anything
	  is free.
	 check the dmaps words corresponding to block range to see
	  if the block range is free.  not all bits of the first and
	  last words may be contained within the block range.  if this
	  is the case, we'll work against those words (i.e. partial first
	  andor last) on an individual basis (a single pass) and examine
	  the actual bits to determine if they are free.  a single pass
	  will be used for all dmap words fully contained within the
	  specified range.  within this pass, the leaves of the dmap
	  tree will be examined to determine if the blocks are free. a
	  single leaf may describe the free space of multiple dmap
	  words, so we may visit only a subset of the actual leaves
	  corresponding to the dmap words of the block range.
		 determine the bit number within the word and
		  the number of bits within the word.
		 check if only part of the word is to be examined.
			 check if the bits are free.
			 one or more dmap words are fully contained
			  within the block range.  determine how many
			  words and how many bits.
			 now examine the appropriate leaves to determine
			  if the blocks are free.
				 does the leaf describe any free space ?
				 determine the l2 number of bits provided
				  by this leaf.
				 determine how many words were handled.
	 allocate the blocks.
  NAME:	dbAllocNear()
  FUNCTION:	attempt to allocate a number of contiguous free blocks near
 		a specified block (hint) within a dmap.
 		starting with the dmap leaf that covers the hint, we'll
 		check the next four contiguous leaves for sufficient free
 		space.  if sufficient free space is found, we'll allocate
 		the desired free space.
  PARAMETERS:
 	bmp	-  pointer to bmap descriptor
 	dp	-  pointer to dmap.
 	blkno	-  block number to allocate near.
 	nblocks	-  actual number of contiguous free blocks desired.
 	l2nb	-  log2 number of contiguous free blocks desired.
 	results	-  on successful return, set to the starting block number
 		   of the newly allocated range.
  RETURN VALUES:
 	0	- success
 	-ENOSPC	- insufficient disk resources
 	-EIO	- io error
  serialization: IREAD_LOCK(ipbmap) held on entryexit;
	 determine the word within the dmap that holds the hint
	  (i.e. blkno).  also, determine the last word in the dmap
	  that we'll include in our examination.
	 examine the leaves for sufficient free space.
		 does the leaf describe sufficient free space ?
		 determine the block number within the file system
		  of the first block described by this dmap word.
		 if not all bits of the dmap word are free, get the
		  starting bit number within the dmap word of the required
		  string of free bits and adjust the block number with the
		  value.
		 allocate the blocks.
  NAME:	dbAllocAG()
  FUNCTION:	attempt to allocate the specified number of contiguous
 		free blocks within the specified allocation group.
 		unless the allocation group size is equal to the number
 		of blocks per dmap, the dmap control pages will be used to
 		find the required free space, if available.  we start the
 		search at the highest dmap control page level which
 		distinctly describes the allocation group's free space
 		(i.e. the highest level at which the allocation group's
 		free space is not mixed in with that of any other group).
 		in addition, we start the search within this level at a
 		height of the dmapctl dmtree at which the nodes distinctly
 		describe the allocation group's free space.  at this height,
 		the allocation group's free space may be represented by 1
 		or two sub-trees, depending on the allocation group size.
 		we search the top nodes of these subtrees left to right for
 		sufficient free space.  if sufficient free space is found,
 		the subtree is searched to find the leftmost leaf that
 		has free space.  once we have made it to the leaf, we
 		move the search to the next lower level dmap control page
 		corresponding to this leaf.  we continue down the dmap control
 		pages until we find the dmap that contains or starts the
 		sufficient free space and we allocate at this dmap.
 		if the allocation group size is equal to the dmap size,
 		we'll start at the dmap corresponding to the allocation
 		group and attempt the allocation at this level.
 		the dmap control page search is also not performed if the
 		allocation group is completely free and we go to the first
 		dmap of the allocation group to do the allocation.  this is
 		done because the allocation group may be part (not the first
 		part) of a larger binary buddy system, causing the dmap
 		control pages to indicate no free space (NOFREE) within
 		the allocation group.
  PARAMETERS:
 	bmp	-  pointer to bmap descriptor
 	agno	- allocation group number.
 	nblocks	-  actual number of contiguous free blocks desired.
 	l2nb	-  log2 number of contiguous free blocks desired.
 	results	-  on successful return, set to the starting block number
 		   of the newly allocated range.
  RETURN VALUES:
 	0	- success
 	-ENOSPC	- insufficient disk resources
 	-EIO	- io error
  note: IWRITE_LOCK(ipmap) held on entryexit;
	 allocation request should not be for more than the
	  allocation group size.
	 determine the starting block number of the allocation
	  group.
	 check if the allocation group size is the minimum allocation
	  group size or if the allocation group is completely free. if
	  the allocation group size is the minimum size of BPERDMAP (i.e.
	  1 dmap), there is no need to search the dmap control page (below)
	  that fully describes the allocation group since the allocation
	  group is already fully described by a dmap.  in this case, we
	  just call dbAllocCtl() to search the dmap tree and allocate the
	  required space if available.
	 
	  if the allocation group is completely free, dbAllocCtl() is
	  also called to allocate the required space.  this is done for
	  two reasons.  first, it makes no sense searching the dmap control
	  pages for free space when we know that free space exists.  second,
	  the dmap control pages may indicate that the allocation group
	  has no free space if the allocation group is part (not the first
	  part) of a larger binary buddy system.
	 the buffer for the dmap control page that fully describes the
	  allocation group.
	 search the subtree(s) of the dmap control page that describes
	  the allocation group, looking for sufficient free space.  to begin,
	  determine how many allocation groups are represented in a dmap
	  control page at the control page level (i.e. L0, L1, L2) that
	  fully describes an allocation group. next, determine the starting
	  tree index of this allocation group within the control page.
	 dmap control page trees fan-out by 4 and a single allocation
	  group may be described by 1 or 2 subtrees within the ag level
	  dmap control page, depending upon the ag size. examine the ag's
	  subtrees for sufficient free space, starting with the leftmost
	  subtree.
		 is there sufficient free space ?
		 sufficient free space found in a subtree. now search down
		  the subtree to find the leftmost leaf that describes this
		  free space.
		 determine the block number within the file system
		  that corresponds to this leaf.
 bmp->db_aglevel == 0 
		 release the buffer in preparation for going down
		  the next level of dmap control pages.
		 check if we need to continue to search down the lower
		  level dmap control pages.  we need to if the number of
		  blocks required is less than maximum number of blocks
		  described at the next lower level.
			 search the lower level dmap control pages to get
			  the starting block number of the dmap that
			  contains or starts off the free space.
		 allocate the blocks.
	 no space in the allocation group.  release the buffer and
	  return -ENOSPC.
  NAME:	dbAllocAny()
  FUNCTION:	attempt to allocate the specified number of contiguous
 		free blocks anywhere in the file system.
 		dbAllocAny() attempts to find the sufficient free space by
 		searching down the dmap control pages, starting with the
 		highest level (i.e. L0, L1, L2) control page.  if free space
 		large enough to satisfy the desired free space is found, the
 		desired free space is allocated.
  PARAMETERS:
 	bmp	-  pointer to bmap descriptor
 	nblocks	 -  actual number of contiguous free blocks desired.
 	l2nb	 -  log2 number of contiguous free blocks desired.
 	results	-  on successful return, set to the starting block number
 		   of the newly allocated range.
  RETURN VALUES:
 	0	- success
 	-ENOSPC	- insufficient disk resources
 	-EIO	- io error
  serialization: IWRITE_LOCK(ipbmap) held on entryexit;
	 starting with the top level dmap control page, search
	  down the dmap control levels for sufficient free space.
	  if free space is found, dbFindCtl() returns the starting
	  block number of the dmap that contains or starts off the
	  range of free space.
	 allocate the blocks.
  NAME:	dbDiscardAG()
  FUNCTION:	attempt to discard (TRIM) all free blocks of specific AG
 		algorithm:
 		1) allocate blocks, as large as possible and save them
 		   while holding IWRITE_LOCK on ipbmap
 		2) trim all these saved blocklength values
 		3) mark the blocks free again
 		benefit:
 		- we work only on one ag at some time, minimizing how long we
 		  need to lock ipbmap
 		- reading  writing the fs is possible most time, even on
 		  trimming
 		downside:
 		- we write two times to the dmapctl and dmap pages
 		- but for me, this seems the best way, better ideas?
 		TR 2012
  PARAMETERS:
 	ip	- pointer to in-core inode
 	agno	- ag to trim
 	minlen	- minimum value of contiguous blocks
  RETURN VALUES:
 	s64	- actual number of blocks trimmed
 max blkno  nblocks pairs to trim 
 prevent others from writing new stuff here, while trimming 
 0 = okay, -EIO = fatal, -ENOSPC -> try smaller block 
 the whole ag is free, trim now 
 give a hint for the next while 
 search for next smaller log2 block 
 Trim any already allocated blocks 
 check, if our trim array is full 
 mark the current end 
		 when mounted with online discard, dbFree() will
  NAME:	dbFindCtl()
  FUNCTION:	starting at a specified dmap control page level and block
 		number, search down the dmap control levels for a range of
 		contiguous free blocks large enough to satisfy an allocation
 		request for the specified number of free blocks.
 		if sufficient contiguous free blocks are found, this routine
 		returns the starting block number within a dmap page that
 		contains or starts a range of contiqious free blocks that
 		is sufficient in size.
  PARAMETERS:
 	bmp	-  pointer to bmap descriptor
 	level	-  starting dmap control page level.
 	l2nb	-  log2 number of contiguous free blocks desired.
 	blkno	-  on entry, starting block number for conducting the search.
 		   on successful return, the first block within a dmap page
 		   that contains or starts a range of contiguous free blocks.
  RETURN VALUES:
 	0	- success
 	-ENOSPC	- insufficient disk resources
 	-EIO	- io error
  serialization: IWRITE_LOCK(ipbmap) held on entryexit;
	 starting at the specified dmap control page level and block
	  number, search down the dmap control levels for the starting
	  block number of a dmap page that contains or starts off
	  sufficient free blocks.
		 get the buffer of the dmap control page for the block
		  number and level (i.e. L0, L1, L2).
		 search the tree within the dmap control page for
		  sufficient free space.  if sufficient free space is found,
		  dbFindLeaf() returns the index of the leaf at which
		  free space was found.
		 release the buffer.
		 space found ?
		 adjust the block number to reflect the location within
		  the dmap control page (i.e. the leaf) at which free
		  space was found.
		 we stop the search at this dmap control page level if
		  the number of blocks required is greater than or equal
		  to the maximum number of blocks described at the next
		  (lower) level.
  NAME:	dbAllocCtl()
  FUNCTION:	attempt to allocate a specified number of contiguous
 		blocks starting within a specific dmap.
 		this routine is called by higher level routines that search
 		the dmap control pages above the actual dmaps for contiguous
 		free space.  the result of successful searches by these
 		routines are the starting block numbers within dmaps, with
 		the dmaps themselves containing the desired contiguous free
 		space or starting a contiguous free space of desired size
 		that is made up of the blocks of one or more dmaps. these
 		calls should not fail due to insufficent resources.
 		this routine is called in some cases where it is not known
 		whether it will fail due to insufficient resources.  more
 		specifically, this occurs when allocating from an allocation
 		group whose size is equal to the number of blocks per dmap.
 		in this case, the dmap control pages are not examined prior
 		to calling this routine (to save pathlength) and the call
 		might fail.
 		for a request size that fits within a dmap, this routine relies
 		upon the dmap's dmtree to find the requested contiguous free
 		space.  for request sizes that are larger than a dmap, the
 		requested free space will start at the first block of the
 		first dmap (i.e. blkno).
  PARAMETERS:
 	bmp	-  pointer to bmap descriptor
 	nblocks	 -  actual number of contiguous free blocks to allocate.
 	l2nb	 -  log2 number of contiguous free blocks to allocate.
 	blkno	 -  starting block number of the dmap to start the allocation
 		    from.
 	results	-  on successful return, set to the starting block number
 		   of the newly allocated range.
  RETURN VALUES:
 	0	- success
 	-ENOSPC	- insufficient disk resources
 	-EIO	- io error
  serialization: IWRITE_LOCK(ipbmap) held on entryexit;
	 check if the allocation request is confined to a single dmap.
		 get the buffer for the dmap.
		 try to allocate the blocks.
	 allocation request involving multiple dmaps. it must start on
	  a dmap boundary.
	 allocate the blocks dmap by dmap.
		 get the buffer for the dmap.
		 the dmap better be all free.
		 determine how many blocks to allocate from this dmap.
		 allocate the blocks from the dmap.
		 write the buffer.
	 set the results (starting block number) and return.
	 something failed in handling an allocation request involving
	  multiple dmaps.  we'll try to clean up by backing out any
	  allocation that has already happened for this request.  if
	  we fail in backing out the allocation, we'll mark the file
	  system to indicate that blocks have been leaked.
	 try to backout the allocations dmap by dmap.
		 get the buffer for this dmap.
			 could not back out.  mark the file system
			  to indicate that we have leaked blocks.
		 free the blocks is this dmap.
			 could not back out.  mark the file system
			  to indicate that we have leaked blocks.
		 write the buffer.
  NAME:	dbAllocDmapLev()
  FUNCTION:	attempt to allocate a specified number of contiguous blocks
 		from a specified dmap.
 		this routine checks if the contiguous blocks are available.
 		if so, nblocks of blocks are allocated; otherwise, ENOSPC is
 		returned.
  PARAMETERS:
 	mp	-  pointer to bmap descriptor
 	dp	-  pointer to dmap to attempt to allocate blocks from.
 	l2nb	-  log2 number of contiguous block desired.
 	nblocks	-  actual number of contiguous block desired.
 	results	-  on successful return, set to the starting block number
 		   of the newly allocated range.
  RETURN VALUES:
 	0	- success
 	-ENOSPC	- insufficient disk resources
 	-EIO	- io error
  serialization: IREAD_LOCK(ipbmap), e.g., from dbAlloc(), or
 	IWRITE_LOCK(ipbmap), e.g., dbAllocCtl(), held on entryexit;
 can't be more than a dmaps worth of blocks 
	 search the tree within the dmap page for sufficient
	  free space.  if sufficient free space is found, dbFindLeaf()
	  returns the index of the leaf at which free space was found.
	 determine the block number within the file system corresponding
	  to the leaf at which free space was found.
	 if not all bits of the dmap word are free, get the starting
	  bit number within the dmap word of the required string of free
	  bits and adjust the block number with this value.
 allocate the blocks 
  NAME:	dbAllocDmap()
  FUNCTION:	adjust the disk allocation map to reflect the allocation
 		of a specified block range within a dmap.
 		this routine allocates the specified blocks from the dmap
 		through a call to dbAllocBits(). if the allocation of the
 		block range causes the maximum string of free blocks within
 		the dmap to change (i.e. the value of the root of the dmap's
 		dmtree), this routine will cause this change to be reflected
 		up through the appropriate levels of the dmap control pages
 		by a call to dbAdjCtl() for the L0 dmap control page that
 		covers this dmap.
  PARAMETERS:
 	bmp	-  pointer to bmap descriptor
 	dp	-  pointer to dmap to allocate the block range from.
 	blkno	-  starting block number of the block to be allocated.
 	nblocks	-  number of blocks to be allocated.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error
  serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entryexit;
	 save the current value of the root (i.e. maximum free string)
	  of the dmap tree.
 allocate the specified (blocks) bits 
 if the root has not changed, done. 
	 root changed. bubble the change up to the dmap control pages.
	  if the adjustment of the upper level control pages fails,
	  backout the bit allocation (thus making everything consistent).
  NAME:	dbFreeDmap()
  FUNCTION:	adjust the disk allocation map to reflect the allocation
 		of a specified block range within a dmap.
 		this routine frees the specified blocks from the dmap through
 		a call to dbFreeBits(). if the deallocation of the block range
 		causes the maximum string of free blocks within the dmap to
 		change (i.e. the value of the root of the dmap's dmtree), this
 		routine will cause this change to be reflected up through the
 		appropriate levels of the dmap control pages by a call to
 		dbAdjCtl() for the L0 dmap control page that covers this dmap.
  PARAMETERS:
 	bmp	-  pointer to bmap descriptor
 	dp	-  pointer to dmap to free the block range from.
 	blkno	-  starting block number of the block to be freed.
 	nblocks	-  number of blocks to be freed.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error
  serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entryexit;
	 save the current value of the root (i.e. maximum free string)
	  of the dmap tree.
 free the specified (blocks) bits 
 if error or the root has not changed, done. 
	 root changed. bubble the change up to the dmap control pages.
	  if the adjustment of the upper level control pages fails,
	  backout the deallocation.
		 as part of backing out the deallocation, we will have
		  to back split the dmap tree if the deallocation caused
		  the freed blocks to become part of a larger binary buddy
		  system.
  NAME:	dbAllocBits()
  FUNCTION:	allocate a specified block range from a dmap.
 		this routine updates the dmap to reflect the working
 		state allocation of the specified block range. it directly
 		updates the bits of the working map and causes the adjustment
 		of the binary buddy system described by the dmap's dmtree
 		leaves to reflect the bits allocated.  it also causes the
 		dmap's dmtree, as a whole, to reflect the allocated range.
  PARAMETERS:
 	bmp	-  pointer to bmap descriptor
 	dp	-  pointer to dmap to allocate bits from.
 	blkno	-  starting block number of the bits to be allocated.
 	nblocks	-  number of bits to be allocated.
  RETURN VALUES: none
  serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entryexit;
 pick up a pointer to the leaves of the dmap tree 
	 determine the bit number and word within the dmap of the
	  starting block.
 block range better be within the dmap 
	 allocate the bits of the dmap's words corresponding to the block
	  range. not all bits of the first and last words may be contained
	  within the block range.  if this is the case, we'll work against
	  those words (i.e. partial first andor last) on an individual basis
	  (a single pass), allocating the bits of interest by hand and
	  updating the leaf corresponding to the dmap word. a single pass
	  will be used for all dmap words fully contained within the
	  specified range.  within this pass, the bits of all fully contained
	  dmap words will be marked as free in a single shot and the leaves
	  will be updated. a single leaf may describe the free space of
	  multiple dmap words, so we may update only a subset of the actual
	  leaves corresponding to the dmap words of the block range.
		 determine the bit number within the word and
		  the number of bits within the word.
		 check if only part of a word is to be allocated.
			 allocate (set to 1) the appropriate bits within
			  this dmap word.
			 update the leaf for this dmap word. in addition
			  to setting the leaf value to the binary buddy max
			  of the updated dmap word, dbSplit() will split
			  the binary system of the leaves if need be.
			 one or more dmap words are fully contained
			  within the block range.  determine how many
			  words and allocate (set to 1) the bits of these
			  words.
			 determine how many bits.
			 now update the appropriate leaves to reflect
			  the allocated words.
				 determine what the leaf value should be
				  updated to as the minimum of the l2 number
				  of bits being allocated and the l2 number
				  of bits currently described by this leaf.
				 update the leaf to reflect the allocation.
				  in addition to setting the leaf value to
				  NOFREE, dbSplit() will split the binary
				  system of the leaves to reflect the current
				  allocation (size).
 get the number of dmap words handled 
 update the free count for this dmap 
	 if this allocation group is completely free,
	  update the maximum allocation group number if this allocation
	  group is the new max.
 update the free count for the allocation group and map 
  NAME:	dbFreeBits()
  FUNCTION:	free a specified block range from a dmap.
 		this routine updates the dmap to reflect the working
 		state allocation of the specified block range. it directly
 		updates the bits of the working map and causes the adjustment
 		of the binary buddy system described by the dmap's dmtree
 		leaves to reflect the bits freed.  it also causes the dmap's
 		dmtree, as a whole, to reflect the deallocated range.
  PARAMETERS:
 	bmp	-  pointer to bmap descriptor
 	dp	-  pointer to dmap to free bits from.
 	blkno	-  starting block number of the bits to be freed.
 	nblocks	-  number of bits to be freed.
  RETURN VALUES: 0 for success
  serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entryexit;
	 determine the bit number and word within the dmap of the
	  starting block.
	 block range better be within the dmap.
	 free the bits of the dmaps words corresponding to the block range.
	  not all bits of the first and last words may be contained within
	  the block range.  if this is the case, we'll work against those
	  words (i.e. partial first andor last) on an individual basis
	  (a single pass), freeing the bits of interest by hand and updating
	  the leaf corresponding to the dmap word. a single pass will be used
	  for all dmap words fully contained within the specified range.
	  within this pass, the bits of all fully contained dmap words will
	  be marked as free in a single shot and the leaves will be updated. a
	  single leaf may describe the free space of multiple dmap words,
	  so we may update only a subset of the actual leaves corresponding
	  to the dmap words of the block range.
	 
	  dbJoin() is used to update leaf values and will join the binary
	  buddy system of the leaves if the new leaf values indicate this
	  should be done.
		 determine the bit number within the word and
		  the number of bits within the word.
		 check if only part of a word is to be freed.
			 free (zero) the appropriate bits within this
			  dmap word.
			 update the leaf for this dmap word.
			 one or more dmap words are fully contained
			  within the block range.  determine how many
			  words and free (zero) the bits of these words.
			 determine how many bits.
			 now update the appropriate leaves to reflect
			  the freed words.
				 determine what the leaf value should be
				  updated to as the minimum of the l2 number
				  of bits being freed and the l2 (max) number
				  of bits that can be described by this leaf.
				 update the leaf.
				 get the number of dmap words handled.
	 update the free count for this dmap.
	 update the free count for the allocation group and
	  map.
	 check if this allocation group is not completely free and
	  if it is currently the maximum (rightmost) allocation group.
	  if so, establish the new maximum allocation group number by
	  searching left for the first allocation group with allocation.
		 re-establish the allocation group preference if the
		  current preference is right of the maximum allocation
		  group.
  NAME:	dbAdjCtl()
  FUNCTION:	adjust a dmap control page at a specified level to reflect
 		the change in a lower level dmap or dmap control page's
 		maximum string of free blocks (i.e. a change in the root
 		of the lower level object's dmtree) due to the allocation
 		or deallocation of a range of blocks with a single dmap.
 		on entry, this routine is provided with the new value of
 		the lower level dmap or dmap control page root and the
 		starting block number of the block range whose allocation
 		or deallocation resulted in the root change.  this range
 		is respresented by a single leaf of the current dmapctl
 		and the leaf will be updated with this value, possibly
 		causing a binary buddy system within the leaves to be
 		split or joined.  the update may also cause the dmapctl's
 		dmtree to be updated.
 		if the adjustment of the dmap control page, itself, causes its
 		root to change, this change will be bubbled up to the next dmap
 		control level by a recursive call to this routine, specifying
 		the new root value and the next dmap control page level to
 		be adjusted.
  PARAMETERS:
 	bmp	-  pointer to bmap descriptor
 	blkno	-  the first block of a block range within a dmap.  it is
 		   the allocation or deallocation of this block range that
 		   requires the dmap control page to be adjusted.
 	newval	-  the new value of the lower level dmap or dmap control
 		   page root.
 	alloc	-  'true' if adjustment is due to an allocation.
 	level	-  current level of dmap control page (i.e. L0, L1, L2) to
 		   be adjusted.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error
  serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entryexit;
	 get the buffer for the dmap control page for the specified
	  block number and control page level.
	 determine the leaf number corresponding to the block and
	  the index within the dmap control tree.
	 save the current leaf value and the current root level (i.e.
	  maximum l2 free string described by this dmapctl).
	 check if this is a control page update for an allocation.
	  if so, update the leaf to reflect the new leaf value using
	  dbSplit(); otherwise (deallocation), use dbJoin() to update
	  the leaf with the new value.  in addition to updating the
	  leaf, dbSplit() will also split the binary buddy system of
	  the leaves, if required, and bubble new values within the
	  dmapctl tree, if required.  similarly, dbJoin() will join
	  the binary buddy system of leaves and bubble new values up
	  the dmapctl tree as required by the new leaf value.
		 check if we are in the middle of a binary buddy
		  system.  this happens when we are performing the
		  first allocation out of an allocation group that
		  is part (not the first part) of a larger binary
		  buddy system.  if we are in the middle, back split
		  the system prior to calling dbSplit() which assumes
		  that it is at the front of a binary buddy system.
	 check if the root of the current dmap control page changed due
	  to the update and if the current dmap control page is not at
	  the current top level (i.e. L0, L1, L2) of the map.  if so (i.e.
	  root changed and this is not the top level), call this routine
	  again (recursion) for the next higher level of the mapping to
	  reflect the change in root for the current dmap control page.
		 are we below the top level of the map.  if so,
		  bubble the root up to the next higher level.
			 bubble up the new root of this dmap control page to
			  the next level.
				 something went wrong in bubbling up the new
				  root value, so backout the changes to the
				  current dmap control page.
					 the dbJoin() above might have
					  caused a larger binary buddy system
					  to form and we may now be in the
					  middle of it.  if this is the case,
					  back split the buddies.
				 release the buffer and return the error.
			 we're at the top level of the map. update
			  the bmap control page to reflect the size
			  of the maximum free buddy system.
	 write the buffer.
  NAME:	dbSplit()
  FUNCTION:	update the leaf of a dmtree with a new value, splitting
 		the leaf from the binary buddy system of the dmtree's
 		leaves, as required.
  PARAMETERS:
 	tp	- pointer to the tree containing the leaf.
 	leafno	- the number of the leaf to be updated.
 	splitsz	- the size the binary buddy system starting at the leaf
 		  must be split to, specified as the log2 number of blocks.
 	newval	- the new value for the leaf.
  RETURN VALUES: none
  serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entryexit;
	 check if the leaf needs to be split.
		 the split occurs by cutting the buddy system in half
		  at the specified leaf until we reach the specified
		  size.  pick up the starting split size (current size
		  - 1 in l2) and the corresponding buddy size.
		 split until we reach the specified size.
			 update the buddy's leaf with its new value.
			 on to the next size and buddy.
	 adjust the dmap tree to reflect the specified leaf's new
	  value.
  NAME:	dbBackSplit()
  FUNCTION:	back split the binary buddy system of dmtree leaves
 		that hold a specified leaf until the specified leaf
 		starts its own binary buddy system.
 		the allocators typically perform allocations at the start
 		of binary buddy systems and dbSplit() is used to accomplish
 		any required splits.  in some cases, however, allocation
 		may occur in the middle of a binary system and requires a
 		back split, with the split proceeding out from the middle of
 		the system (less efficient) rather than the start of the
 		system (more efficient).  the cases in which a back split
 		is required are rare and are limited to the first allocation
 		within an allocation group which is a part (not first part)
 		of a larger binary buddy system and a few exception cases
 		in which a previous join operation must be backed out.
  PARAMETERS:
 	tp	- pointer to the tree containing the leaf.
 	leafno	- the number of the leaf to be updated.
  RETURN VALUES: none
  serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entryexit;
	 leaf should be part (not first part) of a binary
	  buddy system.
	 the back split is accomplished by iteratively finding the leaf
	  that starts the buddy system that contains the specified leaf and
	  splitting that system in two.  this iteration continues until
	  the specified leaf becomes the start of a buddy system.
	 
	  determine maximum possible l2 size for the specified leaf.
	 determine the number of leaves covered by this size.  this
	  is the buddy size that we will start with as we search for
	  the buddy system that contains the specified leaf.
	 back split.
		 find the leftmost buddy leaf.
			 determine the buddy.
			 check if this buddy is the start of the system.
				 split the leaf at the start of the
				  system in two.
  NAME:	dbJoin()
  FUNCTION:	update the leaf of a dmtree with a new value, joining
 		the leaf with other leaves of the dmtree into a multi-leaf
 		binary buddy system, as required.
  PARAMETERS:
 	tp	- pointer to the tree containing the leaf.
 	leafno	- the number of the leaf to be updated.
 	newval	- the new value for the leaf.
  RETURN VALUES: none
	 can the new leaf value require a join with other leaves ?
		 pickup a pointer to the leaves of the tree.
		 try to join the specified leaf into a large binary
		  buddy system.  the join proceeds by attempting to join
		  the specified leafno with its buddy (leaf) at new value.
		  if the join occurs, we attempt to join the left leaf
		  of the joined buddies with its buddy at new value + 1.
		  we continue to join until we find a buddy that cannot be
		  joined (does not have a value equal to the size of the
		  last join) or until all leaves have been joined into a
		  single system.
		 
		  get the buddy size (number of words covered) of
		  the new value.
		 try to join.
			 get the buddy leaf.
			 if the leaf's new value is greater than its
			  buddy's value, we join no more.
 It shouldn't be less 
			 check which (leafno or buddy) is the left buddy.
			  the left buddy gets to claim the blocks resulting
			  from the join while the right gets to claim none.
			  the left buddy is also eligible to participate in
			  a join at the next higher level while the right
			  is not.
			 
				 leafno is the left buddy.
				 buddy is the left buddy and becomes
				  leafno.
			 on to try the next join.
	 update the leaf value.
  NAME:	dbAdjTree()
  FUNCTION:	update a leaf of a dmtree with a new value, adjusting
 		the dmtree, as required, to reflect the new leaf value.
 		the combination of any buddies must already be done before
 		this is called.
  PARAMETERS:
 	tp	- pointer to the tree to be adjusted.
 	leafno	- the number of the leaf to be updated.
 	newval	- the new value for the leaf.
  RETURN VALUES: none
	 pick up the index of the leaf for this leafno.
	 is the current value the same as the old value ?  if so,
	  there is nothing to do.
	 set the new value.
	 bubble the new value up the tree as required.
		 get the index of the first leaf of the 4 leaf
		  group containing the specified leaf (leafno).
		 get the index of the parent of this 4 leaf group.
		 determine the maximum of the 4 leaves.
		 if the maximum of the 4 is the same as the
		  parent's value, we're done.
		 parent gets new value.
		 parent becomes leaf for next go-round.
  NAME:	dbFindLeaf()
  FUNCTION:	search a dmtree_t for sufficient free blocks, returning
 		the index of a leaf describing the free blocks if
 		sufficient free blocks are found.
 		the search starts at the top of the dmtree_t tree and
 		proceeds down the tree to the leftmost leaf with sufficient
 		free space.
  PARAMETERS:
 	tp	- pointer to the tree to be searched.
 	l2nb	- log2 number of free blocks to search for.
 	leafidx	- return pointer to be set to the index of the leaf
 		  describing at least l2nb free blocks if sufficient
 		  free blocks are found.
  RETURN VALUES:
 	0	- success
 	-ENOSPC	- insufficient free blocks.
	 first check the root of the tree to see if there is
	  sufficient free space.
	 sufficient free space available. now search down the tree
	  starting at the next level for the leftmost leaf that
	  describes sufficient free space.
		 search the four nodes at this level, starting from
		  the left.
			 sufficient free space found.  move to the next
			  level (or quit if this is the last level).
		 better have found something since the higher
		  levels of the tree said it was here.
	 set the return to the leftmost leaf describing sufficient
	  free space.
  NAME:	dbFindBits()
  FUNCTION:	find a specified number of binary buddy free bits within a
 		dmap bitmap word value.
 		this routine searches the bitmap value for (1 << l2nb) free
 		bits at (1 << l2nb) alignments within the value.
  PARAMETERS:
 	word	-  dmap bitmap word value.
 	l2nb	-  number of free bits specified as a log2 number.
  RETURN VALUES:
 	starting bit number of free bits.
	 get the number of bits.
	 complement the word so we can use a mask (i.e. 0s represent
	  free bits) and compute the mask.
	 scan the word for nb free bits at nb alignments.
	 return the bit number.
  NAME:	dbMaxBud(u8 cp)
  FUNCTION:	determine the largest binary buddy string of free
 		bits within 32-bits of the map.
  PARAMETERS:
 	cp	-  pointer to the 32-bit value.
  RETURN VALUES:
 	largest binary buddy of free bits within a dmap word.
	 check if the wmap word is all free. if so, the
	  free buddy size is BUDMIN.
	 check if the wmap word is half free. if so, the
	  free buddy size is BUDMIN-1.
	 not all free or half free. determine the free buddy
	  size thru table lookup using quarters of the wmap word.
  NAME:	cnttz(uint word)
  FUNCTION:	determine the number of trailing zeros within a 32-bit
 		value.
  PARAMETERS:
 	value	-  32-bit value to be examined.
  RETURN VALUES:
 	count of trailing zeros
  NAME:	cntlz(u32 value)
  FUNCTION:	determine the number of leading zeros within a 32-bit
 		value.
  PARAMETERS:
 	value	-  32-bit value to be examined.
  RETURN VALUES:
 	count of leading zeros
  NAME:	blkstol2(s64 nb)
  FUNCTION:	convert a block count to its log2 value. if the block
 		count is not a l2 multiple, it is rounded up to the next
 		larger l2 multiple.
  PARAMETERS:
 	nb	-  number of blocks
  RETURN VALUES:
 	log2 number of blocks
 meant to be signed 
	 count the leading bits.
		 leading bit found.
			 determine the l2 value.
			 check if we need to round up.
 fix compiler warning 
  NAME:	dbAllocBottomUp()
  FUNCTION:	alloc the specified block range from the working block
 		allocation map.
 		the blocks will be alloc from the working map one dmap
 		at a time.
  PARAMETERS:
 	ip	-  pointer to in-core inode;
 	blkno	-  starting block number to be freed.
 	nblocks	-  number of blocks to be freed.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error
 block to be allocated better be within the mapsize. 
	
	  allocate the blocks a dmap at a time.
 release previous dmap if any 
 get the buffer for the current dmap. 
		 determine the number of blocks to be allocated from
		  this dmap.
 allocate the blocks. 
 write the last buffer. 
	 save the current value of the root (i.e. maximum free string)
	  of the dmap tree.
	 determine the bit number and word within the dmap of the
	  starting block.
 block range better be within the dmap 
	 allocate the bits of the dmap's words corresponding to the block
	  range. not all bits of the first and last words may be contained
	  within the block range.  if this is the case, we'll work against
	  those words (i.e. partial first andor last) on an individual basis
	  (a single pass), allocating the bits of interest by hand and
	  updating the leaf corresponding to the dmap word. a single pass
	  will be used for all dmap words fully contained within the
	  specified range.  within this pass, the bits of all fully contained
	  dmap words will be marked as free in a single shot and the leaves
	  will be updated. a single leaf may describe the free space of
	  multiple dmap words, so we may update only a subset of the actual
	  leaves corresponding to the dmap words of the block range.
		 determine the bit number within the word and
		  the number of bits within the word.
		 check if only part of a word is to be allocated.
			 allocate (set to 1) the appropriate bits within
			  this dmap word.
			 one or more dmap words are fully contained
			  within the block range.  determine how many
			  words and allocate (set to 1) the bits of these
			  words.
 determine how many bits 
 update the free count for this dmap 
 reconstruct summary tree 
	 if this allocation group is completely free,
	  update the highest active allocation group number
	  if this allocation group is the new max.
 update the free count for the allocation group and map 
 if the root has not changed, done. 
	 root changed. bubble the change up to the dmap control pages.
	  if the adjustment of the upper level control pages fails,
	  backout the bit allocation (thus making everything consistent).
  NAME:	dbExtendFS()
  FUNCTION:	extend bmap from blkno for nblocks;
 		dbExtendFS() updates bmap ready for dbAllocBottomUp();
  L2
   |
    L1---------------------------------L1
     |					 |
      L0---------L0---------L0		  L0---------L0---------L0
       |	   |	      |		   |	      |		 |
 	 d0,...,dn  d0,...,dn  d0,...,dn    d0,...,dn  d0,...,dn  d0,.,dm;
  L2L1L0d0,...,dnL0d0,...,dnL0d0,...,dnL1L0d0,...,dnL0d0,...,dnL0d0,..dm
  <---old---><----------------------------extend----------------------->
	
	 	initialize bmap control page.
	 
	  all the data in bmap control page should exclude
	  the mkfs hidden dmap page.
 update mapsize 
 compute new AG size 
 compute new number of AG 
	
	 	reconfigure db_agfree[]
	  from old AG configuration to new AG configuration;
	 
	  coalesce contiguous k (newAGSizeoldAGSize) AGs;
	  i.e., (AGi, ..., AGj) where i = kn and j = k(n+1) - 1 to AGn;
	  note: new AG size = old AG size  (2x).
 save agfree[0] 
 init collection point 
 coalesce contiguous k AGs; 
 merge AGi to AGn 
 restore agfree[0] 
	
	  update highest active ag number
	
	 	extend bmap
	 
	  update bit maps and corresponding level control pages;
	  global control page db_nfree, db_agfree[agno], db_maxfreebud;
 get L2 page 
 L2 page 
 compute start L1 
 L1 page 
	
	  extend each L1 in L2
 get L1 page 
 read in L1 page: (blkno & (MAXL1SIZE - 1)) 
 compute start L0 
 assigninit L1 page 
 compute start L0 
 1st L0 of L1.k 
		
		  extend each L0 in L1
 get L0 page 
 read in L0 page: (blkno & (MAXL0SIZE - 1)) 
 compute start dmap 
 assigninit L0 page 
 compute start dmap 
 1st dmap of L0.j 
			
			  extend each dmap in L0
				
				  reconstruct the dmap page, and
				  initialize corresponding parent L0 leaf
 read in dmap page: 
 assigninit dmap page 
 for each dmap in a L0 
			
			  build current L0 page from its leaves, and
			  initialize corresponding parent L1 leaf
 continue for next L0 
 more than 1 L0 ? 
 build L1 page 
 summarize in global bmap page 
 for each L0 in a L1 
		
		  build current L1 page from its leaves, and
		  initialize corresponding parent L2 leaf
 continue for next L1 
 more than 1 L1 ? 
 build L2 page 
 summarize in global bmap page 
 for each L1 in a L2 
	
	 	finalize bmap control page
 	dbFinalizeBmap()
	
	 	finalize bmap control page
finalize:
	
	  compute db_agpref: preferred ag to allocate from
	  (the leftmost ag with average free space in it);
agpref:
 get the number of active ags and inactive ags 
 ??? 
	 determine how many blocks are in the inactive allocation
	  groups. in doing this, we must account for the fact that
	  the rightmost group might be a partial group (i.e. file
	  system size is not a multiple of the group size).
	 determine how many free blocks are in the active
	  allocation groups plus the average number of free blocks
	  within the active ags.
	 if the preferred allocation group has not average free space.
	  re-establish the preferred group as the leftmost
	  group with average free space.
	
	  compute db_aglevel, db_agheight, db_width, db_agstart:
	  an ag is covered in aglevel dmapctl summary tree,
	  at agheight level height (from leaf) with agwidth number of nodes
	  each, which starts at agstart index node of the smmary tree node
	  array;
  NAME:	dbInitDmap()ujfs_idmap_page()
  FUNCTION:	initialize workingpersistent bitmap of the dmap page
 		for the specified number of blocks:
 		at entry, the bitmaps had been initialized as free (ZEROS);
 		The number of blocks will only account for the actually
 		existing blocks. Blocks which don't actually exist in
 		the aggregate will be marked as allocated (ONES);
  PARAMETERS:
 	dp	- pointer to page of map
 	nblocks	- number of blocks this page
  RETURNS: NONE
 starting block number within the dmap 
 word number containing start block number 
	
	  free the bits corresponding to the block range (ZEROS):
	  note: not all bits of the first and last words may be contained
	  within the block range.
 number of bits preceding range to be freed in the word 
 number of bits to free in the word 
 is partial word to be freed ? 
 free (set to 0) from the bitmap word 
 skip the word freed 
 free (set to 0) contiguous bitmap words 
 skip the words freed 
	
	  mark bits following the range to be freed (non-existing
	  blocks) as allocated (ONES)
 the first word beyond the end of existing blocks 
 does nblocks fall on a 32-bit boundary ? 
 mark a partial word allocated 
 set the rest of the words in the page to allocated (ONES) 
	
	  init tree
  NAME:	dbInitDmapTree()ujfs_complete_dmap()
  FUNCTION:	initialize summary tree of the specified dmap:
 		at entry, bitmap of the dmap has been initialized;
  PARAMETERS:
 	dp	- dmap to complete
 	blkno	- starting block number for this dmap
 	treemax	- will be filled in with max free for this dmap
  RETURNS:	max free string at the root of the tree
 init fixed info of tree 
	 init each leaf from corresponding wmap word:
	  note: leaf is set to NOFREE(-1) if all blocks of corresponding
	  bitmap word are allocated.
 build the dmap's binary buddy summary tree 
  NAME:	dbInitTree()ujfs_adjtree()
  FUNCTION:	initialize binary buddy summary tree of a dmap or dmapctl.
 		at entry, the leaves of the tree has been initialized
 		from corresponding bitmap word or root of summary tree
 		of the child control page;
 		configure binary buddy system at the leaf level, then
 		bubble up the values of the leaf nodes up the tree.
  PARAMETERS:
 	cp	- Pointer to the root of the tree
 	l2leaves- Number of leaf nodes as a power of 2
 	l2min	- Number of blocks that can be covered by a leaf
 		  as a power of 2
  RETURNS: max free string at the root of the tree
 Determine the maximum free string possible for the leaves 
	
	  configure the leaf levevl into binary buddy system
	 
	  Try to combine buddies starting with a buddy size of 1
	  (i.e. two leaves). At a buddy size of 1 two buddy leaves
	  can be combined if both buddies have a maximum free of l2min;
	  the combination will result in the left-most buddy leaf having
	  a maximum free of l2min+1.
	  After processing all buddies for a given size, process buddies
	  at the next higher buddy size (i.e. current size  2) and
	  the next maximum free (current free + 1).
	  This continues until the maximum possible buddy combination
	  yields maximum free.
 get next buddy size == current buddy pair size 
 scan each adjacent buddy pair at current buddy size 
 coalesce if both adjacent buddies are max free 
 left take right 
 right give left 
	
	  bubble summary information of leaves up the tree.
	 
	  Starting at the leaf node level, the four nodes described by
	  the higher level parent node are compared for a maximum free and
	  this maximum becomes the value of the parent node.
	  when all lower level nodes are processed in this fashion then
	  move up to the next level (parent becomes a lower level node) and
	  continue the process for that level.
 get index of 1st node of parent level 
		 set the value of the parent node as the maximum
		  of the four nodes of the current level.
 	dbInitDmapCtl()
  function: initialize dmapctl page
 start leaf index not covered by range 
	
	  initialize the leaves of current level that were not covered
	  by the specified input block range (i.e. the leaves have no
	  low level dmapctl or dmap).
 build the dmap's binary buddy summary tree 
  NAME:	dbGetL2AGSize()ujfs_getagl2size()
  FUNCTION:	Determine log2(allocation group size) from aggregate size
  PARAMETERS:
 	nblocks	- Number of blocks in aggregate
  RETURNS: log2(allocation group size) in aggregate blocks
 round up aggregate size to power of 2 
 agsize = roundupSizemax_number_of_ag 
  NAME:	dbMapFileSizeToMapSize()
  FUNCTION:	compute number of blocks the block allocation map file
 		can cover from the map file size;
  RETURNS:	Number of blocks which can be covered by this block map file;
  maximum number of map pages at each level including control pages
  convert number of map pages to the zero origin top dmapctl level
	 At each level, accumulate the number of dmap pages covered by
	  the number of full child levels below it;
	  repeat for the last incomplete child level.
 skip the first global control page 
 skip higher level control pages above top level covered by map 
 skip top level's control page 
 pages in lastincomplete child 
 skip incomplete child's level control page 
	 convert the number of dmaps into the number of blocks
	  which can be covered by the dmaps;
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
 	jfs_umount.c
  note: file system in transition to aggregatefileset:
  (ref. jfs_mount.c)
  file system unmount is interpreted as mount of the singleonly
  fileset in the aggregate and, if unmount of the last fileset,
  as unmount of the aggerate;
  NAME:	jfs_umount(vfsp, flags, crp)
  FUNCTION:	vfs_umount()
  PARAMETERS:	vfsp	- virtual file system pointer
 		flags	- unmount for shutdown
 		crp	- credential
  RETURN :	EBUSY	- device has open files
	
	 	update superblock and close log
	 
	  if mounted read-write and log based recovery was enabled
		
		  Wait for outstanding transactions to be written to log:
	
	  close fileset inode allocation map (aka fileset inode)
	
	  close secondary aggregate inode allocation map
	
	  close aggregate inode allocation map
	
	  close aggregate block allocation map
	
	  Make sure all metadata makes it to disk before we mark
	  the superblock as clean
	
	  ensure all file system file pages are propagated to their
	  home blocks on disk (and their in-memory buffer pages are
	  invalidated) BEFORE updating file system superblock state
	  (to signify file system is unmounted cleanly, and thus in
	  consistent state) and log superblock active file system
	  list (to signify skip logredo()).
 log = NULL if read-only mount 
		
		  close log:
		 
		  remove file system from log active file system list.
	
	  close log:
	 
	  remove file system from log active file system list.
	
	  Make sure all metadata makes it to disk
	
	  Note that we have to do this even if sync_blockdev() will
	  do exactly the same a few instructions later:  We can't
	  mark the superblock clean before everything is flushed to
	  disk.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) Christoph Hellwig, 2001-2002
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
  forward references
  NAME:	extAlloc()
  FUNCTION:	allocate an extent for a specified page range within a
 		file.
  PARAMETERS:
 	ip	- the inode of the file.
 	xlen	- requested extent length.
 	pno	- the starting page number with the file.
 	xp	- pointer to an xad.  on entry, xad describes an
 		  extent that is used as an allocation hint if the
 		  xaddr of the xad is non-zero.  on successful exit,
 		  the xad describes the newly allocated extent.
 	abnr	- bool indicating whether the newly allocated extent
 		  should be marked as allocated but not recorded.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error.
 	-ENOSPC	- insufficient disk resources.
 This blocks if we are low on resources 
 Avoid race with jfs_commit_inode() 
 validate extent length 
 get the page's starting extent offset 
 check if an allocation hint was provided 
 get the size of the extent described by the hint 
		 check if the hint is for the portion of the file
		  immediately previous to the current allocation
		  request and if hint extent has the same abnr
		  value as the current request.  if so, we can
		  extend the hint extent to include the current
		  extent if we can allocate the blocks immediately
		  following the hint extent.
 adjust the hint to the last block of the extent 
	 allocate the disk blocks for the extent.  initially, extBalloc()
	  will try to allocate disk blocks for the requested size (xlen).
	  if this fails (xlen contiguous free blocks not available), it'll
	  try to allocate a smaller number of blocks (producing a smaller
	  extent), with this smaller number of blocks consisting of the
	  requested number of blocks rounded down to the next smaller
	  power of 2 number (i.e. 16 -> 8).  it'll continue to round down
	  and retry the allocation until the number of blocks to allocate
	  is smaller than the number of blocks per page.
 Allocate blocks to quota. 
 determine the value of the extent flag 
	 if we can extend the hint extent to cover the current request,
	  extend it.  otherwise, insert a new extent to
	  cover the current request.
	 if the extend or insert failed,
	  free the newly allocated blocks and return the error.
 set the results of the extent allocation 
	
	  COMMIT_SyncList flags an anonymous tlock on page that is on
	  sync list.
	  We need to commit the inode to get the page written disk.
  NAME:	extRealloc()
  FUNCTION:	extend the allocation of a file extent containing a
 		partial back last page.
  PARAMETERS:
 	ip	- the inode of the file.
 	cp	- cbuf for the partial backed last page.
 	xlen	- request size of the resulting extent.
 	xp	- pointer to an xad. on successful exit, the xad
 		  describes the newly allocated extent.
 	abnr	- bool indicating whether the newly allocated extent
 		  should be marked as allocated but not recorded.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error.
 	-ENOSPC	- insufficient disk resources.
 This blocks if we are low on resources 
 validate extent length 
	 get the extend (partial) page's disk block address and
	  number of blocks.
	 if the extend page is abnr and if the request is for
	  the extent to be allocated and recorded,
	  make the page allocated and recorded.
	 try to allocated the request number of blocks for the
	  extent.  dbRealloc() first tries to satisfy the request
	  by extending the allocation in place. otherwise, it will
	  try to allocate a new set of blocks large enough for the
	  request.  in satisfying a request, dbReAlloc() may allocate
	  less than what was request but will always allocate enough
	  space as to satisfy the extend page.
 Allocat blocks to quota. 
	 check if the extend page is not abnr but the request is abnr
	  and the allocated disk space is for more than one page.  if this
	  is the case, there is a miss match of abnr between the extend page
	  and the one or more pages following the extend page.  as a result,
	  two extents will have to be manipulated. the first will be that
	  of the extent of the extend page and will be manipulated thru
	  an xtExtend() or an xtTailgate(), depending upon whether the
	  disk allocation occurred as an inplace extension.  the second
	  extent will be manipulated (created) through an xtInsert() and
	  will be for the pages following the extend page.
	 if we were able to extend the disk allocation in place,
	  extend the extent.  otherwise, move the extent to a
	  new disk location.
 extend the extent 
		
		  move the extent to a new location:
		 
		  xtTailgate() accounts for relocated tail extent;
 check if we need to also insert a new extent 
		 perform the insert.  if it fails, free the blocks
		  to be inserted and make it appear that we only did
		  the xtExtend() or xtTailgate() above.
 set the return results 
 _NOTYET 
  NAME:	extHint()
  FUNCTION:	produce an extent allocation hint for a file offset.
  PARAMETERS:
 	ip	- the inode of the file.
 	offset  - file offset for which the hint is needed.
 	xp	- pointer to the xad that is to be filled in with
 		  the hint.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error.
 init the hint as "no hint provided" 
	 determine the starting extent offset of the page previous
	  to the page containing the offset.
	 if the offset is in the first page of the file, no hint provided.
		
		  only preserve the abnr flag within the xad flags
		  of the returned hint.
  NAME:	extRecord()
  FUNCTION:	change a page with a file from not recorded to recorded.
  PARAMETERS:
 	ip	- inode of the file.
 	cp	- cbuf of the file page.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error.
 	-ENOSPC	- insufficient disk resources.
 update the extent 
  NAME:	extFill()
  FUNCTION:	allocate disk space for a file page that represents
 		a file hole.
  PARAMETERS:
 	ip	- the inode of the file.
 	cp	- cbuf of the file page represent the hole.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error.
 	-ENOSPC	- insufficient disk resources.
	assert(ISSPARSE(ip));
 initialize the extent allocation hint 
 allocate an extent to fill the hole 
 _NOTYET 
  NAME:	extBalloc()
  FUNCTION:	allocate disk blocks to form an extent.
 		initially, we will try to allocate disk blocks for the
 		requested size (nblocks).  if this fails (nblocks
 		contiguous free blocks not available), we'll try to allocate
 		a smaller number of blocks (producing a smaller extent), with
 		this smaller number of blocks consisting of the requested
 		number of blocks rounded down to the next smaller power of 2
 		number (i.e. 16 -> 8).  we'll continue to round down and
 		retry the allocation until the number of blocks to allocate
 		is smaller than the number of blocks per page.
  PARAMETERS:
 	ip	 - the inode of the file.
 	hint	 - disk block number to be used as an allocation hint.
 	nblocks - pointer to an s64 value.  on entry, this value specifies
 		   the desired number of block to be allocated. on successful
 		   exit, this value is set to the number of blocks actually
 		   allocated.
 	blkno	 - pointer to a block address that is filled in on successful
 		   return with the starting block number of the newly
 		   allocated block range.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error.
 	-ENOSPC	- insufficient disk resources.
	 get the number of blocks to initially attempt to allocate.
	  we'll first try the number of blocks requested unless this
	  number is greater than the maximum number of contiguous free
	  blocks in the map. in that case, we'll start off with the
	  maximum free.
 try to allocate blocks 
		 if something other than an out of space error,
		  stop and return this error.
 decrease the allocation request size 
 give up if we cannot cover a page 
  NAME:	extBrealloc()
  FUNCTION:	attempt to extend an extent's allocation.
 		Initially, we will try to extend the extent's allocation
 		in place.  If this fails, we'll try to move the extent
 		to a new set of blocks.  If moving the extent, we initially
 		will try to allocate disk blocks for the requested size
 		(newnblks).  if this fails (new contiguous free blocks not
 		available), we'll try to allocate a smaller number of
 		blocks (producing a smaller extent), with this smaller
 		number of blocks consisting of the requested number of
 		blocks rounded down to the next smaller power of 2
 		number (i.e. 16 -> 8).  We'll continue to round down and
 		retry the allocation until the number of blocks to allocate
 		is smaller than the number of blocks per page.
  PARAMETERS:
 	ip	 - the inode of the file.
 	blkno	 - starting block number of the extents current allocation.
 	nblks	 - number of blocks within the extents current allocation.
 	newnblks - pointer to a s64 value.  on entry, this value is the
 		   new desired extent size (number of blocks).  on
 		   successful exit, this value is set to the extent's actual
 		   new size (new number of blocks).
 	newblkno - the starting block number of the extents new allocation.
  RETURN VALUES:
 	0	- success
 	-EIO	- io error.
 	-ENOSPC	- insufficient disk resources.
 try to extend in place 
	 in place extension not possible.
	  try to move the extent to a new set of blocks.
 _NOTYET 
  NAME:	extRoundDown()
  FUNCTION:	round down a specified number of blocks to the next
 		smallest power of 2 number.
  PARAMETERS:
 	nb	- the inode of the file.
  RETURN VALUES:
 	next smallest power of 2 number.
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2002
  Latin upper case
 000-00f 
 010-01f 
 020-02f 
 030-03f 
 040-04f 
 050-05f 
 060-06f 
 070-07f 
 080-08f 
 090-09f 
 0a0-0af 
 0b0-0bf 
 0c0-0cf 
 0d0-0df 
 0e0-0ef 
 0f0-0ff 
 100-10f 
 110-11f 
 120-12f 
 130-13f 
 140-14f 
 150-15f 
 160-16f 
 170-17f 
 180-18f 
 190-19f 
 1a0-1af 
 1b0-1bf 
 1c0-1cf 
 1d0-1df 
 1e0-1ef 
 1f0-1ff 
 Upper case range - Greek 
 3a0-3af 
 3b0-3bf 
 Upper case range - Cyrillic 
 430-43f 
 440-44f 
 450-45f 
 Upper case range - Extended cyrillic 
 490-49f 
 4a0-4af 
 4b0-4bf 
 Upper case range - Extended latin and greek 
 1e00-1e0f 
 1e10-1e1f 
 1e20-1e2f 
 1e30-1e3f 
 1e40-1e4f 
 1e50-1e5f 
 1e60-1e6f 
 1e70-1e7f 
 1e80-1e8f 
 1e90-1e9f 
 1ea0-1eaf 
 1eb0-1ebf 
 1ec0-1ecf 
 1ed0-1edf 
 1ee0-1eef 
 1ef0-1eff 
 1f00-1f0f 
 1f10-1f1f 
 1f20-1f2f 
 1f30-1f3f 
 1f40-1f4f 
 1f50-1f5f 
 1f60-1f6f 
 1f70-1f7f 
 1f80-1f8f 
 1f90-1f9f 
 1fa0-1faf 
 1fb0-1fbf 
 1fc0-1fcf 
 1fd0-1fdf 
 1fe0-1fef 
 Upper case range - Wide latin 
 ff40-ff4f 
  Upper Case Range
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
    Portions Copyright (C) Christoph Hellwig, 2001-2002
 see jfs_debug.h 
 yes, I know this is an ASCIIism.  --hch 
 PROC_FS_JFS 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
  NAME:	ialloc()
  FUNCTION:	Allocate a new inode
	
	  New inodes need to save sane values on disk when
	  uid & gid mount options are used
	
	  Allocate inode to quota.
 inherit flags from parent 
 Zero remaining fields 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
    Portions Copyright (C) Christoph Hellwig, 2001-2002
 	jfs_logmgr.c: log manager
  for related information, see transaction manager (jfs_txnmgr.c), and
  recovery manager (jfs_logredo.c).
  note: for detail, RTFS.
 	log buffer manager:
  special purpose buffer manager supporting log io requirements.
  per log serial pageout of logpage
  queuing io requests and redrive io at iodone
  maintain current logpage buffer
  no caching since append only
  appropriate jfs buffer cache buffers as needed
 	group commit:
  transactions which wrote COMMIT records in the same in-memory
  log page during the pageout of previouscurrent log page(s) are
  committed together by the pageout of the page.
 	TBD lazy commit:
  transactions are committed asynchronously when the log page
  containing it COMMIT is paged out when it becomes full;
 	serialization:
  . a per log lock serialize log write.
  . a per log lock serialize group commit.
  . a per log lock serialize log openclose;
 	TBD log integrity:
  careful-write (ping-pong) of last logpage to recover from crash
  in overwrite.
  detection of split (out-of-order) write of physical sectors
  of last logpage via timestamp at end of each sector
  with its mirror data array at trailer).
 	alternatives:
  lsn - 64-bit monotonically increasing integer vs
  32-bit lspn and page eor.
 for sync_blockdev() 
  lbuf's ready to be redriven.  Protected by log_redrive_lock (jfsIO thread)
 	log readwrite serialization (per log)
 	log group commit serialization (per log)
 	log sync serialization (per log)
#define	LOGSYNC_DELTA(logsize)		min((logsize)4, 256LOGPSIZE)
#define	LOGSYNC_BARRIER(logsize)	((logsize)2)
 	log buffer cache synchronization
  See __SLEEP_COND in jfs_locks.h
 	lbuf buffer cache (lCache) control
 log buffer manager pageout control (cumulative, inclusive) 
#define	lbmWRITE	0x0002	 enqueue at tail of write queue;
				  init pageout if at head of queue;
#define	lbmRELEASE	0x0004	 remove from write queue
				  at completion of pageout;
				  do not freerecycle it yet:
				  caller will free it;
#define	lbmSYNC		0x0008	 do not return to freelist
				  when removed from write queue;
#define lbmFREE		0x0010	 return to freelist
				  at completion of pageout;
				  the buffer may be recycled;
#define lbmGC		0x0080	 lbmIODone to perform post-GC processing
				  of log page
  Global list of active external journals
  forward references
 	statistics
 # of commit 
 # of page written 
 # of pages submitted 
 # of full pages submitted 
 # of partial pages submitted 
  NAME:	lmLog()
  FUNCTION:	write a log record;
  PARAMETER:
  RETURN:	lsn - offset to the next log record to write (end-of-log);
 		-1  - error;
  note: todo: log error handler
 log by (out-of-transaction) JFS ? 
 log from page ? 
	
	 	initializeupdate pagetransaction recovery lsn
	
	  initialize page lsn if first log write of the page
 insert page at tail of logsynclist 
	
	 	initializeupdate lsn of tblock of the page
	 
	  transaction inherits oldest lsn of pages associated
	  with allocationdeallocation of resources (their
	  log records are used to reconstruct allocation map
	  at recovery time: inode for inode allocation map,
	  B+-tree index of extent descriptors for block
	  allocation map);
	  allocation map pages inherit transaction lsn at
	  commit time to allow forwarding log syncpt past log
	  records associated with allocationdeallocation of
	  resources only after persistent map of these map pages
	  have been updated and propagated to home.
	
	  initialize transaction lsn:
 inherit lsn of its first page logged 
 insert tblock after the page on logsynclist 
	
	  update transaction lsn:
 inherit oldestsmallest lsn of page 
 update tblock lsn with page lsn 
 move tblock after page on logsynclist 
	
	 	write the log record
	
	  forward log syncpt if log reached next syncpt trigger
 update end-of-log lsn 
 return end-of-log address 
  NAME:	lmWriteRecord()
  FUNCTION:	move the log record to current log page
  PARAMETER:	cd	- commit descriptor
  RETURN:	end-of-log address
  serialization: LOG_LOCK() held on entryexit
 end-of-log address 
 dst log page buffer 
 dst log page 
 destination address in log page 
 end-of-log offset in log page 
 free space in log page 
 src meta-data page 
 number of bytes to move 
 retrieve destination log page to write 
 any log data to write ? 
	
	 	move log record data
 retrieve source meta-data page to log 
 retrieve source in-memory inode to log 
 _JFS_WIP 
 Probably should trap 
 is page full ? 
 page become full: move on to next page 
		
		  move log vector data
 is page not full ? 
 page become full: move on to next page 
		
		  move log vector descriptor
	
	 	move log record descriptor
 are there more to move than freespace of page ? 
		
		  end of log record descriptor
 update last log record eor 
 # of commit 
			
			  enqueue tblock for group commit:
			 
			  enqueue tblock of non-trivialsynchronous COMMIT
			  at tail of group commit queue
			  (trivialasynchronous COMMITs are ignored by
			  group commit.)
 init tblock gc state 
 enqueue transaction to commit queue 
 page not full ? 
 page become full: move on to next page 
  NAME:	lmNextPage()
  FUNCTION:	write current page and allocate next page.
  PARAMETER:	log
  RETURN:	0
  serialization: LOG_LOCK() held on entryexit
 log sequence page number 
 current page number 
 get current log page number and log sequence page number 
	
	 	write or queue the full page at the tail of write queue
 get the tail tblk on commit queue 
	 every tblk who has COMMIT record on the current page,
	  and has not been committed, must be on commit queue
	  since tblk is queued at commit queueu at the time
	  of writing its COMMIT record on the page before
	  page becomes full (even though the tblk thread
	  who wrote COMMIT record may have been suspended
	  currently);
 is page bound with outstanding tail tblk ? 
 mark tblk for end-of-page 
			 if page is not already on write queue,
			  just enqueue (no lbmWRITE to prevent redrive)
			  buffer to wqueue to ensure correct serial order
			  of the pages since log pages will be added
			  continuously
			
			  No current GC leader, initiate group commit
	 page is not bound with outstanding tblk:
	  init write or mark it to be redriven (lbmWRITE)
 finalize the page 
	
	 	allocateinitialize next page
	 if log wraps, the first data page of log is 2
	  (0 never used, 1 is superblock).
 ? valid page emptyfull at logRedo() 
 allocateinitialize next log page buffer 
 initialize next log page 
  NAME:	lmGroupCommit()
  FUNCTION:	group commit
 	initiate pageout of the pages with COMMIT in the order of
 	page number - redrive pageout of the page at the head of
 	pageout queue until full page has been written.
  RETURN:
  NOTE:
 	LOGGC_LOCK serializes log group commit queue, and
 	transaction blocks on the commit queue.
 	N.B. LOG_LOCK is NOT held during lmGroupCommit().
 group committed already ? 
		
		  No pageout in progress
		 
		  start group commit as its group leader.
		
		  Lazy transactions can leave now
 lmGCwrite gives up LOGGC_LOCK, check again 
	 upcount transaction waiting for completion
 removed from commit queue 
  NAME:	lmGCwrite()
  FUNCTION:	group commit write
 	initiate write of log page, building a group of all transactions
 	with commit records on that page.
  RETURN:	None
  NOTE:
 	LOGGC_LOCK must be held by caller.
 	N.B. LOG_LOCK is NOT held during lmGroupCommit().
 group commit page number 
	
	  build the commit group of a log page
	 
	  scan commit queue and make a commit group of all
	  transactions with COMMIT records on the same log page.
 get the head tblk on the commit queue 
 state transition: (QUEUE, READY) -> COMMIT 
 last tblk of the page 
	
	  pageout to commit transactions on the log page.
 is page already full ? 
 mark page to free at end of group commit of the page 
 page is not yet full 
 ? bp->l_ceor = bp->l_eor; 
  NAME:	lmPostGC()
  FUNCTION:	group commit post-processing
 	Processes transactions after their commit records have been written
 	to disk, redriving log IO if necessary.
  RETURN:	None
  NOTE:
 	This routine is called a interrupt time by lbmIODone
LOGGC_LOCK(log);
	
	  current pageout of group commit completed.
	 
	  removewakeup transactions from commit queue who were
	  group committed with the current log page
		 if transaction was marked GC_COMMIT then
		  it has been shipped in the current pageout
		  and made it to disk - it is committed.
 remove it from the commit queue 
 we can stop flushing the log now 
			
			  Hand tblk over to lazy commit thread
 state transition: COMMIT -> COMMITTED 
		 was page full before pageout ?
		  (and this is the last tblk bound with the page)
		 did page become full after pageout ?
		  (and this is the last tblk bound with the page)
 finalize the page 
	 are there any transactions who have entered lnGroupCommit()
	  (whose COMMITs are after that of the last log page written.
	  They are waiting for new group commit (above at (SLEEP 1))
	  or lazy transactions are on a full (queued) log page,
	  select the latest ready transaction as new group leader and
	  wake her up to lead her group.
		
		  Call lmGCwrite with new group leader
	 no transaction are ready yet (transactions are only just
	  queued (GC_QUEUE) and not entered for group commit yet).
	  the first transaction entering group commit
	  will elect herself as new group leader.
LOGGC_UNLOCK(log);
  NAME:	lmLogSync()
  FUNCTION:	write log SYNCPT record for specified log
 	if new sync address is available
 	(normally the case if sync() is executed by back-ground
 	process).
 	calculate new value of i_nextsync which determines when
 	this code is called again.
  PARAMETERS:	log	- log structure
 		hard_sync - 1 to force all metadata to be written
  RETURN:	0
  serialization: LOG_LOCK() held on entryexit
 written since last syncpt 
 free space left available 
 additional delta to write normally 
 additional write granted 
 push dirty metapages out to disk 
	
	 	forward syncpt
	 if last sync is same as last syncpt,
	  invoke sync point forward processing to update sync.
	 if sync is different from last syncpt,
	  write a SYNCPT record with syncpt = sync.
	  reset syncpt = sync
	
	 	setup next syncpt trigger (SWAG)
		
		 	log wrapping
		 
		  option 1 - panic ? No.!
		  option 2 - shutdown file systems
		 	      associated with log ?
		  option 3 - extend log ?
		  option 4 - second chance
		 
		  mark log wrapped, and continue.
		  when all active transactions are completed,
		  mark log valid for recovery.
		  if crashed during invalid state, log state
		  implies invalid log, forcing fsck().
 mark log state log wrap in log superblock 
 log->state = LOGWRAP; 
 reset sync point computation 
 next syncpt trigger = written + more 
	 if number of bytes written from last sync point is more
	  than 14 of the log size, stop new transactions from
	  starting until all current transactions are completed
	  by setting syncbarrier flag.
		
		  We may have to initiate group commit
  NAME:	jfs_syncpt
  FUNCTION:	write log SYNCPT record for specified log
  PARAMETERS:	log	  - log structure
 		hard_sync - set to 1 to force metadata to be written
  NAME:	lmLogOpen()
  FUNCTION:	open the log on first open;
 	insert filesystem in the active list of the log.
  PARAMETER:	ipmnt	- file system mount inode
 		iplog	- log inode (out)
  RETURN:
  serialization:
			
			  add file system to log active file system list
	
	 	external log as separate logical volume
	 
	  file systems to log may have n-to-1 relationship;
	
	  initialize log:
	
	  add file system to log active file system list
	
	 	unwind on error
 unwind lbmLogInit() 
 close external log device 
 free log descriptor 
	
	  initialize log.
 Make up some stuff 
  NAME:	lmLogInit()
  FUNCTION:	log initialization at first log open.
 	logredo() (or logformat()) should have been run previously.
 	initialize the log from log superblock.
 	set the log state in the superblock to LOGMOUNT and
 	write SYNCPT log record.
  PARAMETER:	log	- log structure
  RETURN:	0	- if ok
 		-EINVAL	- bad log magic number or superblock dirty
 		error returned from logwait()
  serialization: single first open thread
 initialize the group commit serialization lock 
 allocateinitialize the log write serialization lock 
	
	  initialize log io
 check for disabled journaling to disk 
		
		  Journal pages will still be filled.  When the time comes
		  to actually do the IO, the write is not done, and the
		  endio routine is called directly.
		
		  validate log superblock
 logredo() should have been run successfully. 
 initialize log from log superblock 
		
		  initialize for log append write mode
 establish currentend-of-log pagebuffer 
 if current page is full, move on to next page 
		
		  initialize log syncpoint
		
		  write the first SYNCPT record with syncpoint = 0
		  (i.e., log redo up to HERE !);
		  remove current page from lbm write queue at end of pageout
		  (to write log superblock update), but do not release to
		  freelist;
		
		  updatewrite superblock
 initialize logsync parameters 
	
	  initialize for lazygroup commit
	
	 	unwind on error
 release log page 
 release log superblock 
 unwind lbmLogInit() 
  NAME:	lmLogClose()
  FUNCTION:	remove file system <ipmnt> from active list of log <iplog>
 		and close it on last close.
  PARAMETER:	sb	- superblock
  RETURN:	errors from subroutines
  serialization:
	
	  We need to make sure all of the "written" metapages
	  actually make it to disk
		
		 	in-line log in host file system
	
	  TODO: ensure that the dummy_log is in a state to allow
	  lbmLogShutdown to deallocate all the buffers and call
	  kfree against dummy_log.  For now, leave dummy_log & its
	  buffers in memory, and resuse if another no-integrity mount
	  is requested.
	
	 	external log as separate logical volume
  NAME:	jfs_flush_journal()
  FUNCTION:	initiate write of any outstanding transactions to the journal
 		and optionally wait until they are all written to disk
 		wait == 0  flush until latest txn is committed, don't wait
 		wait == 1  flush until latest txn is committed, wait
 		wait > 1   flush until all txn's are complete, wait
 jfs_write_inode may call us during read-only mount 
		
		  This ensures that we will keep writing to the journal as long
		  as there are unwritten commit records
			
			  We're already flushing.
			  if flush_tblk is NULL, we are flushing everything,
			  so leave it that way.  Otherwise, update it to the
			  latest transaction
 Only flush until latest transaction is committed 
			
			  Initiate IO on outstanding transactions
 Flush until all activity complete 
	
	  If there was recent activity, we may need to wait
	  for the lazycommit thread to catch up
 Too much? 
  NAME:	lmLogShutdown()
  FUNCTION:	log shutdown at last LogClose().
 		write log syncpt record.
 		update super block to set redone flag to 0.
  PARAMETER:	log	- log inode
  RETURN:	0	- success
  serialization: single last close thread
	
	  write the last SYNCPT record with syncpoint = 0
	  (i.e., log redo up to HERE !)
	
	  synchronous update log superblock
	  mark log state as shutdown cleanly
	  (i.e., Log does not need to be replayed).
	
	  shutdown per log io
  NAME:	lmLogFileSystem()
  FUNCTION:	insert (<activate> = true)remove (<activate> = false)
 	file system intofrom log active file system list.
  PARAMETE:	log	- pointer to logs inode.
 		fsdev	- kdev_t of filesystem.
 		serial	- pointer to returned log serial number
 		activate - insertremove device from active list.
  RETURN:	0	- success
 		errors returned by vms_iowait().
	
	  insertremove file system device to log active file system list.
 Is there a better rc? 
	
	  synchronous write log superblock:
	 
	  write sidestream bypassing write queue:
	  at file system mount, log super block is updated for
	  activation of the file system before any log record
	  (MOUNT record) of the file system, and at file system
	  unmount, all meta data for the file system has been
	  flushed before log super block is updated for deactivation
	  of the file system.
 		log buffer manager (lbm)
 		------------------------
  special purpose buffer manager supporting log io requirements.
  per log write queue:
  log pageout occurs in serial order by fifo write queue and
  restricting to a single io in pregress at any one time.
  a circular singly-linked list
  (log->wrqueue points to the tail, and buffers are linked via
  bp->wrqueue field), and
  maintains log page in pageout ot waiting for pageout in serial pageout.
 	lbmLogInit()
  initialize per log IO setup at lmLogInit()
 log inode 
 initialize current buffer cursor 
 initialize log device write queue 
	
	  Each log has its own buffer pages allocated to it.  These are
	  not managed by the page cache.  This ensures that a transaction
	  writing to the log does not block trying to allocate a page from
	  the page cache (for the log).  This would be bad, since page
	  allocation waits on the kswapd thread that may be committing inodes
	  which would cause log activity.  Was that clear?  I'm trying to
	  avoid deadlock here.
 we already have one reference 
 	lbmLogShutdown()
  finalize per log IO setup at lmLogShutdown()
 	lbmAllocate()
  allocate an empty log buffer
	
	  recycle from log buffer freelist if any
 	lbmFree()
  release a log buffer to freelist
	
	  return the buffer to head of freelist
  NAME:	lbmRedrive
  FUNCTION:	add a log buffer to the log redrive list
  PARAMETER:
 	bp	- log buffer
  NOTES:
 	Takes log_redrive_lock.
 	lbmRead()
	
	  allocate a log buffer
check if journaling to disk has been disabled
 	lbmWrite()
  buffer at head of pageout queue stays after completion of
  partial-page pageout and redriven by explicit initiation of
  pageout by caller until full-page pageout is completed and
  released.
  device driver io done redrives pageout of new buffer at
  head of pageout queue when current buffer at head of pageout
  queue is released at the completion of its full-page pageout.
  LOGGC_LOCK() serializes lbmWrite() by lmNextPage() and lmGroupCommit().
  LCACHE_LOCK() serializes xflag between lbmWrite() and lbmIODone()
 map the logical block address to physical block address 
 disable+lock 
	
	  initialize buffer for device driver
	
	 	insert bp at tail of write queue associated with log
	 
	  (request is either for bp alreadycurrently at head of queue
	  or new bp to be inserted at tail)
 is buffer not already on write queue ? 
 insert at tail of wqueue 
 is buffer at head of wqueue and for write ? 
 unlock+enable 
 unlock+enable 
 	lbmDirectWrite()
  initiate pageout bypassing write queue for sidestream
  (e.g., log superblock) write;
	
	  initialize buffer for device driver
 map the logical block address to physical block address 
	
	 	initiate pageout of the page
  NAME:	lbmStartIO()
  FUNCTION:	Interface to DD strategy routine
  RETURN:	none
  serialization: LCACHE_LOCK() is NOT held during log io;
 check if journaling to disk has been disabled 
 	lbmIOWait()
 disable+lock 
 unlock+enable 
 	lbmIODone()
  executed at INTIODONE level
	
	  get back jfs buffer bound to the io buffer
 disable+lock 
	
	 	pagein completion
 unlock+enable 
 wakeup IO initiator 
	
	 	pageout completion
	 
	  the bp at the head of write queue has completed pageout.
	 
	  if single-commitfull-page pageout, remove the current buffer
	  from head of pageout queue, and redrive pageout with
	  the new buffer at head of pageout queue;
	  otherwise, the partial-page pageout buffer stays at
	  the head of pageout queue to be redriven for pageout
	  by lmGroupCommit() until full-page pageout is completed.
 update committed lsn 
 single element queue 
		 remove head buffer of full-page pageout
		  from log device write queue
 multi element queue 
		 remove head buffer of full-page pageout
		  from log device write queue
			
			  redrive pageout of next page at head of write queue:
			  redrive next page without any bound tblk
			  (i.e., page wo any COMMIT records), or
			  first page of new group commit which has been
			  queued after current page (subsequent pageout
			  is performed synchronously, except page without
			  any COMMITs) by lmGroupCommit() as indicated
			  by lbmWRITE flag;
				
				  We can't do the IO at interrupt time.
				  The jfsIO thread can do it
	
	 	synchronous pageout:
	 
	  buffer has not necessarily been removed from write queue
	  (e.g., synchronous write of partial-page with COMMIT):
	  leave buffer for io initiator to dispose
 unlock+enable 
 wakeup IO initiator 
	
	 	Group Commit pageout:
	
	 	asynchronous pageout:
	 
	  buffer must have been removed from write queue:
	  insert buffer at head of freelist where it can be recycled
 unlock+enable 
  NAME:	lmLogFormat()jfs_logform()
  FUNCTION:	format file system log
  PARAMETERS:
 	log	- volume log
 	logAddress - start address of log space in FS block
 	logSize	- length of log space in FS block;
  RETURN:	0	- success
 		-EIO	- io error
  XXX: We're synchronously writing one page at a time.  This needs to
 	be improved by writing multiple pages at once.
 log sequence page number 
 allocate a log buffer 
	
	 	log space:
	 
	  page 0 - reserved;
	  page 1 - log superblock;
	  page 2 - log data page: A SYNC log record is written
	 	    into this page at logform time;
	  pages 3-N - log data page: set to empty log data pages;
	
	 	init log superblock: log page 1
 ? 
	
	 	init pages 2 to npages-1 as log data pages:
	 
	  log page sequence number (lpsn) initialization:
	 
	  pn:   0     1     2     3                 n-1
	        +-----+-----+=====+=====+===.....===+=====+
	  lspn:             N-1   0     1           N-2
	                    <--- N page circular file ---->
	 
	  the N (= npages-2) data pages of the log is maintained as
	  a circular file for the log records;
	  lpsn grows by 1 monotonically as each log page is written
	  to the circular file of the log;
	  and setLogpage() will not reset the page number even if
	  the eor is equal to LOGPHDRSIZE. In order for binary search
	  still work in find log end process, we have to simulate the
	  log wrap situation at the log format time.
	  The 1st log page written will have the highest lpsn. Then
	  the succeeding log pages will have ascending order of
	  the lspn starting from 0, ... (N-2)
	
	  initialize 1st log page to be written: lpsn = N - 1,
	  write a SYNCPT log record is written to this page
	
	 	initialize succeeding log pages: lpsn = 0, 1, ..., (N-2)
	
	 	finalize log
 release the buffer 
 CONFIG_JFS_STATISTICS 
 SPDX-License-Identifier: GPL-2.0-or-later
    Copyright (C) International Business Machines Corp., 2000-2004
  NAME:	jfs_strfromUCS()
  FUNCTION:	Convert little-endian unicode string to character string
 Only warn up to 5 times total 
 once per string 
  NAME:	jfs_strtoUCS()
  FUNCTION:	Convert character string to unicode string
  NAME:	get_UCSname()
  FUNCTION:	Allocate and translate to unicode string
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) Neil Brown 2002
  Copyright (C) Christoph Hellwig 2007
  This file contains the code mapping from inodes to NFS file handles,
  and for mapping back from file handles to dentries.
  For details on why we do all the strange and hairy things in here
  take a look at Documentationfilesystemsnfsexporting.rst.
  Check if the dentry or any of it's aliases is acceptable.
  Reconnect a directory dentry with its parent.
  This can return a dentry, or NULL, or an error.
  In the first case the returned dentry is the parent of the given
  dentry, and may itself need to be reconnected to its parent.
  In the NULL case, a concurrent VFS operation has either renamed or
  removed this directory.  The concurrent operation has reconnected our
  dentry, so we no longer need to.
		
		  Somebody has renamed it since exportfs_get_name();
		  great, since it could've only been renamed if it
		  got looked up and thus connected, and it would
		  remain connected afterwards.  We are done.
	
	  Someone must have renamed our entry into another parent, in
	  which case it has been reconnected by the rename.
	 
	  Or someone removed it entirely, in which case filehandle
	  lookup will succeed but the directory is now IS_DEAD and
	  subsequent operations on it will fail.
	 
	  Alternatively, maybe there was no race at all, and the
	  filesystem is just corrupt and gave us a parent that doesn't
	  actually contain any entry pointing to this inode.  So,
	  double check that this worked and return -ESTALE if not:
  Make sure target_dir is fully connected to the dentry tree.
  On successful return, DCACHE_DISCONNECTED will be cleared on
  target_dir, and target_dir->d_parent->...->d_parent will reach the
  root of the filesystem.
  Whenever DCACHE_DISCONNECTED is unset, target_dir is fully connected.
  But the converse is not true: target_dir may have DCACHE_DISCONNECTED
  set but already be connected.  In that case we'll verify the
  connection to root and then clear the flag.
  Note that target_dir could be removed by a concurrent operation.  In
  that case reconnect_path may still succeed with target_dir fully
  connected, but further operations using the filehandle will fail when
  necessary (due to S_DEAD being set on the directory).
	char name;		 name that was found. It already points to a
 the inum we are looking for 
 inode matched? 
 sequence counter 
  A rather strange filldir function to capture
  the name matching the specified inode number.
  get_name - default export_operations->get_name function
  @path:   the directory in which to find a name
  @name:   a pointer to a %NAME_MAX+1 char buffer to store the name
  @child:  the dentry for the child directory.
  calls readdir on the parent until it finds an entry with
  the same inode number as the child, and returns that.
	
	  inode->i_ino is unsigned long, kstat->ino is u64, so the
	  former would be insufficient on 32-bit hosts when the
	  filesystem supports 64-bit inode numbers.  So we need to
	  actually call ->getattr, not just read i_ino:
	
	  Open the directory ...
  export_encode_fh - default export_operations->encode_fh function
  @inode:   the object to encode
  @fid:     where to store the file handle fragment
  @max_len: maximum length to store there
  @parent:  parent directory inode, if wanted
  This default encode_fh function assumes that the 32 inode number
  is suitable for locating an inode, and that the generation number
  can be used to check that it is still valid.  It places them in the
  filehandle fragment where export_decode_fh expects to find them.
		
		  note that while p might've ceased to be our parent already,
		  it's still pinned by and still positive.
	
	  Try to get any dentry for the given file handle from the filesystem.
	
	  If no acceptance criteria was specified by caller, a disconnected
	  dentry is also accepatable. Callers may use this mode to query if
	  file handle is stale or to get a reference to an inode without
	  risking the high overhead caused by directory reconnect.
		
		  This request is for a directory.
		 
		  On the positive side there is only one dentry for each
		  directory inode.  On the negative side this implies that we
		  to ensure our dentry is connected all the way up to the
		  filesystem root.
		
		  It's not a directory.  Life is a little more complicated.
		
		  See if either the dentry we just got from the filesystem
		  or any alias for it is acceptable.  This is always true
		  if this filesystem is exported without the subtreecheck
		  option.  If the filesystem is exported with the subtree
		  check option there's a fair chance we need to look at
		  the parent directory in the file handle and make sure
		  it's connected to the filesystem root.
		
		  Try to extract a dentry for the parent directory from the
		  file handle.  If this fails we'll have to give up.
		
		  And as usual we need to make sure the parent directory is
		  connected to the filesystem root.  The VFS really doesn't
		  like disconnected directories..
		
		  Now that we've got both a well-connected parent and a
		  dentry for the inode we're after, make sure that our
		  inode is actually connected to the parent.
		
		  At this point we are done with the parent, but it's pinned
		  by the child dentry anyway.
		
		  And finally make sure the dentry is actually acceptable
		  to NFSD.
  FUSE: Filesystem in Userspace
  Copyright (C) 2016 Canonical Ltd. <seth.forshee@canonical.com>
  This program can be distributed under the terms of the GNU GPL.
  See the file COPYING.
		
		  Fuse userspace is responsible for updating access
		  permissions in the inode, if needed. fuse_setxattr
		  invalidates the inode attributes, which will force
		  them to be refreshed the next time they are used,
		  and it also updates i_ctime.
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (C) 2017 Red Hat, Inc.
  CUSE servers compiled on 32bit broke on 64bit kernels because the
  ABI was defined to be 'struct iovec' which is different on 32bit
  and 64bit.  Fortunately we can determine which structure the server
  used from the size of the reply.
		
		  With this interface a 32bit server cannot support
		  non-compat (i.e. ones coming from 64bit apps) ioctl
		  requests
 Make sure iov_length() won't overflow 
 Did the server supply an inappropriate value? 
  For ioctls, there is no generic way to determine how much memory
  needs to be read andor written.  Furthermore, ioctls are allowed
  to dereference the passed pointer, so the parameter requires deep
  copying but FUSE has no idea whatsoever about what to copy in or
  out.
  This is solved by allowing FUSE server to retry ioctl with
  necessary inout iovecs.  Let's assume the ioctl implementation
  needs to read in the following structure.
  struct a {
 	char	buf;
 	size_t	buflen;
  }
  On the first callout to FUSE server, inarg->in_size and
  inarg->out_size will be NULL; then, the server completes the ioctl
  with FUSE_IOCTL_RETRY set in out->flags, out->in_iovs set to 1 and
  the actual iov array to
  { { .iov_base = inarg.arg,	.iov_len = sizeof(struct a) } }
  which tells FUSE to copy in the requested area and retry the ioctl.
  On the second round, the server has access to the structure and
  from that it can tell what to look for next, so on the invocation,
  it sets FUSE_IOCTL_RETRY, out->in_iovs to 2 and iov array to
  { { .iov_base = inarg.arg,	.iov_len = sizeof(struct a)	},
    { .iov_base = a.buf,	.iov_len = a.buflen		} }
  FUSE will copy both struct a and the pointed buffer from the
  process doing the ioctl and retry ioctl with both struct a and the
  buffer.
  This time, FUSE server has everything it needs and completes ioctl
  without FUSE_IOCTL_RETRY which finishes the ioctl call.
  Copying data out works the same way.
  Note that if FUSE_IOCTL_UNRESTRICTED is clear, the kernel
  automatically initializes in and out iovs by decoding @cmd with
  _IOC_ macros and the server is not allowed to request RETRY.  This
  limits ioctl data transfers to well-formed ioctls and is the forced
  behavior for all FUSE servers.
 assume all the iovs returned by client always fits in a page 
	
	  If restricted, initialize IO parameters as encoded in @cmd.
	  RETRY from server is not allowed.
	
	  Out data can be used either for actual out data or iovs,
	  make sure there always is at least one page.
 make sure there are enough buffer pages and init request with them 
 okay, let's send it to the client 
 did it ask for retry? 
 no retry if in restricted mode 
		
		  Make sure things are in boundary, separate checks
		  are to protect against overflow.
  FUSE: Filesystem in Userspace
  Copyright (C) 2001-2016  Miklos Szeredi <miklos@szeredi.hu>
  This program can be distributed under the terms of the GNU GPL.
  See the file COPYING.
 This is really two different operations rolled into one 
 This is really two different operations rolled into one 
  FUSE: Filesystem in Userspace
  Copyright (C) 2001-2008  Miklos Szeredi <miklos@szeredi.hu>
  This program can be distributed under the terms of the GNU GPL.
  See the file COPYING.
 Maximum number of outstanding background requests 
 Congestion starts at 75% of maximum 
 Will write inode on closemunmap and in all other dirtiers 
  ino_t is 32-bits on 32-bit arch. We have to squash the 64-bit value down
  so that it will fit.
 mtime from server may be stale due to local buffered write 
	
	  Don't set the sticky bit in i_mode, unless we want the VFS
	  to check permissions.  This prevents failures due to the
	  check in may_delete().
	
	  We are refreshing inode data and it is possible that another
	  client set suidsgid or security.capability xattr. So clear
	  S_NOSEC. Ideally, we could have cleared it only if suidsgid
	  was set or if security.capability xattr was set. But we don't
	  know if security.capability has been set or not. So clear it
	  anyway. Its less efficient but should be safe.
	
	  In case of writeback_cache enabled, writes update mtime, ctime and
	  may update i_size.  In these cases trust the cached value in the
	  inode.
	
	  In case of writeback_cache enabled, the cached writes beyond EOF
	  extend local i_size without keeping userspace server in sync. So,
	  attr->size coming from server can be stale. We cannot trust it.
			
			  Auto inval mode also checks and invalidates if mtime
			  has changed.
	
	  Auto mount points get their node id from the submount root, which is
	  not a unique identifier within this filesystem.
	 
	  To avoid conflicts, do not place submount points into the inode hash
	  table.
 nodeid was reused, any IO on the old inode should fail 
 fsid is left zero 
 Initial active count 
 No outstanding writes? 
	
	  Completion of new bucket depends on completion of this bucket, so add
	  one more count.
	
	  Drop initial active count.  At this point if all writes in this and
	  ancestor buckets complete, the count will go to zero and this task
	  will be woken up.
 Drop temp count on descendant bucket 
	
	  Userspace cannot handle the wait == 0 case.  Avoid a
	  gratuitous roundtrip.
 The filesystem is being unmounted.  Nothing to do. 
		
		  Ignore options coming from mount(MS_REMOUNT) for backward
		  compatibility.
	
	  The default maximum number of async requests is calculated to consume
	  12^13 of the total memory, assuming 392 bytes per request.
 LOOKUP has dependency on proto version 
	 Variable length argument used for backward compatibility
	   with interface version < 7.5.  Rest of init_out is zeroed
		
		  sb->s_bdi points to blkdev's bdi however we want to redirect
		  it to our private bdi...
 fuse does it's own writeback accounting 
	
	  For a single fuse filesystem use max 1% of dirty +
	  writeback threshold.
	 
	  This gives about 1M of write buffer for memory maps on a
	  machine with 1G and 10% dirty_ratio, which should be more
	  than enough.
	 
	  Privileged users can raise it by writing to
	 
	     sysclassbdi<bdi>max_ratio
	
	  If we are not in the initial user namespace posix
	  acls must be translated.
	
	  This inode is just a duplicate, so it is not looked up and
	  its nlookup should not be incremented.  fuse_iget() does
	  that, though, so undo it here.
 Filesystem context private data holds the FUSE inode of the mount point 
 Initialize superblock, making @mp_fi its root 
 Handle umasking inside the fuse code 
 Root dentry doesn't have .d_revalidate 
	
	  Require mount to happen from the same user namespace which
	  opened devfuse to prevent potential attacks.
 file->private_data shall be visible on all CPUs after this 
  This is the path where user supplied an already initialized fuse dev.  In
  this case never create a new super if the old one is gone.
	
	  While block dev mount can be initialized with a dummy device fd
	  (found by device name), normal fuse mounts can't
	
	  Allow creating a fuse mount with an already initialized fuse
	  connection
  Set up the filesystem mount context.
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
 SPDX-License-Identifier: GPL-2.0
  virtio-fs: Virtio Filesystem
  Copyright (C) 2018 Red Hat, Inc.
 Used to help calculate the FUSE connection's max_pages limit for a request's
  size. Parts of the struct fuse_req are sliced into scattergather lists in
  addition to the pages used, so this can help account for that overhead.
 List of virtio-fs device instances and a lock for the list. Also provides
  mutual exclusion in device removal and mounting path
 Per-virtqueue state 
 protected by ->lock 
 End these requests 
 No inflight requests 
 A virtio-fs device instance 
 on virtio_fs_instances 
 number of virtqueues 
 number of request queues 
 DAX memory window where file contents are mapped 
 This request can be temporarily queued on virt queue 
 Should be called with fsvq->lock held. 
 Should be called with fsvq->lock held. 
 Make sure virtiofs_mutex is held 
 Wait for in flight requests to finish.
		 We are holding virtio_fs_mutex. There should not be any
		  waiters waiting for completion.
	 Provides mutual exclusion between ->remove and ->kill_sb
	  paths. We don't want both of these draining queue at the
	  same time. Current completion logic reinits completion
	  and that means there should not be any other thread
	  doing reinit or waiting for completion already.
 Add a new instance to the list or return -EEXIST if tag name exists
 Return the virtio_fs with a given tag, or NULL 
 not found 
 Read filesystem name from virtio config into fs->tag (must kfree()). 
 empty tag 
 Work function for hiprio completion 
 Free completed FUSE_FORGET requests 
 Dispatch pending requests 
  Returns 1 if queue is full and sender should wait a bit before sending
  next request, 0 otherwise.
 Queue is full 
 Allocate and copy args into req->argbuf 
 Copy args out of and free req->argbuf 
 Store the actual size of the variable-length arg 
 Work function for request completion 
	
	  TODO verify that server properly follows FUSE protocol
	  (oh.uniq, oh.len)
 Collect completed requests off the virtqueue 
 End requests 
 blocking async request completes in a worker context 
 Virtqueue interrupt handler 
 Initialize virtqueues 
 Initialize the hiprioforget request virtqueue 
 Initialize the requests virtqueues 
 Free virtqueues (device must already be reset) 
 Map a window offset to a page frame number.  The window offset will have
  been produced by .iomap_begin(), which maps a file offset to a window
  offset.
 Get cache region 
	 Ideally we would directly use the PCI BAR resource but
	  devm_memremap_pages() wants its own copy in pgmap.  So
	  initialize a struct resource from scratch (only the start
	  and end fields will be used).
 TODO vq affinity 
	 Bring the device online in case the filesystem is mounted and
	  requests need to be sent before we return.
 This device is going away. No one should get new reference 
 Put device reference on virtio_fs object 
 TODO need to save state here 
 TODO need to restore state here 
 CONFIG_PM_SLEEP 
 Allocate a buffer for the request 
	
	  TODO interrupts.
	 
	  Normal fs operations on a local filesystems aren't interruptible.
	  Exceptions are blocking lock operations; for example fcntl(F_SETLKW)
	  with shared lock between host and guest.
 Count number of scatter-gather elements required 
 Return the number of scatter-gather list elements required 
 fuse_in_header ;
 fuse_out_header ;
 Add pages to scatter-gather list and return number of elements used 
 Add args to scatter-gather list and return number of elements used 
 Add a request to a virtqueue and kick the device 
 requests need at least 4 elements 
 Does the sglist fit on the stack? 
 Use a bounce buffer since stack args cannot be mapped 
 Request elements 
 Reply elements 
 Request successfully sent. 
 matches barrier in request_wait_answer() 
 TODO multiqueue 
			
			  Virtqueue full. Retry submission from worker
			  context as we might be holding fc->bg_lock.
 Can't end request in submission context. Use a worker 
	 After holding mutex, make sure virtiofs device is still there.
	  Though we are holding a reference to it, drive ->remove might
	  still have cleaned up virtual queues. In that case bail out.
 Allocate fuse_dev for hiprio and notification queues 
 virtiofs allocates and installs its own fuse devices 
 Previous unmount will stop all queues. Start these again 
	 Stop dax worker. Soon evict_inodes() will be called which
	  will free all memory ranges belonging to all inodes.
 Stop forget queue. Soon destroy will be sent 
	 fuse_conn_destroy() must have sent destroy. Stop all queues
	  and drain one more time and free fuse devices. Freeing fuse
	  devices will drop their reference on fuse_conn and that in
	  turn will drop its reference on virtio_fs object.
 If mount failed, we can still be called without any fc 
	 This gets a reference on virtio_fs object. This ptr gets installed
	  in fc->iq->priv. Once fuse_conn is going away, it calls ->put()
	  to drop the reference to this object.
 Tell FUSE to split requests that exceed the virtqueue's size 
 SPDX-License-Identifier: GPL-2.0-only
  CUSE: Character device in Userspace
  Copyright (C) 2008-2009  SUSE Linux Products GmbH
  Copyright (C) 2008-2009  Tejun Heo <tj@kernel.org>
  CUSE enables character devices to be implemented from userland much
  like FUSE allows filesystems.  On initialization devcuse is
  created.  By opening the file and replying to the CUSE_INIT request
  userland CUSE server can create a character device.  After that the
  operation is very similar to FUSE.
  A CUSE instance involves the following objects.
  cuse_conn	: contains fuse_conn and serves as bonding structure
  channel	: file handle connected to the userland CUSE server
  cdev		: the implemented character device
  dev		: generic device for cdev
  Note that 'channel' is what 'dev' is in FUSE.  As CUSE deals with
  devices, it's called 'channel' to reduce confusion.
  channel determines when the character device dies.  When channel is
  closed, everything begins to destruct.  The cuse_conn is taken off
  the lookup table preventing further access from cdev, cdev and
  generic device are removed and the base reference of cuse_conn is
  put.
  On each open, the matching cuse_conn is looked up and if found an
  additional reference is taken which is released when the file is
  closed.
 linked on cuse_conntbl 
 Dummy mount referencing fc 
 fuse connection 
 associated character device 
 device representing @cdev 
 init parameters, set once during initialization 
 protects registration 
  CUSE frontend operations
  These are file operations for the character device.
  On open, CUSE opens a file from the FUSE mnt and stores it to
  private_data of the open file.  All other ops call FUSE ops on the
  FUSE file.
	
	  No locking or generic_write_checks(), the server is
	  responsible for locking and sanity checks.
 look up and get the connection 
 dead? 
	
	  Generic permission check is already done against the chrdev
	  file, proceed to open.
  CUSE channel initialization and destruction
  cuse_parse_one - parse one key=value pair
  @pp: io parameter for the current position
  @end: points to one past the end of the packed string
  @keyp: out parameter for key
  @valp: out parameter for value
  @pp points to packed strings - "key0=val0\0key1=val1\0" which ends
  at @end - 1.  This function parses one pair and set @keyp to the
  start of the key and @valp to the start of the value.  Note that
  the original string is modified such that the key string is
  terminated with '\0'.  @pp is updated to point to the next string.
  RETURNS:
  1 on successful parse, 0 on EOF, -errno on failure.
  cuse_parse_dev_info - parse device info
  @p: device info string
  @len: length of device info string
  @devinfo: out parameter for parsed device info
  Parse @p to extract device info and store it into @devinfo.  String
  pointed to by @p is modified by parsing and @devinfo points into
  them, so @p shouldn't be freed while @devinfo is in use.
  RETURNS:
  0 on success, -errno on failure.
  cuse_process_init_reply - finish initializing CUSE channel
  This function creates the character device and sets up all the
  required data structures for it.  Please read the comment at the
  top of this file for high level overview.
 parse init reply 
 determine and reserve devt 
 devt determined, create device 
 make sure the device-name is unique 
 register cdev 
 make the device available 
 announce device availability 
  cuse_channel_open - open method for devcuse
  @inode: inode for devcuse
  @file: file struct being opened
  Userland CUSE server can create a CUSE device by opening devcuse
  and replying to the initialization request kernel sends.  This
  function is responsible for handling CUSE device initialization.
  Because the fd opened by this function is used during
  initialization, this function only creates cuse_conn and sends
  init.  The rest is delegated to a kthread.
  RETURNS:
  0 on success, -errno on failure.
 set up cuse_conn 
	
	  Limit the cuse channel to requests that can
	  be represented in file->f_cred->user_ns.
  cuse_channel_release - release method for devcuse
  @inode: inode for devcuse
  @file: file struct being closed
  Disconnect the channel, deregister CUSE device and initiate
  destruction by putting the default reference.
  RETURNS:
  0 on success, -errno on failure.
 remove from the conntbl, no more access from this point on 
 remove device 
 puts the base reference 
 initialized during init 
  Misc stuff and module initializatiion
  CUSE exports the same set of attributes to sysfs as fusectl.
 init conntbl 
 inherit and extend fuse_dev_operations 
 CUSE is not prepared for FUSE_DEV_IOC_CLONE 
  FUSE: Filesystem in Userspace
  Copyright (C) 2001-2008  Miklos Szeredi <miklos@szeredi.hu>
  This program can be distributed under the terms of the GNU GPL.
  See the file COPYING.
	
	  Mess with DCACHE_OP_DELETE because dput() will be faster without it.
	  Don't care about races, either way it's just an optimization
  FUSE caches dentries and attributes with separate timeout.  The
  time in jiffies until the dentryattributes are valid is stored in
  dentry->d_fsdata and fuse_inode->i_time respectively.
  Calculate the time in jiffies until a dentryattributes are valid
  Set dentry and possibly attribute timeouts from the lookupmk
  replies
  Mark the attributes as stale, so that at the next call to
  ->getattr() they will be fetched from userspace
  Mark the attributes as stale due to an atime change.  Avoid the invalidate if
  atime is not used.
  Just mark the entry as stale, so that a next attempt to look it up
  will result in a new lookup call to userspace
  This is called when a dentry is about to become negative and the
  timeout is unknown (unlink, rmdir, rename and in some cases
  lookup)
  Same as fuse_invalidate_entry_cache(), but also try to remove the
  dentry from the hash
  Check whether the dentry is still valid
  If the entry validity timeout has expired and the dentry is
  positive, try to redo the lookup.  If the lookup results in a
  different inode, then let the VFS invalidate the dentry and redo
  the lookup once more.  If the lookup results in the same inode,
  then refresh the attributes, timeouts and mark the dentry valid.
 For negative dentries, always do a fresh lookup 
 Zero nodeid is same as -ENOENT 
  Create a fuse_mount object with a new superblock (with path->dentry
  as the root), and return that mount so it can be auto-mounted on
  @path.
 Pass the FUSE inode of the mount for fuse_get_tree_submount() 
 Create the submount 
 Zero nodeid is same as -ENOENT, but with valid timeout 
  Atomic create+open operation
  If the filesystem doesn't support this, then fall back to separate
  'mknod' + 'open' requests.
 Userspace expects S_IFREG in create mode 
 Only creates 
  Code shared between mknod, mkdir, symlink and link
	
	  If i_nlink == 0 then unlink doesn't make sense, yet this can
	  happen if userspace filesystem is careless.  It would be
	  difficult to enforce correct nlink usage so just ignore this
	  condition here
 ctime changes 
 newent will end up negative 
		 If request was interrupted, DEITY only knows if the
		   rename actually took place.  If the invalidation
		   fails (e.g. some process has CWD under the renamed
		   directory), then there can be inconsistency between
 Directories have separate file-handle space 
  Calling into a user-controlled filesystem gives the filesystem
  daemon ptrace-like capabilities over the current process.  This
  means, that the filesystem daemon is able to record the exact
  filesystem operations performed, and can also control the behavior
  of the requester process in otherwise impossible ways.  For example
  it can delay the operation for arbitrary length of time allowing
  DoS against the requester.
  For this reason only those processes can call into the filesystem,
  for which the owner of the mount has ptrace privilege.  This
  excludes processes started by other users, suid or sgid processes.
  Check permission.  The two basic access models of FUSE are:
  1) Local access checking ('default_permissions' mount option) based
  on file mode.  This is the plain old disk filesystem permission
  modell.
  2) "Remote" access checking, where server is responsible for
  checking permission in each inode operation.  An exception to this
  is if ->permission() was invoked from sys_access() in which case an
  access request is sent.  Execute permission is still checked
  locally based on file mode.
	
	  If attributes are needed, refresh them before proceeding
		 If permission is denied, try to refresh file
		   attributes.  This is also needed, because the root
		 Note: the opposite of the above test does not
		   exist.  So if permissions are revoked this won't be
		   noticed immediately, only after the attribute
 FUSE_IOCTL_DIR only supported for API version >= 7.18 
 Always update if mtime is explicitly set  
 Or if kernel i_mtime is the official one 
 If it's an open(O_TRUNC) or an ftruncate(), don't update 
 In all other cases update 
  Prevent concurrent writepages on inode
  This is done by adding a negative bias to the inode write counter
  and waiting for all pending writes to finish.
  Allow writepages on inode
  Remove the bias from the writecounter and send any queued
  writepages.
  Flush inode->i_mtime to the server
  Set attributes, and at the same time refresh them.
  Truncation is slightly complicated, because the 'truncate' request
  may fail, in which case we don't want to touch the mapping.
  vmtruncate() doesn't allow for this case, so do the rlimit checking
  and the actual truncation by hand.
 This is coming from open(..., ... | O_TRUNC); 
			
			  No need to send request to userspace, since actual
			  truncation has already been done by OPEN.  But still
			  need to truncate page cache.
 Flush dirty datametadata before non-truncate SETATTR 
 Kill suidsgid for non-directory chown unconditionally 
 For mandatory locking in truncate 
 Kill suidsgid for truncate only if no CAP_FSETID 
 the kernel maintains i_mtime locally 
 FIXME: clear I_DIRTY_SYNC? 
 see the comment in fuse_change_attributes() 
 NOTE: this may releasereacquire fi->lock 
	
	  Only call invalidate_inode_pages2() after removing
	  FUSE_NOWRITE, otherwise fuse_launder_page() would deadlock.
		
		  The only sane way to reliably kill suidsgid is to do it in
		  the userspace filesystem
		 
		  This should be done on write(), truncate() and chown().
			
			  ia_mode calculation may have used stale i_mode.
			  Refresh and recalculate.
		
		  If filesystem supports acls it may have updated acl xattrs in
		  the filesystem, so forget cached acls for the inode.
 Directory mode changed, may need to revalidate access 
			
			  If user explicitly requested nothing then don't
			  error out, but return st_dev only.
  FUSE: Filesystem in Userspace
  Copyright (C) 2001-2008  Miklos Szeredi <miklos@szeredi.hu>
  This program can be distributed under the terms of the GNU GPL.
  See the file COPYING.
 Do nothing when client does not implement 'open' 
 Default for no-open 
	
	  file may be written through mmap, so chain it onto the
	  inodes's write_file list
 Inode is NULL on error path of fuse_create_open() 
 Hold inode until release is finished 
	
	  Normally this will send the RELEASE request, however if
	  some asynchronous READ or WRITE requests are outstanding,
	  the sending will be delayed.
	 
	  Make the release synchronous if this is a fuseblk mount,
	  synchronous RELEASE is allowed (and desirable) in this case
	  because the server can be trusted not to screw up.
 return value is ignored by VFS 
	
	  iput(NULL) is a no-op and since the refcount is 1 and everything's
	  synchronous, we are fine with not doing igrab() here"
  Scramble the ID space with XTEA, so that the value of the files_struct
  pointer is not exposed to userspace.
  Check if any page in a range is under writeback
  This is currently done by walking the list of writepage requests
  for the inode, which can be pretty inefficient.
  Wait for page writeback to be completed.
  Since fuse doesn't rely on the VM writeback tracking, this has to
  use some other means.
  Wait for all pending writepages on the inode to finish.
  This is currently done by blocking further writes with FUSE_NOWRITE
  and waiting for all sent writes to complete.
  This must be called under i_mutex, otherwise the FUSE_NOWRITE usage
  could conflict with truncation.
	
	  In memory i_blocks is not maintained by fuse, if writeback cache is
	  enabled, i_blocks from cached attr may not be accurate.
	
	  Start writeback against all dirty pages of the inode, then
	  wait for all outstanding writes, before sending the FSYNC
	  request.
	
	  Due to implementation of fuse writeback
	  file_write_and_wait_range() does not catch errors.
	  We have to do this directly after fuse_sync_writes()
  In case of short read, the caller sets 'pos' to the position of
  actual end of fuse request in IO request. Otherwise, if bytes_requested
  == bytes_transferred or rw == WRITE, the caller sets 'pos' to -1.
  An example:
  User requested DIO read of 64K. It was split into two 32K fuse requests,
  both submitted asynchronously. The first of them was ACKed by userspace as
  fully completed (req->out.args[0].size == 32K) resulting in pos == -1. The
  second request was ACKed as short, e.g. only 1K was read, resulting in
  pos == 33K.
  Thus, when all fuse requests are completed, the minimal non-negative 'pos'
  will be equal to the length of the longest contiguous fragment of
  transferred data starting from the beginning of IO request.
 Nothing 
	
	  If writeback_cache is enabled, a short read means there's a hole in
	  the file.  Some data after the hole is in page cache, but has not
	  reached the client fs yet.  So the hole is not present there.
	
	  Page writeback can extend beyond the lifetime of the
	  page-cache page, so make sure we read a properly synced
	  page.
 Don't overflow end offset 
	
	  Short read means EOF.  If file size is larger, truncate it
		
		  Short read means EOF. If file size is larger, truncate it
 Don't overflow end offset 
	
	  In auto invalidate mode, always update attributes on read.
	  Otherwise, only update if we attempt to read past EOF (to ensure
	  i_size is up to date).
 If we copied full page, mark it uptodate 
 break out of the loop on short write 
 Update size (EOF optimization) and mode (SUID clearing) 
 We can write back this queue in page reclaim 
 # bytes already packed in req 
 Special case for kernel IO: can copy directly into the buffer 
 Don't allow parallel writes to the same file 
 Called under fi->lock, may release and reacquire it 
 Got truncated off completely 
 Fails on broken connection only 
 After fuse_writepage_finish() aux request list is private 
  If fi->writectr is positive (no truncate or fsync going on) send
  all queued writepage requests.
  Called with fi->lock
	
	  A writeback finished and this might have updated mtimectime on
	  server making local mtimectime stale.  Hence invalidate attrs.
	  Do this only if writeback_cache is not enabled.  If writeback_cache
	  is enabled, we trust local ctimemtime.
		
		  Skip fuse_flush_writepages() to make it easy to crop requests
		  based on primary request size.
		 
		  1st case (trivial): there are no concurrent activities using
		  fuse_setrelease_nowrite.  Then we're on safe side because
		  fuse_flush_writepages() would call fuse_send_writepage()
		  anyway.
		 
		  2nd case: someone called fuse_set_nowrite and it is waiting
		  now for completion of all in-flight requests.  This happens
		  rarely and no more than once per page, so this should be
		  okay.
		 
		  3rd case: someone (e.g. fuse_do_setattr()) is in the middle
		  of fuse_set_nowrite..fuse_release_nowrite section.  The fact
		  that fuse_set_nowrite returned implies that all in-flight
		  requests were completed along with all of their secondary
		  requests.  Further primary requests are blocked by negative
		  writectr.  Hence there cannot be any in-flight requests and
		  no invocations of fuse_writepage_end() while we're in
		  fuse_set_nowrite..fuse_release_nowrite section.
	
	  Inode is always written before the last reference is dropped and
	  hence this should not be reached from reclaim.
	 
	  Writing back the inode from reclaim can deadlock if the request
	  processing itself needs an allocation.  Allocations triggering
	  reclaim while serving a request can't be prevented, because it can
	  involve any number of unrelated userspace processes.
 Prevent resurrection of dead bucket in unlikely race with syncfs 
		
		  ->writepages() should be called for sync() and friends.  We
		  should only get here on direct reclaim and then we are
		  allowed to skip a page which is already in flight
  Check under fi->lock if the page is under writeback, and insert it onto the
  rb_tree if not. Otherwise iterate auxiliary write requests, to see if there's
  one already added for a page at this offset.  If there's none, then insert
  this new request onto the auxiliary list, otherwise reuse the existing one by
  swapping the new temp page with the old one.
	
	  Being under writeback is unlikely but possible.  For example direct
	  read to an mmaped fuse file will set the page dirty twice; once when
	  the pages are faulted with get_user_pages(), and then after the read
	  completed.
 Reached max pages 
 Reached max write bytes 
 Discontinuity 
 Need to grow the pages array?  If so, did the expansion fail? 
	
	  The page must not be redirtied until the writeout is completed
	  (i.e. userspace has sent a reply to the write request).  Otherwise
	  there could be more than one temporary page instance for each real
	  page.
	 
	  This is ensured by holding the page lock in page_mkwrite() while
	  checking fuse_page_is_writeback().  We already hold the page lock
	  since clear_page_dirty_for_io() and keep it held until we add the
	  request to the fi->writepages list and increment ap->num_pages.
	  After this fuse_page_is_writeback() will indicate that the page is
	  under writeback, so we can release the page lock.
		
		  Protected by fi->lock against concurrent access by
		  fuse_page_is_writeback().
  It's worthy to make sure that space is reserved on disk for the write,
  but how to implement it without killing performance need more thinking.
	
	  Check if the start this page comes after the end of file, in which
	  case the readpage can be optimized away.
 Haven't copied anything?  Skip zeroing, size extending, dirtying. 
 Zero any unwritten bytes at the end of the page 
 Serialize with pending writeback for the same page 
  Write back dirty datametadata now (there may not be any suitable
  open files later for data)
  Wait for writeback against this page to complete before allowing it
  to be marked dirty again, and hence written back again, possibly
  before the previous writepage completed.
  Block here, instead of in ->writepage(), so that the userspace fs
  can only block processes actually operating on the filesystem.
  Otherwise unprivileged userspace fs would be able to block
  unrelated:
  - page migration
  - sync(2)
  - try_to_free_pages() with order > PAGE_ALLOC_COSTLY_ORDER
 DAX mmap is superior to direct_io mmap 
 Can't provide the coherency needed for MAP_SHARED 
		
		  Convert pid into init's pid namespace.  The locks API will
		  translate it into the caller's pid namespace.
 NLM needs asynchronous locks, which we don't support yet 
 Unlock on close is handled by the flush method 
 locking is restartable 
 emulate flock with POSIX locks 
 No i_mutex protection necessary for SEEK_CUR and SEEK_SET 
  All files which have been polled are linked to RB tree
  fuse_conn->polled_files which is indexed by kh.  Walk the tree and
  find the matching one.
  The file is about to be polled.  Make sure it's on the polled_files
  RB tree.  Note that files once added to the polled_files tree are
  not removed before the file is released.  This is because a file
  polled once is likely to be polled again.
	
	  Ask for notification iff there's someone waiting for it.
	  The client may ignore the flag and always notify.
  This is called from fuse_handle_notify() on FUSE_NOTIFY_POLL and
  wakes up the poll waiters.
	
	  By default, we want to optimize all IOs with async request
	  submission to the client filesystem if supported.
 optimization for short read 
	
	  We cannot asynchronously extend the size of a file.
	  In such case the aio will behave exactly like sync io.
		
		  Additional reference to keep io around after
		  calling fuse_aio_complete()
 we have a non-extending, async request, so return 
 we could have extended the file 
	 mark unstable when write-back is not used, and file_out gets
	
	  Write out dirty pages in the destination file before sending the COPY
	  request to userspace.  After the request is completed, truncate off
	  pages (including partial ones) from the cache that have been copied,
	  since these contain stale data at that point.
	 
	  This should be mostly correct, but if the COPY writes to partial
	  pages (at the start or end) and the parts not covered by the COPY are
	  written through a memory map after calling fuse_writeback_range(),
	  then these partial page modifications will be lost on truncation.
	 
	  It is unlikely that someone would rely on such mixed style
	  modifications.  Yet this does give less guarantees than if the
	  copying was performed with write(2).
	 
	  To fix this a mapping->invalidate_lock could be used to prevent new
	  faults while the copy is ongoing.
  FUSE: Filesystem in Userspace
  Copyright (C) 2001-2008  Miklos Szeredi <miklos@szeredi.hu>
  This program can be distributed under the terms of the GNU GPL.
  See the file COPYING.
 Ordinary requests have even IDs, while interrupts IDs are odd 
	
	  Lockless access is OK, because file->private data is set
	  once during mount and is valid until the file is released.
 Must be called with > 1 refcount 
 Make sure stores before this are seen on another CPU 
	
	  lockess check of fc->connected is okay, because atomic_dec_and_test()
	  provides a memory barrier matched with the one in fuse_wait_aborted()
	  to ensure no wake-up is missed.
 wake up aborters 
 Matches smp_wmb() in fuse_set_initialized() 
			
			  We get here in the unlikely case that a background
			  request was allocated but not sent
  A new request is available, wake fiq->waitq
  This function is called when a request is finished.  Either a reply
  has arrived or it was aborted (and not yet sent) or some error
  occurred during communication with userspace, or the device file
  was closed.  The requester thread is woken up (if still waiting),
  the 'end' callback is called if given, else the reference to the
  request is released
	
	  test_and_set_bit() implies smp_mb() between bit
	  changing and below FR_INTERRUPTED check. Pairs with
	  smp_mb() from queue_interrupt().
			
			  Wake up next waiter, if any.  It's okay to use
			  waitqueue_active(), as we've already synced up
			  fc->blocked with waiters with the wake_up() call
			  above.
 Wake up waiter sleeping in request_wait_answer() 
 Check for we've sent request to interrupt this req 
		
		  Pairs with smp_mb() implied by test_and_set_bit()
		  from fuse_request_end().
 Any signal may interrupt this 
 matches barrier in fuse_dev_do_read() 
 Only fatal signals may interrupt this 
 Request is not yet in userspace, bail out 
	
	  Either request is already in userspace, or it was forced.
	  Wait it out.
		 acquire extra reference, since request is still needed
 Pairs with smp_wmb() in fuse_request_end() 
 Needs to be done after fuse_get_req() so that fc->minor is valid 
  Lock the request.  Up to the next unlock_request() there mustn't be
  anything that could cause a page-fault.  If the request was already
  aborted bail out.
  Unlock request.  If it was aborted while locked, caller is responsible
  for unlocking and ending the request.
 Unmap and put previous page of userspace buffer 
  Get another pagefull of userspace buffer, and map it to kernel
  address space, and lock request
 Do as much copy tofrom userspace buffer as we can 
	
	  This is a new and locked page, it shouldn't be mapped or
	  have any special flags on it
	
	  Release while we have extra ref on stolen page.  Otherwise
	  anon_pipe_buf_release() might think the page can be reused.
 Drop ref for ap->pages[] array 
 Drop ref obtained in this function 
  Copy a page in the request tofrom the userspace buffer.  Must be
  done atomically
 Copy pages in the request tofrom userspace buffer 
 Copy a single argument in the request tofrom userspace buffer 
 Copy request arguments tofrom userspace buffer 
  Transfer an interrupt request to userspace
  Unlike other requests this is assembled on demand, without a need
  to allocate a separate fuse_req structure.
  Called with fiq->lock held, releases it
  Read a single request into the userspace filesystem's buffer.  This
  function waits until a request is available, then removes it from
  the pending list and copies request data to userspace buffer.  If
  no reply is needed (FORGET) or request has been aborted or there
  was an error during the copying then it's finished by calling
  fuse_request_end().  Otherwise add it to the processing list, and set
  the 'sent' flag.
	
	  Require sane minimum read buffer - that has capacity for fixed part
	  of any request header + negotiated max_write room for data.
	 
	  Historically libfuse reserves 4K for fixed header room, but e.g.
	  GlusterFS reserves only 80 bytes
	 
	 	= `sizeof(fuse_in_header) + sizeof(fuse_write_in)`
	 
	  which is the absolute minimum any sane filesystem should be using
	  for header room.
 If request is too large, reply with an error and restart the read 
 SETXATTR is special, since it may contain too large data 
	
	   Must not put request on fpq->io queue after having been shut down by
	   fuse_abort_conn()
 matches barrier in request_wait_answer() 
	
	  The fuse device's file's private_data is used to hold
	  the fuse_conn(ection) when it is mounted, and is used to
	  keep track of whether the file has been mounted already.
		
		  Need to be careful about this.  Having buf->ops in module
		  code can Oops if the buffer persists after module unload.
 Don't try to move pages (yet) 
 Look up request on processing list by unique ID 
  Write a single reply to a request.  First the header is copied from
  the write buffer.  The request is then searched on the processing
  list by the unique ID found in the header.  If found, then remove
  it from the list and copy the rest of the buffer to the request.
  The request is finished by calling fuse_request_end().
	
	  Zero oh.unique indicates unsolicited notification message
	  and error contains notification code.
 Is it an interrupt reply ID? 
 Abort all requests on the given list (pending or processing) 
  Abort all requests.
  Emergency exit in case of a malicious or accidental deadlock, or just a hung
  filesystem.
  The same effect is usually achievable through killing the filesystem daemon
  and all users of the filesystem.  The exception is the combination of an
  asynchronous request and the tricky deadlock (see
  Documentationfilesystemsfuse.rst).
  Aborting requests under IO goes as follows: 1: Separate out unlocked
  requests, they should be finished off immediately.  Locked requests will be
  finished after unlock; see unlock_request(). 2: Finish off the unlocked
  requests.  It is possible that some request will finish before we can.  This
  is OK, the request will in that case be removed from the list before we touch
  it.
 Background queuing checks fc->connected under bg_lock 
 matches implicit memory barrier in fuse_drop_waiting() 
 Are we the last open device? 
 No locking - fasync_helper does its own locking 
				
				  Check against file->f_op because CUSE
				  uses the same ioctl handler.
  FUSE: Filesystem in Userspace
  Copyright (C) 2001-2018  Miklos Szeredi <miklos@szeredi.hu>
  This program can be distributed under the terms of the GNU GPL.
  See the file COPYING.
	
	  Is cache already completed?  Or this entry does not go at the end of
	  cache?
 Dirent doesn't fit in current page?  Jump to next page. 
 Raced with another readdir 
 does cache end position match current position? 
 truncate unused tail of cache 
		
		  Unlike in the case of fuse_lookup, zero nodeid does not mean
		  ENOENT. Instead, it only means the userspace filesystem did
		  not want to return attributeshandle for this entry.
		 
		  So do nothing.
		
		  We could potentially refresh the attributes of the directory
		  and its parent?
		
		  The other branch comes via fuse_iget()
		  which bumps nlookup inside
 ignore errors 
			 We fill entries into dstbuf only as much as
			   it can hold. But we still continue iterating
			   over remaining entries to link them. If not,
			   we need to send a FORGET for each of those
			   which we did not link.
 derefs ->namelen 
 Seeked?  If so, reset the cache stream 
	
	  We're just about to start reading into the cache or reading the
	  cache; both cases require an up-to-date mtime value.
 Starting cache? Set cache mtime. 
	
	  When at the beginning of the directory (i.e. just after opendir(3) or
	  rewinddir(3)), then need to check whether directory contents have
	  changed, and reset the cache if so.
	
	  If cache version changed since the last getdents() call, then reset
	  the cache stream.
	
	  If at the beginning of the cache, than reset version to
	  current.
 EOF? 
		
		  Uh-oh: page gone missing, cache is useless
 Make sure it's still the same version after getting the page. 
	
	  Contents of the page are now protected against changing by holding
	  the page lock.
 We hit end of page: skip to next page. 
	
	  End of cache reached.  If found position, then we are done, otherwise
	  need to fall back to uncached, since the position we were looking for
	  wasn't in the cache.
  FUSE: Filesystem in Userspace
  Copyright (C) 2001-2008  Miklos Szeredi <miklos@szeredi.hu>
  This program can be distributed under the terms of the GNU GPL.
  See the file COPYING.
  This is non-NULL when the single instance of the control filesystem
  exists.  Protected by fuse_mutex
	
	  Get any fuse_mount belonging to this fuse_conn; s_bdi is
	  shared between all of them
 setting ->i_op to NULL is not allowed 
  Add a connection to the control filesystem (if it exists).  Caller
  must hold fuse_mutex
  Remove a connection from the control filesystem (if it exists).
  Caller must hold fuse_mutex
 Get rid of submounts: 
 SPDX-License-Identifier: GPL-2.0
  dax: direct host memory access
  Copyright (C) 2020 Red Hat, Inc.
  Default memory range size.  A power of 2 so it agrees with common FUSE_INIT
  map_alignment values 4KB and 64KB.
 Number of ranges reclaimer will try to free in one invocation 
  Dax memory reclaim threshold in percetage of total ranges. When free
  number of free ranges drops below this threshold, reclaim can trigger
  Default is 20%
 Translation information for file offsets to DAX window offsets 
 Pointer to inode where this memory range is mapped 
 Will connect in fcd->free_ranges to keep track of free memory 
 For interval tree in fileinode 
 Will connect in fc->busy_ranges to keep track busy memory 
 Position in DAX window 
 Length of mapping, in bytes 
 Is this mapping read-only or read-write 
 reference count when the mapping is used by dax iomap. 
 Per-inode dax map 
 Semaphore to protect modifications to the dmap tree 
 Sorted rb tree of struct fuse_dax_mapping elements 
 DAX device 
 Lock protecting accessess to  members of this structure 
 List of memory ranges which are busy 
 Worker to free up memory ranges 
 Wait queue for a dax range to become free 
 DAX Window Free Ranges 
 If number of free ranges are below threshold, start reclaim 
 This assumes fcd->lock is held 
 This assumes fcd->lock is held 
 Return fuse_dax_mapping to free list 
 Ask fuse daemon to setup mapping 
		
		  We don't take a reference on inode. inode is valid right now
		  and when inode is going away, cleanup logic should first
		  cleanup dmap entries.
 Protected by fi->dax->sem 
  Cleanup dmap entry and add back to free list. This should be called with
  fcd->lock held.
  Free inode dmap entries whose range falls inside [start, end].
  Does not take any locks. At this point of time it should only be
  called from evict_inode() path where we know all dmap entries can be
  reclaimed.
 inode is going away. There should not be any users of dmap 
 Nothing to remove 
  It is called from evict_inode() and by that time inode is going away. So
  this function does not take any locks like fi->dax->sem for traversing
  that fuse inode interval tree. If that lock is taken then lock validator
  complains of deadlock situation w.r.t fs_reclaim lock.
	
	  fuse_evict_inode() has already called truncate_inode_pages_final()
	  before we arrive here. So we should not have to worry about any
	  pagesexception entries still associated with inode.
 If length is beyond end of file, truncate further 
		
		  increace refcnt so that reclaim code knows this dmap is in
		  use. This assumes fi->dax->sem mutex is held either
		  sharedexclusive.
 iomap->private should be NULL 
 Mapping beyond end of file is hole 
	
	  Can't do inline reclaim in fault path. We call
	  dax_layout_busy_page() before we free a range. And
	  fuse_wait_dax_page() drops mapping->invalidate_lock and requires it.
	  In fault path we enter with mapping->invalidate_lock held and can't
	  drop it. Also in fault path we hold mapping->invalidate_lock shared
	  and not exclusive, so that creates further issues with
	  fuse_wait_dax_page().  Hence return -EAGAIN and fuse_dax_fault()
	  will wait for a memory range to become free and retry.
 If we are here, we should have memory allocated 
	
	  Take write lock so that only one caller can try to setup mapping
	  and other waits.
	
	  We dropped lock. Check again if somebody else setup
	  mapping already.
 Setup one mapping 
	
	  Take exclusive lock so that only one caller can try to setup
	  mapping and others wait.
	 We are holding either inode lock or invalidate_lock, and that should
	  ensure that dmap can't be truncated. We are holding a reference
	  on dmap and that should make sure it can't be reclaimed. So dmap
	  should still be there in tree despite the fact we dropped and
	  re-acquired the fi->dax->sem lock.
	 We took an extra reference on dmap to make sure its not reclaimd.
	  Now we hold fi->dax->sem lock and that reference is not needed
	  anymore. Drop it.
		 refcount should not hit 0. This object only goes
		  away when fuse connection goes away
	 Maybe another thread already upgraded mapping while we were not
	  holding lock.
 This is just for DAX and the mapping is ephemeral, do not use it for other
  purposes since there is no block device with a permanent mapping.
 We don't support FIEMAP 
	
	  Both readwrite and mmap path can race here. So we need something
	  to make sure if we are setting up mapping, then other path waits
	 
	  For now, use a semaphore for this. It probably needs to be
	  optimized later.
			 Upgrade read-only mapping to read-write. This will
			  require exclusive fi->dax->sem lock as we don't want
			  two threads to be trying to this simultaneously
			  for same dmap. So drop shared lock and acquire
			  exclusive lock.
			 
			  Before dropping fi->dax->sem lock, take reference
			  on dmap so that its not freed by range reclaim.
	
	  If read beyond end of file happens, fs code seems to return
	  it as hole
			 refcount should not hit 0. This object only goes
			  away when fuse connection goes away
	 DAX writes beyond end-of-file aren't handled using iomap, so the
	  file size is unchanged and there is nothing to do here.
 Should be called with mapping->invalidate_lock held exclusively 
 dmap_end == 0 leads to unmapping of whole file 
 TODO file_accessed(iocb->f_filp) 
 TODO file_update_time() but we don't want metadata IO 
	 Do not use dax for file extending writes as write and on
	  disk i_size increase are not atomic otherwise.
	
	  We need to serialize against not only truncate but also against
	  fuse dax memory range reclaim. While a range is being reclaimed,
	  we do not want any readwritemmap to make progress and try
	  to populate page cache or access memory we are trying to free.
	
	  igrab() was done to make sure inode won't go under us, and this
	  further avoids the race with evict().
 Remove dax mapping from inode interval tree now 
	 It is possible that umountshutdown has killed the fuse connection
	  and worker thread is trying to reclaim memory in parallel.  Don't
	  warn in that case.
 Find first mapped dmap for an inode and return file offset. Caller needs
  to hold fi->dax->sem lock either shared or exclusive.
 still in use. 
  Find first mapping in the tree and free it and return it. Do not add
  it back to free pool.
 Lookup a dmap and corresponding file offset to reclaim. 
	
	  Make sure there are no references to inode pages using
	  get_user_pages()
 Range already got reclaimed by somebody else 
 still in use. 
 Clean up dmap. Do not add back to free list 
		
		  Either we got a mapping or it is an error, return in both
		  the cases.
		 If we could not reclaim a mapping because it
		  had a reference or some other temporary failure,
		  Try again. We want to give up inline reclaim only
		  if there is no range assigned to this node. Otherwise
		  if a deadlock is possible if we sleep with
		  mapping->invalidate_lock held and worker to free memory
		  can't make progress due to unavailability of
		  mapping->invalidate_lock.  So sleep only if fi->dax->nr=0
		
		  There are no mappings which can be reclaimed. Wait for one.
		  We are not holding fi->dax->sem. So it is possible
		  that range gets added now. But as we are not holding
		  mapping->invalidate_lock, worker should still be able to
		  free up a range and wake us up.
 Find fuse dax mapping at file offset inode. 
 Range already got cleaned up by somebody else 
 still in use. 
 Cleanup dmap entry and add back to free list 
  Free a range of memory.
  Locking:
  1. Take mapping->invalidate_lock to block dax faults.
  2. Take fi->dax->sem to protect interval tree and also to make sure
     readwrite can not reuse a dmap which we might be freeing.
 Pick first busy range and free it for now
 skip this range if it's in use. 
			
			  This inode is going away. That will free
			  up all the ranges anyway, continue to
			  next range.
			
			  Take this element off list and add it tail. If
			  this element can't be freed, it will help with
			  selecting new element in next iteration of loop.
 If number of free ranges are still below threshold, requeue 
 Free All allocated elements 
		 TODO: This offset only works if virtio-fs driver is not
		  having some memory hidden at the beginning. This needs
		  better handling
 Free All allocated elements 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  All Rights Reserved.
  This structure is used during recovery to record the buf log items which
  have been canceled and should not be replayed.
	
	  If we find an existing cancel record, this indicates that the buffer
	  was cancelled multiple times.  To ensure that during pass 2 we keep
	  the record in the table until we reach its last occurrence in the
	  log, a reference count is kept to tell how many times we expect to
	  see this record during the second pass.
  Check if there is and entry for blkno, len in the buffer cancel record table.
  Check if there is and entry for blkno, len in the buffer cancel record table,
  and decremented the reference count on it if there is one.
  Remove the cancel record once the refcount hits zero, so that if the same
  buffer is re-used again after its last cancellation we actually replay the
  changes made at that point.
 log buffer item recovery 
  Sort buffer items for log recovery.  Most buffer items should end up on the
  buffer list and are recovered first, with the following exceptions:
  1. XFS_BLF_CANCEL buffers must be processed last because some log items
     might depend on the incor ecancellation record, and replaying a cancelled
     buffer item can remove the incore record.
  2. XFS_BLF_INODE_BUF buffers are handled after most regular items so that
     we replay di_next_unlinked only after flushing the inode 'free' state
     to the inode buffer.
  See xlog_recover_reorder_trans for more details.
  Build up the table of buf cancel records so that we don't replay cancelled
  data in the second pass.
  Validate the recovered buffer is of the correct type and attach the
  appropriate buffer operations to them for writeback. Magic numbers are in a
  few places:
 	the first 16 bits of the buffer (inode buffer, dquot buffer),
 	the first 32 bits of the buffer (most blocks),
 	inside a struct xfs_da_blkinfo at the start of the buffer.
	
	  We can only do post recovery validation on items on CRC enabled
	  fielsystems as we need to know when the buffer was written to be able
	  to determine if we should have replayed the item. If we replay old
	  metadata over a newer buffer, then it will enter a temporarily
	  inconsistent state resulting in verification failures. Hence for now
	  just avoid the verification stage for non-crc filesystems
 no magic numbers for verification of RT buffers 
 CONFIG_XFS_RT 
	
	  Nothing else to do in the case of a NULL current LSN as this means
	  the buffer is more recent than the change in the log and will be
	  skipped.
	
	  We must update the metadata LSN of the buffer as it is written out to
	  ensure that older transactions never replay over this one and corrupt
	  the buffer. This can occur if log recovery is interrupted at some
	  point after the current transaction completes, at which point a
	  subsequent mount starts recovery from the beginning.
	 
	  Write verifiers update the metadata LSN from log items attached to
	  the buffer. Therefore, initialize a bli purely to carry the LSN to
	  the verifier.
  Perform a 'normal' buffer recovery.  Each logged region of the
  buffer should be copied over the corresponding region in the
  given buffer.  The bitmap in the buf log format structure indicates
  where to place the logged data.
 0 is the buf format structure 
		
		  The dirty regions logged in the buffer, even though
		  contiguous, may span multiple chunks. This is because the
		  dirty region may span a physical page boundary in a buffer
		  and hence be split into two separate vectors for writing into
		  the log. Hence we need to trim nbits back to the length of
		  the current region being copied out of the log.
		
		  Do a sanity check if this is a dquot buffer. Just checking
		  the first dquot in the buffer should do. XXXThis is
		  probably a good thing to do for other buf types also.
 dest 
 source 
 length 
 Shouldn't be any more regions 
  Perform a dquot buffer recovery.
  Simple algorithm: if we have found a QUOTAOFF log item of the same type
  (ie. USR or GRP), then just toss this buffer away; don't recover it.
  Else, treat it as a regular buffer and do recovery.
  Return false if the buffer was tossed and true if we recovered the buffer to
  indicate to the caller if the buffer needs writing.
	
	  Filesystems are required to send in quota flags at mount time.
	
	  This type of quotas was turned off, so ignore this buffer
  Perform recovery for a buffer full of inodes.  In these buffers, the only
  data which should be recovered is that which corresponds to the
  di_next_unlinked pointers in the on disk inode structures.  The rest of the
  data for the inodes is always logged through the inodes themselves rather
  than the inode buffer and is recovered in xlog_recover_inode_pass2().
  The only time when buffers full of inodes are fully recovered is when the
  buffer is full of newly allocated inodes.  In this case the buffer will
  not be marked as an inode buffer and so will be sent to
  xlog_recover_do_reg_buffer() below during recovery.
	
	  Post recovery validation only works properly on CRC enabled
	  filesystems.
			
			  The next di_next_unlinked field is beyond
			  the current logged region.  Find the next
			  logged region that contains or is beyond
			  the current di_next_unlinked field.
			
			  If there are no more logged regions in the
			  buffer, then we're done.
		
		  If the current logged region starts after the current
		  di_next_unlinked field, then move on to the next
		  di_next_unlinked field.
		
		  The current logged region contains a copy of the
		  current di_next_unlinked field.  Extract its value
		  and copy it to the buffer copy.
		
		  If necessary, recalculate the CRC in the on-disk inode. We
		  have to leave the inode in a consistent state for whoever
		  reads it next....
  V5 filesystems know the age of the buffer on disk being recovered. We can
  have newer objects on disk than we are replaying, and so for these cases we
  don't want to replay the current change as that will make the buffer contents
  temporarily invalid on disk.
  The magic number might not match the buffer type we are going to recover
  (e.g. reallocated blocks), so we ignore the xfs_buf_log_format flags.  Hence
  extract the LSN of the existing object in the buffer based on it's current
  magic number.  If we don't recognise the magic number in the buffer, then
  return a LSN of -1 so that the caller knows it was an unrecognised block and
  so can recover the buffer.
  Note: we cannot rely solely on magic number matches to determine that the
  buffer has a valid LSN - we also need to verify that it belongs to this
  filesystem, so we need to extract the object's LSN and compare it to that
  which we read from the superblock. If the UUIDs don't match, then we've got a
  stale metadata block from an old filesystem instance that we need to recover
  over the top of.
 v4 filesystems always recover immediately 
	
	  realtime bitmap and summary file blocks do not have magic numbers or
	  UUIDs, so we must recover them immediately.
		
		  Remote attr blocks are written synchronously, rather than
		  being logged. That means they do not contain a valid LSN
		  (i.e. transactionally ordered) in them, and hence any time we
		  see a buffer to replay over the top of a remote attribute
		  block we should simply do so.
		
		  superblock uuids are magic. We may or may not have a
		  sb_meta_uuid on disk, but it will be set in the in-core
		  superblock. We set the uuid pointer for verification
		  according to the superblock feature mask to ensure we check
		  the relevant UUID in the superblock.
	
	  We do individual object checks on dquot and inode buffers as they
	  have their own individual LSN records. Also, we could have a stale
	  buffer here, so we have to at least recognise these buffer types.
	 
	  A notd complexity here is inode unlinked list processing - it logs
	  the inode directly in the buffer, but we don't know which inodes have
	  been modified, and there is no global buffer LSN. Hence we need to
	  recover all inode buffer types immediately. This problem will be
	  fixed by logical logging of the unlinked list modifications.
 unknown buffer contents, recover immediately 
  This routine replays a modification made to a buffer at runtime.
  There are actually two types of buffer, regular and inode, which
  are handled differently.  Inode buffers are handled differently
  in that we only recover a specific set of data from them, namely
  the inode di_next_unlinked fields.  This is because all other inode
  data is actually logged via inode records and any data we replay
  here which overlaps that may be stale.
  When meta-data buffers are freed at run time we log a buffer item
  with the XFS_BLF_CANCEL bit set to indicate that previous copies
  of the buffer in the log should not be replayed at recovery time.
  This is so that if the blocks covered by the buffer are reused for
  file data before we crash we don't end up replaying old, freed
  meta-data into a user's file.
  To handle the cancellation of buffer log items, we make two passes
  over the log during recovery.  During the first we build a table of
  those buffers which have been cancelled, and during the second we
  only replay those buffers which do not have corresponding cancel
  records in the table.  See xlog_recover_buf_pass[1,2] above
  for more details on the implementation of the table of cancel records.
	
	  In this pass we only want to recover all the buffers which have
	  not been cancelled and are not cancellation buffers themselves.
	
	  Recover the buffer only if we get an LSN from it and it's less than
	  the lsn of the transaction we are replaying.
	 
	  Note that we have to be extremely careful of readahead here.
	  Readahead does not attach verfiers to the buffers so if we don't
	  actually do any replay after readahead because of the LSN we found
	  in the buffer if more recent than that current transaction then we
	  need to attach the verifier directly. Failure to do so can lead to
	  future recovery actions (e.g. EFI and unlinked list recovery) can
	  operate on the buffers and they won't get the verifier attached. This
	  can lead to blocks on disk having the correct content but a stale
	  CRC.
	 
	  It is safe to assume these clean buffers are currently up to date.
	  If the buffer is dirtied by a later transaction being replayed, then
	  the verifier will be reset to match whatever recover turns that
	  buffer into.
	
	  Perform delayed write on the buffer.  Asynchronous writes will be
	  slower when taking into account all the buffers to be flushed.
	 
	  Also make sure that only inode buffers with good sizes stay in
	  the buffer cache.  The kernel moves inodes in buffers of 1 block
	  or inode_cluster_size bytes, whichever is bigger.  The inode
	  buffers in the log can be a different size if the log was generated
	  by an older kernel using unclustered inode buffers or a newer kernel
	  running with a different inode cluster size.  Regardless, if
	  the inode buffer size isn't max(blocksize, inode_cluster_size)
	  for our value of inode_cluster_size, then we need to keep
	  the buffer out of the buffer cache so that the buffer won't
	  overlap with future reads of those inodes.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Tunable XFS parameters.  xfs_params is required even when CONFIG_SYSCTL=n,
  other XFS code uses these values.  Times are measured in centisecs (i.e.
  100ths of a second) with the exception of blockgc_timer, which is measured
  in seconds.
	MIN		DFLT		MAX	
 no delay by default 
 no delay by default 
 assert failures BUG() 
 assert failures WARN() 
 automatic thread detection 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2001,2005 Silicon Graphics, Inc.
  All Rights Reserved.
	
	  To be able to use error injection anywhere, we need to ensure error
	  injection mechanism is already initialized.
	 
	  Code paths like IO completion can be called before the
	  initialization is complete, but be able to inject errors in such
	  places is still useful.
 DEBUG 
  Complain about the kinds of metadata corruption that we can't detect from a
  verifier, such as incorrect inter-block relationship data.  Does not set
  bp->b_error.
  Call xfs_buf_mark_corrupt, not this function.
  Warnings specifically for verifier errors.  Differentiate CRC vs. invalid
  values, and omit the stack trace unless the error level is tuned high.
  Warnings specifically for verifier errors.  Differentiate CRC vs. invalid
  values, and omit the stack trace unless the error level is tuned high.
  Warnings for inode corruption problems.  Don't bother with the stack
  trace unless the error level is turned up high.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2008-2010, 2013 Dave Chinner
  All Rights Reserved.
 inode create item 
  This returns the number of iovecs needed to log the given inode item.
  We only need one iovec for the icreate log structure.
  This is called to fill in the vector of log iovecs for the
  given inode create log item.
  Initialize the inode log item for a newly allocated (in-core) inode.
  Inode extents can only reside within an AG. Hence specify the starting
  block for the inode chunk by offset within an AG as well as the
  length of the allocated extent.
  This joins the item to the transaction and marks it dirty so
  that we don't need a separate call to do this, nor does the
  caller need to know anything about the icreate item.
 single vector 
	
	  Inode allocation buffers must be replayed before subsequent inode
	  items try to modify those buffers.  ICREATE items are the logical
	  equivalent of logging a newly initialized inode buffer, so recover
	  these at the same time that we recover logged buffers.
  This routine is called when an inode create format structure is found in a
  committed transaction in the log.  It's purpose is to initialise the inodes
  being allocated on disk. This requires us to get inode cluster buffers that
  match the range to be initialised, stamped with inode templates and written
  by delayed write so that subsequent modifications will hit the cached buffer
  and only need writing out at the end of recovery.
	
	  The inode chunk is either full or sparse and we only support
	  m_ino_geo.ialloc_min_blks sized sparse allocations at this time.
 verify inode count is consistent with extent length 
	
	  The icreate transaction can cover multiple cluster buffers and these
	  buffers could have been freed and reused. Check the individual
	  buffers for cancellation so we don't overwrite anything written after
	  a cancellation.
	
	  We currently only use icreate for a single allocation at a time. This
	  means we should expect either all or none of the buffers to be
	  cancelled. Be conservative and skip replay if at least one buffer is
	  cancelled, but warn the user that something is awry if the buffers
	  are not consistent.
	 
	  XXX: This must be refined to only skip cancelled clusters once we use
	  icreate for multiple chunk allocations.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  Copyright (c) 2012 Red Hat, Inc.
  All Rights Reserved.
 Kernel only BMAP related definitions and functions 
  Convert the given file system block to a disk block.  We have to treat it
  differently based on whether the file is a real time file or not, because the
  bmap code does.
  Routine to zero an extent on disk allocated to the specific inode.
  The VFS functions take a linearised filesystem block offset, so we have to
  convert the sparse xfs fsb to the right format first.
  VFS types are real funky, too.
 product factor for allocators 
 product factor for allocators 
 realtime allocation length 
 minimum allocation alignment 
	
	  If we shifted the file offset downward to satisfy an extent size
	  hint, increase minlen by that amount so that the allocator won't
	  give us an allocation that's too short to cover at least one of the
	  blocks that the caller asked for.
	
	  If the offset & length are not perfectly aligned
	  then kill prod, it will just get us in trouble.
	
	  Set ralen to be the actual requested length in rtextents.
	
	  If the old value was close enough to MAXEXTLEN that
	  we rounded up to it, cut it back so it's valid again.
	  Note that if it's a really large request (bigger than
	  MAXEXTLEN), we don't hear about that number, and can't
	  adjust the starting point to match it.
	
	  Lock out modifications to both the RT bitmap and summary inodes
	
	  If it's an allocation to an empty file at offset 0,
	  pick an extent that will space things out in the rt area.
 realtime extent no 
	
	  Realtime allocation, done through xfs_rtallocate_extent.
		
		  Adjust the disk quota also. This was reserved
		  earlier.
		
		  We previously enlarged the request length to try to satisfy
		  an extent size hint.  The allocator didn't return anything,
		  so reset the parameters to the original values and try again
		  without alignment criteria.
		
		  If we can't allocate near a specific rt extent, try again
		  without locality criteria.
 CONFIG_XFS_RT 
  Extent tree block counting routines.
  Count leaf blocks given a range of extent records.  Delayed allocation
  extents are not counted towards the totals.
  Count fsblocks of the given fork.  Delayed allocation extents are
  not counted towards the totals.
		
		  xfs_btree_count_blocks includes the root block contained in
		  the inode fork in @btblocks, so subtract one because we're
		  only interested in allocated disk blocks.
		
		  Delalloc extents that start beyond EOF can occur due to
		  speculative EOF allocation when the delalloc extent is larger
		  than the largest freespace extent at conversion time.  These
		  extents cannot be converted by data writeback, so can exist
		  here even if we are not supposed to be finding delalloc
		  extents.
  Get inode's extents as described in bmv, and format for output.
  Calls formatter to fill the user's buffer until all extents
  are mapped, until the passed-in bmv->bmv_count slots have
  been filled, or until the formatter short-circuits the loop,
  if it is tracking filled-in extents on its own.
 error code 
 user bmap structure 
 Only allow CoW fork queries if we're debugging. 
 No CoW fork? Just return 
			
			  Even after flushing the inode, there can still be
			  delalloc blocks on the inode beyond EOF due to
			  speculative preallocation.  These are not removed
			  until the release function is called or the inode
			  is inactivated.  Hence we cannot assert here that
			  ip->i_delayed_blks == 0.
 Local format inode forks report no extents. 
		
		  Report a whole-file hole if the delalloc flag is set to
		  stay compatible with the old implementation.
		
		  Report an entry for a hole if this extent doesn't directly
		  follow the previous one.
		
		  In order to report shared extents accurately, we report each
		  distinct shared  unshared part of a single bmbt record with
		  an individual getbmapx record.
  Dead simple method of punching delalyed allocation blocks from a range in
  the inode.  This will always punch out both the start and end blocks, even
  if the ranges only partially overlap them, so it is up to the caller to
  ensure that partial blocks are not passed in.
		
		  A delete can push the cursor forward. Step back to the
		  previous extent on non-delalloc or extents outside the
		  target range.
  Test whether it is appropriate to check an inode for and free post EOF
  blocks. The 'force' parameter determines whether we should also consider
  regular files that are marked preallocated or append-only.
	
	  Caller must either hold the exclusive io lock; or be inactivating
	  the inode, which guarantees there are no other users of the inode.
 preallocdelalloc exists only on regular files 
	
	  Zero sized files with no cached pages and delalloc blocks will not
	  have speculative preallocdelalloc blocks to remove.
 If we haven't read in the extent list, then don't do it now. 
	
	  Do not free real preallocated or append-only files unless the file
	  has delalloc blocks and we are forced to remove them.
	
	  Do not try to free post-EOF blocks if EOF is beyond the end of the
	  range supported by the page cache, because the truncation will loop
	  forever.
	
	  Look up the mapping for the first block past EOF.  If we can't find
	  it, there's nothing to free.
	
	  If there's a real mapping there or there are delayed allocation
	  reservations, then we have post-EOF blocks to try to free.
  This is called to free any blocks beyond eof. The caller must hold
  IOLOCK_EXCL unless we are in the inode reclaim path and have the only
  reference to the inode.
 Attach the dquots to the inode up front. 
 Wait on dio to ensure i_size has settled. 
	
	  Do not update the on-disk file size.  If we update the on-disk file
	  size and then the system crashes before the contents of the file are
	  flushed to disk then the files may be full of holes (ie NULL files
	  bug).
	
	  If we get an error at this point we simply don't
	  bother truncating the file.
	
	  Allocate file space until done or until there is an error
		
		  Determine space reservations for datarealtime.
		
		  The transaction reservation is limited to a 32-bit block
		  count, hence we need to limit the number of blocks we are
		  trying to reserve to avoid an overflow. We can't allocate
		  more than @nimaps extents, and an extent is limited on disk
		  to MAXEXTLEN (21 bits), so use that to enforce the limit.
		
		  Allocate and setup the transaction.
		
		  Complete the transaction
 Caller must first wait for the completion of any pending DIOs if required. 
 if nothing being freed 
 We can only free complete realtime extents. 
	
	  Need to zero the stuff we're not freeing, on disk.
	
	  Now that we've unmap all full blocks we'll have to zero out any
	  partial block at the beginning andor end.  iomap_zero_range is smart
	  enough to skip any holes, including those we just created, but we
	  must take care not to zero beyond EOF and enlarge i_size.
	
	  If we zeroed right up to EOF and EOF straddles a page boundary we
	  must make sure that the post-EOF area is also zeroed because the
	  page could be mmap'd and iomap_zero_range doesn't do that for us.
	  Writeback of the eof page will do this, albeit clumsily.
	
	  Trim eofblocks to avoid shifting uninitialized post-eof preallocation
	  into the accessible region of the file.
	
	  Shift operations must stabilize the start block offset boundary along
	  with the full range of the operation. If we don't, a COW writeback
	  completion could race with an insert, front merge with the start
	  extent (after split) during the shift and corrupt the file. Start
	  with the block just prior to the start to stabilize the boundary.
	
	  Writeback and invalidate cache for the remainder of the file as we're
	  about to shift down every extent from offset to EOF.
	
	  Clean out anything hanging around in the cow fork now that
	  we've flushed all the dirty data out to disk to avoid having
	  CoW extents at the wrong offsets.
  xfs_collapse_file_space()
 	This routine frees disk space and shift extent for the given file.
 	The first thing we do is to free data blocks in the specified range
 	by calling xfs_free_file_space(). It would also sync dirty data
 	and invalidate page cache over the region on which collapse range
 	is working. And Shift extent records to the left to cover a hole.
  RETURNS:
 	0 on success
 	errno on error
 finish any deferred frees and roll the transaction 
  xfs_insert_file_space()
 	This routine create hole space by shifting extents for the given file.
 	The first thing we do is to sync dirty data and invalidate page cache
 	over the region on which insert range is working. And split an extent
 	to two extents at given offset by calling xfs_bmap_split_extent.
 	And shift all extent records which are laying between [offset,
 	last allocated extent] to the right to reserve hole range.
  RETURNS:
 	0 on success
 	errno on error
	
	  The extent shifting code works on extent granularity. So, if stop_fsb
	  is not the starting block of extent, we need to split the extent at
	  stop_fsb.
  We need to check that the format of the data fork in the temporary inode is
  valid for the target inode before doing the swap. This is not a problem with
  attr1 because of the fixed fork offset, but attr2 has a dynamically sized
  data fork depending on the space the attribute fork is taking so we can get
  invalid formats on the target inode.
  E.g. target has space for 7 extents in extent format, temp inode only has
  space for 6.  If we defragment down to 7 extents, then the tmp format is a
  btree, but when swapped it needs to be in extent format. Hence we can't just
  blindly swap data forks on attr2 filesystems.
  Note that we check the swap in both directions so that we don't end up with
  a corrupt temporary inode, either.
  Note that fixing the way xfs_fsr sets up the attribute fork in the source
  inode will prevent this situation from occurring, so all we do here is
  reject and log the attempt. basically we are putting the responsibility on
  userspace to get this right.
 target inode 
 tmp inode 
 Usergroupproject quota ids must match if quotas are enforced. 
 Should never get a local format 
	
	  if the target inode has less extents that then temporary inode then
	  why did userspace call us?
	
	  If we have to use the (expensive) rmap swap method, we can
	  handle any number of extents and any format.
	
	  if the target inode is in extent form and the temp inode is in btree
	  form then we will end up with the target inode in the wrong format
	  as we already know there are less extents in the temp inode.
 Check temp in extent form to max in target 
 Check target in extent form to max in temp 
	
	  If we are in a btree format, check that the temp root block will fit
	  in the target and that it has enough extents to be in btree format
	  in the target.
	 
	  Note that we have to be careful to allow btree->extent conversions
	  (a common defrag case) which will occur when the temp inode is in
	  extent format...
 Reciprocal target->temp btree format checks 
 Verify O_DIRECT for ftmp 
  Move extents from one file to another, when rmap is enabled.
	
	  If the source file has shared blocks, we must flag the donor
	  file as having shared blocks so that we get the shared-block
	  rmap functions when we go to fix up the rmaps.  The flags
	  will be switch for reals later.
 Read extent from the donor file 
 Unmap the old blocks in the source file. 
 Read extent from the source file 
 Trim the extent. 
 Remove the mapping from the donor file. 
 Remove the mapping from the source file. 
 Map the donor file's blocks into the source file. 
 Map the source file's blocks into the donor file. 
 Roll on... 
 Swap the extents of two files by swapping data forks. 
	
	  Count the number of extended attribute blocks
	
	  Btree format (v3) inodes have the inode number stamped in the bmbt
	  block headers. We can't start changing the bmbt blocks until the
	  inode owner change is logged so recovery does the right thing in the
	  event of a crash. Set the owner change log flags now and leave the
	  bmbt scan as the last step.
	
	  Swap the data forks of the inodes
	
	  Fix the on-disk inode values
	
	  The extents in the source inode could still contain speculative
	  preallocation beyond EOF (e.g. the file is open but not modified
	  while defrag is in progress). In that case, we need to copy over the
	  number of delalloc blocks the data fork in the source inode is
	  tracking beyond EOF so that when the fork is truncated away when the
	  temporary inode is unlinked we don't underrun the i_delayed_blks
	  counter on that inode.
  Fix up the owners of the bmbt blocks to refer to the current inode. The
  change owner scan attempts to order all modified buffers in the current
  transaction. In the event of ordered buffer failure, the offending buffer is
  physically logged as a fallback and the scan returns -EAGAIN. We must roll
  the transaction in this case to replenish the fallback log reservation and
  restart the scan. This process repeats until the scan completes.
 success or fatal error 
		
		  Redirty both inodes so they can relog and keep the log tail
		  moving forward.
 target inode 
 tmp inode 
	
	  Lock the inodes against other IO, page faults and truncate to
	  begin with.  Then we can ensure the inodes are flushed and have no
	  page cache safely. Once we have done this we can take the ilocks and
	  do the rest of the checks.
 Verify that both files have the same format 
 Verify both files are either real-time or non-realtime 
	
	  Extent "swapping" with rmap requires a permanent reservation and
	  a block reservation because it's really just a remap operation
	  performed with log redo items!
		
		  Conceptually this shouldn't affect the shape of either bmbt,
		  but since we atomically move extents one by one, we reserve
		  enough space to rebuild both trees.
		
		  If either inode straddles a bmapbt block allocation boundary,
		  the rmapbt algorithm triggers repeated allocs and frees as
		  extents are remapped. This can exhaust the block reservation
		  prematurely and cause shutdown. Return freed blocks to the
		  transaction reservation to counter this behavior.
	
	  Lock and join the inodes to the tansaction so that transaction commit
	  or cancel will unlock the inodes from this point onwards.
 Verify all data are being swapped 
 check inode formats now that data is flushed 
	
	  Compare the current change & modify times with that
	  passed in.  If they differ, we abort this swap.
	  This is the mechanism used to ensure the calling
	  process that the file was not changed out from
	  under it.
	
	  Note the trickiness in setting the log flags - we set the owner log
	  flag on the opposite inode (i.e. the inode we are setting the new
	  owner to be) because once we swap the forks and log that, log
	  recovery is going to see the fork as owned by the swapped inode,
	  not the pre-swapped inodes.
 Do we have to swap reflink flags? 
 Swap the cow forks. 
	
	  The extent forks have been swapped, but crc=1,rmapbt=0 filesystems
	  have inode number owner values in the bmbt blocks that still refer to
	  the old inode. Scan each bmbt to fix up the owner values with the
	  inode number of the current inode.
	
	  If this is a synchronous mount, make sure that the
	  transaction goes to disk before returning to the user.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2002 Silicon Graphics, Inc.
  All Rights Reserved.
  Add the locked dquot to the transaction.
  The dquot must be locked, and it cannot be associated with any
  transaction.
	
	  Get a log_item_desc to point at the new item.
  This is called to mark the dquot as needing
  to be logged when the transaction is committed.  The dquot must
  already be associated with the given transaction.
  Note that it marks the entire transaction as dirty. In the ordinary
  case, this gets called via xfs_trans_commit, after the transaction
  is already dirty. However, there's nothing stop this from getting
  called directly, as done by xfs_qm_scall_setqlim. Hence, the TRANS_DIRTY
  flag.
 Upgrade the dquot to bigtime format if possible. 
  Carry forward whatever is left of the quota blk reservation to
  the spanky new transaction
			
			  Transfer whatever is left of the reservations.
  Wrap around mod_dquot to account for both user and group quotas.
  Make the changes in the transaction structure.
  The moral equivalent to xfs_trans_mod_sb().
  We don't touch any fields in the dquot, so we don't care
  if it's locked or not (most of the time it won't be).
	
	  Find either the first free slot or the slot that belongs
	  to this dquot.
 regular disk blk reservation 
 inode reservation 
 disk blocks used. 
 Inode Count 
 rtblk reservation 
 rtblk count 
  Given an array of dqtrx structures, lock all the dquots associated and join
  them to the transaction, provided they have been modified.  We know that the
  highest number of dquots of one type - usr, grp and prj - involved in a
  transaction is 3 so we don't need to make this very generic.
 Apply dqtrx changes to the quota reservation counters. 
		
		  Subtle math here: If reserved > res_used (the normal case),
		  we're simply subtracting the unused transaction quota
		  reservation from the dquot reservation.
		 
		  If, however, res_used > reserved, then we have allocated
		  more quota blocks than were reserved for the transaction.
		  We must add that excess to the dquot reservation since it
		  tracks (usage + resv) and by definition we didn't reserve
		  that excess.
		
		  These blks were never reserved, either inside a transaction
		  or outside one (in a delayed allocation). Also, this isn't
		  always a negative number since we sometimes deliberately
		  skip quota reservations.
  Called by xfs_trans_commit() and similar in spirit to
  xfs_trans_apply_sb_deltas().
  Go thru all the dquots belonging to this transaction and modify the
  INCORE dquot to reflect the actual usages.
  Unreserve just the reservations done by this transaction.
  dquot is still left locked at exit.
		
		  Lock all of the dquots and join them to the transaction.
			
			  The array of dquots is filled
			  sequentially, not sparsely.
			
			  adjust the actual number of blocks used
			
			  The issue here is - sometimes we don't make a blkquota
			  reservation intentionally to be fair to users
			  (when the amount is small). On the other hand,
			  delayed allocs do make reservations, but that's
			  outside of a transaction, so we have no
			  idea how much was really reserved.
			  So, here we've accumulated delayed allocation blks and
			  non-delay blks. The assumption is that the
			  delayed ones are always reserved (outside of a
			  transaction), and the others may or may not have
			  quota reservations.
			
			  Get any default limits in use.
			  Startreset the timer(s) if needed.
			
			  add this to the list of items to get logged
			
			  Take off what's left of the original reservation.
			  In case of delayed allocations, there's no
			  reservation that a transaction structure knows of.
			
			  Adjust the RT reservation.
			
			  Adjust the inode reservation.
  Release the reservations, and adjust the dquots accordingly.
  This is called only when the transaction is being aborted. If by
  any chance we have done dquot modifications incore (ie. deltas) already,
  we simply throw those away, since that's the expected behavior
  when a transaction is curtailed without a commit.
			
			  We assume that the array of dquots is filled
			  sequentially, not sparsely.
			
			  Unreserve the original reservation. We don't care
			  about the number of blocks used field, or deltas.
			  Also we don't bother to zero the fields.
  Decide if we can make an additional reservation against a quota resource.
  Returns an inode QUOTA_NL_ warning code and whether or not it's fatal.
  Note that we assume that the numeric difference between the inode and block
  warning codes will always be 3 since it's userspace ABI now, and will never
  decrease the quota reservation, so the BELOW messages are irrelevant.
  This reserves disk blocks and inodes against a dquot.
  Flags indicate if the dquot is to be locked here and also
  if the blk reservation is for RT or regular blocks.
  Sending in XFS_QMOPT_FORCE_RES flag skips the quota check.
		
		  dquot is locked already. See if we'd go over the hardlimit
		  or exceed the timelimit if we'd reserve resources.
			
			  Quota block warning codes are 3 more than the inode
			  codes, which we check above.
	
	  Change the reservation, but not the actual usage.
	  Note that q_blk.reserved = q_blk.count + resv
	
	  note the reservation amt in the trans struct too,
	  so that the transaction knows how much was reserved by
	  it against this particular dquot.
	  We don't do this when we are reserving for a delayed allocation,
	  because we don't have the luxury of a transaction envelope then.
  Given dquot(s), make disk block andor inode reservations against them.
  The fact that this does the reservation against user, group and
  project quotas is important, because this follows a all-or-nothing
  approach.
  flags = XFS_QMOPT_FORCE_RES evades limit enforcement. Used by chown.
 	   XFS_QMOPT_ENOSPC returns ENOSPC not EDQUOT.  Used by pquota.
 	   XFS_TRANS_DQ_RES_BLKS reserves regular disk blocks
 	   XFS_TRANS_DQ_RES_RTBLKS reserves realtime disk blocks
  dquots are unlocked on return, if they were not locked by caller.
	
	  Didn't change anything critical, so, no need to log
  Lock the dquot and change the reservation if we can.
  This doesn't change the actual usage, just the reservation.
  The inode sent in is locked.
 Reserve data device quota against the inode's dquots. 
 Do the same but for realtime blocks. 
 Change the quota reservations for an inode creation activity. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2006-2007 Silicon Graphics, Inc.
  All Rights Reserved.
  The MRU Cache data structure consists of a data store, an array of lists and
  a lock to protect its internal state.  At initialisation time, the client
  supplies an element lifetime in milliseconds and a group count, as well as a
  function pointer to call when deleting elements.  A data structure for
  queueing up work in the form of timed callbacks is also included.
  The group count controls how many lists are created, and thereby how finely
  the elements are grouped in time.  When reaping occurs, all the elements in
  all the lists whose time has expired are deleted.
  To give an example of how this works in practice, consider a client that
  initialises an MRU Cache with a lifetime of ten seconds and a group count of
  five.  Five internal lists will be created, each representing a two second
  period in time.  When the first element is added, time zero for the data
  structure is initialised to the current time.
  All the elements added in the first two seconds are appended to the first
  list.  Elements added in the third second go into the second list, and so on.
  If an element is accessed at any point, it is removed from its list and
  inserted at the head of the current most-recently-used list.
  The reaper function will have nothing to do until at least twelve seconds
  have elapsed since the first element was added.  The reason for this is that
  if it were called at t=11s, there could be elements in the first list that
  have only been inactive for nine seconds, so it still does nothing.  If it is
  called anywhere between t=12 and t=14 seconds, it will delete all the
  elements that remain in the first list.  It's therefore possible for elements
  to remain in the data store even after they've been inactive for up to
  (t + tg) seconds, where t is the inactive element lifetime and g is the
  number of groups.
  The above example assumes that the reaper function gets called at least once
  every (tg) seconds.  If it is called less frequently, unused elements will
  accumulate in the reap list until the reaper function is eventually called.
  The current implementation uses work queue callbacks to carefully time the
  reaper function calls, so this should happen rarely, if at all.
  From a design perspective, the primary reason for the choice of a list array
  representing discrete time intervals is that it's only practical to reap
  expired elements in groups of some appreciable size.  This automatically
  introduces a granularity to element lifetimes, so there's no point storing an
  individual timeout with each element that specifies a more precise reap time.
  The bonus is a saving of sizeof(long) bytes of memory per element stored.
  The elements could have been stored in just one list, but an array of
  counters or pointers would need to be maintained to allow them to be divided
  up into discrete time groups.  More critically, the process of touching or
  removing an element would involve walking large portions of the entire list,
  which would have a detrimental effect on performance.  The additional memory
  requirement for the array of list heads is minimal.
  When an element is touched or deleted, it needs to be removed from its
  current list.  Doubly linked lists are used to make the list maintenance
  portion of these operations O(1).  Since reaper timing can be imprecise,
  inserts and lookups can occur when there are no free lists available.  When
  this happens, all the elements on the LRU list need to be migrated to the end
  of the reap list.  To keep the list maintenance portion of these operations
  O(1) also, list tails need to be accessible without walking the entire list.
  This is the reason why doubly linked list heads are used.
  An MRU Cache is a dynamic data structure that stores its elements in a way
  that allows efficient lookups, but also groups them into discrete time
  intervals based on insertion time.  This allows elements to be efficiently
  and automatically reaped after a fixed period of inactivity.
  When a client data pointer is stored in the MRU Cache it needs to be added to
  both the data store and to one of the lists.  It must also be possible to
  access each of these entries via the other, i.e. to:
     a) Walk a list, removing the corresponding data store entry for each item.
     b) Look up a data store entry, then access its list entry directly.
  To achieve both of these goals, each entry must contain both a list entry and
  a key, in addition to the user's data pointer.  Note that it's not a good
  idea to have the client embed one of these structures at the top of their own
  data structure, because inserting the same item more than once would most
  likely result in a loop in one of the lists.  That's a sure-fire recipe for
  an infinite loop in the code.
 Core storage data structure.  
 Array of lists, one per grp.  
 Elements overdue for reaping. 
 Lock to protect this struct.  
 Number of discrete groups.    
 Time period spanned by grps.  
 Group containing time zero.   
 Time first element was added. 
 Function pointer for freeing. 
 Workqueue data for reaping.   
 work has been queued 
  When inserting, destroying or reaping, it's first necessary to update the
  lists relative to a particular time.  In the case of destroying, that time
  will be well in the future to ensure that all items are moved to the reap
  list.  In all other cases though, the time will be the current time.
  This function enters a loop, moving the contents of the LRU list to the reap
  list again and again until either a) the lists are all empty, or b) time zero
  has been advanced sufficiently to be within the immediate element lifetime.
  Case a) above is detected by counting how many groups are migrated and
  stopping when they've all been moved.  Case b) is detected by monitoring the
  time_zero field, which is updated as each group is migrated.
  The return value is the earliest time that more migration could be needed, or
  zero if there's no need to schedule more work because the lists are empty.
 Nothing to do if the data store is empty. 
 While time zero is older than the time spanned by all the lists. 
		
		  If the LRU list isn't empty, migrate its elements to the tail
		  of the reap list.
		
		  Advance the LRU group number, freeing the old LRU list to
		  become the new MRU list; advance time zero accordingly.
		
		  If reaping is so far behind that all the elements on all the
		  lists have been migrated to the reap list, it's now empty.
 Find the first non-empty list from the LRU end. 
 Check the grp'th list from the LRU end. 
 All the lists must be empty. 
  When inserting or doing a lookup, an element needs to be inserted into the
  MRU list.  The lists must be migrated first to ensure that they're
  up-to-date, otherwise the new element could be given a shorter lifetime in
  the cache than it should.
	
	  If the data store is empty, initialise time zero, leave grp set to
	  zero and start the work queue timer if necessary.  Otherwise, set grp
	  to the number of group times that have elapsed since time zero.
 Insert the element at the tail of the corresponding list. 
  When destroying or reaping, all the elements that were migrated to the reap
  list need to be deleted.  For each element this involves removing it from the
  data store, removing it from the reap list, calling the client's free
  function and deleting the element from the element cache.
  We get called holding the mru->lock, which we drop and then reacquire.
  Sparse need special help with this to tell it we know what we are doing.
 Remove the element from the data store. 
		
		  remove to temp list so it can be freed without
		  needing to hold the lock
  We fire the reap timer every group expiry interval so
  we always have a reaper ready to run. This makes shutdown
  and flushing of the reaper easy to do. Hence we need to
  keep when the next reap must occur so we can determine
  at each interval whether there is anything we need to do.
  To initialise a struct xfs_mru_cache pointer, call xfs_mru_cache_create()
  with the address of the pointer, a lifetime value in milliseconds, a group
  count and a free function to use when deleting elements.  This function
  returns 0 if the initialisation was successful.
 An extra list is needed to avoid reaping up to a grp_time early. 
	
	  We use GFP_KERNEL radix tree preload and do inserts under a
	  spinlock so GFP_ATOMIC is appropriate for the radix tree itself.
  Call xfs_mru_cache_flush() to flush out all cached entries, calling their
  free functions as they're deleted.  When this function returns, the caller is
  guaranteed that all the free functions for all the elements have finished
  executing and the reaper is not running.
  To insert an element, call xfs_mru_cache_insert() with the data store, the
  element's key and the client data pointer.  This function returns 0 on
  success or ENOMEM if memory for the data element couldn't be allocated.
  To remove an element without calling the free function, call
  xfs_mru_cache_remove() with the data store and the element's key.  On success
  the client data pointer for the removed element is returned, otherwise this
  function will return a NULL pointer.
  To remove and element and call the free function, call xfs_mru_cache_delete()
  with the data store and the element's key.
  To look up an element using its key, call xfs_mru_cache_lookup() with the
  data store and the element's key.  If found, the element will be moved to the
  head of the MRU list to indicate that it's been touched.
  The internal data structures are protected by a spinlock that is STILL HELD
  when this function returns.  Call xfs_mru_cache_done() to release it.  Note
  that it is not safe to call any function that might sleep in the interim.
  The implementation could have used reference counting to avoid this
  restriction, but since most clients simply want to get, set or test a member
  of the returned data structure, the extra per-element memory isn't warranted.
  If the element isn't found, this function returns NULL and the spinlock is
  released.  xfs_mru_cache_done() should NOT be called when this occurs.
  Because sparse isn't smart enough to know about conditional lock return
  status, we need to help it get it right by annotating the path that does
  not release the lock.
 help sparse not be stupid 
  To release the internal data structure spinlock after having performed an
  xfs_mru_cache_lookup() or an xfs_mru_cache_peek(), call xfs_mru_cache_done()
  with the data store pointer.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2003,2005 Silicon Graphics, Inc.
  Copyright (C) 2010 Red Hat, Inc.
  All Rights Reserved.
  Initialize the precomputed transaction reservation values
  in the mount structure.
  Free the transaction structure.  If there is more clean up
  to do when the structure is freed, add it here.
  This is called to create a new transaction which will share the
  permanent log reservation of the given transaction.  The remaining
  unused block and rt extent reservations are also inherited.  This
  implies that the original transaction is no longer allowed to allocate
  blocks.  Locks and log items, however, are no inherited.  They must
  be added to the new transaction explicitly.
	
	  Initialize the new transaction structure.
 We gave our writer reference to the new transaction 
 move deferred ops over to the new tp 
  This is called to reserve free disk blocks and log space for the
  given transaction.  This must be done before allocating any resources
  within the transaction.
  This will return ENOSPC if there are not enough blocks available.
  It will sleep waiting for available log space.
  The only valid value for the flags parameter is XFS_RES_LOG_PERM, which
  is used by long running transactions.  If any one of the reservations
  fails then they will all be backed out.
  This does not do quota reservations. That typically is done by the
  caller afterwards.
	
	  Attempt to reserve the needed disk blocks by decrementing
	  the number needed from the number available.  This will
	  fail if the count would go below zero.
	
	  Reserve the log space needed for this transaction.
	
	  Attempt to reserve the needed realtime extents by decrementing
	  the number needed from the number available.  This will
	  fail if the count would go below zero.
	
	  Error cases jump to one of these labels to undo any
	  reservations which have already been performed.
	
	  Allocate the handle before we do our freeze accounting and setting up
	  GFP_NOFS allocation context so that we avoid lockdep false positives
	  by doing GFP_KERNEL allocations inside sb_start_intwrite().
	
	  Zero-reservation ("empty") transactions can't modify anything, so
	  they're allowed to run while we're frozen.
		
		  We weren't able to reserve enough space for the transaction.
		  Flush the other speculative space allocations to free space.
		  Do not perform a synchronous scan because callers can hold
		  other locks.
  Create an empty transaction with no reservation.  This is a defensive
  mechanism for routines that query metadata without actually modifying them --
  if the metadata being queried is somehow cross-linked (think a btree block
  pointer that points higher in the tree), we risk deadlock.  However, blocks
  grabbed as part of a transaction can be re-grabbed.  The verifiers will
  notice the corrupt block and the operation will fail back to userspace
  without deadlocking.
  Note the zero-length reservation; this transaction MUST be cancelled without
  any dirty data.
  Callers should obtain freeze protection to avoid a conflict with fs freezing
  where we can be grabbing buffers at the same time that freeze is trying to
  drain the buffer LRU list.
  Record the indicated change to the given field for application
  to the file system's superblock when the transaction commits.
  For now, just store the change in the transaction structure.
  Mark the transaction structure to indicate that the superblock
  needs to be updated before committing.
  Because we may not be keeping track of allocatedfree inodes and
  used filesystem blocks in the superblock, we do not mark the
  superblock dirty in this transaction if we modify these fields.
  We still need to update the transaction deltas so that they get
  applied to the incore superblock, but we don't want them to
  cause the superblock to get locked and logged if these are the
  only fields in the superblock that the transaction modifies.
		
		  Track the number of blocks allocated in the transaction.
		  Make sure it does not exceed the number reserved. If so,
		  shutdown as this can lead to accounting inconsistency.
			
			  Return freed blocks directly to the reservation
			  instead of the global pool, being careful not to
			  overflow the trans counter. This is used to preserve
			  reservation across chains of transaction rolls that
			  repeatedly free and allocate blocks.
		
		  The allocation has already been applied to the
		  in-core superblock's counter.  This should only
		  be applied to the on-disk superblock.
		
		  Track the number of blocks allocated in the
		  transaction.  Make sure it does not exceed the
		  number reserved.
		
		  The allocation has already been applied to the
		  in-core superblock's counter.  This should only
		  be applied to the on-disk superblock.
  xfs_trans_apply_sb_deltas() is called from the commit code
  to bring the superblock buffer into the current transaction
  and modify it as requested by earlier calls to xfs_trans_mod_sb().
  For now we just look at each field allowed to change and change
  it if necessary.
	
	  Only update the superblock counters if we are logging them
		
		  Log the whole thing, the fields are noncontiguous.
		
		  Since all the modifiable fields are contiguous, we
		  can get away with this.
  xfs_trans_unreserve_and_mod_sb() is called to release unused reservations and
  apply superblock counter changes to the in-core superblock.  The
  t_res_fdblocks_delta and t_res_frextents_delta fields are explicitly NOT
  applied to the in-core superblock.  The idea is that that has already been
  done.
  If we are not logging superblock counters, then the inode allocatedfree and
  used block counts are not updated in the on disk superblock. In this case,
  XFS_TRANS_SB_DIRTY will not be set when the transaction is updated but we
  still need to update the incore superblock with the changes.
  Deltas for the inode count are +-64, hence we use a large batch size of 128
  so we don't need to take the counter lock on every update.
 calculate deltas 
 apply the per-cpu counters 
 apply remaining deltas 
	
	  Debug checks outside of the spinlock so they don't lock up the
	  machine if they fail.
 Add the given log item to the transaction's list of log items. 
  Unlink the log item from the transaction. the log item is no longer
  considered dirty in this transaction, as the linked transaction has
  finished, either by abort or commit completion.
 Detach and unlock all of the items in a transaction 
 xfs_trans_ail_update_bulk drops ailp->ail_lock 
  Bulk operation version of xfs_trans_committed that takes a log vector of
  items to insert into the AIL. This uses bulk AIL insertion techniques to
  minimise lock traffic.
  If we are called with the aborted flag set, it is because a log write during
  a CIL checkpoint commit has failed. In this case, all the items in the
  checkpoint have already gone through iop_committed and iop_committing, which
  means that checkpoint commit abort handling is treated exactly the same
  as an iclog write error even though we haven't started any IO yet. Hence in
  this case all we need to do is iop_committed processing, followed by an
  iop_unpin(aborted) call.
  The AIL cursor is used to optimise the insert process. If commit_lsn is not
  at the end of the AIL, the insert cursor avoids the need to walk
  the AIL to find the insertion point on every xfs_log_item_batch_insert()
  call. This saves a lot of needless list walking and is a net win, even
  though it slightly increases that amount of AIL lock traffic to set it up
  and tear it down.
 unpin all the log items 
 item_lsn of -1 means the item needs no further processing 
		
		  if we are aborting the operation, no point in inserting the
		  object into the AIL as we are in a shutdown situation.
			
			  Not a bulk update option due to unusual item_lsn.
			  Push into AIL immediately, rechecking the lsn once
			  we have the ail lock. Then unpin the item. This does
			  not affect the AIL cursor the bulk insert path is
			  using.
 Item is a candidate for bulk AIL insert.  
 make sure we insert the remainder! 
  Commit the given transaction to the log.
  XFS disk error handling mechanism is not based on a typical
  transaction abort mechanism. Logically after the filesystem
  gets marked 'SHUTDOWN', we can't let any new transactions
  be durable - ie. committed to disk - because some metadata might
  be inconsistent. In such cases, this returns an error, and the
  caller may assume that all locked objects joined to the transaction
  have already been unlocked as if the commit had succeeded.
  Do not reference the transaction structure after this call.
	
	  Finish deferred items on final commit. Only permanent transactions
	  should ever have deferred ops.
	
	  If there is nothing to be logged by the transaction,
	  then unlock all of the items associated with the
	  transaction and free the transaction structure.
	  Also make sure to return any reserved blocks to
	  the free pool.
	
	  If we need to update the superblock, then do it now.
	
	  If the transaction needs to be synchronous, then force the
	  log out now and wait for it.
	
	  It is indeed possible for the transaction to be not dirty but
	  the dqinfo portion to be.  All that means is that we have some
	  (non-persistent) quota reservations that need to be unreserved.
  Unlock all of the transaction's items and free the transaction.
  The transaction must not have modified any of its items, because
  there is no way to restore them to their previous state.
  If the transaction has made a log reservation, make sure to release
  it as well.
	
	  See if the caller is relying on us to shut down the
	  filesystem.  This happens in paths where we detect
	  corruption and decide to give up.
  Roll from one trans in the sequence of PERMANENT transactions to
  the next: permanent transactions are only flushed out when
  committed with xfs_trans_commit(), but we still want as soon
  as possible to let chunks of it go to the log. So we commit the
  chunk we've been working on and get a new transaction to continue.
	
	  Copy the critical parameters from one trans to the next.
	
	  Commit the current transaction.
	  If this commit failed, then it'd just unlock those items that
	  are not marked ihold. That also means that a filesystem shutdown
	  is in progress. The caller takes the responsibility to cancel
	  the duplicate transaction that gets returned.
	
	  Reserve space in the log for the next transaction.
	  This also pushes items in the "AIL", the list of logged items,
	  out to disk if they are taking up space at the tail of the log
	  that we want to use.  This requires that either nothing be locked
	  across this call, or that anything that is locked be logged in
	  the prior and the next transactions.
  Allocate an transaction, lock and join the inode to it, and reserve quota.
  The caller must ensure that the on-disk dquots attached to this inode have
  already been allocated and initialized.  The caller is responsible for
  releasing ILOCK_EXCL if a new transaction is returned.
 Caller should have allocated the dquots! 
  Allocate an transaction in preparation for inode creation by reserving quota
  against the given dquots.  Callers are not required to hold any inode locks.
  Allocate an transaction, lock and join the inode to it, and reserve quota
  in preparation for inode attribute changes that include uid, gid, or prid
  changes.
  The caller must ensure that the on-disk dquots attached to this inode have
  already been allocated and initialized.  The ILOCK will be dropped when the
  transaction is committed or cancelled.
 Caller should have allocated the dquots! 
	
	  For each quota type, skip quota reservations if the inode's dquots
	  now match the ones that came from the caller, or the caller didn't
	  pass one in.  The inode's dquots can change if we drop the ILOCK to
	  perform a blockgc scan, so we must preserve the caller's arguments.
		
		  Reserve enough quota to handle blocks on disk and reserved
		  for a delayed allocation.  We'll actually transfer the
		  delalloc reservation between dquots at chown time, even
		  though that part is only semi-transactional.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2003 Silicon Graphics, Inc.
  All Rights Reserved.
  returns the number of iovecs needed to log the given dquot item.
  fills in the vector of log iovecs for the given dquot log item.
  Increment the pin count of the given dquot.
  Decrement the pin count of the given dquot, and wake up
  anyone in xfs_dqwait_unpin() if the count goes to 0.	 The
  dquot must have been previously pinned with a call to
  xfs_qm_dquot_logitem_pin().
  This is called to wait for the given dquot to be unpinned.
  Most of these pinunpin routines are plagiarized from inode code.
	
	  Give the log a push so we don't wait here too long.
	
	  Re-check the pincount now that we stabilized the value by
	  taking the quota lock.
	
	  Someone else is already flushing the dquot.  Nothing we can do
	  here but wait for the flush to finish and remove the item from
	  the AIL.
	
	  dquots are never 'held' from getting unlocked at the end of
	  a transaction.  Their locking and unlocking is hidden inside the
	  transaction layer, within trans_commit. Hence, no LI_HOLD flag
	  for the logitem.
  Initialize the dquot log item for a newly allocated dquot.
  The dquot isn't locked at this point, but it isn't on any of the lists
  either, so we don't care.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.
  All Rights Reserved.
 inode log item 
  The logged size of an inode fork is always the current size of the inode
  fork. This means that when an inode fork is relogged, the size of the logged
  region is determined by the current state, not the combination of the
  previously logged state + the current state. This is different relogging
  behaviour to most other log items which will retain the size of the
  previously logged changes when smaller regions are relogged.
  Hence operations that remove data from the inode fork (e.g. shortform
  dirattr remove, extent form extent removal, etc), the size of the relogged
  inode gets -smaller- rather than stays the same size as the previously logged
  size and this can result in the committing transaction reducing the amount of
  space being consumed by the CIL.
 worst case, doesn't subtract delalloc extents 
 worst case, doesn't subtract unused space 
  This returns the number of iovecs needed to log the given inode item.
  We need one iovec for the inode log format structure, one for the
  inode core, and possibly one for the inode dataextentsb-tree root
  and one for the inode attribute dataextentsb-tree root.
			
			  Round i_bytes up to a word boundary.
			  The underlying memory is guaranteed
			  to be there by xfs_idata_realloc().
			
			  Round i_bytes up to a word boundary.
			  The underlying memory is guaranteed
			  to be there by xfs_idata_realloc().
  Convert an incore timestamp to a log timestamp.  Note that the log format
  specifies host endian format!
  The legacy DMAPI fields are only present in the on-disk and in-log inodes,
  but not in the in-memory one.  But we are guaranteed to have an inode buffer
  in memory when logging an inode, so we can just copy it from the on-disk
  inode to the in-log inode here so that recovery of file system with these
  fields set to non-zero values doesn't lose them.  For all other cases we zero
  the fields.
 log a dummy value to ensure log structure is fully initialised 
  Format the inode core. Current timestamp data is only in the VFS inode
  fields, so we need to grab them from there. Hence rather than just copying
  the XFS inode core structure, format the fields directly into the iovec.
  This is called to fill in the vector of log iovecs for the given inode
  log item.  It fills the first item with an inode log format structure,
  the second with the on-disk inode structure, and a possible third andor
  fourth with the inode dataextentsb-tree root and inode attributes
  dataextentsb-tree root.
  Note: Always use the 64 bit inode log format structure so we don't
  leave an uninitialised hole in the format item on 64 bit systems. Log
  recovery on 32 bit systems handles this just fine, so there's no reason
  for not using an initialising the properly padded structure all the time.
 format + core 
	
	  make sure we don't leak uninitialised data into the log in the case
	  when we don't log every field in the inode.
 update the format with the exact fields we actually logged 
  This is called to pin the inode associated with the inode log
  item in memory so it cannot be written out.
  This is called to unpin the inode associated with the inode log
  item which was previously pinned with a call to xfs_inode_item_pin().
  Also wake up anyone in xfs_iunpin_wait() if the count goes to 0.
  Note that unpin can race with inode cluster buffer freeing marking the buffer
  stale. In that case, flush completions are run from the buffer unpin call,
  which may happen before the inode is unpinned. If we lose the race, there
  will be no buffer attached to the log item, but the inode will be marked
  XFS_ISTALE.
	
	  We need to hold a reference for flushing the cluster buffer as it may
	  fail the buffer without IO submission. In which case, we better get a
	  reference for that completion because otherwise we don't get a
	  reference for IO until we queue the buffer for delwri submission.
		
		  Release the buffer if we were unable to flush anything. On
		  any other error, the buffer has already been released.
  Unlock the inode associated with the inode log item.
  This is called to find out where the oldest active copy of the inode log
  item in the on disk log resides now that the last log write of it completed
  at the given lsn.  Since we always re-log all dirty data in an inode, the
  latest copy in the on disk log is the only one that matters.  Therefore,
  simply return the given lsn.
  If the inode has been marked stale because the cluster is being freed, we
  don't want to (re-)insert this inode into the AIL. There is a race condition
  where the cluster buffer may be unpinned before the inode is inserted into
  the AIL during transaction committed processing. If the buffer is unpinned
  before the inode item has been committed and inserted, then it is possible
  for the buffer to be written and IO completes before the inode is inserted
  into the AIL. In that case, we'd be inserting a clean, stale inode into the
  AIL which will never get removed. It will, however, get reclaimed which
  triggers an assert in xfs_inode_free() complaining about freein an inode
  still in the AIL.
  To avoid this, just unpin the inode directly and return a LSN of -1 so the
  transaction committed code knows that it does not need to do any further
  processing on the item.
  Initialize the inode log item for a newly allocated (in-core) inode.
  Free the inode log item and any memory hanging off of it.
  We only want to pull the item from the AIL if it is actually there
  and its location in the log has not changed since we started the
  flush.  Thus, we only bother if the inode's lsn has not changed.
 this is an opencoded batch version of xfs_trans_ail_delete 
  Walk the list of inodes that have completed their IOs. If they are clean
  remove them from the list and dissociate them from the buffer. Buffers that
  are still dirty remain linked to the buffer and on the list. Caller must
  handle them appropriately.
		
		  Remove the reference to the cluster buffer if the inode is
		  clean in memory and drop the buffer reference once we've
		  dropped the locks we hold.
  Inode buffer IO completion routine.  It is responsible for removing inodes
  attached to the buffer from the AIL if they have not been re-logged and
  completing the inode flush.
	
	  Pull the attached inodes from the buffer one at a time and take the
	  appropriate action on them.
 Do an unlocked check for needing the AIL lock. 
  This is the inode flushing abort routine.  It is called when
  the filesystem is shutting down to clean up the inode state.  It is
  responsible for removing the inode item from the AIL if it has not been
  re-logged and clearing the inode's flush state.
		
		  Clear the failed bit before removing the item from the AIL so
		  xfs_trans_ail_delete() doesn't try to clear and release the
		  buffer attached to the log item before we are done with it.
		
		  Clear the inode logging fields so no more flushes are
		  attempted.
  convert an xfs_inode_log_format struct from the old 32 bit version
  (which can have different field alignments) to the native 64 bit version
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2014 Christoph Hellwig.
  Ensure that we do not have any outstanding pNFS layouts that can be used by
  clients to directly read from or write to this inode.  This must be called
  before every operation that can remove blocks from the extent map.
  Additionally we call it during the write operation, where aren't concerned
  about exposing unallocated blocks but just want to provide basic
  synchronization between a local writer and pNFS clients.  mmap writes would
  also benefit from this sort of synchronization, but due to the tricky locking
  rules in the page fault path we don't bother.
  Get a unique ID including its location so that the client can identify
  the exported device.
  Get a layout for the pNFS client.
	
	  We can't export inodes residing on the realtime device.  The realtime
	  device doesn't have a UUID to identify it, so the client has no way
	  to find it.
	
	  The pNFS block layout spec actually supports reflink like
	  functionality, but the Linux pNFS server doesn't implement it yet.
	
	  Lock out any other IO before we flush and invalidate the pagecache,
	  and then hand out a layout to the remote system.  This is very
	  similar to direct IO, except that the synchronization is much more
	  complicated.  See the comment near xfs_break_leased_layouts
	  for a detailed explanation.
		
		  Ensure the next transaction is committed synchronously so
		  that the blocks allocated and handed out to the client are
		  guaranteed to be present even after a server crash.
  Ensure the size update falls into a valid allocated block.
  Make sure the blocks described by maps are stable on disk.  This includes
  converting any unwritten extents, flushing the disk cache and updating the
  time stamps.
  Note that we rely on the caller to always send us a timestamp update so that
  we always commit a transaction here.  If that stops being true we will have
  to manually flush the cache here similar to what the fsync code path does
  for datasyncs on files that have no dirty metadata.
		
		  Make sure reads through the pagecache see the new data.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  All Rights Reserved.
  Recover a dquot record
	
	  Filesystems are required to send in quota flags at mount time.
	
	  This type of quotas was turned off, so ignore this record.
	
	  At this point we know that quota was _not_ turned off.
	  Since the mount flags are not indicating to us otherwise, this
	  must mean that quota is on, and the dquot needs to be replayed.
	  Remember that we may not have fully recovered the superblock yet,
	  so we can't do the usual trick of looking at the SB quota bits.
	 
	  The other possibility, of course, is that the quota subsystem was
	  removed since the last mount - ENOSYS.
	
	  At this point we are assuming that the dquots have been allocated
	  and hence the buffer has valid dquots stamped in it. It should,
	  therefore, pass verifier validation. If the dquot is bad, then the
	  we'll return an error here, so we don't need to specifically check
	  the dquot in the buffer after the verifier has run.
	
	  If the dquot has an LSN in it, recover the dquot only if it's less
	  than the lsn of the transaction we are replaying.
  Recover QUOTAOFF records. We simply make a note of it in the xlog
  structure, so that we know not to do any dquot item or dquot buffer recovery,
  of that type.
	
	  The logitem format's flag tells us if this was user quotaoff,
	  groupproject quotaoff or both.
 nothing to commit in pass2 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2006-2007 Silicon Graphics, Inc.
  Copyright (c) 2014 Christoph Hellwig.
  All Rights Reserved.
 AG in use for this directory 
  Allocation group filestream associations are tracked with per-ag atomic
  counters.  These counters allow xfs_filestream_pick_ag() to tell whether a
  particular AG already has active filestreams associated with it.
  Scan the AGs starting at startag looking for an AG that isn't in use and has
  at least minlen blocks free.
 2% of an AG's blocks must be free for it to be chosen. 
 For the first pass, don't sleep trying to init the per-AG. 
 Couldn't lock the AGF, skip this AG. 
 Keep track of the AG with the most free blocks. 
		
		  The AG reference count does two things: it enforces mutual
		  exclusion when examining the suitability of an AG in this
		  loop, and it guards against two filestreams being established
		  in the same AG as each other.
 Break out, retaining the reference on the AG. 
 Drop the reference on this AG, it's not usable. 
 Move to the next AG, wrapping to AG 0 if necessary. 
 If a full pass of the AGs hasn't been done yet, continue. 
 Allow sleeping in xfs_alloc_pagf_init() on the 2nd pass. 
 Finally, if lowspace wasn't set, set it for the 3rd pass. 
		
		  Take the AG with the most free space, regardless of whether
		  it's already in use by another filestream.
 take AG 0 if none matched 
  Find the right allocation group for a file, either by finding an
  existing file stream or creating a new one.
  Returns NULLAGNUMBER in case of an error.
	
	  Set the starting AG using the rotor for inode32, otherwise
	  use the directory inode's AG.
  Pick a new allocation group for the current file and its file stream.
  This is called when the allocator can't find a suitable extent in the
  current AG, and we have to move the stream into a new AG with more space.
	
	  Only free the item here so we skip over the old AG earlier.
	
	  The filestream timer tunable is currently fixed within the range of
	  one second to four minutes, with five seconds being the default.  The
	  group count is somewhat arbitrary, but it'd be nice to adhere to the
	  timer tunable to within about 10 percent.  This requires at least 10
	  groups.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
	
	  No file system can have quotas enabled on disk but not in core.
	  Note that quota utilities (like quotaoff) _expect_
	  errno == -EEXIST here.
	
	  We do not support actually turning off quota accounting any more.
	  Just log a warning and ignore the accounting related flags.
 XXX what to do if error ? Revert back to old vals incore ? 
  Switch on (a given) quota enforcement for a filesystem.  This takes
  effect immediately.
  (Switching on quota accounting must be done at mount time.)
	
	  Switching on quota accounting must be done at mount time,
	  only consider quota enforcement stuff here.
	
	  Can't enforce without accounting. We check the superblock
	  qflags here instead of m_qflags because rootfs can have
	  quota acct on ondisk without m_qflags' knowing.
	
	  If everything's up to-date incore, then don't waste time.
	
	  Change sb_qflags on disk but not incore mp->qflags
	  if this is the root filesystem.
	
	  There's nothing to change if it's the same.
	
	  If we aren't trying to switch on quota enforcement, we are done.
	
	  Switch on quota enforcement in core.
  Adjust limits of this quota, and the defaults if passed in.  Returns true
  if the new limits made sense and were applied, false otherwise.
 The hard limit can't be less than the soft limit. 
 Set the length of the default grace period. 
 Set the grace period expiration on a quota. 
  Adjust quota limits, and startstop timers accordingly.
	
	  We don't want to race with a quotaoff so take the quotaoff lock.
	  We don't hold an inode lock, so there's nothing else to stop
	  a quotaoff from happening.
	
	  Get the dquot (locked) before we start, as we need to do a
	  transaction to allocate it if it doesn't exist. Once we have the
	  dquot, unlock it so we can start the next transaction safely. We hold
	  a reference to the dquot, so it's safe to do this unlocklock without
	  it being reclaimed in the mean time.
	
	  Update quota limits, warnings, and timers, and the defaults
	  if we're touching id == 0.
	 
	  Make sure that hardlimits are >= soft limits before changing.
	 
	  Update warnings counter(s) if requested.
	 
	  Timelimits for the super user set the relative time the other users
	  can be over quota for this file system. If it is zero a default is
	  used.  Ditto for the default soft and hard limit values (already
	  done, above), and for warnings.
	 
	  For other IDs, userspace can bump out the grace period if over
	  the soft limit.
 Blocks on the data device. 
 Blocks on the realtime device. 
 Inodes 
		
		  If the user is now over quota, start the timelimit.
		  The user will not be 'warned'.
		  Note that we keep the timers ticking, whether enforcement
		  is on or off. We don't really want to bother with iterating
		  over all ondisk dquots and turning the timers onoff.
 Fill out the quota context. 
	
	  Internally, we don't reset all the timers when quota enforcement
	  gets turned off. No need to confuse the user level code,
	  so return zeroes in that case.
 Return the quota information for the dquot matching id. 
 Flush inodegc work at the start of a quota reporting scan. 
	
	  Try to get the dquot. We don't want it allocated on disk, so don't
	  set doalloc. If it doesn't exist, we'll get ENOENT back.
	
	  If everything's NULL, this dquot doesn't quite exist as far as
	  our utility programs are concerned.
  Return the quota information for the first initialized dquot whose id
  is at least as high as id.
 Flush inodegc work at the start of a quota reporting scan. 
 Fill in the ID we actually read from disk 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2019 Christoph Hellwig.
  Submit a request for an async cache flush to run. If the request queue does
  not require flush operations, just skip it altogether. If the caller needs
  to wait for the flush completion at a later point in time, they must supply a
  valid completion. This will be signalled when the flush completes.  The
  caller never sees the bio that is issued here.
 SPDX-License-Identifier: GPL-2.0-or-later
  Copyright (C) 2019 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Parallel Work Queue
  ===================
  Abstract away the details of running a large and "obviously" parallelizable
  task across multiple CPUs.  Callers initialize the pwork control object with
  a desired level of parallelization and a work function.  Next, they embed
  struct xfs_pwork in whatever structure they use to pass work context to a
  worker thread and queue that pwork.  The work function will be passed the
  pwork item when it is run (from process context) and any returned error will
  be recorded in xfs_pwork_ctl.error.  Work functions should check for errors
  and abort if necessary; the non-zeroness of xfs_pwork_ctl.error does not
  stop workqueue item processing.
  This is the rough equivalent of the xfsprogs workqueue code, though we can't
  reuse that name here.
 Invoke our caller's function. 
  Set up control data for parallel work.  @work_fn is the function that will
  be called.  @tag will be written into the kernel threads.  @nr_threads is
  the level of parallelism desired, or 0 for no limit.
 Queue some parallel work. 
 Wait for the work to finish and tear down the control structure. 
  Wait for the work to finish by polling completion status and touch the soft
  lockup watchdog.  This is for callers such as mount which hold locks.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2016 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Freeing the CUI requires that we remove it from the AIL if it has already
  been placed there. However, the CUI may not yet have been placed in the AIL
  when called by xfs_cui_release() from CUD processing due to the ordering of
  committed vs unpin operations in bulk insert operations. Hence the reference
  count to ensure only the last caller frees the CUI.
  This is called to fill in the vector of log iovecs for the
  given cui log item. We use only 1 iovec, and we point that
  at the cui_log_format structure embedded in the cui item.
  It is at this point that we assert that all of the extent
  slots in the cui item have been filled.
  The unpin operation is the last place an CUI is manipulated in the log. It is
  either inserted in the AIL or aborted in the event of a log IO error. In
  either case, the CUI transaction has been successfully committed to make it
  this far. Therefore, we expect whoever committed the CUI to either construct
  and commit the CUD or drop the CUD's reference in the event of error. Simply
  drop the log's CUI reference now that the log is done with it.
  The CUI has been either committed or aborted if the transaction has been
  cancelled. If the transaction was cancelled, an CUD isn't going to be
  constructed and thus we free the CUI here directly.
  Allocate and initialize an cui item with the given number of extents.
  This is called to fill in the vector of log iovecs for the
  given cud log item. We use only 1 iovec, and we point that
  at the cud_log_format structure embedded in the cud item.
  It is at this point that we assert that all of the extent
  slots in the cud item have been filled.
  The CUD is either committed or aborted if the transaction is cancelled. If
  the transaction is cancelled, drop our reference to the CUI and free the
  CUD.
  Finish an refcount update and log it to the CUD. Note that the
  transaction is marked dirty regardless of whether the refcount
  update succeeds or fails to support the CUICUD lifecycle rules.
	
	  Mark the transaction dirty, even on error. This ensures the
	  transaction is aborted, which:
	 
	  1.) releases the CUI and frees the CUD
	  2.) shuts down the filesystem
 Sort refcount intents by AG. 
 Set the phys extent flags for this reverse mapping. 
 Log refcount updates in the intent item. 
	
	  atomic_inc_return gives us the value after the increment;
	  we want to use it as an array index so we need to subtract 1 from
	  it.
 Get an CUD so we can process all the deferred refcount updates. 
 Process a deferred refcount update. 
 Did we run out of reservation?  Requeue what we didn't finish. 
 Abort all pending CUIs. 
 Cancel a deferred refcount update. 
 Is this recovered CUI ok? 
  Process a refcount update intent item that was recovered from the log.
  We need to update the refcountbt.
	
	  First check the validity of the extents described by the
	  CUI.  If any are bad, then assume that all are bad and
	  just toss the CUI.
	
	  Under normal operation, refcount updates are deferred, so we
	  wouldn't be adding them directly to a transaction.  All
	  refcount updates manage reservation usage internally and
	  dynamically by deferring work that won't fit in the
	  transaction.  Normally, any work that needs to be deferred
	  gets attached to the same defer_ops that scheduled the
	  refcount update.  However, we're in log recovery here, so we
	  use the passed in defer_ops and to finish up any work that
	  doesn't fit.  We need to reserve enough blocks to handle a
	  full btree split on either end of the refcount range.
 Requeue what we didn't finish. 
 Relog an intent item to push the log tail forward. 
  Copy an CUI format buffer from the given buf, and into the destination
  CUI format structure.  The CUICUD items were designed not to need any
  special alignment handling.
  This routine is called to create an in-core extent refcount update
  item from the cui format structure which was logged on disk.
  It allocates an in-core cui, copies the extents from the format
  structure into it, and adds the cui to the AIL with the given
  LSN.
	
	  Insert the intent into the AIL directly and drop one reference so
	  that finishing or canceling the work will drop the other.
  This routine is called when an CUD format structure is found in a committed
  transaction in the log. Its purpose is to cancel the corresponding CUI if it
  was still in the log. To do this it searches the AIL for the CUI with an id
  equal to that in the CUD format structure. If we find it we drop the CUD
  reference, which removes the CUI from the AIL and frees it.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  Copyright (c) 2016-2018 Christoph Hellwig.
  All Rights Reserved.
		
		  Round up the allocation request to a stripe unit
		  (m_dalign) boundary if the file size is >= stripe unit
		  size, and we are allocating past the allocation eof.
		 
		  If mounted with the "-o swalloc" option the alignment is
		  increased from the strip unit size to the stripe width.
  Check if last_fsb is outside the last extent, and if so grow it to the next
  stripe unit boundary.
	
	  Always round up the allocation request to the extent hint boundary.
	
	  For DAX, we do not allocate unwritten extents, but instead we zero
	  the block before we commit the transaction.  Ideally we'd like to do
	  this outside the transaction context, but if we commit and then crash
	  we may not have zeroed the blocks and this will be exposed on
	  recovery of the allocation. Hence we must zero before commit.
	 
	  Further, if we are mapping unwritten extents here, we need to zero
	  and convert them to written so that we don't need an unwritten extent
	  callback for DAX. This also means that we need to be able to dip into
	  the reserve block pool for bmbt block allocation if there is no space
	  left but we need to do unwritten extent conversion.
	
	  From this point onwards we overwrite the imap pointer that the
	  caller gave to us.
	
	  Complete the transaction
	
	  Copy any maps to caller's array and return any error.
 no hi watermark, no throttle 
 under the lo watermark, no throttle 
 no dq, or over hi wmark, squash the prealloc completely 
 only overwrite the throttle values if we are more aggressive 
  If we don't have a user specified preallocation size, dynamically increase
  the preallocation size as the size of the file grows.  Cap the maximum size
  at a single extent or less if the filesystem is near full. The closer the
  filesystem is to being full, the smaller the maximum preallocation.
	
	  As an exception we don't do any preallocation at all if the file is
	  smaller than the minimum preallocation and we are using the default
	  dynamic preallocation scheme, as it is likely this is the only write
	  to the file that is going to be done.
	
	  Use the minimum preallocation size for small files or if we are
	  writing right after a hole.
	
	  Take the size of the preceding data extents as the basis for the
	  preallocation size. Note that we don't care if the previous extents
	  are written or not.
	
	  If the size of the extents is greater than half the maximum extent
	  length, then use the current offset as the basis.  This ensures that
	  for large files the preallocation size always extends to MAXEXTLEN
	  rather than falling short due to things like stripe unitwidth
	  alignment of real extents.
	
	  MAXEXTLEN is not a power of two value but we round the prealloc down
	  to the nearest power of two value after throttling. To prevent the
	  round down from unconditionally reducing the maximum supported
	  prealloc size, we round up first, apply appropriate throttling,
	  round down and cap the value to MAXEXTLEN.
	
	  Check each quota to cap the prealloc size, provide a shift value to
	  throttle with and adjust amount of available space.
	
	  The final prealloc size is set to the minimum of free space available
	  in each of the quotas and the overall filesystem.
	 
	  The shift throttle value is set to the maximum value as determined by
	  the global low free space values and per-quota low free space values.
	
	  rounddown_pow_of_two() returns an undefined result if we pass in
	  alloc_blocks = 0.
	
	  If we are still trying to allocate more space than is
	  available, squash the prealloc hard. This can happen if we
	  have a large file on a small filesystem and the above
	  lowspace thresholds are smaller than MAXEXTLEN.
	
	  Reserve enough blocks in this transaction for two complete extent
	  btree splits.  We may be converting the middle part of an unwritten
	  extent and in this case we will insert two new extents in the btree
	  each of which could cause a full split.
	 
	  This reservation amount will be used in the first call to
	  xfs_bmbt_split() to select an AG with enough space to satisfy the
	  rest of the operation.
 Attach dquots so that bmbt splits are accounted correctly. 
		
		  Set up a transaction to convert the range of extents
		  from unwritten to real. Do allocations in a loop until
		  we have covered the range passed in.
		 
		  Note that we can't risk to recursing back into the filesystem
		  here as we might be asked to write out the same inode that we
		  complete here and might deadlock on the iolock.
		
		  Modify the unwritten extent state of the buffer.
		
		  Log the updated inode size as we go.  We have to be careful
		  to only log it up to the actual write offset if it is
		  halfway into a block.
			
			  The numblks_fsb value should always get
			  smaller, otherwise the loop is stuck.
 don't allocate blocks when just zeroing 
 we convert unwritten extents before copying the data for DAX 
 when zeroing we don't have to COW holes or unwritten extents 
	
	  COW writes may allocate delalloc space or convert unwritten COW
	  extents, so we need to make sure to take the lock exclusively here.
	
	  Extents not yet cached requires exclusive access, don't block.  This
	  is an opencoded xfs_ilock_data_map_shared() call but with
	  non-blocking behaviour.
	
	  The reflink iflag could have changed since the earlier unlocked
	  check, so if we got ILOCK_SHARED for a write and but we're now a
	  reflink inode we have to switch to ILOCK_EXCL and relock.
  Check that the imap we are going to return to the caller spans the entire
  range that the caller requested for the IO.
	
	  Writes that span EOF might trigger an IO size update on completion,
	  so consider them to be dirty for the purposes of O_DSYNC even if
	  there is no other metadata changes pending or have been made here.
 may drop and re-acquire the ilock 
	
	  NOWAIT and OVERWRITE IO needs to span the entire requested IO with
	  a single map so that we avoid partial IO failures due to the rest of
	  the IO range not covered by this map triggering an EAGAIN condition
	  when it is subsequently mapped and aborting the IO.
	
	  For overwrite only IO, we cannot convert unwritten extents without
	  requiring sub-block zeroing.  This can only be done under an
	  exclusive IOLOCK, hence return -EAGAIN if this is not a written
	  extent to tell the caller to try again.
	
	  We cap the maximum length we map to a sane size  to keep the chunks
	  of work done where somewhat symmetric with the work writeback does.
	  This is a completely arbitrary number pulled out of thin air as a
	  best guess for initial testing.
	 
	  Note that the values needs to be less than 32-bits wide until the
	  lower level functions are updated.
 we can't use delayed allocations when using extent size hints 
	
	  Search the data fork first to look up our source mapping.  We
	  always need the data fork map, as we have to return it to the
	  iomap code so that the higher level write code can read data in to
	  perform read-modify-write cycles for unaligned writes.
 fake hole until the end 
 We never need to allocate blocks for zeroing a hole. 
	
	  Search the COW fork extent list even if we did not find a data fork
	  extent.  This serves two purposes: first this implements the
	  speculative preallocation using cowextsize, so that we also unshare
	  block adjacent to shared blocks instead of just the shared blocks
	  themselves.  Second the lookup in the extent list is generally faster
	  than going out to the shared extent tree.
		
		  For reflink files we may need a delalloc reservation when
		  overwriting shared extents.   This includes zeroing of
		  existing extents that contain data.
 Trim the mapping to the nearest shared extent boundary. 
 Not shared?  Just report the (potentially capped) extent. 
		
		  Fork all the shared blocks from our write offset until the
		  end of the extent.
		
		  We cap the maximum length we map here to MAX_WRITEBACK_PAGES
		  pages to keep the chunks of work done where somewhat
		  symmetric with the work writeback does.  This is a completely
		  arbitrary number pulled out of thin air.
		 
		  Note that the values needs to be less than 32-bits wide until
		  the lower level functions are updated.
		
		  Determine the initial size of the preallocation.
		  We clean up any extra preallocation when the file is closed.
 retry without any preallocation 
	
	  Flag newly allocated delalloc blocks with IOMAP_F_NEW so we punch
	  them out if the write happens to fail.
	
	  Behave as if the write failed if drop writes is enabled. Set the NEW
	  flag to force delalloc cleanup.
	
	  start_fsb refers to the first unused block after a short write. If
	  nothing was written, round offset down to point at the first block in
	  the range.
	
	  Trim delalloc blocks if they were allocated by this write and we
	  didn't manage to write the whole range.
	 
	  We don't need to care about racing delalloc as we hold i_mutex
	  across the reserveallocateunreserve calls. If there are delalloc
	  blocks in the range, they are ours.
		
		  If we found a data extent we are done.
		
		  Fake a hole until the end of the file.
	
	  If a COW fork extent covers the hole, report it - capped to the next
	  data fork extent:
		
		  This is a COW extent, so we must probe the page cache
		  because there could be dirty page cache being backed
		  by this extent.
	
	  Else report a hole, capped to the next found data or COW extent.
 if there are no attribute fork or extents, return ENOENT 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2014 Red Hat, Inc.
  All Rights Reserved.
 debug 
  Override how many threads the parallel work queue is allowed to create.
  This has to be a debug-only global (instead of an errortag) because one of
  the main users of parallel workqueues is mount time quotacheck.
 DEBUG 
 DEBUG 
 stats 
 xlog 
  Metadata IO error configuration
  The sysfs structure here is:
 	...xfs<dev>error<class><errno><error_attrs>
  where <class> allows us to discriminate between data IO and metadata IO,
  and any other future type of IO (e.g. special inode or directory error
  handling) we care to support.
 1 day timeout maximum, -1 means infinite 
  Error initialization tables. These need to be ordered in the same
  order as the enums used to index the array. All class init tables need to
  define a "default" behaviour as the first entry, all other entries can be
  empty.
 in seconds 
 We can't recover from devices disappearing 
 unwind the entries that succeeded 
 ...xfs<dev>error 
 ...xfs<dev>errormetadata 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Check to see if a buffer matching the given parameters is already
  a part of the given transaction.
  Add the locked buffer to the transaction.
  The buffer must be locked, and it cannot be associated with any
  transaction.
  If the buffer does not yet have a buf log item associated with it,
  then allocate one for it.  Then add the buf item to the transaction.
	
	  The xfs_buf_log_item pointer is stored in b_log_item.  If
	  it doesn't have one yet, then allocate one and initialize it.
	  The checks to see if one is there are in xfs_buf_item_init().
	
	  Take a reference for this transaction on the buf item.
	
	  Attach the item to the transaction so we can find it in
	  xfs_trans_get_buf() and friends.
  Get and lock the buffer for the caller if it is not already
  locked within the given transaction.  If it is already locked
  within the transaction, just increment its lock recursion count
  and return a pointer to it.
  If the transaction pointer is NULL, make this just a normal
  get_buf() call.
	
	  If we find the buffer in the cache with this transaction
	  pointer in its b_fsprivate2 field, then we know we already
	  have it locked.  In this case we just increment the lock
	  recursion count and return the buffer to the caller.
  Get and lock the superblock buffer for the given transaction.
	
	  Just increment the lock recursion count if the buffer is already
	  attached to this transaction.
  Get and lock the buffer for the caller if it is not already
  locked within the given transaction.  If it has not yet been
  read in, read it from disk. If it is already locked
  within the transaction and already read in, just increment its
  lock recursion count and return a pointer to it.
  If the transaction pointer is NULL, make this just a normal
  read_buf() call.
	
	  If we find the buffer in the cache with this transaction
	  pointer in its b_fsprivate2 field, then we know we already
	  have it locked.  If it is already read in we just increment
	  the lock recursion count and return the buffer to the caller.
	  If the buffer is not yet read in, then we read it in, increment
	  the lock recursion count, and return it to the caller.
		
		  We never locked this buf ourselves, so we shouldn't
		  brelse it either. Just get out.
		
		  Check if the caller is trying to read a buffer that is
		  already attached to the transaction yet has no buffer ops
		  assigned.  Ops are usually attached when the buffer is
		  attached to the transaction, or by the read caller if
		  special circumstances.  That didn't happen, which is not
		  how this is supposed to go.
		 
		  If the buffer passes verification we'll let this go, but if
		  not we have to shut down.  Let the transaction cleanup code
		  release this buffer when it kills the tranaction.
 bad CRC means corrupted metadata 
 Has this buffer been dirtied by anyone? 
  Release a buffer previously joined to the transaction. If the buffer is
  modified within this transaction, decrement the recursion count but do not
  release the buffer even if the count goes to 0. If the buffer is not modified
  within the transaction, decrement the recursion count and release the buffer
  if the recursion count goes to 0.
  If the buffer is to be released and it was not already dirty before this
  transaction began, then also free the buf_log_item associated with it.
  If the transaction pointer is NULL, this is a normal xfs_buf_relse() call.
	
	  If the release is for a recursive lookup, then decrement the count
	  and return.
	
	  If the buffer is invalidated or dirty in this transaction, we can't
	  release it until we commit.
	
	  Unlink the log item from the transaction and clear the hold flag, if
	  set. We wouldn't want the next user of the buffer to get confused.
 drop the reference to the bli 
  Mark the buffer as not needing to be unlocked when the buf item's
  iop_committing() routine is called.  The buffer must already be locked
  and associated with the given transaction.
 ARGSUSED 
  Cancel the previous buffer hold request made on this buffer
  for this transaction.
  Mark a buffer dirty in the transaction.
	
	  Mark the buffer as needing to be written out eventually,
	  and set its iodone function to remove the buffer's buf log
	  item from the AIL and free it when the buffer is flushed
	  to disk.
	
	  If we invalidated the buffer within this transaction, then
	  cancel the invalidation now that we're dirtying the buffer
	  again.  There are no races with the code in xfs_buf_item_unpin(),
	  because we have a reference to the buffer this entire time.
  This is called to mark bytes first through last inclusive of the given
  buffer as needing to be logged when the transaction is committed.
  The buffer must already be associated with the given transaction.
  First and last are numbers relative to the beginning of this buffer,
  so the first byte in the buffer is numbered 0 regardless of the
  value of b_blkno.
  Invalidate a buffer that is being used within a transaction.
  Typically this is because the blocks in the buffer are being freed, so we
  need to prevent it from being written out when we're done.  Allowing it
  to be written again might overwrite data in the free blocks if they are
  reallocated to a file.
  We prevent the buffer from being written out by marking it stale.  We can't
  get rid of the buf log item at this point because the buffer may still be
  pinned by another transaction.  If that is the case, then we'll wait until
  the buffer is committed to disk for the last time (we can tell by the ref
  count) and free it in xfs_buf_item_unpin().  Until that happens we will
  keep the buffer locked so that the buffer and buf log item are not reused.
  We also set the XFS_BLF_CANCEL flag in the buf log format structure and log
  the buf item.  This will be used at recovery time to determine that copies
  of the buffer in the log before this should not be replayed.
  We mark the item descriptor and the transaction dirty so that we'll hold
  the buffer until after the commit.
  Since we're invalidating the buffer, we also clear the state about which
  parts of the buffer have been logged.  We also clear the flag indicating
  that this is an inode buffer since the data in the buffer will no longer
  be valid.
  We set the stale bit in the buffer as well since we're getting rid of it.
		
		  If the buffer is already invalidated, then
		  just return.
  This call is used to indicate that the buffer contains on-disk inodes which
  must be handled specially during recovery.  They require special handling
  because only the di_next_unlinked from the inodes in the buffer should be
  recovered.  The rest of the data in the buffer is logged via the inodes
  themselves.
  All we do is set the XFS_BLI_INODE_BUF flag in the items flags so it can be
  transferred to the buffer's log format structure so that we'll know what to
  do at recovery time.
  This call is used to indicate that the buffer is going to
  be staled and was an inode buffer. This means it gets
  special processing during unpin - where any inodes
  associated with the buffer should be removed from ail.
  There is also special processing during recovery,
  any replay of the inodes in the buffer needs to be
  prevented as the buffer may have been reused.
  Mark the buffer as being one which contains newly allocated
  inodes.  We need to make sure that even if this buffer is
  relogged as an 'inode buf' we still recover all of the inode
  images in the face of a crash.  This works in coordination with
  xfs_buf_item_committed() to ensure that the buffer remains in the
  AIL at its original location even after it has been relogged.
 ARGSUSED 
  Mark the buffer as ordered for this transaction. This means that the contents
  of the buffer are not recorded in the transaction but it is tracked in the
  AIL as though it was. This allows us to record logical changes in
  transactions rather than the physical changes we make to the buffer without
  changing writeback ordering constraints of metadata buffers.
	
	  We don't log a dirty range of an ordered buffer but it still needs
	  to be marked dirty and that it has been logged.
  Set the type of the buffer for log recovery so that it can correctly identify
  and hence attach the correct buffer ops to the buffer after replay.
  Similar to xfs_trans_inode_buf(), this marks the buffer as a cluster of
  dquots. However, unlike in inode buffer recovery, dquot buffers get
  recovered in their entirety. (Hence, no XFS_BLI_DQUOT_ALLOC_BUF flag).
  The only thing that makes dquot buffers different from regular
  buffers is that we must not replay dquot bufs when recovering
  if a _corresponding_ quotaoff has happened. We also have to distinguish
  between usr dquot bufs and grp dquot bufs, because usr and grp quotas
  can be turned off independently.
 ARGSUSED 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  All Rights Reserved.
  Inode fork owner changes
  If we have been told that we have to reparent the inode fork, it's because an
  extent swap operation on a CRC enabled filesystem has been done and we are
  replaying it. We need to walk the BMBT of the appropriate fork and change the
  owners of it.
  The complexity here is that we don't have an inode context to work with, so
  after we've replayed the inode we need to instantiate one.  This is where the
  fun begins.
  We are in the middle of log recovery, so we can't run transactions. That
  means we cannot use cache coherent inode instantiation via xfs_iget(), as
  that will result in the corresponding iput() running the inode through
  xfs_inactive(). If we've just replayed an inode core that changes the link
  count to zero (i.e. it's been unlinked), then xfs_inactive() will run
  transactions (bad!).
  So, to avoid this, we instantiate an inode directly from the inode core we've
  just recovered. We have the buffer still locked, and all we really need to
  instantiate is the inode core and the forks being modified. We can do this
  manually, then run the inode btree owner change, and then tear down the
  xfs_inode without having to run any transactions at all.
  Also, because we don't have a transaction context available here but need to
  gather all the buffers we modify for writeback so we pass the buffer_list
  instead for the operation to use.
 instantiate the inode 
 Convert a log timestamp to an ondisk timestamp. 
	
	  Inode buffers can be freed, look out for it,
	  and do not replay the inode.
	
	  Make sure the place we're flushing out to really looks
	  like an inode!
	
	  If the inode has an LSN in it, recover the inode only if the on-disk
	  inode's LSN is older than the lsn of the transaction we are
	  replaying. We can have multiple checkpoints with the same start LSN,
	  so the current LSN being equal to the on-disk LSN doesn't necessarily
	  mean that the on-disk inode is more recent than the change being
	  replayed.
	 
	  We must check the current_lsn against the on-disk inode
	  here because the we can't trust the log dinode to contain a valid LSN
	  (see comment below before replaying the log dinode for details).
	 
	  Note: we still need to replay an owner change even though the inode
	  is more recent than the transaction as there is no guarantee that all
	  the btree blocks are more recent than this transaction, too.
	
	  di_flushiter is only valid for v12 inodes. All changes for v3 inodes
	  are transactional and if ordering is necessary we can determine that
	  more accurately by the LSN field in the V3 inode core. Don't trust
	  the inode versions we might be changing them here - use the
	  superblock flag to determine whether we need to look at di_flushiter
	  to skip replay when the on disk inode is newer than the log one
		
		  Deal with the wrap case, DI_MAX_FLUSH is less
		  than smaller numbers
 do nothing 
 Take the opportunity to reset the flush iteration count 
	
	  Recover the log dinode inode into the on disk inode.
	 
	  The LSN in the log dinode is garbage - it can be zero or reflect
	  stale in-memory runtime state that isn't coherent with the changes
	  logged in this transaction or the changes written to the on-disk
	  inode.  Hence we write the current lSN into the inode because that
	  matches what xfs_iflush() would write inode the inode when flushing
	  the changes in this transaction.
		
		  There are no data fork flags set.
	
	  If we logged any attribute data, recover it.  There may or
	  may not have been any other non-core data logged in this
	  transaction.
 Recover the swapext owner change unless inode has been deleted 
 re-generate the checksum. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  The global quota manager. There is only one of these for the entire
  system, _not_ one per file system. XQM keeps track of the overall
  quota functionality, including maintaining the freelist and hash
  tables of dquots.
  We use the batch lookup interface to iterate over the dquots as it
  currently is the only interface into the radix tree code that allows
  fuzzy lookups instead of exact matches.  Holding the lock over multiple
  operations is fine as all callers are used either during mountumount
  or quotaoff.
 bail out if the filesystem is corrupted.  
 we're done if id overflows back to zero 
  Purge a dquot from all tracking data structures and free it.
	
	  If we are turning this type of quotas off, we don't care
	  about the dirty metadata sitting in this dquot. OTOH, if
	  we're unmounting, we do care, so we flush it and wait.
		
		  We don't care about getting disk errors here. We need
		  to purge this dquot anyway, so we go ahead regardless.
	
	  We move dquots to the freelist as soon as their reference count
	  hits zero, so it really should be on the freelist here.
  Purge the dquot cache.
  Just destroy the quotainfo structure.
  Called from the vfsops layer.
	
	  Release the dquots that root inode, et al might be holding,
	  before we flush quotas and blow away the quotainfo structure.
	
	  Release the quota inodes.
	
	  See if we already have it in the inode itself. IO_idqpp is &i_udquot
	  or &i_gdquot. This made the code look weird, but made the logic a lot
	  simpler.
	
	  Find the dquot from somewhere. This bumps the reference count of
	  dquot and returns it locked.  This can return ENOENT if dquot didn't
	  exist on disk and we didn't ask it to allocate; ESRCH if quotas got
	  turned off suddenly.
	
	  dqget may have dropped and re-acquired the ilock, but it guarantees
	  that the dquot returned is the one that should go in the inode.
  Given a locked inode, attach dquot(s) to it, taking UGP-QUOTAON
  into account.
  If @doalloc is true, the dquot(s) will be allocated if needed.
  Inode may get unlocked and relocked in here, and the caller must deal with
  the consequences.
	
	  Don't worry about the dquots that we may have attached before any
	  error - they'll get detached later if it has not already been done.
  Release dquots (and their references) if any.
  The inode should be locked EXCL except when this's called by
  xfs_ireclaim.
	
	  This dquot has acquired a reference in the meantime remove it from
	  the freelist and try again.
	
	  If the dquot is dirty, flush it. If it's already being flushed, just
	  skip it so there is time for the IO to complete before we try to
	  reclaim it again on the next LRU pass.
 we have to drop the LRU lock to flush the dquot 
	
	  Prevent lookups now that we are past the point of no return.
	
	  Timers and warnings have been already set, let's just set the
	  default limits for this quota type
 Initialize quota time limits from the root dquot. 
	
	  We try to get the limits from the superuser's limits fields.
	  This is quite hacky, but it is standard quota practice.
	 
	  Since we may not have done a quotacheck by this point, just read
	  the dquot without attaching it to any hashtables or lists.
	
	  The warnings and timers set the grace period given to
	  a user or group before he or she can not perform any
	  more writing. If it is zero, a default is used.
  This initializes all the quota information that's kept in the
  mount structure
	
	  See if quotainodes are setup, and if not, allocate them,
	  and change the superblock accordingly.
 mutex used to serialize quotaoffs 
 Precalc some constants 
  Gets called when unmounting a filesystem or when all quotas get
  turned off.
  This purges the quota inodes, destroys locks and frees itself.
  Create an inode and return with a reference already taken, but unlocked
  This is how we create quota inodes
	
	  With superblock that doesn't have separate pquotino, we
	  share an inode between gquota and pquota. If the on-disk
	  superblock has GQUOTA and the filesystem is now mounted
	  with PQUOTA, just use sb_gquotino for sb_pquotino and
	  vice-versa.
	
	  Make the changes in the superblock, and log those too.
	  sbfields arg may contain fields other than QUOTINO;
	  VERSIONNUM for example.
 qflags will get updated fully _after_ quotacheck 
	
	  Reset all counters and timers. They'll be
	  started afresh by xfs_qm_quotacheck.
		
		  Do a sanity check, and if needed, repair the dqblk. Don't
		  output any warnings because it's perfectly possible to
		  find uninitialised dquot blks. See comment in
		  xfs_dquot_verify.
		
		  Reset type in case we are reusing group quota file for
		  project quotas or vice versa
		
		  dquot id 0 stores the default grace period and the maximum
		  warning limit that were set by the administrator, so we
		  should not reset them.
	
	  Blkcnt arg can be a very big number, and might even be
	  larger than the log itself. So, we have to break it up into
	  manageable-sized transactions.
	  Note that we don't start a permanent transaction here; we might
	  not be able to get a log reservation for the whole thing up front,
	  and we don't really care to either, because we just discard
	  everything if we were to crash in the middle of this loop.
		
		  CRC and validation errors will return a EFSCORRUPTED here. If
		  this occurs, re-read without CRC validation so that we can
		  repair the damage via xfs_qm_reset_dqcounts(). This process
		  will leave a trace in the log indicating corruption has
		  been detected.
		
		  A corrupt buffer might not have a verifier attached, so
		  make sure we have the correct one attached before writeback
		  occurs.
 goto the next block. 
  Iterate over all allocated dquot blocks in this quota inode, zeroing all
  counters for every chunk of dquots that we find.
 number of map entries 
 return value 
	
	  This looks racy, but we can't keep an inode lock across a
	  trans_reserve. But, this gets called during quotacheck, and that
	  happens only at mount time which is single threaded.
		
		  We aren't changing the inode itself. Just changing
		  some of its data. No new blocks are added here, and
		  the inode is never added to the transaction.
			
			  Do a read-ahead on the next extent.
			
			  Iterate thru all the blks in the extent and
			  reset the counters of all the dquots inside them.
  Called by dqusage_adjust in doing a quotacheck.
  Given the inode, and a dquot id this updates both the incore dqout as well
  as the buffer copy. This is so that once the quotacheck is done, we can
  just log all the buffers, as opposed to logging numerous updates to
  individual dquots.
		
		  Shouldn't be able to turn off quotas here.
	
	  Adjust the inode count and the block count to reflect this inode's
	  resource usage.
	
	  Set default limits, adjust timers (since we changed usages)
	 
	  There are no timers for the default values set in the root dquot.
  callback routine supplied to bulkstat(). Given an inumber, find its
  dquots and update them to account for resources taken by that inode.
 ARGSUSED 
 total rt blks 
	
	  rootino must have its resources accounted for, not so with the quota
	  inodes.
	
	  We don't _need_ to take the ilock EXCL here because quotacheck runs
	  at mount time and therefore nobody will be racing chownchproj.
	
	  Add the (disk blocks and inode) resources occupied by this
	  inode to its dquots. We do this adjustment in the incore dquot,
	  and also copy the changes to its buffer.
	  We don't care about putting these changes in a transaction
	  envelope because if we crash in the middle of a 'quotacheck'
	  we have to start from the beginning anyway.
	  Once we're done, we'll log all the dquot bufs.
	 
	  The QUOTA_ON checks below may look pretty racy, but quotachecks
	  and quotaoffs don't race. (Quotachecks happen at mount time only).
	
	  The only way the dquot is already flush locked by the time quotacheck
	  gets here is if reclaim flushed it before the dqadjust walk dirtied
	  it for the final time. Quotacheck collects all dquot bufs in the
	  local delwri queue before dquots are dirtied, so reclaim can't have
	  possibly queued it for IO. The only way out is to push the buffer to
	  cycle the flush lock.
 buf is pinned in-core by delwri list 
  Walk thru all the filesystem inodes and construct a consistent view
  of the disk quota world. If the quotacheck fails, disable quotas.
	
	  First we go thru all the dquots on disk, USR and GRPPRJ, and reset
	  their counters to zero. We need a clean slate.
	  We don't log our changes till later.
	
	  We've made all the changes that we need to make incore.  Flush them
	  down to disk buffers if everything was updated successfully.
	
	  We can get this error if we couldn't do a dquot allocation inside
	  xfs_qm_dqusage_adjust (via bulkstat). We don't care about the
	  dirty dquots that might be cached, we just want to get rid of them
	  and turn quotaoff. The dquots won't be attached to any of the inodes
	  at this point (because we intentionally didn't in dqget_noattach).
	
	  If one type of quotas is off, then it will lose its
	  quotachecked status, since we won't be doing accounting for
	  that type anymore.
		
		  We must turn off quotas.
  This is called from xfs_mountfs to start quotas and initialize all
  necessary data structures like quotainfo.  This is also responsible for
  running a quotacheck as necessary.  We are guaranteed that the superblock
  is consistently read in at this point.
  If we fail here, the mount will continue with quota turned off. We don't
  need to inidicate success or failure at all.
	
	  If quotas on realtime volumes is not supported, we disable
	  quotas immediately.
	
	  Allocate the quotainfo structure inside the mount struct, and
	  create quotainode(s), and changerev superblock if necessary.
		
		  We must turn off quotas.
	
	  If any of the quotas are not consistent, do a quotacheck.
 Quotacheck failed and disabled quotas. 
	
	  If one type of quotas is off, then it will lose its
	  quotachecked status, since we won't be doing accounting for
	  that type anymore.
	
	  We actually don't have to acquire the m_sb_lock at all.
	  This can only be called from mount, and that's single threaded. XXX
			
			  We could only have been turning quotas off.
			  We aren't in very good shape actually because
			  the incore structures are convinced that quotas are
			  off, but the on disk superblock doesn't know that !
  This is called after the superblock has been read in and we're ready to
  iget the quota inodes.
	
	  Get the uquota and gquota inodes
	
	  Create the three inodes, if they don't exist already. The changes
	  made above will get added to a transaction and logged in one of
	  the qino_alloc calls below.  If the device is readonly,
	  temporarily switch to read-write to do this.
 paranoia 
 --------------- utility functions for vnodeops ---------------- 
  Given an inode, a uid, gid and prid make sure that we have
  allocated relevant dquot(s) on disk, and that we won't exceed inode
  quotas by creating this file.
  This also attaches dquot(s) to the given inode after locking it,
  and returns the dquots corresponding to the uid andor gid.
  in	: inode (unlocked)
  out	: udquot, gdquot with references taken and unlocked
	
	  Attach the dquot(s) to this inode, doing a dquot allocation
	  if necessary. The dquot(s) will not be locked.
			
			  What we need is the dquot that has this uid, and
			  if we send the inode to dqget, the uid of the inode
			  takes priority over what's sent in the uid argument.
			  We must unlock inode here before calling dqget if
			  we're not sending the inode, because otherwise
			  we'll deadlock by doing trans_reserve while
			  holding ilock.
			
			  Get the ilock in the right order.
			
			  Take an extra reference, because we'll return
			  this to caller
  Actually transfer ownership, and do dquot modifications.
  These were already reserved.
 old dquot 
 the sparkling new dquot 
	
	  Back when we made quota reservations for the chown, we reserved the
	  ondisk blocks + delalloc blocks with the new dquot.  Now that we've
	  switched the dquots, decrease the new dquot's block reservation
	  (having already bumped up the real counter) so that we don't have
	  any reservation to give back when we commit.
	
	  Give the incore reservation for delalloc blocks back to the old
	  dquot.  We don't normally handle delalloc quota reservations
	  transactionally, so just lock the dquot and subtract from the
	  reservation.  Dirty the transaction because it's too late to turn
	  back now.
	
	  Take an extra reference, because the inode is going to keep
	  this dquot pointer even after the trans_commit.
		
		  Watch out for duplicate entries in the table.
 Decide if this inode's dquot is near an enforcement boundary. 
 We only care for quotas that are enabled and enforced. 
 For space on the data device, check the various thresholds. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
 Local miscellaneous function prototypes 
 local state machine functions 
		
		  There is a chance that the size of the CIL checkpoints in
		  progress at the last AIL push target calculation resulted in
		  limiting the target to the log head (l_last_sync_lsn) at the
		  time. This may not reflect where the log head is now as the
		  CIL checkpoints may have completed.
		 
		  Hence when we are woken here, it may be that the head of the
		  log that has moved rather than the tail. As the tail didn't
		  move, there still won't be space available for the
		  reservation we require.  However, if the AIL has already
		  pushed to the target defined by the old log head location, we
		  will hang here waiting for something else to update the AIL
		  push target.
		 
		  Therefore, if there isn't space to wake the first waiter on
		  the grant head, we need to push the AIL again to ensure the
		  target reflects both the current log tail and log head
		  position before we wait for the tail to move again.
  Atomically get the log space required for a log ticket.
  Once a ticket gets put onto head->waiters, it will only return after the
  needed reservation is satisfied.
  This function is structured so that it has a lock free fast path. This is
  necessary because every new transaction reservation will come through this
  path. Hence any lock will be globally hot if we take it unconditionally on
  every pass.
  As tickets are only ever moved on and off head->waiters under head->lock, we
  only need to take that lock if we are going to add the ticket to the queue
  and sleep. We can avoid taking the lock if the ticket was never added to
  head->waiters because the t_queue list head will be empty and we hold the
  only reference to it so it can safely be checked unlocked.
	
	  If there are other waiters on the queue then give them a chance at
	  logspace before us.  Wake up the first waiters, if we do not wake
	  up all the waiters then go to sleep waiting for more free space,
	  otherwise try to get some space for this transaction.
 add to overflow and start again 
	
	  Do not write to the log on norecovery mounts, if the data or log
	  devices are read-only, or if the filesystem is shutdown. Read-only
	  mounts allow internal writes for log recovery and unmount purposes,
	  so don't restrict that case.
  Replenish the byte reservation required by moving the grant write head.
	
	  This is a new transaction on the ticket, so we need to change the
	  transaction ID so that the next transaction has a different TID in
	  the log. Just add one to the existing tid so that we can see chains
	  of rolling transactions in the log easily.
	
	  If we are failing, make sure the ticket doesn't have any current
	  reservations.  We don't want to add this back when the ticket
	  transaction gets cancelled.
 ungrant will give back unit_res  t_cnt. 
  Reserve log space and return a ticket corresponding to the reservation.
  Each reservation is going to reserve extra space for a log record header.
  When writes happen to the on-disk log, we don't subtract the length of the
  log record header from any reservation.  By wasting space in each
  reservation, we prevent over allocation problems.
	
	  If we are failing, make sure the ticket doesn't have any current
	  reservations.  We don't want to add this back when the ticket
	  transaction gets cancelled.
 ungrant will give back unit_res  t_cnt. 
  Run all the pending iclog callbacks and wake log force waiters and iclog
  space waiters so they can process the newly set shutdown state. We really
  don't care what order we process callbacks here because the log is shut down
  and so state cannot change on disk anymore.
  We avoid processing actively referenced iclogs so that we don't run callbacks
  while the iclog owner might still be preparing the iclog for IO submssion.
  These will be caught by xlog_state_iclog_release() and call this function
  again to process any callbacks that may have been added to that iclog.
 Reference holder will re-run iclog callbacks. 
  Flush iclog to disk if this is the last reference to the given iclog and the
  it is in the WANT_SYNC state.
  If the caller passes in a non-zero @old_tail_lsn and the current log tail
  does not match, there may be metadata on disk that must be persisted before
  this iclog is written.  To satisfy that requirement, set the
  XLOG_ICL_NEED_FLUSH flag as a condition for writing this iclog with the new
  log tail value.
  If XLOG_ICL_NEED_FUA is already set on the iclog, we need to ensure that the
  log tail is updated correctly. NEED_FUA indicates that the iclog will be
  written to stable storage, and implies that a commit record is contained
  within the iclog. We need to ensure that the log tail does not move beyond
  the tail that the first commit record in the iclog ordered against, otherwise
  correct recovery of that checkpoint becomes dependent on future operations
  performed on this iclog.
  Hence if NEED_FUA is set and the current iclog tail lsn is empty, write the
  current tail into iclog. Once the iclog tail is set, future operations must
  not modify it, otherwise they potentially violate ordering constraints for
  the checkpoint commit that wrote the initial tail lsn value. The tail lsn in
  the iclog will get zeroed on activation of the iclog after sync, so we
  always capture the tail lsn on the iclog on the first NEED_FUA release
  regardless of the number of active reference counts on this iclog.
	
	  Grabbing the current log tail needs to be atomic w.r.t. the writing
	  of the tail LSN into the iclog so we guarantee that the log tail does
	  not move between deciding if a cache flush is required and writing
	  the LSN into the iclog below.
		
		  If there are no more references to this iclog, process the
		  pending iclog callbacks that were waiting on the release of
		  this iclog.
  Mount a log filesystem
  mp		- ubiquitous xfs mount point structure
  log_target	- buftarg of on-disk log device
  blk_offset	- Start block # where block size is 512 bytes (BBSIZE)
  num_bblocks	- Number of BBSIZE blocks in on-disk log
  Return error or zero.
	
	  Validate the given log space and drop a critical message via syslog
	  if the log size is too small that would lead to some unexpected
	  situations in transaction log space reservation stage.
	 
	  Note: we can't just reject the mount if the validation fails.  This
	  would mean that people would have to downgrade their kernel just to
	  remedy the situation as there is no way to grow the log (short of
	  black magic surgery with xfs_db).
	 
	  We can, however, reject mounts for CRC format filesystems, as the
	  mkfs binary being used to make the filesystem should never create a
	  filesystem with a log that is too small.
		
		  Log check errors are always fatal on v5; or whenever bad
		  metadata leads to a crash.
	
	  Initialize the AIL now we have a log.
	
	  skip log recovery on a norecovery mount.  pretend it all
	  just worked.
		
		  log recovery ignores readonly state and so we need to clear
		  mount-based read only state so it can write to disk.
 Normal transactions can now occur 
	
	  Now the log has been fully initialised and we know were our
	  space grant counters are, we can initialise the permanent ticket
	  needed for delayed logging to work.
  Finish the recovery of the file system.  This is separate from the
  xfs_log_mount() call, because it depends on the code in xfs_mountfs() to read
  in the root and real-time bitmap inodes between calling xfs_log_mount() and
  here.
  If we finish recovery successfully, start the background log work. If we are
  not doing recovery, then we have a RO filesystem and we don't need to start
  it.
	
	  log recovery ignores readonly state and so we need to clear
	  mount-based read only state so it can write to disk.
	
	  During the second phase of log recovery, we need iget and
	  iput to behave like they do for an active filesystem.
	  xfs_fs_drop_inode needs to be able to prevent the deletion
	  of inodes before we're done replaying log items on those
	  inodes.  Turn it off immediately after recovery finishes
	  so that we don't leak the quota inodes if subsequent mount
	  activities fail.
	 
	  We let all inodes involved in redo item processing end up on
	  the LRU instead of being evicted immediately so that if we do
	  something to an unlinked inode, the irele won't cause
	  premature truncation and freeing of the inode, which results
	  in log recovery failure.  We have to evict the unreferenced
	  lru inodes after clearing SB_ACTIVE because we don't
	  otherwise clean up the lru if there's a subsequent failure in
	  xfs_mountfs, which leads to us leaking the inodes if nothing
	  else (e.g. quotacheck) references the inodes before the
	  mount failure occurs.
	
	  Drain the buffer LRU after log recovery. This is required for v4
	  filesystems to avoid leaving around buffers with NULL verifier ops,
	  but we do it unconditionally to make sure we're always in a clean
	  cache state after mount.
	 
	  Don't push in the error case because the AIL may have pending intents
	  that aren't removed until recovery is cancelled.
 Make sure the log is dead if we're returning failure. 
  The mount has failed. Cancel the recovery if it hasn't completed and destroy
  the log.
  Flush out the iclog to disk ensuring that device caches are flushed and
  the iclog hits stable storage before any completion waiters are woken.
  Wait for the iclog and all prior iclogs to be written disk as required by the
  log force state machine. Waiting on ic_force_wait ensures iclog completions
  have been ordered and callbacks run before we are woken here, hence
  guaranteeing that all the iclogs up to this one are on stable storage.
  Write out an unmount record using the ticket provided. We have to account for
  the data space used in the unmount ticket as this write is not done from a
  transaction context that has already done the accounting for us.
 account for space used by record data 
  Mark the filesystem clean by writing an unmount record to the head of the
  log.
	
	  At this point, we're umounting anyway, so there's no point in
	  transitioning log state to shutdown. Just continue...
  Unmount record used to have a string "Unmount filesystem--" in the
  data section where the "Un" was really a magic number (XLOG_UNMOUNT_TYPE).
  We just write the magic number now since that particular field isn't
  currently architecture converted and "Unmount" is a bit foo.
  As far as I know, there weren't any dependencies on the old behaviour.
	
	  If we think the summary counters are bad, avoid writing the unmount
	  record to force log recovery at next mount, after which the summary
	  counters will be recalculated.  Refer to xlog_check_unmount_rec for
	  more details.
  Empty the log for unmountfreeze.
  To do this, we first need to shut down the background log work so it is not
  trying to cover the log as we clean up. We then need to unpin all objects in
  the log so we can then flush them out. Once they have completed their IO and
  run the callbacks removing themselves from the AIL, we can cover the log.
	
	  Clear log incompat features since we're quiescing the log.  Report
	  failures, though it's not fatal to have a higher log feature
	  protection level than the log contents actually require.
	
	  The superblock buffer is uncached and while xfs_ail_push_all_sync()
	  will push it, xfs_buftarg_wait() will not wait for it. Further,
	  xfs_buf_iowait() cannot be used because it was pushed with the
	  XBF_ASYNC flag set, so we need to use a lockunlock pair to wait for
	  the IO to complete.
  Shut down and release the AIL and Log.
  During unmount, we need to ensure we flush all the dirty metadata objects
  from the AIL so that the log is empty before we write the unmount record to
  the log. Once this is done, we can tear down the AIL and the log.
  Wake up processes waiting for log space after we have moved the log tail.
  Determine if we have a transaction that has gone to disk that needs to be
  covered. To begin the transition to the idle state firstly the log needs to
  be idle. That means the CIL, the AIL and the iclogs needs to be empty before
  we start attempting to cover the log.
  Only if we are then in a state where covering is needed, the caller is
  informed that dummy transactions are required to move the log into the idle
  state.
  If there are any items in the AIl or CIL, then we do not want to attempt to
  cover the log as we may be in a situation where there isn't log space
  available to run a dummy transaction and this can lead to deadlocks when the
  tail of the log is pinned by an item that is modified in the CIL.  Hence
  there's no point in running a dummy transaction at this point because we
  can't start trying to idle the log until both the CIL and AIL are empty.
  Explicitly cover the log. This is similar to background log covering but
  intended for usage in quiesce codepaths. The caller is responsible to ensure
  the log is idle and suitable for covering. The CIL, iclog buffers and AIL
  must all be empty.
	
	  xfs_log_need_covered() is not idempotent because it progresses the
	  state machine if the log requires covering. Therefore, we must call
	  this function once and use the result until we've issued an sb sync.
	  Do so first to make that abundantly clear.
	 
	  Fall into the covering sequence if the log needs covering or the
	  mount has lazy superblock accounting to sync to disk. The sb sync
	  used for covering accumulates the in-core counters, so covering
	  handles this for us.
	
	  To cover the log, commit the superblock twice (at most) in
	  independent checkpoints. The first serves as a reference for the
	  tail pointer. The sync transaction and AIL push empties the AIL and
	  updates the in-core tail to the LSN of the first checkpoint. The
	  second commit updates the on-disk tail with the in-core LSN,
	  covering the log. Push the AIL one more time to leave it empty, as
	  we found it.
  We may be holding the log iclog lock upon entering this routine.
	
	  To make sure we always have a valid LSN for the log tail we keep
	  track of the last LSN which was committed in log->l_last_sync_lsn,
	  and use that when the AIL was empty.
  Return the space in the log between the tail and the head.  The head
  is passed in the cyclebytes formal parms.  In the special case where
  the reserve head has wrapped passed the tail, this calculation is no
  longer valid.  In this case, just return 0 which means there is no space
  in the log.  This works for all places where this function is called
  with the reserve head.  Of course, if the write head were to ever
  wrap the tail, we should blow up.  Rather than catch this case here,
  we depend on other ASSERTions in other parts of the code.   XXXmiken
  If reservation head is behind the tail, we have a problem. Warn about it,
  but then treat it as if the log is empty.
  If the log is shut down, the head and tail may be invalid or out of whack, so
  shortcut invalidity asserts in this case so that we don't trigger them
  falsely.
 Ignore potential inconsistency when shutdown. 
	
	  The reservation head is behind the tail. In this case we just want to
	  return the size of the log as the amount of space left.
 treat writes with injected CRC errors as failed 
	
	  Race to shutdown the filesystem if we see an error.
	
	  Drop the lock to signal that we are done. Nothing references the
	  iclog after this, so an unmount waiting on this lock can now tear it
	  down safely. As such, it is unsafe to reference the iclog after the
	  unlock as we could race with it being freed.
  Return size of each in-core log record buffer.
  All machines get 8 x 32kB buffers by default, unless tuned otherwise.
  If the filesystem blocksize is too large, we may need to choose a
  larger size since the directory code currently logs entire blocks.
	
	  # headers = size  32k - one header holds cycles from 32k of data.
  Clear the log incompat flags if we have the opportunity.
  This only happens if we're about to log the second dummy transaction as part
  of covering the log and we can get the log incompat feature usage lock.
  Every sync period we need to unpin all items in the AIL and push them to
  disk. If there is nothing dirty, then we might need to cover the log to
  indicate that the filesystem is idle.
 dgc: errors ignored - not fatal and nowhere to report them 
		
		  Dump a transaction into the log that contains no real change.
		  This is needed to stamp the current tail LSN into the log
		  during the covering operation.
		 
		  We cannot use an inode here for this - that will push dirty
		  state back up into the VFS and then periodic inode flushing
		  will prevent log covering from making progress. Hence we
		  synchronously log the superblock instead to ensure the
		  superblock is immediately unpinned and can be written back.
 start pushing all the metadata that is currently dirty 
 queue us up again 
  This routine initializes some of the log structure for a given mount point.
  Its primary purpose is to fill in enough, so recovery can occur.  However,
  some other stuff may be filled in too.
 log->l_tail_lsn = 0x100000000LL; cycle = 1; current block = 0 
 0 is bad since this is initial value 
 for larger sector sizes, must have v2 or external log 
	
	  The amount of memory to allocate for the iclog structure is
	  rather funky due to the way the structure is defined.  It is
	  done this way so that we can use different sizes for machines
	  with different amounts of memory.  See the definition of
	  xlog_in_core_t in xfs_log_priv.h for details.
 new fields 
 complete ring 
 re-write 1st prev ptr 
 xlog_alloc_log 
  Compute the LSN that we'd need to push the log tail towards in order to have
  (a) enough on-disk log space to log the number of bytes specified, (b) at
  least 25% of the log space free, and (c) at least 256 blocks free.  If the
  log free space already meets all three thresholds, this function returns
  NULLCOMMITLSN.
	
	  Set the threshold for the minimum number of free blocks in the
	  log to the maximum of what the caller needs, one quarter of the
	  log, and 256 blocks.
	
	  Don't pass in an lsn greater than the lsn of the last
	  log record known to be on disk. Use a snapshot of the last sync lsn
	  so that it doesn't change between the compare and the set.
  Push the tail of the log if we need to do so to maintain the free log space
  thresholds set out by xlog_grant_push_threshold.  We may need to adopt a
  policy which pushes on an lsn which is further along in the log once we
  reach the high water mark.  In this manner, we would be creating a low water
  mark.
	
	  Get the transaction layer to kick the dirty buffers out to
	  disk asynchronously. No point in trying to do this if
	  the filesystem is shutting down.
  Stamp cycle number in every block
  Calculate the checksum for a log buffer.
  This is a little more complicated than it should be because the various
  headers and the actual data are non-contiguous.
 first generate the crc for the record header ... 
 ... then for additional cycle data for v2 logs ... 
 ... and finally for the payload 
	
	  We lock the iclogbufs here so that we can serialise against IO
	  completion during unmount.  We might be processing a shutdown
	  triggered during unmount, and that can occur asynchronously to the
	  unmount thread, and hence we need to ensure that completes before
	  tearing down the iclogbufs.  Hence we need to hold the buffer lock
	  across the log IO to archieve that.
		
		  It would seem logical to return EIO here, but we rely on
		  the log state machine to propagate IO errors instead of
		  doing it here.  We kick of the state machine and unlock
		  the buffer manually, the code needs to be kept in sync
		  with the IO completion path.
	
	  We use REQ_SYNC | REQ_IDLE here to tell the block layer the are more
	  IOs coming immediately after this one. This prevents the block layer
	  writeback throttle from throttling log writes behind background
	  metadata writeback and causing priority inversions.
		
		  For external log devices, we also need to flush the data
		  device cache first to ensure all metadata writeback covered
		  by the LSN in this iclog is on stable storage. This is slow,
		  but it must complete before we issue the external log IO.
	
	  If this log buffer would straddle the end of the log we will have
	  to split it up into two bios, so that we can continue at the start.
 restart at logical offset zero for the remainder 
  We need to bump cycle number for the part of the iclog that is
  written to the start of the log. Watch out for the header magic
  number case, though.
 Add for LR header 
  Flush out the in-core log (iclog) to the on-disk log in an asynchronous 
  fashion.  Previously, we should have moved the current iclog
  ptr in the log to point to the next available iclog.  This allows further
  write to continue while this code syncs out an iclog ready to go.
  Before an in-core log can be written out, the data section must be scanned
  to save away the 1st word of each BBSIZE block into the header.  We replace
  it with the current cycle count.  Each BBSIZE block is tagged with the
  cycle count because there in an implicit assumption that drives will
  guarantee that entire 512 byte blocks get written at once.  In other words,
  we can't have part of a 512 byte block written and part not written.  By
  tagging each block, we will know which blocks are valid when recovering
  after an unclean shutdown.
  This routine is single threaded on the iclog.  No other thread can be in
  this routine with the same iclog.  Changing contents of iclog can there-
  fore be done without grabbing the state machine lock.  Updating the global
  log will require grabbing the lock though.
  The entire log manager uses a logical block numbering scheme.  Only
  xlog_write_iclog knows about the fact that the log may not start with
  block zero on a given device.
 byte count of bwrite 
 roundoff to BB or stripe 
 move grant heads by roundoff in sync 
 put cycle number in every block 
 real byte length 
 Do we need to split this write into 2 parts? 
 calculcate the checksum 
	
	  Intentionally corrupt the log record CRC based on the error injection
	  frequency, if defined. This facilitates testing log recovery in the
	  event of torn writes. Hence, set the IOABORT state to abort the log
	  write on IO completion and shutdown the fs. The subsequent mount
	  detects the bad CRC and attempts to recover.
  Deallocate a log structure
	
	  Cycle all the iclogbuf locks to make sure all log IO completion
	  is done before we tear down these buffers.
  Update counters atomically now that memcpy is done.
  print out info relating to regions written which consume
  the reservation
 match with XLOG_REG_TYPE_ in xfs_log.h 
  Print a summary of the transaction.
 dump core transaction and ticket info 
 dump each log item 
 dump each iovec for the log item 
  Calculate the potential space needed by the log vector.  We may need a start
  record, and each region gets its own struct xlog_op_header and may need to be
  double word aligned.
 we don't write ordered log vectors 
 are we copying a commit or unmount record? 
	
	  We've seen logs corrupted with bad transaction client ids.  This
	  makes sure that XFS doesn't generate them on.  Turn this into an EIO
	  and shut down the filesystem.
  Set up the parameters of the region copy into the log. This has
  to handle region write split across multiple log buffers - this
  state is kept external to this function so that this code can
  be written in an obvious, self documenting manner.
 write of region completes here 
 partial write of region, needs extra log op header reservation 
 account for new log op header 
		
		  This iclog has already been marked WANT_SYNC by
		  xlog_state_get_iclog_space.
 no more space in this iclog - push it. 
  Write some region out to in-core log
  This will be called when writing externally provided regions or when
  writing out a commit record for a given transaction.
  General algorithm:
 	1. Find total length of this write.  This may include adding to the
 		lengths passed in.
 	2. Check whether we violate the tickets reservation.
 	3. While writing to this iclog
 	    A. Reserve as much space in this iclog as can get
 	    B. If this is first write, save away start lsn
 	    C. While writing this region:
 		1. If first write of transaction, write start record
 		2. Write log operation header (header per region)
 		3. Find out if we can fit entire region into this iclog
 		4. Potentially, verify destination memcpy ptr
 		5. Memcpy (partial) region
 		6. If partial copy, release iclog; otherwise, continue
 			copying more regions into current iclog
 	4. Mark want sync bit (in simulation mode)
 	5. Release iclog for potential flush to on-disk log.
  ERRORS:
  1.	Panic if reservation is overrun.  This should never happen since
 	reservation amounts are generated internal to the filesystem.
  NOTES:
  1. Tickets are single threaded data structures.
  2. The XLOG_END_TRANS & XLOG_CONTINUE_TRANS flags are passed down to the
 	syncing routine.  When a single log_write region needs to span
 	multiple in-core logs, the XLOG_CONTINUE_TRANS bit should be set
 	on all log operation writes which don't contain the end of the
 	region.  The XLOG_END_TRANS bit is used for the in-core log
 	operation which contains the end of the continued log_write region.
  3. When xlog_state_get_iclog_space() grabs the rest of the current iclog,
 	we don't really know exactly how much space will be used.  As a result,
 	we don't update ic_offset until the end when we know exactly how many
 	bytes have been written out.
	
	  If this is a commit or unmount transaction, we don't need a start
	  record to be written.  We do, however, have to account for the
	  commit or unmount header that gets written. Hence we always have
	  to account for an extra xlog_op_header here.
		
		  If we have a context pointer, pass it the first iclog we are
		  writing to so it can record state needed for iclog write
		  ordering.
		
		  This loop writes out as many regions as can fit in the amount
		  of space which was allocated by xlog_state_get_iclog_space().
 ordered log vectors have no regions to write 
			
			  Before we start formatting log vectors, we need to
			  write a start record. Only do this for the first
			  iclog we write to.
			
			  Copy region.
			 
			  Unmount records just log an opheader, so can have
			  empty payloads with no data region to copy. Hence we
			  only copy the payload if the vector says it has data
			  to copy.
			
			  if we had a partial copy, we need to get more iclog
			  space but we don't want to increment the region
			  index because there is still more is this region to
			  write.
			 
			  If we completed writing this region, and we flushed
			  the iclog (indicated by resetting of the record
			  count), then we also need to get more log space. If
			  this was the last record, though, we are done and
			  can just return.
	
	  If the number of ops in this iclog indicate it just contains the
	  dummy transaction, we can change state into IDLE (the second time
	  around). Otherwise we should change the state into NEED a dummy.
	  We don't need to cover the dummy.
		
		  We have two dirty iclogs so start over.  This could also be
		  num of ops indicating this is not the dummy going out.
  Loop through all iclogs and mark all iclogs currently marked DIRTY as
  ACTIVE after iclog IO has completed.
		
		  The ordering of marking iclogs ACTIVE must be maintained, so
		  an iclog doesn't become ACTIVE beyond one that is SYNCING.
	
	  We go to NEED for any non-covering writes. We go to NEED2 if we just
	  wrote the first covering record (DONE). We go to IDLE if we just
	  wrote the second covering record (DONE2) and remain in IDLE until a
	  non-covering write occurs.
  Completion of a iclog IO does not imply that a transaction has completed, as
  transactions can be large enough to span many iclogs. We cannot change the
  tail of the log half way through a transaction as this may be the only
  transaction in the log and moving the tail to point to the middle of it
  will prevent recovery from finding the start of the transaction. Hence we
  should only update the last_sync_lsn if this iclog contains transaction
  completion callbacks on it.
  We have to do this before we drop the icloglock to ensure we are the only one
  that can update it.
  If we are moving the last_sync_lsn forwards, we also need to ensure we kick
  the reservation grant head pushing. This is due to the fact that the push
  target is bound by the current last_sync_lsn value. Hence if we have a large
  amount of log space bound up in this committing transaction then the
  last_sync_lsn value may be the limiting factor preventing tail pushing from
  freeing space in the log. Hence once we've updated the last_sync_lsn we
  should push the AIL to ensure the push target (and hence the grant head) is
  no longer bound by the old log head location and can move forwards and make
  progress again.
  Return true if we need to stop processing, false to continue to the next
  iclog. The caller will need to run callbacks if the iclog is returned in the
  XLOG_STATE_CALLBACK state.
		
		  Skip all iclogs in the ACTIVE & DIRTY states:
		
		  Now that we have an iclog that is in the DONE_SYNC state, do
		  one more check here to see if we have chased our tail around.
		  If this is not the lowest lsn iclog, then we will leave it
		  for another completion to process.
		
		  Can only perform callbacks in order.  Since this iclog is not
		  in the DONE_SYNC state, we skip the rest and just try to
		  clean up.
  Loop over all the iclogs, running attached callbacks on them. Return true if
  we ran any callbacks, indicating that we dropped the icloglock. We don't need
  to handle transient shutdown state here at all because
  xlog_state_shutdown_callbacks() will be run to do the necessary shutdown
  cleanup of the callbacks.
  Loop running iclog completion callbacks until there are no more iclogs in a
  state that can run callbacks.
  Finish transitioning this iclog to the dirty state.
  Callbacks could take time, so they are done outside the scope of the
  global state machine log lock.
	
	  If we got an error, either on the first buffer, or in the case of
	  split log writes, on the second, we shut down the file system and
	  no iclogs should ever be attempted to be written to disk again.
	
	  Someone could be sleeping prior to writing out the next
	  iclog buffer, we wake them all, one will get to do the
	  IO, the others get to wait for the result.
  If the head of the in-core log ring is not (ACTIVE or DIRTY), then we must
  sleep.  We wait on the flush queue on the head iclog as that should be
  the first iclog to complete flushing. Hence if all iclogs are syncing,
  we will wait here and all new writes will sleep until a sync completes.
  The in-core logs are used in a circular fashion. They are not used
  out-of-order even when an iclog past the head is free.
  return:
 	 log_offset where xlog_write() can start writing into the in-core
 		log's data space.
 	 in-core log pointer to which xlog_write() should write.
 	 boolean indicating this is a continued write to an in-core log.
 		If this is the last write, then the in-core log's offset field
 		needs to be incremented, depending on the amount of data which
 		is copied.
 Wait for log writes to have flushed 
 prevents sync 
	 On the 1st write to an iclog, figure out lsn.  This works
	  if iclogs marked XLOG_STATE_WANT_SYNC always write out what they are
	  committing to.  If the offset is set, that's how many blocks
	  must be written.
	 If there is enough room to write everything, then do it.  Otherwise,
	  claim the rest of the region and make sure the XLOG_STATE_WANT_SYNC
	  bit is on, so this will get flushed out.  Don't update ic_offset
	  until you know exactly how many bytes get copied.  Therefore, wait
	  until later to update ic_offset.
	 
	  xlog_write() algorithm assumes that at least 2 xlog_op_header_t's
	  can fit into remaining data section.
		
		  If we are the only one writing to this iclog, sync it to
		  disk.  We need to do an atomic compare and decrement here to
		  avoid racing with concurrent atomic_dec_and_lock() calls in
		  xlog_state_release_iclog() when there is more than one
		  reference to the iclog.
	 Do we have enough room to write the full amount in the remainder
	  of this iclog?  Or must we continue a write on the next iclog and
	  mark this iclog as completely taken?  In the case where we switch
	  iclogs (to mark it taken), this particular iclog will releasesync
	  to disk in xlog_write().
  The first cnt-1 times a ticket goes through here we don't need to move the
  grant write head because the permanent reservation has reserved cnt times the
  unit amount.  Release part of current permanent unit reservation and reset
  current reservation to be one units worth.  Also move grant reservation head
  forward.
 just return if we still have some of the pre-reserved space 
  Give back the space left from a reservation.
  All the information we need to make a correct determination of space left
  is present.  For non-permanent reservations, things are quite easy.  The
  count should have been decremented to zero.  We only need to deal with the
  space remaining in the current reservation part of the ticket.  If the
  ticket contains a permanent reservation, there may be left over space which
  needs to be released.  A count of N means that N-1 refills of the current
  reservation can be done before we need to ask for more space.  The first
  one goes to fill up the first current reservation.  Once we run out of
  space, the count will stay at zero and the only space remaining will be
  in the current reservation field.
	
	  If this is a permanent reservation ticket, we may be able to free
	  up more space based on the remaining count.
  This routine will mark the current iclog in the ring as WANT_SYNC and move
  the current iclog pointer to the next iclog in the ring.
 roll log?: ic_offset changed later 
 Round up to next log-sunit 
		
		  Rewind the current block before the cycle is bumped to make
		  sure that the combined LSN never transiently moves forward
		  when the log wraps to the next cycle. This is to support the
		  unlocked sample of these fields from xlog_valid_lsn(). Most
		  other cases should acquire l_icloglock.
  Force the iclog to disk and check if the iclog has been completed before
  xlog_force_iclog() returns. This can happen on synchronous (e.g.
  pmem) or fast async storage because we drop the icloglock to issue the IO.
  If completion has already occurred, tell the caller so that it can avoid an
  unnecessary wait on the iclog.
	
	  If the iclog has already been completed and reused the header LSN
	  will have been rewritten by completion
  Write out all data in the in-core log as of this exact moment in time.
  Data may be written to the in-core log during this call.  However,
  we don't guarantee this data will be written out.  A change from past
  implementation means this routine will not write out zero length LRs.
  Basically, we try and perform an intelligent scan of the in-core logs.
  If we determine there is no flushable data, we just return.  There is no
  flushable data if:
 	1. the current iclog is active and has no data; the previous iclog
 		is in the active or dirty state.
 	2. the current iclog is drity, and the previous iclog is in the
 		active or dirty state.
  We may sleep if:
 	1. the current iclog is not in the active nor dirty state.
 	2. the current iclog dirty, and the previous iclog is not in the
 		active nor dirty state.
 	3. the current iclog is active, and there is another thread writing
 		to this particular iclog.
 	4. a) the current iclog is active and has no other writers
 	   b) when we return from flushing out this iclog, it is still
 		not in the active nor dirty state.
		
		  If the head is dirty or (active and empty), then we need to
		  look at the previous iclog.
		 
		  If the previous iclog is active or dirty we are done.  There
		  is nothing to sync out. Otherwise, we attach ourselves to the
		  previous iclog and go to sleep.
 We have exclusive access to this iclog. 
			
			  Someone else is still writing to this iclog, so we
			  need to ensure that when they release the iclog it
			  gets synced immediately as we may be waiting on it.
	
	  The iclog we are about to wait on may contain the checkpoint pushed
	  by the above xlog_cil_force() call, but it may not have been pushed
	  to disk yet. Like the ACTIVE case above, we need to make sure caches
	  are flushed when this iclog is written.
  Force the log to a specific LSN.
  If an iclog with that lsn can be found:
 	If it is in the DIRTY state, just return.
 	If it is in the ACTIVE state, move the in-core log into the WANT_SYNC
 		state and go to sleep or return.
 	If it is in any other state, go to sleep or return.
  Synchronous forces are implemented with a wait queue.  All callers trying
  to force a given lsn to disk must wait on the queue attached to the
  specific in-core log.  When given in-core log finally completes its write
  to disk, that thread will wake up all threads waiting on the queue.
		
		  We sleep here if we haven't already slept (e.g. this is the
		  first time we've looked at the correct iclog buf) and the
		  buffer before us is going to be sync'ed.  The reason for this
		  is that if we are doing sync transactions here, by waiting
		  for the previous IO to complete, we can allow a few more
		  transactions into this iclog before we close it down.
		 
		  Otherwise, we mark the buffer WANT_SYNC, and bump up the
		  refcnt so we can release the log (which drops the ref count).
		  The state switch keeps new transaction commits from using
		  this buffer.  When the current commits finish writing into
		  the buffer, the refcount will drop to zero and the buffer
		  will go out then.
		
		  This iclog may contain the checkpoint pushed by the
		  xlog_cil_force_seq() call, but there are other writers still
		  accessing it so it hasn't been pushed to disk yet. Like the
		  ACTIVE case above, we need to make sure caches are flushed
		  when this iclog is written.
		
		  The entire checkpoint was written by the CIL force and is on
		  its way to disk already. It will be stable when it
		  completes, so we don't need to manipulate caches here at all.
		  We just need to wait for completion if necessary.
  Force the log to a specific checkpoint sequence.
  First force the CIL so that all the required changes have been flushed to the
  iclogs. If the CIL force completed it will return a commit LSN that indicates
  the iclog that needs to be flushed to stable storage. If the caller needs
  a synchronous log force, we will wait on the iclog with the LSN returned by
  xlog_cil_force_seq() to be completed.
  Free a used ticket when its refcount falls to zero.
  Figure out the total log space unit (in bytes) that would be
  required for a log ticket.
	
	  Permanent reservations have up to 'cnt'-1 active log operations
	  in the log.  A unit in this case is the amount of space for one
	  of these log operations.  Normal reservations have a cnt of 1
	  and their unit amount is the total amount of space required.
	 
	  The following lines of code account for non-transaction data
	  which occupy space in the on-disk log.
	 
	  Normal form of a transaction is:
	  <oph><trans-hdr><start-oph><reg1-oph><reg1><reg2-oph>...<commit-oph>
	  and then there are LR hdrs, split-recs and roundoff at end of syncs.
	 
	  We need to account for all the leadup data and trailer data
	  around the transaction data.
	  And then we need to account for the worst case in terms of using
	  more space.
	  The worst case will happen if:
	  - the placement of the transaction happens to be such that the
	    roundoff is at its maximum
	  - the transaction data is synced before the commit record is synced
	    i.e. <transaction-data><roundoff> | <commit-rec><roundoff>
	    Therefore the commit record is in its own Log Record.
	    This can happen as the commit record is called with its
	    own region to xlog_write().
	    This then means that in the worst case, roundoff can happen for
	    the commit-rec as well.
	    The commit-rec is smaller than padding in this scenario and so it is
	    not added separately.
 for trans header 
 for start-rec 
	
	  for LR headers - the space for data in an iclog is the size minus
	  the space used for the headers. If we use the iclog size, then we
	  undercalculate the number of headers required.
	 
	  Furthermore - the addition of op headers for split-recs might
	  increase the space required enough to require more log and op
	  headers, so take that into account too.
	 
	  IMPORTANT: This reservation makes the assumption that if this
	  transaction is the first in an iclog and hence has the LR headers
	  accounted to it, then the remaining space in the iclog is
	  exclusively for this transaction.  i.e. if the transaction is larger
	  than the iclog, it will be the only thing in that iclog.
	  Fundamentally, this means we must pass the entire log vector to
	  xlog_write to guarantee this.
 for split-recs - ophdrs added when data split over LRs 
 add extra header reservations if we overrun 
 for commit-rec LR header - note: padding will subsume the ophdr 
 roundoff padding for transaction data and one for commit record 
  Allocate and initialise a new log ticket.
  Make sure that the destination ptr is within the valid data region of
  one of the iclogs.  This uses backup pointers stored in a different
  part of the log in case we trash the log structure.
  Check to make sure the grant write head didn't just over lap the tail.  If
  the cycles are the same, we can't be overlapping.  Otherwise, make sure that
  the cycles differ by exactly one and check the byte count.
  This check is run unlocked, so can give false positives. Rather than assert
  on failures, use a warn-once flag and a panic tag to allow the admin to
  determine if they want to panic the machine when such an error occurs. For
  debug kernels this will have the same effect as using an assert but, unlinke
  an assert, it can be turned off at runtime.
 check if it will fit 
  Perform a number of checks on the iclog before writing to disk.
  1. Make sure the iclogs are still circular
  2. Make sure we have a good magic number
  3. Make sure we don't have magic numbers in the data
  4. Check fields of each log operation header for:
 	A. Valid client identifier
 	B. tid ptr value falls in valid ptr space (user space code)
 	C. Length in log record header is correct according to the
 		individual operation headers within record.
  5. When a bwrite will occur within 5 blocks of the front of the physical
 	log, check the preceding blocks of the physical log to make sure all
 	the cycle numbers agree with the current cycle number.
 check validity of iclog pointers 
 check log magic numbers 
 check fields 
 clientid is only 1 byte 
 check length 
  Perform a forced shutdown on the log. This should be called once and once
  only by the high level filesystem shutdown code to shut the log subsystem
  down cleanly.
  Our main objectives here are to make sure that:
 	a. if the shutdown was not due to a log IO error, flush the logs to
 	   disk. Anything modified after this is ignored.
 	b. the log gets atomically marked 'XLOG_IO_ERROR' for all interested
 	   parties to find out. Nothing new gets queued after this is done.
 	c. Tasks sleeping on log reservations, pinned objects and
 	   other resources get woken up.
  Return true if the shutdown cause was a log IO error and we actually shut the
  log down.
	
	  If this happens during log recovery then we aren't using the runtime
	  log mechanisms yet so there's nothing to shut down.
	
	  Flush all the completed transactions to disk before marking the log
	  being shut down. We need to do this first as shutting down the log
	  before the force will prevent the log force from flushing the iclogs
	  to disk.
	 
	  Re-entry due to a log IO error shutdown during the log force is
	  prevented by the atomicity of higher level shutdown code.
	
	  Atomically set the shutdown state. If the shutdown state is already
	  set, there someone else is performing the shutdown and so we are done
	  here. This should never happen because we should only ever get called
	  once by the first shutdown caller.
	 
	  Much of the log state machine transitions assume that shutdown state
	  cannot change once they hold the log->l_icloglock. Hence we need to
	  hold that lock here, even though we use the atomic test_and_set_bit()
	  operation to set the shutdown state.
	
	  We don't want anybody waiting for log reservations after this. That
	  means we have to wake up everybody queued up on reserveq as well as
	  writeq.  In addition, we make sure in xlog_{re}grant_log_space that
	  we don't enqueue anything once the SHUTDOWN flag is set, and this
	  action is protected by the grant locks.
	
	  Wake up everybody waiting on xfs_log_force. Wake the CIL push first
	  as if the log writes were completed. The abort handling in the log
	  item committed callback functions will do this again under lock to
	  avoid races.
		 endianness does not matter here, zero is zero in
		  any language.
  Verify that an LSN stamped into a piece of metadata is valid. This is
  intended for use in read verifiers on v5 superblocks.
	
	  norecovery mode skips mount-time log processing and unconditionally
	  resets the in-core LSN. We can't validate in this mode, but
	  modifications are not allowed anyways so just return true.
	
	  Some metadata LSNs are initialized to NULL (e.g., the agfl). This is
	  handled by recovery and thus safe to ignore here.
 warn the user about what's gone wrong before verifier failure 
  Notify the log that we're about to start using a feature that is protected
  by a log incompat feature flag.  This will prevent log covering from
  clearing those flags.
 Notify the log that we've finished using log incompat features. 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2016 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Freeing the RUI requires that we remove it from the AIL if it has already
  been placed there. However, the RUI may not yet have been placed in the AIL
  when called by xfs_rui_release() from RUD processing due to the ordering of
  committed vs unpin operations in bulk insert operations. Hence the reference
  count to ensure only the last caller frees the RUI.
  This is called to fill in the vector of log iovecs for the
  given rui log item. We use only 1 iovec, and we point that
  at the rui_log_format structure embedded in the rui item.
  It is at this point that we assert that all of the extent
  slots in the rui item have been filled.
  The unpin operation is the last place an RUI is manipulated in the log. It is
  either inserted in the AIL or aborted in the event of a log IO error. In
  either case, the RUI transaction has been successfully committed to make it
  this far. Therefore, we expect whoever committed the RUI to either construct
  and commit the RUD or drop the RUD's reference in the event of error. Simply
  drop the log's RUI reference now that the log is done with it.
  The RUI has been either committed or aborted if the transaction has been
  cancelled. If the transaction was cancelled, an RUD isn't going to be
  constructed and thus we free the RUI here directly.
  Allocate and initialize an rui item with the given number of extents.
  Copy an RUI format buffer from the given buf, and into the destination
  RUI format structure.  The RUIRUD items were designed not to need any
  special alignment handling.
  This is called to fill in the vector of log iovecs for the
  given rud log item. We use only 1 iovec, and we point that
  at the rud_log_format structure embedded in the rud item.
  It is at this point that we assert that all of the extent
  slots in the rud item have been filled.
  The RUD is either committed or aborted if the transaction is cancelled. If
  the transaction is cancelled, drop our reference to the RUI and free the
  RUD.
 Set the map extent flags for this reverse mapping. 
  Finish an rmap update and log it to the RUD. Note that the transaction is
  marked dirty regardless of whether the rmap update succeeds or fails to
  support the RUIRUD lifecycle rules.
	
	  Mark the transaction dirty, even on error. This ensures the
	  transaction is aborted, which:
	 
	  1.) releases the RUI and frees the RUD
	  2.) shuts down the filesystem
 Sort rmap intents by AG. 
 Log rmap updates in the intent item. 
	
	  atomic_inc_return gives us the value after the increment;
	  we want to use it as an array index so we need to subtract 1 from
	  it.
 Get an RUD so we can process all the deferred rmap updates. 
 Process a deferred rmap update. 
 Abort all pending RUIs. 
 Cancel a deferred rmap update. 
 Is this recovered RUI ok? 
  Process an rmap update intent item that was recovered from the log.
  We need to update the rmapbt.
	
	  First check the validity of the extents described by the
	  RUI.  If any are bad, then assume that all are bad and
	  just toss the RUI.
 Relog an intent item to push the log tail forward. 
  This routine is called to create an in-core extent rmap update
  item from the rui format structure which was logged on disk.
  It allocates an in-core rui, copies the extents from the format
  structure into it, and adds the rui to the AIL with the given
  LSN.
	
	  Insert the intent into the AIL directly and drop one reference so
	  that finishing or canceling the work will drop the other.
  This routine is called when an RUD format structure is found in a committed
  transaction in the log. Its purpose is to cancel the corresponding RUI if it
  was still in the log. To do this it searches the AIL for the RUI with an id
  equal to that in the RUD format structure. If we find it we drop the RUD
  reference, which removes the RUI from the AIL and frees it.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2008, Christoph Hellwig
  All Rights Reserved.
  Return quota status information, such as enforcements, quota file inode
  numbers etc.
  Adjust quota timers & warnings
 Return quota info for active quota >= this qid 
 ID may be different, so convert back what we got 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.
  Copyright (c) 2010 David Chinner.
  Copyright (c) 2011 Christoph Hellwig.
  All Rights Reserved.
 trace before insert to be able to see failed inserts 
  Search for a busy extent within the range of the extent we are about to
  allocate.  You need to be holding the busy extent tree lock when calling
  xfs_extent_busy_search(). This function returns 0 for no overlapping busy
  extent, -1 for an overlapping but not exact busy extent, and 1 for an exact
  match. This is done so that a non-zero return indicates an overlap that
  will require a synchronous transaction, but it can still be
  used to distinguish between a partial or exact match.
 find closest start bno overlap 
 may overlap, but exact start block is lower 
 may overlap, but exact start block is higher 
 bno matches busyp, length determines exact match 
  The found free extent [fbno, fend] overlaps part or all of the given busy
  extent.  If the overlap covers the beginning, the end, or all of the busy
  extent, the overlapping portion can be made unbusy and used for the
  allocation.  We can't split a busy extent because we can't modify a
  transactionCIL context busy list, but we can update an entry's block
  number or length.
  Returns true if the extent can safely be reused, or false if the search
  needs to be restarted.
	
	  This extent is currently being discarded.  Give the thread
	  performing the discard a chance to mark the extent unbusy
	  and retry.
	
	  If there is a busy extent overlapping a user allocation, we have
	  no choice but to force the log and retry the search.
	 
	  Fortunately this does not happen during normal operation, but
	  only if the filesystem is very low on space and has to dip into
	  the AGFL for normal allocations.
		
		  Case 1:
		     bbno           bend
		     +BBBBBBBBBBBBBBBBB+
		         +---------+
		         fbno   fend
		
		  We would have to split the busy extent to be able to track
		  it correct, which we cannot do because we would have to
		  modify the list of busy extents attached to the transaction
		  or CIL context, which is immutable.
		 
		  Force out the log to clear the busy extent and retry the
		  search.
		
		  Case 2:
		     bbno           bend
		     +BBBBBBBBBBBBBBBBB+
		     +-----------------+
		     fbno           fend
		 
		  Case 3:
		     bbno           bend
		     +BBBBBBBBBBBBBBBBB+
		     +--------------------------+
		     fbno                    fend
		 
		  Case 4:
		              bbno           bend
		              +BBBBBBBBBBBBBBBBB+
		     +--------------------------+
		     fbno                    fend
		 
		  Case 5:
		              bbno           bend
		              +BBBBBBBBBBBBBBBBB+
		     +-----------------------------------+
		     fbno                             fend
		 
		
		  The busy extent is fully covered by the extent we are
		  allocating, and can simply be removed from the rbtree.
		  However we cannot remove it from the immutable list
		  tracking busy extents in the transaction or CIL context,
		  so set the length to zero to mark it invalid.
		 
		  We also need to restart the busy extent search from the
		  tree root, because erasing the node can rearrange the
		  tree topology.
		
		  Case 6:
		               bbno           bend
		              +BBBBBBBBBBBBBBBBB+
		              +---------+
		              fbno   fend
		 
		  Case 7:
		              bbno           bend
		              +BBBBBBBBBBBBBBBBB+
		     +------------------+
		     fbno            fend
		 
		
		  Case 8:
		     bbno           bend
		     +BBBBBBBBBBBBBBBBB+
		         +-------------+
		         fbno       fend
		 
		  Case 9:
		     bbno           bend
		     +BBBBBBBBBBBBBBBBB+
		         +----------------------+
		         fbno                fend
  For a given extent [fbno, flen], make sure we can reuse it safely.
  For a given extent [fbno, flen], search the busy extent list to find a
  subset of the extent that is not busy.  If rlen is smaller than
  args->minlen no suitable extent could be found, and the higher level
  code needs to force out the log and retry the allocation.
  Return the current busy generation for the AG if the extent is busy. This
  value can be used to wait for at least one of the currently busy extents
  to be cleared. Note that the busy list is not guaranteed to be empty after
  the gen is woken. The state of a specific extent must always be confirmed
  with another call to xfs_extent_busy_trim() before it can be used.
 start overlap 
			
			  Case 1:
			     bbno           bend
			     +BBBBBBBBBBBBBBBBB+
			         +---------+
			         fbno   fend
			 
			  Case 2:
			     bbno           bend
			     +BBBBBBBBBBBBBBBBB+
			     +-------------+
			     fbno       fend
			 
			  Case 3:
			     bbno           bend
			     +BBBBBBBBBBBBBBBBB+
			         +-------------+
			         fbno       fend
			 
			  Case 4:
			     bbno           bend
			     +BBBBBBBBBBBBBBBBB+
			     +-----------------+
			     fbno           fend
			 
			  No unbusy region in extent, return failure.
			
			  Case 5:
			     bbno           bend
			     +BBBBBBBBBBBBBBBBB+
			         +----------------------+
			         fbno                fend
			 
			  Case 6:
			     bbno           bend
			     +BBBBBBBBBBBBBBBBB+
			     +--------------------------+
			     fbno                    fend
			 
			  Needs to be trimmed to:
			                        +-------+
			                        fbno fend
 end overlap 
			
			  Case 7:
			              bbno           bend
			              +BBBBBBBBBBBBBBBBB+
			     +------------------+
			     fbno            fend
			 
			  Case 8:
			              bbno           bend
			              +BBBBBBBBBBBBBBBBB+
			     +--------------------------+
			     fbno                    fend
			 
			  Needs to be trimmed to:
			     +-------+
			     fbno fend
 middle overlap 
			
			  Case 9:
			              bbno           bend
			              +BBBBBBBBBBBBBBBBB+
			     +-----------------------------------+
			     fbno                             fend
			 
			  Can be trimmed to:
			     +-------+        OR         +-------+
			     fbno fend                   fbno fend
			 
			  Backward allocation leads to significant
			  fragmentation of directories, which degrades
			  directory performance, therefore we always want to
			  choose the option that produces forward allocation
			  patterns.
			  Preferring the lower bno extent will make the next
			  request use "fend" as the start of the next
			  allocation;  if the segment is no longer busy at
			  that point, we'll get a contiguous allocation, but
			  even if it is still busy, we will get a forward
			  allocation.
			  We try to avoid choosing the segment at "bend",
			  because that can lead to the next allocation
			  taking the segment at "fbno", which would be a
			  backward allocation.  We only use the segment at
			  "fbno" if it is much larger than the current
			  requested size, because in that case there's a
			  good chance subsequent allocations will be
			  contiguous.
 left candidate fits perfect 
 right candidate has enough free space 
 left candidate fits minimum requirement 
	
	  Return a zero extent length as failure indications.  All callers
	  re-check if the trimmed extent satisfies the minlen requirement.
  Remove all extents on the passed in list from the busy extents tree.
  If do_discard is set skip extents that need to be discarded, and mark
  these as undergoing a discard operation instead.
  Flush out all busy extents for this AG.
  Callback for list_sort to sort busy extents by the AG they reside in.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2011 Red Hat, Inc.  All Rights Reserved.
  XFS logging functions
 use the more aggressive per-target rate limit for buffers 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Write new AG headers to disk. Non-transactional, but need to be
  written and completed prior to the growfs transaction being logged.
  To do this, we use a delayed write buffer list and wait for
  submission and IO completion of the list as a whole. This allows the
  IO subsystem to merge all the AG headers in a single AG into a single
  IO and hide most of the latency of the IO from us.
  This also means that if we get an error whilst building the buffer
  list to write, we can cancel the entire list without having written
  anything.
  growfs operations
 mount point for filesystem 
 growfs data input struct 
	
	  Reject filesystems with a single AG because they are not
	  supported, and reject a shrink operation that would cause a
	  filesystem to become unsupported.
 allocate the new per-ag structures 
 TODO: shrinking the entire AGs hasn't yet completed 
	
	  Update changed superblock fields transactionally. These are not
	  seen by the rest of the world until the transaction commit applies
	  them atomically to the superblock.
	
	  Sync sb counters now to reflect the updated values. This is
	  particularly important for shrink because the write verifier
	  will fail if sb_fdblocks is ever larger than sb_dblocks.
 New allocation groups fully initialized, so update mount struct 
		
		  If we expanded the last AG, free the per-AG reservation
		  so we can reinitialize it with the new size.
		
		  Reserve AG metadata blocks. ENOSPC here does not mean there
		  was a growfs failure, just that there still isn't space for
		  new user data after the grow has been run.
 mount point for filesystem 
 growfs log input struct 
	
	  Moving the log is hard, need new interfaces to sync
	  the log first, hold off all activity while moving it.
	  Can have shorter or longer log in the same space,
	  or transform internal to external log or vice versa.
  protected versions of growfs function acquire and release locks on the mount
  point - exported through ioctls: XFS_IOC_FSGROWFSDATA, XFS_IOC_FSGROWFSLOG,
  XFS_IOC_FSGROWFSRT
 update imaxpct separately to the physical grow of the filesystem 
 Post growfs calculations needed to reflect new state in operations 
 Update secondary superblocks now the physical grow has completed 
	
	  Increment the generation unconditionally, the error could be from
	  updating the secondary superblocks, in which case the new size
	  is live already.
  exported through ioctl XFS_IOC_FSCOUNTS
  exported through ioctl XFS_IOC_SET_RESBLKS & XFS_IOC_GET_RESBLKS
  xfs_reserve_blocks is called to set m_resblks
  in the in-core mount table. The number of unused reserved blocks
  is kept in m_resblks_avail.
  Reserve the requested number of blocks if available. Otherwise return
  as many as possible to satisfy the request. The actual number
  reserved are returned in outval
  A null inval pointer indicates that only the current reserved blocks
  available  should  be returned no settings are changed.
 If inval is null, report current values and return 
	
	  With per-cpu counters, this becomes an interesting problem. we need
	  to work out if we are freeing or allocation blocks first, then we can
	  do the modification as necessary.
	 
	  We do this under the m_sb_lock so that if we are near ENOSPC, we will
	  hold out any changes while we work out what to do. This means that
	  the amount of free space can change while we do this, so we need to
	  retry if we end up trying to reserve more space than is available.
	
	  If our previous reservation was larger than the current value,
	  then move any unused blocks back to the free pool. Modify the resblks
	  counters directly since we shouldn't have any problems unreserving
	  space.
 release unused blocks 
	
	  If the request is larger than the current reservation, reserve the
	  blocks before we update the reserve counters. Sample m_fdblocks and
	  perform a partial reservation if the request exceeds free space.
 We can't satisfy the request, just get what we can 
		
		  We'll either succeed in getting space from the free block
		  count or we'll get an ENOSPC. If we get a ENOSPC, it means
		  things changed while we were calculating fdblks_delta and so
		  we should try again to see if there is anything left to
		  reserve.
		 
		  Don't set the reserved flag here - we don't want to reserve
		  the extra reserve blocks from the reserve.....
	
	  Update the reserve counters if blocks have been successfully
	  allocated.
  Force a shutdown of the filesystem instantly while keeping the filesystem
  consistent. We don't do an unmount here; just shutdown the shop, make sure
  that absolutely nothing persistent happens to this filesystem after this
  point.
  The shutdown state change is atomic, resulting in the first and only the
  first shutdown call processing the shutdown. This means we only shutdown the
  log once as it requires, and we don't spam the logs when multiple concurrent
  shutdowns race to set the shutdown flags.
  Reserve free space for per-AG metadata.
  Free space reserved for per-AG metadata.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2004-2005 Silicon Graphics, Inc.
  All Rights Reserved.
 The 32-bit variant simply has some padding at the end 
 BROKEN_X86_ALIGNMENT 
 tv_sec differs on 64 vs. 32 
  struct xfs_bstat has differing alignment on intel, & bstime_t sizes
  everywhere
 XFS_IOC_FSBULKSTAT and friends 
 Return 0 on success or positive error (to xfs_bulkstat()) 
 copied from xfs_ioctl.c 
	
	  Output structure handling functions.  Depending on the command,
	  either the xfs_bstat and xfs_inogrp structures are written out
	  to userpace memory via bulkreq.ubuffer.  Normally the compat
	  functions and structure size are the correct ones to use ...
		
		  ... but on x32 the input xfs_fsop_bulkreq has pointers
		  which must be handled in the "compat" (32-bit) way, while
		  the xfs_bstat and xfs_inogrp structures follow native 64-
		  bit layout convention.  So adjust accordingly, otherwise
		  the data written out in compat layout will not match what
		  x32 userspace expects.
 done = 1 if there are more stats to get and if bulkstat 
 should be called again (unused here, but used in dmapi) 
	
	  FSBULKSTAT_SINGLE expects that lastip contains the inode number
	  that we want to stat.  However, FSINUMBERS and FSBULKSTAT expect
	  that lastip contains either zero or the number of the last inode to
	  be examined by the previous call and return results starting with
	  the next inode after that.  The new bulk request back end functions
	  take the inode to start with, so we have to compute the startino
	  parameter from lastino to maintain correct function.  lastino == 0
	  is a special case because it has traditionally meant "first inode
	  in filesystem".
 overflow check 
 long changes size, but xfs only copiese out 32 bits 
 Bulk copy in up to the sx_stat field, then copy bstat 
 try the native version 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.
  Copyright (c) 2008 Dave Chinner
  All Rights Reserved.
  Check that the list is sorted as it should be.
  Called with the ail lock held, but we don't want to assert fail with it
  held otherwise we'll lock everything up and won't be able to debug the
  cause. Hence we sample and check the state under the AIL lock and return if
  everything is fine, otherwise we drop the lock and run the ASSERT checks.
  Asserts may not be fatal, so pick the lock back up and continue onwards.
	
	  Sample then check the next and previous entries are valid.
 !DEBUG 
 DEBUG 
  Return a pointer to the last item in the AIL.  If the AIL is empty, then
  return NULL.
  Return a pointer to the item which follows the given item in the AIL.  If
  the given item is the last item in the list, then return NULL.
  This is called by the log manager code to determine the LSN of the tail of
  the log.  This is exactly the LSN of the first item in the AIL.  If the AIL
  is empty, then this function returns 0.
  We need the AIL lock in order to get a coherent read of the lsn of the last
  item in the AIL.
  Return the maximum lsn held in the AIL, or zero if the AIL is empty.
  The cursor keeps track of where our current traversal is up to by tracking
  the next item in the list for us. However, for this to be safe, removing an
  object from the AIL needs to invalidate any cursor that points to it. hence
  the traversal cursor needs to be linked to the struct xfs_ail so that
  deletion can search all the active cursors for invalidation.
  Get the next item in the traversal and advance the cursor.  If the cursor
  was invalidated (indicated by a lip of 1), restart the traversal.
  When the traversal is complete, we need to remove the cursor from the list
  of traversing cursors.
  Invalidate any cursor that is pointing to this item. This is called when an
  item is removed from the AIL. Any cursor pointing to this object is now
  invalid and the traversal needs to be terminated so it doesn't reference a
  freed object. We set the low bit of the cursor item pointer so we can
  distinguish between an invalidation and the end of the list when getting the
  next item from the cursor.
  Find the first item in the AIL with the given @lsn by searching in ascending
  LSN order and initialise the cursor to point to the next item for a
  ascending traversal.  Pass a @lsn of zero to initialise the cursor to the
  first item in the AIL. Returns NULL if the list is empty.
  Find the last item in the AIL with the given @lsn by searching in descending
  LSN order and initialise the cursor to point to that item.  If there is no
  item with the value of @lsn, then it sets the cursor to the last item with an
  LSN lower than @lsn.  Returns NULL if the list is empty.
  Splice the log item list into the AIL at the given LSN. We splice to the
  tail of the given LSN to maintain insert order for push traversals. The
  cursor is optional, allowing repeated updates to the same LSN to avoid
  repeated traversals.  This should not be called with an empty list.
	
	  Use the cursor to determine the insertion point if one is
	  provided.  If not, or if the one we got is not valid,
	  find the place in the AIL where the items belong.
	
	  If a cursor is provided, we know we're processing the AIL
	  in lsn order, and future items to be spliced in will
	  follow the last one being inserted now.  Update the
	  cursor to point to that last item, now while we have a
	  reliable pointer to it.
	
	  Finally perform the splice.  Unless the AIL was empty,
	  lip points to the item in the AIL _after_ which the new
	  items should go.  If lip is null the AIL was empty, so
	  the new items go at the head of the AIL.
  Delete the given item from the AIL.  Return a pointer to the item.
  Requeue a failed buffer for writeback.
  We clear the log item failed state here as well, but we have to be careful
  about reference counts because the only active reference counts on the buffer
  may be the failed log items. Hence if we clear the log item failed state
  before queuing the buffer for IO we can release all active references to
  the buffer and free it, leading to use after free problems in
  xfs_buf_delwri_queue. It makes no difference to the buffer or log items which
  order we process them in - the buffer is locked, and we own the buffer list
  so nothing on them is going to change while we are performing this action.
  Hence we can safely queue the buffer for IO before we clear the failed log
  item state, therefore  always having an active reference to the buffer and
  avoiding the transient zero-reference state that leads to use-after-free.
 protected by ail_lock 
	
	  If log item pinning is enabled, skip the push and track the item as
	  pinned. This can help induce head-behind-tail conditions.
	
	  Consider the item pinned if a push callback is not defined so the
	  caller will force the log. This should only happen for intent items
	  as they are unpinned once the associated done item is committed to
	  the on-disk log.
	
	  If we encountered pinned items or did not finish writing out all
	  buffers the last time we ran, force a background CIL push to get the
	  items unpinned in the near future. We do not wait on the CIL push as
	  that could stall us for seconds if there is enough background IO
	  load. Stalling for that long when the tail of the log is pinned and
	  needs flushing will hard stop the transaction subsystem when log
	  space runs out.
 barrier matches the ail_target update in xfs_ail_push() 
 we're done if the AIL is empty or our push has reached the end 
		
		  Note that iop_push may unlock and reacquire the AIL lock.  We
		  rely on the AIL cursor implementation to be able to deal with
		  the dropped lock.
			
			  The item or its backing buffer is already being
			  flushed.  The typical reason for that is that an
			  inode buffer is locked because we already pushed the
			  updates to it as part of inode clustering.
			 
			  We do not want to stop flushing just because lots
			  of items are already being flushed, but we need to
			  re-try the flushing relatively soon if most of the
			  AIL is being flushed.
		
		  Are there too many items we can't do anything with?
		 
		  If we are skipping too many items because we can't flush
		  them or they are already being flushed, we back off and
		  given them time to complete whatever operation is being
		  done. i.e. remove pressure from the AIL while we can't make
		  progress so traversals don't slow down further inserts and
		  removals tofrom the AIL.
		 
		  The value of 100 is an arbitrary magic number based on
		  observation.
		
		  We reached the target or the AIL is empty, so wait a bit
		  longer for IO to complete and remove pushed items from the
		  AIL before we start the next scan from the start of the AIL.
		
		  Either there is a lot of contention on the AIL or we are
		  stuck due to operations in progress. "Stuck" in this case
		  is defined as >90% of the items we tried to push were stuck.
		 
		  Backoff a bit more to allow some IO to complete before
		  restarting from the start of the AIL. This prevents us from
		  spinning on the same items, and if they are pinned will all
		  the restart to issue a log force to unpin the stuck items.
		
		  Assume we have more work to do in a short while.
 milliseconds 
		
		  Check kthread_should_stop() after we set the task state to
		  guarantee that we either see the stop bit and exit or the
		  task state is reset to runnable such that it's not scheduled
		  out indefinitely and detects the stop bit at next iteration.
		  A memory barrier is included in above task state set to
		  serialize again kthread_stop().
			
			  The caller forces out the AIL before stopping the
			  thread in the common case, which means the delwri
			  queue is drained. In the shutdown case, the queue may
			  still hold relogged buffers that haven't been
			  submitted because they were pinned since added to the
			  queue.
			 
			  Log IO error processing stales the underlying buffer
			  and clears the delwri state, expecting the buf to be
			  removed on the next submission attempt. That won't
			  happen if we're shutting down, so this is the last
			  opportunity to release such buffers from the queue.
		
		  Idle if the AIL is empty and we are not racing with a target
		  update. We check the AIL after we set the task to a sleep
		  state to guarantee that we either catch an ail_target update
		  or that a wake_up resets the state to TASK_RUNNING.
		  Otherwise, we run the risk of sleeping indefinitely.
		 
		  The barrier matches the ail_target update in xfs_ail_push().
  This routine is called to move the tail of the AIL forward.  It does this by
  trying to flush items in the AIL whose lsns are below the given
  threshold_lsn.
  The push is run asynchronously in a workqueue, which means the caller needs
  to handle waiting on the async flush for space to become available.
  We don't want to interrupt any push that is in progress, hence we only queue
  work if we set the pushing bit appropriately.
  We do this unlocked - we only need to know whether there is anything in the
  AIL at the time we are called. We don't need to access the contents of
  any of the objects, so the lock is not needed.
	
	  Ensure that the new target is noticed in push code before it clears
	  the XFS_AIL_PUSHING_BIT.
  Push out all items in the AIL immediately
  Push out all items in the AIL immediately and wait until the AIL is empty.
 if the tail lsn hasn't changed, don't do updates or wakeups. 
  xfs_trans_ail_update - bulk AIL insertion operation.
  @xfs_trans_ail_update takes an array of log items that all need to be
  positioned at the same LSN in the AIL. If an item is not in the AIL, it will
  be added.  Otherwise, it will be repositioned  by removing it and re-adding
  it to the AIL. If we move the first item in the AIL, update the log tail to
  match the new minimum LSN in the AIL.
  This function takes the AIL lock once to execute the update operations on
  all the items in the array, and as such should not be called with the AIL
  lock held. As a result, once we have the AIL lock, we need to check each log
  item LSN to confirm it needs to be moved forward in the AIL.
  To optimise the insert operation, we delete all the items from the AIL in
  the first pass, moving them into a temporary list, then splice the temporary
  list into the correct position in the AIL. This avoids needing to do an
  insert operation on every item.
  This function must be called with the AIL lock held.  The lock is dropped
  before returning.
 Not required, but true. 
 check if we really need to move the item 
 Insert a log item into the AIL. 
  Delete one log item from the AIL.
  If this item was at the tail of the AIL, return the LSN of the log item so
  that we can use it to check if the LSN of the tail of the log has moved
  when finishing up the AIL delete process in xfs_ail_update_finish().
 xfs_ail_update_finish() drops the AIL lock 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Read and return the summary information for a given extent size,
  bitmap block combination.
  Keeps track of a current summary block, so we don't keep reading
  it from the buffer cache.
 file system mount structure 
 transaction pointer 
 log2 of extent size 
 bitmap block number 
 inout: summary block buffer 
 inout: summary block number 
 out: summary info for this block 
  Return whether there are any free extents in the size range given
  by low and high, for the bitmap block bbno.
 error 
 file system mount structure 
 transaction pointer 
 low log2 extent size 
 high log2 extent size 
 bitmap block number 
 inout: summary block buffer 
 inout: summary block number 
 out: any good extents here? 
 error value 
 loop counter, log2 of ext. size 
 summary data 
 There are no extents at levels < m_rsum_cache[bbno]. 
	
	  Loop over logs of extent sizes.
		
		  Get one summary datum.
		
		  If there are any, return success.
	
	  Found nothing, return failure.
 There were no extents at levels < log. 
  Copy and transform the summary file, given the old and new
  parameters in the mount structures.
 error 
 old file system mount point 
 new file system mount point 
 transaction pointer 
 bitmap block number 
 summary buffer 
 error return value 
 summary level number (log length) 
 summary data 
 summary block number 
  Mark an extent specified by start and len allocated.
  Updates all the summary information as well as the bitmap.
 error 
 file system mount point 
 transaction pointer 
 start block to allocate 
 length to allocate 
 inout: summary block buffer 
 inout: summary block number 
 end of the allocated extent 
 error value 
 first block allocated > end 
 first block allocated < start 
	
	  Assume we're allocating out of the middle of a free extent.
	  We need to find the beginning and end of the extent so we can
	  properly update the summary.
	
	  Find the next allocated block (end of free extent).
	
	  Decrement the summary information corresponding to the entire
	  (old) free extent.
	
	  If there are blocks not being allocated at the front of the
	  old extent, add summary data for them to be free.
	
	  If there are blocks not being allocated at the end of the
	  old extent, add summary data for them to be free.
	
	  Modify the bitmap to mark this extent allocated.
  Attempt to allocate an extent minlen<=len<=maxlen starting from
  bitmap block bbno.  If we don't get maxlen then use prod to trim
  the length, if given.  Returns error; returns starting block in rtblock.
  The lengths are all in rtextents.
 error 
 file system mount point 
 transaction pointer 
 bitmap block number 
 minimum length to allocate 
 maximum length to allocate 
 out: actual length allocated 
 out: next block to try 
 inout: summary block buffer 
 inout: summary block number 
 extent product factor 
 out: start block allocated 
 best rtblock found so far 
 best length found so far 
 last rtblock in chunk 
 error value 
 current rtblock trying 
 next rtblock to try 
 status from internal calls 
	
	  Loop over all the extents starting in this bitmap block,
	  looking for one that's long enough.
 Make sure we don't scan off the end of the rt volume. 
		
		  See if there's a free extent of maxlen starting at i.
		  If it's not so then next will contain the first non-free.
			
			  i for maxlen is all free, allocate and return that.
		
		  In the case where we have a variable-sized allocation
		  request, figure out how big this free piece is,
		  and if it's big enough for the minimum, and the best
		  so far, remember it.
 this extent size 
		
		  If not done yet, find the start of the next free space.
	
	  Searched the whole thing & didn't find a maxlen free extent.
 amount to trim length by 
		
		  If size should be a multiple of prod, make that so.
		
		  Allocate besti for bestlen & return that.
	
	  Allocation failed.  Set nextp to the next block to try.
  Allocate an extent of length minlen<=len<=maxlen, starting at block
  bno.  If we don't get maxlen then use prod to trim the length, if given.
  Returns error; returns starting block in rtblock.
  The lengths are all in rtextents.
 error 
 file system mount point 
 transaction pointer 
 starting block number to allocate 
 minimum length to allocate 
 maximum length to allocate 
 out: actual length allocated 
 inout: summary block buffer 
 inout: summary block number 
 extent product factor 
 out: start block allocated 
 error value 
 extent length trimmed due to prod 
 extent is free 
 next block to try (dummy) 
	
	  Check if the range in question (for maxlen) is free.
		
		  If it is, allocate it and return success.
	
	  If not, allocate what there is, if it's at least minlen.
		
		  Failed, return failure status.
	
	  Trim off tail of extent, if prod is specified.
			
			  Now we can't do it, return failure status.
	
	  Allocate what we can and return it.
  Allocate an extent of length minlen<=len<=maxlen, starting as near
  to bno as possible.  If we don't get maxlen then use prod to trim
  the length, if given.  The lengths are all in rtextents.
 error 
 file system mount point 
 transaction pointer 
 starting block number to allocate 
 minimum length to allocate 
 maximum length to allocate 
 out: actual length allocated 
 inout: summary block buffer 
 inout: summary block number 
 extent product factor 
 out: start block allocated 
 any useful extents from summary 
 bitmap block number 
 error value 
 bitmap block offset (loop control) 
 secondary loop control 
 log2 of minlen 
 next block to try 
 result block 
	
	  If the block number given is off the end, silently set it to
	  the last block.
 Make sure we don't run off the end of the rt volume. 
	
	  Try the exact allocation first.
	
	  If the exact allocation worked, return that.
	
	  Loop over all bitmap blocks (bbno + i is current block).
		
		  Get summary information of extents of all useful levels
		  starting in this bitmap block.
		
		  If there are any useful extents starting here, try
		  allocating one.
			
			  On the positive side of the starting location.
				
				  Try to allocate an extent starting in
				  this block.
				
				  If it worked, return it.
			
			  On the negative side of the starting location.
 i < 0 
				
				  Loop backwards through the bitmap blocks from
				  the starting point-1 up to where we are now.
				  There should be an extent which ends in this
				  bitmap block and is long enough.
					
					  Grab the summary information for
					  this bitmap block.
					
					  If there's no extent given in the
					  summary that means the extent we
					  found must carry over from an
					  earlier block.  If there is an
					  extent given, we've already tried
					  that allocation, don't do it again.
					
					  If it works, return the extent.
				
				  There weren't intervening bitmap blocks
				  with a long enough extent, or the
				  allocation didn't work for some reason
				  (i.e. it's a little  too short).
				  Try to allocate from the summary block
				  that we found.
				
				  If it works, return the extent.
		
		  Loop control.  If we were on the positive side, and there's
		  still more blocks on the negative side, go there.
		
		  If positive, and no more negative, but there are more
		  positive, go there.
		
		  If negative or 0 (just started), and there are positive
		  blocks to go, go there.  The 0 case moves to block 1.
		
		  If negative or 0 and there are more negative blocks,
		  go there.
		
		  Must be done.  Return failure.
  Allocate an extent of length minlen<=len<=maxlen, with no position
  specified.  If we don't get maxlen then use prod to trim
  the length, if given.  The lengths are all in rtextents.
 error 
 file system mount point 
 transaction pointer 
 minimum length to allocate 
 maximum length to allocate 
 out: actual length allocated 
 inout: summary block buffer 
 inout: summary block number 
 extent product factor 
 out: start block allocated 
 error value 
 bitmap block number 
 level number (loop control) 
 next block to be tried 
 result block number 
 summary information for extents 
	
	  Loop over all the levels starting with maxlen.
	  At each level, look at all the bitmap blocks, to see if there
	  are extents starting there that are long enough (>= maxlen).
	  Note, only on the initial level can the allocation fail if
	  the summary says there's an extent.
		
		  Loop over all the bitmap blocks.
			
			  Get the summary for this levelblock.
			
			  Nothing there, on to the next block.
			
			  Try allocating the extent.
			
			  If it worked, return that.
			
			  If the "next block to try" returned from the
			  allocator is beyond the next bitmap block,
			  skip to that bitmap block.
	
	  Didn't find any maxlen blocks.  Try smaller ones, unless
	  we're asking for a fixed size extent.
	
	  Loop over sizes, from maxlen down to minlen.
	  This time, when we do the allocations, allow smaller ones
	  to succeed.
		
		  Loop over all the bitmap blocks, try an allocation
		  starting in that block.
			
			  Get the summary information for this levelblock.
			
			  If nothing there, go on to next.
			
			  Try the allocation.  Make sure the specified
			  minlenmaxlen are in the possible range for
			  this summary level.
			
			  If it worked, return that extent.
			
			  If the "next block to try" returned from the
			  allocator is beyond the next bitmap block,
			  skip to that bitmap block.
	
	  Got nothing, return failure.
  Allocate space to the bitmap or summary file, and zero it, for growfs.
 file system mount point 
 old count of blocks 
 new count of blocks 
 inode (bitmapsummary) 
 block number in file 
 temporary buffer for zeroing 
 disk block address 
 error return value 
 filesystem block for bno 
 block map output 
 number of block maps 
 space reservation 
	
	  Allocate space to the file, as necessary.
		
		  Reserve space & log for one extent added to the file.
		
		  Lock the inode.
		
		  Allocate blocks to the bitmap file.
		
		  Free any blocks freed up in the transaction, then commit.
		
		  Now we need to clear the allocated blocks.
		  Do this one block per transaction, to keep it simple.
			
			  Reserve log for one block zeroing.
			
			  Lock the bitmap inode.
			
			  Get a buffer for the block.
			
			  Commit the transaction.
		
		  Go on to the next extent, if any.
 file system mount structure 
 number of rt bitmap blocks 
	
	  The rsum cache is initialized to all zeroes, which is trivially a
	  lower bound on the minimum level with any free extents. We can
	  continue without the cache if it couldn't be allocated.
  Visible (exported) functions.
  Grow the realtime area of the filesystem.
 mount point for filesystem 
 growfs rt input struct 
 bitmap block number 
 temporary buffer 
 error return value 
 new (fake) mount structure 
 new number of realtime blocks 
 new number of rt bitmap blocks 
 new number of realtime extents 
 new log2 of sb_rextents 
 new number of summary blocks 
 new rt summary levels 
 new size of rt summary, bytes 
 new superblock 
 current number of rt bitmap blocks 
 current number of rt summary blks 
 old superblock 
 summary block number 
 old summary cache 
 Needs to have been mounted with an rt device. 
	
	  Mount should fail if the rt bitmapsummary files don't load, but
	  we'll check anyway.
 Shrink not supported. 
 Can only change rt extent size when adding rt volume. 
 Range check the extent size. 
 Unsupported realtime features. 
	
	  Read in the last block of the device, make sure it exists.
	
	  Calculate new parameters.  These are the final values to be reached.
	
	  New summary size can't be more than half the size of
	  the log.  This prevents us from getting a log overflow,
	  since we'll log basically the whole summary file at once.
	
	  Get the old block counts for bitmap and summary inodes.
	  These can't change since other growfs callers are locked out.
	
	  Allocate space to the bitmap and summary files, as necessary.
	
	  Allocate a new (fake) mountsb.
	
	  Loop over the bitmap blocks.
	  We will do everything one bitmap block at a time.
	  Skip the current block if it is exactly full.
	  This also deals with the case where there were no rtextents before.
		
		  Calculate new sb and mount fields for this round.
		
		  Start a transaction, get the log reservation.
		
		  Lock out other callers by grabbing the bitmap inode lock.
		
		  Update the bitmap inode's size ondisk and incore.  We need
		  to update the incore size so that inode inactivation won't
		  punch what it thinks are "posteof" blocks.
		
		  Get the summary inode into the transaction.
		
		  Update the summary inode's size.  We need to update the
		  incore size so that inode inactivation won't punch what it
		  thinks are "posteof" blocks.
		
		  Copy summary data from old to new sizes.
		  Do this when the real size (not block-aligned) changes.
		
		  Update superblock fields.
		
		  Free new extent.
		
		  Mark more blocks free in the superblock.
		
		  Update mp values into the real mp structure.
 Ensure the mount RT feature flag is now set. 
 Update secondary superblocks now the physical grow has completed 
	
	  Free the fake mp structure.
	
	  If we had to allocate a new rsum_cache, we either need to free the
	  old one (if we succeeded) or free the new one and restore the old one
	  (if there was an error).
  Allocate an extent in the realtime subvolume, with the usual allocation
  parameters.  The length units are all in realtime extents, as is the
  result block number.
 error 
 transaction pointer 
 starting block number to allocate 
 minimum length to allocate 
 maximum length to allocate 
 out: actual length allocated 
 was a delayed allocation extent 
 extent product factor 
 out: start block allocated 
 error value 
 result allocated block 
 summary file block number 
 summary file block buffer 
	
	  If prod is set then figure out what to do to minlen and maxlen.
	
	  If it worked, update the superblock.
  Initialize realtime fields in the mount structure.
 error 
 file system mount structure 
 buffer for last block of subvolume 
 filesystem superblock copy in mount 
 address of last block of subvolume 
	
	  Check that the realtime section is an ok size.
  Get the bitmap and summary inodes and the summary cache into the mount
  structure at mount time.
 error 
 file system mount structure 
 error return value 
  Pick an extent for allocation at the start of a new realtime file.
  Use the sequence number stored in the atime field of the bitmap inode.
  Translate this to a fraction of the rtextents, and return the product
  of rtextents and the fraction.
  The fraction sequence is 0, 12, 14, 34, 18, ..., 78, 116, ...
 error 
 file system mount point 
 transaction pointer 
 allocation length (rtextents) 
 result rt extent 
 result block 
 log of sequence number 
 residual after log removed 
 sequence number of file creation 
 pointer to seqno in inode 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  All Rights Reserved.
  Locking orders
  xfs_buf_ioacct_inc:
  xfs_buf_ioacct_dec:
 	b_sema (caller holds)
 	  b_lock
  xfs_buf_stale:
 	b_sema (caller holds)
 	  b_lock
 	    lru_lock
  xfs_buf_rele:
 	b_lock
 	  pag_buf_lock
 	    lru_lock
  xfs_buftarg_drain_rele
 	lru_lock
 	  b_lock (trylock due to inversion)
  xfs_buftarg_isolate
 	lru_lock
 	  b_lock (trylock due to inversion)
	
	  Return true if the buffer is vmapped.
	 
	  b_addr is null if the buffer is not mapped, but the code is clever
	  enough to know it doesn't have to map a single page, so the check has
	  to be both for b_addr and bp->b_page_count > 1.
  Bump the IO in flight count on the buftarg if we haven't yet done so for
  this buffer. The count is incremented once per buffer (per hold cycle)
  because the corresponding decrement is deferred to buffer release. Buffers
  can undergo IO multiple times in a hold-release cycle and per buffer IO
  tracking adds unnecessary overhead. This is used for sychronization purposes
  with unmount (see xfs_buftarg_drain()), so all we really need is a count of
  in-flight buffers.
  Buffers that are never released (e.g., superblock, iclog buffers) must set
  the XBF_NO_IOACCT flag before IO submission. Otherwise, the buftarg count
  never reaches zero and unmount hangs indefinitely.
  Clear the in-flight state on a buffer about to be released to the LRU or
  freed and unaccount from the buftarg.
  When we mark a buffer stale, we remove the buffer from the LRU and clear the
  b_lru_ref count so that the buffer is freed immediately when the buffer
  reference count falls to zero. If the buffer is already on the LRU, we need
  to remove the reference that LRU holds on the buffer.
  This prevents build-up of stale buffers on the LRU.
	
	  Clear the delwri status so that a delwri queue walker will not
	  flush this buffer to disk now that it is stale. The delwri queue has
	  a reference to the buffer, so this is safe to do.
	
	  Once the buffer is marked stale and unlocked, a subsequent lookup
	  could reset b_flags. There is no guarantee that the buffer is
	  unaccounted (released to LRU) before that occurs. Drop in-flight
	  status now to preserve accounting consistency.
 	Frees b_pages if it was allocated.
	
	  We don't want certain flags to appear in b_flags unless they are
	  specifically set by later operations on the buffer.
 held, no waiters 
	
	  Set length and io_length to the same value initially.
	  IO routines should use io_length, which will be the same in
	  most cases but may be reset (e.g. XFS recovery).
 Assure zeroed buffer for non-read cases. 
 b_addr spans two pages - use alloc_page instead 
 Make sure that we have a page list 
 Assure zeroed buffer for non-read cases. 
	
	  Bulk filling of pages can take multiple calls. Not filling the entire
	  array is not an allocation failure, so don't back off if we get at
	  least one extra page.
 	Map buffer into kernel address-space if necessary.
 A single page buffer is always mappable 
		
		  vm_map_ram() will allocate auxiliary structures (e.g.
		  pagetables) with GFP_KERNEL, yet we are likely to be under
		  GFP_NOFS context here. Hence we need to tell memory reclaim
		  that we are in such a context via PF_MEMALLOC_NOFS to prevent
		  memory reclaim re-entering the filesystem here and
		  potentially deadlocking.
 	Finding and Reading Buffers
	
	  The key hashing in the lookup path depends on the key being the
	  first element of the compare_arg, make sure to assert this.
		
		  found a block number match. If the range doesn't
		  match, the only way this is allowed is if the buffer
		  in the cache is stale and the transaction that made
		  it stale has not yet committed. i.e. we are
		  reallocating a busy extent. Skip this buffer and
		  continue searching for an exact match.
 empty AGs have minimal footprint 
  Look up a buffer in the buffer cache and return it referenced and locked
  in @found_bp.
  If @new_bp is supplied and we have a lookup miss, insert @new_bp into the
  cache.
  If XBF_TRYLOCK is set in @flags, only try to lock the buffer and return
  -EAGAIN if we fail to lock it.
  Return values are:
 	-EFSCORRUPTED if have been supplied with an invalid address
 	-EAGAIN on trylock failure
 	-ENOENT if we fail to find a match and @new_bp was NULL
 	0, with @found_bp:
 		- @new_bp if we inserted it into the cache
 		- the buffer we found and locked.
 Check for IOs smaller than the sector size  not sector aligned 
	
	  Corrupted block numbers can get through to here, unfortunately, so we
	  have to check that the buffer falls within the filesystem bounds.
 No match found 
 the buffer keeps the perag reference until it is freed 
	
	  if the buffer is stale, clear all the external state associated with
	  it. We need to keep flags such as how we allocated the buffer memory
	  intact here.
  Assembles a buffer covering the specified range. The code is optimised for
  cache hits, as metadata intensive workloads will see 3 orders of magnitude
  more hits than misses.
	
	  For buffers that fit entirely within a single page, first attempt to
	  allocate the memory from the heap to minimise memory usage. If we
	  can't get heap memory for these small buffers, we fall back to using
	  the page allocator.
	
	  Clear b_error if this is a lookup from a caller that doesn't expect
	  valid data to be found in the buffer.
  Reverify a buffer found in cache without an attached ->b_ops.
  If the caller passed an ops structure and the buffer doesn't have ops
  assigned, set the ops and use it to verify the contents. If verification
  fails, clear XBF_DONE. We assume the buffer has no recorded errors and is
  already in XBF_DONE state on entry.
  Under normal operations, every in-core buffer is verified on read IO
  completion. There are two scenarios that can lead to in-core buffers without
  an assigned ->b_ops. The first is during log recovery of buffers on a V4
  filesystem, though these buffers are purged at the end of recovery. The
  other is online repair, which intentionally reads with a NULL buffer ops to
  run several verifiers across an in-core buffer in order to establish buffer
  type.  If repair can't establish that, the buffer will be left in memory
  with NULL buffer ops.
 Initiate the buffer read and wait. 
 Readahead iodone already dropped the buffer, so exit. 
 Buffer already read; all we need to do is check it. 
 Readahead already finished; drop the buffer and exit. 
 We do not want read in the flags 
	
	  If we've had a read error, then the contents of the buffer are
	  invalid and should not be used. To ensure that a followup read tries
	  to pull the buffer from disk again, we clear the XBF_DONE flag and
	  mark the buffer stale. This ensures that anyone who has a current
	  reference to the buffer will interpret it's contents correctly and
	  future cache lookups will also treat it as an empty, uninitialised
	  buffer.
 bad CRC means corrupted metadata 
 	If we are not low on memory then do the readahead in a deadlock
 	safe manner.
  Read an uncached buffer from disk. Allocates and returns a locked
  buffer containing the disk contents or nothing. Uncached buffers always have
  a cache index of XFS_BUF_DADDR_NULL so we can easily determine if the buffer
  is cached or uncached during fault diagnosis.
 set up the buffer for a read IO 
 flags might contain irrelevant bits, pass only what we care about 
 	Increment reference count on buffer, to hold the buffer concurrently
 	with another thread which may release (free) the buffer asynchronously.
 	Must hold the buffer already to call this function.
  Release a hold on the specified buffer. If the hold count is 1, the buffer is
  placed on LRU or freed (depending on b_lru_ref).
	
	  We grab the b_lock here first to serialise racing xfs_buf_rele()
	  calls. The pag_buf_lock being taken on the last reference only
	  serialises against racing lookups in xfs_buf_find(). IOWs, the second
	  to last reference we drop here is not serialised against the last
	  reference until we take bp->b_lock. Hence if we don't grab b_lock
	  first, the last "release" reference can win the race to the lock and
	  free the buffer before the second-to-last reference is processed,
	  leading to a use-after-free scenario.
		
		  Drop the in-flight state if the buffer is already on the LRU
		  and it holds the only reference. This is racy because we
		  haven't acquired the pag lock, but the use of _XBF_IN_FLIGHT
		  ensures the decrement occurs only once per-buf.
 the last reference has been dropped ... 
		
		  If the buffer is added to the LRU take a new reference to the
		  buffer for the LRU and clear the (now stale) dispose list
		  state flag
		
		  most of the time buffers will already be removed from the
		  LRU, so optimise that case by checking for the
		  XFS_BSTATE_DISPOSE flag indicating the last list the buffer
		  was on was the disposal list
 	Lock a buffer object, if it is not already locked.
 	If we come across a stale, pinned, locked buffer, we know that we are
 	being asked to lock a buffer that has been reallocated. Because it is
 	pinned, we know that the log has not been pushed to disk and hence it
 	will still be locked.  Rather than continuing to have trylock attempts
 	fail until someone else pushes the log, push it ourselves before
 	returning.  This means that the xfsaild will not get stuck trying
 	to push on stale inode buffers.
 	Lock a buffer object.
 	If we come across a stale, pinned, locked buffer, we know that we
 	are being asked to lock a buffer that has been reallocated. Because
 	it is pinned, we know that the log has not been pushed to disk and
 	hence it will still be locked. Rather than sleeping until someone
 	else pushes the log, push it ourselves before trying to get the lock.
  Account for this latest trip around the retry handler, and decide if
  we've failed enough times to constitute a permanent failure.
 At unmount we may treat errors differently 
  On a sync write or shutdown we just want to stale the buffer and let the
  caller handle the error in bp->b_error appropriately.
  If the write was asynchronous then no one will be looking for the error.  If
  this is the first failure of this type, clear the error state and write the
  buffer out again. This means we always retry an async write failure at least
  once, but we also need to set the buffer up to behave correctly now for
  repeated failures.
  If we get repeated async write failures, then we take action according to the
  error configuration we have been set up to use.
  Returns true if this function took care of error handling and the caller must
  not touch the buffer again.  Return false if the caller should proceed with
  normal IO completion handling.
	
	  If we've already decided to shutdown the filesystem because of IO
	  errors, there's no point in giving this a retry.
	
	  We're not going to bother about retrying this during recovery.
	  One strike!
	
	  Synchronous writes will have callers process the error.
	
	  Permanent error - we need to trigger a shutdown if we haven't already
	  to indicate that inconsistency will result from this action.
 Still considered a transient error. Caller will schedule retries. 
	
	  Pull in IO completion errors now. We are guaranteed to be running
	  single threaded, so we don't need the lock to read b_io_error.
 clear the retry state 
		
		  Note that for things like remote attribute buffers, there may
		  not be a buffer log item here, so processing the buffer log
		  item must remain optional.
  To simulate an IO failure, the buffer must be locked and held with at least
  three references. The LRU reference is dropped by the stale call. The buf
  item reference is dropped via ioend processing. The third reference is owned
  by the caller and is dropped on IO completion if the buffer is XBF_ASYNC.
	
	  don't overwrite existing errors - otherwise we can lose errors on
	  buffers that require multiple bios to complete.
 skip the pages in the buffer before the start offset 
	
	  Limit the IO size to the length of the current vector, and update the
	  remaining IO count for the next time around.
		
		  This is guaranteed not to be the last io reference count
		  because the caller (xfs_buf_submit) holds a count itself.
	
	  Make sure we capture only current IO errors rather than stale errors
	  left over from previous use of the buffer (e.g. failed readahead).
		
		  Run the write verifier callback function if it exists. If
		  this function fails it will mark the buffer with an error and
		  the IO should not be dispatched.
			
			  non-crc filesystems don't attach verifiers during
			  log recovery, so don't warn for such filesystems.
 we only use the buffer cache for meta-data 
	
	  Walk all the vectors issuing IO on them. Set up the initial offset
	  into the buffer and the desired IO size before we start -
	  _xfs_buf_ioapply_vec() will modify them appropriately for each
	  subsequent call.
 all done 
  Wait for IO completion of a sync buffer and return the IO error code.
  Buffer IO submission path, read or write. Asynchronous submission transfers
  the buffer lock ownership and the current reference to the IO. It is not
  safe to reference the buffer after a call to this function unless the caller
  holds an additional reference itself.
 on shutdown we stale and complete the buffer immediately 
	
	  Grab a reference so the buffer does not go away underneath us. For
	  async buffers, IO completion drops the callers reference, which
	  could occur before submission returns.
 clear the internal error state to avoid spurious errors 
	
	  Set the count to 1 initially, this will stop an IO completion
	  callout which happens before we have started all the IO from calling
	  xfs_buf_ioend too early.
	
	  If _xfs_buf_ioapply failed, we can get back here with only the IO
	  reference we took above. If we drop it to zero, run completion so
	  that we don't return to the caller with completion still pending.
	
	  Release the hold that keeps the buffer referenced for the entire
	  IO. Note that if the buffer is async, it is not safe to reference
	  after this release.
  Log a message about and stale a buffer that a caller has decided is corrupt.
  This function should be called for the kinds of metadata corruption that
  cannot be detect from a verifier, such as incorrect inter-block relationship
  data.  Do not call this function from a verifier function.
  The buffer must be XBF_DONE prior to the call.  Afterwards, the buffer will
  be marked stale, but b_error will not be set.  The caller is responsible for
  releasing the buffer or fixing it.
 	Handling of buffer targets (buftargs).
  Wait for any bufs with callbacks that have been submitted but have not yet
  returned. These buffers will have an elevated hold count, so wait on those
  while freeing all the buffers only held by the LRU.
 need to wait, so skip it this pass 
	
	  clear the LRU reference count so the buffer doesn't get
	  ignored in xfs_buf_rele().
  Wait for outstanding IO on the buftarg to complete.
	
	  First wait on the buftarg IO count for all in-flight buffers to be
	  released. This is critical as new buffers do not make the LRU until
	  they are released.
	 
	  Next, flush the buffer workqueue to ensure all completion processing
	  has finished. Just waiting on buffer locks is not sufficient for
	  async IO as the reference count held over IO is not released until
	  after the buffer lock is dropped. Hence we need to ensure here that
	  all reference counts have been dropped before we start walking the
	  LRU list.
 loop until there is nothing left on the lru list. 
	
	  If one or more failed buffers were freed, that means dirty metadata
	  was thrown away. This should only ever happen after IO completion
	  handling has elevated IO error(s) to permanent failures and shuts
	  down the fs.
	
	  we are inverting the lru lockbp->b_lock here, so use a trylock.
	  If we fail to get the lock, just skip it.
	
	  Decrement the b_lru_ref count unless the value is already
	  zero. If the value is already zero, we need to reclaim the
	  buffer, otherwise it gets another trip through the LRU.
 Set up metadata sector size info 
 Set up device logical sector size mask 
  When allocating the initial buffer target we have not yet
  read in the superblock, so don't know what sized sectors
  are being used at this early stage.  Play safe.
	
	  Buffer IO error rate limiting. Limit it to no more than 10 messages
	  per 30 seconds so as to not spam logs too much on repeated errors.
  Cancel a delayed write list.
  Remove each buffer from the list, clear the delwri queue flag and drop the
  associated buffer reference.
  Add a buffer to the delayed write list.
  This queues a buffer for writeout if it hasn't already been.  Note that
  neither this routine nor the buffer list submission functions perform
  any internal synchronization.  It is expected that the lists are thread-local
  to the callers.
  Returns true if we queued up the buffer, or false if it already had
  been on the buffer list.
	
	  If the buffer is already marked delwri it already is queued up
	  by someone else for imediate writeout.  Just ignore it in that
	  case.
	
	  If a buffer gets written out synchronously or marked stale while it
	  is on a delwri list we lazily remove it. To do this, the other party
	  clears the  _XBF_DELWRI_Q flag but otherwise leaves the buffer alone.
	  It remains referenced and on the list.  In a rare corner case it
	  might get readded to a delwri list after the synchronous writeout, in
	  which case we need just need to re-add the flag here.
  Compare function is more complex than it needs to be because
  the return value is only 32 bits and we are doing comparisons
  on 64 bit values
  Submit buffers for write. If wait_list is specified, the buffers are
  submitted using sync IO and placed on the wait list such that the caller can
  iowait each buffer. Otherwise async IO is used and the buffers are released
  at IO completion time. In either case, buffers remain locked until IO
  completes and the buffer is released from the queue.
		
		  Someone else might have written the buffer synchronously or
		  marked it stale in the meantime.  In that case only the
		  _XBF_DELWRI_Q flag got cleared, and we have to drop the
		  reference and remove it from the list here.
		
		  If we have a wait list, each buffer (and associated delwri
		  queue reference) transfers to it and is submitted
		  synchronously. Otherwise, drop the buffer from the delwri
		  queue and submit async.
  Write out a buffer list asynchronously.
  This will take the @buffer_list, write all non-locked and non-pinned buffers
  out and not wait for IO completion on any of the buffers.  This interface
  is only safely useable for callers that can track IO completion by higher
  level means, e.g. AIL pushing as the @buffer_list is consumed in this
  function.
  Note: this function will skip buffers it would block on, and in doing so
  leaves them on @buffer_list so they can be retried on a later pass. As such,
  it is up to the caller to ensure that the buffer list is fully submitted or
  cancelled appropriately when they are finished with the list. Failure to
  cancel or resubmit the list until it is empty will result in leaked buffers
  at unmount time.
  Write out a buffer list synchronously.
  This will take the @buffer_list, write all buffers out and wait for IO
  completion on all of the buffers. @buffer_list is consumed by the function,
  so callers must have some other way of tracking buffers if they require such
  functionality.
 Wait for IO to complete. 
		
		  Wait on the locked buffer, check for errors and unlock and
		  release the delwri queue reference.
  Push a single buffer on a delwri queue.
  The purpose of this function is to submit a single buffer of a delwri queue
  and return with the buffer still on the original queue. The waiting delwri
  buffer submission infrastructure guarantees transfer of the delwri queue
  buffer reference to a temporary wait list. We reuse this infrastructure to
  transfer the buffer back to the original queue.
  Note the buffer transitions from the queued state, to the submitted and wait
  listed state and back to the queued state during this call. The buffer
  locking and queue management logic between _delwri_pushbuf() and
  _delwri_queue() guarantee that the buffer cannot be queued to another list
  before returning.
	
	  Isolate the buffer to a new local list so we can submit it for IO
	  independently from the rest of the original list.
	
	  Delwri submission clears the DELWRI_Q buffer flag and returns with
	  the buffer on the wait list with the original reference. Rather than
	  bounce the buffer from a local wait list back to the original list
	  after IO completion, reuse the original list as the wait list.
	
	  The buffer is now locked, under IO and wait listed on the original
	  delwri queue. Wait for IO completion, restore the DELWRI_Q flag and
	  return with the buffer unlocked and on the original queue.
	
	  Set the lru reference count to 0 based on the error injection tag.
	  This allows userspace to disrupt buffer caching for debugtesting
	  purposes.
  Verify an on-disk magic value against the magic value specified in the
  verifier structure. The verifier magic is in disk byte order so the caller is
  expected to pass the value directly from disk.
  Verify an on-disk magic value against the magic value specified in the
  verifier structure. The verifier magic is in disk byte order so the caller is
  expected to pass the value directly from disk.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2010 Red Hat, Inc. All Rights Reserved.
  Allocate a new ticket. Failing to get a new ticket makes it really hard to
  recover, so we don't allow failure here. Also, we allocate in a context that
  we don't want to be issuing transactions from, so we need to tell the
  allocation code this as well.
  We don't reserve any space for the ticket - we are going to steal whatever
  space we require from transactions as they commit. To ensure we reserve all
  the space required, we need to set the current reservation of the ticket to
  zero so that we know to steal the initial transaction overhead from the
  first transaction commit.
	
	  set the current reservation to zero so we know to steal the basic
	  transaction overhead reservation from the first transaction commit.
  Unavoidable forward declaration - xlog_cil_push_work() calls
  xlog_cil_ctx_alloc() itself.
  After the first stage of log recovery is done, we know where the head and
  tail of the log are. We need this log initialisation done before we can
  initialise the first CIL checkpoint context.
  Here we allocate a log ticket to track space usage during a CIL push.  This
  ticket is passed to xlog_write() directly so that we don't slowly leak log
  space by failing to account for space used by log headers and additional
  region headers for split regions.
  Allocate or pin log vector buffers for CIL insertion.
  The CIL currently uses disposable buffers for copying a snapshot of the
  modified items into the log during a push. The biggest problem with this is
  the requirement to allocate the disposable buffer during the commit if:
 	a) does not exist; or
 	b) it is too small
  If we do this allocation within xlog_cil_insert_format_items(), it is done
  under the xc_ctx_lock, which means that a CIL push cannot occur during
  the memory allocation. This means that we have a potential deadlock situation
  under low memory conditions when we have lots of dirty metadata pinned in
  the CIL and we need a CIL commit to occur to free memory.
  To avoid this, we need to move the memory allocation outside the
  xc_ctx_lock, but because the log vector buffers are disposable, that opens
  up a TOCTOU race condition w.r.t. the CIL committing and removing the log
  vector buffers between the check and the formatting of the item into the
  log vector buffer within the xc_ctx_lock.
  Because the log vector buffer needs to be unchanged during the CIL push
  process, we cannot share the buffer between the transaction commit (which
  modifies the buffer) and the CIL push context that is writing the changes
  into the log. This means skipping preallocation of buffer space is
  unreliable, but we most definitely do not want to be allocating and freeing
  buffers unnecessarily during commits when overwrites can be done safely.
  The simplest solution to this problem is to allocate a shadow buffer when a
  log item is committed for the second time, and then to only use this buffer
  if necessary. The buffer can remain attached to the log item until such time
  it is needed, and this is the buffer that is reallocated to match the size of
  the incoming modification. Then during the formatting of the item we can swap
  the active buffer with the new one if we can't reuse the existing buffer. We
  don't free the old buffer as it may be reused on the next modification if
  it's size is right, otherwise we'll free and reallocate it at that point.
  This function builds a vector for the changes in each log item in the
  transaction. It then works out the length of the buffer needed for each log
  item, allocates them and attaches the vector to the log item in preparation
  for the formatting step which occurs under the xc_ctx_lock.
  While this means the memory footprint goes up, it avoids the repeated
  allocfree pattern that repeated modifications of an item would otherwise
  cause, and hence minimises the CPU overhead of such behaviour.
 Skip items which aren't dirty in this transaction. 
 get number of vecs and size of data to be stored 
		
		  Ordered items need to be tracked but we do not wish to write
		  them. We need a logvec to track the object, but we do not
		  need an iovec or buffer to be allocated for copying data.
		
		  We 64-bit align the length of each iovec so that the start
		  of the next one is naturally aligned.  We'll need to
		  account for that slack space here. Then round nbytes up
		  to 64-bit alignment so that the initial buffer alignment is
		  easy to calculate and verify.
		
		  The data buffer needs to start 64-bit aligned, so round up
		  that space to ensure we can align it appropriately and not
		  overrun the buffer.
		
		  if we have no shadow buffer, or it is too small, we need to
		  reallocate it.
			
			  We free and allocate here as a realloc would copy
			  unnecessary data. We don't use kmem_zalloc() for the
			  same reason - we don't need to zero the data area in
			  the buffer, only the log vector header and the iovec
			  storage.
			
			  We are in transaction context, which means this
			  allocation will pick up GFP_NOFS from the
			  memalloc_nofs_saverestore context the transaction
			  holds. This means we can use GFP_KERNEL here so the
			  generic kvmalloc() code will run vmalloc on
			  contiguous page allocation failure as we require.
 same or smaller, optimise common overwrite case 
 Ensure the lv is set up according to ->iop_size 
 The allocated data region lies beyond the iovec region 
  Prepare the log item for insertion into the CIL. Calculate the difference in
  log space and vectors it will consume, and if it is a new item pin it as
  well.
 Account for the new LV being passed in 
	
	  If there is no old LV, this is the first time we've seen the item in
	  this CIL context and so we need to pin it. If we are replacing the
	  old_lv, then remove the space it accounts for and make it the shadow
	  buffer for later freeing. In both cases we are now switching to the
	  shadow buffer, so update the pointer to it appropriately.
 attach new log vector to log item 
	
	  If this is the first time the item is being committed to the
	  CIL, store the sequence number on the log item so we can
	  tell in future commits whether this is the first checkpoint
	  the item is being committed into.
  Format log item into a flat buffers
  For delayed logging, we need to hold a formatted buffer containing all the
  changes on the log item. This enables us to relog the item in memory and
  write it out asynchronously without needing to relock the object that was
  modified at the time it gets written into the iclog.
  This function takes the prepared log vectors attached to each log item, and
  formats the changes into the log vector buffer. The buffer it uses is
  dependent on the current state of the vector in the CIL - the shadow lv is
  guaranteed to be large enough for the current modification, but we will only
  use that if we can't reuse the existing lv. If we can't reuse the existing
  lv, then simple swap it out for the shadow lv. We don't free it - that is
  done lazily either by th enext modification or the freeing of the log item.
  We don't set up region headers during this process; we simply copy the
  regions into the flat buffer. We can do this because we still have to do a
  formatting step to write the regions into the iclog buffer.  Writing the
  ophdrs during the iclog write means that we can support splitting large
  regions across iclog boundares without needing a change in the format of the
  itemregion encapsulation.
  Hence what we need to do now is change the rewrite the vector array to point
  to the copied region inside the buffer we just allocated. This allows us to
  format the regions into the iclog as though they are being formatted
  directly out of the objects themselves.
 Bail out if we didn't find a log item.  
 Skip items which aren't dirty in this transaction. 
		
		  The formatting size information is already attached to
		  the shadow lv on the log item.
 Skip items that do not have any vectors for writing 
 compare to existing item size 
 same or smaller, optimise common overwrite case 
			
			  set the item up as though it is a new insertion so
			  that the space reservation accounting is correct.
 Ensure the lv is set up according to ->iop_size 
 reset the lv buffer information for new formatting 
 switch to shadow buffer! 
 track as an ordered logvec 
  Insert the log items into the CIL and calculate the difference in space
  consumed by the item. Add the space to the checkpoint ticket and calculate
  if the change requires additional log metadata. If it does, take that space
  as well. Remove the amount of space we added to the checkpoint ticket from
  the current transaction ticket so that the accounting works out correctly.
	
	  We can do this safely because the context can't checkpoint until we
	  are done so it doesn't matter exactly how we update the CIL.
 account for space used by new iovec headers  
 attach the transaction to the CIL if it has any busy extents 
	
	  Now transfer enough transaction reservation to the context ticket
	  for the checkpoint. The context ticket is special - the unit
	  reservation has to grow as well as the current reservation as we
	  steal from tickets so we can correctly determine the space used
	  during the transaction commit.
 do we need space for more log record headers? 
 need to take into account split region headers, too 
	
	  If we've overrun the reservation, dump the tx details before we move
	  the log items. Shutdown is imminent...
	
	  Now (re-)position everything modified at the tail of the CIL.
	  We do this here so we only need to take the CIL lock once during
	  the transaction commit.
 Skip items which aren't dirty in this transaction. 
		
		  Only move the item if it isn't already at the tail. This is
		  to prevent a transient list_empty() state when reinserting
		  an item that is already the only item in the CIL.
  Queue up the actual completion to a thread to avoid IRQ-safe locking for
  pagb_lock.  Note that we need a unbounded workqueue, otherwise we might
  get the execution delayed up to 30 seconds for weird reasons.
  Mark all items committed and clear busy extents. We free the log vector
  chains in a separate pass so that we unpin the log items as quickly as
  possible.
	
	  If the IO failed, we're aborting the commit and already shutdown.
	  Wake any commit waiters before aborting the log items so we don't
	  block async log pushers on callbacks. Async log pushers explicitly do
	  not wait on log force completion because they may be holding locks
	  required to unpin items.
 Record the LSN of the iclog we were just granted space to start writing into.
 If the context doesn't have a start_lsn recorded, then this iclog will
 contain the start record for the checkpoint. Otherwise this write contains
 the commit record for the checkpoint.
		
		  The LSN we need to pass to the log items on transaction
		  commit is the LSN reported by the first log vector write, not
		  the commit lsn. If we use the commit record lsn then we can
		  move the tail beyond the grant write head.
	
	  Take a reference to the iclog for the context so that we still hold
	  it when xlog_write is done and has released it. This means the
	  context controls when the iclog is released for IO.
	
	  xlog_state_get_iclog_space() guarantees there is enough space in the
	  iclog for an entire commit record, so we can attach the context
	  callbacks now.  This needs to be done before we make the commit_lsn
	  visible to waiters so that checkpoints with commit records in the
	  same iclog order their IO completion callbacks in the same order that
	  the commit records appear in the iclog.
	
	  Now we can record the commit LSN and wake anyone waiting for this
	  sequence to have the ordered commit record assigned to a physical
	  location in the log.
  Ensure that the order of log writes follows checkpoint sequence order. This
  relies on the context LSN being zero until the log write has guaranteed the
  LSN that the log write will start at via xlog_state_get_iclog_space().
		
		  Avoid getting stuck in this loop because we were woken by the
		  shutdown, but then went back to sleep once already in the
		  shutdown state.
		
		  Higher sequences will wait for this one so skip them.
		  Don't wait for our own sequence, either.
 Wait until the LSN for the record has been recorded. 
  Write out the log vector change now attached to the CIL context. This will
  write a start record that needs to be strictly ordered in ascending CIL
  sequence order so that log recovery will always use in-order start LSNs when
  replaying checkpoints.
  Write out the commit record of a checkpoint transaction to close off a
  running log write. These commit records are strictly ordered in ascending CIL
  sequence order so that log recovery will always replay the checkpoints in the
  correct order.
  Push the Committed Item List to the log.
  If the current sequence is the same as xc_push_seq we need to do a flush. If
  xc_push_seq is less than the current sequence, then it has already been
  flushed and we don't need to do anything - the caller will wait for it to
  complete if necessary.
  xc_push_seq is checked unlocked against the sequence number for a match.
  Hence we can allow log forces to run racily and not issue pushes for the
  same sequence twice.  If we get a race between multiple pushes for the same
  sequence they will block on the first one and then abort, hence avoiding
  needless pushes.
	
	  As we are about to switch to a new, empty CIL context, we no longer
	  need to throttle tasks on CIL space overruns. Wake any waiters that
	  the hard push throttle may have caught so they can start committing
	  to the new context. The ctx->xc_push_lock provides the serialisation
	  necessary for safely using the lockless waitqueue_active() check in
	  this context.
	
	  Check if we've anything to push. If there is nothing, then we don't
	  move on to a new sequence number and so we have to be able to push
	  this sequence again later.
 check for a previously pushed sequence 
	
	  We are now going to push this context, so add it to the committing
	  list before we do anything else. This ensures that anyone waiting on
	  this push can easily detect the difference between a "push in
	  progress" and "CIL is empty, nothing to do".
	 
	  IOWs, a wait loop can now check for:
	 	the current sequence not being found on the committing list;
	 	an empty CIL; and
	 	an unchanged sequence number
	  to detect a push that had nothing to do and therefore does not need
	  waiting on. If the CIL is not empty, we get put on the committing
	  list before emptying the CIL and bumping the sequence number. Hence
	  an empty CIL and an unchanged sequence number means we jumped out
	  above after doing nothing.
	 
	  Hence the waiter will either find the commit sequence on the
	  committing list or the sequence number will be unchanged and the CIL
	  still dirty. In that latter case, the push has not yet started, and
	  so the waiter will have to continue trying to check the CIL
	  committing list until it is found. In extreme cases of delay, the
	  sequence may fully commit between the attempts the wait makes to wait
	  on the commit sequence.
	
	  The CIL is stable at this point - nothing new will be added to it
	  because we hold the flush lock exclusively. Hence we can now issue
	  a cache flush to ensure all the completed metadata in the journal we
	  are about to overwrite is on stable storage.
	 
	  Because we are issuing this cache flush before we've written the
	  tail lsn to the iclog, we can have metadata IO completions move the
	  tail forwards between the completion of this flush and the iclog
	  being written. In this case, we need to re-issue the cache flush
	  before the iclog write. To detect whether the log tail moves, sample
	  the tail LSN before we issue the flush.
	
	  Pull all the log vectors off the items in the CIL, and remove the
	  items from the CIL. We don't need the CIL lock here because it's only
	  needed on the transaction commit side which is currently locked out
	  by the flush lock.
	
	  Switch the contexts so we can drop the context lock and move out
	  of a shared context. We can't just go straight to the commit record,
	  though - we need to synchronise with previous and future commits so
	  that the commit records are correctly ordered in the log to ensure
	  that we process items during log IO completion in the correct order.
	 
	  For example, if we get an EFI in one checkpoint and the EFD in the
	  next (e.g. due to log forces), we do not want the checkpoint with
	  the EFD to be committed before the checkpoint with the EFI.  Hence
	  we must strictly order the commit records of the checkpoints so
	  that: a) the checkpoint callbacks are attached to the iclogs in the
	  correct order; and b) the checkpoints are replayed in correct order
	  in log recovery.
	 
	  Hence we need to add this context to the committing context list so
	  that higher sequences will wait for us to write out a commit record
	  before they do.
	 
	  xfs_log_force_seq requires us to mirror the new sequence into the cil
	  structure atomically with the addition of this sequence to the
	  committing list. This also ensures that we can do unlocked checks
	  against the current sequence in log forces without risking
	  deferencing a freed context pointer.
	
	  Build a checkpoint transaction header and write it to the log to
	  begin the transaction. We need to account for the space used by the
	  transaction header here as it is not accounted for in xlog_write().
	 
	  The LSN we need to pass to the log items on transaction commit is
	  the LSN reported by the first log vector write. If we use the commit
	  record lsn then we can move the tail beyond the grant write head.
	
	  Before we format and submit the first iclog, we have to ensure that
	  the metadata writeback ordering cache flush is complete.
	
	  If the checkpoint spans multiple iclogs, wait for all previous iclogs
	  to complete before we submit the commit_iclog. We can't use state
	  checks for this - ACTIVE can be either a past completed iclog or a
	  future iclog being filled, while WANT_SYNC through SYNC_DONE can be a
	  past or future iclog awaiting IO or ordered IO completion to be run.
	  In the latter case, if it's a future iclog and we wait on it, the we
	  will hang because it won't get processed through to ic_force_wait
	  wakeup until this commit_iclog is written to disk.  Hence we use the
	  iclog header lsn and compare it to the commit lsn to determine if we
	  need to wait on iclogs or not.
			
			  Waiting on ic_force_wait orders the completion of
			  iclogs older than ic_prev. Hence we only need to wait
			  on the most recent older iclog here.
		
		  We need to issue a pre-flush so that the ordering for this
		  checkpoint is correctly preserved down to stable storage.
	
	  The commit iclog must be written to stable storage to guarantee
	  journal IO vs metadata writeback IO is correctly ordered on stable
	  storage.
	 
	  If the push caller needs the commit to be immediately stable and the
	  commit_iclog is not yet marked as XLOG_STATE_WANT_SYNC to indicate it
	  will be written when released, switch it's state to WANT_SYNC right
	  now.
 Not safe to reference ctx now! 
 Not safe to reference ctx now! 
  We need to push CIL every so often so we don't cache more than we can fit in
  the log. The limit really is that a checkpoint can't be more than half the
  log (the current checkpoint is not allowed to overwrite the previous
  checkpoint), but commit latency and memory usage limit this to a smaller
  size.
	
	  The cil won't be empty because we are called while holding the
	  context lock so whatever we added to the CIL will still be there
	
	  Don't do a background push if we haven't used up all the
	  space available yet.
	
	  Drop the context lock now, we can't hold that if we need to sleep
	  because we are over the blocking threshold. The push_lock is still
	  held, so blocking threshold sleepwakeup is still correctly
	  serialised here.
	
	  If we are well over the space limit, throttle the work that is being
	  done until the push work on this context has begun. Enforce the hard
	  throttle on all transaction commits once it has been activated, even
	  if the committing transactions have resulted in the space usage
	  dipping back down under the hard limit.
	 
	  The ctx->xc_push_lock provides the serialisation necessary for safely
	  using the lockless waitqueue_active() check in this context.
  xlog_cil_push_now() is used to trigger an immediate CIL push to the sequence
  number that is passed. When it returns, the work will be queued for
  @push_seq, but it won't be completed.
  If the caller is performing a synchronous force, we will flush the workqueue
  to get previously queued work moving to minimise the wait time they will
  undergo waiting for all outstanding pushes to complete. The caller is
  expected to do the required waiting for push_seq to complete.
  If the caller is performing an async push, we need to ensure that the
  checkpoint is fully flushed out of the iclogs when we finish the push. If we
  don't do this, then the commit record may remain sitting in memory in an
  ACTIVE iclog. This then requires another full log force to push to disk,
  which defeats the purpose of having an async, non-blocking CIL force
  mechanism. Hence in this case we need to pass a flag to the push work to
  indicate it needs to flush the commit record itself.
 start on any pending background push to minimise wait time on it 
	
	  If the CIL is empty or we've already pushed the sequence then
	  there's no work we need to do.
  Commit a transaction with the given vector to the Committed Item List.
  To do this, we need to format the item, pin it in memory if required and
  account for the space used by the transaction. Once we have done that we
  need to release the unused reservation for the transaction, attach the
  transaction to the checkpoint context so we carry the busy extents through
  to checkpoint completion, and then unlock all the items in the transaction.
  Called with the context lock already held in read mode to lock out
  background commit, returns without it held once background commits are
  allowed again.
	
	  Do all necessary memory allocation before we lock the CIL.
	  This ensures the allocation does not deadlock with a CIL
	  push in memory reclaim (e.g. from kswapd).
 lock out background commit 
	
	  Once all the items of the transaction have been copied to the CIL,
	  the items can be unlocked and possibly freed.
	 
	  This needs to be done before we drop the CIL context lock because we
	  have to update state in the log items and unlock them before they go
	  to disk. If we don't, then the CIL checkpoint can race with us and
	  we can run checkpoint completion before we've updated and unlocked
	  the log items. This affects (at least) processing of stale buffers,
	  inodes and EFIs.
 xlog_cil_push_background() releases cil->xc_ctx_lock 
  Flush the CIL to stable storage but don't wait for it to complete. This
  requires the CIL push to ensure the commit record for the push hits the disk,
  but otherwise is no different to a push done from a log force.
  Conditionally push the CIL based on the sequence passed in.
  We only need to push if we haven't already pushed the sequence number given.
  Hence the only time we will trigger a push here is if the push sequence is
  the same as the current context.
  We return the current commit lsn to allow the callers to determine if a
  iclog flush is necessary following this call.
	
	  check to see if we need to force out the current context.
	  xlog_cil_push() handles racing pushes for the same sequence,
	  so no need to deal with it here.
	
	  See if we can find a previous sequence still committing.
	  We need to wait for all previous sequence commits to complete
	  before allowing the force of push_seq to go ahead. Hence block
	  on commits for those as well.
		
		  Avoid getting stuck in this loop because we were woken by the
		  shutdown, but then went back to sleep once already in the
		  shutdown state.
			
			  It is still being pushed! Wait for the push to
			  complete, then start again from the beginning.
 found it! 
	
	  The call to xlog_cil_push_now() executes the push in the background.
	  Hence by the time we have got here it our sequence may not have been
	  pushed yet. This is true if the current sequence still matches the
	  push sequence after the above wait loop and the CIL still contains
	  dirty objects. This is guaranteed by the push code first adding the
	  context to the committing list before emptying the CIL.
	 
	  Hence if we don't find the context in the committing list and the
	  current sequence number is unchanged then the CIL contents are
	  significant.  If the CIL is empty, if means there was nothing to push
	  and that means there is nothing to wait for. If the CIL is not empty,
	  it means we haven't yet started the push, because if it had started
	  we would have found the context on the committing list.
	
	  We detected a shutdown in progress. We need to trigger the log force
	  to pass through it's iclog state machine error handling, even though
	  we are already in a shutdown state. Hence we can't return
	  NULLCOMMITLSN here as that has special meaning to log forces (i.e.
	  LSN is already stable), so we return a zero LSN instead.
  Check if the current log item was first committed in this sequence.
  We can't rely on just the log item being in the CIL, we have to check
  the recorded commit sequence number.
  Note: for this to be used in a non-racy manner, it has to be called with
  CIL flushing locked out. As a result, it should only be used during the
  transaction commit process when deciding what to format into the item.
	
	  li_seq is written on the first commit of a log item to record the
	  first checkpoint it is written to. Hence if it is different to the
	  current sequence, we're in a new checkpoint.
  Perform initial CIL structure initialisation.
	
	  Limit the CIL pipeline depth to 4 concurrent works to bound the
	  concurrency the log spinlocks will be exposed to.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  All Rights Reserved.
  Used in xfs_itruncate_extents().  This is the maximum number of extents
  freed from a file in a single transaction.
  helper function to extract extent size hint from inode
	
	  No point in aligning allocations if we need to COW to actually
	  write to them.
  Helper function to extract CoW extent size hint from inode.
  Between the extent size hint and the CoW extent size hint, we
  return the greater of the two.  If the value is zero (automatic),
  use the default size.
  These two are wrapper routines around the xfs_ilock() routine used to
  centralize some grungy code.  They are used in places that wish to lock the
  inode solely for reading the extents.  The reason these places can't just
  call xfs_ilock(ip, XFS_ILOCK_SHARED) is that the inode lock also guards to
  bringing in of the extents from disk for a file in b-tree format.  If the
  inode is in b-tree format, then we need to lock the inode exclusively until
  the extents are read in.  Locking it exclusively all the time would limit
  our parallelism unnecessarily, though.  What we do instead is check to see
  if the extents have been read in yet, and only lock the inode exclusively
  if they have not.
  The functions return a value which should be given to the corresponding
  xfs_iunlock() call.
  In addition to i_rwsem in the VFS inode, the xfs inode contains 2
  multi-reader locks: invalidate_lock and the i_lock.  This routine allows
  various combinations of the locks to be obtained.
  The 3 locks should always be ordered so that the IO lock is obtained first,
  the mmap lock second and the ilock last in order to prevent deadlock.
  Basic locking order:
  i_rwsem -> invalidate_lock -> page_lock -> i_ilock
  mmap_lock locking order:
  i_rwsem -> page lock -> mmap_lock
  mmap_lock -> invalidate_lock -> page_lock
  The difference in mmap_lock locking order mean that we cannot hold the
  invalidate_lock over syscall based read(2)write(2) based IO. These IO paths
  can fault in pages during copy inout (for buffered IO) or require the
  mmap_lock in get_user_pages() to map the user pages into the kernel address
  space for direct IO. Similarly the i_rwsem cannot be taken inside a page
  fault because page faults already hold the mmap_lock.
  Hence to serialise fully against both syscall and mmap based IO, we need to
  take both the i_rwsem and the invalidate_lock. These locks should only be
  both taken in places where we need to invalidate the page cache in a race
  free manner (e.g. truncate, hole punch and other extent manipulation
  functions).
	
	  You can't set both SHARED and EXCL for the same lock,
	  and only XFS_IOLOCK_SHARED, XFS_IOLOCK_EXCL, XFS_ILOCK_SHARED,
	  and XFS_ILOCK_EXCL are valid values to set in lock_flags.
  This is just like xfs_ilock(), except that the caller
  is guaranteed not to sleep.  It returns 1 if it gets
  the requested locks and 0 otherwise.  If the IO lock is
  obtained but the inode lock cannot be, then the IO lock
  is dropped before returning.
  ip -- the inode being locked
  lock_flags -- this parameter indicates the inode's locks to be
        to be locked.  See the comment for xfs_ilock() for a list
 	 of valid values.
	
	  You can't set both SHARED and EXCL for the same lock,
	  and only XFS_IOLOCK_SHARED, XFS_IOLOCK_EXCL, XFS_ILOCK_SHARED,
	  and XFS_ILOCK_EXCL are valid values to set in lock_flags.
  xfs_iunlock() is used to drop the inode locks acquired with
  xfs_ilock() and xfs_ilock_nowait().  The caller must pass
  in the flags given to xfs_ilock() or xfs_ilock_nowait() so
  that we know which locks to drop.
  ip -- the inode being unlocked
  lock_flags -- this parameter indicates the inode's locks to be
        to be unlocked.  See the comment for xfs_ilock() for a list
 	 of valid values for this parameter.
	
	  You can't set both SHARED and EXCL for the same lock,
	  and only XFS_IOLOCK_SHARED, XFS_IOLOCK_EXCL, XFS_ILOCK_SHARED,
	  and XFS_ILOCK_EXCL are valid values to set in lock_flags.
  give up write locks.  the io lock cannot be held nested
  if it is being demoted.
	
	  We are checking that the lock is held at least in shared
	  mode but don't care that it might be held exclusively
	  (i.e. shared | excl). Hence we check if the lock is held
	  in any mode rather than an explicit shared mode.
  xfs_lockdep_subclass_ok() is only used in an ASSERT, so is only called when
  DEBUG or XFS_WARN is set. And MAX_LOCKDEP_SUBCLASSES is then only defined
  when CONFIG_LOCKDEP is set. Hence the complex define below to avoid build
  errors and warnings.
  Bump the subclass so xfs_lock_inodes() acquires each lock with a different
  value. This can be called for any type of inode lock combination, including
  parent locking. Care must be taken to ensure we don't overrun the subclass
  storage fields in the class mask we build.
  The following routine will lock n inodes in exclusive mode.  We assume the
  caller calls us with the inodes in i_ino order.
  We need to detect deadlock where an inode that we lock is in the AIL and we
  start waiting for another inode that is locked by a thread in a long running
  transaction (such as truncate). This can result in deadlock since the long
  running trans might need to wait for the inode we just locked in order to
  push the tail and free space in the log.
  xfs_lock_inodes() can only be used to lock one type of lock at a time -
  the iolock, the mmaplock or the ilock, but not more than one at a time. If we
  lock more than one at a time, lockdep will report false positives saying we
  have violated locking orders.
	
	  Currently supports between 2 and 5 inodes with exclusive locking.  We
	  support an arbitrary depth of locking here, but absolute limits on
	  inodes depend on the type of locking and the limits placed by
	  lockdep annotations in xfs_lock_inumorder.  These are all checked by
	  the asserts.
 Already locked 
		
		  If try_lock is not set yet, make sure all locked inodes are
		  not in the AIL.  If any are, set try_lock to be used later.
		
		  If any of the previous locks we have locked is in the AIL,
		  we must TRY to get the second and subsequent locks. If
		  we can't get any, we must release all we have
		  and try again.
 try_lock means we have an inode locked that is in the AIL. 
		
		  Unlock all previous guys and try again.  xfs_iunlock will try
		  to push the tail if the inode is in the AIL.
			
			  Check to see if we've already unlocked this one.  Not
			  the first one going back, and the inode ptr is the
			  same.
 Don't just spin the CPU 
  xfs_lock_two_inodes() can only be used to lock ilock. The iolock and
  mmaplock must be double-locked separately since we use i_rwsem and
  invalidate_lock for that. We now support taking one lock EXCL and the
  other SHARED.
	
	  If the first lock we have locked is in the AIL, we must TRY to get
	  the second lock. If we can't get it, we must release the first one
	  and try again.
 Don't just spin the CPU 
  Lookups up an inode from "name". If ci_name is not NULL, then a CI match
  is allowed, otherwise it has to be an exact match. If a CI match is found,
  ci_name->name will point to a the actual name (caller must free) or
  will be set to NULL if an exact match is found.
 Propagate di_flags from a parent inode to a child inode. 
	
	  Inode verifiers on older kernels only check that the extent size
	  hint is an integer multiple of the rt extent size on realtime files.
	  They did not check the hint alignment on a directory with both
	  rtinherit and extszinherit flags set.  If the misaligned hint is
	  propagated from a directory into a new realtime file, new file
	  allocations will fail due to math errors in the rt allocator andor
	  trip the verifiers.  Validate the hint settings in the new file so
	  that we don't let broken hints propagate.
 Propagate di_flags2 from a parent inode to a child inode. 
 Don't let invalid cowextsize hints propagate. 
  Initialise a newly allocated inode and return the in-core inode to the
  caller locked exclusively.
	
	  Protect against obviously corrupt allocation btree records. Later
	  xfs_iget checks will catch re-allocation of other active in-memory
	  and on-disk inodes. If we don't catch reallocating the parent inode
	  here we will deadlock in xfs_iget() so we have to do these checks
	  first.
	
	  Get the in-core inode with the lock held exclusively to prevent
	  others from looking at until we're done.
	
	  If the group ID of the new file does not match the effective group
	  ID or one of the supplementary group IDs, the S_ISGID bit is cleared
	  (and only if the irix_sgid_inherit compatibility variable is set).
	
	  If we need to create attributes immediately after allocating the
	  inode, initialise an empty attribute fork right now. We use the
	  default fork offset for attributes here as we don't know exactly what
	  size or how many attributes we might be adding. We can do this
	  safely here because we know the data fork is completely empty and
	  this saves us from needing to run a separate transaction to set the
	  fork offset in the immediate future.
	
	  Log the new values stuffed into the inode.
 now that we have an i_mode we can setup the inode structure 
  Decrement the link count on an inode & log the change.  If this causes the
  link count to go to zero, move the inode to AGI unlinked list so that it can
  be freed when the last active reference goes away via xfs_inactive().
 error 
  Increment the link count on an inode & log the change.
	
	  Make sure that we have allocated dquot(s) on disk.
	
	  Initially assume that the file does not exist and
	  reserve the resources for that case.  If that is not
	  the case we'll drop the one we have and get a more
	  appropriate transaction later.
 flush outstanding delalloc blocks and retry 
	
	  A newly created regular or special file just has one directory
	  entry pointing to them, but a directory also the "." entry
	  pointing to itself.
	
	  Now we join the directory inode to the transaction.  We do not do it
	  earlier because xfs_dialloc might commit the previous transaction
	  (and release all the locks).  An error from here on will result in
	  the transaction cancel unlocking dp so don't do it explicitly in the
	  error path.
	
	  If this is a synchronous mount, make sure that the
	  create transaction goes to disk before returning to
	  the user.
	
	  Attach the dquot(s) to the inodes and modify them incore.
	  These ids of the inode couldn't have changed since the new
	  inode has been locked ever since it was created.
	
	  Wait until after the current transaction is aborted to finish the
	  setup of the inode and release the inode.  This prevents recursive
	  transactions and deadlocks from xfs_inactive.
	
	  Make sure that we have allocated dquot(s) on disk.
	
	  Attach the dquot(s) to the inodes and modify them incore.
	  These ids of the inode couldn't have changed since the new
	  inode has been locked ever since it was created.
	
	  Wait until after the current transaction is aborted to finish the
	  setup of the inode and release the inode.  This prevents recursive
	  transactions and deadlocks from xfs_inactive.
	
	  If we are using project inheritance, we only allow hard link
	  creation in our tree when the project IDs are the same; else
	  the tree quota mechanism could be circumvented.
	
	  Handle initial link state of O_TMPFILE inode
	
	  If this is a synchronous mount, make sure that the
	  link transaction goes to disk before returning to
	  the user.
 Clear the reflink flag and the cowblocks tag if possible. 
  Free up the underlying blocks past new_size.  The new size must be smaller
  than the current size.  This routine can be used both for the attribute and
  data fork, and does not modify the inode size, which is left to the caller.
  The transaction passed to this routine must have made a permanent log
  reservation of at least XFS_ITRUNCATE_LOG_RES.  This routine may commit the
  given transaction and start new ones, so make sure everything involved in
  the transaction is tidy before calling here.  Some transaction will be
  returned to the caller to be committed.  The incoming transaction must
  already include the inode, and both inode locks must be held exclusively.
  The inode must also be "held" within the transaction.  On return the inode
  will be "held" within the returned transaction.  This routine does NOT
  require any disk space to be reserved for it within the transaction.
  If we get an error, we must return with the inode locked and linked into the
  current transaction. This keeps things simple for the higher level code,
  because it always knows that the inode is locked and held in the transaction
  that returns to it whether errors occur or not.  We don't mark the inode
  dirty on error so that transactions can be easily aborted if possible.
	
	  Since it is possible for space to become allocated beyond
	  the end of the file (in a crash where the space is allocated
	  but the inode size is not yet updated), simply remove any
	  blocks which show up between the new EOF and the maximum
	  possible file size.
	 
	  We have to free all the blocks to the bmbt maximum offset, even if
	  the page cache can't scale that far.
 free the just unmapped extents 
 Remove all pending CoW reservations. 
	
	  Always re-log the inode so that our permanent transaction can keep
	  on rolling it forward in the log.
 If this is a read-only mount, don't do this (would generate IO) 
		
		  If we previously truncated this file and removed old data
		  in the process, we want to initiate "early" writeout on
		  the last close.  This is an attempt to combat the notorious
		  NULL files problem which is particularly noticeable from a
		  truncate down, buffered (re-)write (delalloc), followed by
		  a crash.  What we are effectively doing here is
		  significantly reducing the time window where we'd otherwise
		  be exposed to that problem.
	
	  If we can't get the iolock just skip truncating the blocks past EOF
	  because we could deadlock with the mmap_lock otherwise. We'll get
	  another chance to drop them once the last reference to the inode is
	  dropped, so we'll never leak blocks permanently.
		
		  Check if the inode is being opened, written and closed
		  frequently and we have delayed allocation blocks outstanding
		  (e.g. streaming writes from the NFS server), truncating the
		  blocks past EOF will cause fragmentation to occur.
		 
		  In this case don't do the truncation, but we have to be
		  careful how we detect this case. Blocks beyond EOF show up as
		  i_delayed_blks even when the inode is clean, so we need to
		  truncate them away first before checking for a dirty release.
		  Hence on the first dirty close we will still remove the
		  speculative allocation, but after that we will leave it in
		  place.
 delalloc blocks after truncation means it really is dirty 
  xfs_inactive_truncate
  Called to perform a truncate when an inode becomes unlinked.
	
	  Log the inode size first to prevent stale data exposure in the event
	  of a system crash before the truncate completes. See the related
	  comment in xfs_vn_setattr_size() for details.
  xfs_inactive_ifree()
  Perform the inode free when an inode is unlinked.
	
	  We try to use a per-AG reservation for any block needed by the finobt
	  tree, but as the finobt feature predates the per-AG reservation
	  support a degraded file system might not have enough space for the
	  reservation at mount time.  In that case try to dip into the reserved
	  pool and pray.
	 
	  Send a warning if the reservation does happen to fail, as the inode
	  now remains allocated and sits on the unlinked list until the fs is
	  repaired.
	
	  We do not hold the inode locked across the entire rolling transaction
	  here. We only need to hold it for the first transaction that
	  xfs_ifree() builds, which may mark the inode XFS_ISTALE if the
	  underlying cluster buffer is freed. Relogging an XFS_ISTALE inode
	  here breaks the relationship between cluster buffer invalidation and
	  stale inode invalidation on cluster buffer item journal commit
	  completion, and can result in leaving dirty stale inodes hanging
	  around in memory.
	 
	  We have no need for serialising this inode operation against other
	  operations - we freed the inode and hence reallocation is required
	  and that will serialise on reallocating the space the deferops need
	  to free. Hence we can unlock the inode on the first commit of
	  the transaction rather than roll it right through the deferops. This
	  avoids relogging the XFS_ISTALE inode.
	 
	  We check that xfs_ifree() hasn't grown an internal transaction roll
	  by asserting that the inode is still locked when it returns.
		
		  If we fail to free the inode, shut down.  The cancel
		  might do that, we need to make sure.  Otherwise the
		  inode might be lost for a long time or forever.
	
	  Credit the quota account(s). The inode is gone.
	
	  Just ignore errors at this point.  There is nothing we can do except
	  to try to keep going. Make sure it's not a silent error.
  Returns true if we need to update the on-disk metadata before we can free
  the memory used by this inode.  Updates include freeing post-eof
  preallocations; freeing COW staging extents; and marking the inode free in
  the inobt if it is on the unlinked list.
	
	  If the inode is already free, then there can be nothing
	  to clean up here.
 If this is a read-only mount, don't do this (would generate IO) 
 If the log isn't running, push inodes straight to reclaim. 
 Metadata inodes require explicit resource cleanup. 
 Want to clean out the cow blocks if there are any. 
 Unlinked files must be freed. 
	
	  This file isn't being freed, so check if there are post-eof blocks
	  to free.  @force is true because we are evicting an inode from the
	  cache.  Post-eof blocks must be freed, lest we end up with broken
	  free space accounting.
	 
	  Note: don't bother with iolock here since lockdep complains about
	  acquiring it in reclaim context. We have the only reference to the
	  inode at this point anyways.
  xfs_inactive
  This is called when the vnode reference count for the vnode
  goes to zero.  If the file has been unlinked, then it must
  now be truncated.  Also, we clear all of the read-ahead state
  kept for the inode here since the file is now closed.
	
	  If the inode is already free, then there can be nothing
	  to clean up here.
 If this is a read-only mount, don't do this (would generate IO) 
 Metadata inodes require explicit resource cleanup. 
 Try to clean out the cow blocks if there are any. 
		
		  force is true because we are evicting an inode from the
		  cache. Post-eof blocks must be freed, lest we end up with
		  broken free space accounting.
		 
		  Note: don't bother with iolock here since lockdep complains
		  about acquiring it in reclaim context. We have the only
		  reference to the inode at this point anyways.
	
	  If there are attributes associated with the file then blow them away
	  now.  The code calls a routine that recursively deconstructs the
	  attribute fork. If also blows away the in-core attribute fork.
	
	  Free the inode.
	
	  We're done making metadata updates for this inode, so we can release
	  the attached dquots.
  In-Core Unlinked List Lookups
  =============================
  Every inode is supposed to be reachable from some other piece of metadata
  with the exception of the root directory.  Inodes with a connection to a
  file descriptor but not linked from anywhere in the on-disk directory tree
  are collectively known as unlinked inodes, though the filesystem itself
  maintains links to these inodes so that on-disk metadata are consistent.
  XFS implements a per-AG on-disk hash table of unlinked inodes.  The AGI
  header contains a number of buckets that point to an inode, and each inode
  record has a pointer to the next inode in the hash chain.  This
  singly-linked list causes scaling problems in the iunlink remove function
  because we must walk that list to find the inode that points to the inode
  being removed from the unlinked hash bucket list.
  What if we modelled the unlinked list as a collection of records capturing
  "X.next_unlinked = Y" relations?  If we indexed those records on Y, we'd
  have a fast way to look up unlinked list predecessors, which avoids the
  slow list walk.  That's exactly what we do here (in-core) with a per-AG
  rhashtable.
  Because this is a backref cache, we ignore operational failures since the
  iunlink code can fall back to the slow bucket walk.  The only errors that
  should bubble out are for obviously incorrect situations.
  All users of the backref cache MUST hold the AGI buffer lock to serialize
  access or have otherwise provided for concurrency control.
 Capture a "X.next_unlinked = Y" relationship. 
 X 
 Y 
 Unlinked list predecessor lookup hashtable construction 
  Return X, where X.next_unlinked == @agino.  Returns NULLAGINO if no such
  relation is found.
  Take ownership of an iunlink cache entry and insert it into the hash table.
  If successful, the entry will be owned by the cache; if not, it is freed.
  Either way, the caller does not own @iu after this call.
	
	  Fail loudly if there already was an entry because that's a sign of
	  corruption of in-memory data.  Also fail loudly if we see an error
	  code we didn't anticipate from the rhashtable code.  Currently we
	  only anticipate ENOMEM.
	
	  Absorb any runtime errors that aren't a result of corruption because
	  this is a cache and we can always fall back to bucket list scanning.
 Remember that @prev_agino.next_unlinked = @this_agino. 
  Replace X.next_unlinked = @agino with X.next_unlinked = @next_unlinked.
  If @next_unlinked is NULLAGINO, we drop the backref and exit.  If there
  wasn't any such entry then we don't bother.
 Look up the old entry; if there wasn't one then exit. 
	
	  Remove the entry.  This shouldn't ever return an error, but if we
	  couldn't remove the old entry we don't want to add it again to the
	  hash table, and if the entry disappeared on us then someone's
	  violated the locking rules and we need to fail loudly.  Either way
	  we cannot remove the inode because internal state is or would have
	  been corrupt.
 If there is no new next entry just free our item and return. 
 Update the entry and re-add it to the hash table. 
 Set up the in-core predecessor structures. 
 Free the in-core predecessor structures. 
  Point the AGI unlinked bucket at an inode and log the results.  The caller
  is responsible for validating the old value.
	
	  We should never find the head of the list already set to the value
	  passed in because either we're adding or removing ourselves from the
	  head of the list.
 Set an on-disk inode's next_unlinked pointer. 
 need to recalc the inode CRC if appropriate 
 Set an in-core inode's unlinked pointer and return the old value. 
 Make sure the old pointer isn't garbage. 
	
	  Since we're updating a linked list, we should never find that the
	  current pointer is the same as the new value, unless we're
	  terminating the list.
 Ok, update the new pointer. 
  This is called when the inode's link count has gone to 0 or we are creating
  a tmpfile via O_TMPFILE.  The inode @ip must have nlink == 0.
  We place the on-disk inode on a list in the AGI.  It will be pulled from this
  list when the inode is freed.
 Get the agi buffer first.  It ensures lock ordering on the list. 
	
	  Get the index into the agi hash table for the list this inode will
	  go on.  Make sure the pointer isn't garbage and that this inode
	  isn't already on the list.
		
		  There is already another inode in the bucket, so point this
		  inode to the current head of the list.
		
		  agino has been unlinked, add a backref from the next inode
		  back to agino.
 Point the head of the list to point to this inode. 
 Return the imap, dinode pointer, and buffer for an inode. 
  Walk the unlinked chain from @head_agino until we find the inode that
  points to @target_agino.  Return the inode number, map, dinode pointer,
  and inode cluster buffer of that inode as @agino, @imap, @dipp, and @bpp.
  @tp, @pag, @head_agino, and @target_agino are input parameters.
  @agino, @imap, @dipp, and @bpp are all output parameters.
  Do not call this function if @target_agino is the head of the list.
 See if our backref cache can find it faster. 
		
		  If we get here the cache contents were corrupt, so drop the
		  buffer and fall back to walking the bucket list.
 Otherwise, walk the entire bucket until we find it. 
		
		  Make sure this pointer is valid and isn't an obvious
		  infinite loop.
  Pull the on-disk inode from the AGI unlinked list.
 Get the agi buffer first.  It ensures lock ordering on the list. 
	
	  Get the index into the agi hash table for the list this inode will
	  go on.  Make sure the head pointer isn't garbage.
	
	  Set our inode's next_unlinked pointer to NULL and then return
	  the old pointer value so that we can update whatever was previous
	  to us in the list to point to whatever was next in the list.
	
	  If there was a backref pointing from the next inode back to this
	  one, remove it because we've removed this inode from the list.
	 
	  Later, if this inode was in the middle of the list we'll update
	  this inode's backref to point from the next inode.
 We need to search the list for the inode being freed. 
 Point the previous inode on the list to the next inode. 
		
		  Now we deal with the backref for this inode.  If this inode
		  pointed at a real inode, change the backref that pointed to
		  us to point to our old next.  If this inode was the end of
		  the list, delete the backref that pointed to us.  Note that
		  change_backref takes care of deleting the backref if
		  next_agino is NULLAGINO.
 Point the head of the list to the next unlinked inode. 
  Look up the inode number specified and if it is not already marked XFS_ISTALE
  mark it stale. We should only find clean inodes in this lookup that aren't
  already stale.
 Inode not in memory, nothing to do 
	
	  because this is an RCU protected lookup, we could find a recently
	  freed or even reallocated inode during the lookup. We need to check
	  under the i_flags_lock for a valid inode here. Skip it if it is not
	  valid, the wrong inode or stale.
	
	  Don't try to lockunlock the current inode, but we _cannot_ skip the
	  other inodes that we did not find in the list attached to the buffer
	  and are not already marked stale. If we can't lock it, back off and
	  retry.
	
	  If the inode is flushing, it is already attached to the buffer.  All
	  we needed to do here is mark the inode stale so buffer IO completion
	  will remove it from the AIL.
	
	  Inodes not attached to the buffer can be released immediately.
	  Everything else has to go through xfs_iflush_abort() on journal
	  commit as the flock synchronises removal of the inode from the
	  cluster buffer against inode reclaim.
 we have a dirty inode in memory that has not yet been flushed. 
  A big issue when freeing the inode cluster is that we _cannot_ skip any
  inodes that are in memory - they all must be marked stale and attached to
  the cluster buffer.
		
		  The allocation bitmap tells us which inodes of the chunk were
		  physically allocated. Skip the cluster if an inode falls into
		  a sparse region.
		
		  We obtain and lock the backing buffer first in the process
		  here to ensure dirty inodes attached to the buffer remain in
		  the flushing state while we mark them stale.
		 
		  If we scan the in-memory inodes first, then buffer IO can
		  complete before we get a lock on it, and hence we may fail
		  to mark all the active inodes on the buffer stale.
		
		  This buffer may not have been correctly initialised as we
		  didn't read it from disk. That's not important because we are
		  only using to mark the buffer as stale in the log, and to
		  attach stale cached inodes on it. That means it will never be
		  dispatched for IO. If it is, we want to know about it, and we
		  want it to fail. We can acheive this by adding a write
		  verifier to the buffer.
		
		  Now we need to set all the cached clean inodes as XFS_ISTALE,
		  too. This requires lookups, and will skip inodes that we've
		  already marked XFS_ISTALE.
  This is called to return an inode to the inode free list.
  The inode should already be truncated to 0 length and have
  no pages associated with it.  This routine also assumes that
  the inode is already a part of the transaction.
  The on-disk copy of the inode will have been added to the list
  of unlinked inodes in the AGI. We need to remove the inode from
  that list atomically with respect to freeing it here.
	
	  Pull the on-disk inode from the AGI unlinked list.
	
	  Free any local-format data sitting around before we reset the
	  data fork to extents format.  Note that the attr fork data has
	  already been freed by xfs_attr_inactive.
 mark incore inode as free 
 mark the attr fork not in use 
 Don't attempt to replay owner changes for a deleted inode 
	
	  Bump the generation count so no one will be confused
	  by reincarnations of this inode.
  This is called to unpin an inode.  The caller must have the inode locked
  in at least shared mode so that the buffer cannot be subsequently pinned
  once someone is waiting for it to be unpinned.
 Give the log a push to start the unpinning IO 
  Removing an inode from the namespace involves removing the directory entry
  and dropping the link count on the inode. Removing the directory entry can
  result in locking an AGF (directory blocks were freed) and removing a link
  count can result in placing the inode on an unlinked list which results in
  locking an AGI.
  The big problem here is that we have an ordering constraint on AGF and AGI
  locking - inode allocation locks the AGI, then can allocate a new extent for
  new inodes, locking the AGF after the AGI. Similarly, freeing the inode
  removes the inode from the unlinked list, requiring that we lock the AGI
  first, and then freeing the inode can result in an inode chunk being freed
  and hence freeing disk space requiring that we lock an AGF.
  Hence the ordering that is imposed by other parts of the code is AGI before
  AGF. This means we cannot remove the directory entry before we drop the inode
  reference count and put it on the unlinked list as this results in a lock
  order of AGF then AGI, and this can deadlock against inode allocation and
  freeing. Therefore we must drop the link counts before we remove the
  directory entry.
  This is still safe from a transactional point of view - it is not until we
  get to xfs_defer_finish() that we have the possibility of multiple
  transactions in this operation. Hence as long as we remove the directory
  entry and drop the link count in the first transaction of the remove
  operation, there are no transactional constraints on the ordering here.
	
	  We try to get the real space reservation first,
	  allowing for directory btree deletion(s) implying
	  possible bmap insert(s).  If we can't get the space
	  reservation then we use 0 instead, and avoid the bmap
	  btree insert(s) in the directory code by, if the bmap
	  insert tries to happen, instead trimming the LAST
	  block from the directory.
	
	  If we're removing a directory perform some additional validation.
 Drop the link from ip's "..".  
 Drop the "." link from ip to self.  
		
		  Point the unlinked child directory's ".." entry to the root
		  directory to eliminate back-references to inodes that may
		  get freed before the child directory is closed.  If the fs
		  gets shrunk, this can lead to dirent inode validation errors.
		
		  When removing a non-directory we need to log the parent
		  inode here.  For a directory this is done implicitly
		  by the xfs_droplink call for the ".." entry.
 Drop the link from dp to ip. 
	
	  If this is a synchronous mount, make sure that the
	  remove transaction goes to disk before returning to
	  the user.
  Enter all inodes for a rename transaction into a sorted array.
 in: old (source) directory inode 
 in: new (target) directory inode 
 in: inode of old entry 
 in: inode of new entry 
 in: whiteout inode 
 out: sorted array of inodes 
 inout: inodes in array 
	
	  i_tab contains a list of pointers to inodes.  We initialize
	  the table here & we'll sort it.  We will then use it to
	  order the acquisition of the inode locks.
	 
	  Note that the table may contain duplicates.  e.g., dp1 == dp2.
	
	  Sort the elements via bubble sort.  (Remember, there are at
	  most 5 elements to sort, so this is adequate.)
	
	  If this is a synchronous mount, make sure that the rename transaction
	  goes to disk before returning to the user.
  xfs_cross_rename()
  responsible for handling RENAME_EXCHANGE flag in renameat2() syscall
 Swap inode number for dirent in first parent 
 Swap inode number for dirent in second parent 
	
	  If we're renaming one or more directories across different parents,
	  update the respective ".." entries (and link counts) to match the new
	  parents.
 transfer ip2 ".." reference to dp1 
			
			  Although ip1 isn't changed here, userspace needs
			  to be warned about the change, so that applications
			  relying on it (like backup ones), will properly
			  notify the change
 transfer ip1 ".." reference to dp2 
			
			  Although ip2 isn't changed here, userspace needs
			  to be warned about the change, so that applications
			  relying on it (like backup ones), will properly
			  notify the change
  xfs_rename_alloc_whiteout()
  Return a referenced, unlinked, unlocked inode that can be used as a
  whiteout in a rename transaction. We use a tmpfile inode here so that if we
  crash between allocating the inode and linking it into the rename transaction
  recovery will free the inode and we won't leak it.
	
	  Prepare the tmpfile inode as if it were created through the VFS.
	  Complete the inode setup and flag it as linkable.  nlink is already
	  zero, so we can skip the drop_nlink.
  xfs_rename
 whiteout inode 
	
	  If we are doing a whiteout operation, allocate the whiteout inode
	  we will be placing at the target and ensure the type is set
	  appropriately.
 setup target dirent info as whiteout 
	
	  Attach the dquots to the inodes
	
	  Lock all the participating inodes. Depending upon whether
	  the target_name exists in the target directory, and
	  whether the target directory is the same as the source
	  directory, we can lock from 2 to 4 inodes.
	
	  Join all the inodes to the transaction. From this point on,
	  we can rely on either trans_commit or trans_cancel to unlock
	  them.
	
	  If we are using project inheritance, we only allow renames
	  into our tree when the project IDs are the same; else the
	  tree quota mechanism would be circumvented.
 RENAME_EXCHANGE is unique from here on. 
	
	  Check for expected errors before we dirty the transaction
	  so we can return an error without a transaction abort.
	 
	  Extent count overflow check:
	 
	  From the perspective of src_dp, a rename operation is essentially a
	  directory entry remove operation. Hence the only place where we check
	  for extent count overflow for src_dp is in
	  xfs_bmap_del_extent_real(). xfs_bmap_del_extent_real() returns
	  -ENOSPC when it detects a possible extent count overflow and in
	  response, the higher layers of directory handling code do the
	  following:
	  1. DataFree blocks: XFS lets these blocks linger until a
	     future remove operation removes them.
	  2. Dabtree blocks: XFS swaps the blocks with the last block in the
	     Leaf space and unmaps the last block.
	 
	  For target_dp, there are two cases depending on whether the
	  destination directory entry exists or not.
	 
	  When destination directory entry does not exist (i.e. target_ip ==
	  NULL), extent count overflow check is performed only when transaction
	  has a non-zero sized space reservation associated with it.  With a
	  zero-sized space reservation, XFS allows a rename operation to
	  continue only when the directory has sufficient free space in its
	  dataleaffree space blocks to hold the new entry.
	 
	  When destination directory entry exists (i.e. target_ip != NULL), all
	  we need to do is change the inode number associated with the already
	  existing entry. Hence there is no need to perform an extent count
	  overflow check.
		
		  If there's no space reservation, check the entry will
		  fit before actually inserting it.
		
		  If target exists and it's a directory, check that whether
		  it can be destroyed.
	
	  Lock the AGI buffers we need to handle bumping the nlink of the
	  whiteout inode off the unlinked list and to handle dropping the
	  nlink of the target inode.  Per locking order rules, do this in
	  increasing AG order and before directory block allocation tries to
	  grab AGFs because we grab AGIs before AGFs.
	 
	  The (vfs) caller must ensure that if src is a directory then
	  target_ip is either null or an empty directory.
	
	  Directory entry creation below may acquire the AGF. Remove
	  the whiteout from the unlinked list first to preserve correct
	  AGIAGF locking order. This dirties the transaction so failures
	  after this point will abort and log recovery will clean up the
	  mess.
	 
	  For whiteouts, we need to bump the link count on the whiteout
	  inode. After this point, we have a real link, clear the tmpfile
	  state flag from the inode so it doesn't accidentally get misused
	  in future.
	
	  Set up the target.
		
		  If target does not exist and the rename crosses
		  directories, adjust the target directory link count
		  to account for the ".." reference from the new entry.
 target_ip != NULL 
		
		  Link the source inode under the target name.
		  If the source inode is a directory and we are moving
		  it across directories, its ".." entry will be
		  inconsistent until we replace that down below.
		 
		  In case there is already an entry with the same
		  name at the destination directory, remove it first.
		
		  Decrement the link count on the target since the target
		  dir no longer points to it.
			
			  Drop the link from the old "." entry.
 target_ip != NULL 
	
	  Remove the source.
		
		  Rewrite the ".." entry to point to the new
		  directory.
	
	  We always want to hit the ctime on the source inode.
	 
	  This isn't strictly required by the standards since the source
	  inode isn't really being changed, but old unix file systems did
	  it and some incremental backup programs won't work without it.
	
	  Adjust the link count on src_dp.  This is necessary when
	  renaming a directory, either within one parent when
	  the target existed, or across two parent directories.
		
		  Decrement link count on src_directory since the
		  entry that's moved no longer points to it.
	
	  For whiteouts, we only need to update the source dirent with the
	  inode number of the whiteout inode rather than removing it
	  altogether.
		
		  NOTE: We don't need to check for extent count overflow here
		  because the dir remove name code will leave the dir block in
		  place if the extent count would overflow.
	
	  We don't flush the inode if any of the following checks fail, but we
	  do still update the log item and attach to the backing buffer as if
	  the flush happened. This is a formality to facilitate predictable
	  error handling as the caller will shutdown and fail the buffer.
	
	  Inode item log recovery for v2 inodes are dependent on the flushiter
	  count for correct sequencing.  We bump the flush iteration count so
	  we can detect flushes which postdate a log record during recovery.
	  This is redundant as we now log every change and hence this can't
	  happen but we need to still do it to ensure backwards compatibility
	  with old kernels that predate logging all inode changes.
	
	  If there are inline format data  attr forks attached to this inode,
	  make sure they are not corrupt.
	
	  Copy the dirty parts of the inode into the on-disk inode.  We always
	  copy out the core of the inode, because if the inode is dirty at all
	  the core must be.
 Wrap, we never let the log put out DI_MAX_FLUSH 
	
	  We've recorded everything logged in the inode, so we'd like to clear
	  the ili_fields bits so we don't log and flush things unnecessarily.
	  However, we can't stop logging all this information until the data
	  we've copied into the disk buffer is written to disk.  If we did we
	  might overwrite the copy of the inode in the log with all the data
	  after re-logging only part of it, and in the face of a crash we
	  wouldn't have all the data we need to recover.
	 
	  What we do is move the bits to the ili_last_fields field.  When
	  logging the inode, these bits are moved back to the ili_fields field.
	  In the xfs_buf_inode_iodone() routine we clear ili_last_fields, since
	  we know that the information those bits represent is permanently on
	  disk.  As long as the flush completes before the inode is logged
	  again, then both ili_fields and ili_last_fields will be cleared.
	
	  Store the current LSN of the inode so that we can tell whether the
	  item has moved in the AIL from xfs_buf_inode_iodone().
 generate the checksum. 
  Non-blocking flush of dirty inode metadata into the backing buffer.
  The caller must have a reference to the inode and hold the cluster buffer
  locked. The function will walk across all the inodes on the cluster buffer it
  can find and lock without blocking, and flush them to the cluster buffer.
  On successful flushing of at least one inode, the caller must write out the
  buffer and release it. If no inodes are flushed, -EAGAIN will be returned and
  the caller needs to release the buffer. On failure, the filesystem will be
  shut down, the buffer will have been unlocked and released, and EFSCORRUPTED
  will be returned.
	
	  We must use the safe variant here as on shutdown xfs_iflush_abort()
	  can remove itself from the list.
		
		  Quick and dirty check to avoid locks if possible.
		
		  The inode is still attached to the buffer, which means it is
		  dirty but reclaim might try to grab it. Check carefully for
		  that, and grab the ilock while still holding the i_flags_lock
		  to guarantee reclaim will not be able to reclaim this inode
		  once we drop the i_flags_lock.
		
		  ILOCK will pin the inode against reclaim and prevent
		  concurrent transactions modifying the inode while we are
		  flushing the inode. If we get the lock, set the flushing
		  state before we drop the i_flags_lock.
		
		  Abort flushing this inode if we are shut down because the
		  inode may not currently be in the AIL. This can occur when
		  log IO failure unpins the inode without inserting into the
		  AIL, leaving a dirtyunpinned inode attached to the buffer
		  that otherwise looks like it should be flushed.
 don't block waiting on a log force to unpin dirty inodes 
 Release an inode. 
  Ensure all commited transactions touching the inode are written to the log.
  Grab the exclusive iolock for a data copy from src to dest, making sure to
  abide vfs locking order (lowest pointer value goes first) and breaking the
  layout leases before proceeding.  The loop is needed because we cannot call
  the blocking break_layout() with the iolocks held, and therefore have to
  back out both locks.
 Wait to break both inodes' layouts before we start locking. 
 Lock one inode and make sure nobody got in and leased it. 
 Lock the other inode and make sure nobody got in and leased it. 
  Lock two inodes so that userspace cannot initiate IO via file syscalls or
  mmap activity.
 Unlock both inodes to allow IO and mmap activity. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  xfs_find_handle maps from userspace xfs_fsop_handlereq structure to
  a file or fs handle.
  XFS_IOC_PATH_TO_FSHANDLE
     returns fs handle for a mount point or path within that mount point
  XFS_IOC_FD_TO_HANDLE
     returns full handle for a FD opened in user space
  XFS_IOC_PATH_TO_HANDLE
     returns full handle for a path
	
	  We can only generate handles for inodes residing on a XFS filesystem,
	  and only for regular files, directories or symbolic links.
		
		  This handle only contains an fsid, zero the rest.
  No need to do permission checks on the various pathname components
  as the handle operations are privileged.
  Convert userspace handle data into a dentry.
	
	  Only allow handle opens under a directory.
 Restrict xfs_open_by_handle to directories & regular files. 
 Can't write directories. 
 Restrict this handle operation to symlinks only. 
  Format an attribute and copy it out to the user's buffer.
  Take care to check values and protect against them changing later,
  we may be reading them directly out of a user buffer.
	
	  Only list entries in the right namespace.
 decrement by the actual bytes used by the attr 
	
	  Reject flags, only allow namespaces.
	
	  Validate the cursor.
	
	  Initialize the output buffer.
 overflow check 
SEEK_SET
SEEK_CUR
SEEK_END
 Return 0 on success or positive error 
 done = 1 if there are more stats to get and if bulkstat 
 should be called again (unused here, but used in dmapi) 
	
	  FSBULKSTAT_SINGLE expects that lastip contains the inode number
	  that we want to stat.  However, FSINUMBERS and FSBULKSTAT expect
	  that lastip contains either zero or the number of the last inode to
	  be examined by the previous call and return results starting with
	  the next inode after that.  The new bulk request back end functions
	  take the inode to start with, so we have to compute the startino
	  parameter from lastino to maintain correct function.  lastino == 0
	  is a special case because it has traditionally meant "first inode
	  in filesystem".
 XFS_IOC_FSBULKSTAT 
 Return 0 on success or positive error 
  Check the incoming bulk request @hdr from userspace and initialize the
  internal @breq bulk request appropriately.  Returns 0 if the bulk request
  should proceed; -ECANCELED if there's nothing to do; or the usual
  negative error code.
	
	  The @ino parameter is a special value, so we must look it up here.
	  We're not allowed to have IREQ_AGNO, and we only return one inode
	  worth of data.
	
	  The IREQ_AGNO flag means that we only want results from a given AG.
	  If @hdr->ino is zero, we start iterating in that AG.  If @hdr->ino is
	  beyond the specified AG then we return no results.
 Asking for an inode past the end of the AG?  We're done! 
 Asking for an inode past the end of the FS?  We're done! 
  Update the userspace bulk request @hdr to reflect the end state of the
  internal bulk request @breq.
 Handle the v5 bulkstat ioctl. 
 Handle the v5 inumbers ioctl. 
  Linux extended inode flags interface.
		
		  Don't let a misaligned extent size hint on a directory
		  escape to userspace if it won't pass the setattr checks
		  later.
 can't set PREALLOC this way, just preserve it 
 Can't change realtime flag if any extents are allocated. 
 If realtime flag is set then must have realtime device 
 Clear reflink if we are actually able to set the rt flag. 
 Don't allow us to set DAX mode for a reflinked file for now. 
 diflags2 only valid for v3 inodes. 
  Set up the transaction structure for the setattr operation, checking that we
  have permission to do so. On success, return a clean transaction and the
  inode locked exclusively ready for further operation specific checks. On
  failure, return an error without modifying or locking the inode.
  Validate a proposed extent size hint.  For regular files, the hint can only
  be changed if no extents are allocated.
	
	  Inode verifiers do not check that the extent size hint is an integer
	  multiple of the rt extent size on a directory with both rtinherit
	  and extszinherit flags set.  Don't let sysadmins misconfigure
	  directories.
 Disallow 32bit project ids if 32bit IDs are not enabled. 
	
	  If disk quotas is on, we make sure that the dquots do exist on disk,
	  before we start any other transactions. Trying to do this later
	  is messy. We don't care to take a readlock to look at the ids
	  in inode here, because we can't hold it across the trans_reserve.
	  If the IDs do change before we take the ilock, we're covered
	  because the i_dquot fields will get updated anyway.
	
	  Change file ownership.  Must be the owner or privileged.  CAP_FSETID
	  overrides the following restrictions:
	 
	  The set-user-ID and set-group-ID bits of a file will be cleared upon
	  successful return from chown()
 Change the ownerships and register project quota modifications 
	
	  Only set the extent size hint if we've already determined that the
	  extent size hint should be set on the inode. If no extent size flags
	  are set on the inode then unconditionally clear the extent size hint.
	
	  Release any dquot(s) the inode had kept before chown.
 struct getbmap is a strict subset of struct getbmapx. 
	
	  Use an internal memory buffer so that we don't have to copy fsmap
	  data to userspace while holding locks.  Start by trying to allocate
	  up to 128k for the buffer, but fall back to a single page if needed.
 Run query, record how many entries we got. 
			
			  There are no more records in the result set.  Copy
			  whatever we got to userspace and break out.
			
			  The internal memory buffer is full.  Copy whatever
			  records we got to userspace and go again if we have
			  not yet filled the userspace buffer.
		
		  If the caller wanted a record count or there aren't any
		  new records to return, we're done.
 Copy all the records we got out to userspace. 
 Remember the last record flags we copied to userspace. 
 Set up the low key for the next iteration. 
	
	  If there are no more records in the query result set and we're not
	  in counting mode, mark the last record returned with the LAST flag.
 copy back header 
 Pull information for the target fd 
	
	  We need to ensure that the fds passed in point to XFS inodes
	  before we cast and access them as XFS structures as we have no
	  control over what the user passes us here.
 Paranoia 
 1 larger than sb_fname, so this ensures a trailing NUL char 
	
	  The generic ioctl allows up to FSLABEL_MAX chars, but XFS is much
	  smaller, at 12 bytes.  We copy one more to be sure we find the
	  (required) NULL character to test the incoming label length.
	  NB: The on disk label doesn't need to be null terminated.
	
	  Now we do several things to satisfy userspace.
	  In addition to normal logging of the primary superblock, we also
	  immediately write these changes to sector zero for the primary, then
	  update all backup supers (as xfs_db does for a label change), then
	  invalidate the block device page cache.  This is so that any prior
	  buffered reads from userspace (i.e. from blkid) are invalidated,
	  and userspace will see the newly-written label.
	
	  growfs also updates backup supers so lock against that.
  Note: some of the ioctl's return positive numbers as a
  byte count indicating success, such as readlink_by_handle.
  So we don't "sign flip" like most other routines.  This means
  true errors need to be returned as a negative value.
 input parameter is passed in resblks field of structure 
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2008 Christoph Hellwig.
  Portions Copyright (C) 2000-2008 Silicon Graphics, Inc.
 no flags implies user namespace 
 insufficient space 
 real name 
		
		  Only show root namespace entries if we are actually allowed to
		  see them.
	
	  First read the regular on-disk attributes.
 SPDX-License-Identifier: GPL-2.0
  Copyright (C) 2010 Red Hat, Inc.
  All Rights Reserved.
	
	  Force out the log.  This means any transactions that might have freed
	  space before we take the AGF buffer lock are now on disk, and the
	  volatile disk cache is flushed.
	
	  Look up the longest btree in the AGF and start with it.
	
	  Loop until we are done with all extents that are large
	  enough to be worth discarding.
		
		  use daddr format for all rangelen calculations as that is
		  the format the rangelen variables are supplied in by
		  userspace.
		
		  Too small?  Give up.
		
		  If the extent is entirely outside of the range we are
		  supposed to discard skip it.  Do not bother to trim
		  down partially overlapping ranges for now.
		
		  If any blocks in the range are still busy, skip the
		  discard and try again the next time.
  trim a range of the filesystem.
  Note: the parameters passed from userspace are byte ranges into the
  filesystem which does not match to the format we use for filesystem block
  addressing. FSB addressing is sparse (AGNO|AGBNO), while the incoming format
  is a linear address range. Hence we need to use DADDR based conversions and
  comparisons for determining the correct offset and regions to trim.
	
	  We haven't recovered the log, so we cannot use our bnobt-guided
	  storage zapping commands.
	
	  Truncating down the len isn't actually quite correct, but using
	  BBTOB would mean we trivially get overflows for values
	  of ULLONG_MAX or slightly lower.  And ULLONG_MAX is the default
	  used by the fstrim application.  In the end it really doesn't
	  matter as trimming blocks is an advisory interface.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
 Radix tree tags for incore inode tree. 
 inode is to be reclaimed 
 Inode has speculative preallocations (posteof or cow) to clean. 
  The goal for walking incore inodes.  These can correspond with incore inode
  radix tree tags when convenient.  Avoid existing XFS_IWALK namespace.
 Goals directly associated with tagged inodes. 
  Private inode cache walk flags for struct xfs_icwalk.  Must not
  coincide with XFS_ICWALK_FLAGS_VALID.
 Stop scanning after icw_scan_limit inodes. 
 union filter algorithm 
  Allocate and initialise an xfs_inode.
	
	  XXX: If this didn't occur in transactions, we could drop GFP_NOFAIL
	  and return NULL here on ENOMEM.
 VFS doesn't initialise i_mode or i_state! 
 initialise the xfs inode 
 asserts to verify all state is correct here 
	
	  Because we use RCU freeing we need to ensure the inode always
	  appears to be reclaimed with an invalid inode number when in the
	  free state. The ip->i_flags_lock provides the barrier against lookup
	  races.
  Queue background inode reclaim work if there are reclaimable inodes and there
  isn't reclaim work already scheduled or in progress.
  Background scanning to trim preallocated space. This is queued based on the
  'speculative_prealloc_lifetime' tunable (5m by default).
 Set a tag on both the AG incore inode tree and the AG radix tree. 
 propagate the tag up into the perag radix tree 
 start background work 
 Clear a tag on both the AG incore inode tree and the AG radix tree. 
	
	  Reclaim can signal (with a null agino) that it cleared its own tag
	  by removing the inode from the radix tree.
 clear the tag from the perag radix tree 
  When we recycle a reclaimable inode, we need to re-initialise the VFS inode
  part of the structure. This is made more complex by the fact we store
  information about the on-disk values in the VFS inode and so we can't just
  overwrite the values unconditionally. Hence we save the parameters we
  need to retain across reinitialisation, and rewrite them into the VFS inode
  after reinitialisation even if it fails.
  Carefully nudge an inode whose VFS state has been torn down back into a
  usable state.  Drops the i_flags_lock and the rcu read lock.
	
	  We need to make it look like the inode is being reclaimed to prevent
	  the actual reclaim workers from stomping over us while we recycle
	  the inode.  We can't clear the radix tree tag yet as it requires
	  pag_ici_lock to be held exclusive.
		
		  Re-initializing the inode failed, and we are in deep
		  trouble.  Try to re-add it to the reclaim list.
	
	  Clear the per-lifetime state in the inode as we are now effectively
	  a new inode and need to return to the initial state before reuse
	  occurs.
  If we are allocating a new inode, then check what was returned is
  actually a free, empty inode. If we are not allocating an inode,
  then check we didn't find a free inode.
  Returns:
 	0		if the inode free state matches the lookup context
 	-ENOENT		if the inode is free and we are not allocating
 	-EFSCORRUPTED	if there is any state mismatch at all
 should be a free inode 
 should be an allocated inode 
 Make all pending inactivation work start immediately. 
  Check the validity of the inode we just found it the cache
	
	  check for re-use of an inode within an RCU grace period due to the
	  radix tree nodes not being updated yet. We monitor for this by
	  setting the inode number to zero before freeing the inode structure.
	  If the inode has been reallocated and set up, then the inode number
	  will not match, so check for that, too.
	
	  If we are racing with another cache hit that is currently
	  instantiating this inode or currently recycling it out of
	  reclaimable state, wait for the initialisation to complete
	  before continuing.
	 
	  If we're racing with the inactivation worker we also want to wait.
	  If we're creating a new file, it's possible that the worker
	  previously marked the inode as free on disk but hasn't finished
	  updating the incore state yet.  The AGI buffer will be dirty and
	  locked to the icreate transaction, so a synchronous push of the
	  inodegc workers would result in deadlock.  For a regular iget, the
	  worker is running already, so we might as well wait.
	 
	  XXX(hch): eventually we should do something equivalent to
	 	     wait_on_inode to wait for these flags to be cleared
	 	     instead of polling for it.
 Unlinked inodes cannot be re-grabbed. 
	
	  Check the inode free state is valid. This also detects lookup
	  racing with unlinks.
 Skip inodes that have no vfs state. 
 The inode fits the selection criteria; process it. 
 Drops i_flags_lock and RCU read lock. 
 If the VFS inode is being torn down, pause and try again. 
 We've got a live one. 
	
	  Do not wait for the workers, because the caller could hold an AGI
	  buffer lock.  We're just going to sleep in a loop anyway.
	
	  For version 5 superblocks, if we are initialising a new inode and we
	  are not utilising the XFS_FEAT_IKEEP inode cluster mode, we can
	  simply build the new inode core with a random generation number.
	 
	  For version 4 (and older) superblocks, log recovery is dependent on
	  the i_flushiter field being initialised from the current on-disk
	  value and hence we must also read the inode off disk even when
	  initializing new inodes.
	
	  Check the inode free state is valid. This also detects lookup
	  racing with unlinks.
	
	  Preload the radix tree so we can insert safely under the
	  write spinlock. Note that we cannot sleep inside the preload
	  region. Since we can be called from transaction context, don't
	  recurse into the file system.
	
	  Because the inode hasn't been added to the radix-tree yet it can't
	  be found by another thread, so we can do the non-sleeping lock here.
	
	  These values must be set before inserting the inode into the radix
	  tree as the moment it is inserted a concurrent lookup (allowed by the
	  RCU locking mechanism) can find it and that lookup must see that this
	  is an inode currently under construction (i.e. that XFS_INEW is set).
	  The ip->i_flags_lock that protects the XFS_INEW flag forms the
	  memory barrier that ensures this detection works correctly at lookup
	  time.
 insert the new inode 
  Look up an inode by number in the given file system.  The inode is looked up
  in the cache held in each AG.  If the inode is found in the cache, initialise
  the vfs inode if necessary.
  If it is not in core, read it in from the file system's device, add it to the
  cache and initialise the vfs inode.
  The inode is locked according to the value of the lock_flags parameter.
  Inode lookup is only done during metadata operations and not as part of the
  data IO path. Hence we only allow locking of the XFS_ILOCK during lookup.
 reject inode numbers outside existing AGs 
 get the perag structure and ensure that it's inode capable 
	
	  If we have a real type for an on-disk inode, we can setup the inode
	  now.	 If it's a new inode being created, xfs_ialloc will handle it.
  "Is this a cached inode that's also allocated?"
  Look up an inode by number in the given file system.  If the inode is
  in cache and isn't in purgatory, return 1 if the inode is allocated
  and 0 if it is not.  For all other cases (not in cache, being torn
  down, etc.), return a negative error code.
  The caller has to prevent inode allocation and freeing activity,
  presumably by locking the AGI buffer.   This is to ensure that an
  inode cannot transition from allocated to freed until the caller is
  ready to allow that.  If the inode is in an intermediate state (new,
  reclaimable, or being reclaimed), -EAGAIN will be returned; if the
  inode is not in the cache, -ENOENT will be returned.  The caller must
  deal with these scenarios appropriately.
  This is a specialized use case for the online scrubber; if you're
  reading this, you probably want xfs_iget.
  Grab the inode for reclaim exclusively.
  We have found this inode via a lookup under RCU, so the inode may have
  already been freed, or it may be in the process of being recycled by
  xfs_iget(). In both cases, the inode will have XFS_IRECLAIM set. If the inode
  has been fully recycled by the time we get the i_flags_lock, XFS_IRECLAIMABLE
  will not be set. Hence we need to check for both these flag conditions to
  avoid inodes that are no longer reclaim candidates.
  Note: checking for other state flags here, under the i_flags_lock or not, is
  racy and should be avoided. Those races should be resolved only after we have
  ensured that we are able to reclaim this inode and the world can see that we
  are going to reclaim it.
  Return true if we grabbed it, false otherwise.
 not a reclaim candidate. 
 Don't reclaim a sick inode unless the caller asked for it. 
  Inode reclaim is non-blocking, so the default action if progress cannot be
  made is to "requeue" the inode for reclaim by unlocking it and clearing the
  XFS_IRECLAIM flag.  If we are in a shutdown state, we don't care about
  blocking anymore and hence we can wait for the inode to be able to reclaim
  it.
  We do no IO here - if callers require inodes to be cleaned they must push the
  AIL first to trigger writeback of dirty inodes.  This enables writeback to be
  done in the background in a non-blocking manner, and enables memory reclaim
  to make progress without blocking.
 for radix_tree_delete 
	
	  Because we use RCU freeing we need to ensure the inode always appears
	  to be reclaimed with an invalid inode number when in the free state.
	  We do this as early as possible under the ILOCK so that
	  xfs_iflush_cluster() and xfs_ifree_cluster() can be guaranteed to
	  detect races with us here. By doing this, we guarantee that once
	  xfs_iflush_cluster() or xfs_ifree_cluster() has locked XFS_ILOCK that
	  it will see either a valid inode that will serialise correctly, or it
	  will see an invalid inode that it can skip.
	
	  Remove the inode from the per-AG radix tree.
	 
	  Because radix_tree_delete won't complain even if the item was never
	  added to the tree assert that it's been there before to catch
	  problems with the inode life time early on.
	
	  Here we do an (almost) spurious inode lock in order to coordinate
	  with inode cache radix tree lookups.  This is because the lookup
	  can reference the inodes in the cache without taking references.
	 
	  We make that OK here by ensuring that we wait until the inode is
	  unlocked after the lookup before we go ahead and free it.
 Reclaim sick inodes if we're unmounting or the fs went down. 
  The shrinker infrastructure determines how many inodes we should scan for
  reclaim. We want as many clean inodes ready to reclaim as possible, so we
  push the AIL here. We also want to proactively free up memory if we can to
  minimise the amount of work memory reclaim has to do so we kick the
  background reclaim if it isn't already scheduled.
 kick background reclaimer and push the AIL 
  Return the number of reclaimable inodes in the filesystem for
  the shrinker to determine how much to reclaim.
  A union-based inode filtering algorithm. Process the inode if any of the
  criteria match. This is for globalinternal scans only.
  Is this inode @ip eligible for eofcow block reclamation, given some
  filtering parameters @icw?  The inode is eligible if @icw is null or
  if the predicate functions match.
 skip the inode if the file size is too small 
  This is a fast pass over the inode cache to try to get reclaim moving on as
  many inodes as possible in a short period of time. It kicks itself every few
  seconds, as well as being kicked by the inode cache shrinker when memory
  goes low.
	
	  If the mapping is dirty the operation can block and wait for some
	  time. Unless we are waiting, skip it.
	
	  If the caller is waiting, return -EAGAIN to keep the background
	  scanner moving and revisit the inode in a subsequent pass.
 inode could be preallocated or append-only 
	
	  Don't bother locking the AG and looking up in the radix trees
	  if we already know that we have the tag set.
  Set ourselves up to free CoW blocks from this file.  If it's already clean
  then we can bail out quickly, but otherwise we must back off if the file
  is undergoing some kind of write.
	
	  Just clear the tag if we have an empty cow fork or none at all. It's
	  possible the inode was fully unshared since it was originally tagged.
	
	  If the mapping is dirty or under writeback we cannot touch the
	  CoW fork.  Leave it alone if we're in the midst of a directio.
  Automatic CoW Reservation Freeing
  These functions automatically garbage collect leftover CoW reservations
  that were made on behalf of a cowextsize hint when we start to run out
  of quota or when the reservations sit around for too long.  If the file
  has dirty pages or is undergoing writeback, its CoW reservations will
  be retained.
  The actual garbage collection piggybacks off the same code that runs
  the speculative EOF preallocation garbage collector.
	
	  If the caller is waiting, return -EAGAIN to keep the background
	  scanner moving and revisit the inode in a subsequent pass.
	
	  Check again, nobody else should be able to dirty blocks or change
	  the reflink iflag now that we have the first two locks held.
 Disable post-EOF and CoW block auto-reclamation. 
 Enable post-EOF and CoW block auto-reclamation. 
 Don't try to run block gc on an inode that's in any of these states. 
  Decide if the given @ip is eligible for garbage collection of speculative
  preallocations, and grab it if so.  Returns true if it's ready to go or
  false if we should just ignore it.
 Check for stale RCU freed inode 
 nothing to sync during shutdown 
 If we can't grab the inode, it must on it's way to reclaim. 
 inode is valid 
 Scan one incore inode for block preallocations that we can remove. 
 Background worker that trims preallocated space. 
  Try to free space in the filesystem by purging inactive inodes, eofblocks
  and cowblocks.
  Reclaim all the free space that we can by scheduling the background blockgc
  and inodegc workers immediately and waiting for them all to clear.
	
	  For each blockgc worker, move its queue time up to now.  If it
	  wasn't queued, it will not be requeued.  Then flush whatever's
	  left.
  Run coweofblocks scans on the supplied dquots.  We don't know exactly which
  quota caused an allocation failure, so we make a best effort by including
  each quota under low free space conditions (less than 1% free space) in the
  scan.
  Callers must not hold any inode's ILOCK.  If requesting a synchronous scan
  (XFS_ICWALK_FLAG_SYNC), the caller also must not hold any inode's IOLOCK or
  MMAPLOCK.
	
	  Run a scan to free blocks using the union filter to cover all
	  applicable quotas in a single scan.
 Run coweofblocks scans on the quotas attached to the inode. 
 XFS Inode Cache Walking Code 
  The inode lookup is done in batches to keep the amount of lock traffic and
  radix tree lookups to a minimum. The batch size is a trade off between
  lookup reduction and stack usage. This is in the reclaim path, so we can't
  be too greedy.
  Decide if we want to grab this inode in anticipation of doing work towards
  the goal.
  Process an inode.  Each processing function must handle any state changes
  made by the icwalk igrab function.  Return -EAGAIN to skip an inode.
  For a given per-AG structure @pag and a goal, grab qualifying inodes and
  process them in some manner.
		
		  Grab the inodes before we drop the lock. if we found
		  nothing, nr == 0 and the loop will be skipped.
			
			  Update the index for the next lookup. Catch
			  overflows into the next AG range which can occur if
			  we have inodes in the last block of the AG and we
			  are currently pointing to the last inode.
			 
			  Because we may see inodes that are from the wrong AG
			  due to RCU freeing and reallocation, only update the
			  index if it lies in this AG. It was a race that lead
			  us to see this inode, so another lookup from the
			  same index will not find it again.
 unlock now we've grabbed the inodes. 
 bail out if the filesystem is corrupted.  
 Walk all incore inodes to achieve a given goal. 
 Schedule the inode for reclaim. 
  Free all speculative preallocations and possibly even the inode itself.
  This is the last chance to make changes to an otherwise unreferenced file
  before incore reclamation happens.
  Force all currently queued inode inactivation work to run immediately, and
  wait for the work to finish. Two pass - queue all the work first pass, wait
  for it in a second pass.
  Flush all the pending work and then disable the inode inactivation background
  workers and wait for them to stop.
  Enable the inode inactivation background workers and schedule deferred inode
  inactivation work if there is any.
 CONFIG_XFS_RT 
  Schedule the inactivation worker when:
   - We've accumulated more than one inode cluster buffer's worth of inodes.
   - There is less than 5% free space left.
   - Any of the quotas for this inode are near an enforcement limit.
  Upper bound on the number of inodes in each AG that can be queued for
  inactivation at any given time, to avoid monopolizing the workqueue.
  Make the frontend wait for inactivations when:
   - Memory shrinkers queued the inactivation worker and it hasn't finished.
   - The queue depth exceeds the maximum allowable percpu backlog.
  Note: If the current thread is running a transaction, we don't ever want to
  wait for other transactions because that could introduce a deadlock.
  Queue a background inactivation worker if there are inodes that need to be
  inactivated and higher level xfs code hasn't disabled the background
  workers.
  Fold the dead CPU inodegc queue into the current CPUs queue.
 Add pending work to current CPU 
  We set the inode flag atomically with the radix tree tag.  Once we get tag
  lookups on the radix tree, this inode flag can go away.
  We always use background reclaim here because even if the inode is clean, it
  still may be under IO and hence we have wait for IO completion to occur
  before we can reclaim the inode. The background reclaim path handles this
  more efficiently than we can here, so simply let background reclaim tear down
  all inodes.
	
	  We should never get here with any of the reclaim flags already set.
 Going straight to reclaim, so drop the dquots. 
  Register a phony shrinker so that we can run background inodegc sooner when
  there's memory pressure.  Inactivation does not itself free any memory but
  it does make inodes reclaimable, which eventually frees memory.
  The count function, seek value, and batch value are crafted to trigger the
  scan function during the second round of scanning.  Hopefully this means
  that we reclaimed enough memory that initiating metadata transactions won't
  make things worse.
	
	  If there are no inodes to inactivate, we don't want the shrinker
	  to think there's deferred work to call us back about.
 Register a shrinker so we can accelerate inodegc and throttle queuing. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2009, Christoph Hellwig
  All Rights Reserved.
  We include this last to have the helpers above available for the trace
  event implementations.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2003 Silicon Graphics, Inc.
  All Rights Reserved.
  Lock order:
  ip->i_lock
    qi->qi_tree_lock
      dquot->q_qlock (xfs_dqlock() and friends)
        dquot->q_flush (xfs_dqflock() and friends)
        qi->qi_lru_lock
  If two dquots need to be locked the order is user before groupproject,
  otherwise by the lowest id first, see xfs_dqlock2.
  This is called to free all the memory associated with a dquot
  If default limits are in force, push them into the dquot now.
  We overwrite the dquot limits only if they are zero and this
  is not the root dquot.
 Set the expiration time of a quota's grace period. 
 Set the length of the default grace period. 
  Determine if this quota counter is over either limit and set the quota
  timers as appropriate.
  Check the limits and timers of a dquot and start or reset timers
  if necessary.
  This gets called even when quota enforcement is OFF, which makes our
  life a little less complicated. (We just don't reject any quota
  reservations in that case, when enforcement is off).
  We also return 0 as the values of the timers in Q_GETQUOTA calls, when
  enforcement's off.
  In contrast, warnings are a little different in that they don't
  'automatically' get started when limits get exceeded.  They do
  get reset to zero, however, when we find the count to be under
  the soft limit (they are only ever set non-zero via userspace).
  initialize a buffer full of dquots and log the whole thing
	
	  ID of the first dquot in the block - id's are zero based.
	
	  quotacheck uses delayed writes to update all the dquots on disk in an
	  efficient manner instead of logging the individual dquot changes as
	  they are made. However if we log the buffer allocated here and crash
	  after quotacheck while the logged initialisation is still in the
	  active region of the log, log recovery can replay the dquot buffer
	  initialisation over the top of the checked dquots and corrupt quota
	  accounting.
	 
	  To avoid this problem, quotacheck cannot log the initialised buffer.
	  We must still dirty the buffer and write it back before the
	  allocation transaction clears the log. Therefore, mark the buffer as
	  ordered instead of logging it directly. This is safe for quotacheck
	  because it detects and repairs allocated but initialized dquot blocks
	  in the quota inodes.
  Initialize the dynamic speculative preallocation thresholds. The lohi
  watermarks correspond to the soft and hard limits by default. If a soft limit
  is not specified, we use 95% of the hard limit.
  Ensure that the given in-core dquot has a buffer on disk backing it, and
  return the buffer locked and held. This is called when the bmapi finds a
  hole.
		
		  Return if this type of quotas is turned off while we didn't
		  have an inode lock
 Create the block mapping. 
	
	  Keep track of the blkno to save a lookup later
 now we can just get the buffer (there's nothing to read yet) 
	
	  Make a chunk of dquots out of this buffer and log
	  the entire thing.
	
	  Hold the buffer and join it to the dfops so that we'll still own
	  the buffer when we return to the caller.  The buffer disposal on
	  error must be paid attention to very carefully, as it has been
	  broken since commit efa092f3d4c6 "[XFS] Fixes a bug in the quota
	  code when allocating a new dquot record" in 2005, and the later
	  conversion to xfs_defer_ops in commit 310a75a3c6c747 failed to keep
	  the buffer locked across the _defer_finish call.  We can now do
	  this correctly with xfs_defer_bjoin.
	 
	  Above, we allocated a disk block for the dquot information and used
	  get_buf to initialize the dquot. If the _defer_finish fails, the old
	  transaction is gone but the new buffer is not joined or held to any
	  transaction, so we must _buf_relse it.
	 
	  If everything succeeds, the caller of this function is returned a
	  buffer that is locked and held to the transaction.  The caller
	  is responsible for unlocking any buffer passed back, either
	  manually or by committing the transaction.  On error, the buffer is
	  released and not passed back.
  Read in the in-core dquot's on-disk metadata and return the buffer.
  Returns ENOENT to signal a hole.
		
		  Return if this type of quotas is turned off while we
		  didn't have the quota inode lock.
	
	  Find the block map; no allocations yet
	
	  store the blkno etc so that we don't have to do the
	  mapping all the time
 Allocate and initialize everything we need for an incore dquot. 
	
	  Offset of dquot in the (fixed sized) dquot chunk.
	
	  Because we want to use a counting completion, complete
	  the flush completion once to allow a single access to
	  the flush completion without blocking.
	
	  Make sure group quotas have a different lock class than user
	  quotas.
 uses the default lock class 
 Check the ondisk dquot's id and type match what the incore dquot expects. 
	
	  V5 filesystems always expect an exact type match.  V4 filesystems
	  expect an exact match for user dquots and for non-root group and
	  project dquots.
	
	  V4 filesystems support either group or project quotas, but not both
	  at the same time.  The non-user quota file can be switched between
	  group and project quota uses depending on the mount options, which
	  means that we can encounter the other type when we try to load quota
	  defaults.  Quotacheck will soon reset the the entire quota file
	  (including the root dquot) anyway, but don't log scary corruption
	  reports to dmesg.
 Copy the in-core quota fields in from the on-disk buffer. 
	
	  Ensure that we got the type and ID we were looking for.
	  Everything else was checked by the dquot buffer verifier.
 copy everything from disk dquot to the incore dquot 
	
	  Reservation counters are defined as reservation plus current usage
	  to avoid having to add every time.
 initialize the dquot speculative prealloc thresholds 
 Copy the in-core quota fields into the on-disk buffer. 
 Allocate and initialize the dquot buffer for this in-core dquot. 
		
		  Buffer was held to the transaction, so we have to unlock it
		  manually here because we're not passing it back.
  Read in the ondisk dquot using dqtobp() then copy it to an incore version,
  and release the buffer immediately.  If @can_alloc is true, fill any
  holes in the on-disk metadata.
 Try to read the buffer, allocating if necessary. 
	
	  At this point we should have a clean locked buffer.  Copy the data
	  to the incore dquot and release the buffer since the incore dquot
	  has its own locking protocol so we needn't tie up the buffer any
	  further.
  Advance to the next id in the current chunk, or if at the
  end of the chunk, skip ahead to first id in next allocated chunk
  using the SEEK_DATA interface.
 simple advance 
 If we'd wrap past the max ID, stop 
 If new ID is within the current chunk, advancing it sufficed 
 Nope, next_id is now past the current chunk, so find the next one 
 contiguous chunk, bump startoff for the id calculation 
  Look up the dquot in the in-core cache.  If found, the dquot is returned
  locked and ready to go.
  Try to insert a new dquot into the in-core cache.  If an error occurs the
  caller should throw away the dquot and start over.  Otherwise, the dquot
  is returned locked (and held by the cache) as if there had been a cache
  hit.
 Duplicate found!  Caller must try again. 
 Return a locked dquot to the caller, with a reference taken. 
 Check our input parameters. 
  Given the file system, id, and type (UDQUOTGDQUOTPDQUOT), return a
  locked dquot, doing an allocation (if requested) as needed.
		
		  Duplicate found. Just throw away the new dquot and start
		  over.
  Given a dquot id and type, read and initialize a dquot from the on-disk
  metadata.  This function is only for use during quota initialization so
  it ignores the dquot cache assuming that the dquot shrinker isn't set up.
  The caller is responsible for _qm_dqdestroy'ing the returned dquot.
 Return the quota id for a given inode and type. 
  Return the dquot for a given inode and type.  If @can_alloc is true, then
  allocate blocks if needed.  The inode's ILOCK must be held and it must not
  have already had an inode attached.
	
	  Dquot cache miss. We don't want to keep the inode lock across
	  a (potential) disk read. Also we don't want to deal with the lock
	  ordering between quotainode and this inode. OTOH, dropping the inode
	  lock here means dealing with a chown that can happen before
	  we re-acquire the lock.
	
	  A dquot could be attached to this inode by now, since we had
	  dropped the ilock.
 inode stays locked on return 
		
		  Duplicate found. Just throw away the new dquot and start
		  over.
  Starting at @id and progressing upwards, look for an initialized incore
  dquot, lock it, and return it.
  Release a reference to the dquot (decrement ref-count) and unlock it.
  If there is a group quota attached to this dquot, carefully release that
  too without tripping over deadlocks'n'stuff.
  Release a dquot. Flush it if dirty, then dqput() it.
  dquot must not be locked.
	
	  We don't care to flush it if the dquot is dirty here.
	  That will create stutters that we want to avoid.
	  Instead we do a delayed write when we try to reclaim
	  a dirty dquot. Also xfs_sync will take part of the burden...
  This is the dquot flushing IO completion routine.  It is called
  from interrupt level when the buffer containing the dquot is
  flushed to disk.  It is responsible for removing the dquot logitem
  from the AIL if it has not been re-logged, and unlocking the dquot's
  flush lock. This behavior is very similar to that of inodes..
	
	  We only want to pull the item from the AIL if its
	  location in the log has not changed since we started the flush.
	  Thus, we only bother if the dquot's lsn has
	  not changed. First we check the lsn outside the lock
	  since it's cheaper, and then we recheck while
	  holding the lock before removing the dquot from the AIL.
 xfs_ail_update_finish() drops the AIL lock 
	
	  Release the dq's flush lock since we're done with it.
 Check incore dquot for errors before we flush. 
 bigtime flag should never be set on root dquots 
  Write a modified dquot to disk.
  The dquot must be locked and the flush lock too taken by caller.
  The flush lock will not be unlocked until the dquot reaches the disk,
  but the dquot is free to be unlocked and modified by the caller
  in the interim. Dquot is still locked on return. This behavior is
  identical to that of inodes.
	
	  Get the buffer containing the on-disk dquot
 Flush the incore dquot to the ondisk buffer. 
	
	  Clear the dirty field and remember the flush lsn for later use.
	
	  copy the lsn into the on-disk dquot now while we have the in memory
	  dquot here. This can't be done later in the write verifier as we
	  can't get access to the log item at that point in time.
	 
	  We also calculate the CRC here so that the on-disk dquot in the
	  buffer always has a valid CRC. This ensures there is no possibility
	  of a dquot without an up-to-date CRC getting to disk.
	
	  Attach the dquot to the buffer so that we can remove this dquot from
	  the AIL and release the flush lock once the dquot is synced to disk.
	
	  If the buffer is pinned then push on the log so we won't
	  get stuck waiting in the write for too long.
  Lock two xfs_dquot structures.
  To avoid deadlocks we always lock the quota structure with
  the lowerd id first.
  Iterate every dquot of a particular type.  The caller must ensure that the
  particular quota type is active.  iter_fn can return negative error codes,
  or -ECANCELED to indicate that it wants to stop iterating.
 SPDX-License-Identifier: GPL-2.0-or-later
  Copyright (C) 2019 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Walking Inodes in the Filesystem
  ================================
  This iterator function walks a subset of filesystem inodes in increasing
  order from @startino until there are no more inodes.  For each allocated
  inode it finds, it calls a walk function with the relevant inode number and
  a pointer to caller-provided data.  The walk function can return the usual
  negative error code to stop the iteration; 0 to continue the iteration; or
  -ECANCELED to stop the iteration.  This return value is returned to the
  caller.
  Internally, we allow the walk function to do anything, which means that we
  cannot maintain the inobt cursor or our lock on the AGI buffer.  We
  therefore cache the inobt records in kernel memory and only call the walk
  function when our memory buffer is full.  @nr_recs is the number of records
  that we've cached, and @sz_recs is the size of our cache.
  It is the responsibility of the walk function to ensure it accesses
  allocated inodes, as the inobt records may be stale by the time they are
  acted upon.
 parallel work control data; will be null if single threaded 
 Where do we start the traversal? 
 What was the last inode number we saw when iterating the inobt? 
 Array of inobt records we cache. 
 Number of entries allocated for the @recs array. 
 Number of entries in the @recs array that are in use. 
 Inode walk function and data pointer. 
	
	  Make it look like the inodes up to startino are free so that
	  bulkstat can start its inode iteration at the correct place without
	  needing to special case everywhere.
 Skip empty inobt records? 
 Drop the (hopefully empty) transaction when calling iwalk_fn. 
  Loop over all clusters in a chunk for a given incore inode allocation btree
  record.  Do a readahead if there are any allocated inodes in that cluster.
 inode chunk index 
  Set the bits in @irec's free mask that correspond to the inodes before
  @agino so that we skip them.  This is how we restart an inode walk that was
  interrupted in the middle of an inode record.
 starting inode of chunk 
 btree record 
 index into inode chunk 
	
	  We got a right chunk with some left inodes allocated at it.  Grab
	  the chunk record.  Mark all the uninteresting inodes free because
	  they're before our start point.
 Allocate memory for a walk. 
 Allocate a prefetch buffer for inobt records. 
 Free memory we allocated for a walk. 
 For each inuse inode in each cached inobt record, call our function. 
 Skip if this inode is free 
 Otherwise call our function. 
 Delete cursor and let go of AGI. 
  Set ourselves up for walking inobt records starting from a given point in
  the filesystem.
  If caller passed in a nonzero start inode number, load the record from the
  inobt and make the record look like all the inodes before agino are free so
  that we skip them, and then move the cursor to the next inobt record.  This
  is how we support starting an iwalk in the middle of an inode chunk.
  If the caller passed in a start number of zero, move the cursor to the first
  inobt record.
  The caller is responsible for cleaning up the cursor and buffer pointer
  regardless of the error status.
 Set up a fresh cursor and empty the inobt cache. 
 Starting at the beginning of the AG?  That's easy! 
	
	  Otherwise, we have to grab the inobt record where we left off, stuff
	  the record into our cache, and then see if there are more records.
	  We require a lookup cache of at least two elements so that the
	  caller doesn't have to deal with tearing down the cursor to walk the
	  records.
	
	  If the LE lookup at @agino yields no records, jump ahead to the
	  inobt cursor increment to see if there are more records to process.
 Get the record, should always work 
	
	  If the LE lookup yielded an inobt record before the cursor position,
	  skip it and see if there's another one after it.
	
	  If agino fell in the middle of the inode record, make it look like
	  the inodes up to agino are free so that we don't return them again.
	
	  The prefetch calculation is supposed to give us a large enough inobt
	  record cache that grab_ichunk can stage a partial first record and
	  the loop body can cache a record without having to check for cache
	  space until after it reads an inobt record.
  The inobt record cache is full, so preserve the inobt cursor state and
  run callbacks on the cached inobt records.  When we're done, restore the
  cursor state to wherever the cursor would have been had the cache not been
  full (and therefore we could've just incremented the cursor) if @has_more
  is true.  On exit, @has_more will indicate whether or not the caller should
  try for more inode records.
 Delete cursor but remember the last record we cached... 
 ...empty the cache... 
 ...and recreate the cursor just past where we left off. 
 Walk all inodes in a single AG, from @iwag->startino to the end of the AG. 
 Set up our cursor at the right place in the inode btree. 
 Fetch the inobt record. 
 Make sure that we always move forward. 
 No allocated inodes in this chunk; skip it. 
		
		  Start readahead for this inode chunk in anticipation of
		  walking the inodes.
		
		  If there's space in the buffer for more records, increment
		  the btree cursor and grab more.
		
		  Otherwise, we need to save cursor state and run the callback
		  function on the cached records.  The run_callbacks function
		  is supposed to return a cursor pointing to the record where
		  we would be if we had been able to increment like above.
 Walk the unprocessed records in the cache. 
  We experimentally determined that the reduction in ioctl call overhead
  diminishes when userspace asks for more than 2048 inodes, so we'll cap
  prefetch at this point.
  Given the number of inodes to prefetch, set the number of inobt records that
  we cache in memory, which controls the number of inodes we try to read
  ahead.  Set the maximum if @inodes == 0.
	
	  If the caller didn't tell us the number of inodes they wanted,
	  assume the maximum prefetch possible for best performance.
	  Otherwise, cap prefetch at that maximum so that we don't start an
	  absurd amount of prefetch.
 Round the inode count up to a full chunk. 
	
	  In order to convert the number of inodes to prefetch into an
	  estimate of the number of inobt records to cache, we require a
	  conversion factor that reflects our expectations of the average
	  loading factor of an inode chunk.  Based on data gathered, most
	  (but not all) filesystems manage to keep the inode chunks totally
	  full, so we'll underestimate slightly so that our readahead will
	  still deliver the performance we want on aging filesystems:
	 
	  inobt = inodes  (INODES_PER_CHUNK  (4  5));
	 
	  The funny math is to avoid integer division.
	
	  Allocate enough space to prefetch at least two inobt records so that
	  we can cache both the record where the iwalk started and the next
	  record.  This simplifies the AG inode walk loop setup code.
  Walk all inodes in the filesystem starting from @startino.  The @iwalk_fn
  will be called for each allocated inode, being passed the inode's number and
  @data.  @max_prefetch controls how many inobt records' worth of inodes we
  try to readahead.
 Run per-thread iwalk work. 
	
	  Grab an empty transaction so that we can use its recursive buffer
	  locking abilities to detect cycles in the inobt without deadlocking.
  Walk all the inodes in the filesystem using multiple threads to process each
  AG.
		
		  perag is being handed off to async work, so take another
		  reference for the async work to release.
  Allow callers to cache up to a page's worth of inobt records.  This reflects
  the existing inumbers prefetching behavior.  Since the inobt walk does not
  itself do anything with the inobt records, we can set a fairly high limit
  here.
  Given the number of records that the user wanted, set the number of inobt
  records that we buffer in memory.  Set the maximum if @inobt_records == 0.
	
	  If the caller didn't tell us the number of inobt records they
	  wanted, assume the maximum prefetch possible for best performance.
	
	  Allocate enough space to prefetch at least two inobt records so that
	  we can cache both the record where the iwalk started and the next
	  record.  This simplifies the AG inode walk loop setup code.
	
	  Cap prefetch at that maximum so that we don't use an absurd amount
	  of memory.
  Walk all inode btree records in the filesystem starting from @startino.  The
  @inobt_walk_fn will be called for each btree record, being passed the incore
  record and @data.  @max_prefetch controls how many inobt records we try to
  cache ahead of time.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  Copyright (c) 2013 Red Hat, Inc.
  All Rights Reserved.
  Invalidate any incore buffers associated with this remote attribute value
  extent.   We never log remote attribute value buffers, which means that they
  won't be attached to a transaction and are therefore safe to mark stale.
  The actual bunmapi will be taken care of later.
	
	  Roll through the "value", invalidating the attribute value's
	  blocks.
		
		  Try to remember where we decided to put the value.
		
		  Mark any incore buffers for the remote value as stale.  We
		  never log remote attr value buffers, so the buffer should be
		  easy to kill.
  Invalidate all of the "remote" value regions pointed to by a particular
  leaf block.
  Note that we must release the lock on the buffer so that we are not
  caught holding something that the logging code wants to flush to disk.
	
	  Find the remote value extents for this leaf and invalidate their
	  incore buffers.
  Recurse (gasp!) through the attribute nodes until we find leaves.
  We're doing a depth-first traversal in order to invalidate everything.
	
	  Since this code is recursive (gasp!) we must protect ourselves.
 no locks for later trans 
 no locks for later trans 
	
	  If this is the node level just above the leaves, simply loop
	  over the leaves removing all of them.  If this is higher up
	  in the tree, recurse downward.
		
		  Read the subsidiary block to see what we have to work with.
		  Don't do this in a transaction.  This is a depth-first
		  traversal of the tree so we may deal with many blocks
		  before we come back to this one.
 save for re-read later 
		
		  Invalidate the subtree, however we have to.
		
		  Remove the subsidiary block from the cache and from the log.
		
		  If we're not done, re-read the parent to get the next
		  child block number.
		
		  Atomically commit the whole invalidate stuff.
  Indiscriminately delete the entire attribute fork
  Recurse (gasp!) through the attribute nodes until we find leaves.
  We're doing a depth-first traversal in order to invalidate everything.
	
	  Read block 0 to see what we have to work with.
	  We only get here if we have extents, since we remove
	  the extents in reverse order the extent containing
	  block 0 must still be there.
	
	  Invalidate the tree, even if the "tree" is only a single leaf block.
	  This is a depth-first traversal!
	
	  Invalidate the incore copy of the root block.
 remove from cache 
	
	  Commit the invalidate and start the next transaction.
  xfs_attr_inactive kills all traces of an attribute fork on an inode. It
  removes both the on-disk and in-memory inode fork. Note that this also has to
  handle the condition of inodes without attributes but with an attribute fork
  configured, so we can't use xfs_inode_hasattr() here.
  The in-memory attribute fork is removed even on error.
	
	  No need to make quota reservations here. We expect to release some
	  blocks, not allocate, in the common case.
	
	  Invalidate and truncate the attribute fork extents. Make sure the
	  fork actually has attributes as otherwise the invalidation has no
	  blocks to read and returns an error. In this case, just do the fork
	  removal below.
 Reset the attribute fork - this also destroys the in-core fork 
 kill the in-core attr fork before we drop the inode lock 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  Copyright (c) 2013 Red Hat, Inc.
  All Rights Reserved.
  Directory file type support functions
 shortform entry number 
 incore directory inode 
 current entry's offset 
 shortform directory entry 
 shortform structure 
	
	  If the block number in the offset is out of range, we're done.
	
	  Precalculate offsets for "." and ".." as we will always need them.
	  This relies on the fact that directories always start with the
	  entries for "." and "..".
	
	  Put . entry unless we're starting past it.
	
	  Put .. entry unless we're starting past it.
	
	  Loop while there are more entries and put'ing works.
  Readdir for block directories.
 incore directory inode 
 buffer for block 
 error return value 
 starting block offset 
	
	  If the block number in the offset is out of range, we're done.
	
	  Extract the byte offset we start at from the seek pointer.
	  We'll skip entries before this.
	
	  Loop over the data portion of the block.
	  Each object is a real entry (dep) or an unused one (dup).
		
		  Unused, skip it.
		
		  Bump pointer for the next iteration.
		
		  The entry is before the desired starting point, skip it.
		
		  If it didn't fit, set the final offset to here & return.
	
	  Reached the end of the block.
	  Set the offset to a non-existent block 1 and return.
  Read a directory block and initiate readahead for blocks beyond that.
  We maintain a sliding readahead window of the remaining space in the
  buffer rounded up to the nearest block.
	
	  Look for mapped directory blocks at or above the current offset.
	  Truncate down to the nearest directory block to start the scanning
	  operation.
 Read the directory block of that first mapping. 
	
	  Start readahead for the next bufsize's worth of dir data blocks.
	  We may have already issued readahead for some of that range;
	  ra_blk tracks the last block we tried to read(ahead).
 Start ra for each dir (not fs) block that has a mapping. 
  Getdents (readdir) for leaf and node directories.
  This reads the data blocks only, so is the same for both forms.
 data block buffer 
 data entry 
 unused entry 
 current readahead block 
 current overall offset 
 temporary length value 
 offset in current block 
 error return value 
	
	  If the offset is at or past the largest allowed value,
	  give up right away.
	
	  Inside the loop we keep the main offset value as a byte offset
	  in the directory file.
	
	  Loop over directory entries until we reach the end offset.
	  Get more blocks and readahead as necessary.
		
		  If we have no buffer, or we're off the end of the
		  current buffer, need to get another one.
			
			  Find our position in the block.
			
			  Skip past the header.
			
			  Skip past entries until we reach our offset.
				
				  Now set our real offset.
		
		  We have a pointer to an entry.  Is it a live one?
		
		  No, it's unused, skip over it.
		
		  Advance to next entry in the block.
 bufsize may have just been a guess; don't go negative 
	
	  All done.  Set output offset value to current offset.
  Read a directory.
  If supplied, the transaction collects locked dir buffers to avoid
  nested buffer deadlocks.  This function does not dirty the
  transaction.  The caller should ensure that the inode is locked
  before calling this function.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Decide if the given file range is aligned to the size of the fundamental
  allocation unit for the file.
  Fsync operations on directories are much simpler than on regular files,
  as there is no file data to flush, and thus also no need for explicit
  cache flush operations, and there are no non-transaction metadata updates
  on directories either.
  All metadata updates are logged, which means that we just have to flush the
  log up to the latest LSN that touched the inode.
  If we have concurrent fsyncfdatasync() calls, we need them to all block on
  the log force before we clear the ili_fsync_fields field. This ensures that
  we don't get a racing sync operation that does not wait for the metadata to
  hit the journal before returning.  If we race with clearing ili_fsync_fields,
  then all that will happen is the log force will do nothing as the lsn will
  already be on disk.  We can't race with setting ili_fsync_fields because that
  is done under XFS_ILOCK_EXCL, and that can't happen because we hold the lock
  shared until after the ili_fsync_fields is cleared.
	
	  If we have an RT andor log subvolume we need to make sure to flush
	  the write cache the device used for file data first.  This is to
	  ensure newly written file data make it to disk before logging the new
	  inode size in case of an extending write.
	
	  Any inode that has dirty modifications in the log is pinned.  The
	  racy check here for a pinned inode while not catch modifications
	  that happen concurrently to the fsync call, but fsync semantics
	  only require to sync previously completed IO.
	
	  If we only have a single device, and the log force about was
	  a no-op we might have to flush the data device cache here.
	  This can only happen for fdatasyncO_DSYNC if we were overwriting
	  an already allocated file and thus do not have any metadata to
	  commit.
 skip atime 
 skip atime 
  Common pre-write limit and setup checks.
  Called with the iolocked held either shared and exclusive according to
  @iolock, and returns with it held.  Might upgrade the iolock to exclusive
  if called for a direct write beyond i_size.
	
	  For changing security info in file_remove_privs() we need i_rwsem
	  exclusively.
	
	  If the offset is beyond the size of the file, we need to zero any
	  blocks that fall between the existing EOF and the start of this
	  write.  If zeroing is needed and we are currently holding the iolock
	  shared, we need to update it to exclusive which implies having to
	  redo all checks before.
	 
	  We need to serialise against EOF updates that occur in IO completions
	  here. We want to make sure that nobody is changing the size while we
	  do this check until we have placed an IO barrier (i.e.  hold the
	  XFS_IOLOCK_EXCL) that prevents new IO from being dispatched.  The
	  spinlock effectively forms a memory barrier once we have the
	  XFS_IOLOCK_EXCL so we are guaranteed to see the latest EOF value and
	  hence be able to correctly determine if we need to run zeroing.
	 
	  We can do an unlocked check here safely as IO completion can only
	  extend EOF. Truncate is locked out at this point, so the EOF can
	  not move backwards, only forwards. Hence we only need to take the
	  slow path and spin locks when we are at or beyond the current EOF.
			
			  We now have an IO submission barrier in place, but
			  AIO can do EOF updates during IO completion and hence
			  we now need to wait for all of them to drain. Non-AIO
			  DIO will have drained before we are given the
			  XFS_IOLOCK_EXCL, and so for most cases this wait is a
			  no-op.
	
	  Capture amount written on completion as we can't reliably account
	  for it on submission.
	
	  We can allocate memory here while doing writeback on behalf of
	  memory reclaim.  To avoid memory allocation deadlocks set the
	  task-wide nofs context for the following operations.
	
	  Unwritten conversion updates the in-core isize after extent
	  conversion but before updating the on-disk size. Updating isize any
	  earlier allows a racing dio read to find unwritten extents before
	  they are converted.
	
	  We need to update the in-core inode size here so that we don't end up
	  with the on-disk inode size being outside the in-core inode size. We
	  have no other method of updating EOF for AIO, so always do it here
	  if necessary.
	 
	  We need to lock the testset EOF update as we can be racing with
	  other IO completions here to update the EOF. Failing to serialise
	  here can result in EOF moving backwards and Bad Things Happen when
	  that occurs.
	 
	  As IO completion only ever extends EOF, we can do an unlocked check
	  here to avoid taking the spinlock. If we land within the current EOF,
	  then we do not need to do an extending update at all, and we don't
	  need to take the lock to check this. If we race with an update moving
	  EOF, then we'll either still be beyond EOF and need to take the lock,
	  or we'll be within EOF and we don't need to take it at all.
  Handle block aligned direct IO writes
	
	  We don't need to hold the IOLOCK exclusively across the IO, so demote
	  the iolock back to shared if we had to take the exclusive lock in
	  xfs_file_write_checks() for other reasons.
  Handle block unaligned direct IO writes
  In most cases direct IO writes will be done holding IOLOCK_SHARED, allowing
  them to be done in parallel with reads and other direct IO writes.  However,
  if the IO is not aligned to filesystem blocks, the direct IO layer may need
  to do sub-block zeroing and that requires serialisation against other direct
  IO to the same block.  In this case we need to serialise the submission of
  the unaligned IO so that we don't get racing block zeroing in the dio layer.
  In the case where sub-block zeroing is not required, we can do concurrent
  sub-block dios to the same block successfully.
  Optimistically submit the IO using the shared lock first, but use the
  IOMAP_DIO_OVERWRITE_ONLY flag to tell the lower layers to return -EAGAIN
  if block allocation or partial block zeroing would be required.  In that case
  we try again with the exclusive lock.
	
	  Extending writes need exclusivity because of the sub-block zeroing
	  that the DIO code always does for partial tail blocks beyond EOF, so
	  don't even bother trying the fast path in this case.
	
	  We can't properly handle unaligned direct IO to reflink files yet,
	  as we can't unshare a partial block.
	
	  If we are doing exclusive unaligned IO, this must be the only IO
	  in-flight.  Otherwise we risk data corruption due to unwritten extent
	  conversions from the AIO end_io handler.  Wait for all other IO to
	  drain first.
	
	  Retry unaligned IO with exclusive blocking semantics if the DIO
	  layer rejected it for mapping or locking reasons. If we are doing
	  nonblocking user IO, propagate the error.
 direct IO must be aligned to device logical sector size 
 Handle various SYNC-type writes 
 We can write back this queue in page reclaim 
	
	  If we hit a space limit, try to free up some lingering preallocated
	  space before returning an error. In the case of ENOSPC, first try to
	  write back all dirty inodes to free up some of the excess reserved
	  metadata space. This reduces the chances that the eofblocks scan
	  waits on dirty mappings. Since xfs_flush_inodes() is serialized, this
	  also behaves as a filter to prevent too many eofblocks scans from
	  running at the same time.  Use a synchronous scan to increase the
	  effectiveness of the scan.
 Handle various SYNC-type writes 
		
		  Allow a directio write to fall back to a buffered
		  write only in the case that we're doing a reflink
		  CoW.  In all other directio scenarios we do not
		  allow an operation to fall back to buffered mode.
	
	  Must wait for all AIO to complete before we continue as AIO can
	  change the file size on completion without holding any locks we
	  currently hold. We must do this first because AIO can update both
	  the on disk and in memory inode sizes, and the operations that follow
	  require the in-memory size to be fully up-to-date.
	
	  Now AIO and DIO has drained we flush and (if necessary) invalidate
	  the cached range over the first operation we are about to run.
	 
	  We care about zero and collapse here because they both run a hole
	  punch over the range first. Because that can zero data, and the range
	  of invalidation for the shift operations is much larger, we still do
	  the required flush for collapse in xfs_prepare_shift().
	 
	  Insert has the same range requirements as collapse, and we extend the
	  file first which can zero data. Hence insert has the same
	  flushinvalidate requirements as collapse and so they are both
	  handled at the right time by xfs_prepare_shift().
		
		  There is no need to overlap collapse range with EOF,
		  in which case it is effectively a truncate operation
		
		  New inode size must not exceed ->s_maxbytes, accounting for
		  possible signed overflow.
 Offset should be less than i_size 
			
			  Punch a hole and prealloc the range.  We use a hole
			  punch rather than unwritten extent conversion for two
			  reasons:
			 
			    1.) Hole punch handles partial block zeroing for us.
			    2.) If prealloc returns ENOSPC, the file range is
			        still zero-valued by virtue of the hole punch.
			
			  If always_cow mode we can't use preallocations and
			  thus should not create them.
 Change file size if needed 
	
	  Perform hole insertion now that the file size has been
	  updated so that if we crash during the operation we don't
	  leave shifted extents past EOF and hence losing access to
	  the data that is contained within them.
	
	  Operations creating pages in page cache need protection from hole
	  punching and similar ops
 Does this file, inode, or mount want synchronous writes? 
 Prepare and then clone file data. 
	
	  Carry the cowextsize hint from src to dest if we're sharing the
	  entire source file to the entire destination file, the source file
	  has a cowextsize hint, and the destination file does not.
	
	  If there are any blocks, read-ahead block 0 as we're almost
	  certain to have the next operation be a read there.
	
	  The Linux API doesn't pass down the total size of the buffer
	  we read into down to the filesystem.  With the filldir concept
	  it's not needed for correct information, but the XFS dir2 leaf
	  code wants an estimate of the buffer size to calculate it's
	  readahead window and size the buffers used for mapping to
	  physical blocks.
	 
	  Try to give it an estimate that's good enough, maybe at some
	  point we can change the ->readdir prototype to include the
	  buffer size.  For now we use the current glibc buffer size.
  Locking for serialisation of IO during page faults. This results in a lock
  ordering of:
  mmap_lock (MM)
    sb_start_pagefault(vfs, freeze)
      invalidate_lock (vfsXFS_MMAPLOCK - truncate serialisation)
        page_lock (MM)
          i_lock (XFS - extent map serialisation)
 DAX can shortcut the normal fault path on write faults! 
 DAX can shortcut the normal fault path on write faults! 
  pfn_mkwrite was originally intended to ensure we capture time stamp updates
  on write faults. In reality, it needs to serialise against truncate and
  prepare memory for writing so handle is as standard write fault.
	
	  We don't support synchronous mappings for non-DAX files and
	  for DAX files if underneath dax_device is not synchronous.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2016 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Copy on Write of Shared Blocks
  XFS must preserve "the usual" file semantics even when two files share
  the same physical blocks.  This means that a write to one file must not
  alter the blocks in a different file; the way that we'll do that is
  through the use of a copy-on-write mechanism.  At a high level, that
  means that when we want to write to a shared block, we allocate a new
  block, write the data to the new block, and if that succeeds we map the
  new block into the file.
  XFS provides a "delayed allocation" mechanism that defers the allocation
  of disk blocks to dirty-but-not-yet-mapped file blocks as long as
  possible.  This reduces fragmentation by enabling the filesystem to ask
  for bigger chunks less often, which is exactly what we want for CoW.
  The delalloc mechanism begins when the kernel wants to make a block
  writable (write_begin or page_mkwrite).  If the offset is not mapped, we
  create a delalloc mapping, which is a regular in-core extent, but without
  a real startblock.  (For delalloc mappings, the startblock encodes both
  a flag that this is a delalloc mapping, and a worst-case estimate of how
  many blocks might be required to put the mapping into the BMBT.)  delalloc
  mappings are a reservation against the free space in the filesystem;
  adjacent mappings can also be combined into fewer larger mappings.
  As an optimization, the CoW extent size hint (cowextsz) creates
  outsized aligned delalloc reservations in the hope of landing out of
  order nearby CoW writes in a single extent on disk, thereby reducing
  fragmentation and improving future performance.
  D: --RRRRRRSSSRRRRRRRR--- (data fork)
  C: ------DDDDDDD--------- (CoW fork)
  When dirty pages are being written out (typically in writepage), the
  delalloc reservations are converted into unwritten mappings by
  allocating blocks and replacing the delalloc mapping with real ones.
  A delalloc mapping can be replaced by several unwritten ones if the
  free space is fragmented.
  D: --RRRRRRSSSRRRRRRRR---
  C: ------UUUUUUU---------
  We want to adapt the delalloc mechanism for copy-on-write, since the
  write paths are similar.  The first two steps (creating the reservation
  and allocating the blocks) are exactly the same as delalloc except that
  the mappings must be stored in a separate CoW fork because we do not want
  to disturb the mapping in the data fork until we're sure that the write
  succeeded.  IO completion in this case is the process of removing the old
  mapping from the data fork and moving the new mapping from the CoW fork to
  the data fork.  This will be discussed shortly.
  For now, unaligned directio writes will be bounced back to the page cache.
  Block-aligned directio writes will use the same mechanism as buffered
  writes.
  Just prior to submitting the actual disk write requests, we convert
  the extents representing the range of the file actually being written
  (as opposed to extra pieces created for the cowextsize hint) to real
  extents.  This will become important in the next step:
  D: --RRRRRRSSSRRRRRRRR---
  C: ------UUrrUUU---------
  CoW remapping must be done after the data block write completes,
  because we don't want to destroy the old data fork map until we're sure
  the new block has been written.  Since the new mappings are kept in a
  separate fork, we can simply iterate these mappings to find the ones
  that cover the file blocks that we just CoW'd.  For each extent, simply
  unmap the corresponding range in the data fork, map the new range into
  the data fork, and remove the extent from the CoW fork.  Because of
  the presence of the cowextsize hint, however, we must be careful
  only to remap the blocks that we've actually written out --  we must
  never remap delalloc reservations nor CoW staging blocks that have
  yet to be written.  This corresponds exactly to the real extents in
  the CoW fork:
  D: --RRRRRRrrSRRRRRRRR---
  C: ------UU--UUU---------
  Since the remapping operation can be applied to an arbitrary file
  range, we record the need for the remap step as a flag in the ioend
  instead of declaring a new IO type.  This is required for direct io
  because we only have ioend for the whole dio, and we have to be able to
  remember the presence of unwritten blocks and CoW blocks with a single
  ioend structure.  Better yet, the more ground we can cover with one
  ioend, the better.
  Given an AG extent, find the lowest-numbered run of shared blocks
  within that range and return the range in fbnoflen.  If
  find_end_of_shared is true, return the longest contiguous extent of
  shared blocks.  If there are no shared extents, fbno and flen will
  be set to NULLAGBLOCK and 0, respectively.
  Trim the mapping to the next block where there's a change in the
  sharedunshared status.  More specifically, this means that we
  find the lowest-numbered extent of shared blocks that coincides with
  the given block mapping.  If the shared extent overlaps the start of
  the mapping, trim the mapping to the end of the shared extent.  If
  the shared region intersects the mapping, trim the mapping to the
  start of the shared extent.  If there are no shared regions that
  overlap, just return the original extent.
 Holes, unwritten, and delalloc extents cannot be shared 
 No shared blocks at all. 
		
		  The start of this extent is shared.  Truncate the
		  mapping at the end of the shared region so that a
		  subsequent iteration starts at the start of the
		  unshared region.
		
		  There's a shared extent midway through this extent.
		  Truncate the mapping at the start of the shared
		  extent so that a subsequent iteration starts at the
		  start of the shared region.
 We can't update any real extents in always COW mode. 
 Trim the mapping to the nearest shared extent boundary. 
 Convert all of the unwritten CoW extents in a file's range to real ones. 
  Find the extent that maps the given range in the COW fork. Even if the extent
  is not shared we might have a preallocation for it in the COW fork. If so we
  use it that rather than trigger a new allocation.
	
	  If we don't find an overlapping extent, trim the range we need to
	  allocate to fit the hole we found.
 real extent found - no need to allocate 
 Allocate all CoW reservations covering a range of blocks in a file. 
	
	  Check for an overlapping extent again now that we dropped the ilock.
 Allocate the entire reservation as unwritten blocks. 
	
	  Allocation succeeded but the requested range was not even partially
	  satisfied?  Bail out!
	
	  COW fork extents are supposed to remain unwritten until we're ready
	  to initiate a disk write.  For direct IO we are going to write the
	  data and need the conversion, but for buffered writes we're done.
  Cancel CoW reservations for some block range of an inode.
  If cancel_real is true this function cancels all COW fork extents for the
  inode; if cancel_real is false, real extents are not cleared.
  Caller must have already joined the inode to the current transaction. The
  inode will be joined to the transaction returned to the caller.
 Walk backwards until we're out of the IO range... 
 Extent delete may have bumped ext forward 
 Free the CoW orphan record. 
 Roll the transaction 
 Remove the mapping from the CoW fork. 
 Remove the quota reservation 
 Didn't do anything, push cursor back. 
 clear tag if cow fork is emptied 
  Cancel CoW reservations for some byte range of an inode.
  If cancel_real is true this function cancels all COW fork extents for the
  inode; if cancel_real is false, real extents are not cleared.
 Start a rolling transaction to remove the mappings 
 Scrape out the old CoW reservations 
  Remap part of the CoW fork into the data fork.
  We aim to remap the range starting at @offset_fsb and ending at @end_fsb
  into the data fork; this function will remap what it can (at the end of the
  range) and update @end_fsb appropriately.  Each remap gets its own
  transaction because we can end up merging and splitting bmbt blocks for
  every remap operation and we'd like to keep the block reservation
  requirements as low as possible.
 No COW extents?  That's easy! 
	
	  Lock the inode.  We have to ijoin without automatic unlock because
	  the lead transaction is the refcountbt record deletion; the data
	  fork update follows as a deferred log item.
	
	  In case of racing, overlapping AIO writes no COW extents might be
	  left by the time IO completes for the loser of the race.  In that
	  case we are done.
	
	  Structure copy @got into @del, then trim @del to the range that we
	  were asked to remap.  We preserve @got for the eventual CoW fork
	  deletion; from now on @del represents the mapping that we're
	  actually remapping.
	
	  Only remap real extents that contain data.  With AIO, speculative
	  preallocations can leak into the range we are called upon, and we
	  need to skip them.
 Unmap the old blocks in the data fork. 
 Trim the extent to whatever got unmapped. 
 Free the CoW orphan record. 
 Map the new blocks into the data fork. 
 Charge this new data fork mapping to the on-disk quota. 
 Remove the mapping from the CoW fork. 
 Update the caller about how much progress we made. 
  Remap parts of a file's data fork after a successful CoW.
	
	  Walk backwards until we're out of the IO range.  The loop function
	  repeatedly cycles the ILOCK to allocate one transaction per remapped
	  extent.
	 
	  If we're being called by writeback then the pages will still
	  have PageWriteback set, which prevents races with reflink remapping
	  and truncate.  Reflink remapping prevents races with writeback by
	  taking the iolock and mmaplock before flushing the pages and
	  remapping, which means there won't be any further writeback or page
	  cache dirtying until the reflink completes.
	 
	  We should never have two threads issuing writeback for the same file
	  region.  There are also have post-eof checks in the writeback
	  preparation code so that we don't bother writing out pages that are
	  about to be truncated.
	 
	  If we're being called as part of directio write completion, the dio
	  count is still elevated, which reflink and truncate will wait for.
	  Reflink remapping takes the iolock and mmaplock and waits for
	  pending dio to finish, which should prevent any directio until the
	  remap completes.  Multiple concurrent directio writes to the same
	  region are handled by end_cow processing only occurring for the
	  threads which succeed; the outcome of multiple overlapping direct
	  writes is not well defined anyway.
	 
	  It's possible that a buffered write and a direct write could collide
	  here (the buffered write stumbles in after the dio flushes and
	  invalidates the page cache and immediately queues writeback), but we
	  have never supported this 100%.  If either disk write succeeds the
	  blocks will be remapped.
  Free leftover CoW reservations that didn't get cleaned out.
  Reflinking (Block) Ranges of Two Files Together
  First, ensure that the reflink flag is set on both inodes.  The flag is an
  optimization to avoid unnecessary refcount btree lookups in the write path.
  Now we can iteratively remap the range of extents (and holes) in src to the
  corresponding ranges in dest.  Let drange and srange denote the ranges of
  logical blocks in dest and src touched by the reflink operation.
  While the length of drange is greater than zero,
     - Read src's bmbt at the start of srange ("imap")
     - If imap doesn't exist, make imap appear to start at the end of srange
       with zero length.
     - If imap starts before srange, advance imap to start at srange.
     - If imap goes beyond srange, truncate imap to end at the end of srange.
     - Punch (imap start - srange start + imap len) blocks from dest at
       offset (drange start).
     - If imap points to a real range of pblks,
          > Increase the refcount of the imap's pblks
          > Map imap's pblks into dest at the offset
            (drange start + imap start - srange start)
     - Advance drange and srange by (imap start - srange start + imap len)
  Finally, if the reflink made dest longer, update both the in-core and
  on-disk file sizes.
  ASCII Art Demonstration:
  Let's say we want to reflink this source file:
  ----SSSSSSS-SSSSS----SSSSSS (src file)
    <-------------------->
  into this destination file:
  --DDDDDDDDDDDDDDDDDDD--DDD (dest file)
         <-------------------->
  '-' means a hole, and 'S' and 'D' are written blocks in the src and dest.
  Observe that the range has different logical offsets in either file.
  Consider that the first extent in the source file doesn't line up with our
  reflink range.  Unmapping  and remapping are separate operations, so we can
  unmap more blocks from the destination file than we remap.
  ----SSSSSSS-SSSSS----SSSSSS
    <------->
  --DDDDD---------DDDDD--DDD
         <------->
  Now remap the source extent into the destination file:
  ----SSSSSSS-SSSSS----SSSSSS
    <------->
  --DDDDD--SSSSSSSDDDDD--DDD
         <------->
  Do likewise with the second hole and extent in our range.  Holes in the
  unmap range don't affect our operation.
  ----SSSSSSS-SSSSS----SSSSSS
             <---->
  --DDDDD--SSSSSSS-SSSSS-DDD
                  <---->
  Finally, unmap and remap part of the third extent.  This will increase the
  size of the destination file.
  ----SSSSSSS-SSSSS----SSSSSS
                   <----->
  --DDDDD--SSSSSSS-SSSSS----SSS
                        <----->
  Once we update the destination file's i_size, we're done.
  Ensure the reflink bit is set in both inodes.
 Lock both files against IO 
  Update destination inode size & cowextsize hint, if necessary.
  Do we have enough reserve in this AG to handle a reflink?  The refcount
  btree already reserved all the space it needs, but the rmap btree can grow
  infinitely, so we won't allow more reflinks when the AG is down to the
  btree reserves.
  Remap the given extent into the file.  The dmap blockcount will be set to
  the number of blocks that were actually remapped.
	
	  Start a rolling transaction to switch the mappings.
	 
	  Adding a written extent to the extent map can cause a bmbt split,
	  and removing a mapped extent from the extent can cause a bmbt split.
	  The two operations cannot both cause a split since they operate on
	  the same index in the bmap btree, so we only need a reservation for
	  one bmbt split if either thing is happening.  However, we haven't
	  locked the inode yet, so we reserve assuming this is the case.
	 
	  The first allocation call tries to reserve enough space to handle
	  mapping dmap into a sparse part of the file plus the bmbt split.  We
	  haven't locked the inode or read the existing mapping yet, so we do
	  not know for sure that we need the space.  This should succeed most
	  of the time.
	 
	  If the first attempt fails, try again but reserving only enough
	  space to handle a bmbt split.  This is the hard minimum requirement,
	  and we revisit quota reservations later when we know more about what
	  we're remapping.
	
	  Read what's currently mapped in the destination file into smap.
	  If smap isn't a hole, we will have to remove it before we can add
	  dmap to the destination file.
	
	  We can only remap as many blocks as the smaller of the two extent
	  maps, because we can only remap one extent at a time.
	
	  Two extents mapped to the same physical block must not have
	  different states; that's filesystem corruption.  Move on to the next
	  extent if they're both holes or both the same physical extent.
 If both extents are unwritten, leave them alone. 
 No reflinking if the AG of the dest mapping is low on space. 
	
	  Increase quota reservation if we think the quota block counter for
	  this file could increase.
	 
	  If we are mapping a written extent into the file, we need to have
	  enough quota block count reservation to handle the blocks in that
	  extent.  We log only the delta to the quota block counts, so if the
	  extent we're unmapping also has blocks allocated to it, we don't
	  need a quota reservation for the extent itself.
	 
	  Note that if we're replacing a delalloc reservation with a written
	  extent, we have to take the full quota reservation because removing
	  the delalloc reservation gives the block count back to the quota
	  count.  This is suboptimal, but the VFS flushed the dest range
	  before we started.  That should have removed all the delalloc
	  reservations, but we code defensively.
	 
	  xfs_trans_alloc_inode above already tried to grab an even larger
	  quota reservation, and kicked off a blockgc scan if it couldn't.
	  If we can't get a potentially smaller quota reservation now, we're
	  done.
		
		  If the extent we're unmapping is backed by storage (written
		  or not), unmap the extent and drop its refcount.
		
		  If the extent we're unmapping is a delalloc reservation,
		  we can use the regular bunmapi function to release the
		  incore state.  Dropping the delalloc reservation takes care
		  of the quota reservation for us.
	
	  If the extent we're sharing is backed by written storage, increase
	  its refcount and map it into the file.
 Update dest isize if needed. 
 Commit everything and unlock. 
 Remap a range of one file to the other. 
 Read extent from the source file 
		
		  The caller supposedly flushed all dirty pages in the source
		  file range, which means that writeback should have allocated
		  or deleted all delalloc reservations in that range.  If we
		  find one, that's a good sign that something is seriously
		  wrong here.
 Remap into the destination file at the given offset. 
 Advance drangesrange 
  If we're reflinking to a point past the destination file's EOF, we must
  zero any speculative post-EOF preallocations that sit between the old EOF
  and the destination file offset.
  Prepare two files for range cloning.  Upon a successful return both inodes
  will have the iolock and mmaplock held, the page cache of the out file will
  be truncated, and any leases on the out file will have been broken.  This
  function borrows heavily from xfs_file_aio_write_checks.
  The VFS allows partial EOF blocks to "match" for dedupe even though it hasn't
  checked that the bytes beyond EOF physically match. Hence we cannot use the
  EOF block in the source dedupe range because it's not a complete block match,
  hence can introduce a corruption into the file that has it's block replaced.
  In similar fashion, the VFS file cloning also allows partial EOF blocks to be
  "block aligned" for the purposes of cloning entire files.  However, if the
  source file range includes the EOF block and it lands within the existing EOF
  of the destination file, then we can expose stale data from beyond the source
  file EOF in the destination file.
  XFS doesn't support partial block sharing, so in both cases we have check
  these cases ourselves. For dedupe, we can simply round the length to dedupe
  down to the previous whole block and ignore the partial EOF block. While this
  means we can't dedupe the last block of a file, this is an acceptible
  tradeoff for simplicity on implementation.
  For cloning, we want to share the partial EOF block if it is also the new EOF
  block of the destination file. If the partial EOF block lies inside the
  existing destination EOF, then we have to abort the clone to avoid exposing
  stale data in the destination file. Hence we reject these clone attempts with
  -EINVAL in this case.
 Lock both files against IO 
 Check file eligibility and prepare for block sharing. 
 Don't reflink realtime inodes 
 Don't share DAX file data for now. 
 Attach dquots to dest inode before changing block map 
	
	  Zero existing post-eof speculative preallocations in the destination
	  file.
 Set flags and remap blocks. 
	
	  If pos_out > EOF, we may have dirtied blocks between EOF and
	  pos_out. In that case, we need to extend the flush and unmap to cover
	  from EOF to the end of the copy length.
 Does this inode need the reflink flag? 
 Is there still a shared block here? 
  Clear the inode reflink flag if there are no shared extents.
  The caller is responsible for joining the inode to the transaction passed in.
  The inode will be joined to the transaction that is returned to the caller.
	
	  We didn't find any shared blocks so turn off the reflink flag.
	  First, get rid of any leftover CoW mappings.
 Clear the inode flag. 
  Clear the inode reflink flag if there are no shared extents and the size
  hasn't changed.
 Start a rolling transaction to remove the mappings 
  Pre-COW all shared blocks within a given byte range of a file and turn off
  the reflink flag if we unshare all of the file's blocks.
 Turn off the reflink flag if possible. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  All Rights Reserved.
  Sector aligned buffer routines for buffer createreadwriteaccess
  Verify the log-relative block number and length in basic blocks are valid for
  an operation involving the given XFS log buffer. Returns true if the fields
  are valid, false otherwise.
  Allocate a buffer to hold log data.  The buffer needs to be able to map to
  a range of nbblks basic blocks at any valid offset within the log.
	
	  Pass log block 0 since we don't have an addr yet, buffer will be
	  verified on read.
	
	  We do log IO in units of log sectors (a power-of-2 multiple of the
	  basic block size), so we round up the requested size to accommodate
	  the basic blocks required for complete log sectors.
	 
	  In addition, the buffer may be used for a non-sector-aligned block
	  offset, in which case an IO of the requested size could extend
	  beyond the end of the buffer.  If the requested size is only 1 basic
	  block it will never straddle a sector boundary, so this won't be an
	  issue.  Nor will this be a problem if the log IO is done in basic
	  blocks (sector size 1).  But otherwise we extend the buffer by one
	  extra log sector to ensure there's space to accommodate this
	  possibility.
  Return the address of the start of the given block number's data
  in a log buffer.  The buffer covers a log sector-aligned region.
  dump debug superblock and log record information
  check log record header for recovery
	
	  IRIX doesn't write the h_fmt field and leaves it zeroed
	  (XLOG_FMT_UNKNOWN). This stops us from trying to recover
	  a dirty log created in IRIX.
  read the head block of the log and check the header
		
		  IRIX doesn't write the h_fs_uuid or h_fmt fields. If
		  h_fs_uuid is null, we assume this log was last mounted
		  by IRIX and continue.
  This routine finds (to an approximation) the first block in the physical
  log which contains the given cycle.  It uses a binary search algorithm.
  Note that the algorithm can not be perfect because the disk will not
  necessarily be perfect.
 last_half_cycle == mid_cycle 
 first_half_cycle == mid_cycle 
  Check that a range of blocks does not contain stop_on_cycle_no.
  Fill in new_blk with the block offset where such a block is
  found, or with -1 (an invalid block number) if there is no such
  block in the range.  The scan needs to occur from front to back
  and the pointer into the region must be updated since a later
  routine will need to perform another test.
	
	  Greedily allocate a buffer big enough to handle the full
	  range of basic blocks we'll be examining.  If that fails,
	  try a smaller size.  We need to be able to read at least
	  a log sector, or we're out of luck.
  Potentially backup over partial log record write.
  In the typical case, last_blk is the number of the block directly after
  a good log record.  Therefore, we subtract one to get the block number
  of the last block in the given buffer.  extra_bblks contains the number
  of blocks we would have read on a previous read.  This happens when the
  last log record is split over the end of the physical log.
  extra_bblks is the number of blocks potentially verified on a previous
  call to this routine.
 valid log record not found 
	
	  We hit the beginning of the physical log & still no header.  Return
	  to caller.  If caller can handle a return of -1, then this routine
	  will be called again for the end of the physical log.
	
	  We have the final block of the good log (the first block
	  of the log record _before_ the head. So we check the uuid.
	
	  We may have found a log record header before we expected one.
	  last_blk will be the 1st block # with a given cycle #.  We may end
	  up reading an entire log record.  In this case, we don't want to
	  reset last_blk.  Only when last_blk points in the middle of a log
	  record do we update last_blk.
  Head is defined to be the point of the log where the next log write
  could go.  This means that incomplete LR writes at the end are
  eliminated when calculating the head.  We aren't guaranteed that previous
  LR have complete transactions.  We only know that a cycle number of
  current cycle number -1 won't be present in the log if we start writing
  from our current block number.
  last_blk contains the block number of the first block with a given
  cycle number.
  Return: zero if normal, non-zero if error.
 Is the end of the log device zeroed? 
 Is the whole lot zeroed? 
			 Linux XFS shouldn't generate totally zeroed logs -
			  mkfs etc write a dummy unmount record to a fresh
			  log so we can store the uuid in there
 get cycle # of 1st block 
 get cycle # of last block 
	
	  If the 1st half cycle number is equal to the last half cycle number,
	  then the entire log is stamped with the same cycle number.  In this
	  case, head_blk can't be set to zero (which makes sense).  The below
	  math doesn't work out properly with head_blk equal to zero.  Instead,
	  we set it to log_bbnum which is an invalid block number, but this
	  value makes the math correct.  If head_blk doesn't changed through
	  all the tests below, head_blk is set to zero at the very end rather
	  than log_bbnum.  In a sense, log_bbnum and zero are the same block
	  in a circular file.
		
		  In this case we believe that the entire log should have
		  cycle number last_half_cycle.  We need to scan backwards
		  from the end verifying that there are no holes still
		  containing last_half_cycle - 1.  If we find such a hole,
		  then the start of that hole will be the new head.  The
		  simple case looks like
		         x | x ... | x - 1 | x
		  Another case that fits this picture would be
		         x | x + 1 | x ... | x
		  In this case the head really is somewhere at the end of the
		  log, as one of the latest writes at the beginning was
		  incomplete.
		  One more case is
		         x | x + 1 | x ... | x - 1 | x
		  This is really the combination of the above two cases, and
		  the head has to end up at the start of the x-1 hole at the
		  end of the log.
		 
		  In the 256k log case, we will read from the beginning to the
		  end of the log and search for cycle numbers equal to x-1.
		  We don't worry about the x+1 blocks that we encounter,
		  because we know that they cannot be the head since the log
		  started with x.
		
		  In this case we want to find the first block with cycle
		  number matching last_half_cycle.  We expect the log to be
		  some variation on
		         x + 1 ... | x ... | x
		  The first block with cycle number x (last_half_cycle) will
		  be where the new head belongs.  First we do a binary search
		  for the first occurrence of last_half_cycle.  The binary
		  search may not be totally accurate, so then we scan back
		  from there looking for occurrences of last_half_cycle before
		  us.  If that backwards scan wraps around the beginning of
		  the log, then we look for occurrences of last_half_cycle - 1
		  at the end of the log.  The cases we're looking for look
		  like
		                                v binary search stopped here
		         x + 1 ... | x | x + 1 | x ... | x
		                    ^ but we want to locate this spot
		  or
		         <---------> less than scan distance
		         x + 1 ... | x ... | x - 1 | x
		                            ^ we want to locate this spot
	
	  Now validate the answer.  Scan back some number of maximum possible
	  blocks and make sure each one has the expected cycle number.  The
	  maximum is determined by the total possible amount of buffering
	  in the in-core log.  The following number can be made tighter if
	  we actually look at the block size of the filesystem.
		
		  We are guaranteed that the entire check can be performed
		  in one buffer.
 need to read 2 parts of log 
		
		  We are going to scan backwards in the log in two parts.
		  First we scan the physical end of the log.  In this part
		  of the log, we are looking for blocks with cycle number
		  last_half_cycle - 1.
		  If we find one, then we know that the log starts there, as
		  we've found a hole that didn't get written in going around
		  the end of the physical log.  The simple case for this is
		         x + 1 ... | x ... | x - 1 | x
		         <---------> less than scan distance
		  If all of the blocks at the end of the log have cycle number
		  last_half_cycle, then we check the blocks at the start of
		  the log looking for occurrences of last_half_cycle.  If we
		  find one, then our current estimate for the location of the
		  first occurrence of last_half_cycle is wrong and we move
		  back to the hole we've found.  This case looks like
		         x + 1 ... | x | x + 1 | x ...
		                                ^ binary search stopped here
		  Another case we need to handle that only occurs in 256k
		  logs is
		         x + 1 ... | x ... | x+1 | x ...
		                    ^ binary search stops here
		  In a 256k log, the scan at the end of the log will see the
		  x + 1 blocks.  We need to skip past those since that is
		  certainly not the head of the log.  By searching for
		  last_half_cycle-1 we accomplish that.
		
		  Scan beginning of log now.  The last part of the physical
		  log is good.  This scan needs to verify that it doesn't find
		  the last_half_cycle.
	
	  Now we need to make sure head_blk is not pointing to a block in
	  the middle of a log record.
 don't read head_blk 
 start ptr at last block ptr before head_blk 
 We hit the beginning of the log during our search 
	
	  When returning here, we have a good block number.  Bad block
	  means that during a previous crash, we didn't have a clean break
	  from cycle number N to cycle number N-1.  In this case, we need
	  to find the first block with cycle number N-1.
  Seek backwards in the log for log record headers.
  Given a starting log block, walk backwards until we find the provided number
  of records or hit the provided tail block. The return value is the number of
  records encountered or a negative error code. The log block and buffer
  pointer of the last record seen are returned in rblk and rhead respectively.
	
	  Walk backwards from the head block until we hit the tail or the first
	  block in the log.
	
	  If we haven't hit the tail block or the log record header count,
	  start looking again from the end of the physical log. Note that
	  callers can pass head == tail if the tail is not yet known.
  Seek forward in the log for log record headers.
  Given head and tail blocks, walk forward from the tail block until we find
  the provided number of records or hit the head block. The return value is the
  number of records encountered or a negative error code. The log block and
  buffer pointer of the last record seen are returned in rblk and rhead
  respectively.
	
	  Walk forward from the tail block until we hit the head or the last
	  block in the log.
	
	  If we haven't hit the head block or the log record header count,
	  start looking again from the start of the physical log.
  Calculate distance from head to tail (i.e., unused space in the log).
  Verify the log tail. This is particularly important when torn or incomplete
  writes have been detected near the front of the log and the head has been
  walked back accordingly.
  We also have to handle the case where the tail was pinned and the head
  blocked behind the tail right before a crash. If the tail had been pushed
  immediately prior to the crash and the subsequent checkpoint was only
  partially written, it's possible it overwrote the last referenced tail in the
  log with garbage. This is not a coherency problem because the tail must have
  been pushed before it can be overwritten, but appears as log corruption to
  recovery because we have no way to know the tail was updated if the
  subsequent checkpoint didn't write successfully.
  Therefore, CRC check the log from tail to head. If a failure occurs and the
  offending record is within max iclog bufs from the head, walk the tail
  forward and retry until a valid tail is found or corruption is detected out
  of the range of a possible overwrite.
	
	  Make sure the tail points to a record (returns positive count on
	  success).
	
	  Run a CRC check from the tail to the head. We can't just check
	  MAX_ICLOGS records past the tail because the tail may point to stale
	  blocks cleared during the search for the headtail. These blocks are
	  overwritten with zero-length records and thus record count is not a
	  reliable indicator of the iclog state before a crash.
		
		  Is corruption within range of the head? If so, retry from
		  the next record. Otherwise return an error.
 skip to the next record; returns positive count on success 
  Detect and trim torn writes from the head of the log.
  Storage without sector atomicity guarantees can result in torn writes in the
  log in the event of a crash. Our only means to detect this scenario is via
  CRC verification. While we can't always be certain that CRC verification
  failure is due to a torn write vs. an unrelated corruption, we do know that
  only a certain number (XLOG_MAX_ICLOGS) of log records can be written out at
  one time. Therefore, CRC verify up to XLOG_MAX_ICLOGS records at the head of
  the log and treat failures in this range as torn writes as a matter of
  policy. In the event of CRC failure, the head is walked back to the last good
  record in the log and the tail is updated from that record and verified.
 inout: unverified head 
 out: tail block 
 start blk of last record 
 ptr to last record 
 last rec. wraps phys. log 
	
	  Check the head of the log for torn writes. Search backwards from the
	  head until we hit the tail or the maximum number of log record IOs
	  that could have been in flight at one time. Use a temporary buffer so
	  we don't trash the rheadbuffer pointers from the caller.
	
	  Now run a CRC verification pass over the records starting at the
	  block found above to the current head. If a CRC failure occurs, the
	  log block of the first bad record is saved in first_bad.
		
		  We've hit a potential torn write. Reset the error and warn
		  about it.
		
		  Get the header block and buffer pointer for the last good
		  record before the bad record.
		 
		  Note that xlog_find_tail() clears the blocks at the new head
		  (i.e., the records with invalid CRC) if the cycle number
		  matches the current cycle.
 XXX: right thing to do here? 
		
		  Reset the head block to the starting block of the first bad
		  log record and set the tail block based on the last good
		  record.
		 
		  Bail out if the updated headtail match as this indicates
		  possible corruption outside of the acceptable
		  (XLOG_MAX_ICLOGS) range. This is a job for xfs_repair...
  We need to make sure we handle log wrapping properly, so we can't use the
  calculated logbno directly. Make sure it wraps to the correct bno inside the
  log.
  The log is limited to 32 bit sizes, so we use the appropriate modulus
  operation here and cast it back to a 64 bit daddr on return.
  Check whether the head of the log points to an unmount record. In other
  words, determine whether the log is clean. If so, update the in-core state
  appropriately.
	
	  Look for unmount record. If we find it, then we know there was a
	  clean unmount. Since 'i' could be the last block in the physical
	  log, we convert to a log block before comparing to the head_blk.
	 
	  Save the current tail lsn to use to pass to xlog_clear_stale_blocks()
	  below. We won't want to clear the unmount record if there is one, so
	  we pass the lsn of the unmount record rather than the block after it.
			
			  Set tail and last sync so that newly written log
			  records will point recovery to after the current
			  unmount record.
	
	  Reset log values according to the state of the log when we
	  crashed.  In the case where head_blk == 0, we bump curr_cycle
	  one because the next write starts a new cycle rather than
	  continuing the cycle of the last good log record.  At this
	  point we have guaranteed that all partial log records have been
	  accounted for.  Therefore, we know that the last good log record
	  written was complete and ended exactly on the end boundary
	  of the physical log.
  Find the sync block number or the tail of the log.
  This will be the block number of the last record to have its
  associated buffers synced to disk.  Every log record header has
  a sync lsn embedded in it.  LSNs hold block numbers, so it is easy
  to get a sync block number.  The only concern is to figure out which
  log record header to believe.
  The following algorithm uses the log record header with the largest
  lsn.  The entire log record does not need to be valid.  We only care
  that the header is valid.
  We could speed up search by using current head_blk buffer, but it is not
  available.
	
	  Find previous log record
 special case 
 leave all other log inited values alone 
	
	  Search backwards through the log looking for the log record header
	  block. This wraps all the way back around to the head so something is
	  seriously wrong if we can't find it.
	
	  Set the log state based on the current head record.
	
	  Look for an unmount record at the head of the log. This sets the log
	  state to determine whether recovery is necessary.
	
	  Verify the log head if the log is not clean (e.g., we have anything
	  but an unmount record at the head). This uses CRC verification to
	  detect and trim torn writes. If discovered, CRC failures are
	  considered torn writes and the log head is trimmed accordingly.
	 
	  Note that we can only run CRC verification when the log is dirty
	  because there's no guarantee that the log data behind an unmount
	  record is compatible with the current architecture.
 update in-core state again if the head changed 
	
	  Note that the unmount was clean. If the unmount was not clean, we
	  need to know this to rebuild the superblock counters from the perag
	  headers if we have a filesystem using non-persistent counters.
	
	  Make sure that there are no blocks in front of the head
	  with the same cycle number as the head.  This can happen
	  because we allow multiple outstanding log writes concurrently,
	  and the later writes might make it out before earlier ones.
	 
	  We use the lsn from before modifying it so that we'll never
	  overwrite the unmount record after a clean unmount.
	 
	  Do this only if we are going to recover the filesystem
	 
	  NOTE: This used to say "if (!readonly)"
	  However on Linux, we can & do recover a read-only filesystem.
	  We only skip recovery if NORECOVERY is specified on mount,
	  in which case we would not be here.
	 
	  But... if the -device- itself is readonly, just skip this.
	  We can't recover this device anyway, so it won't matter.
  Is the log zeroed at all?
  The last binary search should be changed to perform an X block read
  once X becomes small enough.  You can then search linearly through
  the X blocks.  This will cut down on the number of reads we need to do.
  If the log is partially zeroed, this routine will pass back the blkno
  of the first block with cycle number 0.  It won't have a complete LR
  preceding it.
  Return:
 	0  => the log is completely written to
 	1 => use blk_no as the first block of the log
 	<0 => error has occurred
 check totally zeroed log 
 completely zeroed log 
 check partially zeroed log 
 log completely written to 
 we have a partially zeroed log 
	
	  Validate the answer.  Because there is no way to guarantee that
	  the entire log is made up of log records which are the same size,
	  we scan over the defined maximum blocks.  At this point, the maximum
	  is not chosen to mean anything special.   XXXmiken
	
	  We search for any instances of cycle number 0 that occur before
	  our current estimate of the head.  What we're trying to detect is
	         1 ... | 0 | 1 | 0...
	                        ^ binary search ends here
	
	  Potentially backup over partial log record write.  We don't need
	  to search the end of the log because we know it is zero.
  These are simple subroutines used by xlog_clear_stale_blocks() below
  to initialize a buffer full of empty log record headers and write
  them into the log.
	
	  Greedily allocate a buffer big enough to handle the full
	  range of basic blocks to be written.  If that fails, try
	  a smaller size.  We need to be able to write at least a
	  log sector, or we're out of luck.
	 We may need to do a read at the start to fill in part of
	  the buffer in the starting sector not covered by the first
	  write below.
		 We may need to do a read at the end to fill in part of
		  the buffer in the final sector not covered by the write.
		  If this is the same sector as the above read, skip it.
  This routine is called to blow away any incomplete log writes out
  in front of the log head.  We do this so that we won't become confused
  if we come up, write only a little bit more, and then crash again.
  If we leave the partial log records out there, this situation could
  cause us to think those partial writes are valid blocks since they
  have the current cycle number.  We get rid of them by overwriting them
  with empty log records with the old cycle number rather than the
  current one.
  The tail lsn is passed in rather than taken from
  the log so that we will not write over the unmount record after a
  clean unmount in a 512 block log.  Doing so would leave the log without
  any valid log records in it until a new one was written.  If we crashed
  during that time we would not be able to recover.
	
	  Figure out the distance between the new head of the log
	  and the tail.  We want to write over any blocks beyond the
	  head that we may have written just before the crash, but
	  we don't want to overwrite the tail of the log.
		
		  The tail is behind the head in the physical log,
		  so the distance from the head to the tail is the
		  distance from the head to the end of the log plus
		  the distance from the beginning of the log to the
		  tail.
		
		  The head is behind the tail in the physical log,
		  so the distance from the head to the tail is just
		  the tail block minus the head block.
	
	  If the head is right up against the tail, we can't clear
	  anything.
	
	  Take the smaller of the maximum amount of outstanding IO
	  we could have and the distance to the tail to clear out.
	  We take the smaller so that we don't overwrite the tail and
	  we don't waste all day writing from the head to the tail
	  for no reason.
		
		  We can stomp all the blocks we need to without
		  wrapping around the end of the log.  Just do it
		  in a single write.  Use the cycle number of the
		  current cycle minus one so that the log will look like:
		      n ... | n - 1 ...
		
		  We need to wrap around the end of the physical log in
		  order to clear all the blocks.  Do it in two separate
		  IOs.  The first write should be from the head to the
		  end of the physical log, and it should use the current
		  cycle number minus one just like above.
		
		  Now write the blocks at the start of the physical log.
		  This writes the remainder of the blocks we want to clear.
		  It uses the current cycle number since we're now on the
		  same cycle as the head so that we get:
		     n ... n ... | n - 1 ...
		     ^^^^^ blocks we're writing
  Release the recovered intent item in the AIL that matches the given intent
  type and intent id.
 		Log recover routines
  Sort the log items in the transaction.
  The ordering constraints are defined by the inode allocation and unlink
  behaviour. The rules are:
 	1. Every item is only logged once in a given transaction. Hence it
 	   represents the last logged state of the item. Hence ordering is
 	   dependent on the order in which operations need to be performed so
 	   required initial conditions are always met.
 	2. Cancelled buffers are recorded in pass 1 in a separate table and
 	   there's nothing to replay from them so we can simply cull them
 	   from the transaction. However, we can't do that until after we've
 	   replayed all the other items because they may be dependent on the
 	   cancelled buffer and replaying the cancelled buffer can remove it
 	   form the cancelled buffer table. Hence they have tobe done last.
 	3. Inode allocation buffers must be replayed before inode items that
 	   read the buffer and replay changes into it. For filesystems using the
 	   ICREATE transactions, this means XFS_LI_ICREATE objects need to get
 	   treated the same as inode allocation buffers as they create and
 	   initialise the buffers directly.
 	4. Inode unlink buffers must be replayed after inode items are replayed.
 	   This ensures that inodes are completely flushed to the inode buffer
 	   in a "free" state before we remove the unlinked inode list pointer.
  Hence the ordering needs to be inode allocation buffers first, inode items
  second, inode unlink buffers third and cancelled buffers last.
  But there's a problem with that - we can't tell an inode allocation buffer
  apart from a regular buffer, so we can't separate them. We can, however,
  tell an inode unlink buffer from the others, and so we can separate them out
  from all the other buffers and move them to last.
  Hence, 4 lists, in order from head to tail:
 	- buffer_list for all buffers except cancelledinode unlink buffers
 	- item_list for all non-buffer items
 	- inode_buffer_list for inode unlink buffers
 	- cancel_list for the cancelled buffers
  Note that we add objects to the tail of the lists so that first-to-last
  ordering is preserved within the lists. Adding objects to the head of the
  list means when we traverse from the head we walk them in last-to-first
  order. For cancelled buffers and inode unlink buffers this doesn't matter,
  but for all other items there may be specific ordering that we need to
  preserve.
			
			  return the remaining items back to the transaction
			  item list so they can be freed in caller.
  Perform the transaction.
  If the transaction modifies a buffer or inode, do it now.  Otherwise,
  EFIs and EFDs get queued up by adding entries into the AIL for them.
	
	  If the transaction is empty, the header was split across this and the
	  previous record. Copy the rest of the header.
 take the tail entry 
  The next region to add is the start of a new region.  It could be
  a whole region or it could be the first part of a new region.  Because
  of this, the assumption here is that the type and size fields of all
  format structures fit into the first 32 bits of the structure.
  This works because all regions must be 32 bit aligned.  Therefore, we
  either have both fields or we have neither field.  In the case we have
  neither field, the data part of the region is zero length.  We only have
  a log_op_header and can throw away the header since a new one will appear
  later.  If we have at least 4 bytes, then we can determine how many regions
  will appear in the current log item.
 any will do 
 we need to catch log corruptions here 
		
		  The transaction header can be arbitrarily split across op
		  records. If we don't have the whole thing here, copy what we
		  do have and handle the rest in the next record.
 take the tail entry 
 tail item is in use, get a new one 
 first region to be added 
 Description region is ri_buf[0] 
  Free up any resources allocated by the transaction
  Remember that EFIs, EFDs, and IUNLINKs are handled later.
 Free the regions in the item. 
 Free the item itself 
 Free the transaction recover structure 
  On error or completion, trans is freed.
 mask off ophdr transaction container flags 
	
	  Callees must not free the trans structure. We'll decide if we need to
	  free it or not based on the operation being done and it's result.
 expected flag values 
 success or fail, we are now done with this transaction. 
 unexpected flag values 
 just skip trans 
  Lookup the transaction recovery structure associated with the ID in the
  current ophdr. If the transaction doesn't exist and the start flag is set in
  the ophdr, then allocate a new transaction for future ID matches to find.
  Either way, return what we found during the lookup - an existing transaction
  or nothing.
	
	  skip over non-start transaction headers - we could be
	  processing slack space before the next transaction starts
	
	  This is a new transaction so allocate a new recovery container to
	  hold the recovery ops that will follow.
	
	  Nothing more to do for this ophdr. Items to be added to this new
	  transaction will be in subsequent ophdr containers.
 Do we understand who wrote this op? 
	
	  Check the ophdr contains all the data it is supposed to contain.
 nothing to do, so skip over this ophdr 
	
	  The recovered buffer queue is drained only once we know that all
	  recovery items for the current LSN have been processed. This is
	  required because:
	 
	  - Buffer write submission updates the metadata LSN of the buffer.
	  - Log recovery skips items with a metadata LSN >= the current LSN of
	    the recovery item.
	  - Separate recovery items against the same metadata buffer can share
	    a current LSN. I.e., consider that the LSN of a recovery item is
	    defined as the starting LSN of the first record in which its
	    transaction appears, that a record can hold multiple transactions,
	    andor that a transaction can span multiple records.
	 
	  In other words, we are allowed to submit a buffer from log recovery
	  once per current LSN. Otherwise, we may incorrectly skip recovery
	  items and cause corruption.
	 
	  We don't know up front whether buffers are updated multiple times per
	  LSN. Therefore, track the current LSN of each commit log record as it
	  is processed and drain the queue when it changes. Use commit records
	  because they are ordered correctly by the logging code.
  There are two valid states of the r_state field.  0 indicates that the
  transaction structure is in a normal state.  We have either seen the
  start of the transaction or the last operation we added was not a partial
  operation.  If the last operation we added to the transaction was a
  partial operation, we need to mark r_state with XLOG_WAS_CONT_TRANS.
  NOTE: skip LRs with 0 data length.
 check the log format matches our own - else we can't recover 
 errors will abort recovery 
 Take all the collected deferred ops and finish them in order. 
		
		  Create a new transaction reservation from the captured
		  information.  Set logcount to 1 to force the new transaction
		  to regrant every roll so that we can make forward progress
		  in recovery no matter how full the log might be.
		
		  Transfer to this new transaction all the dfops we captured
		  from recovering a single intent item.
 Release all the captured defer ops and capture structures in this list. 
  When this is called, all of the log intent items which did not have
  corresponding log done items should be in the AIL.  What we do now
  is update the data structures associated with each one.
  Since we process the log intent items in normal transactions, they
  will be removed at some point after the commit.  This prevents us
  from just walking down the list processing each one.  We'll use a
  flag in the intent item to skip those that we've already processed
  and use the AIL iteration mechanism's generation count to try to
  speed this up at least a bit.
  When we start, we know that the intents are the only things in the
  AIL.  As we process them, however, other items are added to the
  AIL.
		
		  We're done when we see something other than an intent.
		  There should be no intents left in the AIL now.
		
		  We should never see a redo item with a LSN higher than
		  the last transaction we found in the log at the start
		  of recovery.
		
		  NOTE: If your intent processing routine can create more
		  deferred ops, you must attach them to the capture list in
		  the recover routine or else those subsequent intents will be
		  replayed in the wrong order!
  A cancel occurs when the mount has failed and we're bailing out.
  Release all pending log intent items so they don't pin the AIL.
		
		  We're done when we see something other than an intent.
		  There should be no intents left in the AIL now.
  This routine performs a transaction to null out a bad inode pointer
  in an agi unlinked inode hash bucket.
	
	  Get the on disk inode to find the next inode in the bucket.
 setup for the next pass 
	
	  We can't read in the inode this bucket points to, or this inode
	  is messed up.  Just ditch this bucket of inodes.  We will lose
	  some inodes and space, but at least we won't hang.
	 
	  Call xlog_recover_clear_agi_bucket() to perform a transaction to
	  clear the inode pointer in the bucket.
  Recover AGI unlinked lists
  This is called during recovery to process any inodes which we unlinked but
  not freed when the system crashed.  These inodes will be on the lists in the
  AGI blocks. What we do here is scan all the AGIs and fully truncate and free
  any inodes found on the lists. Each inode is removed from the lists when it
  has been fully truncated and is freed. The freeing of the inode and its
  removal from the list must be atomic.
  If everything we touch in the agi processing loop is already in memory, this
  loop can hold the cpu for a long time. It runs without lock contention,
  memory allocation contention, the need wait for IO, etc, and so will run
  until we either run out of inodes to process, run low on memory or we run out
  of log space.
  This behaviour is bad for latency on single CPU and non-preemptible kernels,
  and can prevent other filesystem work (such as CIL pushes) from running. This
  can lead to deadlocks if the recovery process runs out of log reservation
  space. Hence we need to yield the CPU when there is other kernel work
  scheduled on this CPU to ensure other scheduled work can run without undue
  latency.
			
			  AGI is b0rked. Don't process it.
			 
			  We should probably mark the filesystem as corrupt
			  after we've recovered all the ag's we can....
		
		  Unlock the buffer so that it can be acquired in the normal
		  course of the transaction to truncate and free each inode.
		  Because we are not racing with anyone else here for the AGI
		  buffer, we don't even need to hold it locked to read the
		  initial unlinked bucket entries out of the buffer. We keep
		  buffer reference though, so that it stays pinned in memory
		  while we need the buffer.
	
	  Flush the pending unlinked inodes to ensure that the inactivations
	  are fully completed on disk and the incore inodes can be reclaimed
	  before we signal that recovery is complete.
  CRC check, unpack and process a log record.
	
	  Nothing else to do if this is a CRC verification pass. Just return
	  if this a record with a non-zero crc. Unfortunately, mkfs always
	  sets old_crc to 0 so we must consider this valid even on v5 supers.
	  Otherwise, return EFSBADCRC on failure so the callers up the stack
	  know precisely what failed.
	
	  We're in the normal recovery path. Issue a warning if and only if the
	  CRC in the header is non-zero. This is an advisory warning and the
	  zero CRC check prevents warnings from being emitted when upgrading
	  the kernel from one that does not add CRCs by default.
		
		  If the filesystem is CRC enabled, this mismatch becomes a
		  fatal log corruption failure.
	
	  LR body must have data (or it wouldn't have been written)
	  and h_len must not be greater than LR buffer size.
  Read the log from tail to head and process the log records found.
  Handle the two cases where the tail and head are in the same cycle
  and where the active portion of the log wraps around the end of
  the physical log separately.  The pass parameter is passed through
  to the routines called to process the data and is not looked at
  here.
 out: first bad log rec 
	
	  Read the header of the tail block and get the iclog buffer size from
	  h_size.  Use this to tell how many sectors make up the log header.
		
		  When using variable length iclogs, read first sector of
		  iclog header and extract the header size from it.  Get a
		  new hbp that is the correct size.
		
		  xfsprogs has a bug where record length is based on lsunit but
		  h_size (iclog size) is hardcoded to 32k. Now that we
		  unconditionally CRC verify the unmount record, this means the
		  log buffer can be too small for the record and cause an
		  overrun.
		 
		  Detect this condition here. Use lsunit for the buffer size as
		  long as this looks like the mkfs case. Otherwise, return an
		  error to avoid a buffer overrun.
		
		  Perform recovery around the end of the physical log.
		  When the head is not on the same cycle number as the tail,
		  we can't do a sequential recovery.
			
			  Check for header wrapping around physical end-of-log
 Read header in one read 
 This LR is split across physical log end 
 some data before physical log end 
				
				  Note: this black magic still works with
				  large sector sizes (non-512) only because:
				  - we increased the buffer size originally
				    by 1 sector giving us enough extra space
				    for the second read;
				  - the log start is guaranteed to be sector
				    aligned;
				  - we read the log end (LR header start)
				    _first_, then the log start (LR header end)
				    - order is important.
			
			  Read the log record data in multiple reads if it
			  wraps around the end of the log. Note that if the
			  header already wrapped, blk_no could point past the
			  end of the log. The record data is contiguous in
			  that case.
				 This log record is split across the
					 some data is before the physical
				
				  Note: this black magic still works with
				  large sector sizes (non-512) only because:
				  - we increased the buffer size originally
				    by 1 sector giving us enough extra space
				    for the second read;
				  - the log start is guaranteed to be sector
				    aligned;
				  - we read the log end (LR header start)
				    _first_, then the log start (LR header end)
				    - order is important.
 read first part of physical log 
 blocks in data section 
	
	  Submit buffers that have been added from the last record processed,
	  regardless of error status.
	
	  Transactions are freed at commit time but transactions without commit
	  records on disk are never committed. Free any that may be left in the
	  hash table.
  Do the recovery of the log.  We actually do this in two phases.
  The two passes are necessary in order to implement the function
  of cancelling a record written into the log.  The first pass
  determines those things which have been cancelled, and the
  second pass replays log items normally except for those which
  have been cancelled.  The handling of the replay and cancellations
  takes place in the log item type specific routines.
  The table of items which have cancel records in the log is allocated
  and freed at this level, since only here do we know when all of
  the log recovery has been completed.
	
	  First do a pass to find all of the cancelled buf log items.
	  Store them in the buf_cancel_table for use in the second pass.
	
	  Then do a second pass to actually recover the items in the log.
	  When it is complete free the table of buf cancel items.
 DEBUG 
  Do the actual recovery
	
	  First replay the images in the log.
	
	  We now update the tail_lsn since much of the recovery has completed
	  and there may be space available to use.  If there were no extent
	  or iunlinks, we can free up the entire log and set the tail_lsn to
	  be the last_sync_lsn.  This was set in xlog_find_tail to be the
	  lsn of the last known good LR on disk.  If there are extent frees
	  or iunlinks they will have some entries in the AIL; so we look at
	  the AIL to determine how to set the tail_lsn.
	
	  Now that we've finished replaying all buffer and inode updates,
	  re-read the superblock and reverify it.
 Convert superblock from on-disk format 
 re-initialise in-core superblock and geometry structures 
 Normal transactions can now occur 
  Perform recovery and re-initialize some log variables in xlog_find_tail.
  Return error or zero.
 find the tail of the log 
	
	  The superblock was read before the log was available and thus the LSN
	  could not be verified. Check the superblock LSN against the current
	  LSN now that it's known.
		 There used to be a comment here:
		 
		  disallow recovery on read-only mounts.  note -- mount
		  checks for ENOSPC and turns it into an intelligent
		  error message.
		  ...but this is no longer true.  Now, unless you specify
		  NORECOVERY (in which case this function would never be
		  called), we just go ahead and recover.  We do this all
		  under the vfs layer, so we can get away with it unless
		  the device itself is read-only, in which case we fail.
		
		  Version 5 superblock log feature mask validation. We know the
		  log is dirty so check if there are any unknown log features
		  in what we need to recover. If there are unknown features
		  (e.g. unsupported transactions, then simply reject the
		  attempt at recovery before touching anything.
		
		  Delay log recovery if the debug hook is set. This is debug
		  instrumentation to coordinate simulation of IO failures with
		  log recovery.
  In the first part of recovery we replay inodes and buffers and build up the
  list of intents which need to be processed. Here we process the intents and
  clean up the on disk unlinked inode lists. This is separated from the first
  part of recovery so that the root and real-time bitmap inodes can be read in
  from disk in between the two stages.  This is necessary so that we can free
  space in the real-time portion of the file system.
		
		  Cancel all the unprocessed intent items now so that we don't
		  leave them pinned in the AIL.  This can cause the AIL to
		  livelock on the pinned item if anyone tries to push the AIL
		  (inode reclaim does this) before we get around to
		  xfs_log_mount_cancel.
	
	  Sync the log to get all the intents out of the AIL.  This isn't
	  absolutely necessary, but it helps in case the unlink transactions
	  would have problems pushing the intents out of the way.
	
	  Now that we've recovered the log and all the intents, we can clear
	  the log incompat feature bits in the superblock because there's no
	  longer anything to protect.  We rely on the AIL push to write out the
	  updated superblock after everything else.
  Read all of the agf and agi counters and check that they
  are consistent with the superblock counters.
 DEBUG 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2016 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Freeing the BUI requires that we remove it from the AIL if it has already
  been placed there. However, the BUI may not yet have been placed in the AIL
  when called by xfs_bui_release() from BUD processing due to the ordering of
  committed vs unpin operations in bulk insert operations. Hence the reference
  count to ensure only the last caller frees the BUI.
  This is called to fill in the vector of log iovecs for the
  given bui log item. We use only 1 iovec, and we point that
  at the bui_log_format structure embedded in the bui item.
  It is at this point that we assert that all of the extent
  slots in the bui item have been filled.
  The unpin operation is the last place an BUI is manipulated in the log. It is
  either inserted in the AIL or aborted in the event of a log IO error. In
  either case, the BUI transaction has been successfully committed to make it
  this far. Therefore, we expect whoever committed the BUI to either construct
  and commit the BUD or drop the BUD's reference in the event of error. Simply
  drop the log's BUI reference now that the log is done with it.
  The BUI has been either committed or aborted if the transaction has been
  cancelled. If the transaction was cancelled, an BUD isn't going to be
  constructed and thus we free the BUI here directly.
  Allocate and initialize an bui item with the given number of extents.
  This is called to fill in the vector of log iovecs for the
  given bud log item. We use only 1 iovec, and we point that
  at the bud_log_format structure embedded in the bud item.
  It is at this point that we assert that all of the extent
  slots in the bud item have been filled.
  The BUD is either committed or aborted if the transaction is cancelled. If
  the transaction is cancelled, drop our reference to the BUI and free the
  BUD.
  Finish an bmap update and log it to the BUD. Note that the
  transaction is marked dirty regardless of whether the bmap update
  succeeds or fails to support the BUIBUD lifecycle rules.
	
	  Mark the transaction dirty, even on error. This ensures the
	  transaction is aborted, which:
	 
	  1.) releases the BUI and frees the BUD
	  2.) shuts down the filesystem
 Sort bmap intents by inode. 
 Set the map extent flags for this mapping. 
 Log bmap updates in the intent item. 
	
	  atomic_inc_return gives us the value after the increment;
	  we want to use it as an array index so we need to subtract 1 from
	  it.
 Get an BUD so we can process all the deferred rmap updates. 
 Process a deferred rmap update. 
 Abort all pending BUIs. 
 Cancel a deferred rmap update. 
 Is this recovered BUI ok? 
 Only one mapping operation per BUI... 
  Process a bmap update intent item that was recovered from the log.
  We need to update some inode's bmbt.
 Allocate transaction and do the work. 
	
	  Commit transaction, which frees the transaction and saves the inode
	  for later replay activities.
 Relog an intent item to push the log tail forward. 
  Copy an BUI format buffer from the given buf, and into the destination
  BUI format structure.  The BUIBUD items were designed not to need any
  special alignment handling.
  This routine is called to create an in-core extent bmap update
  item from the bui format structure which was logged on disk.
  It allocates an in-core bui, copies the extents from the format
  structure into it, and adds the bui to the AIL with the given
  LSN.
	
	  Insert the intent into the AIL directly and drop one reference so
	  that finishing or canceling the work will drop the other.
  This routine is called when an BUD format structure is found in a committed
  transaction in the log. Its purpose is to cancel the corresponding BUI if it
  was still in the log. To do this it searches the AIL for the BUI with an id
  equal to that in the BUD format structure. If we find it we drop the BUD
  reference, which removes the BUI from the AIL and frees it.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  Copyright (c) 2016-2018 Christoph Hellwig.
  All Rights Reserved.
  Fast and loose check if this write could update the on-disk inode size.
  Update on-disk file size now that data has been written to disk.
  IO write completion.
	
	  We can allocate memory here while doing writeback on behalf of
	  memory reclaim.  To avoid memory allocation deadlocks set the
	  task-wide nofs context for the following operations.
	
	  Just clean up the in-memory structures if the fs has been shut down.
	
	  Clean up all COW blocks and underlying data fork delalloc blocks on
	  IO error. The delalloc punch is required because this ioend was
	  mapped to blocks in the COW fork and the associated pages are no
	  longer dirty. If we don't remove delalloc blocks here, they become
	  stale and can corrupt free space accounting on unmount.
	
	  Success: commit the COW or unwritten blocks if needed.
 Finish all pending io completions. 
  Fast revalidation of the cached writeback mapping. Return true if the current
  mapping is valid, false otherwise.
	
	  If this is a COW mapping, it is sufficient to check that the mapping
	  covers the offset. Be careful to check this first because the caller
	  can revalidate a COW mapping without updating the data seqno.
	
	  This is not a COW mapping. Check the sequence number of the data fork
	  because concurrent changes could have invalidated the extent. Check
	  the COW fork because concurrent changes since the last time we
	  checked (and found nothing at this offset) could have added
	  overlapping blocks.
  Pass in a dellalloc extent and convert it to real extents, return the real
  extent that maps offset_fsb in wpc->iomap.
  The current page is held locked so nothing could have removed the block
  backing offset_fsb, although it could have moved from the COW to the data
  fork by another thread.
	
	  Attempt to allocate whatever delalloc extent currently backs offset
	  and put the result into wpc->iomap.  Allocate in a loop because it
	  may take several attempts to allocate real blocks for a contiguous
	  delalloc extent if free space is sufficiently fragmented.
	
	  COW fork blocks can overlap data fork blocks even if the blocks
	  aren't shared.  COW IO always takes precedent, so we must always
	  check for overlap on reflink inodes unless the mapping is already a
	  COW one, or the COW fork hasn't changed from the last time we looked
	  at it.
	 
	  It's safe to check the COW fork if_seq here without the ILOCK because
	  we've indirectly protected against concurrent updates: writeback has
	  the page locked, which prevents concurrent invalidations by reflink
	  and directio and prevents concurrent buffered writes to the same
	  page.  Changes to if_seq always happen under i_lock, which protects
	  against concurrent updates and provides a memory barrier on the way
	  out that ensures that we always see the current value.
	
	  If we don't have a valid map, now it's time to get a new one for this
	  offset.  This will convert delayed allocations (including COW ones)
	  into real extents.  If we return without a valid map, it means we
	  landed in a hole and we skip the block.
	
	  Check if this is offset is covered by a COW extents, and if yes use
	  it directly instead of looking up anything in the data fork.
	
	  No COW extent overlap. Revalidate now that we may have updated
	  ->cow_seq. If the data mapping is still valid, we're done.
	
	  If we don't have a valid map, now it's time to get a new one for this
	  offset.  This will convert delayed allocations (including COW ones)
	  into real extents.
 fake a hole past EOF 
 landed in a hole or beyond EOF? 
	
	  Truncate to the next COW extent if there is one.  This is the only
	  opportunity to do this because we can skip COW fork lookups for the
	  subsequent blocks in the mapping; however, the requirement to treat
	  the COW range separately remains.
 got a delalloc extent? 
		
		  If we failed to find the extent in the COW fork we might have
		  raced with a COW to data fork conversion or truncate.
		  Restart the lookup to catch the extent in the data fork for
		  the former case, but prevent additional retries to avoid
		  looping forever for the latter case.
	
	  Due to merging the return real extent might be larger than the
	  original delalloc one.  Trim the return extent to the next COW
	  boundary again to force a re-lookup.
	
	  We can allocate memory here while doing writeback on behalf of
	  memory reclaim.  To avoid memory allocation deadlocks set the
	  task-wide nofs context for the following operations.
 Convert CoW extents to regular 
 send ioends that might require a transaction to the completion wq 
  If the page has delalloc blocks on it, we need to punch them out before we
  invalidate the page.  If we don't, we leave a stale delalloc mapping on the
  inode that can trip up a later direct IO read operation on the same region.
  We prevent this by truncating away the delalloc regions on the page.  Because
  they are delalloc, we can do this without needing a transaction. Indeed - if
  we get ENOSPC errors, we have to be able to do this truncation without a
  transaction as there is no space left for block reservation (typically why we
  see a ENOSPC in writeback).
	
	  Writing back data in a transaction context can result in recursive
	  transactions. This is bad, so issue a warning and get out of here.
	
	  The swap code (ab-)uses ->bmap to get a block mapping and then
	  bypasses the file system for actual IO.  We really can't allow
	  that on reflinks inodes, so we have to skip out here.  And yes,
	  0 is the magic code for a bmap error.
	 
	  Since we don't pass back blockdev info, we can't return bmap
	  information for rt files either.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
 Is this log iovec plausibly large enough to contain the buffer log format? 
  Return the number of log iovecs and space needed to log the given buf log
  item segment.
  It calculates this as 1 iovec for the buf log format structure and 1 for each
  stretch of non-contiguous chunks to be logged.  Contiguous chunks are logged
  in a single iovec.
		
		  Straddling a page is rare because we don't log contiguous
		  chunks of unmapped buffers anywhere.
		
		  This takes the bit number to start looking from and
		  returns the next set bit from there.  It returns -1
		  if there are no more bits set or the start bit is
		  beyond the end of the bitmap.
 Count the first bit we jumped out of the above loop from 
		
		  This takes the bit number to start looking from and
		  returns the next set bit from there.  It returns -1
		  if there are no more bits set or the start bit is
		  beyond the end of the bitmap.
		
		  If we run out of bits, leave the loop,
		  else if we find a new set of bits bump the number of vecs,
		  else keep scanning the current set of bits.
  Return the number of log iovecs and space needed to log the given buf log
  item.
  Discontiguous buffers need a format structure per region that is being
  logged. This makes the changes in the buffer appear to log recovery as though
  they came from separate buffers, just like would occur if multiple buffers
  were used instead of a single discontiguous buffer. This enables
  discontiguous buffers to be in-memory constructs, completely transparent to
  what ends up on disk.
  If the XFS_BLI_STALE flag has been set, then log nothing but the buf log
  format structures. If the item has previously been logged and has dirty
  regions, we do not relog them in stale buffers. This has the effect of
  reducing the size of the relogged item by the amount of dirty data tracked
  by the log item. This can result in the committing transaction reducing the
  amount of space being consumed by the CIL.
		
		  The buffer is stale, so all we need to log is the buf log
		  format structure with the cancel flag in it as we are never
		  going to replay the changes tracked in the log item.
		
		  The buffer has been logged just to order it. It is not being
		  included in the transaction commit, so no vectors are used at
		  all.
	
	  The vector count is based on the number of buffer vectors we have
	  dirty bits in. This will only be greater than one when we have a
	  compound buffer with more than one segment dirty. Hence for compound
	  buffers we need to track which segment the dirty bits correspond to,
	  and when we move from one segment to the next increment the vector
	  count for the extra buf log format structure that will need to be
	  written.
	
	  Round up the buffer size required to minimise the number of memory
	  allocations that need to be done as this item grows when relogged by
	  repeated modifications.
 copy the flags across from the base format item 
	
	  Base size is the actual size of the ondisk structure - it reflects
	  the actual size of the dirty bitmap rather than the size of the in
	  memory structure.
		
		  If the map is not be dirty in the transaction, mark
		  the size as zero and do not advance the vector pointer.
		
		  The buffer is stale, so all we need to log
		  is the buf log format structure with the
		  cancel flag in it.
	
	  Fill in an iovec for each set of contiguous chunks.
		
		  Straddling a page is rare because we don't log contiguous
		  chunks of unmapped buffers anywhere.
		
		  This takes the bit number to start looking from and
		  returns the next set bit from there.  It returns -1
		  if there are no more bits set or the start bit is
		  beyond the end of the bitmap.
		
		  This takes the bit number to start looking from and
		  returns the next set bit from there.  It returns -1
		  if there are no more bits set or the start bit is
		  beyond the end of the bitmap.
		
		  If we run out of bits fill in the last iovec and get out of
		  the loop.  Else if we start a new set of bits then fill in
		  the iovec for the series we were looking at and start
		  counting the bits in the new one.  Else we're still in the
		  same set of bits so just keep counting and scanning.
  This is called to fill in the vector of log iovecs for the
  given log buf item.  It fills the first entry with a buf log
  format structure, and the rest point to contiguous chunks
  within the buffer.
	
	  If it is an inode buffer, transfer the in-memory state to the
	  format flags and clear the in-memory state.
	 
	  For buffer based inode allocation, we do not transfer
	  this state if the inode buffer allocation has not yet been committed
	  to the log as setting the XFS_BLI_INODE_BUF flag will prevent
	  correct replay of the inode allocation.
	 
	  For icreate item based inode allocation, the buffers aren't written
	  to the journal during allocation, and hence we should always tag the
	  buffer as an inode buffer so that the correct unlinked list replay
	  occurs during recovery.
	
	  Check to make sure everything is consistent.
  This is called to pin the buffer associated with the buf log item in memory
  so it cannot be written out.
  We also always take a reference to the buffer log item here so that the bli
  is held while the item is pinned in memory. This means that we can
  unconditionally drop the reference count a transaction holds when the
  transaction is completed.
  This is called to unpin the buffer associated with the buf log item which
  was previously pinned with a call to xfs_buf_item_pin().
	
	  Drop the bli ref associated with the pin and grab the hold required
	  for the IO simulation failure in the abort case. We have to do this
	  before the pin count drops because the AIL doesn't acquire a bli
	  reference. Therefore if the refcount drops to zero, the bli could
	  still be AIL resident and the buffer submitted for IO (and freed on
	  completion) at any point before we return. This can be removed once
	  the AIL properly holds a reference on the bli.
 nothing to do but drop the pin count if the bli is active 
		
		  If we get called here because of an IO error, we may or may
		  not have the item on the AIL. xfs_trans_ail_delete() will
		  take care of that situation. xfs_trans_ail_delete() drops
		  the AIL lock.
		
		  The buffer must be locked and held by the caller to simulate
		  an async IO failure. We acquired the hold for this case
		  before the buffer was unpinned.
		
		  If we have just raced with a buffer being pinned and it has
		  been marked stale, we could end up stalling until someone else
		  issues a log force to unpin the stale buffer. Check for the
		  race condition here so xfsaild recognizes the buffer is pinned
		  and queues a log force to move it along.
 has a previous flush failed due to IO errors? 
  Drop the buffer log item refcount and take appropriate action. This helper
  determines whether the bli must be freed or not, since a decrement to zero
  does not necessarily mean the bli is unused.
  Return true if the bli is freed, false otherwise.
 drop the bli ref and return if it wasn't the last one 
	
	  We dropped the last ref and must free the item if clean or aborted.
	  If the bli is dirty and non-aborted, the buffer was clean in the
	  transaction but still awaiting writeback from previous changes. In
	  that case, the bli is freed on buffer writeback completion.
	
	  The bli is aborted or clean. An aborted item may be in the AIL
	  regardless of dirty state.  For example, consider an aborted
	  transaction that invalidated a dirty bli and cleared the dirty
	  state.
  Release the buffer associated with the buf log item.  If there is no dirty
  logged data associated with the buffer recorded in the buf log item, then
  free the buf log item and remove the reference to it in the buffer.
  This call ignores the recursion count.  It is only called when the buffer
  should REALLY be unlocked, regardless of the recursion count.
  We unconditionally drop the transaction's reference to the log item. If the
  item was logged, then another reference was taken when it was pinned, so we
  can safely drop the transaction reference now.  This also allows us to avoid
  potential races with the unpin code freeing the bli by not referencing the
  bli after we've dropped the reference count.
  If the XFS_BLI_HOLD flag is set in the buf log item, then free the log item
  if necessary but do not unlock the buffer.  This is for support of
  xfs_trans_bhold(). Make sure the XFS_BLI_HOLD field is cleared if we don't
  free the item.
	
	  The bli dirty state should match whether the blf has logged segments
	  except for ordered buffers, where only the bli should be dirty.
	
	  Clear the buffer's association with this transaction and
	  per-transaction state from the bli, which has been copied above.
	
	  Unref the item and unlock the buffer unless held or stale. Stale
	  buffers remain locked until final unpin unless the bli is freed by
	  the unref call. The latter implies shutdown because buffer
	  invalidation dirties the bli and transaction.
  This is called to find out where the oldest active copy of the
  buf log item in the on disk log resides now that the last log
  write of it completed at the given lsn.
  We always re-log all the dirty data in a buffer, so usually the
  latest copy in the on disk log is the only one that matters.  For
  those cases we simply return the given lsn.
  The one exception to this is for buffers full of newly allocated
  inodes.  These buffers are only relogged with the XFS_BLI_INODE_BUF
  flag set, indicating that only the di_next_unlinked fields from the
  inodes in the buffers will be replayed during recovery.  If the
  original newly allocated inode images have not yet been flushed
  when the buffer is so relogged, then we need to make sure that we
  keep the old images in the 'active' portion of the log.  We do this
  by returning the original lsn of that transaction here rather than
  the current one.
  Allocate a new buf log item to go with the given buffer.
  Set the buffer's b_log_item field to point to the new
  buf log item.
	
	  Check to see if there is already a buf log item for
	  this buffer. If we do already have one, there is
	  nothing to do here so return.
	
	  chunks is the number of XFS_BLF_CHUNK size pieces the buffer
	  can be divided into. Make sure not to truncate any pieces.
	  map_size is the size of the bitmap needed to describe the
	  chunks of the buffer.
	 
	  Discontiguous buffer support follows the layout of the underlying
	  buffer. This makes the implementation as simple as possible.
  Mark bytes first through last inclusive as dirty in the buf
  item's bitmap.
	
	  Convert byte offsets to bit numbers.
	
	  Calculate the total number of bits to be set.
	
	  Get a pointer to the first word in the bitmap
	  to set a bit in.
	
	  Calculate the starting bit in the first word.
	
	  First set any bits in the first word of our range.
	  If it starts at bit 0 of the word, it will be
	  set below rather than here.  That is what the variable
	  bit tells us. The variable bits_set tracks the number
	  of bits that have been set so far.  End_bit is the number
	  of the last bit to be set in this word plus one.
	
	  Now set bits a whole word at a time that are between
	  first_bit and last_bit.
	
	  Finally, set any bits left to be set in one last partial word.
  Mark bytes first through last inclusive as dirty in the buf
  item's bitmap.
	
	  walk each buffer segment and mark them dirty appropriately.
 skip to the map that includes the first byte to log 
		
		  Trim the range to this segment and mark it in the bitmap.
		  Note that we must convert buffer offsets to segment relative
		  offsets (e.g., the first byte of each segment is byte 0 of
		  that segment).
  Return true if the buffer has any ranges loggeddirtied by a transaction,
  false otherwise.
  xfs_buf_item_relse() is called when the buf log item is no longer needed.
	
	  If we are forcibly shutting down, this may well be off the AIL
	  already. That's because we simulate the log-committed callbacks to
	  unpin these buffers. Or we may never have put this item on AIL
	  because of the transaction was aborted forcibly.
	  xfs_trans_ail_delete() takes care of these.
	 
	  Either way, AIL is useless if we're forcing a shutdown.
	 
	  Note that log recovery writes might have buffer items that are not on
	  the AIL even when the file system is not shut down.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  Copyright (c) 2012-2013 Red Hat, Inc.
  All rights reserved.
 ----- Kernel only functions below ----- 
	
	  Check component lengths of the target path name.
 total string too long 
	
	  Make sure that we have allocated dquot(s) on disk.
	
	  The symlink will fit into the inode data fork?
	  There can't be any attributes so we get the whole variable part.
	
	  Check whether the directory allows new symlinks or not.
	
	  Allocate an inode for the symlink.
	
	  Now we join the directory inode to the transaction.  We do not do it
	  earlier because xfs_dir_ialloc might commit the previous transaction
	  (and release all the locks).  An error from here on will result in
	  the transaction cancel unlocking dp so don't do it explicitly in the
	  error path.
	
	  Also attach the dquot(s) to it, if applicable.
	
	  If the symlink will fit into the inode, write it inline.
	
	  Create the directory entry for the symlink.
	
	  If this is a synchronous mount, make sure that the
	  symlink transaction goes to disk before returning to
	  the user.
	
	  Wait until after the current transaction is aborted to finish the
	  setup of the inode and release the inode.  This prevents recursive
	  transactions and deadlocks from xfs_inactive.
  Free a symlink that has blocks associated with it.
  Note: zero length symlinks are not allowed to exist. When we set the size to
  zero, also change it to a regular file so that it does not get written to
  disk as a zero length symlink. The inode is on the unlinked list already, so
  userspace cannot find this inode anymore, so this change is not user visible
  but allows us to catch corrupt zero-length symlinks in the verifiers.
	
	  We're freeing a symlink that has some
	  blocks allocated to it.  Free the
	  blocks here.  We know that we've got
	  either 1 or 2 extents and that we can
	  free them all in one bunmapi call.
	
	  Lock the inode, fix the size, turn it into a regular file and join it
	  to the transaction.  Hold it so in the normal path, we still have it
	  locked for the second transaction.  In the error paths we need it
	  held so the cancel won't rele it, see below.
	
	  Find the block(s) so we can inval and unmap them.
	
	  Invalidate the block(s). No validation is done.
	
	  Unmap the dead block(s) to the dfops.
	
	  Commit the transaction. This first logs the EFI and the inode, then
	  rolls and commits the transaction that frees the extents.
	
	  Remove the memory for extent descriptions (just bookkeeping).
  xfs_inactive_symlink - free a symlink
	
	  Inline fork state gets removed by xfs_difree() so we have nothing to
	  do here in that case.
 remove the remote symlink 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Directories have different lock order w.r.t. mmap_lock compared to regular
  files. This is due to readdir potentially triggering page faults on a user
  buffer inside filldir(), and this happens with the ilock on the directory
  held. For regular files, the lock order is the other way around - the
  mmap_lock is taken during the page fault, and then we lock the ilock to do
  block mapping. Hence we need a different class for the directory ilock so
  that lockdep can tell them apart.
  Hook in SELinux.  This is not quite correct yet, what we really need
  here (as we do for default ACLs) is a mechanism by which creation of
  these attrs can be journalled at inode creation time (along with the
  inode, of course, such that log replay can't cause these to be lost).
	 Oh, the horror.
	  If we can't add the ACL or we fail in
	  xfs_init_security we must back out.
	  ENOSPC can hit here, among other things.
  Check to see if we are likely to need an extended attribute to be added to
  the inode we are about to allocate. This allows the attribute fork to be
  created during the inode allocation, reducing the number of transactions we
  need to do in this fast path.
  The security checks are optimistic, but not guaranteed. The two LSMs that
  require xattrs to be added here (selinux and smack) are also the only two
  LSMs that add a sb->s_security structure to the superblock. Hence if security
  is enabled and sb->s_security is set, we have a pretty good idea that we are
  going to be asked to add a security xattr immediately after allocating the
  xfs inode and instantiating the VFS inode.
 unnamed file 
	
	  Irix uses Missed'em'V split, but doesn't want to see
	  the upper 5 bits of (14bit) major.
 Verify mode is valid also for tmpfile case 
		
		  The VFS requires that any inode fed to d_tmpfile must have
		  nlink == 1 so that it can decrement the nlink in d_tmpfile.
		  However, we created the temp file with nlink == 0 because
		  we're not allowed to put an inode with nlink > 0 on the
		  unlinked list.  Therefore we have to set nlink to 1 so that
		  d_tmpfile can immediately set it back to zero.
		
		  call d_add(dentry, NULL) here when d_drop_negative_children
		  is called in xfs_vn_mknod (ie. allow negative dentries
		  with CI filesystems).
 if exact match, just splice and exit 
 else case-insensitive match... 
	
	  With unlink, the VFS makes the dentry "negative": no inode,
	  but still hashed. This is incompatible with case-insensitive
	  mode, so invalidate (unhash) the dentry in CI-mode.
 if we are exchanging files, we need to set i_mode of both files 
  careful here - this function can get called recursively, so
  we need to be very careful about how much stack we use.
  uio is kmalloced for this reason...
	
	  The VFS crashes on a NULL pointer, so return -EFSCORRUPTED if
	  if_data is junk.
	
	  If the file blocks are being allocated from a realtime volume, then
	  always return the realtime extent size.
	
	  Allow large block sizes to be reported to userspace programs if the
	  "largeio" mount option is used.
	 
	  If compatibility mode is specified, simply return the basic unit of
	  caching so that we don't get inefficient readmodifywrite IO from
	  user apps. Otherwise....
	 
	  If the underlying volume is a stripe, then return the stripe width in
	  bytes as the recommended IO size. It is not a stripe and we've set a
	  default buffered IO size, return that, otherwise return the compat
	  default.
	
	  Note: If you add another clause to set an attribute flag, please
	  update attributes_mask below.
  Set non-size attributes of an inode.
  Caution: The caller of this function is responsible for calling
  setattr_prepare() or otherwise verifying the change is fine.
	
	  If disk quotas is on, we make sure that the dquots do exist on disk,
	  before we start any other transactions. Trying to do this later
	  is messy. We don't care to take a readlock to look at the ids
	  in inode here, because we can't hold it across the trans_reserve.
	  If the IDs do change before we take the ilock, we're covered
	  because the i_dquot fields will get updated anyway.
		
		  We take a reference when we initialize udqp and gdqp,
		  so it is important that we never blindly double trip on
		  the same variable. See xfs_create() for an example.
	
	  Change file ownership.  Must be the owner or privileged.
		
		  These IDs could have changed since we last looked at them.
		  But, we're assured that if the ownership did change
		  while we didn't have the inode locked, inode's dquot(s)
		  would have changed also.
		
		  CAP_FSETID overrides the following restrictions:
		 
		  The set-user-ID and set-group-ID bits of a file will be
		  cleared upon successful return from chown()
		
		  Change the ownerships and register quota modifications
		  in the transaction.
	
	  Release any dquot(s) the inode had kept before chown.
	
	  XXX(hch): Updating the ACL entries is not atomic vs the i_mode
	  	     update.  We could avoid this with linked transactions
	  	     and passing down the transaction pointer all the way
	 	     to attr_set.  No previous user of the generic
	  	     Posix ACL code seems to care about this issue either.
  Truncate file.  Must have write permission and not be a directory.
  Caution: The caller of this function is responsible for calling
  setattr_prepare() or otherwise verifying the change is fine.
	
	  Short circuit the truncate case for zero length files.
		
		  Use the regular setattr path to update the timestamps.
	
	  Make sure that the dquots are attached to the inode.
	
	  Wait for all direct IO to complete.
	
	  File data changes must be complete before we start the transaction to
	  modify the inode.  This needs to be done before joining the inode to
	  the transaction because the inode cannot be unlocked once it is a
	  part of the transaction.
	 
	  Start with zeroing any data beyond EOF that we may expose on file
	  extension, or zeroing out the rest of the block on a downward
	  truncate.
		
		  iomap won't detect a dirty page over an unwritten block (or a
		  cow block over a hole) and subsequently skips zeroing the
		  newly post-EOF portion of the page. Flush the new EOF to
		  convert the block before the pagecache truncate.
	
	  We've already locked out new page faults, so now we can safely remove
	  pages from the page cache knowing they won't get refaulted until we
	  drop the XFS_MMAP_EXCL lock after the extent manipulations are
	  complete. The truncate_setsize() call also cleans partial EOF page
	  PTEs on extending truncates and hence ensures sub-page block size
	  filesystems are correctly handled, too.
	 
	  We have to do all the page cache truncate work outside the
	  transaction context as the "lock" order is page lock->log space
	  reservation as defined by extent allocation in the writeback path.
	  Hence a truncate can fail with ENOMEM from xfs_trans_alloc(), but
	  having already truncated the in-memory version of the file (i.e. made
	  user visible changes). There's not much we can do about this, except
	  to hope that the caller sees ENOMEM and retries the truncate
	  operation.
	 
	  And we update in-core i_size and truncate page cache beyond newsize
	  before writeback the [i_disk_size, newsize] range, so we're
	  guaranteed not to write stale data past the new EOF on truncate down.
	
	  We are going to log the inode size change in this transaction so
	  any previous writes that are beyond the on disk EOF and the new
	  EOF that have not been written out need to be written here.  If we
	  do not write the data out, we expose ourselves to the null files
	  problem. Note that this includes any block zeroing we did above;
	  otherwise those blocks may not be zeroed after a crash.
	
	  Only change the cmtime if we are changing the size or we are
	  explicitly asked to change it.  This handles the semantic difference
	  between truncate() and ftruncate() as implemented in the VFS.
	 
	  The regular truncate() case without ATTR_CTIME and ATTR_MTIME is a
	  special case where we need to update the times despite not having
	  these flags set.  For all other operations the VFS set these flags
	  explicitly if it wants a timestamp update.
	
	  The first thing we do is set the size to new_size permanently on
	  disk.  This way we don't have to worry about anyone ever being able
	  to look at the data being freed even in the face of a crash.
	  What we're getting around here is the case where we free a block, it
	  is allocated to another file, it is written to, and then we crash.
	  If the new data gets written to the file but the log buffers
	  containing the free and reallocation don't, then we'd end up with
	  garbage in the blocks being freed.  As long as we make the new size
	  permanent before actually freeing any blocks it doesn't matter if
	  they get written to.
		
		  Truncated "down", so we're removing references to old data
		  here - if we delay flushing for a long time, we expose
		  ourselves unduly to the notorious NULL files problem.  So,
		  we mark this inode and flush it when the file is closed,
		  and do not wait the usual (long) time for writeout.
 A truncate down always removes post-EOF blocks. 
 Capture the iversion update that just occurred 
	
	  Yes, XFS uses the same method for rmdir and unlink.
	 
	  There are some subtile differences deeper in the code,
	  but we use S_ISDIR to check for those.
	
	  Yes, XFS uses the same method for rmdir and unlink.
	 
	  There are some subtile differences deeper in the code,
	  but we use S_ISDIR to check for those.
 Figure out if this file actually supports DAX. 
 Only supported on regular files. 
 Only supported on non-reflinked files. 
 Block size must match page size 
 Device has to support DAX too. 
	
	  S_DAX can only be set during inode initialization and is never set by
	  the VFS, so we cannot mask off S_DAX in i_flags.
  Initialize the Linux inode.
  When reading existing inodes from disk this is called directly from xfs_iget,
  when creating a new inode it is called from xfs_ialloc after setting up the
  inode. These callers have different criteria for clearing XFS_INEW, so leave
  it up to the caller to deal with unlocking the inode appropriately.
 make the inode look hashed for the writeback code 
		
		  We set the i_rwsem class here to avoid potential races with
		  lockdep_annotate_inode_mutex_key() reinitialising the lock
		  after a filehandle lookup has already found the inode in
		  cache before it has been unlocked via unlock_new_inode().
	
	  Ensure all page cache allocations are done from GFP_NOFS context to
	  prevent direct reclaim recursion back into the filesystem and blowing
	  stacks or deadlocking.
	
	  If there is no attribute fork no ACL can exist on this inode,
	  and it can't have any file capabilities attached to it either.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  All Rights Reserved.
  Directory tree accounting is implemented using project quotas, where
  the project identifier is inherited from parent directories.
  A statvfs (df, etc.) of a directory that is using project quota should
  return a statvfs of the project, not the entire filesystem.
  This makes such trees appear as if they are filesystems in themselves.
	
	  If the device itself is read-only, we can't allow
	  the user to change the state of quota on the mount -
	  this would generate a transaction on the ro device,
	  which would lead to an IO error and shutdown
		
		  Call mount_quotas at this point only if we won't have to do
		  a quotacheck.
			
			  If an error occurred, qm_mount_quotas code
			  has already disabled quotas. So, just finish
			  mounting, and get on with the boring life
			  without disk quotas.
			
			  Clear the quota flags, but remember them. This
			  is so that the quota code doesn't get invoked
			  before we're ready. This can happen when an
			  inode goes inactive and wants to free blocks,
			  or via xfs_log_mount_finish.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2008, Christoph Hellwig
  All Rights Reserved.
  Locking scheme:
   - all ACL updates are protected by inode->i_mutex, which is taken before
     calling into this file.
		
		  The tag is 32 bits on disk and 16 bits in core.
		 
		  Because every access to it goes through the core
		  format first this is not a problem.
	
	  If the attribute doesn't exist make sure we have a negative cache
	  entry, for any other error assume it is transient.
	
	  If the attribute didn't exist to start with that's fine.
	
	  We set the mode after successfully updating the ACL xattr because the
	  xattr update can fail at ENOSPC and we don't want to change the mode
	  if the ACL update hasn't been applied.
  Invalidate any cached ACLs if the user has bypassed the ACL interface.
  We don't validate the content whatsoever so it is caller responsibility to
  provide data in valid format and ensure i_mode is consistent.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2019 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Warn about metadata corruption that we detected but haven't fixed, and
  make sure we're not sitting on anything that would get in the way of
  recovery.
 Measure AG corruption levels. 
 Measure realtime volume corruption levels. 
	
	  Measure fs corruption and keep the sample around for the warning.
	  See the note below for why we exempt FS_COUNTERS.
		
		  We discovered uncorrected metadata problems at some point
		  during this filesystem mount and have advised the
		  administrator to run repair once the unmount completes.
		 
		  However, we must be careful -- when FSCOUNTERS are flagged
		  unhealthy, the unmount procedure omits writing the clean
		  unmount record to the log so that the next mount will run
		  recovery and recompute the summary counters.  In other
		  words, we leave a dirty log to get the counters fixed.
		 
		  Unfortunately, xfs_repair cannot recover dirty logs, so if
		  there were filesystem problems, FSCOUNTERS was flagged, and
		  the administrator takes our advice to run xfs_repair,
		  they'll have to zap the log before repairing structures.
		  We don't really want to encourage this, so we mark the
		  FSCOUNTERS healthy so that a subsequent repair run won't see
		  a dirty log.
 Mark unhealthy per-fs metadata. 
 Mark a per-fs metadata healed. 
 Sample which per-fs metadata are unhealthy. 
 Mark unhealthy realtime metadata. 
 Mark a realtime metadata healed. 
 Sample which realtime metadata are unhealthy. 
 Mark unhealthy per-ag metadata. 
 Mark per-ag metadata ok. 
 Sample which per-ag metadata are unhealthy. 
 Mark the unhealthy parts of an inode. 
	
	  Keep this inode around so we don't lose the sickness report.  Scrub
	  grabs inodes with DONTCACHE assuming that most inode are ok, which
	  is not the case here.
 Mark parts of an inode healed. 
 Sample which parts of an inode are unhealthy. 
 Mappings between internal sick masks and ioctl sick masks. 
 Fill out fs geometry health info. 
 Fill out ag geometry health info. 
 Fill out bulkstat health info. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2003,2005 Silicon Graphics, Inc.
  All Rights Reserved.
 we print both series of quota information together 
 Loop over all stats groups 
 inner loop does each group 
 extra precision counters 
 save vn_active, it's a universal truth! 
 legacy quota interfaces 
 maximum; incore; ratio free to inuse; freelist 
 legacy quota stats interface no 2 
 CONFIG_XFS_QUOTA 
 CONFIG_PROC_FS 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2004-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Note that we only accept fileids which are long enough rather than allow
  the parent generation number to default to zero.  XFS considers zero a
  valid generation number not an invalidwildcard value.
 Directories don't need their parent encoded, they have ".." 
	
	  If the filesystem may contain 64bit inode numbers, we need
	  to use larger file handles that can represent them.
	 
	  While we only allocate inodes that do not fit into 32 bits any
	  large enough filesystem may contain them, thus the slightly
	  confusing looking conditional below.
	
	  Only encode if there is enough space given.  In practice
	  this means we can't export a filesystem with 64bit inodes
	  over NFSv2 with the subtree_check export option; the other
	  seven combinations work.  The real answer is "don't use v2".
	
	  NFS can sometimes send requests for ino 0.  Fail them gracefully.
	
	  The XFS_IGET_UNTRUSTED means that an invalid inode number is just
	  fine and not an indication of a corrupted filesystem as clients can
	  send invalid file handles and we have to handle it gracefully..
		
		  EINVAL means the inode cluster doesn't exist anymore.
		  EFSCORRUPTED means the metadata pointing to the inode cluster
		  or the inode cluster itself is corrupt.  This implies the
		  filehandle is stale, so we should translate it here.
		  We don't use ESTALE directly down the chain to not
		  confuse applications using bulkstat that expect EINVAL.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Bulk Stat
  =========
  Use the inode walking functions to fill out struct xfs_bulkstat for every
  allocated inode, then pass the stat information to some externally provided
  iteration function.
  Fill out the bulkstat info for a single inode and report it somewhere.
  bc->breq->lastino is effectively the inode cursor as we walk through the
  filesystem.  Therefore, we update it any time we need to move the cursor
  forward, regardless of whether or not we're sending any bstat information
  back to userspace.  If the inode is internal metadata or, has been freed
  out from under us, we just simply keep going.
  However, if any other type of error happens we want to stop right where we
  are so that userspace will call back with exact number of the bad inode and
  we can send back an error code.
  Note that if the formatter tells us there's no space left in the buffer we
  move the cursor forward and abort the walk.
 incore inode pointer 
	 xfs_iget returns the following without needing
	  further change.
	
	  Advance the cursor to the inode that comes after the one we just
	  looked at.  We want the caller to move along if the bulkstat
	  information was copied successfully; if we tried to grab the inode
	  but it's no longer allocated; or if it's internal metadata.
 Bulkstat a single inode. 
	
	  Grab an empty transaction so that we can use its recursive buffer
	  locking abilities to detect cycles in the inobt without deadlocking.
	
	  If we reported one inode to userspace then we abort because we hit
	  the end of the buffer.  Don't leak that back to userspace.
 bulkstat just skips over missing inodes 
  Check the incoming lastino parameter.
  We allow any inode value that could map to physical space inside the
  filesystem because if there are no inodes there, bulkstat moves on to the
  next chunk.  In other words, the magic agino value of zero takes us to the
  first chunk in the AG, and an agino value past the end of the AG takes us to
  the first chunk in the next AG.
  Therefore we can end early if the requested inode is beyond the end of the
  filesystem or doesn't map properly.
 Return stat information in bulk (by-inode) for the filesystem. 
	
	  Grab an empty transaction so that we can use its recursive buffer
	  locking abilities to detect cycles in the inobt without deadlocking.
	
	  We found some inodes, so clear the error status and return them.
	  The lastino pointer will point directly at the inode that triggered
	  any error that occurred, so on the next call the error will be
	  triggered again and propagated to userspace as there will be no
	  formatted inodes in the buffer.
 Convert bulkstat (v5) to bstat (v1). 
 memset is needed here because of padding holes in the structure. 
  INUMBERS
  ========
  This is how we export inode btree records to userspace, so that XFS tools
  can figure out where inodes are allocated.
  Format the inode group structure and report it somewhere.
  Similar to xfs_bulkstat_one_int, lastino is the inode cursor as we walk
  through the filesystem so we move it forward unless there was a runtime
  error.  If the formatter tells us the buffer is now full we also move the
  cursor forward and abort the walk.
  Return inode number table for the filesystem.
	
	  Grab an empty transaction so that we can use its recursive buffer
	  locking abilities to detect cycles in the inobt without deadlocking.
	
	  We found some inode groups, so clear the error status and return
	  them.  The lastino pointer will point directly at the inode that
	  triggered any error that occurred, so on the next call the error
	  will be triggered again and propagated to userspace as there will be
	  no formatted inode groups in the buffer.
 Convert an inumbers (v5) struct to a inogrp (v1) struct. 
 memset is needed here because of padding holes in the structure. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  Copyright (c) 2013 Red Hat, Inc.
  All Rights Reserved.
  Copy out entries of shortform attribute lists for attr_list().
  Shortform attribute lists are not stored in hashval sorted order.
  If the output buffer is not large enough to hold them all, then
  we have to calculate each entries' hashvalue and sort them before
  we can begin returning them to the user.
	
	  If the buffer is large enough and the cursor is at the start,
	  do not bother with sorting since we will return everything in
	  one buffer and another call using the cursor won't need to be
	  made.
	  Note the generous fudge factor of 16 overhead bytes per entry.
	  If bufsize is zero then put_listent must be a search function
	  and can just scan through what we have.
			
			  Either search callback finished early or
			  didn't fit it all in the buffer after all.
 do no more for a search callback 
	
	  It didn't all fit, so we have to sort everything on hashval.
	
	  Scan the attribute list for the rest of the entries, storing
	  the relevant info from only those that match into a buffer.
 These are bytes, and both on-disk, don't endian-flip 
	
	  Sort the entries on hash then entno.
	
	  Re-find our place IN THE SORTED LIST.
	
	  Loop putting entries into the user buffer.
  We didn't find the block & hash mentioned in the cursor state, so
  walk down the attr btree looking for the hash.
 Tree taller than we can handle; bail out! 
 Check the level from the root node. 
 We can't point back to the root. 
	
	  Do all sorts of validation on the passed-in cursor structure.
	  If anything is amiss, ignore the cursor and look up the hashval
	  starting from the btree root.
	
	  We did not find what we expected given the cursor's contents,
	  so we start from the top and work down based on the hash value.
	  Note that start of node block is same as start of leaf block.
	
	  Roll upward through the blocks, processing each leaf block in
	  order.  As long as there is space in the result buffer, keep
	  adding the information.
  Copy out attribute list entries for attr_list(), for leaf attribute lists.
	
	  Re-find our place in the leaf block if this is a new syscall.
	
	  We have found our place, start copying out the new attributes.
  Copy out attribute entries for attr_list(), for leaf attribute lists.
	
	  Decide on what work routines to call based on the inode size.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2001,2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Freeing the efi requires that we remove it from the AIL if it has already
  been placed there. However, the EFI may not yet have been placed in the AIL
  when called by xfs_efi_release() from EFD processing due to the ordering of
  committed vs unpin operations in bulk insert operations. Hence the reference
  count to ensure only the last caller frees the EFI.
  This returns the number of iovecs needed to log the given efi item.
  We only need 1 iovec for an efi item.  It just logs the efi_log_format
  structure.
  This is called to fill in the vector of log iovecs for the
  given efi log item. We use only 1 iovec, and we point that
  at the efi_log_format structure embedded in the efi item.
  It is at this point that we assert that all of the extent
  slots in the efi item have been filled.
  The unpin operation is the last place an EFI is manipulated in the log. It is
  either inserted in the AIL or aborted in the event of a log IO error. In
  either case, the EFI transaction has been successfully committed to make it
  this far. Therefore, we expect whoever committed the EFI to either construct
  and commit the EFD or drop the EFD's reference in the event of error. Simply
  drop the log's EFI reference now that the log is done with it.
  The EFI has been either committed or aborted if the transaction has been
  cancelled. If the transaction was cancelled, an EFD isn't going to be
  constructed and thus we free the EFI here directly.
  Allocate and initialize an efi item with the given number of extents.
  Copy an EFI format buffer from the given buf, and into the destination
  EFI format structure.
  The given buffer can be in 32 bit or 64 bit form (which has different padding),
  one of which will be the native format for this kernel.
  It will handle the conversion of formats if necessary.
  This returns the number of iovecs needed to log the given efd item.
  We only need 1 iovec for an efd item.  It just logs the efd_log_format
  structure.
  This is called to fill in the vector of log iovecs for the
  given efd log item. We use only 1 iovec, and we point that
  at the efd_log_format structure embedded in the efd item.
  It is at this point that we assert that all of the extent
  slots in the efd item have been filled.
  The EFD is either committed or aborted if the transaction is cancelled. If
  the transaction is cancelled, drop our reference to the EFI and free the EFD.
  Allocate an "extent free done" log item that will hold nextents worth of
  extents.  The caller must use all nextents extents, because we are not
  flexible about this at all.
  Free an extent and log it to the EFD. Note that the transaction is marked
  dirty regardless of whether the extent free succeeds or fails to support the
  EFIEFD lifecycle rules.
	
	  Mark the transaction dirty, even on error. This ensures the
	  transaction is aborted, which:
	 
	  1.) releases the EFI and frees the EFD
	  2.) shuts down the filesystem
 Sort bmap items by AG. 
 Log a free extent to the intent item. 
	
	  atomic_inc_return gives us the value after the increment;
	  we want to use it as an array index so we need to subtract 1 from
	  it.
 Get an EFD so we can process all the free extents. 
 Process a free extent. 
 Abort all pending EFIs. 
 Cancel a free extent. 
  AGFL blocks are accounted differently in the reserve pools and are not
  inserted into the busy extent list.
	
	  Mark the transaction dirty, even on error. This ensures the
	  transaction is aborted, which:
	 
	  1.) releases the EFI and frees the EFD
	  2.) shuts down the filesystem
 sub-type with special handling for AGFL deferred frees 
 Is this recovered EFI ok? 
  Process an extent free intent item that was recovered from
  the log.  We need to free the extents that it describes.
	
	  First check the validity of the extents described by the
	  EFI.  If any are bad, then assume that all are bad and
	  just toss the EFI.
 Relog an intent item to push the log tail forward. 
  This routine is called to create an in-core extent free intent
  item from the efi format structure which was logged on disk.
  It allocates an in-core efi, copies the extents from the format
  structure into it, and adds the efi to the AIL with the given
  LSN.
	
	  Insert the intent into the AIL directly and drop one reference so
	  that finishing or canceling the work will drop the other.
  This routine is called when an EFD format structure is found in a committed
  transaction in the log. Its purpose is to cancel the corresponding EFI if it
  was still in the log. To do this it searches the AIL for the EFI with an id
  equal to that in the EFD format structure. If we find it we drop the EFD
  reference, which removes the EFI from the AIL and frees it.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2001-2005 Silicon Graphics, Inc.
  All Rights Reserved.
 CONFIG_PROC_FS 
 please keep this the last entry 
 CONFIG_PROC_FS 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  All Rights Reserved.
 top-level xfs sysfs dir 
 global debug sysfs attrs 
 !CONFIG_HOTPLUG_CPU 
  Table driven mount option parser.
 the few simple ones we can get from the mount struct 
  Set parameters for inode allocation heuristics, taking into account
  filesystem size and inode32inode64 mount options; i.e. specifically
  whether or not XFS_FEAT_SMALL_INUMS is set.
  Inode allocation patterns are altered only if inode32 is requested
  (XFS_FEAT_SMALL_INUMS), and the filesystem is sufficiently large.
  If altered, XFS_OPSTATE_INODE32 is set as well.
  An agcount independent of that in the mount structure is provided
  because in the growfs case, mp->m_sb.sb_agcount is not yet updated
  to the potentially higher ag count.
  Returns the maximum AG index which may contain inodes.
	
	  Calculate how much should be reserved for inodes to meet
	  the max inode percentage.  Used only for inode32.
 Get the last possible inode in the filesystem 
	
	  If user asked for no more than 32-bit inodes, and the fs is
	  sufficiently large, set XFS_OPSTATE_INODE32 if we must alter
	  the allocator to accommodate the request.
  The file system configurations are:
 	(1) device (partition) with data and internal log
 	(2) logical volume with data and log subvolumes.
 	(3) logical volume with data, log, and realtime subvolumes.
  We only have to handle opening the log and realtime volumes here if
  they are present.  The data subvolume has already been opened by
  get_sb_bdev() and is stored in sb->s_bdev.
	
	  Open real time and log devices - order is important.
	
	  Setup xfs_mount buffer target pointers
  Setup xfs_mount buffer target pointers based on superblock
  Flush all dirty data to disk. Must not be called while holding an XFS_ILOCK
  or a page lock. We use sync_inodes_sb() here to ensure we block while waiting
  for IO to complete so that we effectively throttle multiple callers to the
  rate at which IO is completing.
	
	  If flush_work() returns true then that means we waited for a flush
	  which was already in progress.  Don't bother running another scan.
 Catch misguided souls that try to use this interface on XFS 
  Now that the generic code is guaranteed not to be accessing
  the linux inode, we can inactivate and reclaim the inode.
  Slab object creation initialisation for the XFS inode.
  This covers only the idempotent fields in the XFS inode;
  all other fields need to be initialised on allocation
  from the slab. This avoids the need to repeatedly initialise
  fields in the xfs inode that left in the initialise state
  when freeing the inode.
 vfs inode 
 xfs inode 
  We do an unlocked check for XFS_IDONTCACHE here because we are already
  serialised against cache hits here via the inode->i_lock and igrab() in
  xfs_iget_cache_hit(). Hence a lookup that might clear this flag will not be
  racing with us, and it avoids needing to grab a spinlock here for every inode
  we drop the final reference on.
	
	  If this unlinked inode is in the middle of recovery, don't
	  drop the inode just yet; log recovery will take care of
	  that.  See the comment for this inode flag.
	
	  Doing anything during the async pass would be counterproductive.
		
		  The disk must be active because we're syncing.
		  We schedule log work now (now that the disk is
		  active) instead of later (when it might not be).
	
	  If we are called with page faults frozen out, it means we are about
	  to freeze the transaction subsystem. Take the opportunity to shut
	  down inodegc because once SB_FREEZE_FS is set it's too late to
	  prevent inactivation races with freeze. The fs doesn't get called
	  again by the freezing process until after SB_FREEZE_FS has been set,
	  so it's now or never.  Same logic applies to speculative allocation
	  garbage collection.
	 
	  We don't care if this is a normal syncfs call that does this or
	  freeze that does this - we can run this multiple times without issue
	  and we won't race with a restart because a restart can only occur
	  when the state is either SB_FREEZE_FS or SB_FREEZE_COMPLETE.
 Wait for whatever inactivations are in progress. 
 make sure statp->f_bfree does not underflow 
 If sb_icount overshot maxicount, report actual allocation 
 make sure statp->f_ffree does not underflow 
  Second stage of a freeze. The data is already frozen so we only
  need to take care of the metadata. Once that's done sync the superblock
  to the log to dirty it in case of a crash while frozen. This ensures that we
  will recover the unlinked inode lists on the next mount.
	
	  The filesystem is now frozen far enough that memory reclaim
	  cannot safely operate on the filesystem. Hence we need to
	  set a GFP_NOFS context here to avoid recursion deadlocks.
	
	  For read-write filesystems, we need to restart the inodegc on error
	  because we stopped it at SB_FREEZE_PAGEFAULT level and a thaw is not
	  going to be run to restart it now.  We are at SB_FREEZE_FS level
	  here, so we can restart safely without racing with a stop in
	  xfs_fs_sync_fs().
	
	  Don't reactivate the inodegc worker on a readonly filesystem because
	  inodes are sent directly to reclaim.  Don't reactivate the blockgc
	  worker because there are no speculative preallocations on a readonly
	  filesystem.
  This function fills in xfs_mount_t fields based on mount args.
  Note: the superblock _has_ now been read in.
 Fail a mount where the logbuf is smaller than the log stripe 
 Fail a mount if the logbuf is larger than 32K 
	
	  V5 filesystems always use attr2 format for attributes.
	
	  prohibit rw mounts of read-only filesystems
 if ->fill_super failed, we have no mount to tear down 
 Paranoia: catch incorrect calls during mount setup or teardown 
	 Don't print the warning if reconfiguring and current mount point
	  already had the flag set
  Set mount state from a mount option.
  NOTE: mp->m_super is NULL here!
 Following mount options will be removed in September 2025 
 No recovery flag requires a read-only mount 
	
	  We have not read the superblock at this point, so only the attr2
	  mount option can set the attr2 feature by this stage.
	
	  Delay mount work if the debug hook is set. This is debug
	  instrumention to coordinate simulation of xfs mount failures with
	  VFS superblock operations
	
	  All percpu data structures requiring cleanup when a cpu goes offline
	  must be allocated before adding this @mp to the cpu-dead handler's
	  mount list.
 Allocate stats memory before we do operations that might use it 
 V4 support is undergoing deprecation. 
 Filesystem claims it needs repair, so refuse the mount. 
	
	  Don't touch the filesystem if a user tool thinks it owns the primary
	  superblock.  mkfs doesn't clear the flag from secondary supers, so
	  we don't check them at all.
	
	  Until this is fixed only page-sized or smaller data blocks work.
 Ensure this filesystem fits in the page cache limits 
	
	  XFS block mappings use 54 bits to store the logical block offset.
	  This should suffice to handle the maximum file size that the VFS
	  supports (currently 2^63 bytes on 64-bit and ULONG_MAX << PAGE_SHIFT
	  bytes on 32-bit), but as XFS and VFS have gotten the s_maxbytes
	  calculation wrong on 32-bit kernels in the past, we'll add a WARN_ON
	  to check this assertion.
	 
	  Avoid integer overflow by comparing the maximum bmbt offset to the
	  maximum pagecache offset in units of fs blocks.
	
	  we must configure the block size in the superblock before we run the
	  full mount process as the mount process can lookup and cache inodes.
 version 5 superblocks support inode version counters. 
	
	  If this is the first remount to writeable state we might have some
	  superblock changes to update.
	
	  Fill out the reserve pool if it is empty. Use the stashed value if
	  it is non-zero, otherwise go with the default.
 Recover any CoW blocks that never got remapped. 
 Create the per-AG metadata reservation pool .
 Re-enable the background inode inactivation worker. 
	
	  Cancel background eofb scanning so it cannot race with the final
	  log force+buftarg wait and deadlock the remount.
 Get rid of any leftover CoW reservations... 
	
	  Stop the inodegc background worker.  xfs_fs_reconfigure already
	  flushed all pending inodegc work when it sync'd the filesystem.
	  The VFS holds s_umount, so we know that inodes cannot enter
	  xfs_fs_destroy_inode during a remount operation.  In readonly mode
	  we send inodes straight to reclaim, so no inodes will be queued.
 Free the per-AG metadata reservation pool. 
	
	  Before we sync the metadata, we need to free up the reserve block
	  pool so that the used block count in the superblock on disk is
	  correct at the end of the remount. Stash the current reserve pool
	  size so that if we get remounted rw, we can return it to the same
	  size.
  Logically we would return an error here to prevent users from believing
  they might have changed mount options using remount which can't be changed.
  But unfortunately mount(8) adds all options from mtab and fstab to the mount
  arguments in some cases so we can't blindly reject options, but have to
  check for each specified option if it actually differs from the currently
  set option and only reject it if that's the case.
  Until that is implemented we return success for every remount request, and
  silently ignore all options that we can't actually change.
 version 5 superblocks always support version counters. 
 inode32 -> inode64 
 inode64 -> inode32 
 ro -> rw 
 rw -> ro 
	
	  mp is stored in the fs_context when it is initialized.
	  mp is transferred to the superblock on a successful mount,
	  but if an error occurs before the transfer we have to free
	  it here.
	
	  We don't create the finobt per-ag space reservation until after log
	  recovery, so we must set this to true so that an ifree transaction
	  started during log recovery will not depend on space reservations
	  for finobt expansion.
	
	  These can be overridden by the mount option parsing.
 64k 
	
	  Copy binary VFS mount flags we are interested in.
	
	  The size of the cache-allocated buf log item is the maximum
	  size possible under XFS.  This wastes a little bit of memory,
	  but it is much faster.
	
	  Make sure all delayed rcu free are flushed before we
	  destroy caches.
	
	  The allocation workqueue can be used in memory reclaim situations
	  (writepage path), and parallelism is only limited by the number of
	  AGs in all the filesystems mounted. Hence use the default large
	  max_active value for this workqueue.
 !CONFIG_HOTPLUG_CPU 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  See if the UUID is unique among mounted XFS filesystems.
  Mount fails if UUID is nil or a FS with the same UUID is already mounted.
 Publish UUID in struct super_block 
  Check size of device based on the (datarealtime) block count.
  Note: this check is used by the growfs code as well as mount.
 Limited by ULONG_MAX of page cache index 
  xfs_readsb
  Does the initial read of the superblock.
	
	  For the initial read, we must guess at the sector
	  size based on the block device.  It's enough to
	  get the sb_sectsize out of the superblock and
	  then reread with the proper length.
	  We don't verify it yet, because it may not be complete.
	
	  Allocate a (locked) buffer to hold the superblock. This will be kept
	  around at all times to optimize access to the superblock. Therefore,
	  set XBF_NO_IOACCT to make sure it doesn't hold the buftarg count
	  elevated.
 bad CRC means corrupted metadata 
	
	  Initialize the mount structure from the superblock.
	
	  If we haven't validated the superblock, do so now before we try
	  to check the sector size and reread the superblock appropriately.
	
	  We must be able to do sector-sized and sector-aligned IO.
		
		  Re-read the superblock so the buffer is correctly sized,
		  and properly verified.
 no need to be quiet anymore, so reset the buf ops 
  If the sunitswidth change would move the precomputed root inode value, we
  must reject the ondisk change because repair will stumble over that.
  However, we allow the mount to proceed because we never rejected this
  combination before.  Returns true to update the sb, false otherwise.
	
	  XXX: Next time we add a new incompat feature, this should start
	  returning -EINVAL to fail the mount.  Until then, spit out a warning
	  that we're ignoring the administrator's instructions.
  If we were provided with new sunitswidth values as mount options, make sure
  that they pass basic alignment and superblock feature checks, and convert
  them into the same units (FSB) that everything else expects.  This step
  must be done before computing the inode geometry.
	
	  If stripe unit and stripe width are not multiples
	  of the fs blocksize turn off alignment.
		
		  Convert the stripe unit and width to FSBs.
 Update alignment values based on mount options and sb values. 
  precalculate the low space thresholds for dynamic speculative preallocation.
  Check that the data (and log if separate) is an ok size.
  Clear the quotaflags in memory and in the superblock.
 It is OK to look at sb_qflags in the mount path without m_sb_lock. 
	
	  We default to 5% or 8192 fsbs of space reserved, whichever is
	  smaller.  This is intended to cover concurrent allocation
	  transactions when we initially hit enospc. These each require a 4
	  block reservation. Hence by default we cover roughly 2000 concurrent
	  allocation reservations.
 Ensure the summary counts are correct. 
	
	  The AG0 superblock verifier rejects in-progress filesystems,
	  so we should never see the flag set this far into mounting.
	
	  Now the log is mounted, we know if it was an unclean shutdown or
	  not. If it was, with the first phase of recovery has completed, we
	  have consistent AG blocks on disk. We have not recovered EFIs yet,
	  but they are recovered transactionally in the second recovery phase
	  later.
	 
	  If the log was clean when we mounted, we can check the summary
	  counters.  If any of them are obviously incorrect, we can recompute
	  them from the AGF headers in the next step.
	
	  We can safely re-initialise incore superblock counters from the
	  per-ag data. These may not be correct if the filesystem was not
	  cleanly unmounted, so we waited for recovery to finish before doing
	  this.
	 
	  If the filesystem was cleanly unmounted or the previous check did
	  not flag anything weird, then we can trust the values in the
	  superblock to be correct and we don't need to do anything here.
	  Otherwise, recalculate the summary counters.
  Flush and reclaim dirty inodes in preparation for unmount. Inodes and
  internal inode structures can be sitting in the CIL and AIL at this point,
  so we need to unpin them, write them back andor reclaim them before unmount
  can proceed.  In other words, callers are required to have inactivated all
  inodes.
  An inode cluster that has been freed can have its buffer still pinned in
  memory because the transaction is still sitting in a iclog. The stale inodes
  on that buffer will be pinned to the buffer until the transaction hits the
  disk and the callbacks run. Pushing the AIL will skip the stale inodes and
  may never see the pinned buffer, so nothing will push out the iclog and
  unpin the buffer.
  Hence we need to force the log to unpin everything first. However, log
  forces don't wait for the discards they issue to complete, so we have to
  explicitly wait for them to complete here as well.
  Then we can tell the world we are unmounting so that error handling knows
  that the filesystem is going away and we should error out anything that we
  have been retrying in the background.  This will prevent never-ending
  retries in AIL pushing from hanging the unmount.
  Finally, we can push the AIL to clean all the remaining dirty objects, then
  reclaim the remaining inodes that are still in memory at this point in time.
 Compute maximum possible height for per-AG btree types for this fs. 
  This function does the following on an initial mount of a file system:
 	- reads the superblock from disk and init the mount struct
 	- if we're a 32-bit kernel, do a size check on the superblock
 		so we don't mount terabyte filesystems
 	- init mount struct realtime fields
 	- allocate inode hash table for fs
 	- init directory manager
 	- perform recovery and init the log manager
	
	  Check for a mismatched features2 values.  Older kernels read & wrote
	  into the wrong sb offset for sb_features2 on some platforms due to
	  xfs_sb_t not being 64bit size aligned when sb_features2 was added,
	  which made older superblock readingwriting routines swap it as a
	  64-bit value.
	 
	  For backwards compatibility, we make both slots equal.
	 
	  If we detect a mismatched field, we OR the set bits into the existing
	  features2 field in case it has already been modified; we don't want
	  to lose any features.  We then update the bad location with the ORed
	  value so that older kernels will see any features2 flags. The
	  superblock writeback code ensures the new sb_features2 is copied to
	  sb_bad_features2 before it is logged or written to disk.
 always use v2 inodes by default now 
	
	  If we were given new sunitswidth options, do some basic validation
	  checks and convert the incore dalign and swidth values to the
	  same units (FSB) that everything else uses.  This must happen
	  before computing the inode geometry.
	
	  Check if sb_agblocks is aligned at stripe boundary.  If sb_agblocks
	  is NOT aligned turn off m_dalign since allocator alignment is within
	  an ag, therefore ag has to be aligned at stripe boundary.  Note that
	  we must compute the free space and rmap btree geometry before doing
	  this.
 enable fail_at_unmount as default 
	
	  Update the preferred write size based on the information from the
	  on-disk superblock.
 set the low space thresholds for dynamic preallocation 
	
	  If enabled, sparse inode chunk alignment is expected to match the
	  cluster size. Full inode chunk alignment must match the chunk size,
	  but that is checked on sb read verification...
	
	  Check that the data (and log if separate) is an ok size.
	
	  Initialize realtime fields in the mount structure
	
	   Copies the low order bits of the timestamp and the randomly
	   set "sequence" number out of a UUID.
	
	  Initialize the precomputed transaction reservations values.
	
	  Allocate and initialize the per-ag data.
	
	  Log's mount-time initialization. The first part of recovery can place
	  some items on the AIL, to be handled when recovery is finished or
	  cancelled.
 Make sure the summary counts are ok. 
 Enable background inode inactivation workers. 
	
	  Now that we've recovered any pending superblock feature bit
	  additions, we can finish setting up the attr2 behaviour for the
	  mount. The noattr2 option overrides the superblock flag, so only
	  check the superblock feature flag if the mount option is not set.
	
	  Get and sanity-check the root inode.
	  Save the pointer to it in the mount structure.
 save it 
	
	  Initialize realtime inode pointers in the mount structure
		
		  Free up the root inode.
	
	  If this is a read-only mount defer the superblock updates until
	  the next remount into writeable mode.  Otherwise we would never
	  perform the update e.g. for the root filesystem.
	
	  Initialise the XFS quota management subsystem for this mount
		
		  If a file system had quotas running earlier, but decided to
		  mount without -o uquotapquotagquota options, revoke the
		  quotachecked license.
	
	  Finish recovering the file system.  This part needed to be delayed
	  until after the root and real-time bitmap inodes were consistently
	  read in.  Temporarily create per-AG space reservations for metadata
	  btree shape changes because space freeing transactions (for inode
	  inactivation) require the per-AG reservation in lieu of reserving
	  blocks.
	
	  Now the log is fully replayed, we can transition to full read-only
	  mode for read-only mounts. This will sync all the metadata and clean
	  the log so that the recovery we just performed does not have to be
	  replayed again on the next mount.
	 
	  We use the same quiesce mechanism as the rw->ro remount, as they are
	  semantically identical operations.
	
	  Complete the quota initialisation, post-log-replay component.
	
	  Now we are mounted, reserve a small amount of unused space for
	  privileged transactions. This is needed so that transaction
	  space required for critical operations can dip into this pool
	  when at ENOSPC. This is needed for operations like create with
	  attr, unwritten extent conversion at ENOSPC, etc. Data allocations
	  are not allowed to use this reserved space.
	 
	  This may drive us straight to ENOSPC on mount, but that implies
	  we were already there on the last unmount. Warn if this occurs.
 Recover any CoW blocks that never got remapped. 
 Reserve AG blocks for future btree expansion. 
 Clean out dquots that might be in memory after quotacheck. 
	
	  Inactivate all inodes that might still be in memory after a log
	  intent recovery failure so that reclaim can free them.  Metadata
	  inodes and the root directory shouldn't need inactivation, but the
	  mount failed for some reason, so pull down all the state and flee.
	
	  Flush all inode reclamation work and flush the log.
	  We have to do this after rtunmount and qm_unmount because those
	  two will have scheduled delayed reclaim for the rtquota inodes.
	 
	  This is slightly different from the unmountfs call sequence
	  because we could be tearing down a partially set up mount.  In
	  particular, if log_mount_finish fails we bail out without calling
	  qm_unmount_quotas and therefore rely on qm_unmount to release the
	  quota inodes.
  This flushes out the inodes,dquots and the superblock, unmounts the
  log and makes sure that incore structures are freed.
	
	  Perform all on-disk metadata updates required to inactivate inodes
	  that the VFS evicted earlier in the unmount process.  Freeing inodes
	  and discarding CoW fork preallocations can cause shape changes to
	  the free inode and refcount btrees, respectively, so we must finish
	  this before we discard the metadata space reservations.  Metadata
	  inodes and the root directory do not require inactivation.
	
	  Unreserve any blocks we have so that when we unmount we don't account
	  the reserved free space as used. This is really only necessary for
	  lazy superblock counting because it trusts the incore superblock
	  counters to be absolutely correct on clean unmount.
	 
	  We don't bother correcting this elsewhere for lazy superblock
	  counting because on mount of an unclean filesystem we reconstruct the
	  correct counter value and this is irrelevant.
	 
	  For non-lazy counter filesystems, this doesn't matter at all because
	  we only every apply deltas to the superblock and hence the incore
	  value does not matter....
  Determine whether modifications can proceed. The caller specifies the minimum
  freeze level for which modifications should not be allowed. This allows
  certain operations to proceed while the freeze sequence is in progress, if
  necessary.
		
		  If the reserve pool is depleted, put blocks back into it
		  first. Most of the time the pool is full.
	
	  Taking blocks away, need to be more accurate the closer we
	  are to zero.
	 
	  If the counter has a value of less than 2  max batch size,
	  then make everything serialise as we are real close to
	  ENOSPC.
	
	  Set aside allocbt blocks because these blocks are tracked as free
	  space but not available for allocation. Technically this means that a
	  single reservation cannot consume all remaining free space, but the
	  ratio of allocbt blocks to usable free blocks should be rather small.
	  The tradeoff without this is that filesystems that maintain high
	  perag block reservations can over reserve physical block availability
	  and fail physical allocation, which leads to much more serious
	  problems (i.e. transaction abort, pagecache discards, etc.) than
	  slightly premature -ENOSPC.
 we had space! 
	
	  lock up the sb for dipping into reserves before releasing the space
	  that took us to ENOSPC.
  Used to free the superblock along various error paths.
  If the underlying (datalogrt) device is readonly, there are some
  operations that cannot proceed.
 Force the summary counters to be recalculated at next mount. 
  Enable a log incompat feature flag in the primary superblock.  The caller
  cannot have any other transactions in progress.
	
	  Force the log to disk and kick the background AIL thread to reduce
	  the chances that the bwrite will stall waiting for the AIL to unpin
	  the primary superblock buffer.  This isn't a data integrity
	  operation, so we don't need a synchronous push.
	
	  Lock the primary superblock buffer to serialize all callers that
	  are trying to set feature bits.
	
	  Write the primary superblock to disk immediately, because we need
	  the log_incompat bit to be set in the primary super now to protect
	  the log items that we're going to commit later.
	
	  Add the feature bits to the incore superblock before we unlock the
	  buffer.
 Log the superblock to disk. 
  Clear all the log incompat flags from the superblock.
  The caller cannot be in a transaction, must ensure that the log does not
  contain any log items protected by any log incompat bit, and must ensure
  that there are no other threads that depend on the state of the log incompat
  feature flags in the primary super.
  Returns true if the superblock is dirty.
	
	  Update the incore superblock.  We synchronize on the primary super
	  buffer lock to be consistent with the add function, though at least
	  in theory this shouldn't be necessary.
  Update the in-core delayed block counter.
  We prefer to update the counter without having to take a spinlock for every
  counter update (i.e. batching).  Each change to delayed allocation
  reservations can change can easily exceed the default percpu counter
  batching, so we use a larger batch factor here.
  Note that we don't currently have any callers requiring fast summation
  (e.g. percpu_counter_read) so we can use a big batch value here.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 Convert an xfs_fsmap to an fsmap. 
 Convert an fsmap to an xfs_fsmap. 
 Convert an fsmap owner into an rmapbt owner. 
 "lowest owner id possible" 
 "highest owner id possible" 
 not implemented 
 fall through 
 Convert an rmapbt owner into an fsmap owner. 
 "free" 
 getfsmap query state 
 mapping records 
 AGF, for refcount queries 
 AG info, if applicable 
 next daddr we expect 
 owner of holes 
 device id 
 low rmap key 
 high rmap key 
 last extent? 
 Associate a device with a getfsmap handler. 
 Compare two getfsmap device handlers. 
 Decide if this mapping is shared. 
 rt files will have no perag structure 
 Are there any shared blocks here? 
  Format a reverse mapping for getfsmap, having translated rm_startblock
  into the appropriate daddr units.
	
	  Filter out records that start before our startpoint, if the
	  caller requested that.
 Are we just counting mappings? 
	
	  If the record starts past the last physical block we saw,
	  then we've found a gap.  Report the gap as being owned by
	  whatever the caller specified is the missing owner.
 Fill out the extent we found 
 Transform a rmapbt irec into a fsmap 
 Transform a bnobt irec into a fsmap 
 "free" 
 Set rmap flags based on the getfsmap flags 
 Execute a getfsmap query against the log device. 
 Set up search keys 
 Fabricate an rmap entry for the external log device. 
 Transform a rtbitmap "record" into a fsmap 
 "free" 
 Execute a getfsmap query against the realtime device. 
 Set up search keys 
 Actually query the realtime bitmap. 
	
	  Set up query parameters to return free rtextents covering the range
	  we want.
	
	  Report any gaps at the end of the rtbitmap by simulating a null
	  rmap starting at the block after the end of the query range.
 Execute a getfsmap query against the realtime device rtbitmap. 
 CONFIG_XFS_RT 
 Execute a getfsmap query against the regular data device. 
	
	  Convert the fsmap lowhigh keys to AG based keys.  Initialize
	  low to the fsmap low key and max out the high key to the end
	  of the AG.
		
		  Set the AG high key from the fsmap high key if this
		  is the last AG that we're querying.
		
		  Set the AG low key to the start of the AG prior to
		  moving on to the next AG.
		
		  If this is the last AG, report any gap at the end of it
		  before we drop the reference to the perag when the loop
		  terminates.
 loop termination case 
 Actually query the rmap btree. 
 Report any gap at the end of the last AG. 
 Allocate cursor for this AG and query_range it. 
 Execute a getfsmap query against the regular data device rmapbt. 
 Actually query the bno btree. 
 Report any gap at the end of the last AG. 
 Allocate cursor for this AG and query_range it. 
 Execute a getfsmap query against the regular data device's bnobt. 
 Do we recognize the device? 
 Ensure that the low key is less than the high key. 
  There are only two devices if we didn't configure RT devices at build time.
 CONFIG_XFS_RT 
  Get filesystem's extents as described in head, and format for output. Fills
  in the supplied records array until there are no more reverse mappings to
  return or head.fmh_entries == head.fmh_count.  In the second case, this
  function returns -ECANCELED to indicate that more records would have been
  returned.
  Key to Confusion
  ----------------
  There are multiple levels of keys and counters at work here:
  xfs_fsmap_head.fmh_keys	-- low and high fsmap keys passed in;
  				   these reflect fs-wide sector addrs.
  dkeys			-- fmh_keys used to query each device;
  				   these are fmh_keys but w the low key
  				   bumped up by fmr_length.
  xfs_getfsmap_info.next_daddr	-- next disk addr we expect to see; this
 				   is how we detect gaps in the fsmap
				   records and report them.
  xfs_getfsmap_info.lowhigh	-- per-AG lowhigh keys computed from
  				   dkeys; used to query the metadata.
 per-dev keys 
 Set up our device handlers. 
 CONFIG_XFS_RT 
	
	  To continue where we left off, we allow userspace to use the
	  last mapping from a previous call as the low key of the next.
	  This is identified by a non-zero length in the low key. We
	  have to increment the low key in this scenario to ensure we
	  don't return the same mapping again, and instead return the
	  very next mapping.
	 
	  If the low key mapping refers to file data, the same physical
	  blocks could be mapped to several other filesoffsets.
	  According to rmapbt record ordering, the minimal next
	  possible record for the block range is the next starting
	  offset in the same inode. Therefore, bump the file offset to
	  continue the search appropriately.  For all other low key
	  mapping types (attr blocks, metadata), bump the physical
	  offset as there can be no other mapping for the same physical
	  block range.
 For each device we support... 
 Is this device within the range the user asked for? 
		
		  If this device number matches the high key, we have
		  to pass the high key to the handler to limit the
		  query results.  If the device number exceeds the
		  low key, zero out the low key so that we get
		  everything from the beginning.
		
		  Grab an empty transaction so that we can use its recursive
		  buffer locking abilities to detect cycles in the rmapbt
		  without deadlocking.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Set us up to scrub inode btrees.
  If we detect a discrepancy between the inobt and the inode,
  try again after forcing logged inode cores out to disk.
 Inode btree scrubber. 
 Number of inodes we see while scanning inobt. 
 Expected next startino, for big block filesystems. 
 Expected end of the current inode cluster. 
  If we're checking the finobt, cross-reference with the inobt.
  Otherwise we're checking the inobt; if there is an finobt, make sure
  we have a record or not depending on freecount.
 Cross-reference with the other btrees. 
 Is this chunk worth checking? 
 Count the number of free inodes. 
  Check that an inode's allocation status matches ir_free in the inobt
  record.  First we try querying the in-core inode state, and if the inode
  isn't loaded we examine the on-disk inode directly.
  Since there can be 1:M and M:1 mappings between inobt records and inode
  clusters, we pass in the inode location information as an inobt record;
  the index of an inode cluster within the inobt record (as well as the
  cluster buffer itself); and the index of the inode within the cluster.
  @irec is the inobt record.
  @irec_ino is the inode offset from the start of the record.
  @dip is the on-disk inode.
	
	  Given an inobt record and the offset of an inode from the start of
	  the record, compute which fs inode we're talking about.
 Not cached, just read the disk buffer 
		
		  Inode is only half assembled, or there was an IO error,
		  or the verifier failed, so don't bother trying to check.
		  The inode scrubber can deal with this.
 Inode is all there. 
  Check that the holemask and freemask of a hypothetical inode cluster match
  what's actually on disk.  If sparse inodes are enabled, the cluster does
  not actually have to map to inodes if the corresponding holemask bit is set.
  @cluster_base is the first inode in the cluster within the @irec.
 Map this inode cluster 
 Compute a bitmask for this cluster that can be used for holemask. 
	
	  Map the first inode of this cluster to a buffer and offset.
	  Be careful about inobt records that don't align with the start of
	  the inode buffer when block sizes are large enough to hold multiple
	  inode chunks.  When this happens, cluster_base will be zero but
	  ir_startino can be large enough to make im_boffset nonzero.
 The whole cluster must be a hole or not a hole. 
 If any part of this is a hole, skip it. 
 Grab the inode cluster buffer. 
 Check free status of each inode within this cluster. 
  For all the inode clusters that could map to this inobt record, make sure
  that the holemask makes sense and that the allocation status of each inode
  matches the freemask.
	
	  For the common case where this inobt record maps to multiple inode
	  clusters this will call _check_cluster for each cluster.
	 
	  For the case that multiple inobt records map to a single cluster,
	  this will call _check_cluster once.
  Make sure this inode btree record is aligned properly.  Because a fs block
  contains multiple inodes, we check that the inobt record is aligned to the
  correct inode, not just the correct block on disk.  This results in a finer
  grained corruption check.
	
	  finobt records have different positioning requirements than inobt
	  records: each finobt record must have a corresponding inobt record.
	  That is checked in the xref function, so for now we only catch the
	  obvious case where the record isn't at all aligned properly.
	 
	  Note that if a fs block contains more than a single chunk of inodes,
	  we will have finobt records only for those chunks containing free
	  inodes, and therefore expect chunk alignment of finobt records.
	  Otherwise, we expect that the finobt record is aligned to the
	  cluster alignment as told by the superblock.
		
		  We're midway through a cluster of inodes that is mapped by
		  multiple inobt records.  Did we get the record for the next
		  irec in the sequence?
 Are we done with the cluster? 
 inobt records must be aligned to cluster and inoalignmnt size. 
	
	  If this is the start of an inode cluster that can be mapped by
	  multiple inobt records, the next inobt record must follow exactly
	  after this one.
 Scrub an inobtfinobt record. 
 Record has to be properly aligned within the AG. 
 Handle non-sparse inodes 
 Check each chunk of a sparse inode cluster. 
  Make sure the inode btrees are as large as the rmap thinks they are.
  Don't bother if we're missing btree cursors, as we're already corrupt.
 Check that we saw as many inobt blocks as the rmap says. 
  Make sure that the inobt records point to the same number of blocks as
  the rmap says are owned by inodes.
 Check that we saw as many inode blocks as the rmap knows about. 
 Scrub the inode btrees for some AG. 
	
	  If we're scrubbing the inode btree, inode_blocks is the number of
	  blocks pointed to by all the inode chunk records.  Therefore, we
	  should compare to the number of inode chunk blocks that the rmap
	  knows about.  We can't do this for the finobt since it only points
	  to inode chunks with free inodes.
 See if an inode btree has (or doesn't have) an inode chunk record. 
 xref check that the extent is not covered by inodes 
 xref check that the extent is covered by inodes 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Grab total control of the inode metadata.  It doesn't matter here if
  the file data is still changing; exclusive access to the metadata is
  the goal.
	
	  Try to get the inode.  If the verifiers fail, we try again
	  in raw mode.
 Got the inode, lock it and we're ready to go. 
 scrub teardown will unlock and release the inode for us 
 Inode core 
 Validate di_extsize hint. 
	
	  XFS allows a sysadmin to change the rt extent size when adding a rt
	  section to a filesystem after formatting.  If there are any
	  directories with extszinherit and rtinherit set, the hint could
	  become misaligned with the new rextsize.  The verifier doesn't check
	  this, because we allow rtinherit directories even without an rt
	  device.  Flag this as an administrative warning since we will clean
	  this up eventually.
  Validate di_cowextsize hint.
  The rules are documented at xfs_ioctl_setattr_check_cowextsize().
  These functions must be kept in sync with each other.
 Make sure the di_flags make sense for the inode. 
 di_flags are all taken, last bit cannot be used 
 rt flags require rt device 
 new rt bitmap flag only valid for rbmino 
 directory-only flags 
 file-only flags 
 filestreams and rt make no sense 
 Make sure the di_flags2 make sense for the inode. 
 Unknown di_flags2 could be from a future kernel 
 reflink flag requires reflink feature 
 cowextsize flag is checked w.r.t. mode separately 
 filedir-only flags 
 file-only flags 
 realtime and reflink make no sense, currently 
 no bigtime iflag without the bigtime feature 
 Scrub all the ondisk inode fields. 
 di_mode 
 mode is recognized 
 v1v2 fields 
		
		  We autoconvert v1 inodes into v2 inodes on writeout,
		  so just mark this inode for preening.
	
	  di_uiddi_gid -- -1 isn't invalid, but there's no way that
	  userspace could have created that.
 di_format 
 di_[amc]time.nsec 
	
	  di_size.  xfs_dinode_verify checks for things that screw up
	  the VFS such as the upper bit being set and zero-length
	  symlinksdirectories, but we can do more here.
 Devices, fifos, and sockets must have zero size 
 Directories can't be larger than the data section size (32G) 
 Symlinks can't be larger than SYMLINK_MAXLEN 
	
	  Warn if the running kernel can't handle the kinds of offsets
	  needed to deal with the file size.  In other words, if the
	  pagecache can't cache all the blocks in this file due to
	  overly large offsets, flag the inode for admin review.
 di_nblocks 
 nblocks can exceed dblocks 
		
		  nblocks is the sum of data extents (in the rtdev),
		  attr extents (in the datadev), and both forks' bmbt
		  blocks (in the datadev).  This clumsy check is the
		  best we can do without cross-referencing with the
		  inode forks.
 di_nextents 
 di_forkoff 
 di_aformat 
 di_anextents 
  Make sure the finobt doesn't think this inode is free.
  We don't have to check the inobt ourselves because we got the inode via
  IGET_UNTRUSTED, which checks the inobt for us.
	
	  Try to get the finobt record.  If we can't get it, then we're
	  in good shape.
	
	  Otherwise, make sure this record either doesn't cover this inode,
	  or that it does but it's marked present.
 Cross reference the inode fields with the forks. 
 Walk all the extents to check nextentsnaextentsnblocks. 
 Check nblocks against the inode. 
 Cross-reference with the other btrees. 
  If the reflink iflag disagrees with a scan for shared data fork extents,
  either flag an error (shared extents w no flag) or a preen (flag set wo
  any shared extents).  We already checked for reflink iflag set on a non
  reflink filesystem.
 Scrub an inode. 
	
	  If sc->ip is NULL, that means that the setup function called
	  xfs_iget to look up the inode.  xfs_iget returned a EFSCORRUPTED
	  and a NULL inode, so flag the corruption error and return.
 Scrub the inode core. 
	
	  Look for discrepancies between file's data blocks and the reflink
	  iflag.  We already checked the iflag against the file mode when
	  we scrubbed the dinode.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Set us up to scrub reverse mapping btrees.
 Reverse-mapping scrubber. 
 Cross-reference a rmap against the refcount btree. 
 If this is shared, must be a data fork extent. 
 Cross-reference with the other btrees. 
 Scrub an rmapbt record. 
 Check extent. 
		
		  xfs_verify_agbno returns false for static fs metadata.
		  Since that only exists at the start of the AG, validate
		  that by hand.
		
		  Otherwise we must point somewhere past the static metadata
		  but before the end of the FS.  Run the regular check.
 Check flags. 
 Non-inode owner within the magic values? 
 Scrub the rmap btree for some AG. 
 xref check that the extent is owned by a given owner 
 xref check that the extent is owned by a given owner 
 xref check that the extent is not owned by a given owner 
 xref check that the extent has no reverse mapping at all 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2018 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Attempt to repair some metadata, if the metadata is corrupt and userspace
  told us to fix it.  This function returns -EAGAIN to mean "re-run scrub",
  and will set fixed to true if it thinks it repaired anything.
 Repair whatever's broken. 
		
		  Repair succeeded.  Commit the fixes and perform a second
		  scrub so that we can tell userspace if we fixed the problem.
 Tell the caller to try again having grabbed all the locks. 
		
		  We tried harder but still couldn't grab all the resources
		  we needed to fix it.  The corruption has not been fixed,
		  so report back to userspace.
  Complain about unfixable problems in the filesystem.  We don't log
  corruptions when IFLAG_REPAIR wasn't set on the assumption that the driver
  program is xfs_scrub, which will call back with IFLAG_REPAIR set if the
  administrator isn't running xfs_scrub in no-repairs mode.
  Use this helper function because _ratelimited silently declares a static
  structure to track rate limiting information.
  Repair probe -- userspace uses this to probe if we're willing to repair a
  given mountpoint.
  Roll a transaction, keeping the AG headers locked and reinitializing
  the btree cursors.
 Keep the AG header buffers locked so we can keep going. 
	
	  Roll the transaction.  We still own the buffer and the buffer lock
	  regardless of whether or not the roll succeeds.  If the roll fails,
	  the buffers will be released during teardown on our way out of the
	  kernel.  If it succeeds, we join them to the new transaction and
	  move on.
 Join AG headers to the new transaction. 
  Does the given AG have enough space to rebuild a btree?  Neither AG
  reservation can be critical, and we must have enough space (factoring
  in AG reservations) to construct a whole btree.
  Figure out how many blocks to reserve for an AG repair.  We calculate the
  worst case estimate for the number of blocks we'd need to rebuild one of
  any type of per-AG btree.
 Use in-core icount if possible. 
 Try to get the actual counters from disk. 
 Now grab the block counters from the AGF. 
 If the icount is impossible, make some worst-case assumptions. 
 If the block counts are impossible, make worst-case assumptions. 
	
	  Figure out how many blocks we'd need worst case to rebuild
	  each type of btree.  Note that we can only rebuild the
	  bnobtcntbt or inobtfinobt as pairs.
		
		  Guess how many blocks we need to rebuild the rmapbt.
		  For non-reflink filesystems we can't have more records than
		  used blocks.  However, with reflink it's possible to have
		  more than one rmap record per AG block.  We don't know how
		  many rmaps there could be in the AG, so we start off with
		  what we hope is an generous over-estimation.
 Allocate a block in an AG. 
 Initialize a new AG btree root block with zero entries. 
  Reconstructing per-AG Btrees
  When a space btree is corrupt, we don't bother trying to fix it.  Instead,
  we scan secondary space metadata to derive the records that should be in
  the damaged btree, initialize a fresh btree root, and insert the records.
  Note that for rebuilding the rmapbt we scan all the primary data to
  generate the new records.
  However, that leaves the matter of removing all the metadata describing the
  old broken structure.  For primary metadata we use the rmap data to collect
  every extent with a matching rmap owner (bitmap); we then iterate all other
  metadata structures with the same rmap owner to collect the extents that
  cannot be removed (sublist).  We then subtract sublist from bitmap to
  derive the blocks that were used by the old btree.  These blocks can be
  reaped.
  For rmapbt reconstructions we must use different tactics for extent
  collection.  First we iterate all primary metadata (this excludes the old
  rmapbt, obviously) to generate new rmap records.  The gaps in the rmap
  records are collected as bitmap.  The bnobt records are collected as
  sublist.  As with the other btrees we subtract sublist from bitmap, and the
  result (since the rmapbt lives in the free space) are the blocks from the
  old rmapbt.
  Disposal of Blocks from Old per-AG Btrees
  Now that we've constructed a new btree to replace the damaged one, we want
  to dispose of the blocks that (we think) the old btree was using.
  Previously, we used the rmapbt to collect the extents (bitmap) with the
  rmap owner corresponding to the tree we rebuilt, collected extents for any
  blocks with the same rmap owner that are owned by another data structure
  (sublist), and subtracted sublist from bitmap.  In theory the extents
  remaining in bitmap are the old btree's blocks.
  Unfortunately, it's possible that the btree was crosslinked with other
  blocks on disk.  The rmap data can tell us if there are multiple owners, so
  if the rmapbt says there is an owner of this block other than @oinfo, then
  the block is crosslinked.  Remove the reverse mapping and continue.
  If there is one rmap record, we can free the block, which removes the
  reverse mapping but doesn't add the block to the free space.  Our repair
  strategy is to hope the other metadata objects crosslinked on this block
  will be rebuilt (atop different blocks), thereby removing all the cross
  links.
  If there are no rmap records at all, we also free the block.  If the btree
  being rebuilt lives in the free space (bnobtcntbtrmapbt) then there isn't
  supposed to be a rmap record and everything is ok.  For other btrees there
  had to have been an rmap entry for the block to have ended up on @bitmap,
  so if it's gone now there's something wrong and the fs will shut down.
  Note: If there are multiple rmap records with only the same rmap owner as
  the btree we're trying to rebuild and the block is indeed owned by another
  data structure with the same rmap owner, then the block will be in sublist
  and therefore doesn't need disposal.  If there are multiple rmap records
  with only the same rmap owner but the block is not owned by something with
  the same rmap owner, the block will be freed.
  The caller is responsible for locking the AG headers for the entire rebuild
  operation so that nothing else can sneak in and change the AG state while
  we're not looking.  We also assume that the caller already invalidated any
  buffers associated with @bitmap.
  Invalidate buffers for per-AG btree blocks we're dumping.  This function
  is not intended for use with file data repairs; we have bunmapi for that.
	
	  For each block in each extent, see if there's an incore buffer for
	  exactly that block; if so, invalidate it.  The buffer cache only
	  lets us look for one buffer at a time, so we have to look one block
	  at a time.  Avoid invalidating AG headers and post-EOFS blocks
	  because we never own those; and if we can't TRYLOCK the buffer we
	  assume it's owned by someone else.
 Skip AG headers and post-EOFS blocks 
 Ensure the freelist is the correct size. 
  Put a block back on the AGFL.
 Make sure there's space on the freelist. 
	
	  Since we're "freeing" a lost block onto the AGFL, we have to
	  create an rmap for the block prior to merging it or else other
	  parts will break.
 Put the block on the AGFL. 
 Dispose of a single block. 
	
	  If we are repairing per-inode metadata, we need to read in the AGF
	  buffer.  Otherwise, we're repairing a per-AG structure, so reuse
	  the AGF buffer that the setup functions already grabbed.
 Can we find any other rmappings? 
	
	  If there are other rmappings, this block is cross linked and must
	  not be freed.  Remove the reverse mapping and move on.  Otherwise,
	  we were the only owner of the block, so free the extent, which will
	  also remove the rmap.
	 
	  XXX: XFS doesn't support detecting the case where a single block
	  metadata structure is crosslinked with a multi-block structure
	  because the buffer cache doesn't detect aliasing problems, so we
	  can't fix 100% of crosslinking problems (yet).  The verifiers will
	  blow on writeout, the filesystem will shut down, and the admin gets
	  to run xfs_repair.
 Dispose of every block of every extent in the bitmap. 
  Finding per-AG Btree Roots for AGFAGI Reconstruction
  If the AGF or AGI become slightly corrupted, it may be necessary to rebuild
  the AG headers by using the rmap data to rummage through the AG looking for
  btree roots.  This is not guaranteed to work if the AG is heavily damaged
  or the rmap data are corrupt.
  Callers of xrep_find_ag_btree_roots must lock the AGF and AGFL
  buffers if the AGF is being rebuilt; or the AGF and AGI buffers if the
  AGI is being rebuilt.  It must maintain these locks until it's safe for
  other threads to change the btrees' shapes.  The caller provides
  information about the btrees to look for by passing in an array of
  xrep_find_ag_btree with the (rmap owner, buf_ops, magic) fields set.
  The (root, height) fields will be set on return if anything is found.  The
  last element of the array should have a NULL buf_ops to mark the end of the
  array.
  For every rmapbt record matching any of the rmap owners in btree_info,
  read each block referenced by the rmap record.  If the block is a btree
  block from this filesystem matching any of the magic numbers and has a
  level higher than what we've already seen, remember the block and the
  height of the tree required to have such a block.  When the call completes,
  we return the highest block we've found for each btree description; those
  should be the roots.
 See if our block is in the AGFL. 
 Does this block match the btree information passed in? 
	
	  Blocks in the AGFL have stale contents that might just happen to
	  have a matching magic and uuid.  We don't want to pull these blocks
	  in as part of a tree root, so we have to filter out the AGFL stuff
	  here.  If the AGFL looks insane we'll just refuse to repair.
	
	  Read the buffer into memory so that we can see if it's a match for
	  our btree type.  We have no clue if it is beforehand, and we want to
	  avoid xfs_trans_read_buf's behavior of dumping the DONE state (which
	  will cause needless disk reads in subsequent calls to this function)
	  and logging metadata verifier failures.
	 
	  Therefore, pass in NULL buffer ops.  If the buffer was already in
	  memory from some other caller it will already have b_ops assigned.
	  If it was in memory from a previous unsuccessful findroot_block
	  call, the buffer won't have b_ops but it should be clean and ready
	  for us to try to verify if the read call succeeds.  The same applies
	  if the buffer wasn't in memory at all.
	 
	  Note: If we never match a btree type with this buffer, it will be
	  left in memory with NULL b_ops.  This shouldn't be a problem unless
	  the buffer gets written.
 Ensure the block magic matches the btree type we're looking for. 
	
	  If the buffer already has ops applied and they're not the ones for
	  this btree type, we know this block doesn't match the btree and we
	  can bail out.
	 
	  If the buffer ops match ours, someone else has already validated
	  the block for us, so we can move on to checking if this is a root
	  block candidate.
	 
	  If the buffer does not have ops, nobody has successfully validated
	  the contents and the buffer cannot be dirty.  If the magic, uuid,
	  and structure match this btree type then we'll move on to checking
	  if it's a root block candidate.  If there is no match, bail out.
		
		  Read verifiers can reference b_ops, so we set the pointer
		  here.  If the verifier fails we'll reset the buffer state
		  to what it was before we touched the buffer.
		
		  Some read verifiers will (re)set b_ops, so we must be
		  careful not to change b_ops after running the verifier.
	
	  This block passes the magicuuid and verifier tests for this btree
	  type.  We don't need the caller to try the other tree types.
	
	  Compare this btree block's level to the height of the current
	  candidate root block.
	 
	  If the level matches the root we found previously, throw away both
	  blocks because there can't be two candidate roots.
	 
	  If level is lower in the tree than the root we found previously,
	  ignore this block.
	
	  This is the highest block in the tree that we've found so far.
	  Update the btree height to reflect what we've learned from this
	  block.
	
	  If this block doesn't have sibling pointers, then it's the new root
	  block candidate.  Otherwise, the root will be found farther up the
	  tree.
  Do any of the blocks in this rmap record match one of the btrees we're
  looking for?
 Ignore anything that isn't AG metadata. 
 Otherwise scan each block + btree type. 
 Find the roots of the per-AG btrees described in btree_info. 
 Force a quotacheck the next time we mount. 
  Attach dquots to this inode, or schedule quotacheck to fix them.
  This function ensures that the appropriate dquots are attached to an inode.
  We cannot allow the dquot code to allocate an on-disk dquot block here
  because we're already in transaction context with the inode locked.  The
  on-disk dquot should already exist anyway.  If the quota code signals
  corruption or missing quota information, schedule quotacheck, which will
  repair corruptions in the quota metadata.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 Set us up to scrub directories. 
 Directories 
 Scrub a directory entry. 
 VFS fill-directory iterator 
 Check that an inode's mode matches a given DT_ type. 
	
	  Grab the inode pointed to by the dirent.  We release the
	  inode before we cancel the scrub transaction.  Since we're
	  don't know a priori that releasing the inode won't trigger
	  eofblocks cleanup (which allocates what would be a nested
	  transaction), we can't use DONTCACHE here because DONTCACHE
	  inodes can trigger immediate inactive cleanup of the inode.
	 
	  If _iget returns -EINVAL or -ENOENT then the child inode number is
	  garbage and the directory is corrupt.  If the _iget returns
	  -EFSCORRUPTED or -EFSBADCRC then the child is corrupt which is a
	   cross referencing error.  Any other error is an operational error.
 Convert mode to the DT_ values that dir_emit uses. 
  Scrub a single directory entry.
  We use the VFS directory iterator (i.e. readdir) to call this
  function for every directory entry in a directory.  Once we're here,
  we check the inode number to make sure it's sane, then we check that
  we can look up this filename.  Finally, we check the ftype.
 Does this inode number make sense? 
 Does this name make sense? 
 If this is "." then check that the inum matches the dir. 
		
		  If this is ".." in the root inode, check that the inum
		  matches this dir.
 Verify that we can look up this name by hash. 
 ENOENT means the hash lookup failed and the dir is corrupt 
 Verify the file type.  This function absorbs error codes. 
	
	  A negative error code returned here is supposed to cause the
	  dir_emit caller (xfs_readdir) to abort the directory iteration
	  and return zero to xchk_directory.
 Scrub a directory btree record. 
 Check the hash of the entry. 
 Valid hash pointer? 
 Find the directory entry's location. 
 Make sure we got a real directory entry. 
 Retrieve the entry, sanity check it, and compare hashes. 
  Is this unused entry either in the bestfree or smaller than all of
  them?  We've already checked that the bestfrees are sorted longest to
  shortest, and that there aren't any bogus entries.
 Unused entry is shorter than any of the bestfrees 
 Unused entry should be in the bestfrees but wasn't found. 
 Check free space info in a directory data block. 
 dir block format 
 dir data format 
 XXX: Check xfs_dir3_data_hdr.pad is zero once we start setting it. 
 Do the bestfrees correspond to actual free space? 
 bestfree doesn't match the entry it points at? 
 bestfree records should be ordered largest to smallest 
 Make sure the bestfrees are actually the best free spaces. 
 Iterate the entries, stopping when we hit or go past the end. 
 Skip real entries 
 Spot check this free entry 
		
		  Either this entry is a bestfree or it's smaller than
		  any of the bestfrees.
 Move on. 
 We're required to fill all the space. 
 Did we see at least as many free slots as there are bestfrees? 
  Does the free space length in the free space index block ($len) match
  the longest length in the directory data block's bestfree array?
  Assume that we've already checked that the data block's bestfree
  array is in order.
 Check free space info in a directory leaf1 block. 
 Read the free space block. 
	
	  There should be as many bestfree slots as there are dir data
	  blocks that can fit under i_size.
 Is the leaf count even remotely sane? 
 Leaves and bests don't overlap in leaf format. 
 Check hash value order, count stale entries.  
 Check all the bestfree entries. 
 Check free space info in a directory freespace block. 
 Read the free space block 
 Check all the entries. 
 Check free space information in directories. 
 Ignore local format directories. 
 Is this a block dir? 
 Iterate all the data extents in the directory... 
 No more data blocks... 
		
		  Check each data block's bestfree data.
		 
		  Iterate all the fsbcount-aligned block offsets in
		  this directory.  The directory block reading code is
		  smart enough to do its own bmap lookups to handle
		  discontiguous directory blocks.  When we're done
		  with the extent record, re-query the bmap at the
		  next fsbcount-aligned offset to avoid redundant
		  block checks.
 Look for a leaf1 block, which has free info. 
 Scan for free blocks 
		
		  Dirs can't have blocks mapped above 2^32.
		  Single-block dirs shouldn't even be here.
		
		  Check each dir free block's bestfree data.
		 
		  Iterate all the fsbcount-aligned block offsets in
		  this directory.  The directory block reading code is
		  smart enough to do its own bmap lookups to handle
		  discontiguous directory blocks.  When we're done
		  with the extent record, re-query the bmap at the
		  next fsbcount-aligned offset to avoid redundant
		  block checks.
 Scrub a whole directory. 
 Plausible size? 
 Check directory tree structure 
 Check the freespace. 
	
	  Check that every dirent we see can also be looked up by hash.
	  Userspace usually asks for a 32k buffer, so we will too.
	
	  Look up every name in this directory by hash.
	 
	  Use the xfs_readdir function to call xchk_dir_actor on
	  every directory entry in this directory.  In _actor, we check
	  the name, inode number, and ftype (if applicable) of the
	  entry.  xfs_readdir uses the VFS filldir functions to provide
	  iteration context.
	 
	  The VFS grabs a read or write lock via i_rwsem before it reads
	  or writes to a directory.  If we've gotten this far we've
	  already obtained IOLOCK_EXCL, which (since 4.10) is the same as
	  getting a write lock on i_rwsem.  Therefore, it is safe for us
	  to drop the ILOCK here in order to reuse the _readdir and
	  _dir_lookup routines, which do their own ILOCK locking.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Allocate enough memory to hold an attr value and attr block bitmaps,
  reallocating the buffer if necessary.  Buffer contents are not preserved
  across a reallocation.
	
	  We need enough space to read an xattr value from the file or enough
	  space to hold three copies of the xattr free space bitmap.  We don't
	  need the buffer space for both purposes at the same time.
	
	  If there's already a buffer, figure out if we need to reallocate it
	  to accommodate a larger size.
	
	  Don't zero the buffer upon allocation to avoid runtime overhead.
	  All users must be careful never to read uninitialized contents.
 Set us up to scrub an inode's extended attributes. 
	
	  We failed to get memory while checking attrs, so this time try to
	  get all the memory we're ever going to need.  Allocate the buffer
	  without the inode lock held, which means we can sleep.
 Extended Attributes 
  Check that an extended attribute key can be looked up by hash.
  We use the XFS attribute list iterator (i.e. xfs_attr_list_ilocked)
  to call this function for every attribute key in an inode.  Once
  we're here, we load the attribute value to see if any errors happen,
  or if we get more or less data than we expected.
 Incomplete attr key, just mark the inode for preening. 
 Does this name make sense? 
	
	  Try to allocate enough memory to extrat the attr value.  If that
	  doesn't work, we overload the seen_enough variable to convey
	  the error message back to the main scrub function.
 ENODATA means the hash lookup failed and the attr is bad 
  Mark a range [start, start+len) in this map.  Returns true if the
  region was free, and false if there's a conflict or a problem.
  Within a char, the lowest bit of the char represents the byte with
  the smallest address
  Check the leaf freemap from the usage bitmap.  Returns false if the
  attr freemap has problems or points to used space.
 Construct bitmap of freemap contents. 
 Look for bits that are set in freemap and are marked in use. 
  Check this leaf entry's relations to everything else.
  Returns the number of bytes used for the namevalue data.
 Hash values in order? 
 Check the name information. 
 Scrub an attribute leaf. 
 Allocate memory for block usage checking. 
 Check all the padding. 
 Check the leaf header 
 Mark the leaf entry itself. 
 Check the entry and nameval. 
 Scrub a attribute btree record. 
 Check the whole block, if necessary. 
 Check the hash of the entry. 
 Find the attr entry's location. 
 Retrieve the entry and check it. 
 Scrub the extended attribute metadata. 
 Check attribute tree structure 
 Check that every attr key can also be looked up by hash. 
	
	  Look up every xattr in this file by name.
	 
	  Use the backend implementation of xfs_attr_list to call
	  xchk_xattr_listent on every attribute key in this inode.
	  In other words, we use the same iteratorcallback mechanism
	  that listattr uses to scrub extended attributes, though in our
	  _listent function, we check the value of the attribute.
	 
	  The VFS only locks i_rwsem when modifying attrs, so keep all
	  three locks held because that's the only way to ensure we're
	  the only thread poking into the da btree.  We traverse the da
	  btree while holding a leaf buffer locked for the xattr name
	  iteration, which doesn't really follow the usual buffer
	  locking order.
 Did our listent function try to return any errors? 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 DirectoryAttribute Btree 
  Check for da btree operation errors.  See the section about handling
  operational errors in common.c.
 Used to restart an op with deadlock avoidance. 
 Note the badness but don't abort. 
  Check for da btree corruption.  See the section about handling
  operational errors in common.c.
 Scrub a da btree hash (key). 
 Is this hash in order? 
 Is this hash no larger than the parent hash? 
  Check a da btree pointer.  Returns true if it's ok to use this
  pointer.
  The da btree scrubber can handle leaf1 blocks as a degenerate
  form of leafn blocks.  Since the regular da code doesn't handle
  leaf1, we must multiplex the verifiers.
		
		  xfs_da3_node_buf_ops already know how to handle
		  DA_NODE, ATTR_LEAF, and DIR_LEAFN blocks.
		
		  xfs_da3_node_buf_ops already know how to handle
		  DA_NODE, ATTR_LEAF, and DIR_LEAFN blocks.
 Check a block's sibling. 
	
	  If the pointer is null, we shouldn't be able to move the upper
	  level pointer anywhere.
 Move the alternate cursor one block in the direction given. 
 Compare upper level pointer to sibling pointer. 
 Free all buffers in the altpath that aren't referenced from path. 
 Check a block's sibling pointers. 
 Top level blocks should not have sibling pointers. 
	
	  Check back (left) and forw (right) pointers.  These functions
	  absorb error codes for us.
 Load a dirattribute block from a btree. 
 Release old block. 
 Check the pointer. 
 Read the buffer. 
	
	  We didn't find a dir btree root block, which means that
	  there's no LEAF1LEAFN tree (at least not where it's supposed
	  to be), so jump out now.
 It's not ok for attr trees not to have a da btree. 
 We only started zeroing the header on v5 filesystems. 
 Check the owner. 
 Check the siblings. 
 Interpret the buffer. 
 XXX: Check hdr3.pad32 once we know how to fix it. 
	
	  If we've been handed a block that is below the dabtree root, does
	  its hashval match what the parent block expected to see?
 Visit all nodes and leaves of a da btree. 
 Skip short format data structures; no btree to scan. 
 Set up initial da state. 
 Find the root of the da tree, if present. 
	
	  We didn't find a block at ds->lowest, which means that there's
	  no LEAF1LEAFN tree (at least not where it's supposed to be),
	  so jump out now.
 Handle leaf block. 
 End of leaf, pop back towards the root. 
 Dispatch record scrubbing. 
 End of node, pop back towards the root. 
 Hashes in order for scrub? 
 Drill another level deeper. 
 Too deep! 
 Release all the buffers we're tracking. 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2019 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Scrub and In-Core Filesystem Health Assessments
  ===============================================
  Online scrub and repair have the time and the ability to perform stronger
  checks than we can do from the metadata verifiers, because they can
  cross-reference records between data structures.  Therefore, scrub is in a
  good position to update the online filesystem health assessments to reflect
  the goodbad state of the data structure.
  We therefore extend scrub in the following ways to achieve this:
  1. Create a "sick_mask" field in the scrub context.  When we're setting up a
  scrub call, set this to the default XFS_SICK_ flag(s) for the selected
  scrub type (call it A).  Scrub and repair functions can override the default
  sick_mask value if they choose.
  2. If the scrubber returns a runtime error code, we exit making no changes
  to the incore sick state.
  3. If the scrubber finds that A is clean, use sick_mask to clear the incore
  sick flags before exiting.
  4. If the scrubber finds that A is corrupt, use sick_mask to set the incore
  sick flags.  If the user didn't want to repair then we exit, leaving the
  metadata structure unfixed and the sick flag set.
  5. Now we know that A is corrupt and the user wants to repair, so run the
  repairer.  If the repairer returns an error code, we exit with that error
  code, having made no further changes to the incore sick state.
  6. If repair rebuilds A correctly and the subsequent re-scrub of A is clean,
  use sick_mask to clear the incore sick flags.  This should have the effect
  that A is no longer marked sick.
  7. If repair rebuilds A incorrectly, the re-scrub will find it corrupt and
  use sick_mask to set the incore sick flags.  This should have no externally
  visible effect since we already set them in step (4).
  There are some complications to this story, however.  For certain types of
  complementary metadata indices (e.g. inobtfinobt), it is easier to rebuild
  both structures at the same time.  The following principles apply to this
  type of repair strategy:
  8. Any repair function that rebuilds multiple structures should update
  sick_mask_visible to reflect whatever other structures are rebuilt, and
  verify that all the rebuilt structures can pass a scrub check.  The outcomes
  of 5-7 still apply, but with a sick_mask that covers everything being
  rebuilt.
 Map our scrub type to a sick mask and a set of health update functions. 
 Return the health status mask for this scrub type. 
  Update filesystem health assessments based on what we found and did.
  If the scrubber finds errors, we mark sick whatever's mentioned in
  sick_mask, no matter whether this is a first scan or an
  evaluation of repair effectiveness.
  Otherwise, no direct corruption was found, so mark whatever's in
  sick_mask as healthy.
 Is the given per-AG btree healthy enough for scanning? 
	
	  We always want the cursor if it's the same type as whatever we're
	  scrubbing, even if we already know the structure is corrupt.
	 
	  Otherwise, we're only interested in the btree for cross-referencing.
	  If we know the btree is bad then don't bother, just set XFAIL.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 Superblock 
 Cross-reference with the other btrees. 
 scrub teardown will take care of sc->sa for us 
  Scrub the filesystem superblock.
  Note: We do not attempt to check AG 0's superblock.  Mount is
  responsible for validating all the geometry information in sb 0, so
  if the filesystem is capable of initiating online scrub, then clearly
  sb 0 is ok and we can use its information to check everything else.
	
	  Grab an active reference to the perag structure.  If we can't get
	  it, we're racing with something that's tearing down the AG, so
	  signal that the AG no longer exists.
	
	  The superblock verifier can return several different error codes
	  if it thinks the superblock doesn't look right.  For a mount these
	  would all get bounced back to userspace, but if we're here then the
	  fs mounted successfully, which means that this secondary superblock
	  is simply incorrect.  Treat all these codes the same way we treat
	  any corruption.
 also -EWRONGFS 
	
	  Verify the geometries match.  Fields that are permanently
	  set by mkfs are checked; fields that can be updated later
	  (and are not propagated to backup superblocks) are preen
	  checked.
 Check sb_versionnum bits that are set at mkfs time. 
 Check sb_versionnum bits that can be set after mkfs time. 
	
	  Skip the summary counters since we track them in memory anyway.
	  sb_icount, sb_ifree, sb_fdblocks, sb_frexents
	
	  Skip the quota flags since repair will force quotacheck.
	  sb_qflags
 Do we see any invalid bits in sb_features2? 
 Check sb_features2 flags that are set at mkfs time. 
 Check sb_features2 flags that can be set after mkfs time. 
 all v5 fields must be zero 
 Check compat flags; all are set at mkfs time. 
 Check ro compat flags; all are set at mkfs time. 
 Check incompat flags; all are set at mkfs time. 
 Check log incompat flags; all are set at mkfs time. 
 Don't care about sb_crc 
 Don't care about sb_lsn 
 The metadata UUID must be the same for all supers 
 Everything else must be zero. 
 AGF 
 Tally freespace record lengths. 
 Check agf_freeblks 
 Cross reference the AGF with the cntbt (freespace by length btree) 
 Any freespace at all? 
 Check agf_longest 
 Check the btree block counts in the AGF against the btrees. 
 agf_btreeblks didn't exist before lazysbcount 
 Check agf_rmap_blocks; set up for agf_btreeblks check 
	
	  No rmap cursor; we can't xref if we have the rmapbt feature.
	  We also can't do it if we're missing the free space btree cursors.
 Check agf_btreeblks 
 Check agf_refcount_blocks against tree size 
 Cross-reference with the other btrees. 
 scrub teardown will take care of sc->sa for us 
 Scrub the AGF. 
 Check the AG length 
 Check the AGF btree roots and levels 
 Check the AGFL counters 
 Do the incore counters match? 
 AGFL 
 Cross-reference with the other btrees. 
 Scrub an AGFL block. 
 Cross-reference with the other btrees. 
	
	  Scrub teardown will take care of sc->sa for us.  Leave sc->sa
	  active so that the agfl block xref can use it too.
 Scrub the AGFL. 
 Allocate buffer to ensure uniqueness of AGFL entries. 
 Check the blocks in the AGFL. 
 Sort entries, check for duplicates. 
 AGI 
 Check agi_countagi_freecount 
 Check agi_[fi]blocks against tree size 
 Cross-reference with the other btrees. 
 scrub teardown will take care of sc->sa for us 
 Scrub the AGI. 
 Check the AG length 
 Check btree roots and levels 
 Check inode counters 
 Check inode pointers 
 Check unlinked inode buckets 
 Do the incore counters match? 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Online Scrub and Repair
  Traditionally, XFS (the kernel driver) did not know how to check or
  repair on-disk data structures.  That task was left to the xfs_check
  and xfs_repair tools, both of which require taking the filesystem
  offline for a thorough but time consuming examination.  Online
  scrub & repair, on the other hand, enables us to check the metadata
  for obvious errors while carefully stepping around the filesystem's
  ongoing operations, locking rules, etc.
  Given that most XFS metadata consist of records stored in a btree,
  most of the checking functions iterate the btree blocks themselves
  looking for irregularities.  When a record block is encountered, each
  record can be checked for obviously bad values.  Record values can
  also be cross-referenced against other btrees to look for potential
  misunderstandings between pieces of metadata.
  It is expected that the checkers responsible for per-AG metadata
  structures will lock the AG headers (AGI, AGF, AGFL), iterate the
  metadata structure, and perform any relevant cross-referencing before
  unlocking the AG and returning the results to userspace.  These
  scrubbers must not keep an AG locked for too long to avoid tying up
  the block and inode allocators.
  Block maps and b-trees rooted in an inode present a special challenge
  because they can involve extents from any AG.  The general scrubber
  structure of lock -> check -> xref -> unlock still holds, but AG
  locking order rules must be obeyed to avoid deadlocks.  The
  ordering rule, of course, is that we must lock in increasing AG
  order.  Helper functions are provided to track which AG headers we've
  already locked.  If we detect an imminent locking order violation, we
  can signal a potential deadlock, in which case the scrubber can jump
  out to the top level, lock all the AGs in order, and retry the scrub.
  For file data (directories, extended attributes, symlinks) scrub, we
  can simply lock the inode and walk the data.  For btree data
  (directories and attributes) we follow the same btree-scrubbing
  strategy outlined previously to check the records.
  We use a bit of trickery with transactions to avoid buffer deadlocks
  if there is a cycle in the metadata.  The basic problem is that
  travelling down a btree involves locking the current buffer at each
  tree level.  If a pointer should somehow point back to a buffer that
  we've already examined, we will deadlock due to the second buffer
  locking attempt.  Note however that grabbing a buffer in transaction
  context links the locked buffer to the transaction.  If we try to
  re-grab the buffer in the context of the same transaction, we avoid
  the second lock attempt and continue.  Between the verifier and the
  scrubber, something will notice that something is amiss and report
  the corruption.  Therefore, each scrubber will allocate an empty
  transaction, attach buffers to it, and cancel the transaction at the
  end of the scrub run.  Cancelling a non-dirty transaction simply
  unlocks the buffers.
  There are four pieces of data that scrub can communicate to
  userspace.  The first is the error code (errno), which can be used to
  communicate operational errors in performing the scrub.  There are
  also three flags that can be set in the scrub context.  If the data
  structure itself is corrupt, the CORRUPT flag will be set.  If
  the metadata is correct but otherwise suboptimal, the PREEN flag
  will be set.
  We perform secondary validation of filesystem metadata by
  cross-referencing every record with all other available metadata.
  For example, for block mapping extents, we verify that there are no
  records in the free space and inode btrees corresponding to that
  space extent and that there is a corresponding entry in the reverse
  mapping btree.  Inconsistent metadata is noted by setting the
  XCORRUPT flag; btree query function errors are noted by setting the
  XFAIL flag and deleting the cursor to prevent further attempts to
  cross-reference with a defective btree.
  If a piece of metadata proves corrupt or suboptimal, the userspace
  program can ask the kernel to apply some tender loving care (TLC) to
  the metadata object by setting the REPAIR flag and re-calling the
  scrub ioctl.  "Corruption" is defined by metadata violating the
  on-disk specification; operations cannot continue if the violation is
  left untreated.  It is possible for XFS to continue if an object is
  "suboptimal", however performance may be degraded.  Repairs are
  usually performed by rebuilding the metadata entirely out of
  redundant metadata.  Optimizing, on the other hand, can sometimes be
  done without rebuilding entire structures.
  Generally speaking, the repair code has the following code structure:
  Lock -> scrub -> repair -> commit -> re-lock -> re-scrub -> unlock.
  The first check helps us figure out if we need to rebuild or simply
  optimize the structure so that the rebuild knows what to do.  The
  second check evaluates the completeness of the repair; that is what
  is reported to userspace.
  A quick note on symbol prefixes:
  - "xfs_" are general XFS symbols.
  - "xchk_" are symbols related to metadata checking.
  - "xrep_" are symbols related to metadata repair.
  - "xfs_scrub_" are symbols that tie online fsck to the rest of XFS.
  Scrub probe -- userspace uses this to probe if we're willing to scrub
  or repair a given mountpoint.  This will be used by xfs_scrub to
  probe the kernel's abilities to scrub (and repair) the metadata.  We
  do this by validating the ioctl inputs from userspace, preparing the
  filesystem for a scrub (or a repair) operation, and immediately
  returning to userspace.  Userspace can use the returned errno and
  structure state to decide (in broad terms) if scrubrepair are
  supported by the running kernel.
 Scrub setup and teardown 
 Free all the resources and finish the transactions. 
 Scrubbing dispatch. 
 ioctl presence test 
 superblock 
 agf 
 agfl 
 agi 
 bnobt 
 cntbt 
 inobt 
 finobt 
 rmapbt 
 refcountbt 
 inode record 
 inode data fork 
 inode attr fork 
 inode CoW fork 
 directory 
 extended attributes 
 symbolic link 
 parent pointers 
 realtime bitmap 
 realtime summary 
 user quota 
 group quota 
 project quota 
 fs summary counters 
 This isn't a stable feature, warn once per day. 
 Check our inputs. 
 sm_reserved[] must be zero 
 Do we know about this type of metadata? 
 Does this fs even support this type of metadata? 
 restricting fields must be appropriate for type 
	
	  We only want to repair read-write v5+ filesystems.  Defer the check
	  for ops->repair until after our scrub confirms that we need to
	  perform repairs so that we avoid failing due to not supporting
	  repairing an object that doesn't need repairs.
	
	  Userspace asked us to repair something, we repaired it, rescanned
	  it, and the rescan says it's still broken.  Scream about this in
	  the system logs.
	
	  Userspace asked us to scrub something, it's broken, and we have no
	  way of fixing it.  Scream in the logs.
 CONFIG_XFS_ONLINE_REPAIR 
 Dispatch metadata scrubbing. 
 Forbidden if we are shut down or mounted norecovery. 
	
	  When repairs are allowed, prevent freezing or readonly remount while
	  scrub is running with a real transaction.
 Set up for the operation. 
 Scrub for errors. 
		
		  Scrubbers return -EDEADLOCK to mean 'try harder'.
		  Tear down everything we hold, then set up again with
		  preparation for worst-case scenarios.
 Let debug users force us into the repair routines. 
		
		  If userspace asked for a repair but it wasn't necessary,
		  report that back to userspace.
		
		  If it's broken, userspace wants us to fix it, and we haven't
		  already tried to fix it, then attempt a repair.
			
			  Either the repair function succeeded or it couldn't
			  get all the resources it needs; either way, we go
			  back to the beginning and call the scrub function.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2018 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 Superblock 
 Repair the superblock. 
 Don't try to repair AG 0's sb; let xfs_repair deal with it. 
 Copy AG 0's superblock to this one. 
 Write this to disk. 
 AGF 
 Record free space shape information. 
 Does this AGFL block look sane? 
  Offset within the xrep_find_ag_btree array for each btree type.  Avoid the
  XFS_BTNUM_ names here to avoid creating a sparse array.
 Check a btree root candidate. 
  Given the btree roots described by fab, find the roots, check them for
  sanity, and pass the root data back out via fab.
  This is also a chicken and egg problem because we have to use the rmapbt
  (rooted in the AGF) to find the btrees rooted in the AGF.  We also have no
  idea if the btrees make any sense.  If we hit obvious corruptions in those
  btrees we'll bail out.
 Go find the root data. 
 We must find the bnobt, cntbt, and rmapbt roots. 
	
	  We relied on the rmapbt to reconstruct the AGF.  If we get a
	  different root then something's seriously wrong.
 We must find the refcountbt root if that feature is enabled. 
  Reinitialize the AGF header, making an in-core copy of the old contents so
  that we know which in-core state needs to be reinitialized.
 Mark the incore AGF data stale until we're done fixing things. 
 Set btree root information in an AGF. 
 Update all AGF fields which derive from btree contents. 
 Update the AGF counters from the bnobt. 
 Update the AGF counters from the cntbt. 
 Update the AGF counters from the rmapbt. 
 Update the AGF counters from the refcountbt. 
 Commit the new AGF and reinitialize the incore state. 
 Trigger fdblocks recalculation 
 Write this to disk. 
 Now reinitialize the in-core counters we changed. 
 Repair the AGF. v5 filesystems only. 
 We require the rmapbt to rebuild anything. 
	
	  Make sure we have the AGF buffer, as scrub might have decided it
	  was corrupt after xfs_alloc_read_agf failed with -EFSCORRUPTED.
	
	  Load the AGFL so that we can screen out OWN_AG blocks that are on
	  the AGFL now; these blocks might have once been part of the
	  bnocntrmap btrees but are not now.  This is a chicken and egg
	  problem: the AGF is corrupt, so we have to trust the AGFL contents
	  because we can't do any serious cross-referencing with any of the
	  btrees rooted in the AGF.  If the AGFL contents are obviously bad
	  then we'll bail out.
	
	  Spot-check the AGFL blocks; if they're obviously corrupt then
	  there's nothing we can do but bail out.
	
	  Find the AGF btree roots.  This is also a chicken-and-egg situation;
	  see the function for more details.
 Start rewriting the header and implant the btrees we found. 
 Commit the changes and reinitialize incore state. 
 Mark the incore AGF state stale and revert the AGF. 
 AGFL 
 Bitmap of other OWN_AG metadata blocks. 
 Bitmap of free space. 
 Record all OWN_AG (free space btree) information from the rmap data. 
 Record all the OWN_AG blocks. 
  Map out all the non-AGFL OWN_AG space in this AG so that we can deduce
  which blocks belong to the AGFL.
  Compute the set of old AGFL blocks by subtracting from the list of OWN_AG
  blocks the list of blocks owned by all other OWN_AG metadata (bnobt, cntbt,
  rmapbt).  These are the old AGFL blocks, so return that list and the number
  of blocks we're actually going to put back on the AGFL.
 Find all space used by the free space btrees & rmapbt. 
 Find all blocks currently being used by the bnobt. 
 Find all blocks currently being used by the cntbt. 
	
	  Drop the freesp meta blocks that are in use by btrees.
	  The remaining blocks should be AGFL blocks.
	
	  Calculate the new AGFL size.  If we found more blocks than fit in
	  the AGFL we'll free them later.
 Update the AGF and reset the in-core state. 
 Trigger fdblocks recalculation 
 Update the AGF counters. 
 Write out a totally new AGFL. 
	
	  Start rewriting the header by setting the bno[] array to
	  NULLAGBLOCK, then setting AGFL header fields.
	
	  Fill the AGFL with the remaining blocks.  If agfl_extents has more
	  blocks than fit in the AGFL, they will be freed in a subsequent
	  step.
			
			  We've now used br->start by putting it in the AGFL,
			  so bump br so that we don't reap the block later.
 Write new AGFL to disk. 
 Repair the AGFL. 
 We require the rmapbt to rebuild anything. 
	
	  Read the AGF so that we can query the rmapbt.  We hope that there's
	  nothing wrong with the AGF, but all the AG header repair functions
	  have this chicken-and-egg problem.
	
	  Make sure we have the AGFL buffer, as scrub might have decided it
	  was corrupt after xfs_alloc_read_agfl failed with -EFSCORRUPTED.
 Gather all the extents we're going to put on the new AGFL. 
	
	  Update AGF and AGFL.  We reset the global free block counter when
	  we adjust the AGF flcount (which can fail) so avoid updating any
	  buffers until we know that part works.
	
	  Ok, the AGFL should be ready to go now.  Roll the transaction to
	  make the new AGFL permanent before we start using it to return
	  freespace overflow to the freespace btrees.
 Dump any AGFL overflow. 
 AGI 
  Offset within the xrep_find_ag_btree array for each btree type.  Avoid the
  XFS_BTNUM_ names here to avoid creating a sparse array.
  Given the inode btree roots described by fab, find the roots, check them
  for sanity, and pass the root data back out via fab.
 Read the AGF. 
 Find the btree roots. 
 We must find the inobt root. 
 We must find the finobt root if that feature is enabled. 
  Reinitialize the AGI header, making an in-core copy of the old contents so
  that we know which in-core state needs to be reinitialized.
 We don't know how to fix the unlinked list yet. 
 Mark the incore AGF data stale until we're done fixing things. 
 Set btree root information in an AGI. 
 Update the AGI counters. 
 Trigger reinitialization of the in-core data. 
 Trigger inode count recalculation 
 Write this to disk. 
 Now reinitialize the in-core counters if necessary. 
 Repair the AGI. 
 We require the rmapbt to rebuild anything. 
	
	  Make sure we have the AGI buffer, as scrub might have decided it
	  was corrupt after xfs_ialloc_read_agi failed with -EFSCORRUPTED.
 Find the AGI btree roots. 
 Start rewriting the header and implant the btrees we found. 
 Reinitialize in-core state. 
 Mark the incore AGI state stale and revert the AGI. 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 Set us up to scrub a symbolic link. 
 Allocate the buffer without the inode lock held. 
 Symbolic links. 
 Plausible size? 
 Inline symlink? 
 Remote symlink; must read the contents. 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 Convert a scrub type code to a DQ flag, or return 0 if error. 
 Set us up to scrub a quota. 
 Quotas. 
 Scrub the fields in an individual quota item. 
	
	  Except for the root dquot, the actual dquot we got must either have
	  the same or higher id as we saw before.
	
	  Warn if the hard limits are larger than the fs.
	  Administrators can do this, though in production this seems
	  suspect, which is why we flag it for review.
	 
	  Complain about corruption if the soft limit is greater than
	  the hard limit.
 Check the resource counts. 
	
	  Check that usage doesn't exceed physical limits.  However, on
	  a reflink filesystem we're allowed to exceed physical space
	  if there are no quota limits.
	
	  We can violate the hard limits if the admin suddenly sets a
	  lower limit than the actual usage.  However, we flag it for
	  admin review.
 Check the quota's data fork. 
 Invoke the fork scrubber. 
 Check for data fork problems that apply only to quota files. 
		
		  delalloc extents or blocks mapped above the highest
		  quota id shouldn't happen.
 Scrub all of a quota type's items. 
 Look for problem extents. 
	
	  Check all the quota items.  Now that we've checked the quota inode
	  data fork we have to drop ILOCK_EXCL to use the regular dquot
	  functions.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2019 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  FS Summary Counters
  ===================
  The basics of filesystem summary counter checking are that we iterate the
  AGs counting the number of free blocks, free space btree blocks, per-AG
  reservations, inodes, delayed allocation reservations, and free inodes.
  Then we compare what we computed against the in-core counters.
  However, the reality is that summary counters are a tricky beast to check.
  While we could freeze the filesystem and scramble around the AGs counting
  the free blocks, in practice we prefer not do that for a scan because
  freezing is costly.  To get around this, we added a per-cpu counter of the
  delalloc reservations so that we can rotor around the AGs relatively
  quickly, and we allow the counts to be slightly off because we're not taking
  any locks while we do this.
  So the first thing we do is warm up the buffer cache in the setup routine by
  walking all the AGs to make sure the incore per-AG structure has been
  initialized.  The expected value calculation then iterates the incore per-AG
  structures as quickly as it can.  We snapshot the percpu counters before and
  after this operation and use the difference in counter values to guess at
  our tolerance for mismatch between expected and actual counter values.
  Since the expected value computation is lockless but only browses incore
  values, the percpu counters should be fairly close to each other.  However,
  we'll allow ourselves to be off by at least this (arbitrary) amount.
  Make sure the per-AG structure has been initialized from the on-disk header
  contents and trust that the incore counters match the ondisk counters.  (The
  AGF and AGI scrubbers check them, and a normal xfs_scrub run checks the
  summary counters after checking all AG headers).  Do this from the setup
  function so that the inner AG aggregation loop runs as quickly as possible.
  This function runs during the setup phase before we start checking any
  metadata.
 Lock both AG headers. 
		
		  These are supposed to be initialized by the header read
		  function.
 We must get the incore counters set up before we can proceed. 
	
	  Pause background reclaim while we're scrubbing to reduce the
	  likelihood of background perturbations to the counters throwing off
	  our calculations.
 Count free space btree blocks manually for pre-lazysbcount filesystems. 
  Calculate what the global in-core counters ought to be from the incore
  per-AG structure.  Callers can compare this to the actual in-core counters
  to estimate by how much both in-core and on-disk counters need to be
  adjusted.
 This somehow got unset since the warmup? 
 Count all the inodes 
 Add up the freefreelistbnobtcntbt blocks 
		
		  Per-AG reservations are taken out of the incore counters,
		  so they must be left out of the free blocks computation.
	
	  The global incore space reservation is taken from the incore
	  counters, so leave that out of the computation.
	
	  Delayed allocation reservations are taken out of the incore counters
	  but not recorded on disk, so leave them and their indlen blocks out
	  of the computation.
 Bail out if the values we compute are totally nonsense. 
	
	  If ifree > icount then we probably had some perturbation in the
	  counters while we were calculating things.  We'll try a few times
	  to maintain ifree <= icount before giving up.
  Is the @counter reasonably close to the @expected value?
  We neither locked nor froze anything in the filesystem while aggregating the
  per-AG data to compute the @expected value, which means that the counter
  could have changed.  We know the @old_value of the summation of the counter
  before the aggregation, and we re-sum the counter now.  If the expected
  value falls between the two summations, we're ok.
  Otherwise, we might have a problem.  If the change in the summations is
  more than we want to tolerate, the filesystem is probably busy and we should
  just send back INCOMPLETE and see if userspace will try again.
 Negative values are always wrong. 
 Exact matches are always ok. 
 Within the before-and-after range is ok. 
	
	  If the difference between the two summations is too large, the fs
	  might just be busy and so we'll mark the scrub incomplete.  Return
	  true here so that we don't mark the counter corrupt.
	 
	  XXX: In the future when userspace can grant scrub permission to
	  quiesce the filesystem to solve the outsized variance problem, this
	  check should be moved up and the return code changed to signal to
	  userspace that we need quiesce permission.
 Check the superblock counters. 
 Snapshot the percpu counters. 
 No negative values, please! 
 See if icount is obviously wrong. 
 See if fdblocks is obviously wrong. 
	
	  If ifree exceeds icount by more than the minimum variance then
	  something's probably wrong with the counters.
 Walk the incore AG headers to calculate the expected counters. 
 Compare the in-core counters with whatever we counted. 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2018 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Set a range of this bitmap.  Caller must ensure the range is not set.
  This is the logical equivalent of bitmap |= mask(start, len).
 Free everything related to this bitmap. 
 Set up a per-AG block bitmap. 
 Compare two btree extents. 
  Remove all the blocks mentioned in @sub from the extents in @bitmap.
  The intent is that callers will iterate the rmapbt for all of its records
  for a given owner to generate @bitmap; and iterate all the blocks of the
  metadata structures that are not being rebuilt and have the same rmapbt
  owner to generate @sub.  This routine subtracts all the extents
  mentioned in sub from all the extents linked in @bitmap, which leaves
  @bitmap as the list of blocks that are not accounted for, which we assume
  are the dead blocks of the old metadata structure.  The blocks mentioned in
  @bitmap can be reaped.
  This is the logical equivalent of bitmap &= ~sub.
	
	  Now that we've sorted both lists, we iterate bitmap once, rolling
	  forward through sub andor bitmap as necessary until we find an
	  overlap or reach the end of either list.  We do not reset lp to the
	  head of bitmap nor do we reset sub_br to the head of sub.  The
	  list traversal is similar to merge sort, but we're deleting
	  instead.  In this manner we avoid O(n^2) operations.
		
		  Advance sub_br andor br until we find a pair that
		  intersect or we run out of extents.
 trim sub_br to fit the extent we have 
 Coincides with only the left. 
 Coincides with only the right. 
 Total overlap, just delete ex. 
			
			  Deleting from the middle: add the new right extent
			  and then shrink the left extent.
  Record all btree blocks seen while iterating all records of a btree.
  We know that the btree query_all function starts at the left edge and walks
  towards the right edge of the tree.  Therefore, we know that we can walk up
  the btree cursor towards the root; if the pointer for a given level points
  to the first recordkey in that block, we haven't seen this block before;
  and therefore we need to remember that we saw this block in the btree.
  So if our btree is:
     4
    | \
  1  2  3
  Pretend for this example that each leaf block has 100 btree records.  For
  the first btree record, we'll observe that bc_levels[0].ptr == 1, so we
  record that we saw block 1.  Then we observe that bc_levels[1].ptr == 1, so
  we record block 4.  The list is [1, 4].
  For the second btree record, we see that bc_levels[0].ptr == 2, so we exit
  the loop.  The list remains [1, 4].
  For the 101st btree record, we've moved onto leaf block 2.  Now
  bc_levels[0].ptr == 1 again, so we record that we saw block 2.  We see that
  bc_levels[1].ptr == 2, so we exit the loop.  The list is now [1, 4, 2].
  For the 102nd record, bc_levels[0].ptr == 2, so we continue.
  For the 201st record, we've moved on to leaf block 3.
  bc_levels[0].ptr == 1, so we add 3 to the list.  Now it is [1, 4, 2, 3].
  For the 300th record we just exit, with the list being [1, 4, 2, 3].
  Record all the buffers pointed to by the btree cursor.  Callers already
  engaged in a btree walk should call this function to capture the list of
  blocks going from the leaf towards the root.
 Collect a btree's block in the bitmap. 
 Walk the btree and mark the bitmap wherever a btree block is found. 
 How many bits are set in this bitmap? 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 Set us up to scrub parents. 
 Parent pointers 
 Look for an entry in a parent pointing to this inode. 
 Look for a single entry in a directory pointing to an inode. 
	
	  If we're facing a fatal signal, bail out.  Store the cancellation
	  status separately because the VFS readdir code squashes error codes
	  into short directory reads.
 Count the number of dentries in the parent dir that point to this inode. 
	
	  If there are any blocks, read-ahead block 0 as we're almost
	  certain to have the next operation be a read there.  This is
	  how we guarantee that the parent's extent map has been loaded,
	  if there is one.
	
	  Iterate the parent dir to confirm that there is
	  exactly one entry pointing back to the inode being
	  scanned.
  Given the inode number of the alleged parent of the inode being
  scrubbed, try to validate that the parent has exactly one directory
  entry pointing back to the inode being scrubbed.
 '..' must not point to ourselves. 
	
	  If we're an unlinked directory, the parent won't have a link
	  to us.  Otherwise, it should have one link.
	
	  Grab this parent inode.  We release the inode before we
	  cancel the scrub transaction.  Since we're don't know a
	  priori that releasing the inode won't trigger eofblocks
	  cleanup (which allocates what would be a nested transaction)
	  if the parent pointer erroneously points to a file, we
	  can't use DONTCACHE here because DONTCACHE inodes can trigger
	  immediate inactive cleanup of the inode.
	 
	  If _iget returns -EINVAL or -ENOENT then the parent inode number is
	  garbage and the directory is corrupt.  If the _iget returns
	  -EFSCORRUPTED or -EFSBADCRC then the parent is corrupt which is a
	   cross referencing error.  Any other error is an operational error.
	
	  We prefer to keep the inode locked while we lock and search
	  its alleged parent for a forward reference.  If we can grab
	  the iolock, validate the pointers and we're done.  We must
	  use nowait here to avoid an ABBA deadlock on the parent and
	  the child inodes.
	
	  The game changes if we get here.  We failed to lock the parent,
	  so we're going to try to verify both pointers while only holding
	  one lock so as to avoid deadlocking with something that's actually
	  trying to traverse down the directory tree.
 Go looking for our dentry. 
 Drop the parent lock, relock this inode. 
	
	  If we're an unlinked directory, the parent won't have a link
	  to us.  Otherwise, it should have one link.  We have to re-set
	  it here because we dropped the lock on sc->ip.
 Look up '..' to see if the inode changed. 
 Drat, parent changed.  Try again! 
	
	  '..' didn't change, so check that there was only one entry
	  for us in the parent.
 Scrub a parent pointer. 
	
	  If we're a directory, check that the '..' link points up to
	  a directory that has one entry pointing to us.
 We're not a special inode, are we? 
	
	  The VFS grabs a read or write lock via i_rwsem before it reads
	  or writes to a directory.  If we've gotten this far we've
	  already obtained IOLOCK_EXCL, which (since 4.10) is the same as
	  getting a write lock on i_rwsem.  Therefore, it is safe for us
	  to drop the ILOCK here in order to do directory lookups.
 Look up '..' 
 Is this the root dir?  Then '..' must point to itself. 
	
	  We gave it our best shot but failed, so mark this scrub
	  incomplete.  Userspace can decide if it wants to try again.
	
	  If we failed to lock the parent inode even after a retry, just mark
	  this scrub incomplete and return.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 Common code for the metadata scrubbers. 
  Handling operational errors.
  The _process_error() family of functions are used to process error return
  codes from functions called as part of a scrub operation.
  If there's no error, we return true to tell the caller that it's ok
  to move on to the next check in its list.
  For non-verifier errors (e.g. ENOMEM) we return false to tell the
  caller that something bad happened, and we preserve error so that
  the caller can return the error up the stack to userspace.
  Verifier errors (EFSBADCRCEFSCORRUPTED) are recorded by setting
  OFLAG_CORRUPT in sm_flags and the error is cleared.  In other words,
  we track verifier errors (and failed scrub checks) via OFLAG_CORRUPT,
  not via return codes.  We return false to tell the caller that
  something bad happened.  Since the error has been cleared, the caller
  will (presumably) return that zero and scrubbing will move on to
  whatever's next.
  ftrace can be used to record the precise metadata location and the
  approximate code location of the failed operation.
 Check for operational errors. 
 Used to restart an op with deadlock avoidance. 
 Note the badness but don't abort. 
 Check for operational errors for a file offset. 
 Used to restart an op with deadlock avoidance. 
 Note the badness but don't abort. 
  Handling scrub corruptionoptimizationwarning checks.
  The _set_{corrupt,preen,warning}() family of functions are used to
  record the presence of metadata that is incorrect (corrupt), could be
  optimized somehow (preen), or should be flagged for administrative
  review but is not incorrect (warn).
  ftrace can be used to record the precise metadata location and
  approximate code location of the failed check.
 Record a block which could be optimized. 
  Record an inode which could be optimized.  The trace data will
  include the block given by bp if bp is given; otherwise it will use
  the block location of the inode record itself.
 Record something being wrong with the filesystem primary superblock. 
 Record a corrupt block. 
 Record a corruption while cross-referencing. 
  Record a corrupt inode.  The trace data will include the block given
  by bp if bp is given; otherwise it will use the block location of the
  inode record itself.
 Record a corruption while cross-referencing with an inode. 
 Record corruption in a block indexed by a file fork. 
 Record a corruption while cross-referencing a fork block. 
  Warn about inodes that need administrative review but is not
  incorrect.
 Warn about a block indexed by a file fork that needs review. 
 Signal an incomplete scrub. 
  rmap scrubbing -- compute the number of blocks with a given owner,
  at least according to the reverse mapping data.
  Calculate the number of blocks the rmap thinks are owned by something.
  The caller should pass us an rmapbt cursor.
  AG scrubbing
  These helpers facilitate locking an allocation group's header
  buffers, setting up cursors for all btrees that are present, and
  cleaning everything up once we're through.
 Decide if we want to return an AG header read failure. 
 Return all AG header read failures when scanning btrees. 
	
	  If we're scanning a given type of AG header, we only want to
	  see read failures from that specific header.  We'd like the
	  other headers to cross-check them, but this isn't required.
  Grab the perag structure and all the headers for an AG.
  The headers should be released by xchk_ag_free, but as a fail safe we attach
  all the buffers we grab to the scrub transaction so they'll all be freed
  when we cancel it.  Returns ENOENT if we can't grab the perag structure.
 Release all the AG btree cursors. 
 Initialize all the btree cursors for an AG. 
 Set up a bnobt cursor for cross-referencing. 
 Set up a cntbt cursor for cross-referencing. 
 Set up a inobt cursor for cross-referencing. 
 Set up a finobt cursor for cross-referencing. 
 Set up a rmapbt cursor for cross-referencing. 
 Set up a refcountbt cursor for cross-referencing. 
 Release the AG header context and btree cursors. 
  For scrub, grab the perag structure, the AGI, and the AGF headers, in that
  order.  Locking order requires us to get the AGI before the AGF.  We use the
  transaction to avoid deadlocking on crosslinked metadata buffers; either the
  caller passes one in (bmap scrub) or we have to create a transaction
  ourselves.  Returns ENOENT if the perag struct cannot be grabbed.
 Per-scrubber setup functions 
  Grab an empty transaction so that we can re-grab locked buffers if
  one of our btrees turns out to be cyclic.
  If we're going to repair something, we need to ask for the largest possible
  log reservation so that we can handle the worst case scenario for metadata
  updates while rebuilding a metadata item.  We also need to reserve as many
  blocks in the head transaction as we think we're going to need to rebuild
  the metadata object.
 Set us up with a transaction and an empty context. 
 Set us up with AG headers and btree cursors. 
	
	  If the caller asks us to checkpont the log, do so.  This
	  expensive operation should be performed infrequently and only
	  as a last resort.  Any caller that sets force_log should
	  document why they need to do so.
 Push everything out of the log onto disk. 
  Given an inode and the scrub control structure, grab either the
  inode referenced in the control structure or the inode passed in.
  The inode is not locked.
 We want to scan the inode we already had opened. 
 Look up the inode, see if the generation number matches. 
 Inode doesn't exist, just bail out. 
 Got an inode, continue. 
		
		  -EINVAL with IGET_UNTRUSTED could mean one of several
		  things: userspace gave us an inode number that doesn't
		  correspond to fs space, or doesn't have an inobt entry;
		  or it could simply mean that the inode buffer failed the
		  read verifiers.
		 
		  Try just the inode mapping lookup -- if it succeeds, then
		  the inode buffer verifier failed and something needs fixing.
		  Otherwise, we really couldn't find it so tell userspace
		  that it no longer exists.
 Set us up to scrub a file's contents. 
 Got the inode, lock it and we're ready to go. 
 scrub teardown will unlock and release the inode for us 
  Predicate that decides if we need to evaluate the cross-reference check.
  If there was an error accessing the cross-reference btree, just delete
  the cursor and skip the check.
 No point in xref if we already know we're corrupt. 
 If we've already given up on xref, just bail out. 
 xref error, delete cursor and bail out. 
	
	  Errors encountered during cross-referencing with another
	  data structure should not cause this scrubber to abort.
 Run the structure verifiers on in-memory buffers to detect bad memory. 
  Scrub the attrdata forks of a metadata inode.  The metadata inode must be
  pointed to by sc->ip and the ILOCK must be held.
 Metadata inodes don't live on the rt device. 
 They should never participate in reflink. 
 They also should never have extended attributes. 
 Invoke the data fork scrubber. 
 Look for incorrect shared blocks. 
  Try to lock an inode in violation of the usual locking order rules.  For
  example, trying to get the IOLOCK while in transaction context, or just
  plain breaking AG-order or inode-order inode locking rules.  Either way,
  the only way to avoid an ABBA deadlock is to use trylock and back off if
  we can't.
 Pause background reaping of resources. 
 Restart background reaping of resources. 
	
	  Readonly filesystems do not perform inactivation or speculative
	  preallocation, so there's no need to restart the workers.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Set us up to scrub reference count btrees.
 Reference count btree scrubber. 
  Confirming Reference Counts via Reverse Mappings
  We want to count the reverse mappings overlapping a refcount record
  (bno, len, refcount), allowing for the possibility that some of the
  overlap may come from smaller adjoining reverse mappings, while some
  comes from single extents which overlap the range entirely.  The
  outer loop is as follows:
  1. For all reverse mappings overlapping the refcount extent,
     a. If a given rmap completely overlaps, mark it as seen.
     b. Otherwise, record the fragment (in agbno order) for later
        processing.
  Once we've seen all the rmaps, we know that for all blocks in the
  refcount record we want to find $refcount owners and we've already
  visited $seen extents that overlap all the blocks.  Therefore, we
  need to find ($refcount - $seen) owners for every block in the
  extent; call that quantity $target_nr.  Proceed as follows:
  2. Pull the first $target_nr fragments from the list; all of them
     should start at or before the start of the extent.
     Call this subset of fragments the working set.
  3. Until there are no more unprocessed fragments,
     a. Find the shortest fragments in the set and remove them.
     b. Note the block number of the end of these fragments.
     c. Pull the same number of fragments from the list.  All of these
        fragments should start at the block number recorded in the
        previous step.
     d. Put those fragments in the set.
  4. Check that there are $target_nr fragments remaining in the list,
     and that they all end at or beyond the end of the refcount extent.
  If the refcount is correct, all the check conditions in the algorithm
  should always hold true.  If not, the refcount is incorrect.
 refcount extent we're examining 
 number of owners seen 
  Decide if the given rmap is large enough that we can redeem it
  towards refcount verification now, or if it's a fragment, in
  which case we'll hang onto it in the hopes that we'll later
  discover that we've collected exactly the correct number of
  fragments as the refcountbt says we should have.
 Confirm that a single-owner refc extent is a CoW stage. 
		
		  The rmap overlaps the refcount record, so we can confirm
		  one refcount owner seen.
		
		  This rmap covers only part of the refcount record, so
		  save the fragment for later processing.  If the rmapbt
		  is healthy each rmap_irec we see will be in agbno order
		  so we don't need insertion sort here.
  Given a bunch of rmap fragments, iterate through them, keeping
  a running tally of the refcount.  If this ever deviates from
  what we expect (which is the refcountbt's refcount minus the
  number of extents that totally covered the refcountbt extent),
  we have a refcountbt error.
	
	  There are (refchk->rc.rc_refcount - refchk->nr refcount)
	  references we haven't found yet.  Pull that many off the
	  fragment list and figure out where the smallest rmap ends
	  (and therefore the next rmap should start).  All the rmaps
	  we pull off should start at or before the beginning of the
	  refcount record's range.
 Make sure the fragments actually are in agbno order. 
	
	  Find all the rmaps that start at or before the refc extent,
	  and put them on the worklist.
	
	  We should have found exactly $target_nr rmap fragments starting
	  at or before the refcount extent.
 Discard any fragments ending at rbno from the worklist. 
 Try to add nr rmaps starting at rbno to the worklist. 
		
		  If we get here and nr > 0, this means that we added fewer
		  items to the worklist than we discarded because the fragment
		  list ran out of items.  Therefore, we cannot maintain the
		  required refcount.  Something is wrong, so we're done.
	
	  Make sure the last extent we processed ends at or beyond
	  the end of the refcount extent.
 Actually record us having seen the remaining refcount. 
 Delete fragments and work list. 
 Use the rmap entries covering this extent to verify the refcount. 
 Cross-reference with the rmapbt to confirm the refcount. 
 Cross-reference with the other btrees. 
 Scrub a refcountbt record. 
 Only CoW records can have refcount == 1. 
 Check the extent. 
 Make sure we have as many refc blocks as the rmap says. 
 Check that we saw as many refcbt blocks as the rmap knows about. 
 Check that we saw as many cow blocks as the rmap knows about. 
 Scrub the refcount btree for some AG. 
 xref check that a cow staging extent is marked in the refcountbt. 
 Find the CoW staging extent. 
 CoW flag must be set, refcount must be 1. 
 Must be at least as long as what was passed in 
  xref check that the extent is not shared.  Only file data blocks
  can have multiple owners.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 Set us up with the realtime metadata locked. 
 Realtime bitmap. 
 Scrub a free extent record from the realtime bitmap. 
 Make sure the entire rtbitmap file is mapped with written extents. 
 Make sure we have a written extent. 
 Scrub the realtime bitmap. 
 Is the size of the rtbitmap correct? 
 Invoke the fork scrubber. 
 Scrub the realtime summary. 
	
	  We ILOCK'd the rt bitmap ip in the setup routine, now lock the
	  rt summary ip in compliance with the rt inode locking rules.
	 
	  Since we switch sc->ip to rsumip we have to save the old ilock
	  flags so that we don't mix up the inode state that @sc tracks.
 Invoke the fork scrubber. 
 XXX: implement this some day 
 Switch back to the rtbitmap inode and lock flags. 
 xref check that the extent is not free in the rtbitmap 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 btree scrubbing 
  Check for btree operation errors.  See the section about handling
  operational errors in common.c.
 Used to restart an op with deadlock avoidance. 
 Note the badness but don't abort. 
 Record btree block corruption. 
  Make sure this record is in order and doesn't stray outside of the parent
  keys.
 If this isn't the first record, are they in order? 
 Is this at least as large as the parent low key? 
 Is this no larger than the parent high key? 
  Make sure this key is in order and doesn't stray outside of the parent
  keys.
 If this isn't the first key, are they in order? 
 Is this at least as large as the parent low key? 
 Is this no larger than the parent high key? 
  Check a btree pointer.  Returns true if it's ok to use this pointer.
  Callers do not need to set the corrupt flag.
 A btree rooted in an inode has no block pointer to the root. 
 Otherwise, check the pointers. 
 Check that a btree block's sibling matches what we expect it. 
	
	  If the pointer is null, we shouldn't be able to move the upper
	  level pointer anywhere.
 Increment upper level pointer. 
 Compare upper level pointer to sibling pointer. 
 Check the siblings of a btree block. 
 Root block should never have siblings. 
	
	  Does the left & right sibling pointers match the adjacent
	  parent level pointers?
	  (These function absorbs error codes for us.)
  Make sure this btree block isn't in the free list and that there's
  an rmap record for it.
	
	  The bnobt scrubber aliases bs->cur to bs->sc->sa.bno_cur, so we
	  have to nullify it (to shut down further block owner checks) if
	  self-xref encounters problems.
 Check the owner of a btree block. 
	
	  In theory, xfs_btree_get_block should only give us a null buffer
	  pointer for the root of a root-in-inode btree type, but we need
	  to check defensively here in case the cursor state is also screwed
	  up.
	
	  We want to cross-reference each btree block with the bnobt
	  and the rmapbt.  We cannot cross-reference the bnobt or
	  rmapbt while scanning the bnobt or rmapbt, respectively,
	  because we cannot alter the cursor and we'd prefer not to
	  duplicate cursors.  Therefore, save the buffer daddr for
	  later scanning.
 Decide if we want to check minrecs of a btree block in the inode root. 
	
	  xfs_bmap_add_attrfork_btree had an implementation bug wherein it
	  would miscalculate the space required for the data fork bmbt root
	  when adding an attr fork, and promote the iroot contents to an
	  external block unnecessarily.  This went unnoticed for many years
	  until scrub found filesystems in this state.  Inode rooted btrees are
	  not supposed to have immediate child blocks that are small enough
	  that the contents could fit in the inode root, but we can't fail
	  existing filesystems, so instead we disable the check for data fork
	  bmap btrees when there's an attr fork.
  Check that this btree block has at least minrecs records or is one of the
  special blocks that don't require that.
 More records than minrecs means the block is ok. 
	
	  For btrees rooted in the inode, it's possible that the root block
	  contents spilled into a regular ondisk block because there wasn't
	  enough space in the inode root.  The number of records in that
	  child block might be less than the standard minrecs, but that's ok
	  provided that there's only one direct child of the root.
	
	  Otherwise, only the root level is allowed to have fewer than minrecs
	  records or keyptrs.
  Grab and scrub a btree block given a btree pointer.  Returns block
  and buffer pointers (if applicable) if they're ok to use.
	
	  Check the block's owner; this function absorbs error codes
	  for us.
	
	  Check the block's siblings; this function absorbs error codes
	  for us.
  Check that the low and high keys of this block match the keys stored
  in the parent block.
 Calculate the keys for this block. 
 Obtain the parent's copy of the keys for this block. 
 Get high keys 
  Visit all nodes and leaves of a btree.  Check that all pointers and
  records are in order, that the keys reflect the records, and use a callback
  so that the caller can verify individual records.
	
	  Allocate the btree scrub context from the heap, because this
	  structure can get rather large.  Don't let a caller feed us a
	  totally absurd size.
 Initialize scrub state 
	
	  Load the root of the btree.  The helper function absorbs
	  error codes for us.
 End of leaf, pop back towards the root. 
 Records in order for scrub? 
 Call out to the record checker. 
 End of node, pop back towards the root. 
 Keys in order for scrub? 
 Drill another level deeper. 
 Process deferred owner checks on btree blocks. 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 Set us up with an inode's bmap. 
	
	  We don't want any ephemeral data fork updates sitting around
	  while we inspect block mappings, so wait for directio to finish
	  and flush dirty data if we have delalloc reservations.
		
		  Try to flush all incore state to disk before we examine the
		  space mappings for the data fork.  Leave accumulated errors
		  in the mapping for the writer threads to consume.
		 
		  On ENOSPC or EIO writeback errors, we continue into the
		  extent mapping checks because write failures do not
		  necessarily imply anything about the correctness of the file
		  metadata.  The metadata and the file data could be on
		  completely separate devices; a media failure might only
		  affect a subset of the disk, etc.  We can handle delalloc
		  extents in the scrubber, so leaving them in memory is fine.
 Got the inode, lock it and we're ready to go. 
 scrub teardown will unlock and release the inode 
  Inode fork block mapping (BMBT) scrubber.
  More complex than the others because we have to scrub
  all the extents regardless of whether or not the fork
  is in btree format.
 Look for a corresponding rmap for this irec. 
	
	  CoW staging extents are owned (on disk) by the refcountbt, so
	  their rmaps do not have offsets.
	
	  If the caller thinks this could be a shared bmbt extent (IOWs,
	  any data fork extent of a reflink inode) then we have to use the
	  range rmap lookup to make sure we get the correct owneroffset.
	
	  Otherwise, use the (faster) regular lookup.
 Make sure that we have rmapbt records for this extent. 
 Find the rmap record for this irec. 
 Check the rmap. 
	
	  Check the logical offsets if applicable.  CoW staging extents
	  don't track logical offsets since the mappings only exist in
	  memory.
	
	  Check for discrepancies between the unwritten flag in the irec and
	  the rmap.  Note that the (in-memory) CoW fork distinguishes between
	  unwritten and written extents, but we don't track that in the rmap
	  records because the blocks are owned (on-disk) by the refcountbt,
	  which doesn't track unwritten state.
 Cross-reference a single rtdev extent record. 
 Cross-reference a single datadev extent record. 
  Directories and attr forks should never have blocks that can't be addressed
  by a xfs_dablk_t.
 Scrub a single extent record. 
	
	  Check for out-of-order extents.  This record could have come
	  from the incore list, for which there is no ordering check.
 There should never be a "hole" extent in either extent list. 
	
	  Check for delalloc extents.  We never iterate the ones in the
	  in-core extent scan, and we should never see these in the bmbt.
 Make sure the extent points to a valid place. 
 We don't allow unwritten extents on attr forks. 
 Scrub a bmbt record. 
	
	  Check the owners of the btree blocks up to the level below
	  the root since the verifiers don't do that.
	
	  Check that the incore extent tree contains an extent that matches
	  this one exactly.  We validate those cached bmaps later, so we don't
	  need to check them here.  If the incore extent tree was just loaded
	  from disk by the scrubber, we assume that its contents match what's
	  on disk (we still hold the ILOCK) and skip the equivalence check.
 Scan the btree records. 
 Load the incore bmap cache if it's not loaded. 
 Check the btree structure. 
 Can we find bmaps that fit this rmap? 
 Is this even the right fork? 
 Now look up the bmbt record. 
	
	  bmap extent record lengths are constrained to 2^21 blocks in length
	  because of space constraints in the on-disk metadata structure.
	  However, rmap extent record lengths are constrained only by AG
	  length, so we have to loop through the bmbt to make sure that the
	  entire rmap is covered by bmbt records.
 Make sure each rmap has a corresponding bmbt entry. 
 Make sure each rmap has a corresponding bmbt entry. 
 Don't support realtime rmap checks yet. 
	
	  Only do this for complex maps that are in btree format, or for
	  situations where we would seem to have a size but zero extents.
	  The inode repair code can zap broken iforks, which means we have
	  to flag this bmap as corrupt if there are rmaps that need to be
	  reattached.
  Scrub an inode fork's block mappings.
  First we scan every record in every btree block, if applicable.
  Then we unconditionally scan the incore extent cache.
 Non-existent forks can be ignored. 
 No CoW forks on non-reflink inodesfilesystems. 
 Check the fork values 
 No mappings to check. 
 Find the offset of the last extent in the mapping. 
 Scrub extent records. 
 Scrub an inode's data fork. 
 Scrub an inode's attr fork. 
 Scrub an inode's CoW fork. 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 Figure out which block the btree cursor was pointing to. 
  We include this last to have the helpers above available for the trace
  event implementations.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2017 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Set us up to scrub free space btrees.
 Free space btree scrubber. 
  Ensure there's a corresponding cntbtbnobt record matching this
  bnobtcntbt record, respectively.
 Cross-reference with the other btrees. 
 Scrub a bnobtcntbt record. 
 Scrub the freespace btrees for some AG. 
 xref check that the extent is not free 
 SPDX-License-Identifier: GPL-2.0 
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  Copyright (c) 2018 Red Hat, Inc.
  All rights reserved.
  Passive reference counting access wrappers to the perag structures.  If the
  per-ag structure is to be freed, the freeing code is responsible for cleaning
  up objects with passive references before freeing the structure. This is
  things like cached buffers.
  search from @first to find the next perag with the given tag set.
  xfs_initialize_perag_data
  Read in each per-ag structure so we can count up the number of
  allocated inodes, free inodes and used filesystem blocks as this
  information is no longer persistent in the superblock. Once we have
  this information, write it into the in-core superblock structure.
		
		  read the agf, then the agi. This gets us
		  all the information we need and populates the
		  per-ag structures for us.
	
	  If the new summary counts are obviously incorrect, fail the
	  mount operation because that implies the AGFs are also corrupt.
	  Clear FS_COUNTERS so that we don't unmount with a dirty log, which
	  will prevent xfs_repair from fixing anything.
 Overwrite incore superblock counters with just-read data 
  Free up the per-ag resources associated with the mount structure.
	
	  Walk the current per-ag tree so we don't try to initialise AGs
	  that already exist (growfs case). Allocate and insert all the
	  AGs we don't find ready for initialisation.
 Place kernel structure only init below this point. 
 __KERNEL__ 
 first new pag is fully initialized 
 unwind any prior newly initialized pags 
  Generic btree root block init function
 Finish initializing a free space btree. 
			
			  Modify first record to pad stripe align of log
			
			  Insert second record at start of internal log
			  which then gets trimmed.
		
		  Change record start to after the internal log
	
	  Calculate the record block count and check for the case where
	  the log might have consumed all available space in the AG. If
	  so, reset the record count to 0 to avoid exposure of an invalid
	  record start block.
  Alloc btree root block init functions
  Reverse map root block init
	
	  mark the AG header regions as static metadata The BNO
	  btree block is the first block after the headers, so
	  it's location defines the size of region the static
	  metadata consumes.
	 
	  Note: unlike mkfs, we never have to account for log
	  space when growing the data regions
 account freespace btree root blocks 
 account inode btree root blocks 
 account for rmap btree root 
 account for refc btree root 
 account for the log space 
  Initialise new secondary superblocks with the pre-grow geometry, but mark
  them as "in progress" so we know they haven't yet been activated. This will
  get cleared when the update with the new geometry information is done after
  changes to the primary are committed. This isn't strictly necessary, but we
  get it for free with the delayed buffer write lists and it means we can tell
  if a grow operation didn't complete properly after the fact.
  Prepare new AG headers to be written to disk. We use uncached buffers here,
  as it is assumed these new AG headers are currently beyond the currently
  valid filesystem address space. Using cached buffers would trip over EOFS
  corruption detection alogrithms in the buffer cache lookup routines.
  This is a non-transactional function, but the prepared buffers are added to a
  delayed write buffer list supplied by the caller so they can submit them to
  disk and wait on them as required.
 SB 
 AGF 
 AGFL 
 AGI 
 BNO root block 
 CNT root block 
 INO root block 
 FINO root block 
 RMAP root block 
 REFC root block 
 NULL terminating block 
 Account for AG free space in new AG 
 some extra paranoid checks before we shrink the ag 
	
	  Make sure that the last inode cluster cannot overlap with the new
	  end of the AG, even if it's sparse.
	
	  Disable perag reservations so it doesn't cause the allocation request
	  to fail. We'll reestablish reservation before we return.
 internal log shouldn't also show up in the free space btrees 
		
		  if extent allocation fails, need to roll the transaction to
		  ensure that the AGFL fixup has been committed anyway.
	
	  if successfully deleted from freespace btrees, need to confirm
	  per-AG reservation works as expected.
		
		  Roll the transaction before trying to re-init the per-ag
		  reservation. The new transaction is clean so it will cancel
		  without any side effects.
  Extent the AG indicated by the @id by the length passed in
	
	  Change the agi length.
	
	  Change agf length.
	
	  Free the new space.
	 
	  XFS_RMAP_OINFO_SKIP_UPDATE is used here to tell the rmap btree that
	  this doesn't actually exist in the rmap btree.
 Retrieve AG geometry. 
 Lock the AG headers. 
 Fill out form. 
 Release resources. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Btree magic numbers.
 Ensure we asked for crc for crc-only magics. 
  Check a long btree block header.  Return the address of the failing check,
  or NULL if everything is ok.
 Check a long btree block header. 
  Check a short btree block header.  Return the address of the failing check,
  or NULL if everything is ok.
 Check a short btree block header. 
  Debug routine: check that block header is ok.
 btree cursor 
 generic btree block pointer 
 level of the btree block 
 buffer containing block, if any 
 Check that this long pointer is valid and points within the fs. 
 Check that this short pointer is valid and points within the AG. 
  Check that a given (indexed) btree pointer at a certain level of a
  btree is valid and doesn't point past where it should.
  Calculate CRC on the whole btree block and stuff it into the
  long-form btree header.
  Prior to calculting the CRC, pull the LSN out of the buffer log item and put
  it into the buffer so recovery knows what the last modification was that made
  it to disk.
  Calculate CRC on the whole btree block and stuff it into the
  short-form btree header.
  Prior to calculting the CRC, pull the LSN out of the buffer log item and put
  it into the buffer so recovery knows what the last modification was that made
  it to disk.
  Delete the btree cursor.
 btree cursor 
 del because of error 
 btree level 
	
	  Clear the buffer pointers and release the buffers. If we're doing
	  this because of an error, inspect all of the entries in the bc_bufs
	  array for buffers to be unlocked. This is because some of the btree
	  code works from level n down to 0, and if we get an error along the
	  way we won't have initialized all the entries down to 0.
  Duplicate the btree cursor.
  Allocate a new one, copy the record, re-get the buffers.
 error 
 input cursor 
 output cursor 
 btree block's buffer pointer 
 error return value 
 level number of btree block 
 mount structure for filesystem 
 new cursor value 
 transaction pointer, can be NULL 
	
	  Allocate a new cursor like the old one.
	
	  Copy the record currently in the cursor.
	
	  For each level current, re-get the buffer and copy the ptr value.
  XFS btree block layout and addressing:
  There are two types of blocks in the btree: leaf and non-leaf blocks.
  The leaf record start with a header then followed by records containing
  the values.  A non-leaf block also starts with the same header, and
  then first contains lookup keys followed by an equal number of pointers
  to the btree blocks at the previous level.
 		+--------+-------+-------+-------+-------+-------+-------+
  Leaf:	| header | rec 1 | rec 2 | rec 3 | rec 4 | rec 5 | rec N |
 		+--------+-------+-------+-------+-------+-------+-------+
 		+--------+-------+-------+-------+-------+-------+-------+
  Non-Leaf:	| header | key 1 | key 2 | key N | ptr 1 | ptr 2 | ptr N |
 		+--------+-------+-------+-------+-------+-------+-------+
  The header is called struct xfs_btree_block for reasons better left unknown
  and comes in different versions for short (32bit) and long (64bit) block
  pointers.  The record and key structures are defined by the btree instances
  and opaque to the btree core.  The block pointers are simple disk endian
  integers, available in a short (32bit) and long (64bit) variant.
  The helpers below calculate the offset of a given record, key or pointer
  into a btree block (xfs_btree__offset) or return a pointer to the given
  record, key or pointer (xfs_btree__addr).  Note that all addressing
  inside the btree block is done using indices starting at one, not zero!
  If XFS_BTREE_OVERLAPPING is set, then this btree supports keys containing
  overlapping intervals.  In such a tree, records are still sorted lowest to
  highest and indexed by the smallest key value that refers to the record.
  However, nodes are different: each pointer has two associated keys -- one
  indexing the lowest key available in the block(s) below (the same behavior
  as the key in a regular btree) and another indexing the highest key
  available in the block(s) below.  Because records are not sorted by the
  highest key, all leaf block updates require us to compute the highest key
  that matches any record in the leaf and to recursively update the high keys
  in the nodes going further up in the tree, if necessary.  Nodes look like
  this:
 		+--------+-----+-----+-----+-----+-----+-------+-------+-----+
  Non-Leaf:	| header | lo1 | hi1 | lo2 | hi2 | ... | ptr 1 | ptr 2 | ... |
 		+--------+-----+-----+-----+-----+-----+-------+-------+-----+
  To perform an interval query on an overlapped tree, perform the usual
  depth-first search and use the low and high keys to decide if we can skip
  that particular node.  If a leaf node is reached, return the records that
  intersect the interval.  Note that an interval query may return numerous
  entries.  For a non-overlapped tree, simply search for the record associated
  with the lowest key and iterate forward until a non-matching record is
  found.  Section 14.3 ("Interval Trees") of _Introduction to Algorithms_ by
  Cormen, Leiserson, Rivest, and Stein (2nd or 3rd ed. only) discuss this in
  more detail.
  Why do we care about overlapping intervals?  Let's say you have a bunch of
  reverse mapping records on a reflink filesystem:
  1: +- file A startblock B offset C length D -----------+
  2:      +- file E startblock F offset G length H --------------+
  3:      +- file I startblock F offset J length K --+
  4:                                                        +- file L... --+
  Now say we want to map block (B+D) into file A at offset (C+D).  Ideally,
  we'd simply increment the length of record 1.  But how do we find the record
  that ends at (B+D-1) (i.e. record 1)?  A LE lookup of (B+D-1) would return
  record 3 because the keys are ordered first by startblock.  An interval
  query would return records 1 and 2 because they both overlap (B+D-1), and
  from that we can pick out record 1 as the appropriate left neighbor.
  In the non-overlapped case you can do a LE lookup and decrement the cursor
  because a record's interval must end before the next record.
  Return size of the btree block header for this btree instance.
  Return size of btree block pointers for this btree instance.
  Calculate offset of the n-th record in a btree block.
  Calculate offset of the n-th key in a btree block.
  Calculate offset of the n-th high key in a btree block.
  Calculate offset of the n-th block pointer in a btree block.
  Return a pointer to the n-th record in the btree block.
  Return a pointer to the n-th key in the btree block.
  Return a pointer to the n-th high key in the btree block.
  Return a pointer to the n-th block pointer in the btree block.
  Get the root block which is stored in the inode.
  For now this btree implementation assumes the btree root is always
  stored in the if_broot field of an inode fork.
  Retrieve the block pointer from the cursor at the given level.
  This may be an inode btree root or from a buffer.
 generic btree block pointer 
 btree cursor 
 level in btree 
 buffer containing the block 
  Change the cursor to point to the first record at the given level.
  Other levels are unaffected.
 success=1, failure=0 
 btree cursor 
 level to change 
 generic btree block pointer 
 buffer containing block 
	
	  Get the block pointer for this level.
	
	  It's empty, there is no such record.
	
	  Set the ptr value to 1, that's the first recordkey.
  Change the cursor to point to the last record in the current block
  at the given level.  Other levels are unaffected.
 success=1, failure=0 
 btree cursor 
 level to change 
 generic btree block pointer 
 buffer containing block 
	
	  Get the block pointer for this level.
	
	  It's empty, there is no such record.
	
	  Set the ptr value to numrecs, that's the last recordkey.
  Compute first and last byte offsets for the fields given.
  Interprets the offsets table, which contains struct field offsets.
 bitmask of fields 
 table of field offsets 
 number of bits to inspect 
 output: first byte offset 
 output: last byte offset 
 current bit number 
 mask for current bit number 
	
	  Find the lowest bit, so the first byte offset.
	
	  Find the highest bit, so the last byte offset.
  Get a buffer for the block, return it read in.
  Long-form addressing.
 file system mount point 
 transaction pointer 
 file system block number 
 buffer for fsbno 
 ref count value for buffer 
 return value 
 real disk block address 
  Read-ahead the block, don't wait for it, don't return a buffer.
  Long-form addressing.
 ARGSUSED 
 file system mount point 
 file system block number 
 count of filesystem blocks 
  Read-ahead the block, don't wait for it, don't return a buffer.
  Short-form addressing.
 ARGSUSED 
 file system mount point 
 allocation group number 
 allocation group block number 
 count of filesystem blocks 
  Read-ahead btree blocks, at the given level.
  Bits in lr are set from XFS_BTCUR_{LEFT,RIGHT}RA.
 btree cursor 
 level in btree 
 leftright bits 
	
	  No readahead needed if we are at the root level and the
	  btree root is stored in the inode.
  Readahead @count btree blocks at the given @ptr location.
  We don't need to care about long or short form btrees here as we have a
  method of converting the ptr directly to a daddr available to us.
  Set the buffer for level "lev" in the cursor to bp, releasing
  any previous buffer.
 btree cursor 
 level in btree 
 new buffer to set 
 btree block 
  Getsetinit sibling pointers
 owner is a 32 bit value on short blocks 
	
	  we can pull the owner from the cursor right now as the different
	  owners align directly with the pointer size of the btree. This may
	  change in future, but is safe for current users of the generic btree
	  code.
  Return true if ptr is the last record in the btree and
  we need to track updates to this record.  The decision
  will be further refined in the update_lastrec method.
  Read in the buffer at the given ptr and return the buffer and
  the block pointer within the buffer.
 need to sort out how callers deal with failures first 
  Copy keys from one btree block to another.
  Copy records from one btree block to another.
  Copy block pointers from one btree block to another.
  Shift keys one index leftright inside a single btree block.
  Shift records one index leftright inside a single btree block.
  Shift block pointers one index leftright inside a single btree block.
  Log key values from the btree block.
  Log record values from the btree block.
  Log block pointer fields from a btree block (nonleaf).
 btree cursor 
 buffer containing btree block 
 index of first pointer to log 
 index of last pointer to log 
  Log fields from a btree block header.
 btree cursor 
 buffer containing btree block 
 mask of fields: XFS_BB_... 
 first byte offset logged 
 last byte offset logged 
 table of offsets (short) 
 table of offsets (long) 
			
			  We don't log the CRC when updating a btree
			  block but instead recreate it during log
			  recovery.  As the log buffers have checksums
			  of their own this is safe and avoids logging a crc
			  update in a lot of places.
  Increment cursor by one record at the level.
  For nonzero levels the leaf-ward information is untouched.
 error 
 successfailure 
 error return value 
 Read-ahead to the right at this level. 
 Get a pointer to the btree block. 
 We're done if we remain in the block after the increment. 
 Fail if we just went off the right edge of the tree. 
	
	  March up the tree incrementing pointers.
	  Stop when we don't go off the right edge of a block.
 Read-ahead the right block for the next loop. 
	
	  If we went off the root then we are either seriously
	  confused or have the tree root in an inode.
	
	  Now walk back down the tree, fixing up the cursor's buffer
	  pointers and key numbers.
  Decrement cursor by one record at the level.
  For nonzero levels the leaf-ward information is untouched.
 error 
 successfailure 
 error return value 
 Read-ahead to the left at this level. 
 We're done if we remain in the block after the decrement. 
 Get a pointer to the btree block. 
 Fail if we just went off the left edge of the tree. 
	
	  March up the tree decrementing pointers.
	  Stop when we don't go off the left edge of a block.
 Read-ahead the left block for the next loop. 
	
	  If we went off the root then we are seriously confused.
	  or the root of the tree is in an inode.
	
	  Now walk back down the tree, fixing up the cursor's buffer
	  pointers and key numbers.
 btree cursor 
 level in the btree 
 ptr to btree block 
 return btree block 
 buffer pointer for btree block 
 special case the root block if in an inode 
	
	  If the old buffer at this level for the disk address we are
	  looking for re-use it.
	 
	  Otherwise throw it away and get a new one.
 Check the inode owner since the verifiers don't. 
 Did we get the level we were looking for? 
 Check that internal nodes have at least one record. 
  Get current search key.  For level 0 we don't actually have a key
  structure so we make one up from the record.  For all other levels
  we just return the right key.
  Lookup the record.  The cursor is made to point to it, based on dir.
  stat is set to 0 if can't find any such record, 1 for success.
 error 
 btree cursor 
 <=, ==, or >= 
 successfailure 
 current btree block 
 difference for the current key 
 error return value 
 current key number 
 level in the btree 
 ptr to btree block 
 ptr to btree block 
 No such thing as a zero-level tree. 
 initialise start pointer from cursor 
	
	  Iterate over each level in the btree, starting at the root.
	  For each level above the leaves, find the key we need, based
	  on the lookup record, then follow the corresponding block
	  pointer down to the next level.
 Get the block we need to do the lookup on. 
			
			  If we already had a key match at a higher level, we
			  know we need to use the first entry in this block.
 Otherwise search this block. Do a binary search. 
 high entry number 
 low entry number 
 Set low and high entry numbers, 1-based. 
 Block is empty, must be an empty leaf. 
 Binary search the block. 
 keyno is average of low and high. 
 Get current search key 
				
				  Compute difference to get next direction:
				   - less than, move right
				   - greater than, move left
				   - equal, we're done
		
		  If there are more levels, set up for the next level
		  by getting the block number and filling in the cursor.
			
			  If we moved left, need the previous key number,
			  unless there isn't one.
 Done with the search. See if we need to adjust the results. 
		
		  If ge search and we went off the end of the block, but it's
		  not the last block, we're in the wrong block.
 Return if we succeeded or not. 
 Find the high key storage area from a regular key. 
 Determine the low (and high if overlapped) keys of a leaf block 
 Determine the low (and high if overlapped) keys of a node block 
 Derive the keys for any btree block. 
  Decide if we need to update the parent keys of a btree block.  For
  a standard btree this is only necessary if we're updating the first
  recordkey.  For an overlapping btree, we must always update the
  keys because the highest key can be in any of the records or keys
  in the block.
  Update the low and high parent keys of the given level, progressing
  towards the root.  If force_all is false, stop if the keys for a given
  level do not need updating.
 keys from current level 
 keys from the next level up 
 keys from the next level up 
 Exit if there aren't any parent levels to update. 
 Update all the keys from some level in cursor back to the root. 
  Update the parent keys of the given level, progressing towards the root.
	
	  Go up the tree from this level toward the root.
	  At each level, update the key value to the value input.
	  Stop when we reach a level where the cursor isn't pointing
	  at the first entry in the block.
  Update the record referred to by cur to the value in the
  given record. This either works (return 0) or gets an
  EFSCORRUPTED error.
 Pick up the current block. 
 Get the address of the rec to be updated. 
 Fill in the new contents and log them. 
	
	  If we are tracking the last record in the tree and
	  we are at the far right edge of the tree, update it.
 Pass new key value up to our parent. 
  Move 1 record left from curlevel if possible.
  Update cur to reflect the new path.
 error 
 successfailure 
 left buffer pointer 
 left btree block 
 left record count 
 right buffer pointer 
 right btree block 
 temporary btree cursor 
 right record count 
 left btree pointer 
 right btree key 
 right address pointer 
 right record pointer 
 error return value 
 Set up variables for this block as "right". 
 If we've got no left sibling then we can't shift an entry left. 
	
	  If the cursor entry is the one that would be moved, don't
	  do it... it's too complicated.
 Set up the left neighbor as "left". 
 If it's full, it can't take another entry. 
	
	  We add one entry to the left side and remove one for the right side.
	  Account for it here, the changes will be updated on disk and logged
	  later.
	
	  If non-leaf, copy a key and a ptr to the left block.
	  Log the changes to the left block.
 It's a non-leaf.  Move keys and pointers. 
 left btree key 
 left address pointer 
 It's a leaf.  Move records.  
 left record pointer 
	
	  Slide the contents of right down one entry.
 It's a nonleaf. operate on keys and ptrs 
 It's a leaf. operate on records 
	
	  Using a temporary cursor, update the parent key values of the
	  block on the left.
 Update the parent high keys of the left block, if needed. 
 Update the parent keys of the right block. 
 Slide the cursor value left one. 
  Move 1 record right from curlevel if possible.
  Update cur to reflect the new path.
 error 
 successfailure 
 left buffer pointer 
 left btree block 
 right buffer pointer 
 right btree block 
 temporary btree cursor 
 right block pointer 
 right btree key 
 right record count 
 left record count 
 error return value 
 loop counter 
 Set up variables for this block as "left". 
 If we've got no right sibling then we can't shift an entry right. 
	
	  If the cursor entry is the one that would be moved, don't
	  do it... it's too complicated.
 Set up the right neighbor as "right". 
 If it's full, it can't take another entry. 
	
	  Make a hole at the start of the right neighbor block, then
	  copy the last left block entry to the hole.
 It's a nonleaf. make a hole in the keys and ptrs 
 Now put the new data in, and log it. 
 It's a leaf. make a hole in the records 
 Now put the new data in, and log it. 
	
	  Decrement and log left's numrecs, bump and log right's numrecs.
	
	  Using a temporary cursor, update the parent key values of the
	  block on the right.
 Update the parent high keys of the left block, if needed. 
 Update the parent keys of the right block. 
  Split curlevel block in half.
  Return new block number and the key to its first
  record (to be inserted into parent).
 error 
 successfailure 
 left sibling block ptr 
 left buffer pointer 
 left btree block 
 right sibling block ptr 
 right buffer pointer 
 right btree block 
 right-right sibling ptr 
 right-right buffer pointer 
 right-right btree block 
 error return value 
 Set up left block (current one). 
 Allocate the new block. If we can't do it, we're toast. Give up. 
 Set up the new block as "right". 
 Fill in the btree header for the new right block. 
	
	  Split the entries between the old and the new block evenly.
	  Make sure that if there's an odd number of entries now, that
	  each new block will have the same number of entries.
 Adjust numrecs for the later get__keys() calls. 
	
	  Copy btree block entries from the left block over to the
	  new block, the right. Update the right block and log the
	  changes.
 It's a non-leaf.  Move keys and pointers. 
 left btree key 
 left address pointer 
 right btree key 
 right address pointer 
 Copy the keys & pointers to the new block. 
 Stash the keys of the new block for later insertion. 
 It's a leaf.  Move records.  
 left record pointer 
 right record pointer 
 Copy records to the new block. 
 Stash the keys of the new block for later insertion. 
	
	  Find the left block number by looking in the buffer.
	  Adjust sibling pointers.
	
	  If there's a block to the new block's right, make that block
	  point back to right instead of to left.
 Update the parent high keys of the left block, if needed. 
	
	  If the cursor is really in the right block, move it there.
	  If it's just pointing past the last entry in left, then we'll
	  insert there, so don't change anything in that case.
	
	  If there are more levels, we'll need another cursor which refers
	  the right block, no matter where this cursor was.
 successfailure 
 allocation in kswapd context 
  Stack switching interfaces for allocation
	
	  we are in a transaction context here, but may also be doing work
	  in kswapd context, and hence we may need to inherit that state
	  temporarily to ensure that we don't block waiting for memory reclaim
	  in any way.
	
	  Do not access args after complete() has run here. We don't own args
	  and the owner may run and free args before we return here.
  BMBT split requests often come in with little stack to work on. Push
  them off to a worker thread so there is lots of stack to use. For the other
  btree types, just call directly to avoid the context switch overhead here.
 error 
 successfailure 
 __KERNEL__ 
  Copy the old inode root contents into a real block and make the
  broot point to it.
 error 
 btree cursor 
 logging flags for inode 
 return status - 0 fail 
 buffer for cblock 
 btree block 
 child btree block 
 child key pointer 
 child ptr pointer 
 pointer to btree key 
 pointer to block addr 
 new block addr 
 btree level 
 error return code 
 loop counter 
 Allocate the new block. If we can't do it, we're toast. Give up. 
 Copy the root into a real block. 
	
	  we can't just memcpy() the root in for CRC enabled btree blocks.
	  In that case have to also ensure the blkno remains correct
	
	  Do all this logging at the end so that
	  the root is at the right level.
  Allocate a new root block, fill it in.
 error 
 btree cursor 
 successfailure 
 one half of the old root block 
 buffer containing block 
 error return value 
 left buffer pointer 
 left btree block 
 new (root) buffer 
 new (root) btree block 
 new value for key index, 1 or 2 
 right buffer pointer 
 right btree block 
 initialise our start point from the cursor 
 Allocate the new block. If we can't do it, we're toast. Give up. 
 Set up the new block. 
 Set the root in the holding structure  increasing the level by 1. 
	
	  At the previous root level there are now two blocks: the old root,
	  and the new block generated when it was split.  We don't know which
	  one the cursor is pointing at, so we set up variables "left" and
	  "right" for each case.
 Our block is left, pick up the right block. 
 Our block is right, pick up the left block. 
 Fill in the new block's btree header and log it. 
 Fill in the key data in the new root. 
		
		  Get the keys for the left block's keys and put them directly
		  in the parent block.  Do the same for the right block.
		
		  Get the keys for the left block's records and put them
		  directly in the parent block.  Do the same for the right
		  block.
 Fill in the pointer data in the new root. 
 Fix up the cursor. 
 btree cursor 
 btree level 
 # of recs in block 
 old tree index 
 new tree index 
 new btree ptr 
 new btree cursor 
 key of new block 
 A root block that can be made bigger. 
 A root block that needs replacing 
 First, try shifting an entry to the right neighbor. 
 Next, try shifting an entry to the left neighbor. 
	
	  Next, try splitting the current block in half.
	 
	  If this works we have to re-set our variables because we
	  could be in a different block now.
  Insert one recordlevel.  Return information to the caller
  allowing the next level up to proceed if necessary.
 btree cursor 
 level to insert record at 
 io: block number inserted 
 record to insert 
 io: block key for ptrp 
 output: new cursor replacing cur 
 successfailure 
 btree block 
 buffer for block 
 new block ptr 
 new btree cursor 
 new block key 
 old keyrecord index 
 keyrecord index 
 number of records 
 error return value 
	
	  If we have an external root pointer, and we've made it to the
	  root level, allocate a new root block and we're done.
 If we're off the left edge, return failure. 
 Get pointers to the btree buffer and block. 
 Check that the new entry is being inserted in the right place. 
	
	  If the block is full, we can't insert the new entry until we
	  make the block un-full.
	
	  The current block may have changed if the block was
	  previously full and we have just made space in it.
	
	  At this point we know there's room for our new entry in the block
	  we're pointing at.
 It's a nonleaf. make a hole in the keys and ptrs 
 Now put the new data in, bump numrecs and log it. 
 It's a leaf. make a hole in the records 
 Now put the new data in, bump numrecs and log it. 
 Log the new number of records in the btree header. 
	
	  If we just inserted into a new tree block, we have to
	  recalculate nkey here because nkey is out of date.
	 
	  Otherwise we're just updating an existing block (having shoved
	  some records into the new tree block), so use the regular key
	  update mechanism.
	
	  If we are tracking the last record in the tree and
	  we are at the far right edge of the tree, update it.
	
	  Return the new block number, if any.
	  If there is one, give back a record value and a cursor too.
  Insert the record at the point referenced by cur.
  A multi-level split of the tree on insert will invalidate the original
  cursor.  All callers of this function should assume that the cursor is
  no longer valid and revalidate it.
 error return value 
 result value, 0 for failure 
 current level number in btree 
 new block number (split result) 
 new cursor (split result) 
 previous level's cursor 
 key of block to insert 
 record to insert 
 Make a key out of the record data to be inserted, and save it. 
	
	  Loop going up the tree, starting at the leaf level.
	  Stop when we don't get a split block, that must mean that
	  the insert is finished with this level.
		
		  Insert nrecnptr into this level of the tree.
		  Note if we fail, nptr will be null.
		
		  See if the cursor we just used is trash.
		  Can't trash the caller's cursor, but otherwise we should
		  if ncur is a new cursor or we're about to be done.
 Save the state from the cursor before we trash it 
 If we got a new cursor, switch to it. 
  Try to merge a non-leaf block back into the inode root.
  Note: the killroot names comes from the fact that we're effectively
  killing the old root block.  But because we can't just delete the
  inode we have to copy the single block it was pointing to into the
  inode.
	
	  Don't deal with the root block needs to be a leaf case.
	  We're just going to turn the thing back into extents anyway.
	
	  Give up if the root has multiple children.
	
	  Only do this if the next level will fit.
	  Then the data must be copied up to the inode,
	  instead of freeing the root you free the next level.
  Kill the current root node, and replace it with it's only child node.
	
	  Update the root pointer, decreasing the level by 1 and then
	  free the old root.
  Single level of the btree record deletion routine.
  Delete record pointed to by curlevel.
  Remove the record from its block then rebalance the tree.
  Return 0 for error, 1 for done, 2 to go on to the next level.
 error 
 btree cursor 
 level removing record from 
 faildonego-on 
 btree block 
 current block ptr 
 buffer for block 
 error return value 
 loop counter 
 left sibling block ptr 
 left buffer pointer 
 left btree block 
 left record count 
 keyrecord index 
 right sibling block ptr 
 right buffer pointer 
 right btree block 
 right-right btree block 
 right-right buffer pointer 
 right record count 
 temporary btree cursor 
 temporary numrec count 
 Get the index of the entry being deleted, check for nothing there. 
 Get the buffer & block containing the record or keyptr. 
 Fail if we're off the end of the block. 
 Excise the entries being deleted. 
 It's a nonleaf. operate on keys and ptrs 
 It's a leaf. operate on records 
	
	  Decrement and log the number of entries in the block.
	
	  If we are tracking the last record in the tree and
	  we are at the far right edge of the tree, update it.
	
	  We're at the root level.  First, shrink the root block in-memory.
	  Try to get rid of the next level down.  If we can't then there's
	  nothing left to do.
		
		  If this is the root level, and there's only one entry left,
		  and it's NOT the leaf level, then we can get rid of this
		  level.
			
			  pp is still set to the first pointer in the block.
			  Make it the new root of the btree.
	
	  If we deleted the leftmost entry in the block, update the
	  key values above us in the tree.
	
	  If the number of records remaining in the block is at least
	  the minimum, we're done.
	
	  Otherwise, we have to move some records around to keep the
	  tree balanced.  Look at the left and right sibling blocks to
	  see if we can re-balance by moving only one record.
		
		  One child of root, need to get a chance to copy its contents
		  into the root and delete it. Can't go up to next level,
		  there's nothing to delete there.
	
	  Duplicate the cursor so our btree manipulations here won't
	  disrupt the next level up.
	
	  If there's a right sibling, see if it's ok to shift an entry
	  out of it.
		
		  Move the temp cursor to the last entry in the next block.
		  Actually any entry but the first would suffice.
 Grab a pointer to the block. 
 Grab the current block number, for future use. 
		
		  If right block is full enough so that removing one entry
		  won't make it too empty, and left-shifting an entry out
		  of right to us works, we're done.
		
		  Otherwise, grab the number of records in right for
		  future reference, and fix up the temp cursor to point
		  to our block again (last record).
	
	  If there's a left sibling, see if it's ok to shift an entry
	  out of it.
		
		  Move the temp cursor to the first entry in the
		  previous block.
 Grab a pointer to the block. 
 Grab the current block number, for future use. 
		
		  If left block is full enough so that removing one entry
		  won't make it too empty, and right-shifting an entry out
		  of left to us works, we're done.
		
		  Otherwise, grab the number of records in right for
		  future reference.
 Delete the temp cursor, we're done with it. 
 If here, we need to do a join to keep the tree balanced. 
		
		  Set "right" to be the starting block,
		  "left" to be the left neighbor.
	
	  If that won't work, see if we can join with the right neighbor block.
		
		  Set "left" to be the starting block,
		  "right" to be the right neighbor.
	
	  Otherwise, we can't fix the imbalance.
	  Just return.  This is probably a logic error, but it's not fatal.
	
	  We're now going to join "left" and "right" by moving all the stuff
	  in "right" to "left" and deleting "right".
 It's a non-leaf.  Move keys and pointers. 
 left btree key 
 left address pointer 
 right btree key 
 right address pointer 
 It's a leaf.  Move records.  
 left record pointer 
 right record pointer 
	
	  Fix up the number of records and right block pointer in the
	  surviving block, and log it.
 If there is a right sibling, point it to the remaining block. 
 Free the deleted block. 
	
	  If we joined with the left neighbor, set the buffer in the
	  cursor to the left block, and fix up the index.
	
	  If we joined with the right neighbor and there's a level above
	  us, increment the cursor at that level.
	
	  Readjust the ptr at this level if it's not a leaf, since it's
	  still pointing at the deletion point, which makes the cursor
	  inconsistent.  If this makes the ptr 0, the caller fixes it up.
	  We can't use decrement because it would change the next level up.
	
	  We combined blocks, so we have to update the parent keys if the
	  btree supports overlapped intervals.  However,
	  bc_levels[level + 1].ptr points to the old block so that the caller
	  knows which record to delete.  Therefore, the caller must be savvy
	  enough to call updkeys for us if we return stat == 2.  The other
	  exit points from this function don't require deletions further up
	  the tree, so they can call updkeys directly.
 Return value means the next level up has something to do. 
  Delete the record pointed to by cur.
  The cursor refers to the place where the record was (could be inserted)
  when the operation returns.
 error 
 successfailure 
 error return value 
	
	  Go up the tree, starting at leaf level.
	 
	  If 2 is returned then a join was done; go to the next level.
	  Otherwise we are done.
	
	  If we combined blocks as part of deleting the record, delrec won't
	  have updated the parent high keys so we have to do that here.
  Get the data from the pointed-to record.
 error 
 btree cursor 
 output: btree record 
 output: successfailure 
 btree block 
 buffer pointer 
 record number 
 error return value 
	
	  Off the right end or left end, return failure.
	
	  Point to the record and extract its data.
 Visit a block in a btree. 
 do right sibling readahead 
 process the block 
 now read rh sibling block for next iteration 
 Visit every block in a btree. 
 for each level 
 grab the left hand block 
 readahead the left most block for the next level down 
 save for the next iteration of the loop 
 for each buffer in the level 
  Change the owner of a btree.
  The mechanism we use here is ordered buffer logging. Because we don't know
  how many buffers were are going to need to modify, we don't really want to
  have to make transaction reservations for the worst case of every buffer in a
  full size btree as that may be more space that we can fit in the log....
  We do the btree walk in the most optimal manner possible - we have sibling
  pointers so we can just walk all the blocks on each level from left to right
  in a single pass, and then move to the next level and do the same. We can
  also do readahead on the sibling pointers to get IO moving more quickly,
  though for slow disks this is unlikely to make much difference to performance
  as the amount of CPU work we have to do before moving to the next block is
  relatively small.
  For each btree block that we load, modify the owner appropriately, set the
  buffer as an ordered buffer and log it appropriately. We need to ensure that
  we mark the region we change dirty so that if the buffer is relogged in
  a subsequent transaction the changes we make here as an ordered buffer are
  correctly relogged in that transaction.  If we are in recovery context, then
  just queue the modified buffer as delayed write buffer so the transaction
  recovery completion writes the changes to disk.
 modify the owner 
	
	  If the block is a root block hosted in an inode, we might not have a
	  buffer pointer here and we shouldn't attempt to log the change as the
	  information is already held in the inode and discarded when the root
	  block is formatted into the on-disk inode fork. We still change it,
	  though, so everything is consistent in memory.
 Verify the v5 fields of a long-format btree block. 
 Verify a long-format btree block. 
 numrecs verification 
 sibling pointer verification 
  xfs_btree_sblock_v5hdr_verify() -- verify the v5 fields of a short-format
 				      btree block
  @bp: buffer containing the btree block
  xfs_btree_sblock_verify() -- verify a short-format btree block
  @bp: buffer containing the btree block
  @max_recs: maximum records allowed in this btree node
 numrecs verification 
 sibling pointer verification 
  For the given limits on leaf and keyptr records per block, calculate the
  height of the tree needed to index the number of leaf records.
  For the given limits on leaf and keyptr records per block, calculate the
  number of blocks needed to index the given number of leaf records.
  Given a number of available blocks for the btree to consume with records and
  pointers, calculate the height of the tree needed to index all the records
  that space can hold based on the number of pointers each interior node
  holds.
  We start by assuming a single level tree consumes a single block, then track
  the number of blocks each node level consumes until we no longer have space
  to store the next node level. At this point, we are indexing all the leaf
  blocks in the space, and there's no more free space to split the tree any
  further. That's our maximum btree height.
  Query a regular btree for all records overlapping a given interval.
  Start with a LE lookup of the key of low_rec and return all records
  until we find a record with a key greater than the key of high_rec.
	
	  Find the leftmost record.  The btree cursor must be set
	  to the low record used to generate low_key.
 Nothing?  See if there's anything to the right. 
 Find the record. 
 Skip if high_key(rec) < low_key. 
 Stop if high_key < low_key(rec). 
 Callback 
 Move on to the next record. 
  Query an overlapped interval btree for all records overlapping a given
  interval.  This function roughly follows the algorithm given in
  "Interval Trees" of _Introduction to Algorithms_, which is section
  14.3 in the 2nd and 3rd editions.
  First, generate keys for the low and high records passed in.
  For any leaf node, generate the high and low keys for the record.
  If the record keys overlap with the query lowhigh keys, pass the
  record to the function iterator.
  For any internal node, compare the low and high keys of each
  pointer against the query lowhigh keys.  If there's an overlap,
  follow the pointer.
  As an optimization, we stop scanning a block when we find a low key
  that is greater than the query's high key.
 Load the root of the btree. 
 End of node, pop back towards the root. 
 Handle a leaf node. 
			
			  If (record's high key >= query's low key) and
			     (query's high key >= record's low key), then
			  this record overlaps the query range; callback.
 Record is larger than high key; pop. 
 Handle an internal node. 
		
		  If (pointer's high key >= query's low key) and
		     (query's high key >= pointer's low key), then
		  this record overlaps the query range; follow pointer.
 The low key is larger than the upper range; pop. 
	
	  If we don't end this function with the cursor pointing at a record
	  block, a subsequent non-error cursor deletion will not release
	  node-level buffers, causing a buffer leak.  This is quite possible
	  with a zero-results range query, so release the buffers if we
	  failed to return any results.
  Query a btree for all records overlapping a given interval of keys.  The
  supplied function will be called with each record found; return one of the
  XFS_BTREE_QUERY_RANGE_{CONTINUE,ABORT} values or the usual negative error
  code.  This function returns -ECANCELED, zero, or a negative error code.
 Find the keys of both ends of the interval. 
 Enforce low key < high key. 
 Query a btree for all records. 
 Count the blocks in a btree and return the result in blocks. 
 Compare two btree pointers. 
 If there's an extent, we're done. 
 Is there a record covering a given range of keys? 
 Are there more records in this btree? 
 There are still records in this block. 
 There are more record blocks. 
 Set up all the btree cursor caches. 
 Destroy all the btree cursor caches, if they've been allocated. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.
  Copyright (c) 2013 Red Hat, Inc.
  All Rights Reserved.
  Pointer to an entry's tag word.
  The number of leaf entries is limited by the size of the block and the amount
  of space used by the data entries.  We don't know how much space is used by
  the data entries yet, so just ensure that the count falls somewhere inside
  the block right now.
  Check the consistency of the data block.
  The input can also be a block-format directory.
  Return NULL if the buffer is good, otherwise the address of the error.
 incore inode pointer 
 data block's buffer 
 addr for leaf lookup 
 bestfree table 
 block tail 
 count of entries found 
 data block header 
 bestfree entry 
 mask of bestfrees seen 
 hash of current name 
 leaf index 
 last entry was unused 
 block leaf entries 
 count of stale leaves 
	
	  If this isn't a directory, something is seriously wrong.  Bail out.
	
	  Account for zero bestfree entries.
	
	  Loop over the dataunused entries.
		
		  If it's unused, look for the space in the bestfree table.
		  If we find it, account for that, else make sure it
		  doesn't need to be there.
		
		  It's a real entry.  Validate the fields.
		  If this is a block directory then make sure it's
		  in the leaf section of the block.
		  The linear search is crude but this is DEBUG code.
	
	  Need to have seen all the entries and all the bestfree slots.
  Readahead of the first block of the directory when it is opened is completely
  oblivious to the format of the directory. Hence we can either get a block
  format buffer or a data format buffer on readahead.
 Check things that we can't do in the verifier. 
  Find the bestfree entry that exactly coincides with unused directory space
  or a verifier error because the bestfree data are bad.
	
	  Validate some consistency in the bestfree table.
	  Check order, non-overlapping entries, and if we find the
	  one we're looking for it has to be exact.
 Looks ok so far; now try to match up with a bestfree entry. 
  Given a data block and an unused entry from that block,
  return the bestfree entry if any that corresponds to it.
 data block header 
 bestfree table pointer 
 unused space 
 bestfree entry 
 offset value needed 
	
	  If this is smaller than the smallest bestfree entry,
	  it can't be there since they're sorted.
	
	  Look at the three bestfree entries for our guy.
	
	  Didn't find it.  This only happens if there are duplicate lengths.
  Insert an unused-space entry into the bestfree table.
 entry inserted 
 data block pointer 
 bestfree table pointer 
 unused space 
 log the data header (out) 
 new bestfree entry 
	
	  Insert at position 0, 1, or 2; or not at all.
  Remove a bestfree entry from the table.
 data block header 
 bestfree table pointer 
 bestfree entry pointer 
 out: log data header 
	
	  It's the first entry, slide the next 2 up.
	
	  It's the second entry, slide the 3rd entry up.
	
	  Must be the last entry.
	
	  Clear the 3rd entry, must be zero now.
  Given a data block, reconstruct its bestfree map.
	
	  Start by clearing the table.
		
		  If it's a free entry, insert it.
		
		  For active entries, check their tags and skip them.
  Initialize a data block at the given block number in the directory.
  Give back the buffer for the created block.
 error 
 directory operation args 
 logical dir block number 
 output block buffer 
	
	  Get the buffer set up for the block.
	
	  Initialize the header.
	
	  Set up an unused entry for the block's body.
	
	  Log it and return it.
  Log an active data entry from the block.
 data entry pointer 
  Log a data block header.
  Log a data unused entry.
 data unused pointer 
	
	  Log the first part of the unused entry.
	
	  Log the end (tag) of the unused entry.
  Make a byte range in the data block unused.
  Its current contents are unimportant.
 starting byte offset 
 length in bytes 
 out: log header 
 out: regen bestfree 
 data block pointer 
 bestfree pointer 
 need to regen bestfree 
 new unused entry 
 unused entry after us 
 unused entry before us 
	
	  Figure out where the end of the data area is.
	
	  If this isn't the start of the block, then back up to
	  the previous entry and see if it's free.
 tag just before us 
	
	  If this isn't the end of the block, see if the entry after
	  us is free.
	
	  Previous and following entries are both free,
	  merge everything into a single free entry.
 another bestfree pointer 
		
		  See if prevdup andor postdup are in bestfree table.
		
		  We need a rescan unless there are exactly 2 free entries
		  namely our two.  Then we know what's happening, otherwise
		  since the third bestfree is there, there might be more
		  entries.
		
		  Fix up the new big freespace.
			
			  Has to be the case that entries 0 and 1 are
			  dfp and dfp2 (don't know which is which), and
			  entry 2 is empty.
			  Remove entry 1 first then entry 0.
			
			  Now insert the new entry.
	
	  The entry before us is free, merge with it.
		
		  If the previous entry was in the table, the new entry
		  is longer, so it will be in the table too.  Remove
		  the old one and add the new one.
		
		  Otherwise we need a scan if the new entry is big enough.
	
	  The following entry is free, merge with it.
		
		  If the following entry was in the table, the new entry
		  is longer, so it will be in the table too.  Remove
		  the old one and add the new one.
		
		  Otherwise we need a scan if the new entry is big enough.
	
	  Neither neighbor is free.  Make a new entry.
 Check our free data for obvious signs of corruption. 
 Sanity-check a new bestfree entry. 
  Take a byte range out of an existing unused space and make it un-free.
 unused entry 
 starting offset to use 
 length to use 
 out: need to log header 
 out: need regen bestfree 
 data block header 
 bestfree pointer 
 new unused entry 
 another new unused entry 
 matches end of freespace 
 matches start of freespace 
 need to regen bestfree 
 old unused entry's length 
	
	  Look up the entry in the bestfree table.
	
	  Check for alignment with front and back of the entry.
	
	  If we matched it exactly we just need to get rid of it from
	  the bestfree table.
	
	  We match the first part of the entry.
	  Make a new entry with the remaining freespace.
		
		  If it was in the table, remove it and add the new one.
			
			  If we got inserted at the last slot,
			  that means we don't know if there was a better
			  choice for the last slot, or not.  Rescan.
	
	  We match the last part of the entry.
	  Trim the allocated space off the tail of the entry.
		
		  If it was in the table, remove it and add the new one.
			
			  If we got inserted at the last slot,
			  that means we don't know if there was a better
			  choice for the last slot, or not.  Rescan.
	
	  Poking out the middle of an entry.
	  Make two new entries.
		
		  If the old entry was in the table, we need to scan
		  if the 3rd entry was valid, since these entries
		  are smaller than the old one.
		  If we don't need to scan that means there were 1 or 2
		  entries in the table, and removing the old and adding
		  the 2 new will work.
 Find the end of the entry data in a datablock format dir block. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  Copyright (c) 2013 Red Hat, Inc.
  All Rights Reserved.
  xfs_da_btree.c
  Routines to implement directories as Btrees of hashed names.
========================================================================
  Function prototypes for the kernel.
  Routines used for growing the Btree.
  Routines used for shrinking the Btree.
  Utility routines.
 anchor for dirattr state 
  Allocate a dir-state structure.
  We don't put them on the stack since they're large.
  Kill the altpath contents of a da-state structure.
  Free a da-state structure.
 DEBUG 
  Verify an xfs_da3_blkinfo structure. Note that the da3 fields are only
  accessible on v5 filesystems. This header format is common across da node,
  attr leaf and dir leaf blocks.
	
	  we don't know if the node is for and attribute or directory tree,
	  so only fail if the count is outside both bounds
 XXX: hash order check? 
  leafnode format detection on trees is sketchy, so a node read can be done on
  leaf level blocks when detection identifies the tree as a node format tree
  incorrectly. In this case, we need to swap the verifier to match the correct
  format of the block being read.
 Verify the structure of a da3 block. 
========================================================================
  Routines used for growing the Btree.
  Create the initial contents of an intermediate node.
  Split a leaf node, rebalance, then possibly split
  intermediate nodes, rebalance, etc.
 error 
	
	  Walk back up the tree splittinginsertingadjusting as necessary.
	  If we need to insert and there isn't room, split the node, then
	  decide which fragment to insert the new block from below into.
	  Note that we may split the root this way, but we need more fixup.
 initial dummy value 
		
		  If a leaf node then
		      Allocate a new leaf node, then rebalance across them.
		  else if an intermediate node then
		      We split on the last layer, must we split the node?
 GROT: attr is inconsistent 
			
			  Entry wouldn't fit, split the leaf again. The new
			  extrablk will be consumed by xfs_da3_node_split if
			  the node is split.
 before newblk 
 after newblk 
 GROT: attr inconsistent 
 GROT: dir is inconsistent 
			
			  Record the newly split block for the next time thru?
		
		  Update the btree to show the new hashval for this child.
	
	  xfs_da3_node_split() should have consumed any extra blocks we added
	  during a double leaf split in the attr fork. This is guaranteed as
	  we can't be here if the attr fork only has a single leaf block.
	
	  Split the root node.
	
	  Update pointers to the node which used to be block 0 and just got
	  bumped because of the addition of a new root node.  Note that the
	  original block 0 could be at any position in the list of blocks in
	  the tree.
	 
	  Note: the magic numbers and sibling pointers are in the same physical
	  place for both v2 and v3 headers (by design). Hence it doesn't matter
	  which version of the xfs_da_intnode structure we use here as the
	  result will be the same using either structure.
  Split the root.  We have to create a new root and point to the two
  parts (the split old root) that we just created.  Copy block zero to
  the EOF, extending the inode in process.
 error 
	
	  Copy the existing (incorrect) block from the root node position
	  to a free space somewhere.
		
		  we are about to copy oldroot to bp, so set up the type
		  of bp while we know exactly what it will be.
		
		  we are about to copy oldroot to bp, so set up the type
		  of bp while we know exactly what it will be.
	
	  we can copy most of the information in the node from one block to
	  another, but for CRC enabled headers we have to make sure that the
	  block specific identifiers are kept intact. We update the buffer
	  directly for this.
	
	  Set up the new root node.
 Header is already logged by xfs_da_node_create 
  Split the node, rebalance, then add the new entry.
 error 
	
	  With V2 dirs the extra block is data or freespace.
	
	  Do we have to split the node?
		
		  Allocate a new node, add to the doubly linked chain of
		  nodes, then move some of our excess entries into it.
 GROT: dir is inconsistent 
 GROT: dir is inconsistent 
	
	  Insert the new entry(s) into the correct block
	  (updating last hashval in the process).
	 
	  xfs_da3_node_add() inserts BEFORE the given index,
	  and as a result of using node_lookup_int() we always
	  point to a valid entry (not after one), but a split
	  operation always results in a new block whose hashvals
	  FOLLOW the current block.
	 
	  If we had double-split op below us, then add the extra block too.
  Balance the btree elements between two intermediate nodes,
  usually one full and one empty.
  NOTE: if blk2 is empty, then it will get the upper half of blk1.
	
	  Figure out how many entries need to move, and in which direction.
	  Swap the nodes around if that makes it simpler.
	
	  Two cases: high-to-low and low-to-high.
		
		  Move elements in node2 up to make a hole.
		
		  Move the req'd B-tree elements from high in node1 to
		  low in node2.
		
		  Move the req'd B-tree elements from low in node2 to
		  high in node1.
		
		  Move elements in node2 down to fill the hole.
	
	  Log header of node 1 and all current bits of node 2.
	
	  Record the last hashval from each block for upward propagation.
	  (note: don't use the swapped node pointers)
	
	  Adjust the expected index for insertion.
 make it invalid 
  Add a new entry to an intermediate node.
	
	  We may need to make some room before we insert the new node.
	
	  Copy the last hash value from the oldblk to propagate upwards.
========================================================================
  Routines used for shrinking the Btree.
  Deallocate an empty leaf node, remove it from its parent,
  possibly deallocating that block, etc...
	
	  Walk back up the tree joiningdeallocating as necessary.
	  When we stop dropping blocks, break out.
		
		  See if we can combine the block with a neighbor.
		    (action == 0) => no options, just leave
		    (action == 1) => coalesce, then unlink
		    (action == 2) => block empty, unlink it
			
			  Remove the offending node, fixup hashvals,
			  check for a toosmall neighbor.
	
	  We joined all the way to the top.  If it turns out that
	  we only have one entry in the root, make the child block
	  the new root.
 !DEBUG 
 !DEBUG 
  We have only one entry in the root.  Copy the only remaining child of
  the old root to block 0 as the new root node.
	
	  If the root has more than one child, then don't do anything.
	
	  Read in the (only) child block, then copy those bytes into
	  the root block's buffer and free the original child block.
	
	  This could be copying a leaf back into the root block in the case of
	  there only being a single leaf block left in the tree. Hence we have
	  to update the b_ops pointer as well to match the buffer type change
	  that could occur. For dir3 blocks we also need to update the block
	  number in the buffer header.
  Check a node block and its neighbors to see if the block should be
  collapsed into one or the other neighbor.  Always keep the block
  with the smaller block number.
  If the current block is over 50% full, don't try to join it, return 0.
  If the block is empty, fill in the state structure and return 2.
  If it can be collapsed, fill in the state structure and return 1.
  If nothing can be done, return 0.
	
	  Check for the degenerate case of the block being over 50% full.
	  If so, it's not worth even looking to see if we might be able
	  to coalesce with a sibling.
 blk over 50%, don't try to join 
 blk over 50%, don't try to join 
	
	  Check for the degenerate case of the block being empty.
	  If the block is empty, we'll simply delete it, no need to
	  coalesce it with a sibling block.  We choose (arbitrarily)
	  to merge with the forward block unless it is NULL.
		
		  Make altpath point to the block we want to keep and
		  path point to the block we want to drop (this one).
	
	  Examine each sibling block to see if we can coalesce with
	  at least 25% free space to spare.  We need to figure out
	  whether to merge with the forward or the backward block.
	  We prefer coalescing with the lower numbered sibling so as
	  to shrink a directory over time.
 start with smaller blk num 
 fits with at least 25% to spare 
	
	  Make altpath point to the block we want to keep (the lower
	  numbered block) and path point to the block we want to drop.
  Pick up the last hashvalue from an intermediate node.
  Walk back up the tree adjusting hash values as necessary,
  when we stop making changes, return.
  Remove an entry from an intermediate node.
	
	  Copy over the offending entry, or just zero it out.
	
	  Copy the last hash value from the block to propagate upwards.
  Unbalance the elements between two intermediate nodes,
  move all Btree elements from one node into another.
	
	  If the dying block has lower hashvals, then move all the
	  elements in the remaining block up to make a hole.
 XXX: check this - is memmove dst correct? 
	
	  Move all the B-tree elements from drop_blk to save_blk.
	
	  Save the last hashval in the remaining block for upward propagation.
========================================================================
  Routines used for finding things in the Btree.
  Walk down the Btree looking for a particular filename, filling
  in the state structure as we go.
  We will set the state structure to point to each of the elements
  in each of the nodes where either the hashval is or should be.
  We support duplicate hashval's so for each entry in the current
  node that could contain the desired hashval, descend.  This is a
  pruned depth-first tree search.
 error 
	
	  Descend thru the B-tree searching each level for the right
	  node to use, until the right hashval is found.
		
		  Read the next node down in the tree.
		
		  Search an intermediate node for a match.
 Tree taller than we can handle; bail out! 
 Check the level from the root. 
		
		  Binary search.  (note: small blocks will skip loop)
		
		  Since we may have duplicate hashval's, find the first
		  matching hashval in the node.
		
		  Pick the right block to descend on.
 We can't point back to the root. 
	
	  A leaf block that ends in the hashval that we are interested in
	  (final hashval == search hashval) means that the next block may
	  contain more entries with the same hashval, shift upward to the
	  next leaf and keep searching.
 path_shift() gives ENOENT 
========================================================================
  Utility routines.
  Compare two intermediate nodes for "order".
  Link a new block into a doubly linked list of blocks (of whatever type).
 error 
	
	  Set up environment.
	
	  Link blocks in appropriate order.
		
		  Link new block in before existing block.
		
		  Link new block in after existing block.
  Unlink a block from a doubly linked list of blocks.
 error 
	
	  Set up environment.
	
	  Unlink the leaf block from the doubly linked chain of leaves.
  Move a path "forward" or "!forward" one block at the current level.
  This routine will adjust a "path" to point to the next block
  "forward" (higher hashvalues) or "!forward" (lower hashvals) in the
  Btree, including updating pointers to the intermediate nodes between
  the new bottom and the root.
 error 
	
	  Roll up the Btree looking for the first block where our
	  current index is not at the edge of the block.  Note that
	  we skip the bottom layer because we want the sibling block.
 skip bottom layer in path 
 we're out of our tree 
	
	  Roll down the edge of the subtree until we reach the
	  same depth we were at originally.
		
		  Read the next child block into a local buffer.
		
		  Release the old block (if it's dirty, the trans doesn't
		  actually let go) and swap the local buffer into the path
		  structure. This ensures failure of the above read doesn't set
		  a NULL buffer in an active slot in the path.
		
		  Note: we flatten the magic number to a single type so we
		  don't have to compare against crcnon-crc types elsewhere.
========================================================================
  Utility routines.
  Implement a simple hash on a character string.
  Rotate the hash value by 7 bits, then XOR each character in.
  This is implemented with some source-level loop unrolling.
	
	  Do four characters at a time as long as we can.
	
	  Now do the rest of the characters.
 case 0: 
	
	  Find a spot in the file space to put the new block.
	
	  Try mapping it in one filesystem block.
		
		  If we didn't get it and the block might work if fragmented,
		  try without the CONTIG flag.  Loop until we get it all.
	
	  Count the blocks we got, make sure it matches the total.
 account for newly allocated blocks in reserved blocks total 
  Add a block to the btree ahead of the file.
  Return the new block number to the caller.
  Ick.  We need to always be able to remove a btree block, even
  if there's no space reservation because the filesystem is full.
  This is called if xfs_bunmapi on a btree block fails due to ENOSPC.
  It swaps the target block with the last block in the file.  The
  last block in the file can always be removed since it can't cause
  a bmap btree split to do that.
	
	  Read the last block in the btree space.
	
	  Copy the last block into the dead buffer and log it.
	
	  Get values from the moved block.
	
	  If the moved block has a left sibling, fix up the pointers.
	
	  If the moved block has a right sibling, fix up the pointers.
	
	  Walk down the tree looking for the parent of the moved block.
	
	  We're in the right parent block.
	  Look for the right entry.
	
	  Update the parent entry pointing to the moved block.
  Remove a btree block from a directory or attribute.
		
		  Remove extents.  If we get ENOSPC for a dir we have to move
		  the last block to the place we want to kill.
	
	  Use the caller provided map for the single map case, else allocate a
	  larger one that needs to be free by the caller.
 Caller ok with no mapping. 
  Get a buffer for the dirattr block.
  Get a buffer for the dirattr block, fill in the contents.
  Readahead the dirattr block.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2001,2005 Silicon Graphics, Inc.
  All Rights Reserved.
 Allocate the new block from the freelist. If we can't, give up.  
  Update the longest extent in the AGF
		
		  If this is the last leaf block and it's the last record,
		  then update the size of the longest extent in the AG.
	
	  The perag may not be attached during grow operations or fully
	  initialized from the AGF during log recovery. Therefore we can only
	  check against maximum tree depth from those contexts.
	 
	  Otherwise check against the per-tree limit. Peek at one of the
	  verifier magic values to determine the type of tree we're verifying
	  against.
 Allocate most of a new allocation btree cursor. 
 take a reference for the cursor 
  Allocate a new allocation btree cursor.
 new alloc btree cursor 
 file system mount point 
 transaction pointer 
 buffer for agf structure 
 btree identifier 
 Create a free space btree cursor with a fake root for staging. 
  Install a new free space btree root.  Caller is responsible for invalidating
  and freeing the old btree blocks.
 Calculate number of records in an alloc btree block. 
  Calculate number of records in an alloc btree block.
 Free space btrees are at their largest when every other block is free. 
 Compute the max possible height for free space btrees. 
 Calculate the freespace btree size for some records. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2003,2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Convert on-disk form of btree root to in-memory form.
  Extract the blockcount field from an on disk bmap extent record.
  Extract the startoff field from a disk format bmap extent record.
  Set all the fields in a bmap extent record from the uncompressed form.
  Convert in-memory form of btree root to on-disk form.
	
	  Copy the firstblock, dfops, and flags values,
	  since init cursor doesn't get them.
 block allocation args 
 error return value 
		
		  Make sure there is sufficient room left in the AG to
		  complete a full tree split for an extent insert.  If
		  we are converting the middle part of an extent then
		  we may need space for two tree splits.
		 
		  We are relying on the caller to make the correct block
		  reservation for this operation to succeed.  If the
		  reservation amount is insufficient then we may fail a
		  block allocation here and corrupt the filesystem.
		
		  Could not find an AG with enough free space to satisfy
		  a full btree split.  Try again and if
		  successful activate the lowspace algorithm.
  Get the maximum records we could store in the on-disk format.
  For non-root nodes this is equivalent to xfs_bmbt_get_maxrecs, but
  for the root node this checks the available space in the dinode fork
  so that we can resize the in-memory buffer to match it.  After a
  resize to the maximum size this function returns the same value
  as xfs_bmbt_get_maxrecs for the root node, too.
	
	  Note: This routine previously casted a and b to int64 and subtracted
	  them to generate a result.  This lead to problems if b was the
	  "maximum" key value (all ones) being signed incorrectly, hence this
	  somewhat less efficient version.
		
		  XXX: need a better way of verifying the owner here. Right now
		  just make sure there has been one set.
	
	  numrecs and level verification.
	 
	  We don't know what fork we belong to, so just verify that the level
	  is less than the maximum of the two. Later checks will be more
	  precise.
  Allocate a new bmap btree cursor.
 new bmap btree cursor 
 file system mount point 
 transaction pointer 
 inode owning the btree 
 data or attr fork 
 Calculate number of records in a block mapping btree block. 
  Calculate number of records in a bmap btree block.
 Compute the max possible height for block mapping btrees. 
 One extra level for the inode root. 
  Calculate number of records in a bmap btree inode root.
  Change the owner of a btree format fork fo the inode passed in. Change it to
  the owner of that is passed in so that we can change owners before or after
  we switch forks between inodes. The operation that the caller is doing will
  determine whether is needs to change owner before or after the switch.
  For demand paged transactional modification, the fork switch should be done
  after reading in all the blocks, modifying them and pinning them in the
  transaction. For modification when the buffers are already pinned in memory,
  the fork switch can be done before changing the owner as we won't need to
  validate the owner until the btree buffers are unpinned and writes can occur
  again.
  For recovery based ownership change, there is no transactional context and
  so a buffer list must be supplied so that we can record the buffers that we
  modified for the caller to issue IO on.
 Calculate the bmap btree size for some records. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  XFS bit manipulation routines, used in non-realtime code.
  Return whether bitmap is empty.
  Size is number of words in the bitmap, which is padded to word boundary
  Returns 1 for empty, 0 for non-empty.
  Count the number of contiguous bits set in the bitmap starting with bit
  start_bit.  Size is the size of the bitmap in words.
 set to one first offset bits prior to start 
  This takes the bit number to start looking from and
  returns the next set bit from there.  It returns -1
  if there are no more bits set or the start bit is
  beyond the end of the bitmap.
  Size is the number of words, not bytes, in the bitmap.
 set to zero first offset bits prior to start 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  xfs_attr.c
  Provide the external interfaces to manage attribute lists.
========================================================================
  Function prototypes for the kernel.
  Internal routines when attribute list fits inside the inode.
  Internal routines when attribute list is one block.
  Internal routines when attribute list is more than one block.
  Returns true if the there is exactly only block in the attr fork, in which
  case the attribute fork consists of a single leaf block entry.
========================================================================
  Overall external interface routines.
  Retrieve an extended attribute and its value.  Must have ilock.
  Returns 0 on successful retrieval, otherwise an error.
  Retrieve an extended attribute by name, and its value if requested.
  If args->valuelen is zero, then the caller does not want the value, just an
  indication whether the attribute exists and the size of the value if it
  exists. The size is returned in args.valuelen.
  If args->value is NULL but args->valuelen is non-zero, allocate the buffer
  for the value after existence of the attribute has been determined. The
  caller always has to free args->value if it is set, no matter if this
  function was successful or not.
  If the attribute is found, but exceeds the size limit set by the caller in
  args->valuelen, return -ERANGE with the size of the attribute that was found
  in args->valuelen.
 Entirely possible to look up a name which doesn't exist 
  Calculate how many blocks we need for the new attribute,
	
	  Determine space new attribute will use, and if it would be
	  "local" or "remote" (note: local != inline).
 Double split possible 
		
		  Out of line attribute, cannot double split, but
		  make room for the attribute value itself.
	
	  Build initial attribute list (if required).
	
	  Commit the shortform mods, and we're done.
	  NOTE: this is also the error path (EEXIST, etc).
  Check to see if the attr should be upgraded from non-existent or shortform to
  single-leaf-block attribute list.
  Checks to see if a delayed attribute transaction should be rolled.  If so,
  transaction is finished or rolled as needed.
		
		  The caller wants us to finish all the deferred ops so that we
		  avoid pinning the log tail with a large number of deferred
		  ops.
  Set the attribute specified in @args.
	
	  Try to add the attr to the attribute list in the inode.
 Should only be 0, -EEXIST or -ENOSPC 
	
	  It won't fit in the shortform, transform to a leaf block.  GROT:
	  another possible req'mt for a double-split btree op.
	
	  Prevent the leaf buffer from being unlocked so that a concurrent AIL
	  push cannot grab the half-baked leaf buffer and run into problems
	  with the write verifier.
	
	  We're still in XFS_DAS_UNINIT state here.  We've converted
	  the attr fork to leaf format and will restart with the leaf
	  add.
  Set the attribute specified in @args.
  This routine is meant to function as a delayed operation, and may return
  -EAGAIN when the transaction needs to be rolled.  Calling functions will need
  to handle this, and recall the function until a successful error code is
  returned.
 State machine switch 
		
		  If the fork is shortform, attempt to add the attr. If there
		  is no space, this converts to leaf format and returns
		  -EAGAIN with the leaf buffer held across the roll. The caller
		  will deal with a transaction roll error, but otherwise
		  release the hold once we return with a clean transaction.
				
				  Finish any deferred work items and roll the
				  transaction once more.  The goal here is to
				  call node_addname with the inode and
				  transaction in the same state (inode locked
				  and joined, transaction clean) no matter how
				  we got to this step.
				 
				  At this point, we are still in
				  XFS_DAS_UNINIT, but when we come back, we'll
				  be a node, so we'll fall down into the node
				  handling code below
		
		  If there was an out-of-line value, allocate the blocks we
		  identified for its storage and copy the value.  This is done
		  after we create the attribute so that we don't overflow the
		  maximum size of a transaction andor hit a deadlock.
 Open coded xfs_attr_rmtval_set without trans handling 
		
		  Repeat allocating remote blocks for the attr value until
		  blkcnt drops to zero.
		
		  If this is not a rename, clear the incomplete flag and we're
		  done.
		
		  If this is an atomic rename operation, we must "flip" the
		  incomplete flags on the "new" and "old" attributevalue pairs
		  so that one disappears and one appears atomically.  Then we
		  must remove the "old" attributevalue pair.
		 
		  In a separate transaction, set the incomplete flag on the
		  "old" attr and clear the incomplete flag on the "new" attr.
		
		  Commit the flag value change and start the next trans in
		  series.
		
		  Dismantle the "old" attributevalue pair by removing a
		  "remote" value (if it exists).
 Set state in case xfs_attr_rmtval_remove returns -EAGAIN 
		
		  This is the last step for leaf format. Read the block with
		  the old attr, remove the old attr, check for shortform
		  conversion and return.
 bp is gone due to xfs_da_shrink_inode 
		
		  Find space for remote blocks and fall into the allocation
		  state.
		
		  If there was an out-of-line value, allocate the blocks we
		  identified for its storage and copy the value.  This is done
		  after we create the attribute so that we don't overflow the
		  maximum size of a transaction andor hit a deadlock.
		
		  If this was not a rename, clear the incomplete flag and we're
		  done.
		
		  If this is an atomic rename operation, we must "flip" the
		  incomplete flags on the "new" and "old" attributevalue pairs
		  so that one disappears and one appears atomically.  Then we
		  must remove the "old" attributevalue pair.
		 
		  In a separate transaction, set the incomplete flag on the
		  "old" attr and clear the incomplete flag on the "new" attr.
		
		  Commit the flag value change and start the next trans in
		  series
		
		  Dismantle the "old" attributevalue pair by removing a
		  "remote" value (if it exists).
 Set state in case xfs_attr_rmtval_remove returns -EAGAIN 
		
		  The last state for node format. Look up the old attr and
		  remove it.
  Return EEXIST if attr is found, or ENOATTR if not
  Remove the attribute specified in @args.
  Note: If args->value is NULL the attribute will be removed, just like the
  Linux ->setattr API.
	
	  We have no control over the attribute names that userspace passes us
	  to remove, so we have to allow the name lookup prior to attribute
	  removal to fail as well.
		
		  If the inode doesn't have an attribute fork, add one.
		  (inode must not be locked when we call this routine)
	
	  Root fork attributes can use reserved data blocks for this
	  operation if necessary
 shortform attribute has already been committed 
	
	  If this is a synchronous mount, make sure that the
	  transaction goes to disk before returning to the user.
	
	  Commit the last in the sequence of transactions.
========================================================================
  External routines when attribute list is inside the inode
  Add a name to the shortform attribute list structure
  This is the external routine.
		
		  Since we have removed the old attr, clear ATTR_REPLACE so
		  that the leaf format add routine won't trip over the attr
		  not being around.
========================================================================
  External routines when attribute list is one block
 Store info about a remote block 
 Set stored info about a remote block 
  Tries to add an attribute to an inode in leaf form
  This function is meant to execute as part of a delayed operation and leaves
  the transaction handling to the caller.  On success the attribute is added
  and the inode and transaction are left dirty.  If there is not enough space,
  the attr data is converted to node format and -ENOSPC is returned. Caller is
  responsible for handling the dirty inode and transaction or adding the attr
  in node format.
	
	  Look up the given attribute in the leaf block.  Figure out if
	  the given flags produce an error or call for an atomic rename.
 save the attribute state for later removal
 an atomic rename 
		
		  clear the remote attr state now that it is saved so that the
		  values reflect the state of the attribute we are about to
		  add, not the attribute we just found and will remove later.
	
	  Add the attribute to the leaf block
  Return EEXIST if attr is found, or ENOATTR if not
  Remove a name from the leaf attribute list structure
  This leaf block cannot have a "remote" value, we only call this routine
  if bmap_one_block() says there is only one block (ie: no remote blks).
	
	  Remove the attribute.
	
	  If the result is small enough, shrink it all into the inode.
 bp is gone due to xfs_da_shrink_inode 
  Look up a name in a leaf attribute list structure.
  This leaf block cannot have a "remote" value, we only call this routine
  if bmap_one_block() says there is only one block (ie: no remote blks).
  Returns 0 on successful retrieval, otherwise an error.
  Return EEXIST if attr is found, or ENOATTR if not
  statep: If not null is set to point at the found state.  Caller will
          be responsible for freeing the state in this case.
	
	  Search to see if name exists, and get back a pointer to it.
========================================================================
  External routines when attribute list size > geo->blksize
	
	  Search to see if name already exists, and get back a pointer
	  to where it should go.
 save the attribute state for later removal
 atomic rename op 
		
		  clear the remote attr state now that it is saved so that the
		  values reflect the state of the attribute we are about to
		  add, not the attribute we just found and will remove later.
  Add a name to a Btree-format attribute list.
  This will involve walking down the Btree, and may involve splitting
  leaf nodes and even splitting intermediate nodes up to and including
  the root node (a special case of an intermediate node).
  "Remote" attribute values confuse the issue and atomic rename operations
  add a whole extra layer of confusion on top of that.
  This routine is meant to function as a delayed operation, and may return
  -EAGAIN when the transaction needs to be rolled.  Calling functions will need
  to handle this, and recall the function until a successful error code is
 returned.
			
			  Its really a single leaf node, but it had
			  out-of-line values so it looked like it might
			  have been a b-tree.
			
			  Now that we have converted the leaf to a node, we can
			  roll the transaction, and try xfs_attr3_leaf_add
			  again on re-entry.  No need to set dela_state to do
			  this. dela_state is still unset by this function at
			  this point.
		
		  Split as many Btree elements as required.
		  This code tracks the new and old attr's location
		  in the indexblknormtblknormtblkcnt fields and
		  in the index2blkno2rmtblkno2rmtblkcnt2 fields.
		
		  Addition succeeded, update Btree hashvals.
	
	  Re-find the "old" attribute entry after any split ops. The INCOMPLETE
	  flag means that we will find the "old" attr, not the "new" one.
	
	  Check to see if the tree needs to be collapsed.
  Shrink an attribute from leaf to shortform
	
	  Have to get rid of the copy of this dabuf in the state.
 bp is gone due to xfs_da_shrink_inode 
  Mark an attribute entry INCOMPLETE and save pointers to the relevant buffers
  for later deletion of the entry.
	
	  Fill in disk block numbers in the state structure
	  so that we can get the buffers back after we commit
	  several transactions in the following calls.
	
	  Mark the attribute as INCOMPLETE
  Initial setup for xfs_attr_node_removename.  Make sure the attr is there and
  the blocks are valid.  Attr keys with remote blocks will be marked
  incomplete.
	
	  Remove the name and update the hashvals in the tree.
  Remove the attribute specified in @args.
  This will involve walking down the Btree, and may involve joining
  leaf nodes and even joining intermediate nodes up to and including
  the root node (a special case of an intermediate node).
  This routine is meant to function as either an in-line or delayed operation,
  and may return -EAGAIN when the transaction needs to be rolled.  Calling
  functions will need to handle this, and call the function until a
  successful error code is returned.
		
		  Shortform or leaf formats don't require transaction rolls and
		  thus state transitions. Call the right helper and return.
		
		  Node format may require transaction rolls. Set up the
		  state context and fall into the state machine.
		
		  If there is an out-of-line value, de-allocate the blocks.
		  This is done before we remove the attribute so that we don't
		  overflow the maximum size of a transaction andor hit a
		  deadlock.
			
			  May return -EAGAIN. Roll and repeat until all remote
			  blocks are removed.
			
			  Refill the state structure with buffers (the prior
			  calls released our buffers) and close out this
			  transaction before proceeding.
		
		  If we came here fresh from a transaction roll, reattach all
		  the buffers to the current transaction.
		
		  Check to see if the tree needs to be collapsed. If so, roll
		  the transacton and fall into the shrink state.
		
		  If the result is small enough, push it all into the inode.
		  This is our final state so it's safe to return a dirty
		  transaction.
  Fill in the disk block numbers in the state structure for the buffers
  that are attached to the state structure.
  This is done so that we can quickly reattach ourselves to those buffers
  after some set of transaction commits have released these buffers.
	
	  Roll down the "path" in the state structure, storing the on-disk
	  block number for those buffers in the "path".
	
	  Roll down the "altpath" in the state structure, storing the on-disk
	  block number for those buffers in the "altpath".
  Reattach the buffers to the state structure based on the disk block
  numbers stored in the state structure.
  This is done after some set of transaction commits have released those
  buffers from our grip.
	
	  Roll down the "path" in the state structure, storing the on-disk
	  block number for those buffers in the "path".
	
	  Roll down the "altpath" in the state structure, storing the on-disk
	  block number for those buffers in the "altpath".
  Retrieve the attribute data from a node attribute list.
  This routine gets called for any attribute fork that has more than one
  block, ie: both true Btree attr lists and for single-leaf-blocks with
  "remote" values taking up more blocks.
  Returns 0 on successful retrieval, otherwise an error.
	
	  Search to see if name exists, and get back a pointer to it.
	
	  Get the value, local or "remote"
	
	  If not in a transaction, we have to release all the buffers.
 Returns true if the attribute entry name is valid. 
	
	  MAXNAMELEN includes the trailing null, but (namelength) leave it
	  out, so use >= for the length check.
 There shouldn't be any nulls here 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Realtime allocator bitmap functions shared with userspace.
  Real time buffers need verifiers to avoid runtime warnings during IO.
  We don't have anything to verify, however, so these are just dummy
  operations.
  Get a buffer for the bitmap or summary file block specified.
  The buffer is returned read and locked.
 file system mount structure 
 transaction pointer 
 block number in bitmap or summary 
 is summary not bitmap 
 output: buffer for the block 
 block buffer, result 
 bitmap or summary inode 
 error value 
  Searching backward from start to limit, find the first block whose
  allocatedfree state is different from start's.
 file system mount point 
 transaction pointer 
 starting block to look at 
 last block to look at 
 out: start block found 
 current word in buffer 
 bit number in the word 
 bitmap block number 
 buf for the block 
 starting word in buffer 
 error value 
 first useful bit in the word 
 current bit number rel. to start 
 length of inspected area 
 mask of relevant bits for value 
 mask for "good" values 
 difference from wanted value 
 word number in the buffer 
	
	  Compute and read in starting bitmap block for starting block.
	
	  Get the first word's index & point to it.
	
	  Compute match value, based on the bit at start: if 1 (free)
	  then all-ones, else all-zeroes.
	
	  If the starting position is not word-aligned, deal with the
	  partial word.
		
		  Calculate first (leftmost) bit number to look at,
		  and mask for all the relevant bits in this word.
		
		  Calculate the difference between the value there
		  and what we're looking for.
			
			  Different.  Mark where we are and return.
		
		  Go on to previous block if that's where the previous word is
		  and we need the previous word.
			
			  If done with this block, get the previous one.
			
			  Go on to the previous word in the buffer.
		
		  Starting on a word boundary, no partial word.
	
	  Loop over whole words in buffers.  When we use up one buffer
	  we move on to the previous one.
		
		  Compute difference between actual and desired value.
			
			  Different, mark where we are and return.
		
		  Go on to previous block if that's where the previous word is
		  and we need the previous word.
			
			  If done with this block, get the previous one.
			
			  Go on to the previous word in the buffer.
	
	  If not ending on a word boundary, deal with the last
	  (partial) word.
		
		  Calculate first (leftmost) bit number to look at,
		  and mask for all the relevant bits in this word.
		
		  Compute difference between actual and desired value.
			
			  Different, mark where we are and return.
	
	  No match, return that we scanned the whole area.
  Searching forward from start to limit, find the first block whose
  allocatedfree state is different from start's.
 file system mount point 
 transaction pointer 
 starting block to look at 
 last block to look at 
 out: start block found 
 current word in buffer 
 bit number in the word 
 bitmap block number 
 buf for the block 
 starting word in buffer 
 error value 
 current bit number rel. to start 
 last useful bit in the word 
 length of inspected area 
 mask of relevant bits for value 
 mask for "good" values 
 difference from wanted value 
 word number in the buffer 
	
	  Compute and read in starting bitmap block for starting block.
	
	  Get the first word's index & point to it.
	
	  Compute match value, based on the bit at start: if 1 (free)
	  then all-ones, else all-zeroes.
	
	  If the starting position is not word-aligned, deal with the
	  partial word.
		
		  Calculate last (rightmost) bit number to look at,
		  and mask for all the relevant bits in this word.
		
		  Calculate the difference between the value there
		  and what we're looking for.
			
			  Different.  Mark where we are and return.
		
		  Go on to next block if that's where the next word is
		  and we need the next word.
			
			  If done with this block, get the previous one.
			
			  Go on to the previous word in the buffer.
		
		  Starting on a word boundary, no partial word.
	
	  Loop over whole words in buffers.  When we use up one buffer
	  we move on to the next one.
		
		  Compute difference between actual and desired value.
			
			  Different, mark where we are and return.
		
		  Go on to next block if that's where the next word is
		  and we need the next word.
			
			  If done with this block, get the next one.
			
			  Go on to the next word in the buffer.
	
	  If not ending on a word boundary, deal with the last
	  (partial) word.
		
		  Calculate mask for all the relevant bits in this word.
		
		  Compute difference between actual and desired value.
			
			  Different, mark where we are and return.
	
	  No match, return that we scanned the whole area.
  Read andor modify the summary information for a given extent size,
  bitmap block combination.
  Keeps track of a current summary block, so we don't keep reading
  it from the buffer cache.
  Summary information is returned in sum if specified.
  If no delta is specified, returns summary only.
 file system mount structure 
 transaction pointer 
 log2 of extent size 
 bitmap block number 
 change to make to summary info 
 inout: summary block buffer 
 inout: summary block number 
 out: summary info for this block 
 buffer for the summary block 
 error value 
 summary fsblock 
 index into the summary file 
 pointer to returned data 
	
	  Compute entry number in the summary file.
	
	  Compute the block number in the summary file.
	
	  If we have an old buffer, and the block number matches, use that.
	
	  Otherwise we have to get the buffer.
		
		  If there was an old one, get rid of it first.
		
		  Remember this buffer and block for the next call.
	
	  Point to the summary information, modifylog it, andor copy it out.
 file system mount structure 
 transaction pointer 
 log2 of extent size 
 bitmap block number 
 change to make to summary info 
 inout: summary block buffer 
 inout: summary block number 
  Set the given range of bitmap bits to the given value.
  Do whatever IO and logging is required.
 file system mount point 
 transaction pointer 
 starting block to modify 
 length of extent to modify 
 1 for free, 0 for allocated 
 current word in buffer 
 bit number in the word 
 bitmap block number 
 buf for the block 
 starting word in buffer 
 error value 
 first used word in the buffer 
 current bit number rel. to start 
 last useful bit in word 
 mask o frelevant bits for value 
 word number in the buffer 
	
	  Compute starting bitmap block number.
	
	  Read the bitmap block, and point to its data.
	
	  Compute the starting word's address, and starting bit.
	
	  0 (allocated) => all zeroes; 1 (free) => all ones.
	
	  If not starting on a word boundary, deal with the first
	  (partial) word.
		
		  Compute first bit not changed and mask of relevant bits.
		
		  Setclear the active bits.
		
		  Go on to the next block if that's where the next word is
		  and we need the next word.
			
			  Log the changed part of this block.
			  Get the next one.
			
			  Go on to the next word in the buffer
		
		  Starting on a word boundary, no partial word.
	
	  Loop over whole words in buffers.  When we use up one buffer
	  we move on to the next one.
		
		  Set the word value correctly.
		
		  Go on to the next block if that's where the next word is
		  and we need the next word.
			
			  Log the changed part of this block.
			  Get the next one.
			
			  Go on to the next word in the buffer
	
	  If not ending on a word boundary, deal with the last
	  (partial) word.
		
		  Compute a mask of relevant bits.
		
		  Setclear the active bits.
	
	  Log any remaining changed bytes.
  Mark an extent specified by start and len freed.
  Updates all the summary information as well as the bitmap.
 file system mount point 
 transaction pointer 
 starting block to free 
 length to free 
 inout: summary block buffer 
 inout: summary block number 
 end of the freed extent 
 error value 
 first block freed > end 
 first block freed < start 
	
	  Modify the bitmap to mark this extent freed.
	
	  Assume we're freeing out of the middle of an allocated extent.
	  We need to find the beginning and end of the extent so we can
	  properly update the summary.
	
	  Find the next allocated block (end of allocated extent).
	
	  If there are blocks not being freed at the front of the
	  old extent, add summary data for them to be allocated.
	
	  If there are blocks not being freed at the end of the
	  old extent, add summary data for them to be allocated.
	
	  Increment the summary information corresponding to the entire
	  (new) free extent.
  Check that the given range is either all allocated (val = 0) or
  all free (val = 1).
 file system mount point 
 transaction pointer 
 starting block number of extent 
 length of extent 
 1 for free, 0 for allocated 
 out: first block not matching 
 out: 1 for matches, 0 for not 
 current word in buffer 
 bit number in the word 
 bitmap block number 
 buf for the block 
 starting word in buffer 
 error value 
 current bit number rel. to start 
 last useful bit in word 
 mask of relevant bits for value 
 difference from wanted value 
 word number in the buffer 
	
	  Compute starting bitmap block number
	
	  Read the bitmap block.
	
	  Compute the starting word's address, and starting bit.
	
	  0 (allocated) => all zero's; 1 (free) => all one's.
	
	  If not starting on a word boundary, deal with the first
	  (partial) word.
		
		  Compute first bit not examined.
		
		  Mask of relevant bits.
		
		  Compute difference between actual and desired value.
			
			  Different, compute first wrong bit and return.
		
		  Go on to next block if that's where the next word is
		  and we need the next word.
			
			  If done with this block, get the next one.
			
			  Go on to the next word in the buffer.
		
		  Starting on a word boundary, no partial word.
	
	  Loop over whole words in buffers.  When we use up one buffer
	  we move on to the next one.
		
		  Compute difference between actual and desired value.
			
			  Different, compute first wrong bit and return.
		
		  Go on to next block if that's where the next word is
		  and we need the next word.
			
			  If done with this block, get the next one.
			
			  Go on to the next word in the buffer.
	
	  If not ending on a word boundary, deal with the last
	  (partial) word.
		
		  Mask of relevant bits.
		
		  Compute difference between actual and desired value.
			
			  Different, compute first wrong bit and return.
	
	  Successful, return.
  Check that the given extent (block range) is allocated already.
 error 
 file system mount point 
 transaction pointer 
 starting block number of extent 
 length of extent 
 dummy for xfs_rtcheck_range 
  Free an extent in the realtime subvolume.  Length is expressed in
  realtime extents, as is the block number.
 error 
 transaction pointer 
 starting block number to free 
 length of extent freed 
 error value 
 file system mount structure 
 summary file block number 
 summary file block buffer 
	
	  Free the range of realtime blocks.
	
	  Mark more blocks free in the superblock.
	
	  If we've now freed all the blocks, reset the file sequence
	  number to 0.
 Find all the free records within a given range. 
 Iterate the bitmap, looking for discrepancies. 
 Is the first block free? 
 How long does the extent go for? 
 Find all the free records. 
 Is the given extent all free? 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2003,2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Prototypes for internal functions.
 DEBUG 
 namelen + offset 
 ino # 
  In short-form directory entries the inode numbers are stored at variable
  offset behind the entry name. If the entry stores a filetype value, then it
  sits between the name and the inode number.  The actual inode numbers can
  come in two formats as well, either 4 bytes or 8 bytes wide.
  The file type field is stored at the end of the name for filetype enabled
  shortform directories, or not at all otherwise.
  Given a block directory (dpblock), calculate its size as a shortform (sf)
  directory and a header for the sf directory, if it will fit it the
  space currently present in the inode.  If it won't fit, the output
  size is too big (but not accurate).
 size for sf form 
 incore inode pointer 
 block directory data 
 output: header for sf form 
 data entry address 
 leaf area of the block 
 tail area of the block 
 shortform entry count 
 data entry in the block 
 block entry index 
 count of big-inode entries 
 entry is "." 
 entry is ".." 
 mount structure pointer 
 total name bytes 
 parent inode number 
 total computed size 
	
	  if there is a filetype field, add the extra byte to the namelen
	  for each entry that we see.
	
	  Iterate over the block's data entries by using the leaf pointers.
		
		  Calculate the pointer to the entry at hand.
		
		  Detect . and .., so we can special-case them.
		  . is not included in sf directories.
		  .. is included by just the parent inode number.
 take into account the file type field 
		
		  Calculate the new size, see if we should give up yet.
 header 
 namelen + offset 
 name 
 inumber 
 size value is a failure 
	
	  Create the output header, if it worked.
  Convert a block format directory to shortform.
  Caller has already checked that it will fit, and built us a header.
 error 
 operation arguments 
 shortform directory size 
 shortform directory hdr 
 error return value 
 inode logging flags 
 shortform entry 
 shortform directory header 
	
	  Allocate a temporary destination buffer the size of the inode to
	  format the data into.  Once we have formatted the data, we can free
	  the block and copy the formatted data into the inode literal area.
	
	  Loop over the active and unused entries.  Stop when we reach the
	  leaftail portion of the block.
		
		  If it's unused, just skip over it.
		
		  Skip .
		
		  Skip .., but make sure the inode number is right.
		
		  Normal entry, copy it into shortform.
 now we are done with the block, we can shrink the inode 
	
	  The buffer is now unconditionally gone, whether
	  xfs_dir2_shrink_inode worked or not.
	 
	  Convert the inode to local format and copy the data in.
  Add a name to a shortform directory.
  There are two algorithms, "easy" and "hard" which we decide on
  before changing anything.
  Convert to block form if necessary, if the new entry won't fit.
 error 
 operation arguments 
 incore directory inode 
 error return value 
 total change in size 
 size after adding name 
 changing to 8-byte inodes 
 offset for new entry 
 which algorithm to use 
 shortform structure 
 shortform entry 
	
	  Compute entry (and change in) size.
	
	  Do we have to change to 8 byte inodes?
		
		  Yes, adjust the inode size.  old count + (parent + new)
	
	  Won't fit as shortform any more (due to size),
	  or the pick routine says it won't (due to offset values).
		
		  Just checking or no space reservation, it doesn't fit.
		
		  Convert to block form then add the name.
	
	  Just checking, it fits.
	
	  Do it the easy way - just add it at the end.
	
	  Do it the hard way - look for a place to insert the new entry.
	  Convert to 8 byte inode numbers first if necessary.
  Add the new entry the "easy" way.
  This is copying the old directory and adding the new entry at the end.
  Since it's sorted by "offset" we need room after the last offset
  that's already there, and then room to convert to a block directory.
  This is already checked by the pick routine.
 operation arguments 
 pointer to new entry 
 offset to use for new ent 
 new directory size 
 byte offset in sf dir 
 shortform structure 
	
	  Grow the in-inode space.
	
	  Need to set up again due to realloc of the inode data.
	
	  Fill in the new entry.
	
	  Update the header and inode.
  Add the new entry the "hard" way.
  The caller has already converted to 8 byte inode numbers if necessary,
  in which case we need to leave the i8count at 1.
  Find a hole that the new entry will fit into, and copy
  the first part of the entries, the new entry, and the last part of
  the entries.
 ARGSUSED 
 operation arguments 
 changing inode number size 
 new directory size 
 data size need for new ent 
 buffer for old 
 reached end of old dir 
 temp for byte copies 
 next offset value 
 current offset value 
 previous size 
 entry in original dir 
 original shortform dir 
 entry in new dir 
 new shortform dir 
	
	  Copy the old directory to the stack buffer.
	
	  Loop over the old directory finding the place we're going
	  to insert the new entry.
	  If it's going to end up at the end then oldsfep will point there.
	
	  Get rid of the old directory, then allocate space for
	  the new one.  We do this so xfs_idata_realloc won't copy
	  the data.
	
	  Reset the pointer since the buffer was reallocated.
	
	  Copy the first part of the directory, including the header.
	
	  Fill in the new entry, and update the header counts.
	
	  If there's more left to copy, do that.
  Decide if the new entry will fit at all.
  If it will fit, pick between adding the new entry to the end (easy)
  or somewhere else (hard).
  Return 0 (won't fit), 1 (easy), 2 (hard).
ARGSUSED
 pick result 
 operation arguments 
 inode # size changes 
 out(1): new entry ptr 
 out(1): new offset 
 found hole it will fit in 
 entry number 
 data block offset 
 shortform entry 
 shortform structure 
 entry's data size 
 data bytes used 
	
	  Loop over sf entries.
	  Keep track of data offset and whether we've seen a place
	  to insert the new entry.
	
	  Calculate data bytes used excluding the new entry, if this
	  was a data block (block form directory).
	
	  If it won't fit in a block form then we can't insert it,
	  we'll go back, convert to block, then try the insert and convert
	  to leaf.
	
	  If changing the inode number size, do it the hard way.
	
	  If it won't fit at the end then do it the hard way (use the hole).
	
	  Do it the easy way.
  Check consistency of shortform directory, assert if bad.
 operation arguments 
 entry number 
 number of big inode#s 
 entry inode number 
 data offset 
 shortform dir entry 
 shortform structure 
 DEBUG 
 Verify the consistency of an inline directory. 
	
	  Give up if the directory is way too short.
 Check .. entry 
 Check all reported entries 
		
		  struct xfs_dir2_sf_entry has a variable length.
		  Check the fixed-offset parts of the structure are
		  within the data buffer.
 Don't allow names with known bad length. 
		
		  Check that the variable-length part of the structure is
		  within the data buffer.  The next entry starts after the
		  name component, so nextentry is an acceptable test.
 Check that the offsets always increase. 
 Check the inode number. 
 Check the file type. 
 Make sure this whole thing ought to be in local format. 
  Create a new (shortform) directory.
 error, always 0 
 operation arguments 
 parent inode number 
 incore directory inode 
 parent inode is an 8-byte number 
 shortform structure 
 directory size 
	
	  If it's currently a zero-length extent file,
	  convert it to local format.
	
	  Make a buffer for the data.
	
	  Fill in the header,
	
	  Now can put in the inode number, since i8count is set.
  Lookup an entry in a shortform directory.
  Returns EEXIST if found, ENOENT if not found.
 error 
 operation arguments 
 entry index 
 shortform directory entry 
 shortform structure 
 comparison result 
 case-insens. entry 
	
	  Special case for .
	
	  Special case for ..
	
	  Loop over all the entries trying to match ours.
		
		  Compare name and if it's an exact match, return the inode
		  number. If it's the first case-insensitive match, store the
		  inode number and continue looking for an exact match.
	
	  Here, we can only be doing a lookup (not a rename or replace).
	  If a case-insensitive match was not found, return -ENOENT.
 otherwise process the CI match as required by the caller 
  Remove an entry from a shortform directory.
 error 
 offset of removed entry 
 this entry's size 
 shortform entry index 
 new inode size 
 old inode size 
 shortform directory entry 
 shortform structure 
	
	  Loop over the old directory entries.
	  Find the one we're deleting.
	
	  Didn't find it.
	
	  Calculate sizes.
	
	  Copy the part if any after the removed entry, sliding it down.
	
	  Fix up the header and file size.
	
	  Reallocate, making it smaller.
	
	  Are we changing inode number size?
  Check whether the sf dir replace operation need more blocks.
  Replace the inode number of an entry in a shortform directory.
 error 
 operation arguments 
 entry index 
 entry old inode number 
 sf_toino8 set i8count=1 
 shortform directory entry 
 shortform structure 
	
	  New inode number is large, and need to convert to 8-byte inodes.
 error return value 
		
		  Won't fit as shortform, convert to block then do replace.
		
		  Still fits, convert to 8-byte now.
	
	  Replace ..'s entry.
	
	  Normal entry, look for the name.
		
		  Didn't find it.
	
	  See if the old number was large, the new number is small.
		
		  And the old count was one, so need to convert to small.
	
	  See if the old number was small, the new number is large.
		
		  add to the i8count unless we just converted to 8-byte
		  inodes (which does an implied i8count = 1)
  Convert from 8-byte inode numbers to 4-byte inode numbers.
  The last 8-byte inode number is gone, but the count is still 1.
 operation arguments 
 old dir's buffer 
 entry index 
 new inode size 
 old sf entry 
 old sf directory 
 old inode size 
 new sf entry 
 new sf directory 
	
	  Copy the old directory to the buffer.
	  Then nuke it from the inode, and add the new buffer to the inode.
	  Don't want xfs_idata_realloc copying the data here.
	
	  Compute the new inode size.
	
	  Reset our pointers, the data has moved.
	
	  Fill in the new header.
	
	  Copy the entries field by field.
	
	  Clean up the inode.
  Convert existing entries from 4-byte inode numbers to 8-byte inode numbers.
  The new entry w an 8-byte inode number is not there yet; we leave with
  i8count set to 1, but no corresponding 8-byte entry.
 operation arguments 
 old dir's buffer 
 entry index 
 new inode size 
 old sf entry 
 old sf directory 
 old inode size 
 new sf entry 
 new sf directory 
	
	  Copy the old directory to the buffer.
	  Then nuke it from the inode, and add the new buffer to the inode.
	  Don't want xfs_idata_realloc copying the data here.
	
	  Compute the new inode size (nb: entry count + 1 for parent)
	
	  Reset our pointers, the data has moved.
	
	  Fill in the new header.
	
	  Copy the entries field by field.
	
	  Clean up the inode.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2016 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Deferred Operations in XFS
  Due to the way locking rules work in XFS, certain transactions (block
  mapping and unmapping, typically) have permanent reservations so that
  we can roll the transaction to adhere to AG locking order rules and
  to unlock buffers between metadata updates.  Prior to rmapreflink,
  the mapping code had a mechanism to perform these deferrals for
  extents that were going to be freed; this code makes that facility
  more generic.
  When adding the reverse mapping and reflink features, it became
  necessary to perform complex remapping multi-transactions to comply
  with AG locking order rules, and to be able to spread a single
  refcount update operation (an operation on an n-block extent can
  update as many as n records!) among multiple transactions.  XFS can
  roll a transaction to facilitate this, but using this facility
  requires us to log "intent" items in case log recovery needs to
  redo the operation, and to log "done" items to indicate that redo
  is not necessary.
  Deferred work is tracked in xfs_defer_pending items.  Each pending
  item tracks one type of deferred work.  Incoming work items (which
  have not yet had an intent logged) are attached to a pending item
  on the dop_intake list, where they wait for the caller to finish
  the deferred operations.
  Finishing a set of deferred operations is an involved process.  To
  start, we define "rolling a deferred-op transaction" as follows:
  > For each xfs_defer_pending item on the dop_intake list,
    - Sort the work items in AG order.  XFS locking
      order rules require us to lock buffers in AG order.
    - Create a log intent item for that type.
    - Attach it to the pending item.
    - Move the pending item from the dop_intake list to the
      dop_pending list.
  > Roll the transaction.
  NOTE: To avoid exceeding the transaction reservation, we limit the
  number of items that we attach to a given xfs_defer_pending.
  The actual finishing process looks like this:
  > For each xfs_defer_pending in the dop_pending list,
    - Roll the deferred-op transaction as above.
    - Create a log done item for that type, and attach it to the
      log intent item.
    - For each work item attached to the log intent item,
       Perform the described action.
       Attach the work item to the log done item.
       If the result of doing the work was -EAGAIN, ->finish work
        wants a new transaction.  See the "Requesting a Fresh
        Transaction while Finishing Deferred Work" section below for
        details.
  The key here is that we must log an intent item for all pending
  work items every time we roll the transaction, and that we must log
  a done item as soon as the work is completed.  With this mechanism
  we can perform complex remapping operations, chaining intent items
  as needed.
  Requesting a Fresh Transaction while Finishing Deferred Work
  If ->finish_item decides that it needs a fresh transaction to
  finish the work, it must ask its caller (xfs_defer_finish) for a
  continuation.  The most likely cause of this circumstance are the
  refcount adjust functions deciding that they've logged enough items
  to be at risk of exceeding the transaction reservation.
  To get a fresh transaction, we want to log the existing log done
  item to prevent the log intent item from replaying, immediately log
  a new log intent item with the unfinished work items, roll the
  transaction, and re-call ->finish_item wherever it left off.  The
  log done item and the new log intent item must be in the same
  transaction or atomicity cannot be guaranteed; defer_finish ensures
  that this happens.
  This requires some coordination between ->finish_item and
  defer_finish.  Upon deciding to request a new transaction,
  ->finish_item should update the current work item to reflect the
  unfinished work.  Next, it should reset the log done item's list
  count to the number of items finished, and return -EAGAIN.
  defer_finish sees the -EAGAIN, logs the new log intent item
  with the remaining work items, and leaves the xfs_defer_pending
  item at the head of the dop_work queue.  Then it rolls the
  transaction and picks up processing where it left off.  It is
  required that ->finish_item must be careful to leave enough
  transaction reservation to fit the new log intent item.
  This is an example of remapping the extent (E, E+B) into file X at
  offset A and dealing with the extent (C, C+B) already being mapped
  there:
  +-------------------------------------------------+
  | Unmap file X startblock C offset A length B     | t0
  | Intent to reduce refcount for extent (C, B)     |
  | Intent to remove rmap (X, C, A, B)              |
  | Intent to free extent (D, 1) (bmbt block)       |
  | Intent to map (X, A, B) at startblock E         |
  +-------------------------------------------------+
  | Map file X startblock E offset A length B       | t1
  | Done mapping (X, E, A, B)                       |
  | Intent to increase refcount for extent (E, B)   |
  | Intent to add rmap (X, E, A, B)                 |
  +-------------------------------------------------+
  | Reduce refcount for extent (C, B)               | t2
  | Done reducing refcount for extent (C, 9)        |
  | Intent to reduce refcount for extent (C+9, B-9) |
  | (ran out of space after 9 refcount updates)     |
  +-------------------------------------------------+
  | Reduce refcount for extent (C+9, B+9)           | t3
  | Done reducing refcount for extent (C+9, B-9)    |
  | Increase refcount for extent (E, B)             |
  | Done increasing refcount for extent (E, B)      |
  | Intent to free extent (C, B)                    |
  | Intent to free extent (F, 1) (refcountbt block) |
  | Intent to remove rmap (F, 1, REFC)              |
  +-------------------------------------------------+
  | Remove rmap (X, C, A, B)                        | t4
  | Done removing rmap (X, C, A, B)                 |
  | Add rmap (X, E, A, B)                           |
  | Done adding rmap (X, E, A, B)                   |
  | Remove rmap (F, 1, REFC)                        |
  | Done removing rmap (F, 1, REFC)                 |
  +-------------------------------------------------+
  | Free extent (C, B)                              | t5
  | Done freeing extent (C, B)                      |
  | Free extent (D, 1)                              |
  | Done freeing extent (D, 1)                      |
  | Free extent (F, 1)                              |
  | Done freeing extent (F, 1)                      |
  +-------------------------------------------------+
  If we should crash before t2 commits, log recovery replays
  the following intent items:
  - Intent to reduce refcount for extent (C, B)
  - Intent to remove rmap (X, C, A, B)
  - Intent to free extent (D, 1) (bmbt block)
  - Intent to increase refcount for extent (E, B)
  - Intent to add rmap (X, E, A, B)
  In the process of recovering, it should also generate and take care
  of these intent items:
  - Intent to free extent (C, B)
  - Intent to free extent (F, 1) (refcountbt block)
  - Intent to remove rmap (F, 1, REFC)
  Note that the continuation requested between t2 and t3 is likely to
  reoccur.
  For each pending item in the intake list, log its intent item and the
  associated extents, then add the entire intake list to the end of
  the pending list.
 Abort all the intents that were committed. 
 Abort intent items that don't have a done item. 
  Capture resources that the caller said not to release ("held") when the
  transaction commits.  Caller is responsible for zero-initializing @dres.
 Attach the held resources to the transaction. 
 Rejoin the joined inodes. 
 Rejoin the buffers and dirty them so the log moves forward. 
 Roll a transaction so we can do some deferred op processing. 
	
	  Roll the transaction.  Rolling always given a new transaction (even
	  if committing the old one fails!) to hand back to the caller, so we
	  join the held resources to the new transaction so that we always
	  return with the held resources joined to @tpp, no matter what
	  happened.
  Free up any items left in the list.
	
	  Free the pending items.  Caller should already have arranged
	  for the intent items to be released.
  Prevent a log intent item from pinning the tail of the log by logging a
  done item to release the intent item; and then log a new intent item.
  The caller should provide a fresh transaction and roll it after we're done.
		
		  If the log intent item for this deferred op is not a part of
		  the current log checkpoint, relog the intent item to keep
		  the log tail moving forward.  We're ok with this being racy
		  because an incorrect decision means we'll be a little slower
		  at pushing the tail.
		
		  Figure out where we need the tail to be in order to maintain
		  the minimum required free space in the log.  Only sample
		  the log threshold once per call.
  Log an intent-done item for the first pending intent, and finish the work
  items.
			
			  Caller wants a fresh transaction; put the work item
			  back on the list and log a new log intent item to
			  replace the old one.  See "Requesting a Fresh
			  Transaction while Finishing Deferred Work" above.
 Done with the dfp, free it. 
  Finish all the pending work.  This involves logging intent items for
  any work items that wandered in since the last transaction roll (if
  one has even happened), rolling the transaction, and finishing the
  work items in the first item on the logged-and-pending list.
  If an inode is provided, relog it to the new transaction.
 Until we run out of pending work to finish... 
		
		  Deferred items that are created in the process of finishing
		  other deferred work items should be queued at the head of
		  the pending list, which puts them ahead of the deferred work
		  that was created by the caller.  This keeps the number of
		  pending work items to a minimum, which decreases the amount
		  of time that any one intent item can stick around in memory,
		  pinning the log tail.
 Possibly relog intent items to keep the log moving. 
	
	  Finish and roll the transaction once more to avoid returning to the
	  caller with a dirty transaction.
 Reset LOWMODE now that we've finished all the dfops. 
 Add an item for later deferred processing. 
	
	  Add the item to a pending item at the end of the intake list.
	  If the last pending item has the same type, reuse it.  Else,
	  create a new pending item at the end of the intake list.
  Move deferred ops from one transaction to another and reset the source to
  initial state. This is primarily used to carry state forward across
  transaction rolls with pending dfops.
	
	  Low free space mode was historically controlled by a dfops field.
	  This meant that low mode state potentially carried across multiple
	  transaction rolls. Transfer low mode on a dfops move to preserve
	  that behavior.
  Prepare a chain of fresh deferred ops work items to be completed later.  Log
  recovery requires the ability to put off until later the actual finishing
  work so that it can process unfinished items recovered from the log in
  correct order.
  Create and log intent items for all the work that we're capturing so that we
  can be assured that the items will get replayed if the system goes down
  before log recovery gets a chance to finish the work it put off.  The entire
  deferred ops state is transferred to the capture structure and the
  transaction is then ready for the caller to commit it.  If there are no
  intent items to capture, this function returns NULL.
  If capture_ip is not NULL, the capture structure will obtain an extra
  reference to the inode.
 Create an object to capture the defer ops. 
 Move the dfops chain and transaction state to the capture struct. 
 Capture the remaining block reservations along with the dfops. 
 Preserve the log reservation size. 
		
		  Resource capture should never fail, but if it does, we
		  still have to shut down the log and release things
		  properly.
	
	  Grab extra references to the inodes and buffers because callers are
	  expected to release their held references after we commit the
	  transaction.
 Release all resources that we used to capture deferred ops. 
  Capture any deferred ops and commit the transaction.  This is the last step
  needed to finish a log intent item that we recovered from the log.  If any
  of the deferred ops operate on an inode, the caller must pass in that inode
  so that the reference can be transferred to the capture structure.  The
  caller must hold ILOCK_EXCL on the inode, and must unlock it before calling
  xfs_defer_ops_continue.
 If we don't capture anything, commit transaction and exit. 
 Commit the transaction and add the capture structure to the list. 
  Attach a chain of captured deferred ops to a new transaction and free the
  capture structure.  If an inode was captured, it will be passed back to the
  caller with ILOCK_EXCL held and joined to the transaction with lockflags==0.
  The caller now owns the inode reference.
 Lock and join the captured inode to the new transaction. 
 Move captured dfops chain and state to the transaction. 
 Release the resources captured and continued during recovery. 
 Set up caches for deferred work items. 
 Destroy all the deferred work item caches, if they've been allocated. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  Copyright (c) 2012-2013 Red Hat, Inc.
  All rights reserved.
  Each contiguous block has a header, so it is not just a simple pathlen
  to FSB conversion.
  Checking of the symlink header is split into two parts. the verifier does
  CRC, location and bounds checking, the unpacking function checks the path
  parameters and owner.
 ok 
 no verification of non-crc buffers 
 no verification of non-crc buffers 
	
	  As this symlink fits in an inode literal area, it must also fit in
	  the smallest buffer the filesystem supports.
  Verify the in-memory consistency of an inline symlink data fork. This
  does not do on-disk format checks.
	
	  Zero length symlinks should never occur in memory as they are
	  never alllowed to exist on disk.
 No negative sizes or overly long symlink targets. 
 No NULLs in the target either. 
 We did null-terminate the buffer, right? 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Lookup a record by ino in the btree given by cur.
 error 
 btree cursor 
 starting inode of chunk 
 <=, >=, == 
 successfailure 
  Update the record referred to by cur to the value given.
  This either works (return 0) or gets an EFSCORRUPTED error.
 error 
 btree cursor 
 btree record 
 ir_holemaskir_count not supported on-disk 
 Convert on-disk btree record to incore inobt record. 
		
		  ir_holemaskir_count not supported on-disk. Fill in hardcoded
		  values for full inode chunks.
  Get the data from the pointed-to record.
 if there are no holes, return the first available offset 
  Insert a single inobt record. Cursor must already point to desired location.
  Insert records describing a newly allocated inode chunk into the inobt.
  Verify that the number of free inodes in the AGI is correct.
  Initialise a new set of inodes. When called without a transaction context
  (e.g. from recovery) we initiate a delayed write of the inode buffers rather
  than logging them (which in a transaction context puts them into the AIL
  for writeback rather than the xfsbufd queue).
	
	  Loop over the new block(s), filling in the inodes.  For small block
	  sizes, manipulate the inodes in buffers  which are multiples of the
	  blocks size.
	
	  Figure out what version number to use in the inodes we create.  If
	  the superblock version has caught up to the one that supports the new
	  inode format, then use the new inode version.  Otherwise use the old
	  version so that old kernels will continue to be able to use the file
	  system.
	 
	  For v3 inodes, we also need to write the inode number into the inode,
	  so calculate the first inode number of the chunk here as
	  XFS_AGB_TO_AGINO() only works within a filesystem block, not
	  across multiple filesystem blocks (such as a cluster) and so cannot
	  be used in the cluster buffer loop below.
	 
	  Further, because we are writing the inode directly into the buffer
	  and calculating a CRC on the entire inode, we have ot log the entire
	  inode so that the entire range the CRC covers is present in the log.
	  That means for v3 inode we log the entire buffer rather than just the
	  inode cores.
		
		  log the initialisation that is about to take place as an
		  logical operation. This means the transaction does not
		  need to log the physical changes to the inode buffers as log
		  recovery will know what initialisation is actually needed.
		  Hence we only need to log the buffers as "ordered" buffers so
		  they track in the AIL as if they were physically logged.
		
		  Get the block.
 Initialize the inode buffers and log them appropriately. 
 just log the inode core 
			
			  Mark the buffer as an inode allocation buffer so it
			  sticks in AIL at the point of this allocation
			  transaction. This ensures the they are on disk before
			  the tail of the log can be moved past this
			  transaction (i.e. by preventing relogging from moving
			  it forward in the log).
				
				  Mark the buffer as ordered so that they are
				  not physically logged in the transaction but
				  still tracked in the AIL as part of the
				  transaction and pin the log appropriately.
  Align startino and allocmask for a recently allocated sparse chunk such that
  they are fit for insertion (or merge) into the on-disk inode btrees.
  Background:
  When enabled, sparse inode support increases the inode alignment from cluster
  size to inode chunk size. This means that the minimum range between two
  non-adjacent inode records in the inobt is large enough for a full inode
  record. This allows for cluster sized, cluster aligned block allocation
  without need to worry about whether the resulting inode record overlaps with
  another record in the tree. Without this basic rule, we would have to deal
  with the consequences of overlap by potentially undoing recent allocations in
  the inode allocation codepath.
  Because of this alignment rule (which is enforced on mount), there are two
  inobt possibilities for newly allocated sparse chunks. One is that the
  aligned inode record for the chunk covers a range of inodes not already
  covered in the inobt (i.e., it is safe to insert a new sparse record). The
  other is that a record already exists at the aligned startino that considers
  the newly allocated range as sparse. In the latter case, record content is
  merged in hope that sparse inode chunks fill to full chunks over time.
 calculate the inode offset and align startino 
	
	  Since startino has been aligned down, left shift allocmask such that
	  it continues to represent the same physical inodes relative to the
	  new startino.
  Determine whether the source inode record can merge into the target. Both
  records must be sparse, the inode ranges must match and there must be no
  allocation overlap between the records.
 tgt record 
 src record 
 records must cover the same inode range 
 both records must be sparse 
 both records must track some inodes 
 can't exceed capacity of a full record 
 verify there is no allocation overlap 
  Merge the source inode record into the target. The caller must call
  __xfs_inobt_can_merge() to ensure the merge is valid.
 target 
 src 
 combine the counts 
	
	  Merge the holemask and free mask. For both fields, 0 bits refer to
	  allocated inodes. We combine the allocated ranges with bitwise AND.
  Insert a new sparse inode chunk into the associated inode btree. The inode
  record for the sparse chunk is pre-aligned to a startino that should match
  any pre-existing sparse inode record in the tree. This allows sparse chunks
  to fill over time.
  This function supports two modes of handling preexisting records depending on
  the merge flag. If merge is true, the provided record is merged with the
  existing record and updated in place. The merged record is returned in nrec.
  If merge is false, an existing record is replaced with the provided record.
  If no preexisting record exists, the provided record is always inserted.
  It is considered corruption if a merge is requested and not possible. Given
  the sparse inode alignment constraints, this should never happen.
 inout: newmerged rec. 
 merge or replace 
 the new record is pre-aligned so we know where to look 
 if nothing there, insert a new record and return 
	
	  A record exists at this startino. Merge or replace the record
	  depending on what we've been asked to do.
		
		  This should never fail. If we have coexisting records that
		  cannot merge, something is seriously wrong.
 merge to nrec to output the updated record 
  Allocate new inodes in the allocation group specified by agbp.  Returns 0 if
  inodes were allocated in this AG; -EAGAIN if there was no space in this AG so
  the caller knows it can try another AG, a hard -ENOSPC when over the maximum
  inode count threshold, or the usual negative error code for other errors.
 new first inode's number 
 new number of inodes 
 inode allocation at stripe 
 unit boundary 
 init. to full chunk 
 randomly do sparse inode allocations 
	
	  Locking will ensure that we don't have two callers in here
	  at one time.
	
	  First try to allocate inodes contiguous with the last-allocated
	  chunk of inodes.  If the filesystem is striped, this will fill
	  an entire stripe unit with inodes.
		
		  We need to take into account alignment here to ensure that
		  we don't modify the free list if we fail to have an exact
		  block. If we don't have an exact match, and every oher
		  attempt allocation attempt fails, we'll end up cancelling
		  a dirty transaction and shutting down.
		 
		  For an exact allocation, alignment must be 1,
		  however we need to take cluster alignment into account when
		  fixing up the freelist. Use the minalignslop field to
		  indicate that extra blocks might be required for alignment,
		  but not to use them in the actual exact allocation.
 Allow space for the inode btree to split. 
		
		  This request might have dirtied the transaction if the AG can
		  satisfy the request, but the exact block was not available.
		  If the allocation did fail, subsequent requests will relax
		  the exact agbno requirement and increase the alignment
		  instead. It is critical that the total size of the request
		  (len + alignment + slop) does not increase from this point
		  on, so reset minalignslop to ensure it is not included in
		  subsequent requests.
		
		  Set the alignment for the allocation.
		  If stripe alignment is turned on then align at stripe unit
		  boundary.
		  If the cluster size is smaller than a filesystem block
		  then we're doing IO for inodes in filesystem block size
		  pieces, so don't need alignment anyway.
		
		  Need to figure out where to allocate the inode blocks.
		  Ideally they should be spaced out through the a.g.
		  For now, just allocate blocks up front.
		
		  Allocate a fixed-size extent of inodes.
		
		  Allow space for the inode btree to split.
	
	  If stripe alignment is turned on, then try again with cluster
	  alignment.
	
	  Finally, try a sparse allocation if the filesystem supports it and
	  the sparse allocation length is smaller than a full chunk.
		
		  The inode record will be aligned to full chunk size. We must
		  prevent sparse allocation from AG boundaries that result in
		  invalid inode records, such as records that start at agbno 0
		  or extend beyond the AG.
		 
		  Set min agbno to the first aligned, non-zero agbno and max to
		  the last aligned agbno that is at least one full chunk from
		  the end of the AG.
	
	  Stamp and write the inode buffers.
	 
	  Seed the new inode cluster with a random generation number. This
	  prevents short-term reuse of generation numbers if a chunk is
	  freed and then immediately reallocated. We use random numbers
	  rather than a linear progression to prevent the next generation
	  number from being easily guessable.
	
	  Convert the results.
		
		  We've allocated a sparse chunk. Align the startino and mask.
		
		  Insert the sparse record into the inobt and allow for a merge
		  if necessary. If a merge does occur, rec is updated to the
		  merged record.
		
		  We can't merge the part we've just allocated as for the inobt
		  due to finobt semantics. The original record may or may not
		  exist independent of whether physical inodes exist in this
		  sparse chunk.
		 
		  We must update the finobt record based on the inobt record.
		  rec contains the fully merged and up to date inobt record
		  from the previous call. Set merge false to replace any
		  existing record with this one.
 full chunk - insert new records to both btrees 
	
	  Update AGI counts and newino.
	
	  Log allocation group header fields
	
	  Modifylog superblock values for inode count and inode free count.
  Try to retrieve the next record to the leftright from the current one.
  Return the offset of the first free inode in the record. If the inode chunk
  is sparsely allocated, we convert the record holemask to inode granularity
  and mask off the unallocated regions from the inode free mask.
 if there are no holes, return the first available offset 
  Allocate an inode using the inobt-only algorithm.
	
	  If pagino is 0 (this is the root inode allocation) use newino.
	  This must work because we've just allocated some.
	
	  If in the same AG as the parent, try to get near the parent.
 done, to the left 
 done, to the right 
			
			  Found a free inode in the same chunk
			  as the parent, done.
		
		  In the same AG as parent, but parent's chunk is full.
 duplicate the cursor, search left & right simultaneously 
		
		  Skip to last blocks looked up if same parent inode.
 search left with tcur, back up 1 record 
 search right with cur, go forward 1 record. 
		
		  Loop until we find an inode chunk with a free inode.
 using left inode chunk this time 
 figure out the closer block if both are valid. 
 free inodes to the left? 
 free inodes to the right? 
 get next record to check 
			
			  Not in range - save last search
			  location and allocate a new inode
			
			  We've reached the end of the btree. because
			  we are only searching a small chunk of the
			  btree each search, there is obviously free
			  inodes closer to the parent inode than we
			  are now. restart the search again.
	
	  In a different AG from the parent.
	  See if the most recently allocated block has any free.
				
				  The last chunk allocated in the group
				  still has a free inode.
	
	  None left in the last group, search the whole AG
  Use the free inode btree to allocate an inode based on distance from the
  parent. Note that the provided cursor may be deleted and replaced.
 left search cursor 
 right search cursor 
		
		  See if we've landed in the parent inode record. The finobt
		  only tracks chunks with at least one free inode, so record
		  existence is enough.
		
		  Both the left and right records are valid. Choose the closer
		  inode chunk to the target.
 only the right record is valid 
 only the left record is valid 
  Use the free inode btree to find a free inode based on a newino hint. If
  the hint is NULL, find the first free inode in the AG.
	
	  Find the first inode available in the AG.
  Update the inobt based on a modification made to the finobt. Also ensure that
  the records from both trees are equivalent post-modification.
 inobt cursor 
 finobt record 
 inode offset 
  Allocate an inode using the free inode btree, if available. Otherwise, fall
  back to the inobt search algorithm.
  The caller selected an AG for us, and made sure that free inodes are
  available.
 finobt cursor 
 inobt cursor 
	
	  If pagino is 0 (this is the root inode allocation) use newino.
	  This must work because we've just allocated some.
	
	  The search algorithm depends on whether we're in the same AG as the
	  parent. If so, find the closest available inode to the parent. If
	  not, consider the agi hint or find the first free inode in the AG.
	
	  Modify or remove the finobt record.
	
	  The finobt has now been updated appropriately. We haven't updated the
	  agi and superblock yet, so we can create an inobt cursor and validate
	  the original freecount. If all is well, make the equivalent update to
	  the inobt using the finobt record and offset information.
	
	  Both trees have now been updated. We must update the perag and
	  superblock before we can check the freecount for each btree.
	
	  Hold to on to the agibp across the commit so no other allocation can
	  come in and take the free inodes we just allocated for our caller.
	
	  We want the quota changes to be associated with the next transaction,
	  NOT this one. So, detach the dqinfo from this and attach it to the
	  next transaction.
 Re-attach the quota info that we detached from prev trx. 
	
	  Join the buffer even on commit error so that the buffer is released
	  when the caller cancels the transaction and doesn't have to handle
	  this error case specially.
	
	  Check that there is enough free space for the file plus a chunk of
	  inodes if we need to allocate some. If this is the first pass across
	  the AGs, take into account the potential space needed for alignment
	  of inode chunks when checking the longest contiguous free space in
	  the AG - this prevents us from getting ENOSPC because we have free
	  space larger than ialloc_blks but alignment constraints prevent us
	  from using it.
	 
	  If we can't find an AG with space for full alignment slack to be
	  taken into account, we must be near ENOSPC in all AGs.  Hence we
	  don't include alignment for the second pass and so if we fail
	  allocation due to alignment issues then it is most likely a real
	  ENOSPC condition.
	 
	  XXX(dgc): this calculation is now bogus thanks to the per-ag
	  reservations that xfs_alloc_fix_freelist() now does via
	  xfs_alloc_space_available(). When the AG fills up, pagf_freeblks will
	  be more than large enough for the check below to succeed, but
	  xfs_alloc_space_available() will fail because of the non-zero
	  metadata reservation and hence we won't actually be able to allocate
	  more inodes in this AG. We do soooo much unnecessary work near ENOSPC
	  because of this.
	
	  Then read in the AGI buffer and recheck with the AGI buffer
	  lock held.
		
		  We successfully allocated space for an inode cluster in this
		  AG.  Roll the transaction so that we can allocate one of the
		  new inodes.
 Allocate an inode in the found AG 
  Allocate an on-disk inode.
  Mode is used to tell whether the new inode is a directory and hence where to
  locate it. The on-disk inode that is allocated will be returned in @new_ino
  on success, otherwise an error will be set to indicate the failure (e.g.
  -ENOSPC).
	
	  Directories, symlinks, and regular files frequently allocate at least
	  one block, so factor that potential expansion when we examine whether
	  an AG has enough space for file creation.
	
	  If we have already hit the ceiling of inode blocks then clear
	  ok_alloc so we scan all available agi structures for a free
	  inode.
	 
	  Read rough value of mp->m_icount by percpu_counter_read_positive,
	  which will sacrifice the preciseness but improve the performance.
	
	  Loop until we find an allocation group that either has free inodes
	  or in which we can allocate some inodes.  Iterate through the
	  allocation groups upward, wrapping at the end.
  Free the blocks of an inode chunk. We must consider that the inode chunk
  might be sparse and only free the regions that are allocated as part of the
  chunk.
 not sparse, calculate extent info directly 
 holemask is only 16-bits (fits in an unsigned long) 
	
	  Find contiguous ranges of zeroes (i.e., allocated regions) in the
	  holemask and convert the startend index of each range to an extent.
	  We start with the start and end index both pointing at the first 0 in
	  the mask.
		
		  If the next zero bit is contiguous, update the end index of
		  the current range and continue.
		
		  nextbit is not contiguous with the current end index. Convert
		  the current startend to an extent and add it to the free
		  list.
 reset range to current bit and carry on... 
	
	  Initialize the cursor.
	
	  Look for the entry describing this inode.
	
	  Get the offset in the inode chunk.
	
	  Mark the inode free & increment the count.
	
	  When an inode chunk is free, it becomes eligible for removal. Don't
	  remove the chunk if the block size is large enough for multiple inode
	  chunks (that might not be free).
		
		  Remove the inode cluster from the AGI B+Tree, adjust the
		  AGI and Superblock inode counts, and mark the disk space
		  to be freed when the transaction is committed.
		
		  Change the inode free counts and log the agsb changes.
  Free an inode in the free inode btree.
 inobt record 
		
		  If the record does not exist in the finobt, we must have just
		  freed an inode in a previously fully allocated chunk. If not,
		  something is out of sync.
	
	  Read and update the existing record. We could just copy the ibtrec
	  across here, but that would defeat the purpose of having redundant
	  metadata. By making the modifications independently, we can catch
	  corruptions that we wouldn't see if we just copied from one record
	  to another.
	
	  The content of inobt records should always match between the inobt
	  and finobt. The lifecycle of records in the finobt is different from
	  the inobt in that the finobt only tracks records with at least one
	  free inode. Hence, if all of the inodes are free and we aren't
	  keeping inode chunks permanently on disk, remove the record.
	  Otherwise, update the record with the new information.
	 
	  Note that we currently can't free chunks when the block size is large
	  enough for multiple chunks. Leave the finobt record to remain in sync
	  with the inobt.
  Free disk inode.  Carefully avoids touching the incore inode, all
  manipulations incore are the caller's responsibility.
  The on-disk inode is not changed by this operation, only the
  btree (free inode mask) is changed.
 REFERENCED 
 block number containing inode 
 buffer for allocation group header 
 allocation group inode number 
 error return value 
 btree record 
	
	  Break up inode number into its components.
	
	  Get the allocation group header.
	
	  Fix up the inode allocation btree.
	
	  Fix up the free inode btree.
	
	  Lookup the inode record for the given agino. If the record cannot be
	  found, then it's an invalid inode number and we should abort. Once
	  we have a record, we need to ensure it contains the inode number
	  we are looking up.
 check that the returned record contains the required inode 
 for untrusted inodes check it is allocated first 
  Return the location of the inode in imap, for mapping it into a buffer.
 file system mount structure 
 transaction pointer 
 inode to locate 
 location map structure 
 flags for inode btree lookup 
 block number of inode in the alloc group 
 inode number within alloc group 
 first block in inode chunk 
 first block in inode cluster 
 error code 
 index of inode in its buffer 
 blks from chunk start to inode 
	
	  Split up the inode number into its parts.
		
		  Don't output diagnostic information for untrusted inodes
		  as they can be invalid without implying corruption.
 DEBUG 
	
	  For bulkstat and handle lookups, we have an untrusted inode number
	  that we have to verify is valid. We cannot do this just by reading
	  the inode buffer as it may have been unlinked and removed leaving
	  inodes in stale state on disk. Hence we have to do a btree lookup
	  in all cases where an untrusted inode number is passed.
	
	  If the inode cluster size is the same as the blocksize or
	  smaller we get to the buffer by simple arithmetics.
	
	  If the inode chunks are aligned then use simple maths to
	  find the location. Otherwise we have to do a btree
	  lookup to find the location.
	
	  If the inode number maps to a block outside the bounds
	  of the file system then return NULL rather than calling
	  read_buf and panicing when we get an error from the
	  driver.
  Log specified fields for the ag hdr (inode section). The growth of the agi
  structure over time requires that we interpret the buffer as two logical
  regions delineated by the end of the unlinked list. This is due to the size
  of the hash table and its location in the middle of the agi.
  For example, a request to log a field before agi_unlinked and a field after
  agi_unlinked could cause us to log the entire hash table and use an excessive
  amount of log space. To avoid this behavior, log the region up through
  agi_unlinked in one call and the region after agi_unlinked through the end of
  the structure in another.
 transaction pointer 
 allocation group header buffer 
 bitmask of fields to log 
 first byte number 
 last byte number 
 field starting offsets 
 keep in sync with bit definitions 
	
	  Compute byte offsets for the first and last fields in the first
	  region and log the agi buffer. This only logs up through
	  agi_unlinked.
	
	  Mask off the bits in the first region and calculate the first and
	  last field offsets for any bits in the second region.
	
	  Validate the magic number of the agi block.
	
	  during growfs operations, the perag is not fully initialised,
	  so we can't use it for any useful checking. growfs ensures we can't
	  use it by using uncached buffers that don't have the perag attached
	  so we can detect and avoid this problem.
  Read in the allocation group header (inode allocation section)
 file system mount structure 
 transaction pointer 
 allocation group number 
 allocation group hdr buf 
 file system mount structure 
 transaction pointer 
 allocation group number 
 allocation group hdr buf 
 allocation group header 
 per allocation group data 
	
	  It's possible for these to be out of sync if
	  we are in the middle of a forced shutdown.
  Read in the agi to initialise the per-ag data in the mount structure
 file system mount structure 
 transaction pointer 
 allocation group number 
 Is there an inode record covering a given range of inode numbers? 
 Is there an inode record covering a given extent? 
 Record inode counts across all inobt records. 
 Count allocated and free inodes under an inobt. 
  Initialize inode-related geometry information.
  Compute the inode btree min and max levels and set maxicount.
  Set the inode cluster size.  This may still be overridden by the file
  system block size if it is larger than the chosen cluster size.
  For v5 filesystems, scale the cluster size with the inode size to keep a
  constant ratio of inode per cluster buffer, but only if mkfs has set the
  inode alignment value appropriately for larger cluster sizes.
  Then compute the inode cluster alignment information.
 Compute inode btree geometry. 
 Compute and fill in value of m_ino_geo.inobt_maxlevels. 
	
	  Set the maximum inode count for this filesystem, being careful not
	  to use obviously garbage sb_inopblogsb_inopblock values.  Regular
	  users should never get here due to failing sb verification, but
	  certain users (xfs_db) need to be usable even with corrupt metadata.
		
		  Make sure the maximum inode count is a multiple
		  of the units we allocate inodes in.
	
	  Compute the desired size of an inode cluster buffer size, which
	  starts at 8K and (on v5 filesystems) scales up with larger inode
	  sizes.
	 
	  Preserve the desired inode cluster size because the sparse inodes
	  feature uses that desired size (not the actual size) to compute the
	  sparse inode alignment.  The mount code validates this value, so we
	  cannot change the behavior.
 Calculate inode cluster ratios. 
 Calculate inode cluster alignment. 
	
	  If we are using stripe alignment, check whether
	  the stripe unit is a multiple of the inode alignment
 Compute the location of the root directory inode that is laid out by mkfs. 
	
	  Pre-calculate the geometry of AG 0.  We know what it looks like
	  because libxfs knows how to create allocation groups now.
	 
	  first_bno is the first block in which mkfs could possibly have
	  allocated the root directory inode, once we factor in the metadata
	  that mkfs formats before it.  Namely, the four AG headers...
 ...the two free space btree roots... 
 ...the inode btree root... 
 ...the initial AGFL... 
 ...the free inode btree root... 
 ...the reverse mapping btree root... 
 ...the reference count btree... 
	
	  ...and the log, if it is allocated in the first allocation group.
	 
	  This can happen with filesystems that only have a single
	  allocation group, or very odd geometries created by old mkfs
	  versions on very small filesystems.
	
	  Now round first_bno up to whatever allocation alignment is given
	  by the filesystem or was passed in.
  Ensure there are not sparse inode clusters that cross the new EOAG.
  This is a no-op for non-spinode filesystems since clusters are always fully
  allocated and checking the bnobt suffices.  However, a spinode filesystem
  could have a record where the upper inodes are free blocks.  If those blocks
  were removed from the filesystem, the inode record would extend beyond EOAG,
  which will be flagged as corruption.
 Look up the inobt record that would correspond to the new EOFS. 
 If the record covers inodes that would be beyond EOFS, bail out. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2003,2005 Silicon Graphics, Inc.
  Copyright (C) 2010 Red Hat, Inc.
  All Rights Reserved.
  A buffer has a format structure overhead in the log in addition
  to the data, so we need to take this into account when reserving
  space in a transaction for a buffer.  Round the space required up
  to a multiple of 128 bytes so that we don't change the historical
  reservation that has been used for this overhead.
  Calculate out transaction log reservation per item in bytes.
  The nbufs argument is used to indicate the number of items that
  will be changed in a transaction.  size is used to tell how many
  bytes should be reserved per item.
  Per-extent log reservation for the btree changes involved in freeing or
  allocating an extent.  In classic XFS there were two trees that will be
  modified (bnobt + cntbt).  With rmap enabled, there are three trees
  (rmapbt).  With reflink, there are four trees (refcountbt).  The number of
  blocks reserved is based on the formula:
  num trees  ((2 blockslevel  max depth) - 1)
  Keep in mind that max depth is calculated separately for each type of tree.
  Logging inodes is really tricksy. They are logged in memory format,
  which means that what we write into the log doesn't directly translate into
  the amount of space they use on disk.
  Case in point - btree format forks in memory format use more space than the
  on-disk format. In memory, the buffer contains a normal btree block header so
  the btree code can treat it as though it is just another generic buffer.
  However, when we write it to the inode fork, we don't write all of this
  header as it isn't needed. e.g. the root is only ever in the inode, so
  there's no need for sibling pointers which would waste 16 bytes of space.
  Hence when we have an inode with a maximally sized btree format fork, then
  amount of information we actually log is greater than the size of the inode
  on disk. Hence we need an inode reservation function that calculates all this
  correctly. So, we log:
  - 4 log op headers for object
 	- for the ilf, the inode core and 2 forks
  - inode log format object
  - the inode core
  - two inode forks containing bmap btree root blocks.
 	- the btree data contained by both forks will fit into the inode size,
 	  hence when combined with the inode core above, we have a total of the
 	  actual inode size.
 	- the BMBT headers need to be accounted separately, as they are
 	  additional to the records and pointers that fit inside the inode
 	  forks.
  Inode btree record insertionremoval modifies the inode btree and free space
  btrees (since the inobt does not use the agfl). This requires the following
  reservation:
  the inode btree: max depth  blocksize
  the allocation btrees: 2 trees  (max depth - 1)  block size
  The caller must account for SB and AG header modifications, etc.
  The free inode btree is a conditional feature. The behavior differs slightly
  from that of the traditional inode btree in that the finobt tracks records
  for inode chunks with at least one free inode. A record can be removed from
  the tree during individual inode allocation. Therefore the finobt
  reservation is unconditional for both the inode chunk allocation and
  individual inode allocation (modify) cases.
  Behavior aside, the reservation for finobt modification is equivalent to the
  traditional inobt: cover a full finobt shape change plus block allocation.
  Calculate the reservation required to allocate or free an inode chunk. This
  includes:
  the allocation btrees: 2 trees  (max depth - 1)  block size
  the inode chunk: m_ino_geo.ialloc_blks  N
  The size N of the inode chunk reservation depends on whether it is for
  allocation or free and which type of create transaction is in use. An inode
  chunk free always invalidates the buffers and only requires reservation for
  headers (N == 0). An inode chunk allocation requires a chunk sized
  reservation on v4 and older superblocks to initialize the chunk. No chunk
  reservation is required for allocation on v5 supers, which use ordered
  buffers to initialize.
 icreate tx uses ordered buffers 
  Per-extent log reservation for the btree changes involved in freeing or
  allocating a realtime extent.  We have to be able to log as many rtbitmap
  blocks as needed to mark inuse MAXEXTLEN blocks' worth of realtime extents,
  as well as the realtime summary block.
  Various log reservation values.
  These are based on the size of the file system block because that is what
  most transactions manipulate.  Each adds in an additional 128 bytes per
  item logged to try to account for the overhead of the transaction mechanism.
  Note:  Most of the reservations underestimate the number of allocation
  groups into which they could free extents in the xfs_defer_finish() call.
  This is because the number in the worst case is quite high and quite
  unusual.  In order to fix this we need to change xfs_defer_finish() to free
  extents in only a single AG at a time.  This will require changes to the
  EFI code as well, however, so that the EFI for the extents not freed is
  logged again in each transaction.  See SGI PV #261917.
  Reservation functions here avoid a huge stack in xfs_trans_init due to
  register overflow from temporaries in the calculations.
  In a write transaction we can allocate a maximum of 2
  extents.  This gives (t1):
     the inode getting the new extents: inode size
     the inode's bmap btree: max depth  block size
     the agfs of the ags from which the extents are allocated: 2  sector
     the superblock free block counter: sector size
     the allocation btrees: 2 exts  2 trees  (2  max depth - 1)  block size
  Or, if we're writing to a realtime file (t2):
     the inode getting the new extents: inode size
     the inode's bmap btree: max depth  block size
     the agfs of the ags from which the extents are allocated: 2  sector
     the superblock free block counter: sector size
     the realtime bitmap: ((MAXEXTLEN  rtextsize)  NBBY) bytes
     the realtime summary: 1 block
     the allocation btrees: 2 trees  (2  max depth - 1)  block size
  And the bmap_finish transaction can free bmap blocks in a join (t3):
     the agfs of the ags containing the blocks: 2  sector size
     the agfls of the ags containing the blocks: 2  sector size
     the super block free block counter: sector size
     the allocation btrees: 2 exts  2 trees  (2  max depth - 1)  block size
  In truncating a file we free up to two extents at once.  We can modify (t1):
     the inode being truncated: inode size
     the inode's bmap btree: (max depth + 1)  block size
  And the bmap_finish transaction can free the blocks and bmap blocks (t2):
     the agf for each of the ags: 4  sector size
     the agfl for each of the ags: 4  sector size
     the super block to reflect the freed blocks: sector size
     worst case split in allocation btrees per extent assuming 4 extents:
 		4 exts  2 trees  (2  max depth - 1)  block size
  Or, if it's a realtime file (t3):
     the agf for each of the ags: 2  sector size
     the agfl for each of the ags: 2  sector size
     the super block to reflect the freed blocks: sector size
     the realtime bitmap: 2 exts  ((MAXEXTLEN  rtextsize)  NBBY) bytes
     the realtime summary: 2 exts  1 block
     worst case split in allocation btrees per extent assuming 2 extents:
 		2 exts  2 trees  (2  max depth - 1)  block size
  In renaming a files we can modify:
     the four inodes involved: 4  inode size
     the two directory btrees: 2  (max depth + v2)  dir block size
     the two directory bmap btrees: 2  max depth  block size
  And the bmap_finish transaction can free dir and bmap blocks (two sets
 	of bmap blocks) giving:
     the agf for the ags in which the blocks live: 3  sector size
     the agfl for the ags in which the blocks live: 3  sector size
     the superblock for the free block count: sector size
     the allocation btrees: 3 exts  2 trees  (2  max depth - 1)  block size
  For removing an inode from unlinked list at first, we can modify:
     the agi hash list and counters: sector size
     the on disk inode before ours in the agi hash list: inode cluster size
     the on disk inode in the agi hash list: inode cluster size
  For creating a link to an inode:
     the parent directory inode: inode size
     the linked inode: inode size
     the directory btree could split: (max depth + v2)  dir block size
     the directory bmap btree could join or split: (max depth + v2)  blocksize
  And the bmap_finish transaction can free some bmap blocks giving:
     the agf for the ag in which the blocks live: sector size
     the agfl for the ag in which the blocks live: sector size
     the superblock for the free block count: sector size
     the allocation btrees: 2 trees  (2  max depth - 1)  block size
  For adding an inode to unlinked list we can modify:
     the agi hash list: sector size
     the on disk inode: inode cluster size
  For removing a directory entry we can modify:
     the parent directory inode: inode size
     the removed inode: inode size
     the directory btree could join: (max depth + v2)  dir block size
     the directory bmap btree could join or split: (max depth + v2)  blocksize
  And the bmap_finish transaction can free the dir and bmap blocks giving:
     the agf for the ag in which the blocks live: 2  sector size
     the agfl for the ag in which the blocks live: 2  sector size
     the superblock for the free block count: sector size
     the allocation btrees: 2 exts  2 trees  (2  max depth - 1)  block size
  For create, break it in to the two cases that the transaction
  covers. We start with the modify case - allocation done by modification
  of the state of existing inodes - and the allocation case.
  For create we can modify:
     the parent directory inode: inode size
     the new inode: inode size
     the inode btree entry: block size
     the superblock for the nlink flag: sector size
     the directory btree: (max depth + v2)  dir block size
     the directory inode's bmap btree: (max depth + v2)  block size
     the finobt (record modification and allocation btrees)
  For icreate we can allocate some inodes giving:
     the agi and agf of the ag getting the new inodes: 2  sectorsize
     the superblock for the nlink flag: sector size
     the inode chunk (allocation, optional init)
     the inobt (record insertion)
     the finobt (optional, record insertion)
  Making a new directory is the same as creating a new file.
  Making a new symplink is the same as creating a new file, but
  with the added blocks for remote symlink data which can be up to 1kB in
  length (XFS_SYMLINK_MAXLEN).
  In freeing an inode we can modify:
     the inode being freed: inode size
     the super block free inode counter, AGF and AGFL: sector size
     the on disk inode (agi unlinked list removal)
     the inode chunk (invalidated, headers only)
     the inode btree
     the finobt (record insertion, removal or modification)
  Note that the inode chunk res. includes an allocfree res. for freeing of the
  inode chunk. This is technically extraneous because the inode chunk free is
  deferred (it occurs after a transaction roll). Include the extra reservation
  anyways since we've had reports of ifree transaction overruns due to too many
  agfl fixups during inode chunk frees.
  When only changing the inode we log the inode and possibly the superblock
  We also add a bit of slop for the transaction stuff.
  Growing the data section of the filesystem.
 	superblock
 	agi and agf
 	allocation btrees
  Growing the rt section of the filesystem.
  In the first set of transactions (ALLOC) we allocate space to the
  bitmap or summary files.
 	superblock: sector size
 	agf of the ag from which the extent is allocated: sector size
 	bmap btree for bitmapsummary inode: max depth  blocksize
 	bitmapsummary inode: inode size
 	allocation btrees for 1 block alloc: 2  (2  maxdepth - 1)  blocksize
  Growing the rt section of the filesystem.
  In the second set of transactions (ZERO) we zero the new metadata blocks.
 	one bitmapsummary block: blocksize
  Growing the rt section of the filesystem.
  In the third set of transactions (FREE) we update metadata without
  allocating any new blocks.
 	superblock: sector size
 	bitmap inode: inode size
 	summary inode: inode size
 	one bitmap block: blocksize
 	summary blocks: new summary size
  Logging the inode modification timestamp on a synchronous write.
 	inode
  Logging the inode mode bits when writing a setuidsetgid file
 	inode
  Converting the inode from non-attributed to attributed.
 	the inode being converted: inode size
 	agf block and superblock (for block allocation)
 	the new block (directory sized)
 	bmap blocks for the new directory block
 	allocation btrees
  Removing the attribute fork of a file
     the inode being truncated: inode size
     the inode's bmap btree: max depth  block size
  And the bmap_finish transaction can free the blocks and bmap blocks:
     the agf for each of the ags: 4  sector size
     the agfl for each of the ags: 4  sector size
     the super block to reflect the freed blocks: sector size
     worst case split in allocation btrees per extent assuming 4 extents:
 		4 exts  2 trees  (2  max depth - 1)  block size
  Setting an attribute at mount time.
 	the inode getting the attribute
 	the superblock for allocations
 	the agfs extents are allocated from
 	the attribute btree  max depth
 	the inode allocation btree
  Since attribute transaction space is dependent on the size of the attribute,
  the calculation is done partially at mount time and partially at runtime(see
  below).
  Setting an attribute at runtime, transaction space unit per block.
  	the superblock for allocations: sector size
 	the inode bmap btree could join or split: max depth  block size
  Since the runtime attribute transaction space is dependent on the total
  blocks needed for the 1st bmap, here we calculate out the space unit for
  one block so that the caller could figure out the total space according
  to the attibute extent length in blocks by:
 	ext  M_RES(mp)->tr_attrsetrt.tr_logres
  Removing an attribute.
     the inode: inode size
     the attribute btree could join: max depth  block size
     the inode bmap btree could join or split: max depth  block size
  And the bmap_finish transaction can free the attr blocks freed giving:
     the agf for the ag in which the blocks live: 2  sector size
     the agfl for the ag in which the blocks live: 2  sector size
     the superblock for the free block count: sector size
     the allocation btrees: 2 exts  2 trees  (2  max depth - 1)  block size
  Clearing a bad agino number in an agi hash bucket.
  Adjusting quota limits.
     the disk quota buffer: sizeof(struct xfs_disk_dquot)
  Allocating quota on disk if needed.
 	the write transaction log space for quota file extent allocation
 	the unit of quota allocation: one system block size
  Syncing the incore super block changes to disk.
      the super block to reflect the changes: sector size
	
	  In the early days of rmap+reflink, we always set the rmap maxlevels
	  to 9 even if the AG was small enough that it would never grow to
	  that height.  Transaction reservation sizes influence the minimum
	  log size calculation, which influences the size of the log that mkfs
	  creates.  Use the old value here to ensure that newly formatted
	  small filesystems will mount on older kernels.
	
	  The following transactions are logged in physical format and
	  require a permanent reservation on space.
	
	  The following transactions are logged in logical format with
	  a default log count.
 growdata requires permanent res; it can free space to the last AG 
 The following transaction are logged in logical format 
 Put everything back the way it was.  This goes at the end. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  Copyright (c) 2013 Red Hat, Inc.
  All Rights Reserved.
 # of map entries at once 
  Remote Attribute Values
  =======================
  Remote extended attribute values are conceptually simple -- they're written
  to data blocks mapped by an inode's attribute fork, and they have an upper
  size limit of 64k.  Setting a value does not involve the XFS log.
  However, on a v5 filesystem, maximally sized remote attr values require one
  block more than 64k worth of space to hold both the remote attribute value
  header (64 bytes).  On a 4k block filesystem this results in a 68k buffer;
  on a 64k block filesystem, this would be a 128k buffer.  Note that the log
  format can only handle a dirty buffer of XFS_MAX_BLOCKSIZE length (64k).
  Therefore, we must ensure that remote attribute value buffers never touch
  the logging system and therefore never have a log item.
  Each contiguous block has a header, so it is not just a simple attribute
  length to FSB conversion.
  Checking of the remote attribute header is split into two parts. The verifier
  does CRC, location and bounds checking, the unpacking function checks the
  attribute parameters and owner.
 ok 
 no verification of non-crc buffers 
 no verification of non-crc buffers 
		
		  Ensure we aren't writing bogus LSNs to disk. See
		  xfs_attr3_rmt_hdr_set() for the explanation.
	
	  Remote attribute blocks are written synchronously, so we don't
	  have an LSN that we can stamp in them that makes any sense to log
	  recovery. To ensure that log recovery handles overwrites of these
	  blocks sanely (i.e. once they've been freed and reallocated as some
	  other type of metadata) we need to ensure that the LSN has a value
	  that tells log recovery to ignore the LSN and overwrite the buffer
	  with whatever is in it's log. To do this, we use the magic
	  NULLCOMMITLSN to indicate that the LSN is invalid.
  Helper functions to copy attribute data in and out of the one disk extents
 roll buffer forwards 
 roll attribute data forwards 
		
		  If this is the last block, zero the remainder of it.
		  Check that we are actually the last block, too.
 roll buffer forwards 
 roll attribute data forwards 
  Read the value associated with an attribute from the out-of-line buffer
  that we stored it in.
  Returns 0 on successful retrieval, otherwise an error.
 roll attribute extent map forwards 
  Find a "hole" in the attribute address space large enough for us to drop the
  new attributes value into
	
	  Because CRC enable attributes have headers, we can't just do a
	  straight byte to FSB conversion and have to take the header space
	  into account.
	
	  Roll through the "value", copying the attribute value to the
	  already-allocated blocks.  Blocks are written synchronously
	  so that we can know they are all on disk before we turn off
	  the INCOMPLETE flag.
 GROT: NOTE: synchronous write 
 roll attribute extent map forwards 
 Mark stale any incore buffers for the remote value. 
  Find a hole for the attr and store it in the delayed attr context.  This
  initializes the context to roll through allocating an attr extent for a
  delayed attr operation
  Write one block of the value associated with an attribute into the
  out-of-line buffer that we have defined for it. This is similar to a subset
  of xfs_attr_rmtval_set, but records the current block to the delayed attr
  context, and leaves transaction handling to the caller.
 roll attribute extent map forwards 
  Remove the value associated with an attribute by deleting the
  out-of-line buffer that it is stored on.
	
	  Roll through the "value", invalidating the attribute value's blocks.
		
		  Try to remember where we decided to put the value.
  Remove the value associated with an attribute by deleting the out-of-line
  buffer that it is stored on. Returns -EAGAIN for the caller to refresh the
  transaction and re-call the function.  Callers should keep calling this
  routine until it returns something other than -EAGAIN.
	
	  Unmap value blocks for this attr.
	
	  We don't need an explicit state here to pick up where we left off. We
	  can figure it out using the !done return code. The actual value of
	  attr->xattri_dela_state may be some value reminiscent of the calling
	  function, but it's value is irrelevant with in the context of this
	  function. Once we are done here, the next state is set as needed by
	  the parent
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000,2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Add a locked inode to the transaction.
  The inode must be locked, and it cannot be associated with any transaction.
  If lock_flags is non-zero the inode will be unlocked on transaction commit.
	
	  Get a log_item_desc to point at the new item.
  Transactional inode timestamp update. Requires the inode to be locked and
  joined to the transaction supplied. Relies on the transaction subsystem to
  track dirty state and updatewriteback the inode accordingly.
  This is called to mark the fields indicated in fieldmask as needing to be
  logged when the transaction is committed.  The inode must already be
  associated with the given transaction.
  The values for fieldmask are defined in xfs_inode_item.h.  We always log all
  of the core inode if any of it has changed, and we always log all of the
  inline dataextentsb-tree root if any of them has changed.
  Grab and pin the cluster buffer associated with this inode to avoid RMW
  cycles at inode writeback time. Avoid the need to add error handling to every
  xfs_trans_log_inode() call by shutting down on read error.  This will cause
  transactions to fail and everything to error out, just like if we return a
  read error in a dirty transaction and cancel it.
	
	  Don't bother with i_lock for the I_DIRTY_TIME check here, as races
	  don't matter - we either will need an extra transaction in 24 hours
	  to log the timestamps, or will clear already cleared fields in the
	  worst case.
	
	  First time we log the inode in a transaction, bump the inode change
	  counter if it is configured for this to occur. While we have the
	  inode locked exclusively for metadata modification, we can usually
	  avoid setting XFS_ILOG_CORE if no one has queried the value since
	  the last time it was incremented. If we have XFS_ILOG_CORE already
	  set however, then go ahead and bump the i_version counter
	  unconditionally.
	
	  If we're updating the inode core or the timestamps and it's possible
	  to upgrade this inode to bigtime format, do so now.
	
	  Inode verifiers do not check that the extent size hint is an integer
	  multiple of the rt extent size on a directory with both rtinherit
	  and extszinherit flags set.  If we're logging a directory that is
	  misconfigured in this way, clear the hint.
	
	  Record the specific change for fdatasync optimisation. This allows
	  fdatasync to skip log forces for inodes that are only timestamp
	  dirty.
		
		  We hold the ILOCK here, so this inode is not going to be
		  flushed while we are here. Further, because there is no
		  buffer attached to the item, we know that there is no IO in
		  progress, so nothing will clear the ili_fields while we read
		  in the buffer. Hence we can safely drop the spin lock and
		  read the buffer knowing that the state will not change from
		  here.
		
		  We need an explicit buffer reference for the log item but
		  don't want the buffer to remain attached to the transaction.
		  Hold the buffer but release the transaction reference once
		  we've attached the inode log item to the buffer log item
		  list.
	
	  Always OR in the bits from the ili_last_fields field.  This is to
	  coordinate with the xfs_iflush() and xfs_buf_inode_iodone() routines
	  in the eventual clearing of the ili_fields bits.  See the big comment
	  in xfs_iflush() for an explanation of this coordination mechanism.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2013 Jie Liu.
  All Rights Reserved.
  Calculate the maximum length in bytes that would be required for a local
  attribute value as large attributes out of line are not logged.
  Iterate over the log space reservation table to figure out and return
  the maximum one in terms of the pre-calculated values which were done
  at mount time.
 struct copy 
 struct copy 
  Calculate the minimum valid log size for the given superblock configuration.
  Used to calculate the minimum log size at mkfs time, and to determine if
  the log is large enough or not at mount time. Returns the minimum size in
  filesystem block size units.
	
	  Two factors should be taken into account for calculating the minimum
	  log space.
	  1) The fundamental limitation is that no single transaction can be
	     larger than half size of the log.
	 
	     From mkfs.xfs, this is considered by the XFS_MIN_LOG_FACTOR
	     define, which is set to 3. That means we can definitely fit
	     maximally sized 2 transactions in the log. We'll use this same
	     value here.
	 
	  2) If the lsunit option is specified, a transaction requires 2 LSU
	     for the reservation because there are two log writes that can
	     require padding - the transaction data and the commit record which
	     are written separately and both can require padding to the LSU.
	     Consider that we can have an active CIL reservation holding 2LSU,
	     but the CIL is not over a push threshold, in this case, if we
	     don't have enough log space for at one new transaction, which
	     includes another 2LSU in the reservation, we will run into dead
	     loop situation in log space grant procedure. i.e.
	     xlog_grant_head_wait().
	 
	     Hence the log size needs to be able to contain two maximally sized
	     and padded transactions, which is (2  (2  LSU + maxlres)).
	 
	  Also, the log size should be a multiple of the log stripe unit, round
	  it up to lsunit boundary if lsunit is specified.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2003,2005 Silicon Graphics, Inc.
  Copyright (c) 2013 Red Hat, Inc.
  All Rights Reserved.
  Local function prototypes.
  One-time startup routine called from xfs_init().
 Check things that we can't do in the verifier. 
	
	  If there are stale entries we'll use one for the leaf.
			
			  The biggest entry enough to avoid compaction.
		
		  Will need to compact to make this work.
		  Tag just before the first leaf entry.
 Data object just before the first leaf entry.  
		
		  If it's not free then the data will go where the
		  leaf data starts now, if it works at all.
	
	  no stale entries, so just use free space.
	  Tag just before the first leaf entry.
 Data object just before the first leaf entry.  
	
	  If it's not free then can't do this add without cleaning up:
	  the space before the first leaf entry needs to be free so it
	  can be expanded to hold the pointer to the new entry.
		
		  Check out the biggest freespace and see if it's the same one.
			
			  Not the same free entry, just check its length.
		
		  It is the biggest freespace, can it hold the leaf too?
			
			  Yes, use the second-largest entry instead if it works.
  compact the leaf entries.
  Leave the highest-numbered stale entry stale.
  XXX should be the one closest to mid but mid is not yet computed.
 source leaf index 
 target leaf index 
 high stale index 
	
	  If we now need to rebuild the bestfree map, do so.
	  This needs to happen before the next call to use_free.
  Add an entry to a block directory.
 error 
 directory op arguments 
 block header 
 block leaf entries 
 buffer for block 
 block tail 
 need to compact leaf ents 
 block data entry 
 directory inode 
 block unused entry 
 error return value 
 unused at end of data 
 hash value of found entry 
 high index for binary srch 
 high stale index 
 last final leaf to log 
 first final leaf to log 
 length of the new entry 
 low index for binary srch 
 low stale index 
 midpoint for binary srch 
 need to log header 
 need to rescan freespace 
 pointer to tag value 
 transaction structure 
 Read the (one and only) directory block into bp. 
	
	  Set up pointers to parts of the block.
	
	  Find out if we can reuse stale entries or whether we need extra
	  space for entry and new leaf.
	
	  Done everything we need for a space check now.
	
	  If we don't have space for the new entry & leaf ...
 Don't have a space reservation: return no-space.  
		
		  Convert to the next larger format.
		  Then add the new entry in that format.
	
	  If need to compact the leaf entries, do it now.
 recalculate blp post-compaction 
		
		  Set leaf logging boundaries to impossible state.
		  For the no-stale case they're set explicitly.
	
	  Find the slot that's first lower than our hash value, -1 if none.
	
	  No stale entries, will use enddup space to hold new leaf.
		
		  Mark the space needed for the new leaf entry, now in use.
		
		  Update the tail (entry count).
		
		  If we now need to rebuild the bestfree map, do so.
		  This needs to happen before the next call to use_free.
		
		  Adjust pointer to the first leaf entry, we're about to move
		  the table up one to open up space for the new leaf entry.
		  Then adjust our index to match.
	
	  Use a stale leaf for our new entry.
		
		  Move entries toward the low-numbered stale entry.
		
		  Move entries toward the high-numbered stale entry.
	
	  Point to the new data entry.
	
	  Fill in the leaf entry.
	
	  Mark space for the data entry used.
	
	  Create the new data entry.
	
	  Clean up the bestfree array and log the header, tail, and entry.
  Log leaf entries from the block.
 transaction structure 
 block buffer 
 index of first logged leaf 
 index of last logged leaf 
  Log the block tail.
 transaction structure 
 block buffer 
  Look up an entry in the block.  This is the external routine,
  xfs_dir2_block_lookup_int does the real work.
 error 
 dir lookup arguments 
 block header 
 block leaf entries 
 block buffer 
 block tail 
 block data entry 
 incore inode 
 entry index 
 error return value 
	
	  Get the buffer, look up the entry.
	  If not found (ENOENT) then return, have no buffer.
	
	  Get the offset from the leaf entry, to point to the data.
	
	  Fill in inode number, CI name if appropriate, release the block.
  Internal block lookup routine.
 error 
 dir lookup arguments 
 returned block buffer 
 returned entry number 
 data entry address 
 block header 
 block leaf entries 
 block buffer 
 block tail 
 block data entry 
 incore inode 
 error return value 
 found hash value 
 binary search high index 
 binary search low index 
 binary search current idx 
 transaction pointer 
 comparison result 
	
	  Loop doing a binary search for our hash value.
	  Find our entry, ENOENT if it's not there.
	
	  Back up to the first one with the right hash value.
	
	  Now loop forward through all the entries with the
	  right hash value looking for our name.
		
		  Get pointer to the entry from the leaf.
		
		  Compare name and if it's an exact match, return the index
		  and buffer. If it's the first case-insensitive match, store
		  the index and buffer and continue looking for an exact match.
	
	  Here, we can only be doing a lookup (not a rename or replace).
	  If a case-insensitive match was found earlier, return success.
	
	  No match, release the buffer and return ENOENT.
  Remove an entry from a block format directory.
  If that makes the block small enough to fit in shortform, transform it.
 error 
 directory operation args 
 block header 
 block leaf pointer 
 block buffer 
 block tail 
 block data entry 
 incore inode 
 block leaf entry index 
 error return value 
 need to log block header 
 need to fixup bestfree 
 shortform header 
 shortform size 
 transaction pointer 
	
	  Look up the entry in the block.  Gets the buffer and entry index.
	  It will always be there, the vnodeops level does a lookup first.
	
	  Point to the data entry using the leaf entry.
	
	  Mark the data entry's space free.
	
	  Fix up the block tail.
	
	  Remove the leaf entry by marking it stale.
	
	  Fix up bestfree, log the header if necessary.
	
	  See if the size as a shortform is good enough.
	
	  If it works, do the conversion.
  Replace an entry in a V2 block directory.
  Change the inode number to the new value.
 error 
 directory operation args 
 block header 
 block leaf entries 
 block buffer 
 block tail 
 block data entry 
 incore inode 
 leaf entry index 
 error return value 
	
	  Lookup the entry in the directory.  Get buffer and entry index.
	  This will always succeed since the caller has already done a lookup.
	
	  Point to the data entry we need to change.
	
	  Change the inode number to the new value.
  Qsort comparison routine for the block leaf entries.
 sort order 
 first leaf entry 
 second leaf entry 
 first leaf entry 
 second leaf entry 
  Convert a V2 leaf directory to a V2 block directory if possible.
 error 
 operation arguments 
 leaf buffer 
 data buffer 
 leaf bests table 
 block header 
 block tail 
 incore directory inode 
 unused data entry 
 error return value 
 leaf from index 
 leaf structure 
 leaf entry 
 leaf tail structure 
 file system mount point 
 need to log data header 
 need to scan for bestfree 
 shortform header 
 bytes used 
 end of entry (tag) 
 blockleaf to index 
 transaction pointer 
	
	  If there are data blocks other than the first one, take this
	  opportunity to remove trailing empty data blocks that may have
	  been left behind during no-space-reservation operations.
	  These will show up in the leaf bests table.
	
	  Read the data block if we don't already have it, give up if it fails.
	
	  Size of the "leaf" area in the block.
	
	  Look at the last data entry.
	
	  If it's not free or is too short we can't do it.
	
	  Start converting it to block form.
	
	  Use up the space at the end of the block (blpbtp).
	
	  Initialize the block tail.
	
	  Initialize the block leaf area.  We compact out stale entries.
	
	  Scan the bestfree if we need it and log the data block header.
	
	  Pitch the old leaf block.
	
	  Now see if the resulting block can be shrunken to shortform.
  Convert the shortform directory to block form.
 error 
 dir-relative block # (0) 
 block header 
 block leaf entries 
 block buffer 
 block tail pointer 
 data entry pointer 
 trash 
 unused entry pointer 
 end of data objects 
 error return value 
 index 
 need to log block header 
 need to scan block freespc 
 offset from current entry 
 sf entry pointer 
 old shortform header  
 shortform header  
 end of data entry 
	
	  Copy the directory into a temporary buffer.
	  Then pitch the incore inode data so we can make extents.
	
	  Add block 0 to the inode.
	
	  Initialize the data block, then convert it to block format.
	
	  Compute size of block "tail" area.
	
	  The whole thing is initialized to free by the init routine.
	  Say we're using the leaf and tail area.
	
	  Fill in the tail.
 ., .. 
	
	  Remove the freespace, we'll manage it.
	
	  Create entry for .
	
	  Create entry for ..
	
	  Loop over existing entries, stuff them in.
	
	  Need to preserve the existing offset values in the sf directory.
	  Insert holes (unused entries) where necessary.
		
		  sfep is null when we reach the end of the list.
		
		  There should be a hole here, make one.
		
		  Copy a real entry.
 Done with the temporary buffer 
	
	  Sort the leaf entries by hash value.
	
	  Log the leaf entry area and tail.
	  Already logged the header in data_init, ignore needlog.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2017 Christoph Hellwig.
  In-core extent record layout:
  +-------+----------------------------+
  | 00:53 | all 54 bits of startoff    |
  | 54:63 | low 10 bits of startblock  |
  +-------+----------------------------+
  | 00:20 | all 21 bits of length      |
  |    21 | unwritten extent bit       |
  | 22:63 | high 42 bits of startblock |
  +-------+----------------------------+
  Given that the length can't be a zero, only an empty hi value indicates an
  unused record.
  In-core extent btree block layout:
  There are two types of blocks in the btree: leaf and inner (non-leaf) blocks.
  The leaf blocks are made up by %KEYS_PER_NODE extent records, which each
  contain the startoffset, blockcount, startblock and unwritten extent flag.
  See above for the exact format, followed by pointers to the previous and next
  leaf blocks (if there are any).
  The inner (non-leaf) blocks first contain KEYS_PER_NODE lookup keys, followed
  by an equal number of pointers to the btree blocks at the next lower level.
 		+-------+-------+-------+-------+-------+----------+----------+
  Leaf:	| rec 1 | rec 2 | rec 3 | rec 4 | rec N | prev-ptr | next-ptr |
 		+-------+-------+-------+-------+-------+----------+----------+
 		+-------+-------+-------+-------+-------+-------+------+-------+
  Inner:	| key 1 | key 2 | key 3 | key N | ptr 1 | ptr 2 | ptr3 | ptr N |
 		+-------+-------+-------+-------+-------+-------+------+-------+
 for sequential append operations just spill over into the new node 
	
	  Update the pointers in higher levels if the first entry changes
	  in an existing node.
 for sequential append operations just spill over into the new node 
 now that we have a node step into it 
 account for the prevnext pointers 
  Increment the sequence counter on extent tree changes. If we are on a COW
  fork, this allows the writeback code to skip looking for a COW extent if the
  COW fork hasn't changed. We use WRITE_ONCE here to ensure the update to the
  sequence counter is seen before the modifications to the extent tree itself
  take effect.
	
	  Update the pointers in higher levels if the first entry changes
	  in an existing node.
	
	  If the neighbouring nodes are completely full, or have different
	  parents, we might never be able to merge our node, and will only
	  delete it once the number of entries hits zero.
			
			  Merge the next node into this node so that we don't
			  have to do an additional update of the keys in the
			  higher levels.
		
		  If we aren't at the root yet try to find a neighbour node to
		  merge with (or delete the node if it is empty), and then
		  recurse up to the next level.
		
		  If we are at the root and only one entry is left we can just
		  free this node and update the root pointer.
	
	  If the neighbouring nodes are completely full we might never be able
	  to merge our node, and will only delete it once the number of
	  entries hits zero.
			
			  Merge the next node into this node so that we don't
			  have to do an additional update of the keys in the
			  higher levels.
  Lookup the extent covering bno.
  If there is an extent covering bno return the extent index, and store the
  expanded extent structure in gotp, and the extent cursor in cur.
  If there is no extent covering bno, but there is an extent after it (e.g.
  it lies in a hole) return that extent in gotp and its cursor in cur
  instead.
  If bno is beyond the last extent return false, and return an invalid
  cursor value.
 Try looking in the next node for an entry > offset 
  Returns the last extent before end, and if this extent doesn't cover
  end, update end to the end of the extent.
 could be optimized to not even look up the next on a match.. 
  Return true if the cursor points at an extent and return the extent structure
  in gotp.  Else return false.
  This is a recursive function, because of that we need to be extremely
  careful with stack usage.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2014 Red Hat, Inc.
  All Rights Reserved.
  Reverse map btree.
  This is a per-ag tree used to track the owner(s) of a given extent. With
  reflink it is possible for there to be multiple owners, which is a departure
  from classic XFS. Owner records for data extents are inserted when the
  extent is mapped and removed when an extent is unmapped.  Owner records for
  all other block types (i.e. metadata) are inserted when an extent is
  allocated and removed when an extent is freed. There can only be one owner
  of a metadata extent, usually an inode or some other metadata structure like
  an AG btree.
  The rmap btree is part of the free space management, so blocks for the tree
  are sourced from the agfl. Hence we need transaction reservation support for
  this tree so that the freelist is always large enough. This also impacts on
  the minimum space we need to leave free in the AG.
  The tree is ordered by [ag block, owner, offset]. This is a large key size,
  but it is the only way to enforce unique keys when a block can be owned by
  multiple files at any offset. There's no need to ordersearch by extent
  size for online updatingmanagement of the tree. It is intended that most
  reverse lookups will be to find the owner(s) of a particular block, or to
  try to recover tree and file data from corrupt primary metadata.
 Allocate the new block from the freelist. If we can't, give up.  
  The high key for a reverse mapping record can be computed by shifting
  the startblock and offset to the highest value that would still map
  to that record.  In practice this means that we add blockcount-1 to
  the startblock for all records, and if the record is for a dataattr
  fork mapping, we add blockcount-1 to the offset too.
	
	  magic number and level verification
	 
	  During growfs operations, we can't verify the exact level or owner as
	  the perag is not fully initialised and hence not attached to the
	  buffer.  In this case, check against the maximum tree depth.
	 
	  Similarly, during log recovery we will have a perag structure
	  attached, but the agf information will not yet have been initialised
	  from the on disk AGF. Again, we can only check against maximum limits
	  in this case.
 Overlapping btree; 2 keys per pointer. 
 take a reference for the cursor 
 Create a new reverse mapping btree cursor. 
 Create a new reverse mapping btree cursor with a fake root for staging. 
  Install a new reverse mapping btree root.  Caller is responsible for
  invalidating and freeing the old btree blocks.
 Calculate number of records in a reverse mapping btree block. 
  Calculate number of records in an rmap btree block.
 Compute the max possible height for reverse mapping btrees. 
	
	  Compute the asymptotic maxlevels for an rmapbt on any reflink fs.
	 
	  On a reflink filesystem, each AG block can have up to 2^32 (per the
	  refcount record format) owners, which means that theoretically we
	  could face up to 2^64 rmap records.  However, we're likely to run
	  out of blocks in the AG long before that happens, which means that
	  we must compute the max height based on what the btree will look
	  like if it consumes almost all the blocks in the AG due to maximal
	  sharing factor.
 Compute the maximum height of an rmap btree. 
		
		  Compute the asymptotic maxlevels for an rmap btree on a
		  filesystem that supports reflink.
		 
		  On a reflink filesystem, each AG block can have up to 2^32
		  (per the refcount record format) owners, which means that
		  theoretically we could face up to 2^64 rmap records.
		  However, we're likely to run out of blocks in the AG long
		  before that happens, which means that we must compute the
		  max height based on what the btree will look like if it
		  consumes almost all the blocks in the AG due to maximal
		  sharing factor.
		
		  If there's no block sharing, compute the maximum rmapbt
		  height assuming one rmap record per AG block.
 Calculate the refcount btree size for some records. 
  Calculate the maximum refcount btree size.
 Bail out if we're uninitialized, which can happen in mkfs. 
  Figure out how many blocks to reserve and how many are used by this btree.
	
	  The log is permanently allocated, so the space it occupies will
	  never be available for the kinds of things that would require btree
	  expansion.  We therefore can pretend the space isn't there.
 Reserve 1% of the AG or enough for 1 block per record. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2014 Red Hat, Inc.
  All Rights Reserved.
  Lookup the first record less than or equal to [bno, len, owner, offset]
  in the btree given by cur.
  Lookup the record exactly matching [bno, len, owner, offset]
  in the btree given by cur.
  Update the record referred to by cur to the value given
  by [bno, len, owner, offset].
  This either works (return 0) or gets an EFSCORRUPTED error.
 Convert an internal btree record to an rmap record. 
  Get the data from the pointed-to record.
 check for valid extent range, including overflow 
 For each rmap given, figure out if it matches the key we want. 
  Find the record to the left of the given extent, being careful only to
  return a match with the same owner and adjacent physical and logical
  block ranges.
 For each rmap given, figure out if it matches the key we want. 
  Find the record to the left of the given extent, being careful only to
  return a match with the same owner and overlapping physical and logical
  block ranges.  This is the overlapping-interval version of
  xfs_rmap_lookup_le.
  Perform all the relevant owner checks for a removal op.  If we're doing an
  unknown-owner removal then we have no owner information to check.
 Make sure the unwritten flag matches. 
 Make sure the owner matches what we expect to find in the tree. 
 Check the offset, if necessary. 
  Find the extent in the rmap btree and remove it.
  The record we find should always be an exact match for the extent that we're
  looking for, since we insert them into the btree without modification.
  Special Case #1: when growing the filesystem, we "free" an extent when
  growing the last AG. This extent is new space and so it is not tracked as
  used space in the btree. The growfs code will pass in an owner of
  XFS_RMAP_OWN_NULL to indicate that it expected that there is no owner of this
  extent. We verify that - the extent lookup result in a record that does not
  overlap.
  Special Case #2: EFIs do not record the owner of the extent, so when
  recovering EFIs from the log we pass in XFS_RMAP_OWN_UNKNOWN to tell the rmap
  btree to ignore the owner (i.e. wildcard match) so we don't trigger
  corruption checks during log recovery.
	
	  We should always have a left record because there's a static record
	  for the AG headers at rm_startblock == 0 created by mkfsgrowfs that
	  will not ever be removed from the tree.
	
	  For growfs, the incoming extent must be beyond the left record we
	  just found as it is new space and won't be used by anyone. This is
	  just a corruption check as we don't actually do anything with this
	  extent.  Note that we need to use >= instead of > because it might
	  be the case that the "left" extent goes all the way to EOFS.
	
	  If we're doing an unknown-owner removal for EFI recovery, we expect
	  to find the full range in the rmapbt or nothing at all.  If we
	  don't find any rmaps overlapping either end of the range, we're
	  done.  Hopefully this means that the EFI creator already queued
	  (and finished) a RUI to remove the rmap.
 Make sure the extent we found covers the entire freeing range. 
 Check owner information. 
 exact match, simply remove the record from rmap tree 
		
		  overlap left hand side of extent: move the start, trim the
		  length and update the current record.
		 
		        ltbno                ltlen
		  Orig:    |oooooooooooooooooooo|
		  Freeing: |fffffffff|
		  Result:            |rrrrrrrrrr|
		          bno       len
		
		  overlap right hand side of extent: trim the length and update
		  the current record.
		 
		        ltbno                ltlen
		  Orig:    |oooooooooooooooooooo|
		  Freeing:            |fffffffff|
		  Result:  |rrrrrrrrrr|
		                     bno       len
		
		  overlap middle of extent: trim the length of the existing
		  record to the length of the new left-extent size, increment
		  the insertion position so we can insert a new record
		  containing the remaining right-extent space.
		 
		        ltbno                ltlen
		  Orig:    |oooooooooooooooooooo|
		  Freeing:       |fffffffff|
		  Result:  |rrrrr|         |rrrr|
		                bno       len
  Remove a reference to an extent in the rmap btree.
  A mergeable rmap must have the same owner and the same values for
  the unwritten, attr_fork, and bmbt flags.  The startblock and
  offset are checked separately.
  When we allocate a new block, the first thing we do is add a reference to
  the extent in the rmap btree. This takes the form of a [agbno, length,
  owner, offset] record.  Flags are encoded in the high bits of the offset
  field.
	
	  For the initial lookup, look for an exact match or the left-adjacent
	  record for our insertion point. This will also give us the record for
	  start block contiguity tests.
	
	  Increment the cursor to see if we have a right-adjacent record to our
	  insertion point. This will give us the record for end block
	  contiguity tests.
	
	  Note: cursor currently points one record to the right of ltrec, even
	  if there is no record in the tree to the right.
		
		  left edge contiguous, merge into left record.
		 
		        ltbno     ltlen
		  orig:   |ooooooooo|
		  adding:           |aaaaaaaaa|
		  result: |rrrrrrrrrrrrrrrrrrr|
		                   bno       len
			
			  right edge also contiguous, delete right record
			  and merge into left record.
			 
			        ltbno     ltlen    gtbno     gtlen
			  orig:   |ooooooooo|         |ooooooooo|
			  adding:           |aaaaaaaaa|
			  result: |rrrrrrrrrrrrrrrrrrrrrrrrrrrrr|
 point the cursor back to the left record and update 
		
		  right edge contiguous, merge into right record.
		 
		                  gtbno     gtlen
		  Orig:             |ooooooooo|
		  adding: |aaaaaaaaa|
		  Result: |rrrrrrrrrrrrrrrrrrr|
		         bno       len
		
		  no contiguous edge with identical owner, insert
		  new record at current cursor position.
  Add a reference to an extent in the rmap btree.
  Convert an unwritten extent to a real extent or vice versa.
  Does not handle overlapping extents.
 neighbor extent entries 
 left is 0, right is 1, 
 prev is 2, new is 3 
	
	  For the initial lookup, look for an exact match or the left-adjacent
	  record for our insertion point. This will also give us the record for
	  start block contiguity tests.
	
	  Set flags determining what part of the previous oldext allocation
	  extent is being replaced by a newext allocation.
	
	  Decrement the cursor to see if we have a left-adjacent record to our
	  insertion point. This will give us the record for end block
	  contiguity tests.
	
	  Increment the cursor to see if we have a right-adjacent record to our
	  insertion point. This will give us the record for end block
	  contiguity tests.
 check that left + prev + right is not too long 
 reset the cursor back to PREV 
	
	  Switch out based on the FILLING and CONTIG state bits.
		
		  Setting all of a previous oldext extent to newext.
		  The left and right neighbors are both contiguous with new.
		
		  Setting all of a previous oldext extent to newext.
		  The left neighbor is contiguous, the right is not.
		
		  Setting all of a previous oldext extent to newext.
		  The right neighbor is contiguous, the left is not.
		
		  Setting all of a previous oldext extent to newext.
		  Neither the left nor right neighbors are contiguous with
		  the new one.
		
		  Setting the first part of a previous oldext extent to newext.
		  The left neighbor is contiguous.
		
		  Setting the first part of a previous oldext extent to newext.
		  The left neighbor is not contiguous.
		
		  Setting the last part of a previous oldext extent to newext.
		  The right neighbor is contiguous with the new allocation.
		
		  Setting the last part of a previous oldext extent to newext.
		  The right neighbor is not contiguous.
		
		  Setting the middle part of a previous oldext extent to
		  newext.  Contiguity is impossible here.
		  One extent becomes three extents.
 new right extent - oldext 
 new left extent - oldext 
		
		  Reset the cursor to the position of the new extent
		  we are about to insert as we can't trust it after
		  the previous insert.
 new middle extent - newext 
		
		  These cases are all impossible.
  Convert an unwritten extent to a real extent or vice versa.  If there is no
  possibility of overlapping extents, delegate to the simpler convert
  function.
 neighbor extent entries 
 left is 0, right is 1, 
 prev is 2, new is 3 
	
	  For the initial lookup, look for and exact match or the left-adjacent
	  record for our insertion point. This will also give us the record for
	  start block contiguity tests.
	
	  Set flags determining what part of the previous oldext allocation
	  extent is being replaced by a newext allocation.
 Is there a left record that abuts our range? 
 Is there a right record that abuts our range? 
 check that left + prev + right is not too long 
	
	  Switch out based on the FILLING and CONTIG state bits.
		
		  Setting all of a previous oldext extent to newext.
		  The left and right neighbors are both contiguous with new.
		
		  Setting all of a previous oldext extent to newext.
		  The left neighbor is contiguous, the right is not.
		
		  Setting all of a previous oldext extent to newext.
		  The right neighbor is contiguous, the left is not.
		
		  Setting all of a previous oldext extent to newext.
		  Neither the left nor right neighbors are contiguous with
		  the new one.
		
		  Setting the first part of a previous oldext extent to newext.
		  The left neighbor is contiguous.
		
		  Setting the first part of a previous oldext extent to newext.
		  The left neighbor is not contiguous.
		
		  Setting the last part of a previous oldext extent to newext.
		  The right neighbor is contiguous with the new allocation.
		
		  Setting the last part of a previous oldext extent to newext.
		  The right neighbor is not contiguous.
		
		  Setting the middle part of a previous oldext extent to
		  newext.  Contiguity is impossible here.
		  One extent becomes three extents.
 new right extent - oldext 
 new left extent - oldext 
 new middle extent - newext 
		
		  These cases are all impossible.
  Find an extent in the rmap btree and unmap it.  For rmap extent types that
  can overlap (data fork rmaps on reflink filesystems) we must be careful
  that the prevnext records in the btree might belong to another owner.
  Therefore we must use delete+insert to alter any of the key fields.
  For every other situation there can only be one owner for a given extent,
  so we can call the regular _free function.
	
	  We should always have a left record because there's a static record
	  for the AG headers at rm_startblock == 0 created by mkfsgrowfs that
	  will not ever be removed from the tree.
 Make sure the extent we found covers the entire freeing range. 
 Make sure the owner matches what we expect to find in the tree. 
 Make sure the unwritten flag matches. 
 Check the offset. 
 Exact match, simply remove the record from rmap tree. 
		
		  Overlap left hand side of extent: move the start, trim the
		  length and update the current record.
		 
		        ltbno                ltlen
		  Orig:    |oooooooooooooooooooo|
		  Freeing: |fffffffff|
		  Result:            |rrrrrrrrrr|
		          bno       len
 Delete prev rmap. 
 Add an rmap at the new offset. 
		
		  Overlap right hand side of extent: trim the length and
		  update the current record.
		 
		        ltbno                ltlen
		  Orig:    |oooooooooooooooooooo|
		  Freeing:            |fffffffff|
		  Result:  |rrrrrrrrrr|
		                     bno       len
		
		  Overlap middle of extent: trim the length of the existing
		  record to the length of the new left-extent size, increment
		  the insertion position so we can insert a new record
		  containing the remaining right-extent space.
		 
		        ltbno                ltlen
		  Orig:    |oooooooooooooooooooo|
		  Freeing:       |fffffffff|
		  Result:  |rrrrr|         |rrrr|
		                bno       len
 Shrink the left side of the rmap 
 Add an rmap at the new offset 
  Find an extent in the rmap btree and map it.  For rmap extent types that
  can overlap (data fork rmaps on reflink filesystems) we must be careful
  that the prevnext records in the btree might belong to another owner.
  Therefore we must use delete+insert to alter any of the key fields.
  For every other situation there can only be one owner for a given extent,
  so we can call the regular _alloc function.
 Is there a left record that abuts our range? 
 Is there a right record that abuts our range? 
		
		  Left edge contiguous, merge into left record.
		 
		        ltbno     ltlen
		  orig:   |ooooooooo|
		  adding:           |aaaaaaaaa|
		  result: |rrrrrrrrrrrrrrrrrrr|
		                   bno       len
			
			  Right edge also contiguous, delete right record
			  and merge into left record.
			 
			        ltbno     ltlen    gtbno     gtlen
			  orig:   |ooooooooo|         |ooooooooo|
			  adding:           |aaaaaaaaa|
			  result: |rrrrrrrrrrrrrrrrrrrrrrrrrrrrr|
 Point the cursor back to the left record and update. 
		
		  Right edge contiguous, merge into right record.
		 
		                  gtbno     gtlen
		  Orig:             |ooooooooo|
		  adding: |aaaaaaaaa|
		  Result: |rrrrrrrrrrrrrrrrrrr|
		         bno       len
 Delete the old record. 
 Move the start and re-add it. 
		
		  No contiguous edge with identical owner, insert
		  new record at current cursor position.
 Insert a raw rmap into the rmapbt. 
 Format btree record and pass to our callback. 
 Find all rmaps between two keys. 
 Find all rmaps. 
 Clean up after calling xfs_rmap_finish_one. 
  Process one of the deferred rmap operations.  We pass back the
  btree cursor to maintain our lock on the rmapbt between calls.
  This saves time and eliminates a buffer deadlock between the
  superblock and the AGF because we'll always grab them in the same
  order.
	
	  If we haven't gotten a cursor or the cursor AG doesn't match
	  the startblock, get one now.
		
		  Refresh the freelist before we start changing the
		  rmapbt, because a shape change could cause us to
		  allocate blocks.
  Don't defer an rmap if we aren't an rmap filesystem.
  Record a rmap intent; the list is kept sorted first by AG and then by
  increasing age.
 Map an extent into a file. 
 Unmap an extent out of a file. 
  Convert a data fork extent from unwritten to real or vice versa.
  Note that tp can be NULL here as no transaction is used for COW fork
  unwritten conversion.
 Schedule the creation of an rmap for non-file data. 
 Schedule the deletion of an rmap for non-file data. 
 Compare rmap records.  Returns -1 if a < b, 1 if a > b, and 0 if equal. 
 Is there a record covering a given extent? 
  Is there a record for this owner completely covering a given physical
  extent?  If so, has_rmap will be set to true.  If there is no record
  or the record only covers part of the range, we set has_rmap to false.
  This function doesn't perform range lookups or offset checks, so it is
  not suitable for checking data fork blocks.
 For each rmap given, figure out if it doesn't match the key we want. 
  Given an extent and some owner info, can we find records overlapping
  the extent whose owner info does not match the given owner?
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  All Rights Reserved.
  Miscellaneous helper functions
  Compute and fill in the value of the maximum depth of a bmap btree
  in this filesystem.  Done once, during mount.
 file system mount structure 
 data or attr fork 
 btree level 
 max blocks at this level 
 max leaf entries possible 
 max records in root block 
 min records in leaf block 
 min records in node block 
 root block size 
	
	  The maximum number of extents in a file, hence the maximum number of
	  leaf entries, is controlled by the size of the on-disk extent count,
	  either a signed 32-bit number for the data fork, or a signed 16-bit
	  number for the attr fork.
	 
	  Note that we can no longer assume that if we are in ATTR1 that the
	  fork offset of all the inodes will be
	  (xfs_default_attroffset(ip) >> 3) because we could have mounted with
	  ATTR2 and then mounted back with ATTR1, keeping the i_forkoff's fixed
	  but probably at various positions. Therefore, for both ATTR1 and
	  ATTR2 we have to assume the worst case scenario of a minimum size
	  available.
 error 
 successfailure 
 error 
 successfailure 
  Check if the inode needs to be converted to btree format.
  Check if the inode should be converted to extent format.
  Update the record referred to by cur to the value given by irec
  This either works (return 0) or gets an EFSCORRUPTED error.
  Compute the worst-case number of indirect blocks that will be used
  for ip's delayed extent of length "len".
 incore inode pointer 
 delayed extent length 
 btree level number 
 maximum record count at this level 
 mount structure 
 return value 
  Calculate the default attribute fork offset for newly created inodes.
  Helper routine to reset inode i_forkoff field when switching attribute fork
  from local to extent format - we reset it where possible to make space
  available for inline data fork extents.
 Chase down all the log items to see if the bp is there 
 pointer to block address 
		
		  Compare the block numbers to see if there are dups.
  Check that the extents for the inode ip are in the right order in all
  btree leaves. THis becomes prohibitively expensive for large extent count
  files, so don't bother with inodes that have more than 10,000 extents in
  them. The btree record ordering checks will still be done, so for such large
  bmapbt constructs that is going to catch most corruptions.
 btree cursor or null 
 incore inode pointer 
 data or attr fork 
 current btree block 
 block # of "block" 
 buffer for "block" 
 error return value 
 index into the extents list 
 btree level, for checking 
 pointer to block address 
 pointer to current extent 
 last extent in prev block 
 pointer to next extent 
 skip large extent count inodes 
	
	  Root level must use BMAP_BROOT_PTR_ADDR macro to get ptr out.
	
	  Go down the tree until leaf level is reached, following the first
	  pointer (leftmost) at each level.
 See if buf is in cur first 
		
		  Check this block for basic sanity (increasing keys and
		  no duplicate blocks).
	
	  Here with bp and block set to the leftmost leaf node in the tree.
	
	  Loop over all leaf nodes checking that all extents are in the right order.
		
		  Read-ahead the next leaf block, if any.
		
		  Check all the extents to make sure they are OK.
		  If we had a previous block, the last entry should
		  conform with the first entry in this one.
		
		  If we've reached the end, stop.
  Validate that the bmbt_irecs being returned from bmapi are valid
  given the caller's original parameters.  Specifically check the
  ranges of the returned irecs to ensure that they only extend beyond
  the given parameters if the XFS_BMAPI_ENTIRE flag was set.
 index to map values 
 DEBUG 
  Inode fork format manipulation functions
  Convert the inode format to extent format if it currently is in btree format,
  but the extent list is small enough that it fits into the extent format.
  Since the extents are already in-core, all we have to do is give up the space
  for the btree root and pitch the leaf block.
 error 
 transaction pointer 
 incore inode pointer 
 btree cursor 
 inode logging flags 
 data or attr fork 
 child btree block 
 child block number 
 child block's buffer 
 error return value 
 ptr to block address 
 check if we actually need the extent format first: 
  Convert an extents-format file into a btree-format file.
  The new file will have a root block (in the inode) and a single child block.
 error 
 transaction pointer 
 incore inode pointer 
 cursor returned to caller 
 converting a delayed alloc 
 inode logging flags 
 data or attr fork 
 allocated (child) bt block 
 buffer for ablock 
 allocation arguments 
 child record pointer 
 btree root block 
 bmap btree cursor 
 error return value 
 inode fork pointer 
 root block key pointer 
 mount structure 
 root block address pointer 
	
	  Make space in the inode incore. This needs to be undone if we fail
	  to expand the root.
	
	  Fill in the root.
	
	  Need a cursor.  Can't allocate until bb_level is filled in.
	
	  Convert to a btree with two levels, one record in root.
	
	  Allocation can't fail, the space was reserved.
	
	  Fill in the child block.
	
	  Fill in the root key and pointer.
	
	  Do all this logging at the end so that
	  the root is at the right level.
  Convert a local file to an extents file.
  This code is out of bounds for data forks of regular files,
  since the file data needs to get logged so things will stay consistent.
  (The bmap-level manipulations are ok, though).
 error 
 transaction pointer 
 incore inode pointer 
 total blocks needed by transaction 
 inode logging flags 
 logging flags returned 
 inode fork pointer 
 allocation arguments 
 buffer for extent block 
	
	  We don't want to deal with the case of keeping inode data inline yet.
	  So sending the data fork of a regular inode is invalid.
	
	  Allocate a block.  We know we need only one, since the
	  file currently fits in an inode.
 Can't fail, the space was reserved. 
	
	  Initialize the block, copy the data and log the remote buffer.
	 
	  The callout is responsible for logging because the remote format
	  might differ from the local format and thus we don't know how much to
	  log here. Note that init_fn must also set the buffer log item type
	  correctly.
 account for the change in fork size 
  Called from xfs_bmap_add_attrfork to handle btree format files.
 error 
 transaction pointer 
 incore inode pointer 
 inode logging flags 
 btree cursor 
 error return value 
 file system mount struct 
 newroot status 
 must be at least one entry 
  Called from xfs_bmap_add_attrfork to handle extents format files.
 error 
 transaction pointer 
 incore inode pointer 
 inode logging flags 
 bmap btree cursor 
 error return value 
  Called from xfs_bmap_add_attrfork to handle local format files. Each
  different data fork content type needs a different callout to do the
  conversion. Some are basic and only require special block initialisation
  callouts for the data formating, others (directories) are so specialised they
  handle everything themselves.
  XXX (dgc): investigate whether directory conversion can use the generic
  formatting callout. It should be possible - it's just a very complex
  formatter.
 error 
 transaction pointer 
 incore inode pointer 
 inode logging flags 
 args for dirattr code 
 should only be called for types that support local format data 
  Set an inode attr fork offset based on the format of the data fork.
  Convert inode from non-attributed to attributed.
  Must not be in a transaction, ip must not be locked.
 error code 
 incore inode pointer 
 space new attribute needs 
 xact may use reserved blks 
 mount structure 
 transaction pointer 
 space reservation 
 superblock attr version 
 logging flags 
 error return value 
  Internal and external extent tree search functions.
 Stuff every bmbt record from this block into the incore extent map. 
 Abort if we find more records than nextents. 
 Copy records into the incore cache. 
  Read in extents from a btree-format inode.
  Returns the relative block number of the first unused block(s) in the given
  fork with at least "len" logically contiguous blocks free.  This is the
  lowest-address hole if the fork has holes, else the first block past the end
  of fork.  Return 0 if the fork is currently local (in-inode).
 error 
 transaction pointer 
 incore inode 
 size of hole to find 
 unused block 
 data or attr fork 
		
		  See if the hole before this extent will work.
  Returns the file-relative block number of the last block - 1 before
  last_block (input value) in the file.
  This is not based on i_size, it is based on the extent records.
  Returns 0 for local files, as they do not have extent records.
 error 
 transaction pointer 
 incore inode 
 last block 
 data or attr fork 
  Check the last inode extent to determine whether this allocation will result
  in blocks being allocated at the end of the file. When we allocate new data
  blocks at the end of the file which do not start at the previous data block,
  we will try to align the new blocks at stripe unit boundaries.
  Returns 1 in bma->aeof if the file (fork) is empty as any new write will be
  at, or past the EOF.
	
	  Check if we are allocation or past the last extent, or at least into
	  the last delayed allocated extent.
  Returns the file-relative block number of the first block past eof in
  the file.  This is not based on i_size, it is based on the extent records.
  Returns 0 for local files, as they do not have extent records.
  Extent tree manipulation functions used during allocation.
  Convert a delayed allocation to a real allocation.
 error 
 error return value 
 temp state 
 end offset of new entry 
 neighbor extent entries 
 left is 0, right is 1, prev is 2 
 return value (logging flags) 
 new count del alloc blocks used 
 old count del alloc blocks used 
 value for da_new calculations 
 partial logging flags 
	
	  Set up a bunch of variables to make the tests simpler.
	
	  Set flags determining what part of the previous delayed allocation
	  extent is being replaced by a real allocation.
	
	  Check and set flags if this segment has a left neighbor.
	  Don't set contiguous if the combined extent would be too large.
	
	  Check and set flags if this segment has a right neighbor.
	  Don't set contiguous if the combined extent would be too large.
	  Also check for all-three-contiguous being too large.
	
	  Switch out based on the FILLING and CONTIG state bits.
		
		  Filling in all of a previously delayed allocation extent.
		  The left and right neighbors are both contiguous with new.
		
		  Filling in all of a previously delayed allocation extent.
		  The left neighbor is contiguous, the right is not.
		
		  Filling in all of a previously delayed allocation extent.
		  The right neighbor is contiguous, the left is not. Take care
		  with delay -> unwritten extent allocation here because the
		  delalloc record we are overwriting is always written.
		
		  Filling in all of a previously delayed allocation extent.
		  Neither the left nor right neighbors are contiguous with
		  the new one.
		
		  Filling in the first part of a previous delayed allocation.
		  The left neighbor is contiguous.
		
		  Filling in the first part of a previous delayed allocation.
		  The left neighbor is not contiguous.
		
		  Filling in the last part of a previous delayed allocation.
		  The right neighbor is contiguous with the new allocation.
		
		  Filling in the last part of a previous delayed allocation.
		  The right neighbor is not contiguous.
		
		  Filling in the middle part of a previous delayed allocation.
		  Contiguity is impossible here.
		  This case is avoided almost all the time.
		 
		  We start with a delayed allocation:
		 
		  +ddddddddddddddddddddddddddddddddddddddddddddddddddddddd+
		   PREV @ idx
		 
	          and we are allocating:
		                      +rrrrrrrrrrrrrrrrr+
		 			      new
		 
		  and we set it up for insertion as:
		  +ddddddddddddddddddd+rrrrrrrrrrrrrrrrr+ddddddddddddddddd+
		                             new
		   PREV @ idx          LEFT              RIGHT
		                       inserted at idx + 1
 LEFT is the new middle 
 RIGHT is the new right 
 truncate PREV 
		
		  These cases are all impossible.
 add reverse mapping unless caller opted out 
 convert to a btree if necessary 
 partial log flag return val 
 adjust for changes in reserved delayed indirect blocks 
  Convert an unwritten allocation to a real allocation or vice versa.
 error 
 incore inode pointer 
 if curp is null, not a btree 
 new data to add to file extents 
 inode logging flags 
 btree cursor 
 error return value 
 temp state 
 inode fork pointer 
 end offset of new entry 
 neighbor extent entries 
 left is 0, right is 1, prev is 2 
 return value (logging flags) 
	
	  Set up a bunch of variables to make the tests simpler.
	
	  Set flags determining what part of the previous oldext allocation
	  extent is being replaced by a newext allocation.
	
	  Check and set flags if this segment has a left neighbor.
	  Don't set contiguous if the combined extent would be too large.
	
	  Check and set flags if this segment has a right neighbor.
	  Don't set contiguous if the combined extent would be too large.
	  Also check for all-three-contiguous being too large.
	
	  Switch out based on the FILLING and CONTIG state bits.
		
		  Setting all of a previous oldext extent to newext.
		  The left and right neighbors are both contiguous with new.
		
		  Setting all of a previous oldext extent to newext.
		  The left neighbor is contiguous, the right is not.
		
		  Setting all of a previous oldext extent to newext.
		  The right neighbor is contiguous, the left is not.
		
		  Setting all of a previous oldext extent to newext.
		  Neither the left nor right neighbors are contiguous with
		  the new one.
		
		  Setting the first part of a previous oldext extent to newext.
		  The left neighbor is contiguous.
		
		  Setting the first part of a previous oldext extent to newext.
		  The left neighbor is not contiguous.
		
		  Setting the last part of a previous oldext extent to newext.
		  The right neighbor is contiguous with the new allocation.
		
		  Setting the last part of a previous oldext extent to newext.
		  The right neighbor is not contiguous.
		
		  Setting the middle part of a previous oldext extent to
		  newext.  Contiguity is impossible here.
		  One extent becomes three extents.
 new right extent - oldext 
 new left extent - oldext 
			
			  Reset the cursor to the position of the new extent
			  we are about to insert as we can't trust it after
			  the previous insert.
 new middle extent - newext 
		
		  These cases are all impossible.
 update reverse mappings 
 convert to a btree if necessary 
 partial log flag return val 
 clear out the allocated field, done with it now in any case. 
  Convert a hole to a delayed allocation.
 incore inode pointer 
 new data to add to file extents 
 inode fork pointer 
 left neighbor extent entry 
 new indirect size 
 old indirect size 
 right neighbor extent entry 
 temp for indirect calculations 
	
	  Check and set flags if this segment has a left neighbor
	
	  Check and set flags if the current (right) segment exists.
	  If it doesn't exist, we're converting the hole at end-of-file.
	
	  Set contiguity flags on the left and right neighbors.
	  Don't let extents get too large, even if the pieces are contiguous.
	
	  Switch out based on the contiguity flags.
		
		  New allocation is contiguous with delayed allocations
		  on the left and on the right.
		  Merge all three into a single extent record.
		
		  New allocation is contiguous with a delayed allocation
		  on the left.
		  Merge the new allocation with the left neighbor.
		
		  New allocation is contiguous with a delayed allocation
		  on the right.
		  Merge the new allocation with the right neighbor.
		
		  New allocation is not contiguous with another
		  delayed allocation.
		  Insert a new entry.
		
		  Nothing to do for disk quota accounting here.
  Convert a hole to a real allocation.
 error 
 error return value 
 temp state 
 left neighbor extent entry 
 right neighbor extent entry 
 return value (logging flags) 
	
	  Check and set flags if this segment has a left neighbor.
	
	  Check and set flags if this segment has a current value.
	  Not true if we're inserting into the "hole" at eof.
	
	  We're inserting a real allocation between "left" and "right".
	  Set the contiguity flags.  Don't let extents get too large.
	
	  Select which case we're in here, and implement it.
		
		  New allocation is contiguous with real allocations on the
		  left and on the right.
		  Merge all three into a single extent record.
		
		  New allocation is contiguous with a real allocation
		  on the left.
		  Merge the new allocation with the left neighbor.
		
		  New allocation is contiguous with a real allocation
		  on the right.
		  Merge the new allocation with the right neighbor.
		
		  New allocation is not contiguous with another
		  real allocation.
		  Insert a new entry.
 add reverse mapping unless caller opted out 
 convert to a btree if necessary 
 partial log flag return val 
 clear out the allocated field, done with it now in any case. 
  Functions used in the extent read, allocate and remove paths
  Adjust the size of the new extent based on i_extsize and rt extsize.
 next extent pointer 
 previous extent pointer 
 align to this extent size 
 is this a realtime inode? 
 is extent at end-of-file? 
 creating delalloc extent? 
 overwriting unwritten extent? 
 inout: aligned offset 
 inout: aligned length 
 original offset 
 original length 
 original off+len 
 next file offset 
 previous file offset 
 temp for offset 
 temp for length 
 temp for calculations 
	
	  If this request overlaps an existing extent, then don't
	  attempt to perform any additional alignment.
	
	  If the file offset is unaligned vs. the extent size
	  we need to align it.  This will be possible unless
	  the file was previously written with a kernel that didn't
	  perform this alignment, or if a truncate shot us in the
	  foot.
 Same adjustment for the end of the requested area. 
	
	  For large extent hint sizes, the aligned extent might be larger than
	  MAXEXTLEN. In that case, reduce the size by an extsz so that it pulls
	  the length back under MAXEXTLEN. The outer allocation loops handle
	  short allocation just fine, so it is safe to do this. We only want to
	  do it when we are forced to, though, because it means more allocation
	  operations are required.
	
	  If the previous block overlaps with this proposed allocation
	  then move the start forward without adjusting the length.
	
	  If the next block overlaps with this proposed allocation
	  then move the start back without adjusting the length,
	  but not before offset 0.
	  This may of course make the start overlap previous block,
	  and if we hit the offset 0 limit then the next block
	  can still overlap too.
	
	  If we're now overlapping the next or previous extent that
	  means we can't fit an extsz piece in this hole.  Just move
	  the start forward to the first valid spot and set
	  the length so we hit the end.
	
	  If realtime, and the result isn't a multiple of the realtime
	  extent size we need to remove blocks until it is.
		
		  We're not covering the original request, or
		  we won't be able to once we fix the length.
		
		  Try to fix it by moving the start up.
		
		  Try to fix it by moving the end in.
		
		  Set the start to the minimum then trim the length.
		
		  Result doesn't cover the request, fail it.
 see MAXEXTLEN handling above 
 bmap alloc argument struct 
 adjustment to block numbers 
 ag number of ap->firstblock 
 mount point structure 
 true if ap->firstblock isn't set 
 true if inode is realtime 
	
	  If allocating at eof, and there's a previous real block,
	  try to use its last block as our starting point.
		
		  Adjust for the gap between prevp and us.
	
	  If not at eof, then compare the two neighbor blocks.
	  Figure out whether either one gives us a good starting point,
	  and pick the better one.
 right side block number 
 right side difference 
 left side block number 
 left side difference 
		
		  If there's a previous (left) block, select a requested
		  start block based on it.
			
			  Calculate gap to end of previous block.
			
			  Figure the startblock based on the previous block's
			  end and the gap size.
			  Heuristic!
			  If the gap is large relative to the piece we're
			  allocating, or using it gives us an invalid block
			  number, then just use the end of the previous block.
			
			  If the firstblock forbids it, can't use it,
			  must use default.
		
		  No previous block or can't follow it, just default.
		
		  If there's a following (right) block, select a requested
		  start block based on it.
			
			  Calculate gap to start of next block.
			
			  Figure the startblock based on the next block's
			  start and the gap size.
			
			  Heuristic!
			  If the gap is large relative to the piece we're
			  allocating, or using it gives us an invalid block
			  number, then just use the start of the next block
			  offset by our length.
			
			  If the firstblock forbids it, can't use it,
			  must use default.
		
		  No next block, just default.
		
		  If both valid, pick the better one, else the only good
		  one, else ap->blkno is already set (to 0 or the inode block).
 Couldn't lock the AGF, so skip this AG. 
		
		  Since we did a BUF_TRYLOCK above, it is possible that
		  there is space for this request.
		
		  If the best seen length is less than the request length,
		  use the best as the minimum.
		
		  Otherwise we've seen an extent as big as maxlen, use that
		  as the minimum.
	
	  Set the failure fallback case to look in the selected AG as stream
	  may have moved.
 Update all inode and quota accounting for the allocation we just did. 
		
		  COW fork blocks are in-core only and thus are treated as
		  in-core quota reservation (like delalloc blocks) even when
		  converted to real blocks. The quota reservation is not
		  accounted to disk until blocks are remapped to the data
		  fork. So if these blocks were previously delalloc, we
		  already have quota reservation and there's nothing to do
		  yet.
		
		  Otherwise, we've allocated blocks in a hole. The transaction
		  has acquired in-core quota reservation for this extent.
		  Rather than account these as real blocks, however, we reduce
		  the transaction quota reservation based on the allocation.
		  This essentially transfers the transaction quota reservation
		  to that of a delalloc extent.
 dataattr fork only 
 minimum allocation alignment 
 stripe alignment for allocation is determined by mount parameters 
 apply extent size hints if obtained earlier 
	
	  check the allocation happened at the same or higher AG than
	  the first block that was allocated.
	
	  If the extent size hint is active, we tried to round the
	  caller's allocation request offset down to extsz and the
	  length up to another extsz boundary.  If we found a free
	  extent we mapped it in starting at this new offset.  If the
	  newly mapped space isn't long enough to cover any of the
	  range of offsets that was originally requested, move the
	  mapping up so that we can fill as much of the caller's
	  original request as possible.  Free space is apparently
	  very fragmented so we're unlikely to be able to satisfy the
	  hints anyway.
		
		  Unlike the longest extent available in an AG, we don't track
		  the length of an AG's shortest extent.
		  XFS_ERRTAG_BMAP_ALLOC_MINLEN_EXTENT is a debug only knob and
		  hence we can afford to start traversing from the 0th AG since
		  we need not be concerned about a drop in performance in
		  "debug only" code paths.
 ag number of ap->firstblock 
 true if ap->firstblock isn't set 
	
	  If allowed, use ap->blkno; otherwise must use firstblock since
	  it's in the right allocation group.
	
	  Normal allocation, done through xfs_alloc_vextent.
 Trim the allocation back to the maximum an AG can fit. 
		
		  Search for an allocation group with a single extent large
		  enough for the request.  If one isn't found, then adjust
		  the minimum allocation size to the largest space found.
	
	  If we are not low on available data blocks, and the underlying
	  logical volume manager is a stripe, and the file offset is zero then
	  try to allocate data blocks on stripe unit boundary. NOTE: ap->aeof
	  is only set if the allocation length is >= the stripe unit and the
	  allocation offset is at the end of file.
			
			  Adjust minlen to try and preserve alignment if we
			  can't guarantee an aligned maxlen extent.
			
			  First try an exact bno allocation.
			  If it fails then do a near or start bno
			  allocation with alignment turned on.
			
			  Compute the minlen+alignment for the
			  next case.  Set slop so that the value
			  of minlen+alignment+slop doesn't go up
			  between the calls.
		
		  Exact allocation failed. Now try with alignment
		  turned on.
		
		  allocation failed, so turn off alignment and
		  try again.
 Trim extent to fit a logical block range. 
  Trim the returned map to the required bounds
	
	  Return the minimum of what we got and what we asked for for
	  the length.  We can use the len variable here because it is
	  modified below and we could have been there before coming
	  here if the first part of the allocation didn't overlap what
	  was asked for.
  Update and validate the extent map to return
 update previous map with new information 
  Map file blocks to filesystem blocks without allocation.
 Reading past eof, act as though there's a hole up to end. 
 Reading in a hole.  
 set up the extent map to return. 
 If we're done, stop now. 
 Else go on to the next record. 
  Add a delayed allocation extent to an inode. Blocks are reserved from the
  global pool and the extent inserted into the inode in-core extent tree.
  On entry, got refers to the first extent beyond the offset of the extent to
  allocate or eof is specified if no such extent exists. On return, got refers
  to the extent record that was inserted to the inode fork.
  Note that the allocated extent may have been merged with contiguous extents
  during insertion into the inode fork. Thus, got does not reflect the current
  state of the inode fork on return. If necessary, the caller can use lastx to
  look up the updated record in the inode fork.
	
	  Cap the alloc length. Keep track of prealloc so we know whether to
	  tag the inode before we return.
 Figure out the extent size, adjust alen 
	
	  Make a transaction-less quota reservation for delayed allocation
	  blocks.  This number gets adjusted later.  We return if we haven't
	  allocated blocks already inside this loop.
	
	  Split changing sb for alen and indlen since they could be coming
	  from different places.
	
	  Tag the inode if blocks were preallocated. Note that COW fork
	  preallocation can occur at the start or end of the extent, even when
	  prealloc == 0, so we must also check the aligned offset and length.
	
	  Set the data type being allocated. For the data fork, the first data
	  in the file is treated differently to all other allocations. For the
	  attribute fork, we only need to ensure the allocated range is not on
	  the busy list.
	
	  For the wasdelay case, we could also just allocate the stuff asked
	  for in this bmap call but that wouldn't be as good.
	
	  Bump the number of extents we've allocated
	  in this call.
	
	  Update our extent pointer, given that xfs_bmap_add_extent_delay_real
	  or xfs_bmap_add_extent_hole_real might have merged it into one of
	  the neighbouring ones.
 check if we need to do unwritten->real conversion 
 check if we need to do real->unwritten conversion 
	
	  Modify (by adding) the state flag, if writing.
	
	  Before insertion into the bmbt, zero the range being converted
	  if required.
	
	  Log the inode core unconditionally in the unwritten extent conversion
	  path because the conversion might not have done so (e.g., if the
	  extent count hasn't changed). We need to make sure the inode is dirty
	  in the transaction for the sake of fsync(), even if nothing has
	  changed, because fsync() will not force the log for this transaction
	  unless it sees the inode pinned.
	 
	  Note: If we're only converting cow fork extents, there aren't
	  any on-disk updates to make, so we don't need to log anything.
	
	  Update our extent pointer, given that
	  xfs_bmap_add_extent_unwritten_real might have merged it into one
	  of the neighbouring ones.
	
	  We may have combined previously unwritten space with written space,
	  so generate another request.
  Log whatever the flags say, even if error.  Otherwise we might miss detecting
  a case where the data is changed, there's an error, and it's not logged so we
  don't shutdown when we should.  Don't bother logging extentsbtree changes if
  we converted to the other format.
  Map file blocks to filesystem blocks, and allocate blocks or convert the
  extent state if necessary.  Details behaviour is controlled by the flags
  parameter.  Only allocates blocks from a single allocation group, to avoid
  locking problems.
 transaction pointer 
 incore inode 
 starting file offs. mapped 
 length to map in file 
 XFS_BMAPI_... 
 total blocks needed 
 output: map values 
 io: mval sizecount 
 end of mapped file region 
 after the end of extents 
 error return 
 current extent index 
 old block number (offset) 
 original block number value 
 original flags arg value 
 original value of len arg 
 original value of mval 
 original value of nmap 
 zeroing is for currently only for data extents, not metadata 
	
	  we can allocate unwritten extents or pre-zero allocated blocks,
	  but it makes no sense to do both at once. This would result in
	  zeroing the unwritten extent twice, but it still being an
	  unwritten extent....
 in hole or beyond EOF? 
			
			  CoW fork conversions should never hit EOF or
			  holes.  There should always be something for us
			  to work on.
		
		  First, deal with the hole before the allocated space
		  that we found, if any.
			
			  There's a 3264 bit type mismatch between the
			  allocation length request (which can be 64 bits in
			  length) and the bma length request, which is
			  xfs_extlen_t and therefore 32 bits. Hence we have to
			  check for 32-bit overflows and handle them here.
			
			  If this is a CoW allocation, record the data in
			  the refcount btree for orphan recovery.
 Deal with the allocated space we found.  
 Execute unwritten extent conversion if necessary 
 update the extent map to return 
		
		  If we're done, stop now.  Stop when we've allocated
		  XFS_BMAP_MAX_NMAP extents no matter what.  Otherwise
		  the transaction may get too big.
 Else go on to the next record. 
  Convert an existing delalloc extent to real blocks based on file offset. This
  attempts to allocate the entire delalloc extent and may require multiple
  invocations to allocate the target offset if a large enough physical extent
  is not available.
	
	  Space for the extent and indirect blocks was reserved when the
	  delalloc extent was created so there's no need to do so here.
		
		  No extent found in the range we are trying to convert.  This
		  should only happen for the COW fork, where another thread
		  might have moved the extent to the data fork in the meantime.
	
	  If we find a real extent here we raced with another thread converting
	  the extent.  Just return the real extent at this offset.
	
	  When we're converting the delalloc reservations backing dirty pages
	  in the page cache, we must be careful about how we create the new
	  extents:
	 
	  New CoW fork extents are created unwritten, turned into real extents
	  when we're about to write the data to disk, and mapped into the data
	  fork after the write finishes.  End of story.
	 
	  New data fork extents must be mapped in as unwritten and converted
	  to real extents after the write succeeds to avoid exposing stale
	  disk contents if we crash.
 make sure we only reflink into a hole. 
  When a delalloc extent is split (e.g., due to a hole punch), the original
  indlen reservation must be shared across the two new extents that are left
  behind.
  Given the original reservation and the worst case indlen for the two new
  extents (as calculated by xfs_bmap_worst_indlen()), split the original
  reservation fairly across the two new extents. If necessary, steal available
  blocks from a deleted extent to make up a reservation deficiency (e.g., if
  ores == 1). The number of stolen blocks is returned. The availability and
  subsequent accounting of stolen blocks is the responsibility of the caller.
 original res. 
 ext1 worst indlen 
 ext2 worst indlen 
 stealable blocks 
 new total res. 
	
	  Steal as many blocks as we can to try and satisfy the worst case
	  indlen for both new extents.
 nothing else to do if we've satisfied the new reservation 
	
	  We can't meet the total required reservation for the two extents.
	  Calculate the percent of the overall shortage between both extents
	  and apply this percentage to each of the requested indlen values.
	  This distributes the shortage fairly and reduces the chances that one
	  of the two extents is left with nothing when extents are repeatedly
	  split.
	
	  Hand out the remainder to each extent. If one of the two reservations
	  is zero, we want to make sure that one gets a block first. The loop
	  below starts with len1, so hand len2 a block right off the bat if it
	  is zero.
	
	  Update the inode delalloc counter now and wait to update the
	  sb counters as we might have to borrow some blocks for the
	  indirect block accounting.
		
		  Matches the whole extent.  Delete the entry.
		
		  Deleting the first part of the extent.
		
		  Deleting the last part of the extent.
		
		  Deleting the middle of the extent.
		 
		  Distribute the original indlen reservation across the two new
		  extents.  Steal blocks from the deleted extent if necessary.
		  Stealing blocks simply fudges the fdblocks accounting below.
		  Warn if either of the new indlen reservations is zero as this
		  can lead to delalloc problems.
		
		  Matches the whole extent.  Delete the entry.
		
		  Deleting the first part of the extent.
		
		  Deleting the last part of the extent.
		
		  Deleting the middle of the extent.
  Called by xfs_bmapi to update file extent records and the btree
  after removing space.
 error 
 incore inode pointer 
 current transaction pointer 
 if null, not a btree 
 data to remove from extents 
 inode logging flags 
 data or attr fork 
 bmapi flags 
 first block past del 
 first offset past del 
 free extent at end of routine 
 error return value 
 inode logging flags 
 current extent entry 
 first offset past got 
 temp state 
 inode fork pointer 
 mount structure 
 quotasb block count 
 new record to be inserted 
 REFERENCED 
 quota field to update 
	
	  If it's the case where the directory code is running with no block
	  reservation, and the deleted block is in the middle of its extent,
	  and the resulting insert of an extent would cause transformation to
	  btree format, then reject it.  The calling code will then swap blocks
	  around instead.  We have to do this now, rather than waiting for the
	  conversion to btree format, since the transaction will be dirty then.
		
		  Matches the whole extent.  Delete the entry.
		
		  Deleting the first part of the extent.
		
		  Deleting the last part of the extent.
		
		  Deleting the middle of the extent.
		
		  For directories, -ENOSPC is returned since a directory entry
		  remove operation must not fail due to low extent count
		  availability. -ENOSPC will be handled by higher layers of XFS
		  by letting the corresponding empty DataFree blocks to linger
		  until a future remove operation. Dabtree blocks would be
		  swapped with the last block in the leaf space and then the
		  new last block will be unmapped.
		 
		  The above logic also applies to the source directory entry of
		  a rename operation.
			
			  If get no-space back from btree insert, it tried a
			  split, and we have a zero block reservation.  Fix up
			  our state and return the error.
				
				  Reset the cursor, don't trust it after any
				  insert operation.
				
				  Update the btree record back
				  to the original value.
				
				  Reset the extent record back
				  to the original value.
 remove reverse mapping 
	
	  If we need to, add to list of extents to delete.
	
	  Adjust inode # blocks in the file.
	
	  Adjust quota data.
  Unmap (remove) blocks from a file.
  If nexts is nonzero then the number of extents to remove is limited to
  that value.  If not all extents in the block range can be removed then
  done is set.
 error 
 transaction pointer 
 incore inode 
 first file offset deleted 
 io: amount remaining 
 misc flags 
 number of extents max 
 bmap btree cursor 
 extent being deleted 
 error return value 
 extent number in list 
 current extent record 
 inode fork pointer 
 freeing in rt area 
 transaction logging flags 
 rt extent offset 
 partial logging flags 
 was a delayed alloc extent 
 data or attribute fork 
 length to unmap in file 
	
	  Guesstimate how many blocks we can unmap without running the risk of
	  blowing out the transaction with a mix of EFIs and reflink
	  adjustments.
		
		  Synchronize by locking the bitmap inode.
		
		  Is the found extent after a hole in which end lives?
		  Just back up to the previous extent, if so.
		
		  Is the last block of this extent before the range
		  we're supposed to delete?  If so, we're done.
		
		  Then deal with the (possibly delayed) allocated space
		  we found.
 How much can we safely unmap? 
			
			  Realtime extent not lined up at the end.
			  The extent could have been split into written
			  and unwritten pieces, or we could just be
			  unmapping part of it.  But we can't really
			  get rid of part of a realtime extent.
				
				  This piece is unwritten, or we're not
				  using unwritten extents.  Skip over it.
			
			  It's written, turn it unwritten.
			  This is better than zeroing it.
			
			  If this spans a realtime extent boundary,
			  chop it back to the start of the one we end at.
			
			  Realtime extent is lined up at the end but not
			  at the front.  We'll get rid of full extents if
			  we can.
				
				  Can't make it unwritten.  There isn't
				  a full extent here so just skip it.
				
				  This one is already unwritten.
				  It must have a written left neighbor.
				  Unwrite the killed part of that one and
				  try again.
		
		  If not done go on to the next (previous) record.
	
	  Convert to a btree if necessary.
	
	  Log everything.  Do this after conversion, there's no point in
	  logging the extent records if we've converted to btree format.
	
	  Log inode even in the error case, if the transaction
	  is dirty we'll need to shut down the filesystem.
 Unmap a range of a file. 
  Determine whether an extent shift can be accomplished by a merge with the
  extent that precedes the target hole of the shift.
 preceding extent 
 current extent to shift 
 shift fsb 
	
	  The extent, once shifted, must be adjacent in-file and on-disk with
	  the preceding extent.
  A bmap extent shift adjusts the file offset of an extent to fill a preceding
  hole in the file. If an extent shift would result in the extent being fully
  adjacent to the extent that currently precedes the hole, we can merge with
  the preceding extent rather than do the shift.
  This function assumes the caller has verified a shift-by-merge is possible
  with the provided extents via xfs_bmse_can_merge().
 shift fsb 
 extent to shift 
 preceding extent 
 output 
	
	  Update the on-disk extent count, the btree if necessary and log the
	  inode.
 lookup and remove the extent to merge 
 lookup and update size of the previous extent 
 change to extent format if required after extent removal 
 update reverse mapping. rmap functions merge the rmaps for us 
 update reverse mapping 
 Make sure we won't be right-shifting an extent past the maximum bound. 
		
		  Unlike a left shift (which involves a hole punch), a right
		  shift does not modify extent neighbors in any way.  We should
		  never find mergeable extents in this scenario.  Check anyways
		  and warn if we encounter two extents that could be one.
  Splits an extent into two extents at split_fsb block such that it is the
  first block of the current_ext. @ext is a target extent to be split.
  @split_fsb is a block where the extents is split.  If split_fsb lies in a
  hole or the first block of extents, just return 0.
 split extent 
 new block count for got 
 Read in all the extents 
	
	  If there are not extents, or split_fsb lies in a hole we are done.
 Add new extent 
	
	  Convert to a btree if necessary.
 partial log flag return val 
 Deferred mapping is only for real extents in the data fork. 
 Record a bmap intent. 
 Map an extent into a file. 
 Unmap an extent out of a file. 
  Process one of the deferred bmap operations.  We pass back the
  btree cursor to maintain our lock on the bmapbt between calls.
 Check that an inode's extent does not have invalid flags or bad ranges. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  Copyright (c) 2013 Red Hat, Inc.
  All Rights Reserved.
  Function declarations.
  Convert data space db to the corresponding free db.
  Convert data space db to the corresponding index in a free db.
  Check internal consistency of a leafn block.
 XXX: should bounds check the xfs_dir3_icfree_hdr here 
 Everything ok in the free block header? 
 Check things that we can't do in the verifier. 
 try read returns without an error or bpp if it lands in a hole 
	
	  Initialize the new block to be empty, and remember
	  its first slot as our empty slot.
  Log entries from a freespace block.
 first entry to log 
 last entry to log 
  Log header from a freespace block.
 freespace structure 
  Convert a leaf-format directory to a node-format directory.
  We need to change the magic number of the leaf block, and copy
  the freespace table out of the leaf block into its own block.
 error 
 operation arguments 
 leaf buffer 
 incore directory inode 
 error return value 
 freespace buffer 
 freespace block number 
 pointer to freespace entry 
 leaf freespace index 
 leaf structure 
 leaf tail structure 
 count of live freespc ents 
 freespace entry value 
 transaction pointer 
	
	  Add a freespace block to the directory.
	
	  Get the buffer for the new freespace block.
	
	  Copy freespace entries from the leaf block to the new block.
	  Count active entries.
	
	  Now initialize the freespace block header.
	
	  Converting the leaf to a leafnode is just a matter of changing the
	  magic number and the ops. Do the change directly to the buffer as
	  it's less work (and less code) than decoding the header to host
	  format and back again.
  Add a leaf entry to a leaf block in a node-form directory.
  The other work necessary is done from the caller.
 error 
 leaf buffer 
 operation arguments 
 insertion pt for new entry 
 compacting stale leaves 
 next stale entry 
 high leaf entry logging 
 low leaf entry logging 
 previous stale entry 
	
	  Quick check just to make sure we are not going to index
	  into other peoples memory
	
	  If there are already the maximum number of leaf entries in
	  the block, if there are no stale entries it won't fit.
	  Caller will do a split.  If there are stale entries we'll do
	  a compact.
	
	  Compact out all but one stale leaf entry.  Leaves behind
	  the entry closest to index.
		
		  Set impossible logging indices for this case.
	
	  Insert the new entry, log everything.
 DEBUG 
  Return the last hash value in the leaf.
  Stale entries are ok.
 hash value 
 leaf buffer 
 count of entries in leaf 
  Look up a leaf entry for space to add a name in a node-format leaf block.
  The extrablk in state is a freespace block.
 leaf buffer 
 operation arguments 
 out: leaf entry index 
 state to fill in 
 current datafree buffer 
 current data block number 
 current free block number 
 incore directory inode 
 error return value 
 free entry index 
 free block structure 
 leaf entry index 
 leaf structure 
 length of new data entry 
 leaf entry 
 filesystem mount point 
 new data block number 
 new free block number 
 transaction pointer 
	
	  Look up the hash value in the leaf entries.
	
	  Do we have a buffer coming in?
 If so, it's a free block buffer, get the block number. 
	
	  Loop over leaf entries with the right hash value.
		
		  Skip stale leaf entries.
		
		  Pull the data block number from the entry.
		
		  For addname, we're looking for a place to put the new entry.
		  We want to use a data block with an entry of equal
		  hash value to ours if there is one with room.
		 
		  If this block isn't the data block we already have
		  in hand, take a look at it.
			
			  Convert the data block to the free block
			  holding its freespace information.
			
			  If it's not the one we have in hand, read it in.
				
				  If we had one before, drop it.
			
			  Get the index for our entry.
			
			  If it has room, return it.
 Didn't find any space 
 Giving back a free block. 
		
		  Important: this magic number is not in the buffer - it's for
		  buffer type information and therefore only the freedata type
		  matters here, not whether CRCs are enabled or not.
	
	  Return the index, that will be the insertion point.
  Look up a leaf entry in a node-format leaf block.
  The extrablk in state a data block.
 leaf buffer 
 operation arguments 
 out: leaf entry index 
 state to fill in 
 current datafree buffer 
 current data block number 
 data block entry 
 incore directory inode 
 error return value 
 leaf entry index 
 leaf structure 
 leaf entry 
 filesystem mount point 
 new data block number 
 transaction pointer 
 comparison result 
	
	  Look up the hash value in the leaf entries.
	
	  Do we have a buffer coming in?
	
	  Loop over leaf entries with the right hash value.
		
		  Skip stale leaf entries.
		
		  Pull the data block number from the entry.
		
		  Not adding a new entry, so we really want to find
		  the name given to us.
		 
		  If it's a different data block, go get it.
			
			  If we had a block before that we aren't saving
			  for a CI name, drop it
			
			  If needing the block that is saved with a CI match,
			  use it otherwise read in the new data block.
		
		  Point to the data entry.
		
		  Compare the entry and if it's an exact match, return
		  EEXIST immediately. If it's the first case-insensitive
		  match, store the block & inode number and continue looking.
 If there is a CI match block, drop it 
 Giving back last used data block. 
 If the curbp is not the CI match block, drop it 
  Look up a leaf entry in a node-format leaf block.
  If this is an addname then the extrablk in state is a freespace block,
  otherwise it's a data block.
 leaf buffer 
 operation arguments 
 out: leaf entry index 
 state to fill in 
  Move count leaf entries from source to destination leaf.
  Log entries and headers.  Stale entries are preserved.
 operation arguments 
 source 
 source leaf index 
 destination 
 destination leaf index 
 count of leaves to copy 
 count stale leaves copied 
	
	  Silently return if nothing to do.
	
	  If the destination index is not the end of the current
	  destination leaf entries, open up a hole in the destination
	  to hold the new entries.
	
	  If the source has stale leaves, count the ones in the copy range
	  so we can update the header correctly.
 temp leaf index 
	
	  Copy the leaf entries from source to destination.
	
	  If there are source entries after the ones we copied,
	  delete the ones we copied by sliding the next ones down.
	
	  Update the headers and log them.
  Determine the sort order of two leaf blocks.
  Returns 1 if both are valid and leaf2 should be before leaf1, else 0.
 sort order 
 leaf1 buffer 
 leaf2 buffer 
  Rebalance leaf entries between two leaf blocks.
  This is actually only called when the second block is new,
  though the code deals with the general case.
  A new entry will be inserted in one of the blocks, and that
  entry is taken into account when balancing.
 btree cursor 
 first btree block 
 second btree block 
 operation arguments 
 count (& direction) leaves 
 new goes in left leaf 
 first leaf structure 
 second leaf structure 
 midpoint leaf index 
 old count of stale leaves 
 old total leaf count 
 swapped leaf blocks 
	
	  If the block order is wrong, swap the arguments.
	
	  If the old leaf count was odd then the new one will be even,
	  so we need to divide the new count evenly.
 middle entry hash value 
	
	  If the old count is even then the new count is odd, so there's
	  no preferred side for the new entry.
	  Pick the left one.
	
	  Calculate moved entry count.  Positive means left-to-right,
	  negative means right-to-left.  Then move the entries.
 log the changes made when moving the entries 
	
	  Mark whether we're inserting into the old or new leaf.
	
	  Adjust the expected index for insertion.
	
	  Finally sanity check just to make sure we are not returning a
	  negative index
		
		  Data block is not empty, just set the free entry to the new
		  value.
 One less used entry in the free table. 
	
	  If this was the last entry in the table, we can trim the table size
	  back.  There might be other entries at the end referring to
	  non-existent data blocks, get those too.
 free entry index 
 Not the last entry, just punch it out.  
	
	  If there are no useful entries left in the block, get rid of the
	  block if we can.
		
		  It's possible to get ENOSPC if there is no
		  space reservation.  In this case some one
		  else will eventually get rid of this block.
 Log the free entry that changed, unless we got rid of it.  
  Remove an entry from a node directory.
  This removes the leaf entry and the data entry,
  and updates the free block if necessary.
 error 
 operation arguments 
 leaf buffer 
 leaf entry index 
 data block 
 resulting block needs join 
 data block header 
 data block number 
 data block buffer 
 data block entry 
 incore directory inode 
 leaf structure 
 leaf entry 
 longest data free entry 
 data block entry offset 
 need to log data header 
 need to rescan data frees 
 transaction pointer 
 bestfree table 
	
	  Point to the entry we're removing.
	
	  Extract the data block and offset from the entry.
	
	  Kill the leaf entry by marking it stale.
	  Log the leaf block changes.
	
	  Make the data entry free.  Keep track of the longest freespace
	  in the data block in case it changes.
	
	  Rescan the data block freespaces for bestfree.
	  Log the data block header if needed.
	
	  If the longest data block freespace changes, need to update
	  the corresponding freeblock entry.
 error return value 
 freeblock buffer 
 freeblock block number 
 index in freeblock entries 
 freeblock structure 
		
		  Convert the data block number to a free block,
		  read in the free block.
		
		  Calculate which entry we need to fix.
		
		  If the data block is now empty we can get rid of it
		  (usually).
			
			  Try to punch out the data block.
			
			  We can get ENOSPC if there's no space reservation.
			  In this case just drop the buffer and some one else
			  will eventually get rid of the empty block.
		
		  If we got rid of the data block, we can eliminate that entry
		  in the free block.
	
	  Return indication of whether this leaf block is empty enough
	  to justify trying to join it with a neighbor.
  Split the leaf entries in the old block into old and new blocks.
 error 
 btree cursor 
 original block 
 newly created block 
 operation arguments 
 new leaf block number 
 error return value 
	
	  Allocate space for a new leaf node.
	
	  Initialize the new leaf block.
	
	  Rebalance the entries across the two leaves, link the new
	  block into the leaves.
	
	  Insert the new entry in the correct block.
	
	  Update last hashval in each block since we added the name.
  Check a leaf block and its neighbors to see if the block should be
  collapsed into one or the other neighbor.  Always keep the block
  with the smaller block number.
  If the current block is over 50% full, don't try to join it, return 0.
  If the block is empty, fill in the state structure and return 2.
  If it can be collapsed, fill in the state structure and return 1.
  If nothing can be done, return 0.
 error 
 btree cursor 
 resulting action to take 
 leaf block 
 leaf block number 
 leaf buffer 
 bytes in use 
 leaf live entry count 
 error return value 
 sibling block direction 
 sibling counter 
 leaf structure 
 result from path_shift 
	
	  Check for the degenerate case of the block being over 50% full.
	  If so, it's not worth even looking to see if we might be able
	  to coalesce with a sibling.
		
		  Blk over 50%, don't try to join.
	
	  Check for the degenerate case of the block being empty.
	  If the block is empty, we'll simply delete it, no need to
	  coalesce it with a sibling block.  We choose (arbitrarily)
	  to merge with the forward block unless it is NULL.
		
		  Make altpath point to the block we want to keep and
		  path point to the block we want to drop (this one).
	
	  Examine each sibling block to see if we can coalesce with
	  at least 25% free space to spare.  We need to figure out
	  whether to merge with the forward or the backward block.
	  We prefer coalescing with the lower numbered sibling so as
	  to shrink a directory over time.
		
		  Read the sibling leaf block.
		
		  Count bytes in the two blocks combined.
		
		  Fits with at least 25% to spare.
	
	  Didn't like either block, give up.
	
	  Make altpath point to the block we want to keep (the lower
	  numbered block) and path point to the block we want to drop.
  Move all the leaf entries from drop_blk to save_blk.
  This is done as part of a join operation.
 cursor 
 dead block 
 surviving block 
 operation arguments 
 dead leaf structure 
 surviving leaf structure 
	
	  If there are any stale leaf entries, take this opportunity
	  to purge them.
	
	  Move the entries from drop to the appropriate end of save.
 log the changes made when moving the entries 
  Add a new data block to the directory at the free space index that the caller
  has specified.
 Not allowed to allocate, return failure. 
 Allocate and initialize the new data block.  
	
	  Get the freespace block corresponding to the data block
	  that was just allocated.
	
	  If there wasn't a freespace block, the read will
	  return a NULL fbp.  Allocate and initialize a new one.
 Get a buffer for the new block. 
 Remember the first slot as our empty slot. 
 Set the freespace block index from the data block number. 
 Extend the freespace table if the new data block is off the end. 
	
	  If this entry was for an empty data block (this should always be
	  true) then update the header.
 Update the freespace value for the new block in the table. 
	
	  If we came in with a freespace block that means that lookup
	  found an entry with our hash value.  This is the freespace
	  block for that data entry.
 caller already found the freespace for us. 
		
		  The data block looked at didn't have enough room.
		  We'll start at the beginning of the freespace entries.
	
	  If we don't have a data block yet, we're going to scan the freespace
	  data for a data block with enough free space in it.
 If it's ifbno we already looked at it. 
		
		  Read the block.  There can be holes in the freespace blocks,
		  so this might not succeed.  This should be really rare, so
		  there's no reason to avoid it.
 Scan the free entry array for a large enough free space. 
 Didn't find free space, go on to next free block 
  Add the data entry for a node-format directory name addition.
  The leaf entry is added in xfs_dir2_leafn_add.
  We may enter with a freespace block that the lookup found.
 operation arguments 
 optional freespace block 
 data unused entry pointer 
 data entry pointer 
 data block header 
 data block buffer 
 freespace buffer 
 data block number 
 error return value 
 freespace entry index 
 length of the new entry 
 need to log free entry 
 need to log data header 
 need to rescan data frees 
 data entry tag pointer 
	
	  Now we know if we must allocate blocks, so if we are checking whether
	  we can insert without allocation then we can return now.
	
	  If we don't have a data block, we need to allocate one and make
	  the freespace entries refer to it.
 we're going to have to log the free block index later 
 Read the data block in. 
 setup for data block up now 
 Point to the existing unused space. 
 Mark the first part of the unused space, inuse for us. 
 Fill in the new entry and log it. 
 Rescan the freespace and log the data block if needed. 
 If the freespace block entry is now wrong, update it. 
 Log the freespace entry if needed. 
 Return the data block and offset in args. 
  Top-level node form directory addname routine.
 error 
 operation arguments 
 leaf block for insert 
 error return value 
 sub-return value 
 btree cursor 
	
	  Allocate and initialize the state (btree cursor).
	
	  Look up the name.  We're not supposed to find it, but
	  this gives us the insertion point.
	
	  Add the data entry to a data block.
	  Extravalid is set to a freeblock found by lookup.
	
	  Add the new leaf entry.
		
		  It worked, fix the hash values up the btree.
		
		  It didn't work, we need to split the leaf block.
		
		  Split the leaf block and insert the new entry.
  Lookup an entry in a node-format directory.
  All the real work happens in xfs_da3_node_lookup_int.
  The only real output is the inode number of the entry.
 error 
 operation arguments 
 error return value 
 btree level 
 operation return value 
 btree cursor 
	
	  Allocate and initialize the btree cursor.
	
	  Fill in the path to the entry in the cursor.
 If a CI match, dup the actual name and return -EEXIST 
	
	  Release the btree blocks and leaf block.
	
	  Release the data block if we have it.
  Remove an entry from a node-format directory.
 error 
 operation arguments 
 leaf block 
 error return value 
 operation return value 
 btree cursor 
	
	  Allocate and initialize the btree cursor.
 Look up the entry we're deleting, set up the cursor. 
 Didn't find it, upper layer screwed up. 
	
	  Remove the leaf and data entries.
	  Extrablk refers to the data block.
	
	  Fix the hash values up the btree.
	
	  If we need to join leaf blocks, do it.
	
	  If no errors so far, try conversion to leaf format.
  Replace an entry's inode number in a node-format directory.
 error 
 operation arguments 
 leaf block 
 data block header 
 data entry changed 
 error return value 
 btree level 
 new inode number 
 new file type 
 internal return value 
 btree cursor 
	
	  Allocate and initialize the btree cursor.
	
	  We have to save new inode number and ftype since
	  xfs_da3_node_lookup_int() is going to overwrite them
	
	  Lookup the entry to change in the btree.
	
	  It should be found, since the vnodeops layer has looked it up
	  and locked it.  But paranoia is good.
		
		  Find the leaf entry.
		
		  Point to the data entry.
		
		  Fill in the new inode number and log the entry.
	
	  Didn't find it, and we're holding a data block.  Drop it.
	
	  Release all the buffers in the cursor.
  Trim off a trailing empty freespace block.
  Return (in rvalp) 1 if we did it, 0 if not.
 error 
 operation arguments 
 free block number 
 out: did something 
 freespace buffer 
 incore directory inode 
 error return code 
 freespace structure 
 transaction pointer 
	
	  Read the freespace block.
	
	  There can be holes in freespace.  If fo is a hole, there's
	  nothing to do.
	
	  If there are used entries, there's nothing to do.
	
	  Blow the block away.
		
		  Can't fail with ENOSPC since that only happens with no
		  space reservation, when breaking up an extent into two
		  pieces.  This is the last block of an extent.
	
	  Return that we succeeded.
 SPDX-License-Identifier: GPL-2.0-or-later
  Copyright (C) 2020 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Staging Cursors and Fake Roots for Btrees
  =========================================
  A staging btree cursor is a special type of btree cursor that callers must
  use to construct a new btree index using the btree bulk loader code.  The
  bulk loading code uses the staging btree cursor to abstract the details of
  initializing new btree blocks and filling them with records or keyptr
  pairs.  Regular btree operations (e.g. queries and modifications) are not
  supported with staging cursors, and callers must not invoke them.
  Fake root structures contain all the information about a btree that is under
  construction by the bulk loading code.  Staging btree cursors point to fake
  root structures instead of the usual AG header or inode structure.
  Callers are expected to initialize a fake root structure and pass it into
  the _stage_cursor function for a specific btree type.  When bulk loading is
  complete, callers should call the _commit_staged_btree function for that
  specific btree type to commit the new btree into the filesystem.
  Don't allow staging cursors to be duplicated because they're supposed to be
  kept private to a single thread.
  Don't allow block allocation for a staging cursor, because staging cursors
  do not support regular btree modifications.
  Bulk loading uses a separate callback to obtain new blocks from a
  preallocated list, which prevents ENOSPC failures during loading.
  Don't allow block freeing for a staging cursor, because staging cursors
  do not support regular btree modifications.
 Initialize a pointer to the root block from the fakeroot. 
  Bulk Loading for AG Btrees
  ==========================
  For a btree rooted in an AG header, pass a xbtree_afakeroot structure to the
  staging cursor.  Callers should initialize this to zero.
  The _stage_cursor() function for a specific btree type should call
  xfs_btree_stage_afakeroot to set up the in-memory cursor as a staging
  cursor.  The corresponding _commit_staged_btree() function should log the
  new root and call xfs_btree_commit_afakeroot() to transform the staging
  cursor into a regular btree cursor.
 Update the btree root information for a per-AG fake root. 
  Initialize a AG-rooted btree cursor with the given AG btree fake root.
  The btree cursor's bc_ops will be overridden as needed to make the staging
  functionality work.
  Transform an AG-rooted staging btree cursor back into a regular cursor by
  substituting a real btree root for the fake one and restoring normal btree
  cursor ops.  The caller must log the btree root change prior to calling
  this.
  Bulk Loading for Inode-Rooted Btrees
  ====================================
  For a btree rooted in an inode fork, pass a xbtree_ifakeroot structure to
  the staging cursor.  This structure should be initialized as follows:
  - if_fork_size field should be set to the number of bytes available to the
    fork in the inode.
  - if_fork should point to a freshly allocated struct xfs_ifork.
  - if_format should be set to the appropriate fork type (e.g.
    XFS_DINODE_FMT_BTREE).
  All other fields must be zero.
  The _stage_cursor() function for a specific btree type should call
  xfs_btree_stage_ifakeroot to set up the in-memory cursor as a staging
  cursor.  The corresponding _commit_staged_btree() function should log the
  new root and call xfs_btree_commit_ifakeroot() to transform the staging
  cursor into a regular btree cursor.
  Initialize an inode-rooted btree cursor with the given inode btree fake
  root.  The btree cursor's bc_ops will be overridden as needed to make the
  staging functionality work.  If new_ops is not NULL, these new ops will be
  passed out to the caller for further overriding.
  Transform an inode-rooted staging btree cursor back into a regular cursor by
  substituting a real btree root for the fake one and restoring normal btree
  cursor ops.  The caller must log the btree root change prior to calling
  this.
  Bulk Loading of Staged Btrees
  =============================
  This interface is used with a staged btree cursor to create a totally new
  btree with a large number of records (i.e. more than what would fit in a
  single root block).  When the creation is complete, the new root can be
  linked atomically into the filesystem by committing the staged cursor.
  Creation of a new btree proceeds roughly as follows:
  The first step is to initialize an appropriate fake btree root structure and
  then construct a staged btree cursor.  Refer to the block comments about
  "Bulk Loading for AG Btrees" and "Bulk Loading for Inode-Rooted Btrees" for
  more information about how to do this.
  The second step is to initialize a struct xfs_btree_bload context as
  documented in the structure definition.
  The third step is to call xfs_btree_bload_compute_geometry to compute the
  height of and the number of blocks needed to construct the btree.  See the
  section "Computing the Geometry of the New Btree" for details about this
  computation.
  In step four, the caller must allocate xfs_btree_bload.nr_blocks blocks and
  save them for later use by ->claim_block().  Bulk loading requires all
  blocks to be allocated beforehand to avoid ENOSPC failures midway through a
  rebuild, and to minimize seek distances of the new btree.
  Step five is to call xfs_btree_bload() to start constructing the btree.
  The final step is to commit the staging btree cursor, which logs the new
  btree root and turns the staging cursor into a regular cursor.  The caller
  is responsible for cleaning up the previous btree blocks, if any.
  Computing the Geometry of the New Btree
  =======================================
  The number of items placed in each btree block is computed via the following
  algorithm: For leaf levels, the number of items for the level is nr_records
  in the bload structure.  For node levels, the number of items for the level
  is the number of blocks in the next lower level of the tree.  For each
  level, the desired number of items per block is defined as:
  desired = max(minrecs, maxrecs - slack factor)
  The number of blocks for the level is defined to be:
  blocks = floor(nr_items  desired)
  Note this is rounded down so that the npb calculation below will never fall
  below minrecs.  The number of items that will actually be loaded into each
  btree block is defined as:
  npb =  nr_items  blocks
  Some of the leftmost blocks in the level will contain one extra record as
  needed to handle uneven division.  If the number of records in any block
  would exceed maxrecs for that level, blocks is incremented and npb is
  recalculated.
  In other words, we compute the number of blocks needed to satisfy a given
  loading level, then spread the items as evenly as possible.
  The height and number of fs blocks required to create the btree are computed
  and returned via btree_height and nr_blocks.
  Put a btree block that we're loading onto the ordered list and release it.
  The btree blocks will be written to disk when bulk loading is finished.
  Allocate and initialize one btree block for bulk loading.
  The new btree block will have its level and numrecs fields set to the values
  of the level and nr_this_block parameters, respectively.
  The caller should ensure that ptrp, bpp, and blockp refer to the left
  sibling of the new block, if there is any.  On exit, ptrp, bpp, and blockp
  will all point to the new block.
 inout 
 inout 
 inout 
 Allocate a new incore btree root block. 
 Initialize it and send it out. 
 Claim one of the caller's preallocated blocks. 
	
	  The previous block (if any) is the left sibling of the new block,
	  so set its right sibling pointer to the new block and drop it.
 Initialize the new btree block. 
 Set the out parameters. 
 Load one leaf block. 
 Fill the leaf block with records. 
  Load one node block with keyptr pairs.
  child_ptr must point to a block within the next level down in the tree.  A
  keyptr entry will be created in the new node block to the block pointed to
  by child_ptr.  On exit, child_ptr points to the next block on the child
  level that needs processing.
 Fill the node block with keys and pointers. 
  Compute the maximum number of records (or keyptrs) per block that we want to
  install at this level in the btree.  Caller is responsible for having set
  @cur->bc_ino.forksize to the desired fork size, if appropriate.
  Compute the desired number of records (or keyptrs) per block that we want to
  install at this level in the btree, which must be somewhere between minrecs
  and max_npb.  The caller is free to install fewer records per block.
 Root blocks are not subject to minrecs rules. 
  Compute the number of records to be stored in each block at this level and
  the number of blocks for this level.  For leaf levels, we must populate an
  empty root block even if there are no records, so we have to have at least
  one block.
	
	  Compute the number of blocks we need to fill each block with the
	  desired number of recordskeyptrs per block.  Because desired_npb
	  could be minrecs, we use regular integer division (which rounds
	  the block count down) so that in the next step the effective # of
	  items per block will never be less than desired_npb.
	
	  Compute the number of records that we will actually put in each
	  block, assuming that we want to spread the records evenly between
	  the blocks.  Take care that the effective # of items per block (npb)
	  won't exceed maxrecs even for the blocks that get an extra record,
	  since desired_npb could be maxrecs, and in the previous step we
	  rounded the block count down.
  Ensure a slack value is appropriate for the btree.
  If the slack value is negative, set slack so that we fill the block to
  halfway between minrecs and maxrecs.  Make sure the slack is never so large
  that we can underflow minrecs.
	
	  If slack is negative, automatically set slack so that we load the
	  btree block approximately halfway between minrecs and maxrecs.
	  Generally, this will net us 75% loading.
  Prepare a btree cursor for a bulk load operation by computing the geometry
  fields in bbl.  Caller must ensure that the btree cursor is a staging
  cursor.  This function can be called multiple times.
	
	  Make sure that the slack values make sense for traditional leaf and
	  node blocks.  Inode-rooted btrees will return different minrecs and
	  maxrecs values for the root block (bc_nlevels == level - 1).  We're
	  checking levels 0 and 1 here, so set bc_nlevels such that the btree
	  code doesn't interpret either as the root level.
			
			  If all the items we want to store at this level
			  would fit in the inode root block, then we have our
			  btree root and are done.
			 
			  Note that bmap btrees forbid records in the root.
			
			  Otherwise, we have to store all the items for this
			  level in traditional btree blocks and therefore need
			  another level of btree to point to those blocks.
			 
			  We have to re-compute the geometry for each level of
			  an inode-rooted btree because the geometry differs
			  between a btree root in an inode fork and a
			  traditional btree block.
			 
			  This distinction is made in the btree code based on
			  whether level == bc_nlevels - 1.  Based on the
			  previous root block size check against the root
			  block geometry, we know that we aren't yet ready to
			  populate the root.  Increment bc_nevels and
			  recalculate the geometry for a traditional
			  block-based btree level.
			
			  If all the items we want to store at this level
			  would fit in a single root block, we're done.
 Otherwise, we need another level of btree. 
 Bulk load a btree given the parameters and geometry established in bbl. 
 Load each leaf block. 
		
		  Due to rounding, btree blocks will not be evenly populated
		  in most cases.  blocks_with_extra tells us how many blocks
		  will receive an extra record to distribute the excess across
		  the current level as evenly as possible.
		
		  Record the leftmost leaf pointer so we know where to start
		  with the first node level.
 Populate the internal btree nodes. 
 Load each node block. 
			
			  Record the leftmost node pointer so that we know
			  where to start the next node level above this one.
 Initialize the new root. 
	
	  Write the new blocks to disk.  If the ordered list isn't empty after
	  that, then something went wrong and we have to fail.  This should
	  never happen, but we'll check anyway.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2003,2005 Silicon Graphics, Inc.
  Copyright (c) 2013 Red Hat, Inc.
  All Rights Reserved.
  Local function declarations.
  Check the internal consistency of a leaf1 block.
  Pop an assert if something is wrong.
	
	  XXX (dgc): This value is not restrictive enough.
	  Should factor in the size of the bests table as well.
	  We can deduce a value for that from i_disk_size.
 Leaves and bests don't overlap in leaf format. 
 Check hash value order, count stale entries.  
  We verify the magic numbers before decoding the leaf header so that on debug
  kernels we don't get assertion failures in xfs_dir3_leaf_hdr_from_disk() due
  to incorrect magic numbers.
  Initialize a new leaf block, leaf1 or leafn magic accepted.
	
	  If it's a leaf-format directory initialize the tail.
	  Caller is responsible for initialising the bests table.
  Convert a block form directory to a leaf form directory.
 error 
 operation arguments 
 input block's buffer 
 leaf's bestsp entries 
 leaf block's bno 
 block header 
 block's leaf entries 
 block's tail 
 incore directory inode 
 error return code 
 leaf block's buffer 
 leaf block's bno 
 leaf structure 
 leaf's tail 
 need to log block header 
 need to rescan bestfree 
 transaction pointer 
	
	  Add the leaf block to the inode.
	  This interface will only put blocks in the leafnode range.
	  Since that's empty now, we'll get the root (block 0 in range).
	
	  Initialize the leaf block, get a buffer for it.
	
	  Set the counts in the leaf header.
	
	  Could compact these but I think we always do the conversion
	  after squeezing out stale entries.
	
	  Make the space formerly occupied by the leaf entries and block
	  tail be free.
	
	  Fix up the block header, make it a data block.
	
	  Set up leaf tail and bests table.
	
	  Log the data header and leaf bests table.
	
	  Find the first stale entry before our index, if any.
	
	  Find the first stale entry at or after our index, if any.
	  Stop if the result would require moving more entries than using
	  lowstale.
 leaf table position 
 need to compact leaves 
 index of prev stale leaf 
 index of next stale leaf 
 low leaf logging index 
 high leaf logging index 
 leaf entry table pointer 
		
		  Now we need to make room to insert the leaf entry.
		 
		  If there are no stale entries, just insert a hole at index.
		
		  Record low and high logging indices for the leaf.
	
	  There are stale entries.
	 
	  We will use one of them for the new entry.  It's probably not at
	  the right location, so we'll have to shift some up or down first.
	 
	  If we didn't compact before, we need to find the nearest stale
	  entries before and after our insertion point.
	
	  If the low one is better, use it.
		
		  Copy entries up to cover the stale entry and make room
		  for the new entry.
	
	  The high one is better, so use that one.
	
	  Copy entries down to cover the stale entry and make room for the
	  new entry.
  Add an entry to a leaf form directory.
 error 
 operation arguments 
 freespace table in leaf 
 end of data entry 
 data block buffer 
 leaf's buffer 
 leaf structure 
 incore directory inode 
 data block header 
 data block entry 
 leaf entry table pointer 
 data unused entry 
 leaf tail pointer 
 bestfree table 
 need to compact leaves 
 error return value 
 allocated new data block 
 index of next stale leaf 
 temporary, index 
 leaf table position 
 length of new entry 
 low leaf logging index 
 high leaf logging index 
 index of prev stale leaf 
 leaf block bytes needed 
 need to log data header 
 need to rescan data free 
 data block number 
	
	  Look up the entry by hash value and name.
	  We know it's not there, our caller has already done a lookup.
	  So the index is of the entry to insert in front of.
	  But if there are dup hash values the index is of the first of those.
	
	  See if there are any entries with the same hash value
	  and space in their block for the new entry.
	  This is good because it puts multiple same-hash value entries
	  in a data block, improving the lookup of those entries.
	
	  Didn't find a block yet, linear search all the data blocks.
			
			  Remember a block we see that's missing.
	
	  How many bytes do we need in the leaf block?
	
	  Now kill use_block if it refers to a missing block, so we
	  can use it as an indication of allocation needed.
	
	  If we don't have enough free bytes but we can make enough
	  by compacting out stale entries, we'll do that.
	
	  Otherwise if we don't have enough free bytes we need to
	  convert to node form.
		
		  Just checking or no space reservation, give up.
		
		  Convert to node form.
		
		  Then add the new entry.
	
	  Otherwise it will fit without compaction.
	
	  If just checking, then it will fit unless we needed to allocate
	  a new data block.
	
	  If no allocations are allowed, return now before we've
	  changed anything.
	
	  Need to compact the leaf entries, removing stale ones.
	  Leave one stale entry behind - the one closest to our
	  insertion index - and we'll shift that one to our insertion
	  point later.
	
	  There are stale entries, so we'll need log-low and log-high
	  impossibly bad values later.
	
	  If there was no data block space found, we need to allocate
	  a new one.
		
		  Add the new data block.
		
		  Initialize the block.
		
		  If we're adding a new data block on the end we need to
		  extend the bests table.  Copy it up one entry.
		
		  If we're filling in a previously empty block just log it.
		
		  Already had space in some data block.
		  Just read that one in.
	
	  Point to the biggest freespace in our data block.
	
	  Mark the initial part of our freespace in use for the new entry.
	
	  Initialize our new entry (at last).
	
	  Need to scan fix up the bestfree table.
	
	  Need to log the data block's header.
	
	  If the bests table needs to be changed, do it.
	  Log the change unless we've already done that.
	
	  Fill in the new leaf entry.
	
	  Log the leaf fields and give up the buffers.
  Compact out any stale entries in the leaf.
  Log the header and changed leaf entries, if any.
 operation arguments 
 leaf buffer 
 source leaf index 
 leaf structure 
 first leaf entry to log 
 target leaf index 
	
	  Compress out the stale entries in place.
		
		  Only actually copy the entries that are different.
	
	  Update and log the header, log the leaf entries.
  Compact the leaf entries, removing stale ones.
  Leave one stale entry behind - the one closest to our
  insertion index - and the caller will shift that one to our insertion
  point later.
  Return new insertion index, where the remaining stale entry is,
  and leaf logging indices.
 insertion index 
 out: stale entry before us 
 out: stale entry after us 
 out: low log index 
 out: high log index 
 source copy index 
 stale entry atafter index 
 insertion index 
 source index of kept stale 
 stale entry before index 
 new insertion index 
 destination copy index 
	
	  Pick the better of lowstale and highstale.
	
	  Copy the entries in place, removing all the stale entries
	  except keepstale.
		
		  Notice the new value of index.
		
		  Record the new keepstale value for the insertion.
		
		  Copy only the entries that have moved.
	
	  If the insertion point was past the last entry,
	  set the new insertion point accordingly.
	
	  Adjust the leaf header values.
	
	  Remember the lowhigh stale value only in the "right"
	  direction.
  Log the bests entries indicated from a leaf1 block.
 leaf buffer 
 first entry to log 
 last entry to log 
 pointer to first entry 
 pointer to last entry 
 leaf tail structure 
  Log the leaf entries indicated from a leaf1 or leafn block.
 pointer to first entry 
 pointer to last entry 
  Log the header of the leaf1 or leafn block.
  Log the tail of the leaf1 block.
 leaf tail structure 
  Look up the entry referred to by args in the leaf format directory.
  Most of the work is done by the xfs_dir2_leaf_lookup_int routine which
  is also used by the node-format code.
 operation arguments 
 data block buffer 
 data block entry 
 incore directory inode 
 error return code 
 found entry index 
 leaf buffer 
 leaf entry 
 transaction pointer 
	
	  Look up name in the leaf block, returning both buffers and index.
	
	  Get to the leaf entry and contained data entry address.
	
	  Point to the data entry.
	
	  Return the found inode number & CI name if appropriate
  Look up namehash in the leaf block.
  Fill in indexp with the found index, and dbpp with the data buffer.
  If not found dbpp will be NULL, and ENOENT comes back.
  lbpp will always be filled in with the leaf buffer unless there's an error.
 error 
 operation arguments 
 out: leaf buffer 
 out: index in leaf block 
 out: data buffer 
 current data block number 
 data buffer 
 data entry 
 incore directory inode 
 error return code 
 index in leaf block 
 leaf buffer 
 leaf entry 
 leaf structure 
 filesystem mount point 
 new data block number 
 transaction pointer 
 case match data block no. 
 name compare result 
	
	  Look for the first leaf entry with our hash value.
	
	  Loop over all the entries with the right hash value
	  looking to match the name.
		
		  Skip over stale leaf entries.
		
		  Get the new data block number.
		
		  If it's not the same as the old data block number,
		  need to pitch the old one and read the new one.
		
		  Point to the data entry.
		
		  Compare name and if it's an exact match, return the index
		  and buffer. If it's the first case-insensitive match, store
		  the index and buffer and continue looking for an exact match.
 case exact match: return the current buffer. 
	
	  Here, we can only be doing a lookup (not a rename or remove).
	  If a case-insensitive match was found earlier, re-read the
	  appropriate data block if required and return it.
	
	  No match found, return -ENOENT.
  Remove an entry from a leaf format directory.
 error 
 operation arguments 
 leaf block best freespace 
 data block header 
 data block number 
 data block buffer 
 data entry structure 
 incore directory inode 
 error return code 
 temporary data block # 
 index into leaf entries 
 leaf buffer 
 leaf structure 
 leaf entry 
 leaf tail structure 
 need to log data header 
 need to rescan data frees 
 old value of best free 
 bestfree table 
	
	  Lookup the leaf entry, get the leaf and data blocks read in.
	
	  Point to the leaf entry, use that to point to the data entry.
	
	  Mark the former data entry unused.
	
	  We just mark the leaf entry stale by putting a null in it.
	
	  Scan the freespace in the data block again if necessary,
	  log the data block header if necessary.
	
	  If the longest freespace in the data block has changed,
	  put the new value in the bests table and log that.
	
	  If the data block is now empty then get rid of the data block.
			
			  Nope, can't get rid of it because it caused
			  allocation of a bmap btree block to do so.
			  Just go on, returning success, leaving the
			  empty block in place.
		
		  If this is the last data block then compact the
		  bests table by getting rid of entries.
			
			  Look for the last active entry (i).
			
			  Copy the table down so inactive entries at the
			  end are removed.
	
	  If the data block was not the first one, drop it.
	
	  See if we can convert to block form.
  Replace the inode number in a leaf format directory entry.
 error 
 operation arguments 
 data block buffer 
 data block entry 
 incore directory inode 
 error return code 
 index of leaf entry 
 leaf buffer 
 leaf entry 
 transaction pointer 
	
	  Look up the entry.
	
	  Point to the leaf entry, get data address from it.
	
	  Point to the data entry.
	
	  Put the new inode number in, log it.
  Return index in the leaf block (lbp) which is either the first
  one with this hash value, or if there are none, the insert point
  for that hash value.
 index value 
 operation arguments 
 leaf buffer 
 hash from this entry 
 hash value looking for 
 high leaf index 
 low leaf index 
 leaf entry 
 current leaf index 
	
	  Note, the table cannot be empty, so we have to go through the loop.
	  Binary search the leaf entries looking for our hash value.
	
	  Found one, back up through all the equal hash values.
	
	  Need to point to an entry higher than ours.
  Trim off a trailing data block.  We know it's empty since the leaf
  freespace table says so.
 error 
 operation arguments 
 leaf buffer 
 data block number 
 leaf bests table 
 data block buffer 
 incore directory inode 
 error return value 
 leaf structure 
 leaf tail structure 
 transaction pointer 
	
	  Read the offending data block.  We need its buffer.
	
	  Get rid of the data block.
	
	  Eliminate the last bests entry from the table.
  Convert node form directory to leaf form directory.
  The root of the node form dir needs to already be a LEAFN block.
  Just return if we can't do anything.
 error 
 directory operation state 
 operation arguments 
 incore directory inode 
 error return code 
 buffer for freespace block 
 freespace file offset 
 buffer for leaf block 
 tail of leaf structure 
 leaf structure 
 filesystem mount point 
 successful free trim? 
 transaction pointer 
	
	  There's more than a leaf level in the btree, so there must
	  be multiple leafn blocks.  Give up.
	
	  Get the last offset in the file.
	
	  If there are freespace blocks other than the first one,
	  take this opportunity to remove trailing empty freespace blocks
	  that may have been left behind during no-space-reservation
	  operations.
	
	  Now find the block just before the freespace block.
	
	  If it's not the single leaf block, give up.
	
	  Read the freespace block.
	
	  Now see if the leafn and free data will fit in a leaf1.
	  If not, release the buffer and give up.
	
	  If the leaf has any stale entries in it, compress them out.
	
	  Set up the leaf tail from the freespace block.
	
	  Set up the leaf bests table.
	
	  Get rid of the freespace block.
		
		  This can't fail here because it can only happen when
		  punching out the middle of an extent, and this is an
		  isolated block.
	
	  Now see if we can convert the single-leaf directory
	  down to a block form directory.
	  This routine always kills the dabuf for the leaf, so
	  eliminate it from the path.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  All Rights Reserved.
  If we are doing readahead on an inode buffer, we might be in log recovery
  reading an inode allocation buffer that hasn't yet been replayed, and hence
  has not had the inode cores stamped into it. Hence for readahead, the buffer
  may be potentially invalid.
  If the readahead buffer is invalid, we need to mark it with an error and
  clear the DONE status of the buffer so that a followup read will re-read it
  from disk. We don't report the error otherwise to avoid warnings during log
  recovery and we don't get unnecessary panics on debug kernels. We use EIO here
  because all we want to do is say readahead failed; there is no-one to report
  the error to, so this will distinguish it from a non-ra verifier failure.
  Changes to this readahead error behaviour also need to be reflected in
  xfs_dquot_buf_readahead_verify().
	
	  Validate the magic number and version of every inode in the buffer
  This routine is called to map an inode to the buffer containing the on-disk
  version of the inode.  It returns a pointer to the buffer containing the
  on-disk inode in the bpp parameter.
 Convert an ondisk timestamp to an incore timestamp. 
	
	  First get the permanent information that is needed to allocate an
	  inode. If the inode is unused, mode is zero and we shouldn't mess
	  with the uninitialized part of it.
	
	  Convert v1 inodes immediately to v2 inode format as this is the
	  minimum inode version format we support in the rest of the code.
	  They will also be unconditionally written back to disk as v2 inodes.
	
	  Time is signed, so need to convert to signed 32 bit before
	  storing in inode timestamp which may be 64 bit. Otherwise
	  a time before epoch is converted to a time long after epoch
	  on 64 bit systems.
 Convert an incore timestamp to an ondisk timestamp. 
		
		  no local regular files yet
 fall through ... 
 fall through ... 
 Verify v3 integrity information first 
 don't allow invalid i_size 
 No zero-length symlinksdirs. 
 Fork checks carried over from xfs_iformat_fork 
 check for illegal values of forkoff 
 Do we have appropriate data fork formats for the mode? 
 Uninitialized inode ok. 
		
		  If there is no fork offset, this may be a freshly-made inode
		  in a new disk cluster, in which case di_aformat is zeroed.
		  Otherwise, such an inode must be in EXTENTS format; this goes
		  for freed inodes as well.
 extent size hint validation 
 only version 3 or greater inodes are extensively verified here 
 don't allow reflinkcowextsize if we don't have reflink 
 only regular files get reflink 
 don't let reflink and realtime mix 
 COW extent size hint validation 
 bigtime iflag can only happen on bigtime filesystems 
  Validate di_extsize hint.
  1. Extent size hint is only valid for directories and regular files.
  2. FS_XFLAG_EXTSIZE is only valid for regular files.
  3. FS_XFLAG_EXTSZINHERIT is only valid for directories.
  4. Hint cannot be larger than MAXTEXTLEN.
  5. Can be changed on directories at any time.
  6. Hint value of 0 turns off hints, clears inode flags.
  7. Extent size must be a multiple of the appropriate block size.
     For realtime files, this is the rt extent size.
  8. For non-realtime files, the extent size hint must be limited
     to half the AG size to avoid alignment extending the extent beyond the
     limits of the AG.
	
	  This comment describes a historic gap in this verifier function.
	 
	  For a directory with both RTINHERIT and EXTSZINHERIT flags set, this
	  function has never checked that the extent size hint is an integer
	  multiple of the realtime extent size.  Since we allow users to set
	  this combination  on non-rt filesystems and to change the rt
	  extent size when adding a rt device to a filesystem, the net effect
	  is that users can configure a filesystem anticipating one rt
	  geometry and change their minds later.  Directories do not use the
	  extent size hint, so this is harmless for them.
	 
	  If a directory with a misaligned extent size hint is allowed to
	  propagate that hint into a new regular realtime file, the result
	  is that the inode cluster buffer verifier will trigger a corruption
	  shutdown the next time it is run, because the verifier has always
	  enforced the alignment rule for regular files.
	 
	  Because we allow administrators to set a new rt extent size when
	  adding a rt section, we cannot add a check to this verifier because
	  that will result a new source of directory corruption errors when
	  reading an existing filesystem.  Instead, we rely on callers to
	  decide when alignment checks are appropriate, and fix things up as
	  needed.
 free inodes get flags set to zero but extsize remains 
  Validate di_cowextsize hint.
  1. CoW extent size hint can only be set if reflink is enabled on the fs.
     The inode does not have to have any shared blocks, but it must be a v3.
  2. FS_XFLAG_COWEXTSIZE is only valid for directories and regular files;
     for a directory, the hint is propagated to new files.
  3. Can be changed on files & directories at any time.
  4. Hint value of 0 turns off hints, clears inode flags.
  5. Extent size must be a multiple of the appropriate block size.
  6. The extent size hint must be limited to half the AG size to avoid
     alignment extending the extent beyond the limits of the AG.
 free inodes get flags set to zero but cowextsize remains 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2001,2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Convert inode mode to directory entry filetype
  ASCII case-insensitive (ie. A-Z) support for directories that was
  used in IRIX.
 set up directory geometry 
	
	  Now we've set up the block conversion variables, we can calculate the
	  segment block constants using the geometry structure.
 set up attribute geometry - single fsb only 
  Return 1 if directory contains only "." and "..".
 might happen during shutdown. 
  Validate a given inode number.
  Initialize a directory with its "." and ".." entries.
  Enter a name in a directory, or check for available space.
  If inum is 0, only the available space test is performed.
 new entry inode number 
 bmap's total block count 
 type-checking value 
  If doing a CI lookup and case-insensitive match, dup actual name into
  args.value. Return EEXIST for success (ie. name found) or an error.
  Lookup a name in a directory, give back the inode number.
  If ci_name is not NULL, returns the actual name in ci_name if it differs
  to name, or ci_name->name is set to NULL for an exact match.
 out: inode number 
 out: actual name if CI match 
 type-checking value 
	
	  We need to use KM_NOFS here so that lockdep will not throw false
	  positive deadlock warnings on a non-transactional lookup path. It is
	  safe to recurse into inode recalim in that case, but lockdep can't
	  easily be taught about it. Hence KM_NOFS avoids having to add more
	  lockdep Doing this avoids having to add a bunch of lockdep class
	  annotations into the reclaim path for the ilock.
  Remove an entry from a directory.
 bmap's total block count 
 type-checking value 
  Replace the inode number of a directory entry.
 name of entry to replace 
 new inode number 
 bmap's total block count 
 type-checking value 
  See if this entry can be added to the directory without allocating space.
 name of entry to add 
  Utility routines.
  Add a block to the directory.
  This routine is for data and free blocks, not leafnode blocks which are
  handled by xfs_da_grow_inode.
 v2 dir's space XFS_DIR2_xxx_SPACE 
 out: block number added 
 directory offset of new block 
 count of filesystem blocks 
	
	  Set lowest possible block in the space requested.
	
	  Update file's size if this is the data space and it grew.
 directory file (data) size 
  See if the directory is a single-block form directory.
 out: 1 is block, 0 is not block 
 last file offset 
  See if the directory is a single-leaf form directory.
 out: 1 is block, 0 is not block 
 last file offset 
  Remove the given block from the directory.
  This routine is used for data and free blocks, leafnode are done
  by xfs_da_shrink_inode.
 directory file offset 
 directory file offset 
 bunmap is finished 
 Unmap the fsblock(s). 
		
		  ENOSPC actually can happen if we're in a removename with no
		  space reservation, and the resulting block removal would
		  cause a bmap btree split or conversion from extents to btree.
		  This can only happen for un-fragmented directory blocks,
		  since you need to be punching out the middle of an extent.
		  In this case we need to leave the block in the file, and not
		  binval it.  So the block has to be in a consistent empty
		  state and appropriately logged.  We don't free up the buffer,
		  the caller can tell it hasn't happened since it got an error
		  back.
	
	  Invalidate the buffer from the transaction.
	
	  If it's not a data block, we're done.
	
	  If the block isn't the last one in the directory, we're done.
		
		  This can't really happen unless there's kernel corruption.
	
	  Set the size to the new last block.
 Returns true if the directory entry name is valid. 
	
	  MAXNAMELEN includes the trailing null, but (namelength) leave it
	  out, so use >= for the length check.
 There shouldn't be any slashes or nulls here 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Size of the AGFL.  For CRC-enabled filesystes we steal a couple of slots in
  the beginning of the block for a proper header with the location information
  and CRC.
  In order to avoid ENOSPC-related deadlock caused by out-of-order locking of
  AGF buffer (PV 947395), we place constraints on the relationship among
  actual allocations for data blocks, freelist blocks, and potential file data
  bmap btree blocks. However, these restrictions may result in no actual space
  allocated for a delayed extent, for example, a data block in a certain AG is
  allocated but there is no additional block for the additional bmap btree
  block due to a split of the bmap btree of the file. The result of this may
  lead to an infinite loop when the file gets flushed to disk and all delayed
  extents need to be actually allocated. To get around this, we explicitly set
  aside a few blocks which will not be reserved in delayed allocation.
  We need to reserve 4 fsbs _per AG_ for the freelist and 4 more to handle a
  potential split of the file's bmap btree.
  When deciding how much space to allocate out of an AG, we limit the
  allocation maximum size to the size the AG. However, we cannot use all the
  blocks in the AG - some are permanently used by metadata. These
  blocks are generally:
 	- the AG superblock, AGF, AGI and AGFL
 	- the AGF (bno and cnt) and AGI btree root blocks, and optionally
 	  the AGI free inode and rmap btree root blocks.
 	- blocks on the AGFL according to xfs_alloc_set_aside() limits
 	- the rmapbt root block
  The AG headers are sector sized, so the amount of space they take up is
  dependent on filesystem geometry. The others are all single blocks.
 ag headers 
 AGF, AGI btree root blocks 
 finobt root block 
 rmap root block 
 refcount root block 
  Lookup the record equal to [bno, len] in the btree given by cur.
 error 
 btree cursor 
 starting block of extent 
 length of extent 
 successfailure 
  Lookup the first record greater than or equal to [bno, len]
  in the btree given by cur.
 error 
 btree cursor 
 starting block of extent 
 length of extent 
 successfailure 
  Lookup the first record less than or equal to [bno, len]
  in the btree given by cur.
 error 
 btree cursor 
 starting block of extent 
 length of extent 
 successfailure 
  Update the record referred to by cur to the value given
  by [bno, len].
  This either works (return 0) or gets an EFSCORRUPTED error.
 error 
 btree cursor 
 starting block of extent 
 length of extent 
  Get the data from the pointed-to record.
 error 
 btree cursor 
 output: starting block of extent 
 output: length of extent 
 output: successfailure 
 check for valid extent range, including overflow 
  Compute aligned version of the found extent.
  Takes alignment and min length into account.
 allocation argument structure 
 starting block in found extent 
 length in found extent 
 result block number 
 result length 
 Trim busy sections out of found extent 
	
	  If we have a largish extent that happens to start before min_agbno,
	  see if we can shift it into range...
  Compute best start block and diff for "near" allocations.
  freelen >= wantlen already checked by caller.
 difference value (absolute) 
 target starting block 
 target length 
 target alignment 
 are we allocating data? 
 freespace's starting block 
 freespace's length 
 result: best start block from free 
 end of freespace extent 
 return block number 
 other new block number 
 length with newbno1 
 length with newbno2 
 end of target extent 
	
	  We want to allocate from the start of a free extent if it is past
	  the desired block or if we are allocating user data and the free
	  extent is before desired block. The second case is there to allow
	  for contiguous allocation from the remaining free space if the file
	  grows in the short term.
  Fix up the length, based on mod and prod.
  len should be k  prod + mod for some k.
  If len is too small it is returned unchanged.
  If len hits maxlen it is left alone.
 allocation argument structure 
 casts to (int) catch length underflows 
  Update the two btrees, logically removing from freespace the extent
  starting at rbno, rlen blocks.  The extent is contained within the
  actual (current) free extent fbno for flen blocks.
  Flags are passed in indicating whether the cursors are set to the
  relevant records.
 error code 
 cursor for by-size btree 
 cursor for by-block btree 
 starting block of free extent 
 length of free extent 
 starting block of returned extent 
 length of returned extent 
 flags, XFSA_FIXUP_... 
 error code 
 operation results 
 first new free startblock 
 second new free startblock 
 first new free length 
 second new free length 
	
	  Look up the record in the by-size tree if necessary.
	
	  Look up the record in the by-block tree if necessary.
	
	  Deal with all four cases: the allocated record is contained
	  within the freespace record, so we can have new freespace
	  at either (or both) end, or no freespace remaining.
	
	  Delete the entry from the by-size btree.
	
	  Add new by-size btree entry(s).
	
	  Fix up the by-block btree entry(s).
		
		  No remaining freespace, just delete the by-block tree entry.
		
		  Update the by-block entry to start later|be shorter.
		
		  2 resulting free entries, need to add one.
	
	  There is no verification of non-crc AGFLs because mkfs does not
	  initialise the AGFL to zero or NULL. Hence the only valid part of the
	  AGFL is what the AGF says is active. We can't get to the AGF, so we
	  can't verify just those entries are valid.
	
	  during growfs operations, the perag is not fully initialised,
	  so we can't use it for any useful checking. growfs ensures we can't
	  use it by using uncached buffers that don't have the perag attached
	  so we can detect and avoid this problem.
	
	  There is no verification of non-crc AGFLs because mkfs does not
	  initialise the AGFL to zero or NULL. Hence the only valid part of the
	  AGFL is what the AGF says is active. We can't get to the AGF, so we
	  can't verify just those entries are valid.
 no verification of non-crc AGFLs 
  Read in the allocation group free block array.
 error 
 mount point structure 
 transaction pointer 
 allocation group number 
 buffer for the ag free block array 
 return value 
  Block allocation algorithm and data structures.
 btree cursors 
 current search length 
 extent startblock 
 extent length 
 alloc bno 
 alloc len 
 diff from search bno 
 busy state 
  Set up cursors, etc. in the extent allocation cursor. This function can be
  called multiple times to reset an initialized structure without having to
  reallocate cursors.
	
	  Perform an initial cntbt lookup to check for availability of maxlen
	  extents. If this fails, we'll return -ENOSPC to signal the caller to
	  attempt a small allocation.
	
	  Allocate the bnobt left and right search cursors.
  Check an extent for allocation and track the best available candidate in the
  allocation structure. The cursor is deactivated if it has entered an out of
  range state based on allocation arguments. Optionally return the extent
  extent geometry and allocation status if requested by the caller.
	
	  Check minlen and deactivate a cntbt cursor if out of acceptable size
	  range (i.e., walking backwards looking for a minlen extent).
 deactivate a bnobt cursor outside of locality range 
	
	  We have an aligned record that satisfies minlen and beats or matches
	  the candidate extent size. Compare locality for near allocation mode.
	
	  Deactivate a bnobt cursor with worse locality than the current best.
	
	  We're done if we found a perfect allocation. This only deactivates
	  the current cursor, but this is just an optimization to terminate a
	  cntbt search that otherwise runs to the edge of the tree.
  Complete an allocation of a candidate extent. Remove the extent from both
  trees and update the args structure.
  Locality allocation lookup algorithm. This expects a cntbt cursor and uses
  bno optimized lookup to search for extents with ideal size and locality.
 locality optimized lookup 
 check the current record and update search length from it 
	
	  We looked up the first record >= [agbno, len] above. The agbno is a
	  secondary key and so the current record may lie just before or after
	  agbno. If it is past agbno, check the previous record too so long as
	  the length matches as it may be closer. Don't check a smaller record
	  because that could deactivate our cursor.
	
	  Increment the search key until we find at least one allocation
	  candidate or if the extent we found was larger. Otherwise, double the
	  search key to optimize the search. Efficiency is more important here
	  than absolute best locality.
  Deal with the case where only small freespaces remain. Either return the
  contents of the last freespace record, or allocate space from the freelist if
  there is nothing in the tree.
 error 
 allocation argument structure 
 optional by-size cursor 
 result block number 
 result length 
 status: 0-freelist, 1-normalnone 
	
	  If a cntbt cursor is provided, try to allocate the largest record in
	  the tree. Try the AGFL if the cntbt is empty, otherwise fail the
	  allocation. Make sure to respect minleft even when pulling from the
	  freelist.
	
	  If we're feeding an AGFL block to something that doesn't live in the
	  free space, we need to clear out the OWN_AG rmap.
	
	  Can't do the allocation, give up.
  Allocate a variable extent in the allocation group agno.
  Type and bno are used to determine where in the allocation group the
  extent will start.
  Extent's length (returned in len) will be between minlen and maxlen,
  and of the form k  prod + mod unless there's nothing that large.
  Return the starting a.g. block, or NULLAGBLOCK if we can't do it.
 error 
 argument structure for allocation 
	
	  Branch to correct routine based on the type.
 NOTREACHED 
 if not file data, insert new block into the reverse map btree 
  Allocate a variable extent at exactly agnobno.
  Extent's length (returned in len) will be between minlen and maxlen,
  and of the form k  prod + mod unless there's nothing that large.
  Return the starting a.g. block (bno), or NULLAGBLOCK if we can't do it.
 error 
 allocation argument structure 
 by block-number btree cursor 
 by count btree cursor 
 start block of found extent 
 length of found extent 
 start block of busy extent 
 length of busy extent 
 end block of busy extent 
 successfailure of operation 
	
	  Allocateinitialize a cursor for the by-number freespace btree.
	
	  Lookup bno and minlen in the btree (minlen is irrelevant, really).
	  Look for the closest free block <= bno, it must contain bno
	  if any free block does.
	
	  Grab the freespace record.
	
	  Check for overlapping busy extents.
	
	  Give up if the start of the extent is busy, or the freespace isn't
	  long enough for the minimum request.
	
	  End of extent will be smaller of the freespace end and the
	  maximal requested end.
	 
	  Fix the length according to mod and prod if given.
	
	  We are allocating agbno for args->len
	  Allocateinitialize a cursor for the by-size btree.
 Didn't find it, return null. 
  Search a given number of btree records in a given direction. Check each
  record against the good extent we've already found.
 quit on first candidate 
 rec count (-1 for infinite) 
	
	  Search so long as the cursor is active or we find a better extent.
	  The cursor is deactivated if it extends beyond the range of the
	  current allocation candidate.
  Search the by-bno and by-size btrees in parallel in search of an extent with
  ideal locality based on the NEAR mode ->agbno locality hint.
	
	  Search the bnobt and cntbt in parallel. Search the bnobt left and
	  right and lookup the closest extent to the locality hint for each
	  extent size key in the cntbt. The entire search terminates
	  immediately on a bnobt hit because that means we've found best case
	  locality. Otherwise the search continues until the cntbt cursor runs
	  off the end of the tree. If no allocation candidate is found at this
	  point, give up on locality, walk backwards from the end of the cntbt
	  and take the first available extent.
	 
	  The parallel tree searches balance each other out to provide fairly
	  consistent performance for various situations. The bnobt search can
	  have pathological behavior in the worst case scenario of larger
	  allocation requests and fragmented free space. On the other hand, the
	  bnobt is able to satisfy most smaller allocation requests much more
	  quickly than the cntbt. The cntbt search can sift through fragmented
	  free space and sets of free extents for larger allocation requests
	  more quickly than the bnobt. Since the locality hint is just a hint
	  and we don't want to scan the entire bnobt for perfect locality, the
	  cntbt search essentially bounds the bnobt search such that we can
	  find good enough locality at reasonable performance in most cases.
		
		  Search the bnobt left and right. In the case of a hit, finish
		  the search in the opposite direction and we're done.
		
		  Check the extent with best locality based on the current
		  extent size search key and keep track of the best candidate.
	
	  If we failed to find anything due to busy extents, return empty
	  handed so the caller can flush and retry. If no busy extents were
	  found, walk backwards from the end of the cntbt as a last resort.
	
	  Search in the opposite direction for a better entry in the case of
	  a bnobt hit or walk backwards from the end of the cntbt.
 Check the last block of the cnt btree for allocations. 
 Randomly don't execute the first algorithm. 
	
	  Start from the entry that lookup found, sequence through all larger
	  free blocks.  If we're actually pointing at a record smaller than
	  maxlen, go to the start of this block, and skip all those smaller
	  than minlen.
	
	  It didn't work.  We COULD be in a case where there's a good record
	  somewhere, so try again.
  Allocate a variable extent near bno in the allocation group agno.
  Extent's length (returned in len) will be between minlen and maxlen,
  and of the form k  prod + mod unless there's nothing that large.
  Return the starting a.g. block, or NULLAGBLOCK if we can't do it.
 error code 
 result code, temporary 
 handle uninitialized agbno range so caller doesn't have to 
 clamp agbno to the range if it's outside 
	
	  Set up cursors and see if there are any free extents as big as
	  maxlen. If not, pick the last entry in the tree unless the tree is
	  empty.
	
	  First algorithm.
	  If the requested extent is large wrt the freespaces available
	  in this a.g., then the cursor will be pointing to a btree entry
	  near the right edge of the tree.  If it's in the last btree leaf
	  block, then we just examine all the entries in that block
	  that are big enough, and pick the best one.
	
	  Second algorithm. Combined cntbt and bnobt search to find ideal
	  locality.
	
	  If we couldn't get anything, give up.
 fix up btrees on a successful allocation 
  Allocate a variable extent anywhere in the allocation group agno.
  Extent's length (returned in len) will be between minlen and maxlen,
  and of the form k  prod + mod unless there's nothing that large.
  Return the starting a.g. block, or NULLAGBLOCK if we can't do it.
 error 
 allocation argument structure 
 cursor for bno btree 
 cursor for cnt btree 
 error result 
 start of found freespace 
 length of found freespace 
 temp status variable 
 returned block number 
 length of returned extent 
	
	  Allocate and initialize a cursor for the by-size btree.
	
	  Look for an entry >= maxlen+alignment-1 blocks.
	
	  If none then we have to settle for a smaller extent. In the case that
	  there are no large extents, this will return the last entry in the
	  tree unless the tree is empty. In the case that there are only busy
	  large extents, this will return the largest small extent unless there
	  are no smaller extents available.
		
		  Search for a non-busy extent that is large enough.
				
				  Our only valid extents must have been busy.
				  Make it unbusy by forcing the log out and
				  retrying.
	
	  In the first case above, we got the last entry in the
	  by-size btree.  Now we check to see if the space hits maxlen
	  once aligned; if not, we search left for something better.
	  This can't happen in the second case above.
	
	  Fix up the length.
	
	  Allocate and initialize a cursor for the by-block tree.
  Free the extent starting at agnobno for length.
 start of right neighbor 
 length of right neighbor 
 start of left neighbor 
 length of left neighbor 
 new starting block of freesp 
 new length of freespace 
 have a left neighbor 
 have a right neighbor 
	
	  Allocate and initialize a cursor for the by-block btree.
	
	  Look for a neighboring block on the left (lower block numbers)
	  that is contiguous with this space.
		
		  There is a block to our left.
		
		  It's not contiguous, though.
			
			  If this failure happens the request to free this
			  space was invalid, it's (partly) already free.
			  Very bad.
	
	  Look for a neighboring block on the right (higher block numbers)
	  that is contiguous with this space.
		
		  There is a block to our right.
		
		  It's not contiguous, though.
			
			  If this failure happens the request to free this
			  space was invalid, it's (partly) already free.
			  Very bad.
	
	  Now allocate and initialize a cursor for the by-size tree.
	
	  Have both left and right contiguous neighbors.
	  Merge all three into a single free block.
		
		  Delete the old by-size entry on the left.
		
		  Delete the old by-size entry on the right.
		
		  Delete the old by-block entry for the right block.
		
		  Move the by-block cursor back to the left neighbor.
		
		  Check that this is the right record: delete didn't
		  mangle the cursor.
		
		  Update remaining by-block entry to the new, joined block.
	
	  Have only a left contiguous neighbor.
	  Merge it together with the new freespace.
		
		  Delete the old by-size entry on the left.
		
		  Back up the by-block cursor to the left neighbor, and
		  update its length.
	
	  Have only a right contiguous neighbor.
	  Merge it together with the new freespace.
		
		  Delete the old by-size entry on the right.
		
		  Update the starting block and length of the right
		  neighbor in the by-block tree.
	
	  No contiguous neighbors.
	  Insert the new freespace into the by-block tree.
	
	  In all cases we need to insert the new freespace in the by-size tree.
	
	  Update the freespace totals in the ag and superblock.
  Visible (exported) allocationfree functions.
  Some of these are used just by xfs_alloc_btree.c and this file.
  Compute and fill in value of m_alloc_maxlevels.
 file system mount structure 
  Find the length of the longest extent in an AG.  The 'need' parameter
  specifies how much space we're going to need for the AGFL and the
  'reserved' parameter tells us how many blocks in this AG are reserved for
  other callers.
	
	  If the AGFL needs a recharge, we'll have to subtract that from the
	  longest extent.
	
	  If we cannot maintain others' reservations with space from the
	  not-longest freesp extents, we'll have to subtract that from
	  the longest extent too.
	
	  If the longest extent is long enough to satisfy all the
	  reservations and AGFL rules in place, we can return this extent.
 Otherwise, let the caller try for 1 block if there's space. 
  Compute the minimum length of the AGFL in the given AG.  If @pag is NULL,
  return the largest possible minimum length.
 AG btrees have at least 1 level. 
 space needed by-bno freespace btree 
 space needed by-size freespace btree 
 space needed reverse mapping used space btree 
  Check if the operation we are fixing up the freelist for should go ahead or
  not. If we are freeing blocks, we always allow it, otherwise the allocation
  is dependent on whether the size and shape of free space available will
  permit the requested allocation to take place.
 blocks that are still reserved 
 do we have enough contiguous free space for the allocation? 
	
	  Do we have enough free space remaining for the allocation? Don't
	  account extra agfl blocks because we are about to defer free them,
	  making them unavailable until the current transaction commits.
	
	  Clamp maxlen to the amount of free space available for the actual
	  extent allocation.
  Check the agfl fields of the agf for inconsistency or corruption. The purpose
  is to detect an agfl header padding mismatch between current and early v5
  kernels. This problem manifests as a 1-slot size difference between the
  on-disk flcount and the active [first, last] range of a wrapped agfl. This
  may also catch variants of agfl count corruption unrelated to padding. Either
  way, we'll reset the agfl and warn the user.
  Return true if a reset is required before the agfl can be used, false
  otherwise.
 no agfl header on v4 supers 
	
	  The agf read verifier catches severe corruption of these fields.
	  Repeat some sanity checks to cover a packed -> unpacked mismatch if
	  the verifier allows it.
	
	  Check consistency between the on-disk count and the active range. An
	  agfl padding mismatch manifests as an inconsistent flcount.
  Reset the agfl to an empty state. Ignoredrop any existing blocks since the
  agfl content cannot be trusted. Warn the user that a repair is required to
  recover leaked blocks.
  The purpose of this mechanism is to handle filesystems affected by the agfl
  header padding mismatch problem. A reset keeps the filesystem online with a
  relatively minor free space accounting inconsistency rather than suffer the
  inevitable crash from use of an invalid agfl block.
  Defer an AGFL block free. This is effectively equivalent to
  xfs_free_extent_later() with some special handling particular to AGFL blocks.
  Deferring AGFL frees helps prevent log reservation overruns due to too many
  allocation operations in a transaction. AGFL frees are prone to this problem
  because for one they are always freed one at a time. Further, an immediate
  AGFL block free can cause a btree join and require another block free before
  the real allocation can proceed. Deferring the free disconnects freeing up
  the AGFL slot from freeing the block.
 new element 
  Add the extent to the list of extents to be free at transaction end.
  The list is maintained sorted (by block number).
 new element 
  Check if an AGF has a free extent record whose length is equal to
  args->minlen.
  Decide whether to use this allocation group for this allocation.
  If so, fix up the btree freelist's size.
 error 
 allocation argument structure 
 XFS_ALLOC_FLAG_... 
 local allocation arguments 
 freelist block 
 total blocks needed in freelist 
 deferred ops (AGFL block frees) require permanent transactions 
 Couldn't lock the AGF so skip this AG. 
	
	  If this is a metadata preferred pag and we are user data then try
	  somewhere else if we are not being asked to try harder at this
	  point
	
	  Get the a.g. freespace buffer.
	  Can fail if we're not blocking on locks, and it's held.
 Couldn't lock the AGF so skip this AG. 
 reset a padding mismatched agfl before final free space check 
 If there isn't enough total space or single-extent, reject it. 
	
	  Make the freelist shorter if it's too long.
	 
	  Note that from this point onwards, we will always release the agf and
	  agfl buffers on error. This handles the case where we error out and
	  the buffers are clean or may not have been joined to the transaction
	  and hence need to be released manually. If they have been joined to
	  the transaction, then xfs_trans_brelse() will handle them
	  appropriately based on the recursion count and dirty state of the
	  buffer.
	 
	  XXX (dgc): When we have lots of free space, does this buy us
	  anything other than extra overhead when we need to put more blocks
	  back on the free list? Maybe we should only do this when space is
	  getting low or the AGFL is more than half full?
	 
	  The NOSHRINK flag prevents the AGFL from being shrunk if it's too
	  big; the NORMAP flag prevents AGFL expandshrink operations from
	  updating the rmapbt.  Both flags are used in xfs_repair while we're
	  rebuilding the rmapbt, and neither are used by the kernel.  They're
	  both required to ensure that rmaps are correctly recorded for the
	  regenerated AGFL, bnobt, and cntbt.  See repairphase5.c and
	  repairrmap.c in xfsprogs for details.
 struct copy below 
 defer agfl frees 
 Make the freelist longer if it's too short. 
 Allocate as many blocks as possible at once. 
		
		  Stop if we run out.  Won't happen if callers are obeying
		  the restrictions correctly.  Can happen for free calls
		  on a completely full ag.
		
		  Put each allocated block on the list.
  Get a block from the freelist.
  Returns with the buffer for the block gotten.
	
	  Freelist is empty, give up.
	
	  Read the array of free blocks.
	
	  Get the block number and update the data structures.
  Log the given fields from the agf structure.
 transaction pointer 
 buffer for a.g. freelist header 
 mask of fields to be logged (XFS_AGF_...) 
 first byte offset 
 last byte offset 
 needed so that we don't log the whole rest of the structure: 
  Interface for inode allocation to force the pag data to be initialized.
 error 
 file system mount structure 
 transaction pointer 
 allocation group number 
 XFS_ALLOC_FLAGS_... 
  Put the block on the freelist for the allocation group.
	
	  during growfs operations, the perag is not fully initialised,
	  so we can't use it for any useful checking. growfs ensures we can't
	  use it by using uncached buffers that don't have the perag attached
	  so we can detect and avoid this problem.
  Read in the allocation group header (freealloc section).
 error 
 mount point structure 
 transaction pointer 
 allocation group number 
 XFS_BUF_ 
 buffer for the ag freelist header 
  Read in the allocation group header (freealloc section).
 error 
 mount point structure 
 transaction pointer 
 allocation group number 
 XFS_ALLOC_FLAG_... 
 buffer for the ag freelist header 
 ag freelist header 
 per allocation group data 
 We don't support trylock when freeing. 
		
		  Update the in-core allocbt counter. Filter out the rmapbt
		  subset of the btreeblks counter because the rmapbt is managed
		  by perag reservation. Subtract one for the rmapbt root block
		  because the rmap counter includes it while the btreeblks
		  counter only tracks non-root blocks.
  Allocate an extent (variable-size).
  Depending on the allocation type, we either look in a single allocation
  group or loop over the allocation groups to find the result.
 error 
 allocation argument structure 
 allocation group size 
 XFS_ALLOC_FLAG_... locking flags 
 mount structure pointer 
 starting allocation group number 
 input allocation type 
 inode32 agf stepper 
	
	  Just fix this up, for the case where the last a.g. is shorter
	  (or there's only one a.g.) and the caller couldn't easily figure
	  that out (xfs_bmap_alloc).
		
		  These three force us into a single a.g.
		
		  Try near allocation first, then anywhere-in-ag after
		  the first a.g. fails.
		
		  Rotate through the allocation groups looking for a winner.
			
			  Start with allocation group given by bno.
			
			  Start with the given allocation group.
		
		  Loop over allocation groups twice; first time with
		  trylock set, second time without.
			
			  If we get a buffer back then the allocation will fly.
			
			  Didn't work, figure out the next iteration.
			
			 For the first allocation, we can try any AG to get
			 space.  However, if we already have allocated a
			 block, we don't want to try AGs whose number is below
			 sagno. Otherwise, we may end up with out-of-order
			 locking of AGF, which might cause deadlock.
			
			  Reached the starting a.g., must either be done
			  or switch to non-trylock mode.
 NOTREACHED 
 Ensure that the freelist is at full capacity. 
	
	  validate that the block number is legal - the enables us to detect
	  and handle a silent filesystem corruption rather than crashing.
  Free an extent.
  Just break up the extent address and hand off to xfs_free_ag_extent
  after fixing up the freelist.
 validate the extent size is legal now we have the agf locked 
 Format btree record and pass to our callback. 
 Find all free space within a given range of blocks. 
 Find all free space records. 
 Is there a record covering a given extent? 
  Walk all the blocks in the AGFL.  The @walk_fn can return any negative
  error code or XFS_ITER_.
 Nothing to walk in an empty AGFL. 
 Otherwise, walk from first to last, wrapping as needed. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  Copyright (c) 2013 Red Hat, Inc.
  All Rights Reserved.
 basic block units 
  Do some primitive error checking on ondisk dquot data structures.
  The xfs_dqblk structure contains the xfs_disk_dquot structure;
  we verify them separately because at some points we have only the
  smaller xfs_disk_dquot structure available.
 used only during quotacheck 
	
	  We can encounter an uninitialized dquot buffer for 2 reasons:
	  1. If we crash while deleting the quotainode(s), and those blks got
	     used for user data. This is because we take the path of regular
	     file deletion; however, the size field of quotainodes is never
	     updated, so all the tricks that we play in itruncate_finish
	     don't quite matter.
	 
	  2. We don't play the quota buffers when there's a quotaoff logitem.
	     But the allocation will be replayed so we'll end up with an
	     uninitialized quota block.
	 
	  This is all fine; things are still consistent, and we haven't lost
	  any quota information. Just don't complain about bad dquot blks.
 used only during quotacheck 
  Do some primitive error checking on ondisk dquot data structures.
	
	  Typically, a repair is only requested by quotacheck.
	
	  if we are in log recovery, the quota subsystem has not been
	  initialised so we have no quotainfo structure. In that case, we need
	  to manually calculate the number of dquots in the buffer.
	
	  if we are in log recovery, the quota subsystem has not been
	  initialised so we have no quotainfo structure. In that case, we need
	  to manually calculate the number of dquots in the buffer.
	
	  On the first read of the buffer, verify that each dquot is valid.
	  We don't know what the id of the dquot is supposed to be, just that
	  they should be increasing monotonically within the buffer. If the
	  first id is corrupt, then it will fail on the second dquot in the
	  buffer so corruptions could point to the wrong dquot in this case.
  readahead errors are silent and simply leave the buffer as !done so a real
  read will then be run with the xfs_dquot_buf_ops verifier. See
  xfs_inode_buf_verify() for why we use EIO and ~XBF_DONE here rather than
  reporting the failure.
  we don't calculate the CRC here as that is done when the dquot is flushed to
  the buffer after the update is done. This ensures that the dquot in the
  buffer always has an up-to-date CRC value.
 Convert an on-disk timer value into an incore timer value. 
 Convert an incore timer value into an on-disk timer value. 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2016 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
  Per-AG Block Reservations
  For some kinds of allocation group metadata structures, it is advantageous
  to reserve a small number of blocks in each AG so that future expansions of
  that data structure do not encounter ENOSPC because errors during a btree
  split cause the filesystem to go offline.
  Prior to the introduction of reflink, this wasn't an issue because the free
  space btrees maintain a reserve of space (the AGFL) to handle any expansion
  that may be necessary; and allocations of other metadata (inodes, BMBT,
  dirattr) aren't restricted to a single AG.  However, with reflink it is
  possible to allocate all the space in an AG, have subsequent reflinkCoW
  activity expand the refcount btree, and discover that there's no space left
  to handle that expansion.  Since we can calculate the maximum size of the
  refcount btree, we can reserve space for it and avoid ENOSPC.
  Handling per-AG reservations consists of three changes to the allocator's
  behavior:  First, because these reservations are always needed, we decrease
  the ag_max_usable counter to reflect the size of the AG after the reserved
  blocks are taken.  Second, the reservations must be reflected in the
  fdblocks count to maintain proper accounting.  Third, each AG must maintain
  its own reserved block counter so that we can calculate the amount of space
  that must remain free to maintain the reservations.  Fourth, the "remaining
  reserved blocks" count must be used when calculating the length of the
  longest free extent in an AG and to clamp maxlen in the per-AG allocation
  functions.  In other words, we maintain a virtual allocation via in-core
  accounting tricks so that we don't have to clean up after a crash. :)
  Reserved blocks can be managed by passing one of the enum xfs_ag_resv_type
  values via struct xfs_alloc_arg or directly to the xfs_free_extent
  function.  It might seem a little funny to maintain a reservoir of blocks
  to feed another reservoir, but the AGFL only holds enough blocks to get
  through the next transaction.  The per-AG reservation is to ensure (we
  hope) that each AG never runs out of blocks.  Each data structure wanting
  to use the reservation system should update askused in xfs_ag_resv_init.
  Are we critically low on blocks?  For now we'll define that as the number
  of blocks we can get our hands on being less than 10% of what we reserved
  or less than some arbitrary number (maximum btree height).
 Critically low if less than 10% or max btree height remains. 
  How many blocks are reserved but not used, and therefore must not be
  allocated away?
 empty 
 Clean out a reservation 
	
	  RMAPBT blocks come from the AGFL and AGFL blocks are always
	  considered "free", so whatever was reserved at mount time must be
	  given back at umount.
 Free a per-AG reservation. 
		
		  Space taken by the rmapbt is not subtracted from fdblocks
		  because the rmapbt lives in the free space.  Here we must
		  subtract the entire reservation from fdblocks so that we
		  always have blocks available for rmapbt expansion.
		
		  Space taken by all other metadata btrees are accounted
		  on-disk as used space.  We therefore only hide the space
		  that is reserved but not used by the trees.
	
	  Reduce the maximum per-AG allocation length by however much we're
	  trying to reserve for an AG.  Since this is a filesystem-wide
	  counter, we only make the adjustment for AG 0.  This assumes that
	  there aren't any AGs hungrier for per-AG reservation than AG 0.
 Create a per-AG block reservation. 
 Create the metadata reservation. 
			
			  Because we didn't have per-AG reservations when the
			  finobt feature was added we might not be able to
			  reserve all needed blocks.  Warn and fall back to the
			  old and potentially buggy code in that case, but
			  ensure we do have the reservation for the refcountbt.
 Create the RMAPBT metadata reservation 
	
	  Initialize the pagf if we have at least one active reservation on the
	  AG. This may have occurred already via reservation calculation, but
	  fall back to an explicit init to ensure the in-core allocbt usage
	  counters are initialized as soon as possible. This is important
	  because filesystems with large perag reservations are susceptible to
	  free space reservation problems that the allocbt counter is used to
	  address.
		
		  If there isn't enough space in the AG to satisfy the
		  reservation, let the caller know that there wasn't enough
		  space.  Callers are responsible for deciding what to do
		  next, since (in theory) we can stumble along with
		  insufficient reservation if data blocks are being freed to
		  replenish the AG's free space.
 Allocate a block from the reservation. 
 Allocations of reserved blocks only need on-disk sb updates... 
 ...but non-reserved blocks need in-core and on-disk updates. 
 Free a block to the reservation. 
 Freeing into the reserved pool only requires on-disk update... 
 ...but freeing beyond that requires in-core and on-disk update. 
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2016 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 Allowable refcount adjustment amounts. 
  Look up the first record less than or equal to [bno, len] in the btree
  given by cur.
  Look up the first record greater than or equal to [bno, len] in the btree
  given by cur.
  Look up the first record equal to [bno, len] in the btree
  given by cur.
 Convert on-disk record to in-core format. 
  Get the data from the pointed-to record.
 handle special COW-staging state 
 check for valid extent range, including overflow 
  Update the record referred to by cur to the value given
  by [bno, len, refcount].
  This either works (return 0) or gets an EFSCORRUPTED error.
  Insert the record referred to by cur to the value given
  by [bno, len, refcount].
  This either works (return 0) or gets an EFSCORRUPTED error.
  Remove the record referred to by cur, then set the pointer to the spot
  where the record could be re-inserted, in case we want to increment or
  decrement the cursor.
  This either works (return 0) or gets an EFSCORRUPTED error.
  Adjusting the Reference Count
  As stated elsewhere, the reference count btree (refcbt) stores
  >1 reference counts for extents of physical blocks.  In this
  operation, we're either raising or lowering the reference count of
  some subrange stored in the tree:
       <------ adjustment range ------>
  ----+   +---+-----+ +--+--------+---------
   2  |   | 3 |  4  | |17|   55   |   10
  ----+   +---+-----+ +--+--------+---------
  X axis is physical blocks number;
  reference counts are the numbers inside the rectangles
  The first thing we need to do is to ensure that there are no
  refcount extents crossing either boundary of the range to be
  adjusted.  For any extent that does cross a boundary, split it into
  two extents so that we can increment the refcount of one of the
  pieces later:
       <------ adjustment range ------>
  ----+   +---+-----+ +--+--------+----+----
   2  |   | 3 |  2  | |17|   55   | 10 | 10
  ----+   +---+-----+ +--+--------+----+----
  For this next step, let's assume that all the physical blocks in
  the adjustment range are mapped to a file and are therefore in use
  at least once.  Therefore, we can infer that any gap in the
  refcount tree within the adjustment range represents a physical
  extent with refcount == 1:
       <------ adjustment range ------>
  ----+---+---+-----+-+--+--------+----+----
   2  |"1"| 3 |  2  |1|17|   55   | 10 | 10
  ----+---+---+-----+-+--+--------+----+----
       ^
  For each extent that falls within the interval range, figure out
  which extent is to the left or the right of that extent.  Now we
  have a left, current, and right extent.  If the new reference count
  of the center extent enables us to merge left, center, and right
  into one record covering all three, do so.  If the center extent is
  at the left end of the range, abuts the left extent, and its new
  reference count matches the left extent's record, then merge them.
  If the center extent is at the right end of the range, abuts the
  right extent, and the reference counts match, merge those.  In the
  example, we can left merge (assuming an increment operation):
       <------ adjustment range ------>
  --------+---+-----+-+--+--------+----+----
     2    | 3 |  2  |1|17|   55   | 10 | 10
  --------+---+-----+-+--+--------+----+----
           ^
  For all other extents within the range, adjust the reference count
  or delete it if the refcount falls below 2.  If we were
  incrementing, the end result looks like this:
       <------ adjustment range ------>
  --------+---+-----+-+--+--------+----+----
     2    | 4 |  3  |2|18|   56   | 11 | 10
  --------+---+-----+-+--+--------+----+----
  The result of a decrement operation looks as such:
       <------ adjustment range ------>
  ----+   +---+       +--+--------+----+----
   2  |   | 2 |       |16|   54   |  9 | 10
  ----+   +---+       +--+--------+----+----
       DDDD    111111DD
  The blocks marked "D" are freed; the blocks marked "1" are only
  referenced once and therefore the record is removed from the
  refcount btree.
 Next block after this extent. 
  Split a refcount extent that crosses agbno.
 Establish the right extent. 
 Insert the left extent. 
  Merge the left, center, and right extents.
	
	  Make sure the center and right extents are not in the btree.
	  If the center extent was synthesized, the first delete call
	  removes the right extent and we skip the second deletion.
	  If center and right were in the btree, then the first delete
	  call removes the center and the second one removes the right
	  extent.
 Enlarge the left extent. 
  Merge with the left extent.
 If the extent at agbno (cleft) wasn't synthesized, remove it. 
 Enlarge the left extent. 
  Merge with the right extent.
	
	  If the extent ending at agbno+aglen (cright) wasn't synthesized,
	  remove it.
 Enlarge the right extent. 
  Find the left extent and the one after it (cleft).  This function assumes
  that we've already split any extent crossing agbno.
 We have a left extent; retrieve (or invent) the next right one 
 if tmp starts at the end of our range, just use that 
			
			  There's a gap in the refcntbt at the start of the
			  range we're interested in (refcount == 1) so
			  synthesize the implied extent and pass it back.
			  We assume here that the agbnoaglen range was
			  passed in from a data fork extent mapping and
			  therefore is allocated to exactly one owner.
		
		  No extents, so pretend that there's one covering the whole
		  range.
  Find the right extent and the one before it (cright).  This function
  assumes that we've already split any extents crossing agbno + aglen.
 We have a right extent; retrieve (or invent) the next left one 
 if tmp ends at the end of our range, just use that 
			
			  There's a gap in the refcntbt at the end of the
			  range we're interested in (refcount == 1) so
			  create the implied extent and pass it back.
			  We assume here that the agbnoaglen range was
			  passed in from a data fork extent mapping and
			  therefore is allocated to exactly one owner.
		
		  No extents, so pretend that there's one covering the whole
		  range.
 Is this extent valid? 
  Try to merge with any extents on the boundaries of the adjustment range.
	
	  Find the extent just below agbno [left], just above agbno [cleft],
	  just below (agbno + aglen) [cright], and just above (agbno + aglen)
	  [right].
 No left or right extent to merge; exit. 
 Try to merge left, cleft, and right.  cleft must == cright. 
 Try to merge left and cleft. 
		
		  If we just merged left + cleft and cleft == cright,
		  we no longer have a cright to merge with right.  We're done.
 Try to merge cright and right. 
  XXX: This is a pretty hand-wavy estimate.  The penalty for guessing
  true incorrectly is a shutdown FS; the penalty for guessing false
  incorrectly is more transaction rolls than might be necessary.
  Be conservative here.
	
	  Only allow 2 refcount extent updates per transaction if the
	  refcount continue update "error" has been injected.
  Adjust the refcounts of middle extents.  At this point we should have
  split extents that crossed the adjustment range; merged with adjacent
  extents; and updated agbnoaglen to reflect the merges.  Therefore,
  all we have to do is update the extents inside [agbno, agbno + aglen].
 Merging did all the work already. 
		
		  Deal with a hole in the refcount tree; if a file maps to
		  these blocks and there's no refcountbt record, pretend that
		  there is one with refcount == 1.
			
			  Either cover the hole (increment) or
			  delete the range (decrement).
 Stop if there's nothing left to modify 
		
		  Adjust the reference count and either update the tree
		  (incr) or free the blocks (decr).
 Adjust the reference count of a range of AG blocks. 
	
	  Ensure that no rcextents cross the boundary of the adjustment range.
	
	  Try to merge with the left or right extents of the range.
 Now that we've taken care of the ends, adjust the middle extents 
 Clean up after calling xfs_refcount_finish_one. 
  Process one of the deferred refcount operations.  We pass back the
  btree cursor to maintain our lock on the btree between calls.
  This saves time and eliminates a buffer deadlock between the
  superblock and the AGF because we'll always grab them in the same
  order.
	
	  If we haven't gotten a cursor or the cursor AG doesn't match
	  the startblock, get one now.
  Record a refcount intent for later processing.
  Increase the reference count of the blocks backing a file's extent.
  Decrease the reference count of the blocks backing a file's extent.
  Given an AG extent, find the lowest-numbered run of shared blocks
  within that range and return the range in fbnoflen.  If
  find_end_of_shared is set, return the longest contiguous extent of
  shared blocks; if not, just return the first extent we find.  If no
  shared blocks are found, fbno and flen will be set to NULLAGBLOCK
  and 0, respectively.
 By default, skip the whole range 
 Try to find a refcount extent that crosses the start 
 No left extent, look at the next one 
 If the extent ends before the start, look at the next one 
 If the extent starts after the range we want, bail out 
 We found the start of a shared extent! 
 Otherwise, find the end of this shared extent 
  Recovering CoW Blocks After a Crash
  Due to the way that the copy on write mechanism works, there's a window of
  opportunity in which we can lose track of allocated blocks during a crash.
  Because CoW uses delayed allocation in the in-core CoW fork, writeback
  causes blocks to be allocated and stored in the CoW fork.  The blocks are
  no longer in the free space btree but are not otherwise recorded anywhere
  until the write completes and the blocks are mapped into the file.  A crash
  in between allocation and remapping results in the replacement blocks being
  lost.  This situation is exacerbated by the CoW extent size hint because
  allocations can hang around for long time.
  However, there is a place where we can record these allocations before they
  become mappings -- the reference count btree.  The btree does not record
  extents with refcount == 1, so we can record allocations with a refcount of
  1.  Blocks being used for CoW writeout cannot be shared, so there should be
  no conflict with shared block records.  These mappings should be created
  when we allocate blocks to the CoW fork and deleted when they're removed
  from the CoW fork.
  Minor nit: records for in-progress CoW allocations and records for shared
  extents must never be merged, to preserve the property that (except for CoW
  allocations) there are no refcount btree entries with refcount == 1.  The
  only time this could potentially happen is when unsharing a block that's
  adjacent to CoW allocations, so we must be careful to avoid this.
  At mount time we recover lost CoW allocations by searching the refcount
  btree for these refcount == 1 mappings.  These represent CoW allocations
  that were in progress at the time the filesystem went down, so we can free
  them to get the space back.
  This mechanism is superior to creating EFIs for unmapped CoW extents for
  several reasons -- first, EFIs pin the tail of the log and would have to be
  periodically relogged to avoid filling up the log.  Second, CoW completions
  will have to file an EFD and create new EFIs for whatever remains in the
  CoW fork; this partially takes care of (1) but extent-size reservations
  will have to periodically relog even if there's no writeout in progress.
  This can happen if the CoW extent size hint is set, which you really want.
  Third, EFIs cannot currently be automatically relogged into newer
  transactions to advance the log tail.  Fourth, stuffing the log full of
  EFIs places an upper bound on the number of CoW allocations that can be
  held filesystem-wide at any given time.  Recording them in the refcount
  btree doesn't require us to maintain any state in memory and doesn't pin
  the log.
  Adjust the refcounts of CoW allocations.  These allocations are "magic"
  in that they're not referenced anywhere else in the filesystem, so we
  stash them in the refcount btree with a refcount of 1 until either file
  remapping (or CoW cancellation) happens.
 Find any overlapping refcount records 
 Adding a CoW reservation, there should be nothing here. 
 Removing a CoW reservation, there should be one extent. 
  Add or remove refcount btree entries for CoW reservations.
	
	  Ensure that no rcextents cross the boundary of the adjustment range.
	
	  Try to merge with the left or right extents of the range.
 Now that we've taken care of the ends, adjust the middle extents 
  Record a CoW allocation in the refcount btree.
 Add refcount btree reservation 
  Remove a CoW allocation from the refcount btree.
 Remove refcount btree reservation 
 Record a CoW staging extent in the refcount btree. 
 Add rmap entry 
 Forget a CoW staging event in the refcount btree. 
 Remove rmap entry 
 Stuff an extent on the recovery list. 
 Find and remove leftover CoW reservations. 
	
	  In this first part, we use an empty transaction to gather up
	  all the leftover CoW extents so that we can subsequently
	  delete them.  The empty transaction is used to avoid
	  a buffer lock deadlock if there happens to be a loop in the
	  refcountbt because we're allowed to re-grab a buffer that is
	  already attached to our transaction.  When we're done
	  recording the CoW debris we cancel the (empty) transaction
	  and everything goes away cleanly.
 Find all the leftover CoW staging extents. 
 Now iterate the list to free the leftovers 
 Set up transaction. 
 Free the orphan record 
 Free the block. 
 Free the leftover list 
 Is there a record covering a given extent? 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2001,2005 Silicon Graphics, Inc.
  All Rights Reserved.
 level change 
 level change 
 Update the inode btree block counter for this btree. 
 block allocation args 
 error return value 
 ir_holemaskir_count not supported on-disk 
  initial value of ptr for lookup
	
	  During growfs operations, we can't verify the exact owner as the
	  perag is not fully initialised and hence not attached to the buffer.
	 
	  Similarly, during log recovery we will have a perag structure
	  attached, but the agi information will not yet have been initialised
	  from the on disk AGI. We don't currently use any of this information,
	  but beware of the landmine (i.e. need to check pag->pagi_init) if we
	  ever do.
 level verification 
  Initialize a new inode btree cursor.
 file system mount point 
 transaction pointer 
 ialloc or free ino btree 
 take a reference for the cursor 
 Create an inode btree cursor. 
 Create an inode btree cursor with a fake root for staging. 
  Install a new inobt btree root.  Caller is responsible for invalidating
  and freeing the old btree blocks.
 Calculate number of records in an inode btree block. 
  Calculate number of records in an inobt btree block.
  Maximum number of inode btree records per AG.  Pretend that we can fill an
  entire AG completely full of inodes except for the AG headers.
 Compute the max possible height for the inode btree. 
 Compute the max possible height for the free inode btree. 
 Compute the max possible height for either inode btree. 
  Convert the inode record holemask to an inode allocation bitmap. The inode
  allocation bitmap is inode granularity and specifies whether an inode is
  physically allocated on disk (not whether the inode is considered allocated
  or free by the fs).
  A bit value of 1 means the inode is allocated, a value of 0 means it is free.
	
	  The holemask has 16-bits for a 64 inode record. Therefore each
	  holemask bit represents multiple inodes. Create a mask of bits to set
	  in the allocmask for each holemask bit.
	
	  Allocated inodes are represented by 0 bits in holemask. Invert the 0
	  bits to 1 and convert to a uint so we can use xfs_next_bit(). Mask
	  anything beyond the 16 holemask bits since this casts to a larger
	  type.
	
	  allocbitmap is the inverted holemask so every set bit represents
	  allocated inodes. To expand from 16-bit holemask granularity to
	  64-bit (e.g., bit-per-inode), set inodespbit bits in the target
	  bitmap for every holemask bit.
  Verify that an in-core inode record has a valid inode count.
 DEBUG 
 Bail out if we're uninitialized, which can happen in mkfs. 
	
	  The log is permanently allocated, so the space it occupies will
	  never be available for the kinds of things that would require btree
	  expansion.  We therefore can pretend the space isn't there.
 Read AGI and create inobt cursor. 
 Read finobt block count from AGI header. 
  Figure out how many blocks to reserve and how many are used by this btree.
 Calculate the inobt btree size for some records. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.
  Copyright (C) 2017 Oracle.
  All Rights Reserved.
 Find the size of the AG, in blocks. 
  Verify that an AG block number pointer neither points outside the AG
  nor points at static metadata.
  Verify that an FS block number pointer neither points outside the
  filesystem nor points at static AG metadata.
  Verify that a data device extent is fully contained inside the filesystem,
  does not cross an AG boundary, and does not point at static metadata.
 Calculate the first and last possible inode number in an AG. 
	
	  Calculate the first inode, which will be in the first
	  cluster-aligned block after the AGFL.
	
	  Calculate the last inode, which will be at the end of the
	  last (aligned) cluster that can be allocated in the AG.
  Verify that an AG inode number pointer neither points outside the AG
  nor points at static metadata.
  Verify that an AG inode number pointer neither points outside the AG
  nor points at static metadata, or is NULLAGINO.
  Verify that an FS inode number pointer neither points outside the
  filesystem nor points at static AG metadata.
 Is this an internal inode number? 
  Verify that a directory entry's inode number doesn't point at an internal
  inode, empty space, or static AG metadata.
  Verify that an realtime block number pointer doesn't point off the
  end of the realtime device.
 Verify that a realtime device extent is fully contained inside the volume. 
 Calculate the range of valid icount values. 
 root, rtbitmap, rtsum all live in the first chunk 
 Sanity-checking of inode counts. 
 Sanity-checking of dirattr block offsets. 
 Check that a file block offset does not exceed the maximum. 
 Check that a range of file block offsets do not exceed the maximum. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  All Rights Reserved.
  Physical superblock buffer manipulations. Shared with libxfs in userspace.
  We support all XFS versions newer than a v4 superblock with V2 directories.
 all v5 filesystems are supported 
 versions prior to v4 are not supported 
 V4 filesystems need v2 directories and unwritten extents 
 And must not have any unknown v4 feature bits set 
 It's a supported v4 filesystem 
 optional V4 features 
 Always on V5 features 
 Optional V5 features 
 Check all the superblock fields we care about when reading one in. 
	
	  Version 5 superblock feature mask validation. Reject combinations
	  the kernel cannot support up front before checking anything else.
 Check all the superblock fields we care about when writing one out. 
	
	  Carry out additional sb summary counter sanity checks when we write
	  the superblock.  We skip this in the read validator because there
	  could be newer superblocks in the log and if the values are garbage
	  even after replay we'll recalculate them at the end of log mount.
	 
	  mkfs has traditionally written zeroed counters to inprogress and
	  secondary superblocks, so allow this usage to continue because
	  we never read counters from such superblocks.
	
	  Version 5 superblock feature mask validation. Reject combinations
	  the kernel cannot support since we checked for unsupported bits in
	  the read verifier, which means that memory is corrupt.
	
	  We can't read verify the sb LSN because the read verifier is called
	  before the log is allocated and processed. We know the log is set up
	  before write verifier calls, so check it here.
 Check the validity of the SB. 
	
	  Validate feature flags and state
 V5 has a separate project quota inode 
		
		  Full inode chunks must be aligned to inode chunk size when
		  sparse inodes are enabled to support the sparse chunk
		  allocation algorithm and prevent overlapping inode records.
 Compute agcount for this number of dblocks and agblocks 
	
	  More sanity checking.  Most of these were stolen directly from
	  xfs_repair.
 zero sb_imax_pct is valid )	||
 Validate the realtime geometry; stolen from xfs_repair 
	
	  Either (sb_unit and !hasdalign) or (!sb_unit and hasdalign)
	  would imply the image is corrupted.
	
	  Currently only very few inode sizes are supported.
	
	  older mkfs doesn't initialize quota inodes to NULLFSINO. This
	  leads to in-core values having two different values for a quota
	  inode to be invalid: 0 and NULLFSINO. Change it to a single value
	  NULLFSINO.
	 
	  Note that this change affect only the in-core values. These
	  values are not written back to disk unless any quota information
	  is written to the disk. Even in that case, sb_pquotino field is
	  not written to disk unless the superblock supports pquotino.
	
	  We need to do these manipilations only if we are working
	  with an older version of on-disk superblock.
		
		  In older version of superblock, on-disk superblock only
		  has sb_gquotino, and in-core superblock has both sb_gquotino
		  and sb_pquotino. But, only one of them is supported at any
		  point of time. So, if PQUOTA is set in disk superblock,
		  copy over sb_gquotino to sb_pquotino.  The NULLFSINO test
		  above is to make sure we don't do this twice and wipe them
		  both out!
 crc is only used on disk, not in memory; just init to 0 here. 
	
	  sb_meta_uuid is only on disk if it differs from sb_uuid and the
	  feature flag is set; if not set we keep it only in memory.
 Convert on-disk flags to in-memory flags? 
	
	  The in-memory superblock quota state matches the v5 on-disk format so
	  just write them out and return
	
	  For older superblocks (v4), the in-core version of sb_qflags do not
	  have XFS_OQUOTA_ flags, whereas the on-disk version does.  So,
	  convert incore XFS_{PG}QUOTA_ flags to on-disk XFS_OQUOTA_ flags.
	
	  GQUOTINO and PQUOTINO cannot be used together in versions
	  of superblock that do not have pquotino. from->sb_flags
	  tells us which quota is active and should be copied to
	  disk. If neither are active, we should NULL the inode.
	 
	  In all cases, the separate pquotino must remain 0 because it
	  is beyond the "end" of the valid non-pquotino superblock.
		
		  We can't rely on just the fields being logged to tell us
		  that it is safe to write NULLFSINO - we should only do that
		  if quotas are not actually enabled. Hence only write
		  NULLFSINO if both in-core quota inodes are NULL.
	
	  We need to ensure that bad_features2 always matches features2.
	  Hence we enforce that here rather than having to remember to do it
	  everywhere else that updates features2.
  If the superblock has the CRC feature bit set or the CRC field is non-null,
  check that the CRC is valid.  We check the CRC field is non-null because a
  single bit error could clear the feature bit and unused parts of the
  superblock are supposed to be zero. Hence a non-null crc field indicates that
  we've potentially lost a feature bit and we should check it anyway.
  However, past bugs (i.e. in growfs) left non-zeroed regions beyond the
  last field in V4 secondary superblocks.  So for secondary superblocks,
  we are more forgiving, and ignore CRC failures if the primary doesn't
  indicate that the fs version is V5.
	
	  open code the version check to avoid needing to convert the entire
	  superblock from disk order just to check the version number
 Only fail bad secondaries on a known V5 filesystem 
	
	  Check all the superblock fields.  Don't byteswap the xquota flags
	  because _verify_common checks the on-disk values.
  We may be probed for a filesystem match, so we may not want to emit
  messages when the superblock buffer is not actually an XFS superblock.
  If we find an XFS superblock, then run a normal, noisy mount because we are
  really going to mount it and want to know about errors.
 XFS filesystem, verify noisily! 
 quietly fail 
	
	  Check all the superblock fields.  Don't byteswap the xquota flags
	  because _verify_common checks the on-disk values.
  xfs_mount_common
  Mount initialization code establishing various mount
  fields from the superblock associated with the given
  mount structure.
  Inode geometry are calculated in xfs_ialloc_setup_geometry.
  xfs_log_sb() can be used to copy arbitrary changes to the in-core superblock
  into the superblock buffer to be logged.  It does not provide the higher
  level of locking that is needed to protect the in-core superblock from
  concurrent access.
	
	  Lazy sb counters don't update the in-core superblock so do that now.
	  If this is at unmount, the counters will be exactly correct, but at
	  any other time they will only be ballpark correct because of
	  reservations that have been taken out percpu counters. If we have an
	  unclean shutdown, this will be corrected by log recovery rebuilding
	  the counters from the AGF block counts.
  xfs_sync_sb
  Sync the superblock to disk.
  Note that the caller is responsible for checking the frozen state of the
  filesystem. This procedure uses the non-blocking transaction allocator and
  thus will allow modifications to a frozen fs. This is required because this
  code can be called during the process of freezing where use of the high-level
  allocator would deadlock.
  Update all the secondary superblocks to match the new state of the primary.
  Because we are completely overwriting all the existing fields in the
  secondary superblock buffers, there is no need to read them in from disk.
  Just get a new buffer, stamp it and write it.
  The sb buffers need to be cached here so that we serialise against other
  operations that access the secondary superblocks, but we don't want to keep
  them in memory once it is written so we mark it as a one-shot buffer.
 update secondary superblocks. 
		
		  If we get an error reading or writing alternate superblocks,
		  continue.  xfs_repair chooses the "best" superblock based
		  on most matches; if we break early, we'll leave more
		  superblocks un-updated than updated, and xfs_repair may
		  pick them over the properly-updated primary.
 don't hold too many buffers at once 
  Same behavior as xfs_sync_sb, except that it is always synchronous and it
  also writes the superblock buffer to disk sector 0 immediately.
	
	  write out the sb buffer to get the changes to disk
 Read a secondary superblock. 
 Get an uninitialised secondary superblock buffer. 
  sunit, swidth, sectorsize(optional with 0) should be all in bytes,
  so users won't be confused by values in error messages.
 SPDX-License-Identifier: GPL-2.0+
  Copyright (C) 2016 Oracle.  All Rights Reserved.
  Author: Darrick J. Wong <darrick.wong@oracle.com>
 block allocation args 
 error return value 
  Initialize a new refcount btree cursor.
 take a reference for the cursor 
 Create a btree cursor. 
 Create a btree cursor with a fake root for staging. 
  Swap in the new btree root.  Once we pass this point the newly rebuilt btree
  is in place and we have to kill off all the old btree blocks.
 Calculate number of records in a refcount btree block. 
  Calculate the number of records in a refcount btree block.
 Compute the max possible height of the maximally sized refcount btree. 
 Compute the maximum height of a refcount btree. 
 Calculate the refcount btree size for some records. 
  Calculate the maximum refcount btree size.
 Bail out if we're uninitialized, which can happen in mkfs. 
  Figure out how many blocks to reserve and how many are used by this btree.
	
	  The log is permanently allocated, so the space it occupies will
	  never be available for the kinds of things that would require btree
	  expansion.  We therefore can pretend the space isn't there.
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2006 Silicon Graphics, Inc.
  All Rights Reserved.
	
	  If we are using the local fork to store a symlink body we need to
	  zero-terminate it so that we can pass it back to the VFS directly.
	  Overallocate the in-memory fork by one for that and add a zero
	  to terminate it below.
  The file is in-lined in the on-disk inode.
	
	  If the size is unreasonable, then something
	  is wrong and we just bail out rather than crash in
	  kmem_alloc() or memcpy() below.
  The file consists of a set of extents all of which fit into the on-disk
  inode.
	
	  If the number of extents is unreasonable, then something is wrong and
	  we just bail out rather than crash in kmem_alloc() or memcpy() below.
  The file has too many extents to fit into
  the inode, so they are in B-tree format.
  Allocate a buffer for the root of the B-tree
  and copy the root into it.  The i_extents
  field will remain NULL until all of the
  extents are read in (when they are needed).
 REFERENCED 
	
	  blow out if -- fork has less extents than can fit in
	  fork (fork shouldn't be a btree format), root btree
	  block has more records than can fit into the fork,
	  or the number of extents is greater than the number of
	  blocks.
	
	  Copy and convert from the on-disk structure
	  to the in-memory structure.
	
	  Initialize the extent count early, as the per-format routines may
	  depend on it.
	
	  Initialize the extent count early, as the per-format routines may
	  depend on it.
  Reallocate the space for if_broot based on the number of records
  being added or deleted as indicated in rec_diff.  Move the records
  and pointers in if_broot to fit the new size.  When shrinking this
  will eliminate holes between the records and pointers created by
  the caller.  When growing this will create holes to be filled in
  by the caller.
  The caller must not request to add more records than would fit in
  the on-disk inode root.  If the if_broot is currently NULL, then
  if we are adding records, one will be allocated.  The caller must also
  not request that the number of records go below zero, although
  it can go to zero.
  ip -- the inode whose if_broot area is changing
  ext_diff -- the change in the number of records, positive or negative,
 	 requested for the if_broot array.
	
	  Handle the degenerate case quietly.
		
		  If there wasn't any memory allocated before, just
		  allocate it now and get out.
		
		  If there is already an existing if_broot, then we need
		  to realloc() it and shift the pointers to their new
		  location.  The records don't change location because
		  they are kept butted up against the btree block header.
	
	  rec_diff is less than 0.  In this case, we are shrinking the
	  if_broot buffer.  It must already exist.  If we go to zero
	  records, just get rid of the root and clear the status bit.
		
		  First copy over the btree block header.
	
	  Only copy the records and pointers if there are any.
		
		  First copy the records.
		
		  Then copy the pointers.
  This is called when the amount of space needed for if_data
  is increased or decreased.  The change in size is indicated by
  the number of bytes that need to be added or deleted in the
  byte_diff parameter.
  If the amount of space needed has decreased below the size of the
  inline buffer, then switch to using the inline buffer.  Otherwise,
  use kmem_realloc() or kmem_alloc() to adjust the size of the buffer
  to what is needed.
  ip -- the inode whose if_data area is changing
  byte_diff -- the change in the number of bytes, positive or negative,
 	 requested for the if_data array.
	
	  For inline data, the underlying buffer must be a multiple of 4 bytes
	  in size so that it can be logged and stay on word boundaries.
	  We enforce that here.
  Convert in-core extents to on-disk form
  In the case of the data fork, the in-core and on-disk fork sizes can be
  different due to delayed allocation extents. We only copy on-disk extents
  here, so callers must always use the physical fork size to determine the
  size of the buffer passed to this routine.  We will return the size actually
  used.
  Each of the following cases stores data into the same region
  of the on-disk inode, so only one of them can be valid at
  any given time. While it is possible to have conflicting formats
  and log flags, e.g. having XFS_ILOG_?DATA set when the fork is
  in EXTENTS format, this can only happen when the fork has
  changed formats after being modified but before being flushed.
  In these cases, the format always takes precedence, because the
  format indicates the current state of the fork.
	
	  This can happen if we gave up in iformat in an error path,
	  for the attribute fork.
 Convert bmap state flags to an inode fork. 
  Initialize an inode's copy-on-write fork.
 Verify the inline contents of the data fork of an inode. 
 Verify the inline contents of the attr fork of an inode. 
 SPDX-License-Identifier: GPL-2.0
  Copyright (c) 2000-2005 Silicon Graphics, Inc.
  Copyright (c) 2013 Red Hat, Inc.
  All Rights Reserved.
  xfs_attr_leaf.c
  Routines to implement leaf blocks of attributes as Btrees of hashed names.
========================================================================
  Function prototypes for the kernel.
  Routines used for growing the Btree.
  Utility routines.
  attr3 block 'firstused' conversion helpers.
  firstused refers to the offset of the first used byte of the nameval region
  of an attr leaf block. The region starts at the tail of the block and expands
  backwards towards the middle. As such, firstused is initialized to the block
  size for an empty leaf block and is reduced from there.
  The attr3 block size is pegged to the fsb size and the maximum fsb is 64k.
  The in-core firstused field is 32-bit and thus supports the maximum fsb size.
  The on-disk field is only 16-bit, however, and overflows at 64k. Since this
  only occurs at exactly 64k, we use zero as a magic on-disk value to represent
  the attr block size. The following helpers manage the conversion between the
  in-core and on-disk formats.
	
	  Convert from the magic fsb size value to actual blocksize. This
	  should only occur for empty blocks when the block size overflows
	  16-bits.
 magic value should only be seen on disk 
	
	  Scale down the 32-bit in-core firstused value to the 16-bit on-disk
	  value. This only overflows at the max supported value of 64k. Use the
	  magic on-disk value to represent block size in this case.
 hash order check 
	
	  Check the name information.  The namelen fields are u8 so we can't
	  possibly exceed the maximum name length of 255 bytes.
 must be 32bit - see below 
	
	  firstused is the block offset of the first name info structure.
	  Make sure it doesn't go off the block or crash into the header.
 Make sure the entries array doesn't crash into the name info. 
	
	  NOTE: This verifier historically failed empty leaf buffers because
	  we expect the fork to be in another format. Empty attr fork format
	  conversions are possible during xattr set, however, and format
	  conversion is not atomic with the xattr set that triggers it. We
	  cannot assume leaf blocks are non-empty until that is addressed.
	
	  Quickly check the freemap information.  Attribute data has to be
	  aligned to 4-byte boundaries, and likewise for the free space.
	 
	  Note that for 64k block size filesystems, the freemap entries cannot
	  overflow as they are only be16 fields. However, when checking end
	  pointer of the freemap, we have to be careful to detect overflows and
	  so use uint32_t for those checks.
 be care of 16 bit overflows here 
  leafnode format detection on trees is sketchy, so a node read can be done on
  leaf level blocks when detection identifies the tree as a node format tree
  incorrectly. In this case, we need to swap the verifier to match the correct
  format of the block being read.
========================================================================
  Namespace helper routines
	
	  If we are looking for incomplete entries, show only those, else only
	  show complete entries.
	
	  No copy if all we have to do is get the length
	
	  No copy if the length of the existing buffer is too small
 remote block xattr requires IO for copy-in 
	
	  This is to prevent a GCC warning because the remote xattr case
	  doesn't have a value to pass in. In that case, we never reach here,
	  but GCC can't work that out and so throws a "passing NULL to
	  memcpy" warning.
========================================================================
  External routines when attribute fork size < XFS_LITINO(mp).
  Query whether the total requested number of attr fork bytes of extended
  attribute space will be able to fit inline.
  Returns zero if not, else the i_forkoff fork offset to be used in the
  literal area for attribute data once the new bytes have been added.
  i_forkoff must be 8 byte aligned, hence is stored as a >>3 value;
  special case for devuuid inodes, they have fixed size data forks.
	
	  Check if the new size could fit at all first:
 rounded down 
	
	  If the requested numbers of bytes is smaller or equal to the
	  current attribute fork size we can always proceed.
	 
	  Note that if_bytes in the data fork might actually be larger than
	  the current data fork size is due to delalloc extents. In that
	  case either the extent count will go down when they are converted
	  to real extents, or the delalloc conversion will take care of the
	  literal area rebalancing.
	
	  For attr2 we can try to move the forkoff if there is space in the
	  literal area, but for the old format we are done if there is no
	  space in the fixed attribute fork.
		
		  If there is no attr fork and the data fork is extents,
		  determine if creating the default attr fork will result
		  in the extents form migrating to btree. If so, the
		  minimum offset only needs to be the space required for
		  the btree root.
		
		  If we have a data btree then keep forkoff if we have one,
		  otherwise we are adding a new attr, so then we set
		  minforkoff to where the btree root can finish so we have
		  plenty of room for attrs
	
	  A data fork btree root must have space for at least
	  MINDBTPTRS keyptr pairs if the data fork is small or empty.
 attr fork btree root can have at least this many keyptr pairs 
 rounded down 
  Switch on the ATTR2 superblock bit (implies also FEATURES2) unless:
  - noattr2 mount option is set,
  - on-disk version bit says it is already set, or
  - the attr2 mount option is not set to enable automatic upgrade from attr1.
  Create the initial contents of a shortform attribute list.
  Return -EEXIST if attr is found, or -ENOATTR if not
  args:  args containing attribute name and namelen
  sfep:  If not null, pointer will be set to the last attr entry found on
	  -EEXIST.  On -ENOATTR pointer is left at the last entry in the list
  basep: If not null, pointer is set to the byte offset of the entry in the
 	  list on -EEXIST.  On -ENOATTR, pointer is left at the byte offset of
 	  the last entry in the list
  Add a namevalue pair to the shortform attribute list.
  Overflow from the inode has already been checked for.
  After the last attribute is removed revert to original inode format,
  making all literal area available to the data fork once more.
  Remove an attribute from the shortform attribute list structure.
	
	  Fix up the attribute fork data, covering the hole
	
	  Fix up the start offset of the attribute fork
  Look up a name in a shortform attribute list structure.
ARGSUSED
  Retrieve the attribute value and length.
  If args->valuelen is zero, only the length needs to be returned.  Unlike a
  lookup, we only return an error if the attribute does not exist or we can't
  retrieve the value.
  Convert from using the shortform to the leaf.  On success, return the
  buffer so that we can keep it locked until we're totally done with it.
 set a->index 
  Check a leaf attribute block to see if all the entries would fit into
  a shortform attribute list.
 don't copy partial entries 
 Verify the consistency of an inline attribute fork. 
	
	  Give up if the attribute is way too short.
 Check all reported entries 
		
		  struct xfs_attr_sf_entry has a variable length.
		  Check the fixed-offset parts of the structure are
		  within the data buffer.
		  xfs_attr_sf_entry is defined with a 1-byte variable
		  array at the end, so we must subtract that off.
 Don't allow names with known bad length. 
		
		  Check that the variable-length part of the structure is
		  within the data buffer.  The next entry starts after the
		  name component, so nextentry is an acceptable test.
		
		  Check for unknown flags.  Short form doesn't support
		  the incomplete or local bits, so we can use the namespace
		  mask here.
		
		  Check for invalid namespace combinations.  We only allow
		  one namespace flag per xattr, so we can just count the
		  bits (i.e. hweight) here.
  Convert a leaf attribute list to shortform attribute list
 XXX (dgc): buffer is about to be marked stale - why zero it? 
	
	  Clean out the prior contents of the attribute list.
	
	  Copy the attributes
 don't copy partial entries 
  Convert from using a single leaf to a root node and a leaf.
 copy leaf to new buffer, update identifiers 
	
	  Set up the new root node.
 both on-disk, don't endian-flip twice 
========================================================================
  Routines used for growing the Btree.
  Create the initial contents of a leaf attribute list
  or a leaf in a node attribute list.
  Split the leaf node, rebalance, then add the new entry.
	
	  Allocate space for a new leaf node.
	
	  Rebalance the entries across the two leaves.
	  NOTE: rebalance() currently depends on the 2nd block being empty.
	
	  Save info on "old" attribute for "atomic rename" ops, leaf_add()
	  modifies the indexblknormtblkrmtblkcnt fields to show the
	  "new" attrs info.  Will need the "old" info to remove it later.
	 
	  Insert the "new" entry in the correct block.
	
	  Update last hashval in each block since we added the name.
  Add a name to the leaf attribute list structure.
	
	  Search through freemap for first-fit on new name length.
	  (may need to figure in size of entry struct too)
 no space in this map 
	
	  If there are no holes in the address space of the block,
	  and we don't have enough freespace, then compaction will do us
	  no good and we should just give up.
	
	  Compact the entries to coalesce free space.
	  This may change the hdr->count via dropping INCOMPLETE entries.
	
	  After compaction, the block is guaranteed to have only one
	  free region, in freemap[0].  If it is not big enough, give up.
  Add a name to a leaf attribute list structure.
	
	  Force open some space in the entry array and fill it in.
	
	  Allocate space for the new string (at the end of the run).
	
	  For "remote" attribute values, simply note that we need to
	  allocate space for the "remote" value.  We can't actually
	  allocate the extents in this transaction, and we can't decide
	  which blocks they should be as we might allocate more blocks
	  as part of this transaction (a split operation for example).
 just in case 
	
	  Update the control info for this leaf node
  Garbage collect a leaf attribute list block by copying it to a new buffer.
	
	  Copy the on-disk header back into the destination buffer to ensure
	  all the information in the header that is not part of the incore
	  header structure is preserved.
 Initialise the incore headers 
 struct copy 
 write the header back to initialise the underlying buffer 
	
	  Copy all entry's in the same (sorted) order,
	  but allocate namevalue pairs packed and in sequence.
	
	  this logs the entire buffer, but the caller must write the header
	  back to the buffer when it is finished modifying it.
  Compare two leaf blocks "order".
  Return 0 unless leaf2 should go before leaf1.
  Redistribute the attribute list entries between two leaf nodes,
  taking into account the size of the new entry.
  NOTE: if new block is empty, then it will get the upper half of the
  old block.  At present, all (one) callers pass in an empty second block.
  This code adjusts the args->indexblkno and args->index2blkno2 fields
  to match what it is doing in splitting the attribute leaf block.  Those
  values are used in "atomic rename" operations on attributes.  Note that
  the "new" and "old" values can end up in different blocks.
	
	  Set up environment.
	
	  Check ordering of blocks, reverse if it makes things simpler.
	 
	  NOTE: Given that all (current) callers pass in an empty
	  second block, this code should never set "swap".
 swap structures rather than reconverting them 
	
	  Examine entries until we reduce the absolute difference in
	  byte usage between the two blocks to a minimum.  Then get
	  the direction to copy and the number of elements to move.
	 
	  "inleaf" is true if the new entry should be inserted into blk1.
	  If "swap" is also true, then reverse the sense of "inleaf".
	
	  Move any entries required from leaf to leaf:
		
		  Figure the total bytes to be added to the destination leaf.
 number entries being moved 
		
		  leaf2 is the destination, compact it if it looks tight.
		
		  Move high entries from leaf1 to low end of leaf2.
		
		  I assert that since all callers pass in an empty
		  second buffer, this code should never execute.
		
		  Figure the total bytes to be added to the destination leaf.
 number entries being moved 
		
		  leaf1 is the destination, compact it if it looks tight.
		
		  Move low entries from leaf2 to high end of leaf1.
	
	  Copy out last hashval in each block for B-tree code.
	
	  Adjust the expected index for insertion.
	  NOTE: this code depends on the (current) situation that the
	  second block was originally empty.
	 
	  If the insertion point moved to the 2nd block, we must adjust
	  the index.  We must also track the entry just following the
	  new entry for use in an "atomic rename" operation, that entry
	  is always the "old" entry and the "new" entry is what we are
	  inserting.  The indexblkno fields refer to the "old" entry,
	  while the index2blkno2 fields refer to the "new" entry.
			
			  On a double leaf split, the original attr location
			  is already stored in blkno2index2, so don't
			  overwrite it overwise we corrupt the tree.
				
				  set the new attr location to match the old
				  one and let the higher level split code
				  decide where in the leaf to place it.
  Examine entries until we reduce the absolute difference in
  byte usage between the two blocks to a minimum.
  GROT: Is this really necessary?  With other than a 512 byte blocksize,
  GROT: there will always be enough room in either block for a new entry.
  GROT: Do a double-split for this case?
	
	  Examine entries until we reduce the absolute difference in
	  byte usage between the two blocks to a minimum.
		
		  The new entry is in the first block, account for it.
		
		  Wrap around into the second block if necessary.
		
		  Figure out if next leaf entry would be too much.
	
	  Calculate the number of usedbytes that will end up in lower block.
	  If new entry not in lower block, fix up the count.
========================================================================
  Routines used for shrinking the Btree.
  Check a leaf block and its neighbors to see if the block should be
  collapsed into one or the other neighbor.  Always keep the block
  with the smaller block number.
  If the current block is over 50% full, don't try to join it, return 0.
  If the block is empty, fill in the state structure and return 2.
  If it can be collapsed, fill in the state structure and return 1.
  If nothing can be done, return 0.
  GROT: allow for INCOMPLETE entries in calculation.
	
	  Check for the degenerate case of the block being over 50% full.
	  If so, it's not worth even looking to see if we might be able
	  to coalesce with a sibling.
 blk over 50%, don't try to join 
	
	  Check for the degenerate case of the block being empty.
	  If the block is empty, we'll simply delete it, no need to
	  coalesce it with a sibling block.  We choose (arbitrarily)
	  to merge with the forward block unless it is NULL.
		
		  Make altpath point to the block we want to keep and
		  path point to the block we want to drop (this one).
	
	  Examine each sibling block to see if we can coalesce with
	  at least 25% free space to spare.  We need to figure out
	  whether to merge with the forward or the backward block.
	  We prefer coalescing with the lower numbered sibling so as
	  to shrink an attribute list over time.
 start with smaller blk num 
 fits with at least 25% to spare 
	
	  Make altpath point to the block we want to keep (the lower
	  numbered block) and path point to the block we want to drop.
  Remove a name from the leaf attribute list structure.
  Return 1 if leaf is less than 37% full, 0 if >= 37% full.
  If two leaves are 37% full, when combined they will leave 25% free.
	
	  Scan through free region table:
	     check for adjacency of free'd entry with an existing one,
	     find smallest free region in case we need to replace it,
	     adjust any map that borders the entry table,
	
	  Coalesce adjacent freemap regions,
	  or replace the smallest region.
		
		  Replace smallest region (if it is smaller than free'd entry)
	
	  Did we remove the first entry?
	
	  Compress the remaining entries and zero out the removed stuff.
	
	  If we removed the first entry, re-find the first used byte
	  in the name area.  Note that if the entry was the "firstused",
	  then we don't have a "hole" in our block resulting from
	  removing the name.
 mark as needing compaction 
	
	  Check if leaf is less than 50% full, caller may want to
	  "join" the leaf with a sibling if so.
 leaf is < 37% full 
  Move all the attribute list entries from drop_leaf into save_leaf.
	
	  Save last hashval from dying block for later Btree fixup.
	
	  Check if we need a temp buffer, or can we do it in place.
	  Note that we don't check "leaf" for holes because we will
	  always be dropping it, toosmall() decided that for us already.
		
		  dest leaf has no holes, so we add there.  May need
		  to make some room in the entry array.
		
		  Destination has holes, so we make a temporary copy
		  of the leaf and add them both to that.
		
		  Copy the header into the temp leaf so that all the stuff
		  not in the incore header is present and gets copied back in
		  once we've moved all the entries.
 write the header to the temp buffer to initialise it 
 struct copy 
	
	  Copy out last hashval in each block for B-tree code.
========================================================================
  Routines used for finding things in the Btree.
  Look up a name in a leaf attribute list structure.
  This is the internal routine, it uses the caller's buffer.
  Note that duplicate keys are allowed, but only check within the
  current leaf node.  The Btree code must check in adjacent leaf nodes.
  Return in args->index the index into the entry[] array of either
  the found entry, or where the entry should have been (insert before
  that entry).
  Don't change the args->value unless we find the attribute.
	
	  Binary search.  (note: small blocks will skip this loop)
	
	  Since we may have duplicate hashval's, find the first matching
	  hashval in the leaf.
	
	  Duplicate keys may be present, so search all of them for a match.
  GROT: Add code to remove incomplete entries.
  Get the value associated with an attribute name from a leaf attribute
  list structure.
  If args->valuelen is zero, only the length needs to be returned.  Unlike a
  lookup, we only return an error if the attribute does not exist or we can't
  retrieve the value.
========================================================================
  Utility routines.
  Move the indicated entries from one leaf to another.
  NOTE: this routine modifies both source and destination leaves.
ARGSUSED
	
	  Check for nothing to do.
	
	  Set up environment.
	
	  Move the entries in the destination leaf up to make a hole?
	
	  Copy all entry's in the same (sorted) order,
	  but allocate attribute info packed and in sequence.
		
		  Code to drop INCOMPLETE entries.  Difficult to use as we
		  may also need to change the insertion index.  Code turned
		  off for 6.2, should be revisited later.
 skip partials? 
 to compensate for ++ in loop hdr 
 insertion index adjustment 
 GROT 
 both on-disk, don't endian flip twice 
 GROT 
	
	  Zero out the entries we just copied.
		
		  Move the remaining entries down to fill the hole,
		  then zero the entries at the top.
	
	  Fill in the freemap information
 leaf may not be compact 
  Pick up the last hashvalue from a leaf block.
  Calculate the number of bytes used to store the indicated attribute
  (whether local or remote only calculate bytes in this block).
  Calculate the number of bytes that would be required to store the new
  attribute (whether local or remote only calculate bytes in this block).
  This routine decides as a side effect whether the attribute will be
  a "local" or a "remote" attribute.
========================================================================
  Manage the INCOMPLETE flag in a leaf entry
  Clear the INCOMPLETE flag on an entry in a leaf block.
 DEBUG 
	
	  Set up the operation.
 DEBUG 
  Set the INCOMPLETE flag on an entry in a leaf block.
	
	  Set up the operation.
  In a single transaction, clear the INCOMPLETE flag on the leaf entry
  given by args->blknoindex and set the INCOMPLETE flag on the leaf
  entry given by args->blkno2index2.
  Note that they could be in different blocks, or in the same block.
 DEBUG 
	
	  Read the block containing the "old" attr
	
	  Read the block containing the "new" attr, if it is different
 DEBUG 
 SPDX-License-Identifier: GPL-2.0
  linuxfsbefsio.c
  Copyright (C) 2001 Will Dyson <will_dyson@pobox.com
  Based on portions of file.c and inode.c
  by Makoto Kato (m_kato@ga2.so-net.ne.jp)
  Many thanks to Dominic Giampaolo, author of Practical File System
  Design with the Be File System, for such a helpful book.
  Converts befs notion of disk addr to a disk offset and uses
  linux kernel function sb_bread() to get the buffer containing
  the offset.
  super.c
  Copyright (C) 2001-2002 Will Dyson <will_dyson@pobox.com>
  Licensed under the GNU GPL. See the file COPYING for details.
 for PAGE_SIZE 
  befs_load_sb -- Read from disk and properly byteswap all the fields
  of the befs superblock
 Check the byte order of the filesystem 
 Check magic headers of super block 
	
	  Check blocksize of BEFS.
	 
	  Blocksize of BEFS is 1024, 2048, 4096 or 8192.
	
	  block_shift and block_size encode the same information
	  in different ways as a consistency check.
	 ag_shift also encodes the same information as blocks_per_ag in a
	  different way, non-fatal consistency check
 SPDX-License-Identifier: GPL-2.0
  linuxfsbefsdatastream.c
  Copyright (C) 2001 Will Dyson <will_dyson@pobox.com>
  Based on portions of file.c by Makoto Kato <m_kato@ga2.so-net.ne.jp>
  Many thanks to Dominic Giampaolo, author of "Practical File System
  Design with the Be File System", for such a helpful book.
  befs_read_datastream - get buffer_head containing data, starting from pos.
  @sb: Filesystem superblock
  @ds: datastream to find data with
  @pos: start of data
  @off: offset of data in buffer_head->b_data
  Returns pointer to buffer_head containing data starting with offset @off,
  if you don't need to know offset just set @off = NULL.
 block coresponding to pos 
  befs_fblock2brun - give back block run for fblock
  @sb: the superblock
  @data: datastream to read from
  @fblock: the blocknumber with the file position to find
  @run: The found run is passed back through this pointer
  Takes a file position and gives back a brun who's starting block
  is block number fblock of the file.
  Returns BEFS_OK or BEFS_ERR.
  Calls specialized functions for each of the three possible
  datastream regions.
  befs_read_lsmylink - read long symlink from datastream.
  @sb: Filesystem superblock
  @ds: Datastream to read from
  @buff: Buffer in which to place long symlink data
  @len: Length of the long symlink in bytes
  Returns the number of bytes read
 bytes readed 
  befs_count_blocks - blocks used by a file
  @sb: Filesystem superblock
  @ds: Datastream of the file
  Counts the number of fs blocks that the file represented by
  inode occupies on the filesystem, counting both regular file
  data and filesystem metadata (and eventually attribute data
  when we support attributes)
 File data blocks 
 FS metadata blocks 
 Start with 1 block for inode 
 Size of indirect block 
	
	  Double indir block, plus all the indirect blocks it maps.
	  In the double-indirect range, all block runs of data are
	  BEFS_DBLINDIR_BRUN_LEN blocks long. Therefore, we know
	  how many data block runs are in the double-indirect region,
	  and from that we know how many indirect blocks it takes to
	  map them. We assume that the indirect blocks are also
	  BEFS_DBLINDIR_BRUN_LEN blocks long.
  befs_find_brun_direct - find a direct block run in the datastream
  @sb: the superblock
  @data: the datastream
  @blockno: the blocknumber to find
  @run: The found run is passed back through this pointer
  Finds the block run that starts at file block number blockno
  in the file represented by the datastream data, if that
  blockno is in the direct region of the datastream.
  Return value is BEFS_OK if the blockrun is found, BEFS_ERR
  otherwise.
  Algorithm:
  Linear search. Checks each element of array[] to see if it
  contains the blockno-th filesystem block. This is necessary
  because the block runs map variable amounts of data. Simply
  keeps a count of the number of blocks searched so far (sum),
  incrementing this by the length of each block run as we come
  across it. Adds sum to count before returning (this is so
  you can search multiple arrays that are logicaly one array,
  as in the indirect region code).
  Whenif blockno is found, if blockno is inside of a block
  run as stored on disk, we offset the start and length members
  of the block run, so that blockno is the start and len is
  still valid (the run ends in the same place).
  befs_find_brun_indirect - find a block run in the datastream
  @sb: the superblock
  @data: the datastream
  @blockno: the blocknumber to find
  @run: The found run is passed back through this pointer
  Finds the block run that starts at file block number blockno
  in the file represented by the datastream data, if that
  blockno is in the indirect region of the datastream.
  Return value is BEFS_OK if the blockrun is found, BEFS_ERR
  otherwise.
  Algorithm:
  For each block in the indirect run of the datastream, read
  it in and search through it for search_blk.
  XXX:
  Really should check to make sure blockno is inside indirect
  region.
 Examine blocks of the indirect run one at a time 
 Only fallthrough is an error 
  befs_find_brun_dblindirect - find a block run in the datastream
  @sb: the superblock
  @data: the datastream
  @blockno: the blocknumber to find
  @run: The found run is passed back through this pointer
  Finds the block run that starts at file block number blockno
  in the file represented by the datastream data, if that
  blockno is in the double-indirect region of the datastream.
  Return value is BEFS_OK if the blockrun is found, BEFS_ERR
  otherwise.
  Algorithm:
  The block runs in the double-indirect region are different.
  They are always allocated 4 fs blocks at a time, so each
  block run maps a constant amount of file data. This means
  that we can directly calculate how many block runs into the
  double-indirect region we need to go to get to the one that
  maps a particular filesystem block.
  We do this in two stages. First we calculate which of the
  inode addresses in the double-indirect block will point us
  to the indirect block that contains the mapping for the data,
  then we calculate which of the inode addresses in that
  indirect block maps the data block we are after.
  Oh, and once we've done that, we actually read in the blocks
  that contain the inode addresses we calculated above. Even
  though the double-indirect run may be several blocks long,
  we can calculate which of those blocks will contain the index
  we are after and only read that one. We then follow it to
  the indirect block and perform a similar process to find
  the actual block run that maps the data block we are interested
  in.
  Then we offset the run as in befs_find_brun_array() and we are
  done.
	 number of data blocks mapped by each of the iaddrs in
	  the indirect block pointed to by the double indirect block
	 number of data blocks mapped by each of the iaddrs in
	  the double indirect block
	 First, discover which of the double_indir->indir blocks
	  contains pos. Then figure out how much of pos that
	  accounted for. Then discover which of the iaddrs in
	  the indirect block contains pos.
 Read double indirect block 
 Read indirect block 
 SPDX-License-Identifier: GPL-2.0-only
  linuxfsbefslinuxvfs.c
  Copyright (C) 2001 Will Dyson <will_dyson@pobox.com
 The units the vfs expects inode->i_blocks to be in 
 allocate a new inode 
 deallocate an inode 
 uninit super 
 statfs 
 slab cache for befs_inode_info objects 
  Called by generic_file_read() to read a page of data
  In turn, simply calls a generic block read function and
  passes it the address of befs_get_block, for mapping file
  positions to disk blocks.
  Generic function to map a file position (block) to a
  disk offset (passed back in bh_result).
  Used by many higher level functions.
  Calls befs_fblock2brun() in datastream.c to do the real work.
 Convert to UTF-8 
 Convert to NLS 
 convert from vfs's inode number to befs's inode number 
	
	  set uid and gid.  But since current BeOS is single user OS, so
	  you can change by "uid" or "gid" options.
	
	  BEFS's time is 64 bits, but current VFS is 32 bits...
	  BEFS don't have access time. Nor inode change time. VFS
	  doesn't have creation time.
	  Also, the lower 16 bits of the last_modified_time and
	  create_time are just a counter to help ensure uniqueness
	  for indexing purposes. (PFD, page 54)
 lower 16 bits are not a time 
 Initialize the inode cache. Called at fs setup.
  Taken from NFS implementation by Al Viro.
 Called at fs teardown.
  Taken from NFS implementation by Al Viro.
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy cache.
  The inode of symbolic link is different to data stream.
  The data stream become link name. Unless the LONG_SYMLINK
  flag is set.
  UTF-8 to NLS charset convert routine
  Uses uni2char()  char2uni() rather than the nls tables directly
	 The utf8->nls conversion won't make the final nls string bigger
	  than the utf one, but if the string is pure ascii they'll have the
	  same width and an extra char is needed to save the additional \0
 convert from UTF-8 to Unicode 
 convert from Unicode to nls 
  befs_nls2utf - Convert NLS string to utf8 encodeing
  @sb: Superblock
  @in: Input string buffer in NLS format
  @in_len: Length of input string in bytes
  @out: The output string in UTF-8 format
  @out_len: Length of the output buffer
  Converts input string @in, which is in the format of the loaded NLS map,
  into a utf8 string.
  The destination string @out is allocated by this function and the caller is
  responsible for freeing it with kfree()
  On return, @out_len is the length of @out in bytes.
  On success, the return value is the number of utf8 characters written to
  the output buffer @out.
  On Failure, a negative number coresponding to the error code is returned.
	
	  There are nls characters that will translate to 3-chars-wide UTF-8
	  characters, an additional byte is needed to save the final \0
	  in special cases
 convert from nls to unicode 
 convert from unicode to UTF-8 
 No need to handle i_generation 
  Map a NFS file handle to a corresponding dentry
  Find the parent for a file specified by NFS handle
 Initialize options 
 This function has the responsibiltiy of getting the
  filesystem ready for unmounting.
  Basically, we free everything that we allocated in
  befs_read_inode
 Allocate private field of the superblock, fill it.
  Finish filling the public superblock fields
  Make the root directory
  Load a set of NLS translations if needed.
	
	  Set dummy blocksize to read super block.
	  Will be set to real fs blocksize later.
	 
	  Linux 2.4.10 and later refuse to read blocks smaller than
	  the logical block size for the device. But we also need to read at
	  least 1k to get the second 512 bytes of the volume.
 account for offset of super block on x86 
	
	  set up enough so that it can read an inode
	  Fill in kernel superblock fields from private sb
 Set real blocksize of fs 
 load nls library 
 load default nls if none is specified  in mount options 
 UNKNOWN 
 UNKNOWN 
  Macros that typecheck the init and exit functions,
  ensures that they are called at init and cleanup,
  and eliminates warnings about unused functions.
 SPDX-License-Identifier: GPL-2.0
  inode.c
  Copyright (C) 2001 Will Dyson <will_dyson@pobox.com>
  Validates the correctness of the befs inode
  Returns BEFS_OK if the inode should be used, otherwise
  returns BEFS_BAD_INODE
 check magic header. 
	
	  Sanity check2: inodes store their own block address. Check it.
	
	  check flag
 SPDX-License-Identifier: GPL-2.0
   linuxfsbefsdebug.c
  Copyright (C) 2001 Will Dyson (will_dyson at pobox.com)
  With help from the ntfs-tng driver by Anton Altparmakov
  Copyright (C) 1999  Makoto Kato (m_kato@ga2.so-net.ne.jp)
  debug functions
 __KERNEL__ 
CONFIG_BEFS_DEBUG
CONFIG_BEFS_DEBUG
  Display super block structure for debug.
CONFIG_BEFS_DEBUG
 unused 
 unused 
CONFIG_BEFS_DEBUG
  0  
CONFIG_BEFS_DEBUG
CONFIG_BEFS_DEBUG
  linuxfsbefsbtree.c
  Copyright (C) 2001-2002 Will Dyson <will_dyson@pobox.com>
  Licensed under the GNU GPL. See the file COPYING for details.
  2002-02-05: Sergey S. Kostyliov added binary search within
  		btree nodes.
  Many thanks to:
  Dominic Giampaolo, author of "Practical File System
  Design with the Be File System", for such a helpful book.
  Marcus J. Ranum, author of the b+tree package in
  comp.sources.misc volume 10. This code is not copied from that
  work, but it is partially based on it.
  Makoto Kato, author of the original BeFS for linux filesystem
  driver.
  The btree functions in this file are built on top of the
  datastream.c interface, which is in turn built on top of the
  io.c interface.
 Befs B+tree structure:
  The first thing in the tree is the tree superblock. It tells you
  all kinds of useful things about the tree, like where the rootnode
  is located, and the size of the nodes (always 1024 with current version
  of BeOS).
  The rest of the tree consists of a series of nodes. Nodes contain a header
  (struct befs_btree_nodehead), the packed key data, an array of shorts
  containing the ending offsets for each of the keys, and an array of
  befs_off_t values. In interior nodes, the keys are the ending keys for
  the childnode they point to, and the values are offsets into the
  datastream containing the tree.
 Note:
  The book states 2 confusing things about befs b+trees. First,
  it states that the overflow field of node headers is used by internal nodes
  to point to another node that "effectively continues this one". Here is what
  I believe that means. Each key in internal nodes points to another node that
  contains key values less than itself. Inspection reveals that the last key
  in the internal node is not the last key in the index. Keys that are
  greater than the last key in the internal node go into the overflow node.
  I imagine there is a performance reason for this.
  Second, it states that the header of a btree node is sufficient to
  distinguish internal nodes from leaf nodes. Without saying exactly how.
  After figuring out the first, it becomes obvious that internal nodes have
  overflow nodes and leafnodes do not.
  Currently, this code is only good for directory B+trees.
  In order to be used for other BFS indexes, it needs to be extended to handle
  duplicate keys and non-string keytypes (int32, int64, float, double).
  In memory structure of each btree node
 head of node converted to cpu byteorder 
 on disk node 
 local constants 
 local functions 
  befs_bt_read_super() - read in btree superblock convert to cpu byteorder
  @sb:        Filesystem superblock
  @ds:        Datastream to read from
  @sup:       Buffer in which to place the btree superblock
  Calls befs_read_datastream to read in the btree superblock and
  makes sure it is in cpu byteorder, byteswapping if necessary.
  Return: BEFS_OK on success and if @sup contains the btree superblock in cpu
  byte order. Otherwise return BEFS_ERR on error.
  befs_bt_read_node - read in btree node and convert to cpu byteorder
  @sb: Filesystem superblock
  @ds: Datastream to read from
  @node: Buffer in which to place the btree node
  @node_off: Starting offset (in bytes) of the node in @ds
  Calls befs_read_datastream to read in the indicated btree node and
  makes sure its header fields are in cpu byteorder, byteswapping if
  necessary.
  Note: node->bh must be NULL when this function is called the first time.
  Don't forget brelse(node->bh) after last call.
  On success, returns BEFS_OK and @node contains the btree node that
  starts at @node_off, with the node->head fields in cpu byte order.
  On failure, BEFS_ERR is returned.
  befs_btree_find - Find a key in a befs B+tree
  @sb: Filesystem superblock
  @ds: Datastream containing btree
  @key: Key string to lookup in btree
  @value: Value stored with @key
  On success, returns BEFS_OK and sets @value to the value stored
  with @key (usually the disk block number of an inode).
  On failure, returns BEFS_ERR or BEFS_BT_NOT_FOUND.
  Algorithm:
    Read the superblock and rootnode of the b+tree.
    Drill down through the interior nodes using befs_find_key().
    Once at the correct leaf node, use befs_find_key() again to get the
    actual value stored with the key.
 read in root node 
 if no key set, try the overflow node 
 at a leaf node now, check if it is correct 
  befs_find_key - Search for a key within a node
  @sb: Filesystem superblock
  @node: Node to find the key within
  @findkey: Keystring to search for
  @value: If key is found, the value stored with the key is put here
  Finds exact match if one exists, and returns BEFS_BT_MATCH.
  If there is no match and node's value array is too small for key, return
  BEFS_BT_OVERFLOW.
  If no match and node should countain this key, return BEFS_BT_NOT_FOUND.
  Uses binary search instead of a linear.
 if node can not contain key, just skip this node 
 simple binary search 
 return an existing value so caller can arrive to a leaf node 
  befs_btree_read - Traverse leafnodes of a btree
  @sb: Filesystem superblock
  @ds: Datastream containing btree
  @key_no: Key number (alphabetical order) of key to read
  @bufsize: Size of the buffer to return key in
  @keybuf: Pointer to a buffer to put the key in
  @keysize: Length of the returned key
  @value: Value stored with the returned key
  Here's how it works: Key_no is the index of the keyvalue pair to
  return in keybufvalue.
  Bufsize is the size of keybuf (BEFS_NAME_LEN+1 is a good size). Keysize is
  the number of characters in the key (just a convenience).
  Algorithm:
    Get the first leafnode of the tree. See if the requested key is in that
    node. If not, follow the node->right link to the next leafnode. Repeat
    until the (key_no)th key is found or the tree is out of keys.
 seeks down to first leafnode, reads it into this_node 
 find the leaf node containing the key_no key 
 no more nodes to look in: key_no is too large 
 how many keys into this_node is key_no 
 get pointers to datastructures within the node body 
  befs_btree_seekleaf - Find the first leafnode in the btree
  @sb: Filesystem superblock
  @ds: Datastream containing btree
  @bt_super: Pointer to the superblock of the btree
  @this_node: Buffer to return the leafnode in
  @node_off: Pointer to offset of current node within datastream. Modified
  		by the function.
  Helper function for btree traverse. Moves the current position to the
  start of the first leaf node.
  Also checks for an empty tree. If there are no keys, returns BEFS_BT_EMPTY.
  befs_leafnode - Determine if the btree node is a leaf node or an
  interior node
  @node: Pointer to node structure to test
  Return 1 if leaf, 0 if interior
 all interior nodes (and only interior nodes) have an overflow node 
  befs_bt_keylen_index - Finds start of keylen index in a node
  @node: Pointer to the node structure to find the keylen index within
  Returns a pointer to the start of the key length index array
  of the B+tree node @node
  "The length of all the keys in the node is added to the size of the
  header and then rounded up to a multiple of four to get the beginning
  of the key length index" (p.88, practical filesystem design).
  Except that rounding up to 8 works, and rounding up to 4 doesn't.
  befs_bt_valarray - Finds the start of value array in a node
  @node: Pointer to the node structure to find the value array within
  Returns a pointer to the start of the value array
  of the node pointed to by the node header
  befs_bt_keydata - Finds start of keydata array in a node
  @node: Pointer to the node structure to find the keydata array within
  Returns a pointer to the start of the keydata array
  of the node pointed to by the node header
  befs_bt_get_key - returns a pointer to the start of a key
  @sb: filesystem superblock
  @node: node in which to look for the key
  @index: the index of the key to get
  @keylen: modified to be the length of the key at @index
  Returns a valid pointer into @node on success.
  Returns NULL on failure (bad input) and sets @keylen = 0
  befs_compare_strings - compare two strings
  @key1: pointer to the first key to be compared
  @keylen1: length in bytes of key1
  @key2: pointer to the second key to be compared
  @keylen2: length in bytes of key2
  Returns 0 if @key1 and @key2 are equal.
  Returns >0 if @key1 is greater.
  Returns <0 if @key2 is greater.
 These will be used for non-string keyed btrees 
0
 SPDX-License-Identifier: GPL-2.0
  Simple file system for zoned block devices exposing zones as files.
  Copyright (C) 2019 Western Digital Corporation or its affiliates.
	
	  A full zone is no longer openactive and does not need
	  explicit closing.
 All IOs should always be within the file maximum size 
	
	  Sequential zones can only accept direct writes. This is already
	  checked when writes are issued, so warn if we see a page writeback
	  operation.
	
	  For conventional zones, all blocks are always mapped. For sequential
	  zones, all blocks after always mapped below the inode size (zone
	  write pointer) and unwriten beyond.
  Map blocks for page writeback. This is used only on conventional zone files,
  which implies that the page range can only be within the fixed inode size.
 If the mapping is already OK, nothing needs to be done 
	
	  This may be called for an update after an IO error.
	  So beware of the values seen.
  Check a zone condition and adjust its file inode access permissions for
  offline and readonly zones. Return the inode size corresponding to the
  amount of readable data in the zone.
		
		  Dead zone: make the inode immutable, disable all accesses
		  and set the file size to 0 (zone wp set to zone start).
		
		  The write pointer of read-only zones is invalid. If such a
		  zone is found during mount, the file size cannot be retrieved
		  so we treat the zone as offline (mount == true case).
		  Otherwise, keep the file size as it was when last updated
		  so that the user can recover data. In both cases, writes are
		  always disabled for the zone.
 The write pointer of full zones is invalid. 
	
	  Check the zone condition: if the zone is not "bad" (offline or
	  read-only), read errors are simply signaled to the IO issuer as long
	  as there is no inconsistency between the inode size and the amount of
	  data writen in the zone (data_size).
	
	  At this point, we detected either a bad zone or an inconsistency
	  between the inode size and the amount of data written in the zone.
	  For the latter case, the cause may be a write IO error or an external
	  action on the device. Two error patterns exist:
	  1) The inode size is lower than the amount of data in the zone:
	     a write operation partially failed and data was writen at the end
	     of the file. This can happen in the case of a large direct IO
	     needing several BIOs andor write requests to be processed.
	  2) The inode size is larger than the amount of data in the zone:
	     this can happen with a deferred write error with the use of the
	     device side write cache after getting successful write IO
	     completions. Other possibilities are (a) an external corruption,
	     e.g. an application reset the zone directly, or (b) the device
	     has a serious problem (e.g. firmware bug).
	 
	  In all cases, warn about inode size inconsistency and handle the
	  IO error according to the zone condition and to the mount options.
	
	  First handle bad zones signaled by hardware. The mount options
	  errors=zone-ro and errors=zone-offline result in changing the
	  zone condition to read-only and offline respectively, as if the
	  condition was signaled by the hardware.
	
	  If the filesystem is mounted with the explicit-open mount option, we
	  need to clear the ZONEFS_ZONE_OPEN flag if the zone transitioned to
	  the read-only or offline condition, to avoid attempting an explicit
	  close of the zone when the inode file is closed.
	
	  If error=remount-ro was specified, any error result in remounting
	  the volume as read-only.
	
	  Update block usage stats and the inode size  to prevent access to
	  invalid data.
  When an file IO error occurs, check the file zone to see if there is a change
  in the zone condition (e.g. offline or read-only). For a failed write to a
  sequential zone, the zone write pointer position must also be checked to
  eventually correct the file size and zonefs inode write pointer offset
  (which can be out of sync with the drive due to partial write failures).
	
	  Memory allocations in blkdev_report_zones() can trigger a memory
	  reclaim which may in turn cause a recursion into zonefs as well as
	  struct request allocations for the same device. The former case may
	  end up in a deadlock on the inode truncate mutex, while the latter
	  may prevent IO forward progress. Executing the report zones under
	  the GFP_NOIO context avoids both problems.
	
	  Only sequential zone files can be truncated and truncation is allowed
	  only down to a 0 size, which is equivalent to a zone reset, and to
	  the maximum file size, which is equivalent to a zone finish.
 Serialize against page faults 
 Serialize against zonefs_iomap_begin() 
	
	  If the mount option ZONEFS_MNTOPT_EXPLICIT_OPEN is set,
	  take care of open zones.
		
		  Truncating a zone to EMPTY or FULL is the equivalent of
		  closing the zone. For a truncation to 0, we need to
		  re-open the zone to ensure new writes can be processed.
		  For a truncation to the maximum file size, the zone is
		  closed and writes cannot be accepted anymore, so clear
		  the open flag.
	
	  Since files and directories cannot be created nor deleted, do not
	  allow setting any write attributes on the sub-directories grouping
	  files by zone type.
	
	  Since only direct writes are allowed in sequential files, page cache
	  flush is needed only for conventional zone files.
	
	  Sanity check: only conventional zone files can have shared
	  writeable mappings.
 Serialize against truncates 
	
	  Conventional zones accept random writes, so their files can support
	  shared writable mappings. For sequential zone files, only read
	  mappings are possible since there are no guarantees for write
	  ordering between msync() and page cache writeback.
	
	  Seeks are limited to below the zone size for conventional zones
	  and below the zone write pointer for sequential zones. In both
	  cases, this limit is the inode size.
		
		  Note that we may be seeing completions out of order,
		  but that is not a problem since a write completed
		  successfully necessarily means that all preceding writes
		  were also successful. So we can safely increase the inode
		  size to the write end location.
  Do not exceed the LFS limits nor the file zone size. If pos is under the
  limit it becomes a short access. If it exceeds the limit, return -EFBIG.
  Handle direct writes. For sequential zone files, this is the only possible
  write path. For these files, check that the user is issuing writes
  sequentially from the end of the file. This code assumes that the block layer
  delivers write requests to the device in sequential order. This is always the
  case if a block IO scheduler implementing the ELEVATOR_F_ZBD_SEQ_WRITE
  elevator feature is being used (e.g. mq-deadline). The block layer always
  automatically select such an elevator for zoned block devices during the
  device initialization.
	
	  For async direct IOs to sequential zone files, refuse IOCB_NOWAIT
	  as this can cause write reordering (e.g. the first aio gets EAGAIN
	  on the inode lock but the second goes through but is now unaligned).
 Enforce sequential writes (append only) in sequential zones 
	
	  Direct IO writes are mandatory for sequential zone files so that the
	  write IO issuing order is preserved.
 Write operations beyond the zone size are not allowed 
 Offline zones cannot be read 
 Limit read operations to written data 
		
		  If the file zone is full, it is not open anymore and we only
		  need to decrement the open count.
			
			  Leaving zones explicitly open may lead to a state
			  where most zones cannot be written (zone resources
			  exhausted). So take preventive action by remounting
			  read-only.
	
	  If we explicitly open a zone we must close it again as well, but the
	  zone management operation can fail (either due to an IO error or as
	  the zone has gone offline or read-only). Make sure we don't fail the
	  close(2) for user-space.
  File system stat.
  Create a zone group and populate it with zone files.
 If the group is empty, there is nothing to do 
	
	  The first zone contains the super block: skip it.
		
		  For conventional zones, contiguous zones can be aggregated
		  together to form larger files. Note that this overwrites the
		  length of the first zone of the set of contiguous zones
		  aggregated together. If one offline or read-only zone is
		  found, assume that all zones aggregated have the same
		  condition.
		
		  Use the file number within its group as file name.
	
	  Count the number of usable zones: the first zone at index 0 contains
	  the super block and is ignored.
 Get zones information from the device 
  Read super block information from the device.
  Check that the device is zoned. If it is, get the list of zones and create
  sub-directories and files according to the device zone configuration and
  format options.
	
	  Initialize super block information: the maximum file size is updated
	  when the zone files are created so that the format option
	  ZONEFS_F_AGGRCNV which increases the maximum file size of a file
	  beyond the zone size is taken into account.
	
	  The block size is set to the device zone write granularity to ensure
	  that write operations are always aligned according to the device
	  interface constraints.
 Create root directory inode 
 Create and populate files in zone groups directories 
  File system definition and registration.
	
	  Make sure all delayed rcu free inodes are flushed before we
	  destroy the inode cache.
  Copyright (c) 2014 SGI.
  All rights reserved.
  This program is free software; you can redistribute it andor
  modify it under the terms of the GNU General Public License as
  published by the Free Software Foundation.
  This program is distributed in the hope that it would be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.
  You should have received a copy of the GNU General Public License
  along with this program; if not, write the Free Software Foundation,
  Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
 Generator for a compact trie for unicode normalization 
 Default names of the in- and output files. 
 An arbitrary line size limit on input lines. 
 ------------------------------------------------------------------ 
  Unicode version numbers consist of three parts: major, minor, and a
  revision.  These numbers are packed into an unsigned int to obtain
  a single version number.
  To save space in the generated trie, the unicode version is not
  stored directly, instead we calculate a generation number from the
  unicode versions seen in the DerivedAge file, and use that as an
  index into a table of unicode versions.
 ------------------------------------------------------------------ 
  utf8trie_t
  A compact binary tree, used to decode UTF-8 characters.
  Internal nodes are one byte for the node itself, and up to three
  bytes for an offset into the tree.  The first byte contains the
  following information:
   NEXTBYTE  - flag        - advance to next byte if set
   BITNUM    - 3 bit field - the bit number to tested
   OFFLEN    - 2 bit field - number of bytes in the offset
  if offlen == 0 (non-branching node)
   RIGHTPATH - 1 bit field - set if the following node is for the
                             right-hand path (tested bit is set)
   TRIENODE  - 1 bit field - set if the following node is an internal
                             node, otherwise it is a leaf node
  if offlen != 0 (branching node)
   LEFTNODE  - 1 bit field - set if the left-hand node is internal
   RIGHTNODE - 1 bit field - set if the right-hand node is internal
  Due to the way utf8 works, there cannot be branching nodes with
  NEXTBYTE set, and moreover those nodes always have a righthand
  descendant.
  utf8leaf_t
  The leaves of the trie are embedded in the trie, and so the same
  underlying datatype, unsigned char.
  leaf[0]: The unicode version, stored as a generation number that is
           an index into utf8agetab[].  With this we can filter code
           points based on the unicode version in which they were
           defined.  The CCC of a non-defined code point is 0.
  leaf[1]: Canonical Combining Class. During normalization, we need
           to do a stable sort into ascending order of all characters
           with a non-zero CCC that occur between two characters with
           a CCC of 0, or at the begin or end of a string.
           The unicode standard guarantees that all CCC values are
           between 0 and 254 inclusive, which leaves 255 available as
           a special value.
           Code points with CCC 0 are known as stoppers.
  leaf[2]: Decomposition. If leaf[1] == 255, then leaf[2] is the
           start of a NUL-terminated string that is the decomposition
           of the character.
           The CCC of a decomposable character is the same as the CCC
           of the first character of its decomposition.
           Some characters decompose as the empty string: these are
           characters with the Default_Ignorable_Code_Point property.
           These do affect normalization, as they all have CCC 0.
  The decompositions in the trie have been fully expanded.
  Casefolding, if applicable, is also done using decompositions.
 ------------------------------------------------------------------ 
  UTF8 valid ranges.
  The UTF-8 encoding spreads the bits of a 32bit word over several
  bytes. This table gives the ranges that can be held and how they'd
  be represented.
  0x00000000 0x0000007F: 0xxxxxxx
  0x00000000 0x000007FF: 110xxxxx 10xxxxxx
  0x00000000 0x0000FFFF: 1110xxxx 10xxxxxx 10xxxxxx
  0x00000000 0x001FFFFF: 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
  0x00000000 0x03FFFFFF: 111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx
  0x00000000 0x7FFFFFFF: 1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx
  There is an additional requirement on UTF-8, in that only the
  shortest representation of a 32bit value is to be used.  A decoder
  must not decode sequences that do not satisfy this requirement.
  Thus the allowed ranges have a lower bound.
  0x00000000 0x0000007F: 0xxxxxxx
  0x00000080 0x000007FF: 110xxxxx 10xxxxxx
  0x00000800 0x0000FFFF: 1110xxxx 10xxxxxx 10xxxxxx
  0x00010000 0x001FFFFF: 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
  0x00200000 0x03FFFFFF: 111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx
  0x04000000 0x7FFFFFFF: 1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx
  Actual unicode characters are limited to the range 0x0 - 0x10FFFF,
  17 planes of 65536 values.  This limits the sequences actually seen
  even more, to just the following.
           0 -     0x7f: 0                     0x7f
        0x80 -    0x7ff: 0xc2 0x80             0xdf 0xbf
       0x800 -   0xffff: 0xe0 0xa0 0x80        0xef 0xbf 0xbf
     0x10000 - 0x10ffff: 0xf0 0x90 0x80 0x80   0xf4 0x8f 0xbf 0xbf
  Even within those ranges not all values are allowed: the surrogates
  0xd800 - 0xdfff should never be seen.
  Note that the longest sequence seen with valid usage is 4 bytes,
  the same a single UTF-32 character.  This makes the UTF-8
  representation of Unicode strictly smaller than UTF-32.
  The shortest sequence requirement was introduced by:
     Corrigendum #1: UTF-8 Shortest Form
  It can be found here:
     http:www.unicode.orgversionscorrigendum1.html
  Example lookup function for a tree.
 Right leg 
 Left leg 
  A simple non-recursive tree walker: keep track of visits to the
  left and right branches in the leftmask and rightmask.
  Allocate an initialize a new internal node.
  Insert a new leaf into the tree, and collapse any subtrees that are
  fully populated and end in identical leaves. A nextbyte tagged
  internal node will not be removed to preserve the tree's integrity.
  Note that due to the structure of utf8, no nextbyte tagged node
  will be a candidate for removal.
 Insert, creating path along the way. 
 Merge subtrees if possible. 
 Compare 
 Keep left, drop right leaf. 
 Check in parent 
 root of tree! 
 internal tree error 
 Propagate keymasks up along singleton chains. 
 Nix the mask for parents with two children. 
  Prune internal nodes.
  Fully populated subtrees that end at the same leaf have already
  been collapsed.  There are still internal nodes that have for both
  their left and right branches a sequence of singletons that make
  identical choices and end in identical leaves.  The keymask and
  keybits collected in the nodes describe the choices made in these
  singleton chains.  When they are identical for the left and right
  branch of a node, and the two leaves comare identical, the node in
  question can be removed.
  Note that nodes with the nextbyte tag set will not be removed by
  this to ensure tree integrity.  Note as well that the structure of
  utf8 ensures that these nodes would not have been candidates for
  removal in any case.
		
		  This node has identical singleton-only subtrees.
		  Remove it.
 Propagate keymasks up along singleton chains. 
 Force re-check 
 Force re-check 
  Mark the nodes in the tree that lead to leaves that must be
  emitted.
 second pass: left siblings and singletons 
  Compute the index of each node and leaf, which is the offset in the
  emitted trie.  These values must be pre-computed because relative
  offsets between nodes are used to navigate the tree.
 Align to a cache line (or half a cache line?). 
 Round up to a multiple of 16 
  Mark the nodes in a subtree, helper for size_nodes().
  Compute the size of nodes and leaves. We start by assuming that
  each node needs to store a three-byte offset. The indexes of the
  nodes are calculated based on that, and then this function is
  called to see if the sizes of some nodes can be reduced.  This is
  repeated until no more changes are seen.
				
				  If the right node is not marked,
				  look for a corresponding node in
				  the next tree.  Such a node need
				  not exist.
 Make sure the right node is marked. 
 offset <= 0xffffff 
  Emit a trie for the given tree into the data array.
 ------------------------------------------------------------------ 
  Unicode data.
  We need to keep track of the Canonical Combining Class, the Age,
  and decompositions for a code point.
  For the Age, we store the index into the ages table.  Effectively
  this is a generation number that the table maps to a unicode
  version.
  The correction field is used to indicate that this entry is in the
  corrections array, which contains decompositions that were
  corrected in later revisions.  The value of the correction field is
  the Unicode version in which the mapping was corrected.
  Check the corrections array to see if this entry was corrected at
  some point.
 Count the number of different ages. 
 Two trees per age: nfdi and nfdicf 
 Assign ages to the trees. 
 The ages assigned above are off by one. 
 Set up the forwarding between trees. 
 Assign the callouts. 
 Finish init. 
 ------------------------------------------------------------------ 
www.unicode.orgPublicUCDlatestucd\n");
 ------------------------------------------------------------------ 
 ------------------------------------------------------------------ 
 We must have found something above. 
 There is a 0 entry. 
 And a guard entry. 
 Nix surrogate block 
 Magic - guaranteed not to be exceeded. 
 skip over <tag> 
 decode the decomposition into UTF-32 
 Magic - guaranteed not to be exceeded. 
 Use the C+F casefold. 
 Magic - guaranteed not to be exceeded. 
 ------------------------------------------------------------------ 
  Hangul decomposition (algorithm from Section 3.12 of Unicode 6.3.0)
  AC00;<Hangul Syllable, First>;Lo;0;L;;;;;N;;;;;
  D7A3;<Hangul Syllable, Last>;Lo;0;L;;;;;N;;;;;
  SBase = 0xAC00
  LBase = 0x1100
  VBase = 0x1161
  TBase = 0x11A7
  LCount = 19
  VCount = 21
  TCount = 28
  NCount = 588 (VCount  TCount)
  SCount = 11172 (LCount  NCount)
  Decomposition:
    SIndex = s - SBase
  LV (CanonicalFull)
    LIndex = SIndex  NCount
    VIndex = (Sindex % NCount)  TCount
    LPart = LBase + LIndex
    VPart = VBase + VIndex
  LVT (Canonical)
    LVIndex = (SIndex  TCount)  TCount
    TIndex = (Sindex % TCount)
    LVPart = SBase + LVIndex
    TPart = TBase + TIndex
  LVT (Full)
    LIndex = SIndex  NCount
    VIndex = (Sindex % NCount)  TCount
    TIndex = (Sindex % TCount)
    LPart = LBase + LIndex
    VPart = VBase + VIndex
    if (TIndex == 0) {
           d = <LPart, VPart>
    } else {
           TPart = TBase + TIndex
           d = <LPart, VPart, TPart>
    }
 unsigned int lc = 19; 
 unsigned int sc = (lc  nc); 
 Hangul 
		
		  Add a cookie as a reminder that the hangul syllable
		  decompositions must not be stored in the generated
		  trie.
 Magic - guaranteed not to be exceeded. 
 Add this decomposition to nfdicf if there is no entry. 
 Magic - guaranteed not to be exceeded. 
 ------------------------------------------------------------------ 
  Hangul decomposition (algorithm from Section 3.12 of Unicode 6.3.0)
  AC00;<Hangul Syllable, First>;Lo;0;L;;;;;N;;;;;
  D7A3;<Hangul Syllable, Last>;Lo;0;L;;;;;N;;;;;
  SBase = 0xAC00
  LBase = 0x1100
  VBase = 0x1161
  TBase = 0x11A7
  LCount = 19
  VCount = 21
  TCount = 28
  NCount = 588 (VCount  TCount)
  SCount = 11172 (LCount  NCount)
  Decomposition:
    SIndex = s - SBase
  LV (CanonicalFull)
    LIndex = SIndex  NCount
    VIndex = (Sindex % NCount)  TCount
    LPart = LBase + LIndex
    VPart = VBase + VIndex
  LVT (Canonical)
    LVIndex = (SIndex  TCount)  TCount
    TIndex = (Sindex % TCount)
    LVPart = SBase + LVIndex
    TPart = TBase + TIndex
  LVT (Full)
    LIndex = SIndex  NCount
    VIndex = (Sindex % NCount)  TCount
    TIndex = (Sindex % TCount)
    LPart = LBase + LIndex
    VPart = VBase + VIndex
    if (TIndex == 0) {
           d = <LPart, VPart>
    } else {
           TPart = TBase + TIndex
           d = <LPart, VPart, TPart>
    }
 Constants 
 Algorithmic decomposition of hangul syllable. 
 Calculate the SI, LI, VI, and TI values. 
 Fill in base of leaf. 
 Add LPart, a 3-byte UTF-8 sequence. 
 Add VPart, a 3-byte UTF-8 sequence. 
 Add TPart if required, also a 3-byte UTF-8 sequence. 
 Terminate string. 
  Use trie to scan s, touching at most len bytes.
  Returns the leaf if one exists, NULL otherwise.
  A non-NULL return guarantees that the UTF-8 sequence starting at s
  is well-formed and corresponds to a known unicode code point.  The
  shorthand for this will be "is valid UTF-8 unicode".
 Right leg 
 Right node at offset of trie 
 Right node after this node 
 No right node. 
 Left leg 
 Left node after this node. 
 No left node. 
 Left node after this node 
	
	  Hangul decomposition is done algorithmically. These are the
	  codepoints >= 0xAC00 and <= 0xD7A3. Their UTF-8 encoding is
	  always 3 bytes long, so s has been advanced twice, and the
	  start of the sequence is at s-2.
  Use trie to scan s.
  Returns the leaf if one exists, NULL otherwise.
  Forwards to trie_nlookup().
  Return the number of bytes used by the current UTF-8 sequence.
  Assumes the input points to the first byte of a valid UTF-8
  sequence.
  Maximum age of any character in s.
  Return -1 if s is not valid UTF-8 unicode.
  Return 0 if only non-assigned code points are used.
  Minimum age of any character in s.
  Return -1 if s is not valid UTF-8 unicode.
  Return 0 if non-assigned code points are used.
  Maximum age of any character in s, touch at most len bytes.
  Return -1 if s is not valid UTF-8 unicode.
  Maximum age of any character in s, touch at most len bytes.
  Return -1 if s is not valid UTF-8 unicode.
  Length of the normalization of s.
  Return -1 if s is not valid UTF-8 unicode.
  A string of Default_Ignorable_Code_Point has length 0.
  Length of the normalization of s, touch at most len bytes.
  Return -1 if s is not valid UTF-8 unicode.
  Cursor structure used by the normalizer.
  Set up an utf8cursor for use by utf8byte().
    s      : string.
    len    : length of s.
    u8c    : pointer to cursor.
    trie   : utf8trie_t to use for normalization.
  Returns -1 on error, 0 on success.
 Check we didn't clobber the maximum length. 
 The first byte of s may not be an utf8 continuation. 
  Set up an utf8cursor for use by utf8byte().
    s      : NUL-terminated string.
    u8c    : pointer to cursor.
    trie   : utf8trie_t to use for normalization.
  Returns -1 on error, 0 on success.
  Get one byte from the normalized form of the string described by u8c.
  Returns the byte cast to an unsigned char on succes, and -1 on failure.
  The cursor keeps track of the location in the string in u8c->s.
  When a character is decomposed, the current location is stored in
  u8c->p, and u8c->s is set to the start of the decomposition. Note
  that bytes from a decomposition do not count against u8c->len.
  Characters are emitted if they match the current CCC in u8c->ccc.
  Hitting end-of-string while u8c->ccc == STOPPER means we're done,
  and the function returns 0 in that case.
  Sorting by CCC is done by repeatedly scanning the string.  The
  values of u8c->s and u8c->p are stored in u8c->ss and u8c->sp at
  the start of the scan.  The first pass finds the lowest CCC to be
  emitted and stores it in u8c->nccc, the second pass emits the
  characters with this CCC and finds the next lowest CCC. This limits
  the number of passes to 1 + the number of different CCCs in the
  sequence being scanned.
  Therefore:
   u8c->p  != NULL -> a decomposition is being scanned.
   u8c->ss != NULL -> this is a repeating scan.
   u8c->ccc == -1  -> this is the first scan of a repeating scan.
 Check for the end of a decomposed character. 
 Check for end-of-string. 
 There is no next byte. 
 End-of-string during a scan counts as a stopper. 
 This is a continuation of the current character. 
 Look up the data for the current character. 
 No leaf found implies that the input is a binary blob. 
 Characters that are too new have CCC 0. 
 Empty decomposition implies CCC 0. 
		
		  If this is not a stopper, then see if it updates
		  the next canonical class to be emitted.
		
		  Return the current byte if this is the current
		  combining class.
 Current combining class mismatch. 
			
			  Scan forward for the first canonical class
			  to be emitted.  Save the position from
			  which to restart.
 Not a stopper, and not the ccc we're emitting. 
 At a stopper, restart for next ccc. 
 All done, proceed from here. 
 ------------------------------------------------------------------ 
 First test: null-terminated string. 
 Second test: length-limited string. 
 Replace NUL with a value that will cause an error if seen. 
 Step one, read data from file. 
 ------------------------------------------------------------------ 
 This file is generated code, do not edit. \n");
 %s_%x \n",
 ------------------------------------------------------------------ 
 Prevent "unused function" warning. 
 SPDX-License-Identifier: GPL-2.0 
 String cf is expected to be a valid UTF-8 casefolded
  string.
 SPDX-License-Identifier: GPL-2.0-only
  Kernel module for testing utf-8 support.
  Copyright 2017 Collabora Ltd.
 Tests will be based on this version. 
 UTF-8 strings in this vector _must_ be NULL-terminated. 
 Trivial sequence 
 "ABba" decomposes to itself 
 Simple equivalent sequences 
                'VULGAR FRACTION ONE QUARTER' cannot decompose to
                  'NUMBER 1' + 'FRACTION SLASH' + 'NUMBER 4' on
		 'LATIN SMALL LETTER A WITH DIAERESIS' decomposes to
		 'LATIN SMALL LETTER LJ' can't decompose to
 GREEK ANO TELEIA decomposes to MIDDLE DOT 
 Canonical ordering 
		 A + 'COMBINING ACUTE ACCENT' + 'COMBINING OGONEK' decomposes
		 'LATIN SMALL LETTER A WITH DIAERESIS' + 'COMBINING OGONEK'
		   decomposes to
 UTF-8 strings in this vector _must_ be NULL-terminated. 
 Trivial sequences 
 "ABba" folds to lowercase 
 All ASCII folds to lower-case 
		 LATIN SMALL LETTER SHARP S folds to
		 LATIN CAPITAL LETTER A WITH RING ABOVE folds to
 Introduced by UTF-8.0.0. 
	 Cherokee letters are interesting test-cases because they fold
	   to upper-case.  Before 8.0.0, Cherokee lowercase were
	   undefined, thus, the folding from LC is not stable between
 CHEROKEE SMALL LETTER A folds to CHEROKEE LETTER A 
 CHEROKEE SMALL LETTER YE folds to CHEROKEE LETTER YE 
		 OLD HUNGARIAN CAPITAL LETTER AMB folds to
 Introduced by UTF-9.0.0. 
		 OSAGE CAPITAL LETTER CHA folds to
		 LATIN CAPITAL LETTER SMALL CAPITAL I folds to
 Introduced by UTF-11.0.0. 
		 GEORGIAN SMALL LETTER AN folds to GEORGIAN MTAVRULI
 Unicode 7.0.0 should be supported. 
 Unicode 9.0.0 should be supported. 
 Unicode 1x.0.0 (the latest version) should be supported. 
 Next versions don't exist. 
 SPDX-License-Identifier: GPL-2.0-only
  Copyright (c) 2014 SGI.
  All rights reserved.
  UTF-8 valid ranges.
  The UTF-8 encoding spreads the bits of a 32bit word over several
  bytes. This table gives the ranges that can be held and how they'd
  be represented.
  0x00000000 0x0000007F: 0xxxxxxx
  0x00000000 0x000007FF: 110xxxxx 10xxxxxx
  0x00000000 0x0000FFFF: 1110xxxx 10xxxxxx 10xxxxxx
  0x00000000 0x001FFFFF: 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
  0x00000000 0x03FFFFFF: 111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx
  0x00000000 0x7FFFFFFF: 1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx
  There is an additional requirement on UTF-8, in that only the
  shortest representation of a 32bit value is to be used.  A decoder
  must not decode sequences that do not satisfy this requirement.
  Thus the allowed ranges have a lower bound.
  0x00000000 0x0000007F: 0xxxxxxx
  0x00000080 0x000007FF: 110xxxxx 10xxxxxx
  0x00000800 0x0000FFFF: 1110xxxx 10xxxxxx 10xxxxxx
  0x00010000 0x001FFFFF: 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
  0x00200000 0x03FFFFFF: 111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx
  0x04000000 0x7FFFFFFF: 1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx
  Actual unicode characters are limited to the range 0x0 - 0x10FFFF,
  17 planes of 65536 values.  This limits the sequences actually seen
  even more, to just the following.
           0 -     0x7F: 0                   - 0x7F
        0x80 -    0x7FF: 0xC2 0x80           - 0xDF 0xBF
       0x800 -   0xFFFF: 0xE0 0xA0 0x80      - 0xEF 0xBF 0xBF
     0x10000 - 0x10FFFF: 0xF0 0x90 0x80 0x80 - 0xF4 0x8F 0xBF 0xBF
  Within those ranges the surrogates 0xD800 - 0xDFFF are not allowed.
  Note that the longest sequence seen with valid usage is 4 bytes,
  the same a single UTF-32 character.  This makes the UTF-8
  representation of Unicode strictly smaller than UTF-32.
  The shortest sequence requirement was introduced by:
     Corrigendum #1: UTF-8 Shortest Form
  It can be found here:
     http:www.unicode.orgversionscorrigendum1.html
  Return the number of bytes used by the current UTF-8 sequence.
  Assumes the input points to the first byte of a valid UTF-8
  sequence.
  Decode a 3-byte UTF-8 sequence.
  Encode a 3-byte UTF-8 sequence.
  utf8trie_t
  A compact binary tree, used to decode UTF-8 characters.
  Internal nodes are one byte for the node itself, and up to three
  bytes for an offset into the tree.  The first byte contains the
  following information:
   NEXTBYTE  - flag        - advance to next byte if set
   BITNUM    - 3 bit field - the bit number to tested
   OFFLEN    - 2 bit field - number of bytes in the offset
  if offlen == 0 (non-branching node)
   RIGHTPATH - 1 bit field - set if the following node is for the
                             right-hand path (tested bit is set)
   TRIENODE  - 1 bit field - set if the following node is an internal
                             node, otherwise it is a leaf node
  if offlen != 0 (branching node)
   LEFTNODE  - 1 bit field - set if the left-hand node is internal
   RIGHTNODE - 1 bit field - set if the right-hand node is internal
  Due to the way utf8 works, there cannot be branching nodes with
  NEXTBYTE set, and moreover those nodes always have a righthand
  descendant.
  utf8leaf_t
  The leaves of the trie are embedded in the trie, and so the same
  underlying datatype: unsigned char.
  leaf[0]: The unicode version, stored as a generation number that is
           an index into utf8agetab[].  With this we can filter code
           points based on the unicode version in which they were
           defined.  The CCC of a non-defined code point is 0.
  leaf[1]: Canonical Combining Class. During normalization, we need
           to do a stable sort into ascending order of all characters
           with a non-zero CCC that occur between two characters with
           a CCC of 0, or at the begin or end of a string.
           The unicode standard guarantees that all CCC values are
           between 0 and 254 inclusive, which leaves 255 available as
           a special value.
           Code points with CCC 0 are known as stoppers.
  leaf[2]: Decomposition. If leaf[1] == 255, then leaf[2] is the
           start of a NUL-terminated string that is the decomposition
           of the character.
           The CCC of a decomposable character is the same as the CCC
           of the first character of its decomposition.
           Some characters decompose as the empty string: these are
           characters with the Default_Ignorable_Code_Point property.
           These do affect normalization, as they all have CCC 0.
  The decompositions in the trie have been fully expanded, with the
  exception of Hangul syllables, which are decomposed algorithmically.
  Casefolding, if applicable, is also done using decompositions.
  The trie is constructed in such a way that leaves exist for all
  UTF-8 sequences that match the criteria from the "UTF-8 valid
  ranges" comment above, and only for those sequences.  Therefore a
  lookup in the trie can be used to validate the UTF-8 input.
 Marker for hangul syllable decomposition. 
 Size of the synthesized leaf used for Hangul syllable decomposition. 
  Hangul decomposition (algorithm from Section 3.12 of Unicode 6.3.0)
  AC00;<Hangul Syllable, First>;Lo;0;L;;;;;N;;;;;
  D7A3;<Hangul Syllable, Last>;Lo;0;L;;;;;N;;;;;
  SBase = 0xAC00
  LBase = 0x1100
  VBase = 0x1161
  TBase = 0x11A7
  LCount = 19
  VCount = 21
  TCount = 28
  NCount = 588 (VCount  TCount)
  SCount = 11172 (LCount  NCount)
  Decomposition:
    SIndex = s - SBase
  LV (CanonicalFull)
    LIndex = SIndex  NCount
    VIndex = (Sindex % NCount)  TCount
    LPart = LBase + LIndex
    VPart = VBase + VIndex
  LVT (Canonical)
    LVIndex = (SIndex  TCount)  TCount
    TIndex = (Sindex % TCount)
    LVPart = SBase + LVIndex
    TPart = TBase + TIndex
  LVT (Full)
    LIndex = SIndex  NCount
    VIndex = (Sindex % NCount)  TCount
    TIndex = (Sindex % TCount)
    LPart = LBase + LIndex
    VPart = VBase + VIndex
    if (TIndex == 0) {
           d = <LPart, VPart>
    } else {
           TPart = TBase + TIndex
           d = <LPart, TPart, VPart>
    }
 Constants 
 Algorithmic decomposition of hangul syllable. 
 Calculate the SI, LI, VI, and TI values. 
 Fill in base of leaf. 
 Add LPart, a 3-byte UTF-8 sequence. 
 Add VPart, a 3-byte UTF-8 sequence. 
 Add TPart if required, also a 3-byte UTF-8 sequence. 
 Terminate string. 
  Use trie to scan s, touching at most len bytes.
  Returns the leaf if one exists, NULL otherwise.
  A non-NULL return guarantees that the UTF-8 sequence starting at s
  is well-formed and corresponds to a known unicode code point.  The
  shorthand for this will be "is valid UTF-8 unicode".
 Right leg 
 Right node at offset of trie 
 Right node after this node 
 No right node. 
 Left leg 
 Left node after this node. 
 No left node. 
 Left node after this node 
	
	  Hangul decomposition is done algorithmically. These are the
	  codepoints >= 0xAC00 and <= 0xD7A3. Their UTF-8 encoding is
	  always 3 bytes long, so s has been advanced twice, and the
	  start of the sequence is at s-2.
  Use trie to scan s.
  Returns the leaf if one exists, NULL otherwise.
  Forwards to utf8nlookup().
  Maximum age of any character in s.
  Return -1 if s is not valid UTF-8 unicode.
  Return 0 if only non-assigned code points are used.
  Minimum age of any character in s.
  Return -1 if s is not valid UTF-8 unicode.
  Return 0 if non-assigned code points are used.
  Maximum age of any character in s, touch at most len bytes.
  Return -1 if s is not valid UTF-8 unicode.
  Maximum age of any character in s, touch at most len bytes.
  Return -1 if s is not valid UTF-8 unicode.
  Length of the normalization of s.
  Return -1 if s is not valid UTF-8 unicode.
  A string of Default_Ignorable_Code_Point has length 0.
  Length of the normalization of s, touch at most len bytes.
  Return -1 if s is not valid UTF-8 unicode.
  Set up an utf8cursor for use by utf8byte().
    u8c    : pointer to cursor.
    data   : const struct utf8data to use for normalization.
    s      : string.
    len    : length of s.
  Returns -1 on error, 0 on success.
 Check we didn't clobber the maximum length. 
 The first byte of s may not be an utf8 continuation. 
  Set up an utf8cursor for use by utf8byte().
    u8c    : pointer to cursor.
    data   : const struct utf8data to use for normalization.
    s      : NUL-terminated string.
  Returns -1 on error, 0 on success.
  Get one byte from the normalized form of the string described by u8c.
  Returns the byte cast to an unsigned char on succes, and -1 on failure.
  The cursor keeps track of the location in the string in u8c->s.
  When a character is decomposed, the current location is stored in
  u8c->p, and u8c->s is set to the start of the decomposition. Note
  that bytes from a decomposition do not count against u8c->len.
  Characters are emitted if they match the current CCC in u8c->ccc.
  Hitting end-of-string while u8c->ccc == STOPPER means we're done,
  and the function returns 0 in that case.
  Sorting by CCC is done by repeatedly scanning the string.  The
  values of u8c->s and u8c->p are stored in u8c->ss and u8c->sp at
  the start of the scan.  The first pass finds the lowest CCC to be
  emitted and stores it in u8c->nccc, the second pass emits the
  characters with this CCC and finds the next lowest CCC. This limits
  the number of passes to 1 + the number of different CCCs in the
  sequence being scanned.
  Therefore:
   u8c->p  != NULL -> a decomposition is being scanned.
   u8c->ss != NULL -> this is a repeating scan.
   u8c->ccc == -1   -> this is the first scan of a repeating scan.
 Check for the end of a decomposed character. 
 Check for end-of-string. 
 There is no next byte. 
 End-of-string during a scan counts as a stopper. 
 This is a continuation of the current character. 
 Look up the data for the current character. 
 No leaf found implies that the input is a binary blob. 
 Characters that are too new have CCC 0. 
 Empty decomposition implies CCC 0. 
		
		  If this is not a stopper, then see if it updates
		  the next canonical class to be emitted.
		
		  Return the current byte if this is the current
		  combining class.
 Current combining class mismatch. 
			
			  Scan forward for the first canonical class
			  to be emitted.  Save the position from
			  which to restart.
 Not a stopper, and not the ccc we're emitting. 
 At a stopper, restart for next ccc. 
 All done, proceed from here. 
 SPDX-License-Identifier: GPL-2.0-only
  eCryptfs: Linux filesystem encryption layer
  Copyright (C) 2008 International Business Machines Corp.
    Author(s): Michael A. Halcrow <mhalcrow@us.ibm.com>
  ecryptfs_miscdev_poll
  @file: dev file
  @pt: dev poll table (ignored)
  Returns the poll mask
  ecryptfs_miscdev_open
  @inode: inode of miscdev handle (ignored)
  @file: file for miscdev handle
  Returns zero on success; non-zero otherwise
  ecryptfs_miscdev_release
  @inode: inode of fsecryptfseuid handle (ignored)
  @file: file for fsecryptfseuid handle
  This keeps the daemon registered until the daemon sends another
  ioctl to fsecryptfsctl or until the kernel module unregisters.
  Returns zero on success; non-zero otherwise
  ecryptfs_send_miscdev
  @data: Data to send to daemon; may be NULL
  @data_size: Amount of data to send to daemon
  @msg_ctx: Message context, which is used to handle the reply. If
            this is NULL, then we do not expect a reply.
  @msg_type: Type of message
  @msg_flags: Flags for message
  @daemon: eCryptfs daemon object
  Add msg_ctx to queue and then, if it exists, notify the blocked
  miscdevess about the data being available. Must be called with
  ecryptfs_daemon_hash_mux held.
  Returns zero on success; non-zero otherwise
  miscdevfs packet format:
   Octet 0: Type
   Octets 1-4: network byte order msg_ctx->counter
   Octets 5-N0: Size of struct ecryptfs_message to follow
   Octets N0-N1: struct ecryptfs_message (including data)
   Octets 5-N1 not written if the packet type does not include a message
 4 + ECRYPTFS_MAX_ENCRYPTED_KEY_BYTES comes from tag 65 packet format 
  ecryptfs_miscdev_read - format and send message from queue
  @file: miscdevfs handle
  @buf: User buffer into which to copy the next message on the daemon queue
  @count: Amount of space available in @buf
  @ppos: Offset in file (ignored)
  Pulls the most recent message from the daemon queue, formats it for
  being sent via a miscdevfs handle, and copies it into @buf
  Returns the number of bytes copied into the user buffer
 This daemon will not go away so long as this flag is set 
		 Something else jumped in since the
		  wait_event_interruptable() and removed the
	 We do not expect a reply from the userspace daemon for any
  ecryptfs_miscdev_response - miscdevess response to message previously sent to daemon
  @daemon: eCryptfs daemon object
  @data: Bytes comprising struct ecryptfs_message
  @data_size: sizeof(struct ecryptfs_message) + data len
  @seq: Sequence number for miscdev response packet
  Returns zero on success; non-zero otherwise
  ecryptfs_miscdev_write - handle write to daemon miscdev handle
  @file: File for misc dev handle
  @buf: Buffer containing user data
  @count: Amount of data in @buf
  @ppos: Pointer to offset in file (ignored)
  Returns the number of bytes read from @buf
 Likely a harmless MSG_HELO or MSG_QUIT - no packet length 
  ecryptfs_init_ecryptfs_miscdev
  Messages sent to the userspace daemon from the kernel are placed on
  a queue associated with the daemon. The next read against the
  miscdev handle by that daemon will return the oldest message placed
  on the message queue for the daemon.
  Returns zero on success; non-zero otherwise
  ecryptfs_destroy_ecryptfs_miscdev
  All of the daemons must be exorcised prior to calling this
  function.
 SPDX-License-Identifier: GPL-2.0-or-later
  eCryptfs: Linux filesystem encryption layer
  Copyright (C) 1997-2003 Erez Zadok
  Copyright (C) 2001-2003 Stony Brook University
  Copyright (C) 2004-2006 International Business Machines Corp.
    Author(s): Michael A. Halcrow <mahalcro@us.ibm.com>
  ecryptfs_d_revalidate - revalidate an ecryptfs dentry
  @dentry: The ecryptfs dentry
  @flags: lookup flags
  Called when the VFS needs to revalidate a dentry. This
  is called whenever a name lookup finds a dentry in the
  dcache. Most filesystems leave this as NULL, because all their
  dentries in the dcache are valid.
  Returns 1 if valid, 0 otherwise.
  ecryptfs_d_release
  @dentry: The ecryptfs dentry
  Called when a dentry is really deallocated.
 SPDX-License-Identifier: GPL-2.0-or-later
  eCryptfs: Linux filesystem encryption layer
  Copyright (C) 1997-2003 Erez Zadok
  Copyright (C) 2001-2003 Stony Brook University
  Copyright (C) 2004-2006 International Business Machines Corp.
    Author(s): Michael A. Halcrow <mahalcro@us.ibm.com>
               Michael C. Thompson <mcthomps@us.ibm.com>
  ecryptfs_alloc_inode - allocate an ecryptfs inode
  @sb: Pointer to the ecryptfs super block
  Called to bring an inode into existence.
  Only handle allocation, setting up structures should be done in
  ecryptfs_read_inode. This is because the kernel, between now and
  then, will 0 out the private data pointer.
  Returns a pointer to a newly allocated inode, NULL otherwise
  ecryptfs_destroy_inode
  @inode: The ecryptfs inode
  This is used during the final destruction of the inode.  All
  allocation of memory related to the inode, including allocated
  memory in the crypt_stat struct, will be released here.
  There should be no chance that this deallocation will be missed.
  ecryptfs_statfs
  @dentry: The ecryptfs dentry
  @buf: The struct kstatfs to fill in with stats
  Get the filesystem statistics. Currently, we let this pass right through
  to the lower filesystem and take no action ourselves.
  ecryptfs_evict_inode
  @inode: The ecryptfs inode
  Called by iput() when the inode reference count reached zero
  and the inode is not hashed anywhere.  Used to clear anything
  that needs to be, before the inode is completely destroyed and put
  on the inode free list. We use this to drop out reference to the
  lower inode.
  ecryptfs_show_options
  Prints the mount options for a given superblock.
  Returns zero; does not fail.
 SPDX-License-Identifier: GPL-2.0-or-later
  eCryptfs: Linux filesystem encryption layer
  Copyright (C) 2008 International Business Machines Corp.
    Author(s): Michael A. Halcrow <mahalcro@us.ibm.com>
  ecryptfs_threadfn
  @ignored: ignored
  The eCryptfs kernel thread that has the responsibility of getting
  the lower file with RW permissions.
  Returns zero on success; non-zero otherwise
  ecryptfs_privileged_open
  @lower_file: Result of dentry_open by root on lower dentry
  @lower_dentry: Lower dentry for file to open
  @lower_mnt: Lower vfsmount for file to open
  @cred: credential to use for this call
  This function gets a rw file opened against the lower dentry.
  Returns zero on success; non-zero otherwise
	 Corresponding dput() and mntput() are done when the
	  lower file is fput() when all eCryptfs files for the inode are
 SPDX-License-Identifier: GPL-2.0-or-later
  eCryptfs: Linux filesystem encryption layer
  In-kernel key management code.  Includes functions to parse and
  write authentication token-related packets with the underlying
  file.
  Copyright (C) 2004-2006 International Business Machines Corp.
    Author(s): Michael A. Halcrow <mhalcrow@us.ibm.com>
               Michael C. Thompson <mcthomps@us.ibm.com>
               Trevor S. Highland <trevor.highland@gmail.com>
  request_key returned an error instead of a valid key address;
  determine the type of error, make appropriate log entries, and
  return an error code.
  ecryptfs_parse_packet_length
  @data: Pointer to memory containing length at offset
  @size: This function writes the decoded size to this memory
         address; zero on error
  @length_size: The number of bytes occupied by the encoded length
  Returns zero on success; non-zero on error
 One-byte length 
 Two-byte length 
 If support is added, adjust ECRYPTFS_MAX_PKT_LEN_SIZE 
  ecryptfs_write_packet_length
  @dest: The byte array target into which to write the length. Must
         have at least ECRYPTFS_MAX_PKT_LEN_SIZE bytes allocated.
  @size: The length to write.
  @packet_size_length: The number of bytes used to encode the packet
                       length is written to this address.
  Returns zero on success; non-zero on error.
 If support is added, adjust ECRYPTFS_MAX_PKT_LEN_SIZE 
	
	                TAG 64 Packet Format 
	     | Content Type                       | 1 byte       |
	     | Key Identifier Size                | 1 or 2 bytes |
	     | Key Identifier                     | arbitrary    |
	     | Encrypted File Encryption Key Size | 1 or 2 bytes |
	     | Encrypted File Encryption Key      | arbitrary    |
	
	                TAG 65 Packet Format 
	          | Content Type             | 1 byte       |
	          | Status Indicator         | 1 byte       |
	          | File Encryption Key Size | 1 or 2 bytes |
	          | File Encryption Key      | arbitrary    |
 The decrypted key includes 1 byte cipher code and 2 byte checksum 
	
	                TAG 66 Packet Format 
	          | Content Type             | 1 byte       |
	          | Key Identifier Size      | 1 or 2 bytes |
	          | Key Identifier           | arbitrary    |
	          | File Encryption Key Size | 1 or 2 bytes |
	          | File Encryption Key      | arbitrary    |
 The encrypted key includes 1 byte cipher code and 2 byte checksum 
	
	                TAG 65 Packet Format 
	     | Content Type                       | 1 byte       |
	     | Status Indicator                   | 1 byte       |
	     | Encrypted File Encryption Key Size | 1 or 2 bytes |
	     | Encrypted File Encryption Key      | arbitrary    |
 verify that everything through the encrypted FEK size is present 
  ecryptfs_verify_version
  @version: The version number to confirm
  Returns zero on good version; non-zero otherwise
  ecryptfs_verify_auth_tok_from_key
  @auth_tok_key: key containing the authentication token
  @auth_tok: authentication token
  Returns zero on valid auth tok; -EINVAL if the payload is invalid; or
  -EKEYREVOKED if the key was revoked before we acquired its semaphore.
  ecryptfs_find_auth_tok_for_sig
  @auth_tok_key: key containing the authentication token
  @auth_tok: Set to the matching auth_tok; NULL if not found
  @mount_crypt_stat: inode crypt_stat crypto context
  @sig: Sig of auth_tok to find
  For now, this function simply looks at the registered auth_tok's
  linked off the mount_crypt_stat, so all the auth_toks that can be
  used must be registered at mount time. This function could
  potentially try a lot harder to find auth_tok's (e.g., by calling
  out to ecryptfsd to dynamically retrieve an auth_tok object) so
  that static registration of auth_tok's will no longer be necessary.
  Returns zero on no error; non-zero on error
		 if the flag ECRYPTFS_GLOBAL_MOUNT_AUTH_TOK_ONLY is set in the
		  mount_crypt_stat structure, we prevent to use auth toks that
		  are not inserted through the ecryptfs_add_global_auth_tok
		  function.
  write_tag_70_packet can gobble a lot of stack space. We stuff most
  of the function's parameters in a kmalloc'd struct to help reduce
  eCryptfs' overall stack usage.
  write_tag_70_packet - Write encrypted filename (EFN) packet against FNEK
  @filename: NULL-terminated filename string
  This is the simplest mechanism for achieving filename encryption in
  eCryptfs. It encrypts the given filename with the mount-wide
  filename encryption key (FNEK) and stores it in a packet to @dest,
  which the callee will encode and write directly into the dentry
  name.
	 Plus one for the \0 separator between the random prefix
	 Octet 0: Tag 70 identifier
	  Octets 1-N1: Tag 70 packet size (includes cipher identifier
	               and block-aligned encrypted filename size)
	  Octets N1-N2: FNEK sig (ECRYPTFS_SIG_SIZE)
	  Octet N2-N3: Cipher identifier (1 octet)
	  Octets N3-N4: Block-aligned encrypted filename
	   - Consists of a minimum number of random characters, a \0
 Cipher code 
	 TODO: Support other key modules than passphrase for
	 The characters in the first block effectively do the job
	  of the IV here, so we just use 0's for the IV. Note the
	  constraint that ECRYPTFS_FILENAME_MIN_RANDOM_PREPEND_BYTES
  ecryptfs_parse_tag_70_packet - Parse and process FNEK-encrypted passphrase packet
  @filename: This function kmalloc's the memory for the filename
  @filename_size: This function sets this to the amount of memory
                  kmalloc'd for the filename
  @packet_size: This function sets this to the the number of octets
                in the packet parsed
  @mount_crypt_stat: The mount-wide cryptographic context
  @data: The memory location containing the start of the tag 70
         packet
  @max_packet_size: The maximum legal size of the packet to be parsed
                    from @data
  Returns zero on success; non-zero otherwise
	 Octet 0: Tag 70 identifier
	  Octets 1-N1: Tag 70 packet size (includes cipher identifier
	               and block-aligned encrypted filename size)
	  Octets N1-N2: FNEK sig (ECRYPTFS_SIG_SIZE)
	  Octet N2-N3: Cipher identifier (1 octet)
	  Octets N3-N4: Block-aligned encrypted filename
	   - Consists of a minimum number of random numbers, a \0
	 The characters in the first block effectively do the job of
	  the IV here, so we just use 0's for the IV. Note the
	  constraint that ECRYPTFS_FILENAME_MIN_RANDOM_PREPEND_BYTES
	 TODO: Support other key modules than passphrase for
  decrypt_pki_encrypted_session_key - Decrypt the session key with the given auth_tok.
  @auth_tok: The key authentication token used to decrypt the session key
  @crypt_stat: The cryptographic context
  Returns zero on success; non-zero error otherwise.
  parse_tag_1_packet
  @crypt_stat: The cryptographic context to modify based on packet contents
  @data: The raw bytes of the packet.
  @auth_tok_list: eCryptfs parses packets into authentication tokens;
                  a new authentication token will be placed at the
                  end of this list for this packet.
  @new_auth_tok: Pointer to a pointer to memory that this function
                 allocates; sets the memory address of the pointer to
                 NULL on error. This object is added to the
                 auth_tok_list.
  @packet_size: This function writes the size of the parsed packet
                into this memory location; zero on error.
  @max_packet_size: The maximum allowable packet size
  Returns zero on success; non-zero on error.
	
	  This format is inspired by OpenPGP; see RFC 2440
	  packet tag 1
	 
	  Tag 1 identifier (1 byte)
	  Max Tag 1 packet size (max 3 bytes)
	  Version (1 byte)
	  Key identifier (8 bytes; ECRYPTFS_SIG_SIZE)
	  Cipher identifier (1 byte)
	  Encrypted key size (arbitrary)
	 
	  12 bytes minimum packet size
	 Released: wipe_auth_tok_list called in ecryptfs_parse_packet_set or
	 This byte is skipped because the kernel does not need to
  parse_tag_3_packet
  @crypt_stat: The cryptographic context to modify based on packet
               contents.
  @data: The raw bytes of the packet.
  @auth_tok_list: eCryptfs parses packets into authentication tokens;
                  a new authentication token will be placed at the end
                  of this list for this packet.
  @new_auth_tok: Pointer to a pointer to memory that this function
                 allocates; sets the memory address of the pointer to
                 NULL on error. This object is added to the
                 auth_tok_list.
  @packet_size: This function writes the size of the parsed packet
                into this memory location; zero on error.
  @max_packet_size: maximum number of bytes to parse
  Returns zero on success; non-zero on error.
	
	 This format is inspired by OpenPGP; see RFC 2440
	  packet tag 3
	 
	  Tag 3 identifier (1 byte)
	  Max Tag 3 packet size (max 3 bytes)
	  Version (1 byte)
	  Cipher code (1 byte)
	  S2K specifier (1 byte)
	  Hash identifier (1 byte)
	  Salt (ECRYPTFS_SALT_SIZE)
	  Hash iterations (1 byte)
	  Encrypted key (arbitrary)
	 
	  (ECRYPTFS_SALT_SIZE + 7) minimum packet size
	 Released: wipe_auth_tok_list called in ecryptfs_parse_packet_set or
	 A little extra work to differentiate among the AES key
 TODO: finish the hash mapping 
 See RFC2440 for these numbers and their mappings 
 Choose MD5 
 This conversion was taken straight from RFC2440 
		 Friendly reminder:
		  (new_auth_tok)->session_key.encrypted_key_size =
 MD5 
	 TODO: Parametarize; we might actually want userspace to
  parse_tag_11_packet
  @data: The raw bytes of the packet
  @contents: This function writes the data contents of the literal
             packet into this memory location
  @max_contents_bytes: The maximum number of bytes that this function
                       is allowed to write into contents
  @tag_11_contents_size: This function writes the size of the parsed
                         contents into this memory location; zero on
                         error
  @packet_size: This function writes the size of the parsed packet
                into this memory location; zero on error
  @max_packet_size: maximum number of bytes to parse
  Returns zero on success; non-zero on error.
	 This format is inspired by OpenPGP; see RFC 2440
	  packet tag 11
	 
	  Tag 11 identifier (1 byte)
	  Max Tag 11 packet size (max 3 bytes)
	  Binary format specifier (1 byte)
	  Filename length (1 byte)
	  Filename ("_CONSOLE") (8 bytes)
	  Modification date (4 bytes)
	  Literal data (arbitrary)
	 
	  We need at least 16 bytes of data for the packet to even be
	  valid.
 Ignore filename and modification date 
  decrypt_passphrase_encrypted_session_key - Decrypt the session key with the given auth_tok.
  @auth_tok: The passphrase authentication token to use to encrypt the FEK
  @crypt_stat: The cryptographic context
  Returns zero on success; non-zero error otherwise
  ecryptfs_parse_packet_set
  @crypt_stat: The cryptographic context
  @src: Virtual address of region of memory containing the packets
  @ecryptfs_dentry: The eCryptfs dentry associated with the packet set
  Get crypt_stat to have the file's session key if the requisite key
  is available to decrypt the session key.
  Returns Zero if a valid authentication token was retrieved and
  processed; negative value for file not encrypted or for error
  conditions.
	 Parse the header to find as many packets as we can; these will be
	 auth_tok_list contains the set of authentication tokens
	  parsed from the metadata. We need to find a matching
	  authentication token that has the secret component(s)
	  necessary to decrypt the EFEK in the auth_tok parsed from
	  the metadata. There may be several potential matches, but
  write_tag_1_packet - Write an RFC2440-compatible tag 1 (public key) packet
  @dest: Buffer into which to write the packet
  @remaining_bytes: Maximum number of bytes that can be writtn
  @auth_tok_key: The authentication token key to unlock and put when done with
                 @auth_tok
  @auth_tok: The authentication token used for generating the tag 1 packet
  @crypt_stat: The cryptographic context
  @key_rec: The key record struct for the tag 1 packet
  @packet_size: This function will write the number of bytes that end
                up constituting the packet; set to zero on error
  Returns zero on success; non-zero on error.
	 This format is inspired by OpenPGP; see RFC 2440
 Tag 1 identifier 
 Max Tag 1 packet size 
 Version 
 Key identifier 
 Cipher identifier 
 Encrypted key size 
 version 3 
  write_tag_11_packet
  @dest: Target into which Tag 11 packet is to be written
  @remaining_bytes: Maximum packet length
  @contents: Byte array of contents to copy in
  @contents_length: Number of bytes in contents
  @packet_length: Length of the Tag 11 packet written; zero on error
  Returns zero on success; non-zero on error.
	 This format is inspired by OpenPGP; see RFC 2440
 Tag 11 identifier 
 Max Tag 11 packet size 
 Binary format specifier 
 Filename length 
 Filename ("_CONSOLE") 
 Modification date 
 Literal data 
 binary data format specifier 
  write_tag_3_packet
  @dest: Buffer into which to write the packet
  @remaining_bytes: Maximum number of bytes that can be written
  @auth_tok: Authentication token
  @crypt_stat: The cryptographic context
  @key_rec: encrypted key
  @packet_size: This function will write the number of bytes that end
                up constituting the packet; set to zero on error
  Returns zero on success; non-zero on error.
	 This format is inspired by OpenPGP; see RFC 2440
 Tag 3 identifier 
 Max Tag 3 packet size 
 Version 
 Cipher code 
 S2K specifier 
 Hash identifier 
 Salt 
 Hash iterations 
 Encrypted key size 
	 Chop off the Tag 3 identifier(1) and Tag 3 packet size(3)
 version 4 
	 TODO: Break from RFC2440 so that arbitrary ciphers can be
 S2K 
 MD5 (TODO: parameterize) 
 salt 
 hash iterations (65536) 
  ecryptfs_generate_key_packet_set
  @dest_base: Virtual address from which to write the key record set
  @crypt_stat: The cryptographic context from which the
               authentication tokens will be retrieved
  @ecryptfs_dentry: The dentry, used to retrieve the mount crypt stat
                    for the global parameters
  @len: The amount written
  @max: The maximum amount of data allowed to be written
  Generates a key packet set and writes it to the virtual address
  passed in.
  Returns zero on success; non-zero on error.
 Write auth tok signature packet 
 Caller must hold keysig_list_mutex 
 SPDX-License-Identifier: GPL-2.0-or-later
  eCryptfs: Linux filesystem encryption layer
  Copyright (C) 1997-2004 Erez Zadok
  Copyright (C) 2001-2004 Stony Brook University
  Copyright (C) 2004-2007 International Business Machines Corp.
    Author(s): Michael A. Halcrow <mahalcro@us.ibm.com>
               Michael C. Thompsion <mcthomps@us.ibm.com>
 i_size will be overwritten for encrypted regular files 
  ecryptfs_interpose
  @lower_dentry: Existing dentry in the lower filesystem
  @dentry: ecryptfs' dentry
  @sb: ecryptfs's super_block
  Interposes upper and lower dentries.
  Returns zero on success; non-zero otherwise
 don't even try to make the lower negative
  ecryptfs_do_create
  @directory_inode: inode of the new file's dentry's parent in ecryptfs
  @ecryptfs_dentry: New file's dentry in ecryptfs
  @mode: The mode of the new file
  Creates the underlying file and the eCryptfs inode which will link to
  it. It will also update the eCryptfs directory inode to mimic the
  stat of the lower directory inode.
  Returns the new eCryptfs inode on success; an ERR_PTR on error condition
  ecryptfs_initialize_file
  Cause the file to be changed from a basic empty file to an ecryptfs
  file with a header and first data page.
  Returns zero on success
  ecryptfs_create
  @mode: The mode of the new file.
  Creates a new file.
  Returns zero on success; non-zero on error condition
	 At this point, a file exists on "disk"; we need to make sure
 TODO: lock for crypt_stat comparison 
 Must return 0 to allow non-eCryptfs files to be looked up, too 
  ecryptfs_lookup_interpose - Dentry interposition for a lookup
	
	  negative dentry can go positive under us here - its parent is not
	  locked.  That's OK and that could happen just as we return from
	  ecryptfs_lookup() anyway.  Just need to be careful and fetch
	  ->d_inode only once - it's not stable here.
 We want to add because we couldn't find in lower 
  ecryptfs_lookup
  @ecryptfs_dir_inode: The eCryptfs directory inode
  @ecryptfs_dentry: The eCryptfs dentry that we are looking up
  @flags: lookup flags
  Find a file on disk. If the file does not exist, then we'll add it to the
  dentry cache and continue on to read it from the disk.
 don't even try to make the lower negative
 source should not be ancestor of target 
 target should not be ancestor of source 
  upper_size_to_lower_size
  @crypt_stat: Crypt_stat associated with file
  @upper_size: Size of the upper file
  Calculate the required size of the lower file based on the
  specified size of the upper file. This calculation is based on the
  number of headers in the underlying file and the extent size.
  Returns Calculated size of the lower file.
  truncate_upper
  @dentry: The ecryptfs layer dentry
  @ia: Address of the ecryptfs inode's attributes
  @lower_ia: Address of the lower inode's attributes
  Function to handle truncations modifying the size of the file. Note
  that the file sizes are interpolated. When expanding, we are simply
  writing strings of 0's out. When truncating, we truncate the upper
  inode and update the lower_ia according to the page index
  interpolations. If ATTR_SIZE is set in lower_ia->ia_valid upon return,
  the caller must use lower_ia in a call to notify_change() to perform
  the truncation of the lower inode.
  Returns zero on success; non-zero otherwise
 Switch on growing or shrinking file 
		 Write a single 0 at the last position of the file;
		  this triggers code that will fill in 0's throughout
		  the intermediate portion of the previous end of the
 ia->ia_size < i_size_read(inode) 
		 We're chopping off all the pages down to the page
		  in which ia->ia_size is located. Fill in the end of
		  that page from (ia->ia_size & ~PAGE_MASK) to
		 We are reducing the size of the ecryptfs file, and need to
		
		  The eCryptfs inode and the new lower size are mixed here
		  because we may not have the lower i_mutex held andor it may
		  not be appropriate to call inode_newsize_ok() with inodes
		  from other filesystems.
  ecryptfs_truncate
  @dentry: The ecryptfs layer dentry
  @new_length: The length to expand the file to
  Simple function that handles the truncation of an eCryptfs inode and
  its corresponding lower inode.
  Returns zero on success; non-zero otherwise
  ecryptfs_setattr
  @mnt_userns: user namespace of the target mount
  @dentry: dentry handle to the inode to modify
  @ia: Structure with flags of what to change and values
  Updates the metadata of an inode. If the update is to the size
  i.e. truncation, then ecryptfs_truncate will handle the size modification
  of both the ecryptfs inode and the lower inode.
  All other metadata changes will be passed right to the lower filesystem,
  and we will just update our inode to look like the lower.
	
	  mode change is for clearing setuidsetgid bits. Allow lower fs
	  to interpret this in its own way.
 match anything 
 SPDX-License-Identifier: GPL-2.0-or-later
  eCryptfs: Linux filesystem encryption layer
  Copyright (C) 1997-2004 Erez Zadok
  Copyright (C) 2001-2004 Stony Brook University
  Copyright (C) 2004-2007 International Business Machines Corp.
    Author(s): Michael A. Halcrow <mahalcro@us.ibm.com>
    		Michael C. Thompson <mcthomps@us.ibm.com>
  ecryptfs_from_hex
  @dst: Buffer to take the bytes from src hex; must be at least of
        size (src_size  2)
  @src: Buffer to be converted from a hex string representation to raw value
  @dst_size: size of dst buffer, or number of hex characters pairs to convert
  ecryptfs_calculate_md5 - calculates the md5 of @src
  @dst: Pointer to 16 bytes of allocated memory
  @crypt_stat: Pointer to crypt_stat struct for the current inode
  @src: Data to be md5'd
  @len: Length of @src
  Uses the allocated crypto context that crypt_stat references to
  generate the MD5 sum of the contents of src.
  ecryptfs_derive_iv
  @iv: destination for the derived iv vale
  @crypt_stat: Pointer to crypt_stat struct for the current inode
  @offset: Offset of the extent whose IV we are to derive
  Generate the initialization vector from the given root IV and page
  offset.
  Returns zero on success; non-zero on error.
	 TODO: It is probably secure to just cast the least
	  significant bits of the root IV into an unsigned long and
	  add the offset to that rather than go through all this
  ecryptfs_init_crypt_stat
  @crypt_stat: Pointer to the crypt_stat struct to initialize.
  Initialize the crypt_stat structure.
  ecryptfs_destroy_crypt_stat
  @crypt_stat: Pointer to the crypt_stat struct to initialize.
  Releases all memory associated with a crypt_stat struct.
  virt_to_scatterlist
  @addr: Virtual address
  @size: Size of data; should be an even multiple of the block size
  @sg: Pointer to scatterlist array; set to NULL to obtain only
       the number of scatterlist structs required in array
  @sg_size: Max array size
  Fills in a scatterlist array with page references for a passed
  virtual address.
  Returns the number of scatterlist structs in array used
  crypt_scatterlist
  @crypt_stat: Pointer to the crypt_stat struct to initialize.
  @dst_sg: Destination of the data after performing the crypto operation
  @src_sg: Data to be encrypted or decrypted
  @size: Length of data
  @iv: IV to use
  @op: ENCRYPT or DECRYPT to indicate the desired operation
  Returns the number of bytes encrypted or decrypted; negative value on error
 Consider doing this once, when the file is opened 
  lower_offset_for_page
  Convert an eCryptfs page index into a lower byte offset
  crypt_extent
  @crypt_stat: crypt_stat containing cryptographic context for the
               encryption operation
  @dst_page: The page to write the result into
  @src_page: The page to read from
  @extent_offset: Page extent offset for use in generating IV
  @op: ENCRYPT or DECRYPT to indicate the desired operation
  Encrypts or decrypts one extent of data.
  Return zero on success; non-zero otherwise
  ecryptfs_encrypt_page
  @page: Page mapped from the eCryptfs inode for the file; contains
         decrypted content that needs to be encrypted (to a temporary
         page; not in place) and written out to the lower file
  Encrypt an eCryptfs page. This is done on a per-extent basis. Note
  that eCryptfs pages may straddle the lower pages -- for instance,
  if the file was created on a machine with an 8K page size
  (resulting in an 8K header), and then the file is copied onto a
  host with a 32K page size, then when reading page 0 of the eCryptfs
  file, 24K of page 0 of the lower file will be read and decrypted,
  and then 8K of page 1 of the lower file will be read and decrypted.
  Returns zero on success; negative on error
  ecryptfs_decrypt_page
  @page: Page mapped from the eCryptfs inode for the file; data read
         and decrypted from the lower file will be written into this
         page
  Decrypt an eCryptfs page. This is done on a per-extent basis. Note
  that eCryptfs pages may straddle the lower pages -- for instance,
  if the file was created on a machine with an 8K page size
  (resulting in an 8K header), and then the file is copied onto a
  host with a 32K page size, then when reading page 0 of the eCryptfs
  file, 24K of page 0 of the lower file will be read and decrypted,
  and then 8K of page 1 of the lower file will be read and decrypted.
  Returns zero on success; negative on error
  ecryptfs_init_crypt_ctx
  @crypt_stat: Uninitialized crypt stats structure
  Initialize the crypto context.
  TODO: Performance: Keep a cache of initialized cipher contexts;
  only init if needed
	 Default values; may be overwritten as we are parsing the
  ecryptfs_compute_root_iv
  On error, sets the root IV to all 0's.
  ecryptfs_copy_mount_wide_flags_to_inode_flags
  @crypt_stat: The inode's cryptographic context
  @mount_crypt_stat: The mount point's cryptographic context
  This function propagates the mount-wide flags to individual inode
  flags.
  ecryptfs_set_default_crypt_stat_vals
  @crypt_stat: The inode's cryptographic context
  @mount_crypt_stat: The mount point's cryptographic context
  Default values in the event that policy does not override them.
  ecryptfs_new_file_context
  @ecryptfs_inode: The eCryptfs inode
  If the crypto context for the file has not yet been established,
  this is where we do that.  Establishing a new crypto context
  involves the following decisions:
   - What cipher to use?
   - What set of authentication tokens to use?
  Here we just worry about getting enough information into the
  authentication tokens so that we know that they are available.
  We associate the available authentication tokens with the new file
  via the set of signatures in the crypt_stat struct.  Later, when
  the headers are actually written out, we may again defer to
  userspace to perform the encryption of the session key; for the
  foreseeable future, this will be the case with public key packets.
  Returns zero on success; non-zero otherwise
  ecryptfs_validate_marker - check for the ecryptfs marker
  @data: The data block in which to check
  Returns zero if marker found; -EINVAL if not found
 Add support for additional flags by adding elements here. 
  ecryptfs_process_flags
  @crypt_stat: The cryptographic context
  @page_virt: Source data to be parsed
  @bytes_read: Updated with the number of bytes read
 Version is in top 8 bits of the 32-bit flag vector 
  write_ecryptfs_marker
  @page_virt: The pointer to in a page to begin writing the marker
  @written: Number of bytes written
  Marker = 0x3c81b7f5
 Version is in top 8 bits of the 32-bit flag vector 
 Add support for additional ciphers by adding elements here. The
  cipher_code is whatever OpenPGP applications use to identify the
  ecryptfs_code_for_cipher_string
  @cipher_name: The string alias for the cipher
  @key_bytes: Length of key in bytes; used for AES code selection
  Returns zero on no match, or the cipher code on match
  ecryptfs_cipher_code_to_string
  @str: Destination to write out the cipher name
  @cipher_code: The code to convert to cipher name string
  Returns zero on success
  ecryptfs_write_headers_virt
  @page_virt: The virtual address to write the headers to
  @max: The size of memory allocated at page_virt
  @size: Set to the number of bytes written by this function
  @crypt_stat: The cryptographic context
  @ecryptfs_dentry: The eCryptfs dentry
  Format version: 1
    Header Extent:
      Octets 0-7:        Unencrypted file size (big-endian)
      Octets 8-15:       eCryptfs special marker
      Octets 16-19:      Flags
       Octet 16:         File format version number (between 0 and 255)
       Octets 17-18:     Reserved
       Octet 19:         Bit 1 (lsb): Reserved
                         Bit 2: Encrypted?
                         Bits 3-8: Reserved
      Octets 20-23:      Header extent size (big-endian)
      Octets 24-25:      Number of header extents at front of file
                         (big-endian)
      Octet  26:         Begin RFC 2440 authentication token packet set
    Data Extent 0:
      Lower data (CBC encrypted)
    Data Extent 1:
      Lower data (CBC encrypted)
    ...
  Returns zero on success
  ecryptfs_write_metadata
  @ecryptfs_dentry: The eCryptfs dentry, which should be negative
  @ecryptfs_inode: The newly created eCryptfs inode
  Write the file headers out.  This will likely involve a userspace
  callout, in which the session key is encrypted with one or more
  public keys andor the passphrase necessary to do the encryption is
  retrieved via a prompt.  Exactly what happens at this point should
  be policy-dependent.
  Returns zero on success; non-zero on error
 Released in this function 
 Zeroed page ensures the in-header unencrypted i_size is set to 0 
  set_default_header_data
  @crypt_stat: The cryptographic context
  For version 0 file format; this function is only for backwards
  compatibility for files created with the prior versions of
  eCryptfs.
  ecryptfs_read_headers_virt
  @page_virt: The virtual address into which to read the headers
  @crypt_stat: The cryptographic context
  @ecryptfs_dentry: The eCryptfs dentry
  @validate_header_size: Whether to validate the header size while reading
  Readparse the header data. The header format is detailed in the
  comment block for the ecryptfs_write_headers_virt() function.
  Returns zero on success
  ecryptfs_read_xattr_region
  @page_virt: The vitual address into which to read the xattr data
  @ecryptfs_inode: The eCryptfs inode
  Attempts to read the crypto metadata from the extended attribute
  region of the lower file.
  Returns zero on success; non-zero on error
  ecryptfs_read_metadata
  Common entry point for reading file metadata. From here, we could
  retrieve the header information from the header region of the file,
  the xattr region of the file, or some other repository that is
  stored separately from the file itself. The current implementation
  supports retrieving the metadata information from the file contents
  and from the xattr region.
  Returns zero if valid headers found and parsed; non-zero otherwise
 Read the first page from the underlying file 
 metadata is not in the file header, so try xattrs 
  ecryptfs_encrypt_filename - encrypt filename
  CBC-encrypts the filename. We do not want to encrypt the same
  filename with the same key and IV, which may happen with hard
  links, so we prepend random bits to each filename.
  Returns zero on success; non-zero otherwise
	(copied_name)[(name_size)] = '\0';	 Only for convenience
						  in printing out the
						  string in debug
  ecryptfs_process_key_cipher - Perform key cipher initialization.
  @key_tfm: Crypto context for key material, set by this function
  @cipher_name: Name of the cipher
  @key_size: Size of the key in bytes
  Returns zero on success. Any crypto_tfm structs allocated here
  should be released by other functions, such as on a superblock put
  event, regardless of whether this function succeeds for fails.
  ecryptfs_destroy_crypto - free all cached key_tfms on key_tfm_list
  Called only at module unload time
  ecryptfs_tfm_exists - Search for existing tfm for cipher_name.
  @cipher_name: the name of the cipher to search for
  @key_tfm: set to corresponding tfm if found
  Searches for cached key_tfm matching @cipher_name
  Must be called with &key_tfm_list_mutex held
  Returns 1 if found, with @key_tfm set
  Returns 0 if not found, with @key_tfm set to NULL
  ecryptfs_get_tfm_and_mutex_for_cipher_name
  @tfm: set to cached tfm found, or new tfm created
  @tfm_mutex: set to mutex for cached tfm found, or new tfm created
  @cipher_name: the name of the cipher to search for andor add
  Sets pointers to @tfm & @tfm_mutex matching @cipher_name.
  Searches for cached item first, and creates new if not found.
  Returns 0 on success, non-zero if adding new cipher failed
 64 characters forming a 6-bit target field 
 We could either offset on every reverse map or just pad some 0x00's
 7 
 15 
 23 
 31 
 39 
 47 
 55 
 63 
 71 
 79 
 87 
 95 
 103 
 111 
 119 
 123 - 255 initialized to 0x00 
  ecryptfs_encode_for_filename
  @dst: Destination location for encoded filename
  @dst_size: Size of the encoded filename in bytes
  @src: Source location for the filename to encode
  @src_size: Size of the source in bytes
	 Not exact; conservatively long. Every block of 4
	  encoded characters decodes into a block of 3
	  decoded characters. This segment of code provides
	  the caller with the maximum amount of allocated
	  space that @dst will need to point to in a
  ecryptfs_decode_from_filename
  @dst: If NULL, this function only sets @dst_size and returns. If
        non-NULL, this function decodes the encoded octets in @src
        into the memory that @dst points to.
  @dst_size: Set to the size of the decoded string.
  @src: The encoded set of octets to decode.
  @src_size: The size of the encoded set of octets to decode.
  ecryptfs_encrypt_and_encode_filename - converts a plaintext file name to cipher text
  @encoded_name: The encrypted name
  @encoded_name_size: Length of the encrypted name
  @mount_crypt_stat: The crypt_stat struct associated with the file name to encode
  @name: The plaintext name
  @name_size: The length of the plaintext name
  Encrypts and encodes a filename into something that constitutes a
  valid filename for a filesystem, with printable characters.
  We assume that we have a properly initialized crypto context,
  pointed to by crypt_stat->tfm.
  Returns zero on success; non-zero on otherwise
  ecryptfs_decode_and_decrypt_filename - converts the encoded cipher text name to decoded plaintext
  @plaintext_name: The plaintext name
  @plaintext_name_size: The plaintext name size
  @sb: Ecryptfs's super_block
  @name: The filename in cipher text
  @name_size: The cipher text name size
  Decrypts and decodes the filename.
  Returns zero on error; non-zero otherwise
 Return an exact amount for the common cases 
 Return a safe estimate for the uncommon cases 
 Since this is the max decoded size, subtract 1 "decoded block" len 
 Worst case is that the filename is padded nearly a full block size 
